commit bbe17c625d6843e9cdf14d81fbece1b0f0c3fb2f
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Wed Jan 11 18:06:49 2023 +0000

    KVM: x86/xen: Fix potential deadlock in kvm_xen_update_runstate_guest()
    
    The kvm_xen_update_runstate_guest() function can be called when the vCPU
    is being scheduled out, from a preempt notifier. It *opportunistically*
    updates the runstate area in the guest memory, if the gfn_to_pfn_cache
    which caches the appropriate address is still valid.
    
    If there is *contention* when it attempts to obtain gpc->lock, then
    locking inside the priority inheritance checks may cause a deadlock.
    Lockdep reports:
    
    [13890.148997] Chain exists of:
                     &gpc->lock --> &p->pi_lock --> &rq->__lock
    
    [13890.149002]  Possible unsafe locking scenario:
    
    [13890.149003]        CPU0                    CPU1
    [13890.149004]        ----                    ----
    [13890.149005]   lock(&rq->__lock);
    [13890.149007]                                lock(&p->pi_lock);
    [13890.149009]                                lock(&rq->__lock);
    [13890.149011]   lock(&gpc->lock);
    [13890.149013]
                    *** DEADLOCK ***
    
    In the general case, if there's contention for a read lock on gpc->lock,
    that's going to be because something else is either invalidating or
    revalidating the cache. Either way, we've raced with seeing it in an
    invalid state, in which case we would have aborted the opportunistic
    update anyway.
    
    So in the 'atomic' case when called from the preempt notifier, just
    switch to using read_trylock() and avoid the PI handling altogether.
    
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Message-Id: <20230111180651.14394-2-dwmw2@infradead.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit ab4958975490d0fbd2f9709e40e66bd2c679297b
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Aug 18 22:40:09 2022 -0700

    f2fs: complete checkpoints during remount
    
    commit 4f99484d27961cb194cebcd917176fa038a5025f upstream.
    
    Otherwise, pending checkpoints can contribute a race condition to give a
    quota warning.
    
    - Thread                      - checkpoint thread
                                  add checkpoints to the list
    do_remount()
     down_write(&sb->s_umount);
     f2fs_remount()
                                  block_operations()
                                   down_read_trylock(&sb->s_umount) = 0
     up_write(&sb->s_umount);
                                   f2fs_quota_sync()
                                    dquot_writeback_dquots()
                                     WARN_ON_ONCE(!rwsem_is_locked(&sb->s_umount));
    
    Or,
    
    do_remount()
     down_write(&sb->s_umount);
     f2fs_remount()
                                  create a ckpt thread
                                  f2fs_enable_checkpoint() adds checkpoints
                                  wait for f2fs_sync_fs()
                                  trigger another pending checkpoint
                                   block_operations()
                                    down_read_trylock(&sb->s_umount) = 0
     up_write(&sb->s_umount);
                                    f2fs_quota_sync()
                                     dquot_writeback_dquots()
                                      WARN_ON_ONCE(!rwsem_is_locked(&sb->s_umount));
    
    Cc: stable@vger.kernel.org
    Reviewed-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0fa4033d00be98741169f92be340d29a25be9094
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Aug 18 22:40:09 2022 -0700

    f2fs: complete checkpoints during remount
    
    commit 4f99484d27961cb194cebcd917176fa038a5025f upstream.
    
    Otherwise, pending checkpoints can contribute a race condition to give a
    quota warning.
    
    - Thread                      - checkpoint thread
                                  add checkpoints to the list
    do_remount()
     down_write(&sb->s_umount);
     f2fs_remount()
                                  block_operations()
                                   down_read_trylock(&sb->s_umount) = 0
     up_write(&sb->s_umount);
                                   f2fs_quota_sync()
                                    dquot_writeback_dquots()
                                     WARN_ON_ONCE(!rwsem_is_locked(&sb->s_umount));
    
    Or,
    
    do_remount()
     down_write(&sb->s_umount);
     f2fs_remount()
                                  create a ckpt thread
                                  f2fs_enable_checkpoint() adds checkpoints
                                  wait for f2fs_sync_fs()
                                  trigger another pending checkpoint
                                   block_operations()
                                    down_read_trylock(&sb->s_umount) = 0
     up_write(&sb->s_umount);
                                    f2fs_quota_sync()
                                     dquot_writeback_dquots()
                                      WARN_ON_ONCE(!rwsem_is_locked(&sb->s_umount));
    
    Cc: stable@vger.kernel.org
    Reviewed-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit edbb454fb315ded913cc1f639b37510299fc05d0
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Aug 18 22:40:09 2022 -0700

    f2fs: complete checkpoints during remount
    
    commit 4f99484d27961cb194cebcd917176fa038a5025f upstream.
    
    Otherwise, pending checkpoints can contribute a race condition to give a
    quota warning.
    
    - Thread                      - checkpoint thread
                                  add checkpoints to the list
    do_remount()
     down_write(&sb->s_umount);
     f2fs_remount()
                                  block_operations()
                                   down_read_trylock(&sb->s_umount) = 0
     up_write(&sb->s_umount);
                                   f2fs_quota_sync()
                                    dquot_writeback_dquots()
                                     WARN_ON_ONCE(!rwsem_is_locked(&sb->s_umount));
    
    Or,
    
    do_remount()
     down_write(&sb->s_umount);
     f2fs_remount()
                                  create a ckpt thread
                                  f2fs_enable_checkpoint() adds checkpoints
                                  wait for f2fs_sync_fs()
                                  trigger another pending checkpoint
                                   block_operations()
                                    down_read_trylock(&sb->s_umount) = 0
     up_write(&sb->s_umount);
                                    f2fs_quota_sync()
                                     dquot_writeback_dquots()
                                      WARN_ON_ONCE(!rwsem_is_locked(&sb->s_umount));
    
    Cc: stable@vger.kernel.org
    Reviewed-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4f99484d27961cb194cebcd917176fa038a5025f
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Aug 18 22:40:09 2022 -0700

    f2fs: complete checkpoints during remount
    
    Otherwise, pending checkpoints can contribute a race condition to give a
    quota warning.
    
    - Thread                      - checkpoint thread
                                  add checkpoints to the list
    do_remount()
     down_write(&sb->s_umount);
     f2fs_remount()
                                  block_operations()
                                   down_read_trylock(&sb->s_umount) = 0
     up_write(&sb->s_umount);
                                   f2fs_quota_sync()
                                    dquot_writeback_dquots()
                                     WARN_ON_ONCE(!rwsem_is_locked(&sb->s_umount));
    
    Or,
    
    do_remount()
     down_write(&sb->s_umount);
     f2fs_remount()
                                  create a ckpt thread
                                  f2fs_enable_checkpoint() adds checkpoints
                                  wait for f2fs_sync_fs()
                                  trigger another pending checkpoint
                                   block_operations()
                                    down_read_trylock(&sb->s_umount) = 0
     up_write(&sb->s_umount);
                                    f2fs_quota_sync()
                                     dquot_writeback_dquots()
                                      WARN_ON_ONCE(!rwsem_is_locked(&sb->s_umount));
    
    Cc: stable@vger.kernel.org
    Reviewed-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

commit a1a2d8f0162b27e85e7ce0ae6a35c96a490e0559
Author: Coly Li <colyli@suse.de>
Date:   Sat May 28 20:45:50 2022 +0800

    bcache: avoid unnecessary soft lockup in kworker update_writeback_rate()
    
    The kworker routine update_writeback_rate() is schedued to update the
    writeback rate in every 5 seconds by default. Before calling
    __update_writeback_rate() to do real job, semaphore dc->writeback_lock
    should be held by the kworker routine.
    
    At the same time, bcache writeback thread routine bch_writeback_thread()
    also needs to hold dc->writeback_lock before flushing dirty data back
    into the backing device. If the dirty data set is large, it might be
    very long time for bch_writeback_thread() to scan all dirty buckets and
    releases dc->writeback_lock. In such case update_writeback_rate() can be
    starved for long enough time so that kernel reports a soft lockup warn-
    ing started like:
      watchdog: BUG: soft lockup - CPU#246 stuck for 23s! [kworker/246:31:179713]
    
    Such soft lockup condition is unnecessary, because after the writeback
    thread finishes its job and releases dc->writeback_lock, the kworker
    update_writeback_rate() may continue to work and everything is fine
    indeed.
    
    This patch avoids the unnecessary soft lockup by the following method,
    - Add new member to struct cached_dev
      - dc->rate_update_retry (0 by default)
    - In update_writeback_rate() call down_read_trylock(&dc->writeback_lock)
      firstly, if it fails then lock contention happens.
    - If dc->rate_update_retry <= BCH_WBRATE_UPDATE_MAX_SKIPS (15), doesn't
      acquire the lock and reschedules the kworker for next try.
    - If dc->rate_update_retry > BCH_WBRATE_UPDATE_MAX_SKIPS, no retry
      anymore and call down_read(&dc->writeback_lock) to wait for the lock.
    
    By the above method, at worst case update_writeback_rate() may retry for
    1+ minutes before blocking on dc->writeback_lock by calling down_read().
    For a 4TB cache device with 1TB dirty data, 90%+ of the unnecessary soft
    lockup warning message can be avoided.
    
    When retrying to acquire dc->writeback_lock in update_writeback_rate(),
    of course the writeback rate cannot be updated. It is fair, because when
    the kworker is blocked on the lock contention of dc->writeback_lock, the
    writeback rate cannot be updated neither.
    
    This change follows Jens Axboe's suggestion to a more clear and simple
    version.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Link: https://lore.kernel.org/r/20220528124550.32834-2-colyli@suse.de
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit d58d281d6a3f49ce83b82f8211c396ba34c0304f
Author: Pavel Skripkin <paskripkin@gmail.com>
Date:   Thu Jan 13 16:27:04 2022 +0300

    Bluetooth: hci_serdev: call init_rwsem() before p->open()
    
    [ Upstream commit 9d7cbe2b9cf5f650067df4f402fdd799d4bbb4e1 ]
    
    kvartet reported, that hci_uart_tx_wakeup() uses uninitialized rwsem.
    The problem was in wrong place for percpu_init_rwsem() call.
    
    hci_uart_proto::open() may register a timer whose callback may call
    hci_uart_tx_wakeup(). There is a chance, that hci_uart_register_device()
    thread won't be fast enough to call percpu_init_rwsem().
    
    Fix it my moving percpu_init_rwsem() call before p->open().
    
    INFO: trying to register non-static key.
    The code is fine but needs lockdep annotation, or maybe
    you didn't initialize this object before use?
    turning off the locking correctness validator.
    CPU: 2 PID: 18524 Comm: syz-executor.5 Not tainted 5.16.0-rc6 #9
    ...
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     assign_lock_key kernel/locking/lockdep.c:951 [inline]
     register_lock_class+0x148d/0x1950 kernel/locking/lockdep.c:1263
     __lock_acquire+0x106/0x57e0 kernel/locking/lockdep.c:4906
     lock_acquire kernel/locking/lockdep.c:5637 [inline]
     lock_acquire+0x1ab/0x520 kernel/locking/lockdep.c:5602
     percpu_down_read_trylock include/linux/percpu-rwsem.h:92 [inline]
     hci_uart_tx_wakeup+0x12e/0x490 drivers/bluetooth/hci_ldisc.c:124
     h5_timed_event+0x32f/0x6a0 drivers/bluetooth/hci_h5.c:188
     call_timer_fn+0x1a5/0x6b0 kernel/time/timer.c:1421
    
    Fixes: d73e17281665 ("Bluetooth: hci_serdev: Init hci_uart proto_lock to avoid oops")
    Reported-by: Yiru Xu <xyru1999@gmail.com>
    Signed-off-by: Pavel Skripkin <paskripkin@gmail.com>
    Signed-off-by: Marcel Holtmann <marcel@holtmann.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 7af24ae489f7932f8f6a9786f755578555c110aa
Author: Pavel Skripkin <paskripkin@gmail.com>
Date:   Thu Jan 13 16:27:04 2022 +0300

    Bluetooth: hci_serdev: call init_rwsem() before p->open()
    
    [ Upstream commit 9d7cbe2b9cf5f650067df4f402fdd799d4bbb4e1 ]
    
    kvartet reported, that hci_uart_tx_wakeup() uses uninitialized rwsem.
    The problem was in wrong place for percpu_init_rwsem() call.
    
    hci_uart_proto::open() may register a timer whose callback may call
    hci_uart_tx_wakeup(). There is a chance, that hci_uart_register_device()
    thread won't be fast enough to call percpu_init_rwsem().
    
    Fix it my moving percpu_init_rwsem() call before p->open().
    
    INFO: trying to register non-static key.
    The code is fine but needs lockdep annotation, or maybe
    you didn't initialize this object before use?
    turning off the locking correctness validator.
    CPU: 2 PID: 18524 Comm: syz-executor.5 Not tainted 5.16.0-rc6 #9
    ...
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     assign_lock_key kernel/locking/lockdep.c:951 [inline]
     register_lock_class+0x148d/0x1950 kernel/locking/lockdep.c:1263
     __lock_acquire+0x106/0x57e0 kernel/locking/lockdep.c:4906
     lock_acquire kernel/locking/lockdep.c:5637 [inline]
     lock_acquire+0x1ab/0x520 kernel/locking/lockdep.c:5602
     percpu_down_read_trylock include/linux/percpu-rwsem.h:92 [inline]
     hci_uart_tx_wakeup+0x12e/0x490 drivers/bluetooth/hci_ldisc.c:124
     h5_timed_event+0x32f/0x6a0 drivers/bluetooth/hci_h5.c:188
     call_timer_fn+0x1a5/0x6b0 kernel/time/timer.c:1421
    
    Fixes: d73e17281665 ("Bluetooth: hci_serdev: Init hci_uart proto_lock to avoid oops")
    Reported-by: Yiru Xu <xyru1999@gmail.com>
    Signed-off-by: Pavel Skripkin <paskripkin@gmail.com>
    Signed-off-by: Marcel Holtmann <marcel@holtmann.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 96ea88eb9ba349c92be458e4e69031eb7c014091
Author: Pavel Skripkin <paskripkin@gmail.com>
Date:   Thu Jan 13 16:27:04 2022 +0300

    Bluetooth: hci_serdev: call init_rwsem() before p->open()
    
    [ Upstream commit 9d7cbe2b9cf5f650067df4f402fdd799d4bbb4e1 ]
    
    kvartet reported, that hci_uart_tx_wakeup() uses uninitialized rwsem.
    The problem was in wrong place for percpu_init_rwsem() call.
    
    hci_uart_proto::open() may register a timer whose callback may call
    hci_uart_tx_wakeup(). There is a chance, that hci_uart_register_device()
    thread won't be fast enough to call percpu_init_rwsem().
    
    Fix it my moving percpu_init_rwsem() call before p->open().
    
    INFO: trying to register non-static key.
    The code is fine but needs lockdep annotation, or maybe
    you didn't initialize this object before use?
    turning off the locking correctness validator.
    CPU: 2 PID: 18524 Comm: syz-executor.5 Not tainted 5.16.0-rc6 #9
    ...
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     assign_lock_key kernel/locking/lockdep.c:951 [inline]
     register_lock_class+0x148d/0x1950 kernel/locking/lockdep.c:1263
     __lock_acquire+0x106/0x57e0 kernel/locking/lockdep.c:4906
     lock_acquire kernel/locking/lockdep.c:5637 [inline]
     lock_acquire+0x1ab/0x520 kernel/locking/lockdep.c:5602
     percpu_down_read_trylock include/linux/percpu-rwsem.h:92 [inline]
     hci_uart_tx_wakeup+0x12e/0x490 drivers/bluetooth/hci_ldisc.c:124
     h5_timed_event+0x32f/0x6a0 drivers/bluetooth/hci_h5.c:188
     call_timer_fn+0x1a5/0x6b0 kernel/time/timer.c:1421
    
    Fixes: d73e17281665 ("Bluetooth: hci_serdev: Init hci_uart proto_lock to avoid oops")
    Reported-by: Yiru Xu <xyru1999@gmail.com>
    Signed-off-by: Pavel Skripkin <paskripkin@gmail.com>
    Signed-off-by: Marcel Holtmann <marcel@holtmann.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 041e5b8a62e63d1611bdd8936962e687ddfaf885
Author: Pavel Skripkin <paskripkin@gmail.com>
Date:   Thu Jan 13 16:27:04 2022 +0300

    Bluetooth: hci_serdev: call init_rwsem() before p->open()
    
    [ Upstream commit 9d7cbe2b9cf5f650067df4f402fdd799d4bbb4e1 ]
    
    kvartet reported, that hci_uart_tx_wakeup() uses uninitialized rwsem.
    The problem was in wrong place for percpu_init_rwsem() call.
    
    hci_uart_proto::open() may register a timer whose callback may call
    hci_uart_tx_wakeup(). There is a chance, that hci_uart_register_device()
    thread won't be fast enough to call percpu_init_rwsem().
    
    Fix it my moving percpu_init_rwsem() call before p->open().
    
    INFO: trying to register non-static key.
    The code is fine but needs lockdep annotation, or maybe
    you didn't initialize this object before use?
    turning off the locking correctness validator.
    CPU: 2 PID: 18524 Comm: syz-executor.5 Not tainted 5.16.0-rc6 #9
    ...
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     assign_lock_key kernel/locking/lockdep.c:951 [inline]
     register_lock_class+0x148d/0x1950 kernel/locking/lockdep.c:1263
     __lock_acquire+0x106/0x57e0 kernel/locking/lockdep.c:4906
     lock_acquire kernel/locking/lockdep.c:5637 [inline]
     lock_acquire+0x1ab/0x520 kernel/locking/lockdep.c:5602
     percpu_down_read_trylock include/linux/percpu-rwsem.h:92 [inline]
     hci_uart_tx_wakeup+0x12e/0x490 drivers/bluetooth/hci_ldisc.c:124
     h5_timed_event+0x32f/0x6a0 drivers/bluetooth/hci_h5.c:188
     call_timer_fn+0x1a5/0x6b0 kernel/time/timer.c:1421
    
    Fixes: d73e17281665 ("Bluetooth: hci_serdev: Init hci_uart proto_lock to avoid oops")
    Reported-by: Yiru Xu <xyru1999@gmail.com>
    Signed-off-by: Pavel Skripkin <paskripkin@gmail.com>
    Signed-off-by: Marcel Holtmann <marcel@holtmann.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 24608d8489cb3622f5ca968fb7996d193e4f9f81
Author: Pavel Skripkin <paskripkin@gmail.com>
Date:   Thu Jan 13 16:27:04 2022 +0300

    Bluetooth: hci_serdev: call init_rwsem() before p->open()
    
    [ Upstream commit 9d7cbe2b9cf5f650067df4f402fdd799d4bbb4e1 ]
    
    kvartet reported, that hci_uart_tx_wakeup() uses uninitialized rwsem.
    The problem was in wrong place for percpu_init_rwsem() call.
    
    hci_uart_proto::open() may register a timer whose callback may call
    hci_uart_tx_wakeup(). There is a chance, that hci_uart_register_device()
    thread won't be fast enough to call percpu_init_rwsem().
    
    Fix it my moving percpu_init_rwsem() call before p->open().
    
    INFO: trying to register non-static key.
    The code is fine but needs lockdep annotation, or maybe
    you didn't initialize this object before use?
    turning off the locking correctness validator.
    CPU: 2 PID: 18524 Comm: syz-executor.5 Not tainted 5.16.0-rc6 #9
    ...
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     assign_lock_key kernel/locking/lockdep.c:951 [inline]
     register_lock_class+0x148d/0x1950 kernel/locking/lockdep.c:1263
     __lock_acquire+0x106/0x57e0 kernel/locking/lockdep.c:4906
     lock_acquire kernel/locking/lockdep.c:5637 [inline]
     lock_acquire+0x1ab/0x520 kernel/locking/lockdep.c:5602
     percpu_down_read_trylock include/linux/percpu-rwsem.h:92 [inline]
     hci_uart_tx_wakeup+0x12e/0x490 drivers/bluetooth/hci_ldisc.c:124
     h5_timed_event+0x32f/0x6a0 drivers/bluetooth/hci_h5.c:188
     call_timer_fn+0x1a5/0x6b0 kernel/time/timer.c:1421
    
    Fixes: d73e17281665 ("Bluetooth: hci_serdev: Init hci_uart proto_lock to avoid oops")
    Reported-by: Yiru Xu <xyru1999@gmail.com>
    Signed-off-by: Pavel Skripkin <paskripkin@gmail.com>
    Signed-off-by: Marcel Holtmann <marcel@holtmann.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit c6049161291737242d5a008bd8ebe0409b175a43
Author: Pavel Skripkin <paskripkin@gmail.com>
Date:   Thu Jan 13 16:27:04 2022 +0300

    Bluetooth: hci_serdev: call init_rwsem() before p->open()
    
    [ Upstream commit 9d7cbe2b9cf5f650067df4f402fdd799d4bbb4e1 ]
    
    kvartet reported, that hci_uart_tx_wakeup() uses uninitialized rwsem.
    The problem was in wrong place for percpu_init_rwsem() call.
    
    hci_uart_proto::open() may register a timer whose callback may call
    hci_uart_tx_wakeup(). There is a chance, that hci_uart_register_device()
    thread won't be fast enough to call percpu_init_rwsem().
    
    Fix it my moving percpu_init_rwsem() call before p->open().
    
    INFO: trying to register non-static key.
    The code is fine but needs lockdep annotation, or maybe
    you didn't initialize this object before use?
    turning off the locking correctness validator.
    CPU: 2 PID: 18524 Comm: syz-executor.5 Not tainted 5.16.0-rc6 #9
    ...
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     assign_lock_key kernel/locking/lockdep.c:951 [inline]
     register_lock_class+0x148d/0x1950 kernel/locking/lockdep.c:1263
     __lock_acquire+0x106/0x57e0 kernel/locking/lockdep.c:4906
     lock_acquire kernel/locking/lockdep.c:5637 [inline]
     lock_acquire+0x1ab/0x520 kernel/locking/lockdep.c:5602
     percpu_down_read_trylock include/linux/percpu-rwsem.h:92 [inline]
     hci_uart_tx_wakeup+0x12e/0x490 drivers/bluetooth/hci_ldisc.c:124
     h5_timed_event+0x32f/0x6a0 drivers/bluetooth/hci_h5.c:188
     call_timer_fn+0x1a5/0x6b0 kernel/time/timer.c:1421
    
    Fixes: d73e17281665 ("Bluetooth: hci_serdev: Init hci_uart proto_lock to avoid oops")
    Reported-by: Yiru Xu <xyru1999@gmail.com>
    Signed-off-by: Pavel Skripkin <paskripkin@gmail.com>
    Signed-off-by: Marcel Holtmann <marcel@holtmann.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 436afdfa35dc8aaf43959593f6c433d0ad29abc3
Author: Philip Yang <Philip.Yang@amd.com>
Date:   Tue Mar 15 14:38:51 2022 -0400

    drm/amdgpu: Move reset domain init before calling RREG32
    
    amdgpu_detect_virtualization reads register, amdgpu_device_rreg access
    adev->reset_domain->sem if kernel defined CONFIG_LOCKDEP, below is the
    random boot hang backtrace on Vega10. It may get random NULL pointer
    access backtrace if amdgpu_sriov_runtime is true too.
    
    Move amdgpu_reset_create_reset_domain before calling to RREG32.
    
     BUG: kernel NULL pointer dereference, address:
     #PF: supervisor read access in kernel mode
     #PF: error_code(0x0000) - not-present page
     PGD 0 P4D 0
     Oops: 0000 [#1] PREEMPT SMP NOPTI
     Workqueue: events work_for_cpu_fn
     RIP: 0010:down_read_trylock+0x13/0xf0
     Call Trace:
      <TASK>
      amdgpu_device_skip_hw_access+0x38/0x80 [amdgpu]
      amdgpu_device_rreg+0x1b/0x170 [amdgpu]
      amdgpu_detect_virtualization+0x73/0x100 [amdgpu]
      amdgpu_device_init.cold.60+0xbe/0x16b1 [amdgpu]
      ? pci_bus_read_config_word+0x43/0x70
      amdgpu_driver_load_kms+0x15/0x120 [amdgpu]
      amdgpu_pci_probe+0x1a1/0x3a0 [amdgpu]
    
    Fixes: d0fb18b535679a ("drm/amdgpu: Move reset sem into reset_domain")
    Signed-off-by: Philip Yang <Philip.Yang@amd.com>
    Reviewed-by: Andrey Grodzovsky <andrey.grodzovsky@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

commit 9d7cbe2b9cf5f650067df4f402fdd799d4bbb4e1
Author: Pavel Skripkin <paskripkin@gmail.com>
Date:   Thu Jan 13 16:27:04 2022 +0300

    Bluetooth: hci_serdev: call init_rwsem() before p->open()
    
    kvartet reported, that hci_uart_tx_wakeup() uses uninitialized rwsem.
    The problem was in wrong place for percpu_init_rwsem() call.
    
    hci_uart_proto::open() may register a timer whose callback may call
    hci_uart_tx_wakeup(). There is a chance, that hci_uart_register_device()
    thread won't be fast enough to call percpu_init_rwsem().
    
    Fix it my moving percpu_init_rwsem() call before p->open().
    
    INFO: trying to register non-static key.
    The code is fine but needs lockdep annotation, or maybe
    you didn't initialize this object before use?
    turning off the locking correctness validator.
    CPU: 2 PID: 18524 Comm: syz-executor.5 Not tainted 5.16.0-rc6 #9
    ...
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     assign_lock_key kernel/locking/lockdep.c:951 [inline]
     register_lock_class+0x148d/0x1950 kernel/locking/lockdep.c:1263
     __lock_acquire+0x106/0x57e0 kernel/locking/lockdep.c:4906
     lock_acquire kernel/locking/lockdep.c:5637 [inline]
     lock_acquire+0x1ab/0x520 kernel/locking/lockdep.c:5602
     percpu_down_read_trylock include/linux/percpu-rwsem.h:92 [inline]
     hci_uart_tx_wakeup+0x12e/0x490 drivers/bluetooth/hci_ldisc.c:124
     h5_timed_event+0x32f/0x6a0 drivers/bluetooth/hci_h5.c:188
     call_timer_fn+0x1a5/0x6b0 kernel/time/timer.c:1421
    
    Fixes: d73e17281665 ("Bluetooth: hci_serdev: Init hci_uart proto_lock to avoid oops")
    Reported-by: Yiru Xu <xyru1999@gmail.com>
    Signed-off-by: Pavel Skripkin <paskripkin@gmail.com>
    Signed-off-by: Marcel Holtmann <marcel@holtmann.org>

commit d039f38801245ed99c0351b2259550170d7fe17b
Merge: f8132d62a2de 14c240488411
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 28 09:04:41 2021 -0800

    Merge tag 'locking-urgent-2021-11-28' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fixes from Thomas Gleixner:
     "Two regression fixes for reader writer semaphores:
    
       - Plug a race in the lock handoff which is caused by inconsistency of
         the reader and writer path and can lead to corruption of the
         underlying counter.
    
       - down_read_trylock() is suboptimal when the lock is contended and
         multiple readers trylock concurrently. That's due to the initial
         value being read non-atomically which results in at least two
         compare exchange loops. Making the initial readout atomic reduces
         this significantly. Whith 40 readers by 11% in a benchmark which
         enforces contention on mmap_sem"
    
    * tag 'locking-urgent-2021-11-28' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/rwsem: Optimize down_read_trylock() under highly contended case
      locking/rwsem: Make handoff bit handling more consistent

commit 14c24048841151548a3f4d9e218510c844c1b737
Author: Muchun Song <songmuchun@bytedance.com>
Date:   Thu Nov 18 17:44:55 2021 +0800

    locking/rwsem: Optimize down_read_trylock() under highly contended case
    
    We found that a process with 10 thousnads threads has been encountered
    a regression problem from Linux-v4.14 to Linux-v5.4. It is a kind of
    workload which will concurrently allocate lots of memory in different
    threads sometimes. In this case, we will see the down_read_trylock()
    with a high hotspot. Therefore, we suppose that rwsem has a regression
    at least since Linux-v5.4. In order to easily debug this problem, we
    write a simply benchmark to create the similar situation lile the
    following.
    
      ```c++
      #include <sys/mman.h>
      #include <sys/time.h>
      #include <sys/resource.h>
      #include <sched.h>
    
      #include <cstdio>
      #include <cassert>
      #include <thread>
      #include <vector>
      #include <chrono>
    
      volatile int mutex;
    
      void trigger(int cpu, char* ptr, std::size_t sz)
      {
            cpu_set_t set;
            CPU_ZERO(&set);
            CPU_SET(cpu, &set);
            assert(pthread_setaffinity_np(pthread_self(), sizeof(set), &set) == 0);
    
            while (mutex);
    
            for (std::size_t i = 0; i < sz; i += 4096) {
                    *ptr = '\0';
                    ptr += 4096;
            }
      }
    
      int main(int argc, char* argv[])
      {
            std::size_t sz = 100;
    
            if (argc > 1)
                    sz = atoi(argv[1]);
    
            auto nproc = std::thread::hardware_concurrency();
            std::vector<std::thread> thr;
            sz <<= 30;
            auto* ptr = mmap(nullptr, sz, PROT_READ | PROT_WRITE, MAP_ANON |
                             MAP_PRIVATE, -1, 0);
            assert(ptr != MAP_FAILED);
            char* cptr = static_cast<char*>(ptr);
            auto run = sz / nproc;
            run = (run >> 12) << 12;
    
            mutex = 1;
    
            for (auto i = 0U; i < nproc; ++i) {
                    thr.emplace_back(std::thread([i, cptr, run]() { trigger(i, cptr, run); }));
                    cptr += run;
            }
    
            rusage usage_start;
            getrusage(RUSAGE_SELF, &usage_start);
            auto start = std::chrono::system_clock::now();
    
            mutex = 0;
    
            for (auto& t : thr)
                    t.join();
    
            rusage usage_end;
            getrusage(RUSAGE_SELF, &usage_end);
            auto end = std::chrono::system_clock::now();
            timeval utime;
            timeval stime;
            timersub(&usage_end.ru_utime, &usage_start.ru_utime, &utime);
            timersub(&usage_end.ru_stime, &usage_start.ru_stime, &stime);
            printf("usr: %ld.%06ld\n", utime.tv_sec, utime.tv_usec);
            printf("sys: %ld.%06ld\n", stime.tv_sec, stime.tv_usec);
            printf("real: %lu\n",
                   std::chrono::duration_cast<std::chrono::milliseconds>(end -
                   start).count());
    
            return 0;
      }
      ```
    
    The functionality of above program is simply which creates `nproc`
    threads and each of them are trying to touch memory (trigger page
    fault) on different CPU. Then we will see the similar profile by
    `perf top`.
    
      25.55%  [kernel]                  [k] down_read_trylock
      14.78%  [kernel]                  [k] handle_mm_fault
      13.45%  [kernel]                  [k] up_read
       8.61%  [kernel]                  [k] clear_page_erms
       3.89%  [kernel]                  [k] __do_page_fault
    
    The highest hot instruction, which accounts for about 92%, in
    down_read_trylock() is cmpxchg like the following.
    
      91.89 │      lock   cmpxchg %rdx,(%rdi)
    
    Sice the problem is found by migrating from Linux-v4.14 to Linux-v5.4,
    so we easily found that the commit ddb20d1d3aed ("locking/rwsem: Optimize
    down_read_trylock()") caused the regression. The reason is that the
    commit assumes the rwsem is not contended at all. But it is not always
    true for mmap lock which could be contended with thousands threads.
    So most threads almost need to run at least 2 times of "cmpxchg" to
    acquire the lock. The overhead of atomic operation is higher than
    non-atomic instructions, which caused the regression.
    
    By using the above benchmark, the real executing time on a x86-64 system
    before and after the patch were:
    
                      Before Patch  After Patch
       # of Threads      real          real     reduced by
       ------------     ------        ------    ----------
             1          65,373        65,206       ~0.0%
             4          15,467        15,378       ~0.5%
            40           6,214         5,528      ~11.0%
    
    For the uncontended case, the new down_read_trylock() is the same as
    before. For the contended cases, the new down_read_trylock() is faster
    than before. The more contended, the more fast.
    
    Signed-off-by: Muchun Song <songmuchun@bytedance.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Link: https://lore.kernel.org/r/20211118094455.9068-1-songmuchun@bytedance.com

commit c78416d122243c92992a1d1063f17ddd0bc80e6c
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Sun Sep 19 22:20:30 2021 -0700

    locking/rwbase: Optimize rwbase_read_trylock
    
    Instead of a full barrier around the Rmw insn, micro-optimize
    for weakly ordered archs such that we only provide the required
    ACQUIRE semantics when taking the read lock.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Link: https://lkml.kernel.org/r/20210920052031.54220-2-dave@stgolabs.net

commit 2f1aaf3ea666b737ad717b3d88667225aca23149
Author: Yonghong Song <yhs@fb.com>
Date:   Thu Sep 9 08:49:59 2021 -0700

    bpf, mm: Fix lockdep warning triggered by stack_map_get_build_id_offset()
    
    Currently the bpf selftest "get_stack_raw_tp" triggered the warning:
    
      [ 1411.304463] WARNING: CPU: 3 PID: 140 at include/linux/mmap_lock.h:164 find_vma+0x47/0xa0
      [ 1411.304469] Modules linked in: bpf_testmod(O) [last unloaded: bpf_testmod]
      [ 1411.304476] CPU: 3 PID: 140 Comm: systemd-journal Tainted: G        W  O      5.14.0+ #53
      [ 1411.304479] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.14.0-0-g155821a1990b-prebuilt.qemu.org 04/01/2014
      [ 1411.304481] RIP: 0010:find_vma+0x47/0xa0
      [ 1411.304484] Code: de 48 89 ef e8 ba f5 fe ff 48 85 c0 74 2e 48 83 c4 08 5b 5d c3 48 8d bf 28 01 00 00 be ff ff ff ff e8 2d 9f d8 00 85 c0 75 d4 <0f> 0b 48 89 de 48 8
      [ 1411.304487] RSP: 0018:ffffabd440403db8 EFLAGS: 00010246
      [ 1411.304490] RAX: 0000000000000000 RBX: 00007f00ad80a0e0 RCX: 0000000000000000
      [ 1411.304492] RDX: 0000000000000001 RSI: ffffffff9776b144 RDI: ffffffff977e1b0e
      [ 1411.304494] RBP: ffff9cf5c2f50000 R08: ffff9cf5c3eb25d8 R09: 00000000fffffffe
      [ 1411.304496] R10: 0000000000000001 R11: 00000000ef974e19 R12: ffff9cf5c39ae0e0
      [ 1411.304498] R13: 0000000000000000 R14: 0000000000000000 R15: ffff9cf5c39ae0e0
      [ 1411.304501] FS:  00007f00ae754780(0000) GS:ffff9cf5fba00000(0000) knlGS:0000000000000000
      [ 1411.304504] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      [ 1411.304506] CR2: 000000003e34343c CR3: 0000000103a98005 CR4: 0000000000370ee0
      [ 1411.304508] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      [ 1411.304510] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      [ 1411.304512] Call Trace:
      [ 1411.304517]  stack_map_get_build_id_offset+0x17c/0x260
      [ 1411.304528]  __bpf_get_stack+0x18f/0x230
      [ 1411.304541]  bpf_get_stack_raw_tp+0x5a/0x70
      [ 1411.305752] RAX: 0000000000000000 RBX: 5541f689495641d7 RCX: 0000000000000000
      [ 1411.305756] RDX: 0000000000000001 RSI: ffffffff9776b144 RDI: ffffffff977e1b0e
      [ 1411.305758] RBP: ffff9cf5c02b2f40 R08: ffff9cf5ca7606c0 R09: ffffcbd43ee02c04
      [ 1411.306978]  bpf_prog_32007c34f7726d29_bpf_prog1+0xaf/0xd9c
      [ 1411.307861] R10: 0000000000000001 R11: 0000000000000044 R12: ffff9cf5c2ef60e0
      [ 1411.307865] R13: 0000000000000005 R14: 0000000000000000 R15: ffff9cf5c2ef6108
      [ 1411.309074]  bpf_trace_run2+0x8f/0x1a0
      [ 1411.309891] FS:  00007ff485141700(0000) GS:ffff9cf5fae00000(0000) knlGS:0000000000000000
      [ 1411.309896] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      [ 1411.311221]  syscall_trace_enter.isra.20+0x161/0x1f0
      [ 1411.311600] CR2: 00007ff48514d90e CR3: 0000000107114001 CR4: 0000000000370ef0
      [ 1411.312291]  do_syscall_64+0x15/0x80
      [ 1411.312941] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      [ 1411.313803]  entry_SYSCALL_64_after_hwframe+0x44/0xae
      [ 1411.314223] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      [ 1411.315082] RIP: 0033:0x7f00ad80a0e0
      [ 1411.315626] Call Trace:
      [ 1411.315632]  stack_map_get_build_id_offset+0x17c/0x260
    
    To reproduce, first build `test_progs` binary:
    
      make -C tools/testing/selftests/bpf -j60
    
    and then run the binary at tools/testing/selftests/bpf directory:
    
      ./test_progs -t get_stack_raw_tp
    
    The warning is due to commit 5b78ed24e8ec ("mm/pagemap: add mmap_assert_locked()
    annotations to find_vma*()") which added mmap_assert_locked() in find_vma()
    function. The mmap_assert_locked() function asserts that mm->mmap_lock needs
    to be held. But this is not the case for bpf_get_stack() or bpf_get_stackid()
    helper (kernel/bpf/stackmap.c), which uses mmap_read_trylock_non_owner()
    instead. Since mm->mmap_lock is not held in bpf_get_stack[id]() use case,
    the above warning is emitted during test run.
    
    This patch fixed the issue by (1). using mmap_read_trylock() instead of
    mmap_read_trylock_non_owner() to satisfy lockdep checking in find_vma(), and
    (2). droping lockdep for mmap_lock right before the irq_work_queue(). The
    function mmap_read_trylock_non_owner() is also removed since after this
    patch nobody calls it any more.
    
    Fixes: 5b78ed24e8ec ("mm/pagemap: add mmap_assert_locked() annotations to find_vma*()")
    Suggested-by: Jason Gunthorpe <jgg@ziepe.ca>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Liam R. Howlett <Liam.Howlett@oracle.com>
    Cc: Luigi Rizzo <lrizzo@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: linux-mm@kvack.org
    Link: https://lore.kernel.org/bpf/20210909155000.1610299-1-yhs@fb.com

commit abd4737f67d75563d1d0cc57bd5daab026e8c2d1
Author: Miaohe Lin <linmiaohe@huawei.com>
Date:   Fri Feb 5 04:09:19 2021 -0500

    mm/arm64: Correct obsolete comment in do_page_fault()
    
    commit d8ed45c5dcd4 ("mmap locking API: use coccinelle to convert mmap_sem
    rwsem call sites") has convertd down_read_trylock() to mmap_read_trylock().
    But it forgot to update the relevant comment.
    
    Signed-off-by: Miaohe Lin <linmiaohe@huawei.com>
    Link: https://lore.kernel.org/r/20210205090919.63382-1-linmiaohe@huawei.com
    Signed-off-by: Will Deacon <will@kernel.org>

commit b444b86e37b296a8ed18479d5b17df9b84cd26a8
Author: Juergen Gross <jgross@suse.com>
Date:   Mon Jan 25 14:42:07 2021 +0100

    x86/xen: avoid warning in Xen pv guest with CONFIG_AMD_MEM_ENCRYPT enabled
    
    commit 2e92493637a09547734f92c62a2471f6f0cb9a2c upstream.
    
    When booting a kernel which has been built with CONFIG_AMD_MEM_ENCRYPT
    enabled as a Xen pv guest a warning is issued for each processor:
    
    [    5.964347] ------------[ cut here ]------------
    [    5.968314] WARNING: CPU: 0 PID: 1 at /home/gross/linux/head/arch/x86/xen/enlighten_pv.c:660 get_trap_addr+0x59/0x90
    [    5.972321] Modules linked in:
    [    5.976313] CPU: 0 PID: 1 Comm: swapper/0 Tainted: G        W         5.11.0-rc5-default #75
    [    5.980313] Hardware name: Dell Inc. OptiPlex 9020/0PC5F7, BIOS A05 12/05/2013
    [    5.984313] RIP: e030:get_trap_addr+0x59/0x90
    [    5.988313] Code: 42 10 83 f0 01 85 f6 74 04 84 c0 75 1d b8 01 00 00 00 c3 48 3d 00 80 83 82 72 08 48 3d 20 81 83 82 72 0c b8 01 00 00 00 eb db <0f> 0b 31 c0 c3 48 2d 00 80 83 82 48 ba 72 1c c7 71 1c c7 71 1c 48
    [    5.992313] RSP: e02b:ffffc90040033d38 EFLAGS: 00010202
    [    5.996313] RAX: 0000000000000001 RBX: ffffffff82a141d0 RCX: ffffffff8222ec38
    [    6.000312] RDX: ffffffff8222ec38 RSI: 0000000000000005 RDI: ffffc90040033d40
    [    6.004313] RBP: ffff8881003984a0 R08: 0000000000000007 R09: ffff888100398000
    [    6.008312] R10: 0000000000000007 R11: ffffc90040246000 R12: ffff8884082182a8
    [    6.012313] R13: 0000000000000100 R14: 000000000000001d R15: ffff8881003982d0
    [    6.016316] FS:  0000000000000000(0000) GS:ffff888408200000(0000) knlGS:0000000000000000
    [    6.020313] CS:  e030 DS: 0000 ES: 0000 CR0: 0000000080050033
    [    6.024313] CR2: ffffc900020ef000 CR3: 000000000220a000 CR4: 0000000000050660
    [    6.028314] Call Trace:
    [    6.032313]  cvt_gate_to_trap.part.7+0x3f/0x90
    [    6.036313]  ? asm_exc_double_fault+0x30/0x30
    [    6.040313]  xen_convert_trap_info+0x87/0xd0
    [    6.044313]  xen_pv_cpu_up+0x17a/0x450
    [    6.048313]  bringup_cpu+0x2b/0xc0
    [    6.052313]  ? cpus_read_trylock+0x50/0x50
    [    6.056313]  cpuhp_invoke_callback+0x80/0x4c0
    [    6.060313]  _cpu_up+0xa7/0x140
    [    6.064313]  cpu_up+0x98/0xd0
    [    6.068313]  bringup_nonboot_cpus+0x4f/0x60
    [    6.072313]  smp_init+0x26/0x79
    [    6.076313]  kernel_init_freeable+0x103/0x258
    [    6.080313]  ? rest_init+0xd0/0xd0
    [    6.084313]  kernel_init+0xa/0x110
    [    6.088313]  ret_from_fork+0x1f/0x30
    [    6.092313] ---[ end trace be9ecf17dceeb4f3 ]---
    
    Reason is that there is no Xen pv trap entry for X86_TRAP_VC.
    
    Fix that by adding a generic trap handler for unknown traps and wire all
    unknown bare metal handlers to this generic handler, which will just
    crash the system in case such a trap will ever happen.
    
    Fixes: 0786138c78e793 ("x86/sev-es: Add a Runtime #VC Exception Handler")
    Cc: <stable@vger.kernel.org> # v5.10
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Andrew Cooper <andrew.cooper3@citrix.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2e92493637a09547734f92c62a2471f6f0cb9a2c
Author: Juergen Gross <jgross@suse.com>
Date:   Mon Jan 25 14:42:07 2021 +0100

    x86/xen: avoid warning in Xen pv guest with CONFIG_AMD_MEM_ENCRYPT enabled
    
    When booting a kernel which has been built with CONFIG_AMD_MEM_ENCRYPT
    enabled as a Xen pv guest a warning is issued for each processor:
    
    [    5.964347] ------------[ cut here ]------------
    [    5.968314] WARNING: CPU: 0 PID: 1 at /home/gross/linux/head/arch/x86/xen/enlighten_pv.c:660 get_trap_addr+0x59/0x90
    [    5.972321] Modules linked in:
    [    5.976313] CPU: 0 PID: 1 Comm: swapper/0 Tainted: G        W         5.11.0-rc5-default #75
    [    5.980313] Hardware name: Dell Inc. OptiPlex 9020/0PC5F7, BIOS A05 12/05/2013
    [    5.984313] RIP: e030:get_trap_addr+0x59/0x90
    [    5.988313] Code: 42 10 83 f0 01 85 f6 74 04 84 c0 75 1d b8 01 00 00 00 c3 48 3d 00 80 83 82 72 08 48 3d 20 81 83 82 72 0c b8 01 00 00 00 eb db <0f> 0b 31 c0 c3 48 2d 00 80 83 82 48 ba 72 1c c7 71 1c c7 71 1c 48
    [    5.992313] RSP: e02b:ffffc90040033d38 EFLAGS: 00010202
    [    5.996313] RAX: 0000000000000001 RBX: ffffffff82a141d0 RCX: ffffffff8222ec38
    [    6.000312] RDX: ffffffff8222ec38 RSI: 0000000000000005 RDI: ffffc90040033d40
    [    6.004313] RBP: ffff8881003984a0 R08: 0000000000000007 R09: ffff888100398000
    [    6.008312] R10: 0000000000000007 R11: ffffc90040246000 R12: ffff8884082182a8
    [    6.012313] R13: 0000000000000100 R14: 000000000000001d R15: ffff8881003982d0
    [    6.016316] FS:  0000000000000000(0000) GS:ffff888408200000(0000) knlGS:0000000000000000
    [    6.020313] CS:  e030 DS: 0000 ES: 0000 CR0: 0000000080050033
    [    6.024313] CR2: ffffc900020ef000 CR3: 000000000220a000 CR4: 0000000000050660
    [    6.028314] Call Trace:
    [    6.032313]  cvt_gate_to_trap.part.7+0x3f/0x90
    [    6.036313]  ? asm_exc_double_fault+0x30/0x30
    [    6.040313]  xen_convert_trap_info+0x87/0xd0
    [    6.044313]  xen_pv_cpu_up+0x17a/0x450
    [    6.048313]  bringup_cpu+0x2b/0xc0
    [    6.052313]  ? cpus_read_trylock+0x50/0x50
    [    6.056313]  cpuhp_invoke_callback+0x80/0x4c0
    [    6.060313]  _cpu_up+0xa7/0x140
    [    6.064313]  cpu_up+0x98/0xd0
    [    6.068313]  bringup_nonboot_cpus+0x4f/0x60
    [    6.072313]  smp_init+0x26/0x79
    [    6.076313]  kernel_init_freeable+0x103/0x258
    [    6.080313]  ? rest_init+0xd0/0xd0
    [    6.084313]  kernel_init+0xa/0x110
    [    6.088313]  ret_from_fork+0x1f/0x30
    [    6.092313] ---[ end trace be9ecf17dceeb4f3 ]---
    
    Reason is that there is no Xen pv trap entry for X86_TRAP_VC.
    
    Fix that by adding a generic trap handler for unknown traps and wire all
    unknown bare metal handlers to this generic handler, which will just
    crash the system in case such a trap will ever happen.
    
    Fixes: 0786138c78e793 ("x86/sev-es: Add a Runtime #VC Exception Handler")
    Cc: <stable@vger.kernel.org> # v5.10
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Andrew Cooper <andrew.cooper3@citrix.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

commit e857b6fcc5af0fbe042bec7e56a1533fe78ef594
Merge: 8c1dccc80380 cb262935a166
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 14 17:27:47 2020 -0800

    Merge tag 'locking-core-2020-12-14' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Thomas Gleixner:
     "A moderate set of locking updates:
    
       - A few extensions to the rwsem API and support for opportunistic
         spinning and lock stealing
    
       - lockdep selftest improvements
    
       - Documentation updates
    
       - Cleanups and small fixes all over the place"
    
    * tag 'locking-core-2020-12-14' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (21 commits)
      seqlock: kernel-doc: Specify when preemption is automatically altered
      seqlock: Prefix internal seqcount_t-only macros with a "do_"
      Documentation: seqlock: s/LOCKTYPE/LOCKNAME/g
      locking/rwsem: Remove reader optimistic spinning
      locking/rwsem: Enable reader optimistic lock stealing
      locking/rwsem: Prevent potential lock starvation
      locking/rwsem: Pass the current atomic count to rwsem_down_read_slowpath()
      locking/rwsem: Fold __down_{read,write}*()
      locking/rwsem: Introduce rwsem_write_trylock()
      locking/rwsem: Better collate rwsem_read_trylock()
      rwsem: Implement down_read_interruptible
      rwsem: Implement down_read_killable_nested
      refcount: Fix a kernel-doc markup
      completion: Drop init_completion define
      atomic: Update MAINTAINERS
      atomic: Delete obsolete documentation
      seqlock: Rename __seqprop() users
      lockdep/selftest: Add spin_nest_lock test
      lockdep/selftests: Fix PROVE_RAW_LOCK_NESTING
      seqlock: avoid -Wshadow warnings
      ...

commit 3379116a0ca965b00e6522c7ea3f16c9dbd8f9f9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Dec 8 10:22:16 2020 +0100

    locking/rwsem: Better collate rwsem_read_trylock()
    
    All users of rwsem_read_trylock() do rwsem_set_reader_owned(sem) on
    success, move it into rwsem_read_trylock() proper.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20201207090243.GE3040@hirez.programming.kicks-ass.net

commit 0cc55a0213a02b760ade1d4755fdccfbf7d3157e
Author: Michel Lespinasse <michel@lespinasse.org>
Date:   Mon Jun 8 21:33:37 2020 -0700

    mmap locking API: add mmap_read_trylock_non_owner()
    
    Add a couple APIs used by kernel/bpf/stackmap.c only:
    - mmap_read_trylock_non_owner()
    - mmap_read_unlock_non_owner() (may be called from a work queue).
    
    It's still not ideal that bpf/stackmap subverts the lock ownership in this
    way.  Thanks to Peter Zijlstra for suggesting this API as the least-ugly
    way of addressing this in the short term.
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Laurent Dufour <ldufour@linux.ibm.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-8-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <michel@lespinasse.org>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 099bfaa731ec347d3f16a463ae53b88a1700c0af
Author: David Miller <davem@davemloft.net>
Date:   Mon Feb 24 15:01:53 2020 +0100

    bpf/stackmap: Dont trylock mmap_sem with PREEMPT_RT and interrupts disabled
    
    In a RT kernel down_read_trylock() cannot be used from NMI context and
    up_read_non_owner() is another problematic issue.
    
    So in such a configuration, simply elide the annotated stackmap and
    just report the raw IPs.
    
    In the longer term, it might be possible to provide a atomic friendly
    versions of the page cache traversal which will at least provide the info
    if the pages are resident and don't need to be paged in.
    
    [ tglx: Use IS_ENABLED() to avoid the #ifdeffery, fixup the irq work
            callback and add a comment ]
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200224145644.708960317@linutronix.de

commit 75ff64572e497578e238fefbdff221c96f29067a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 31 12:34:23 2019 +0100

    locking/percpu-rwsem: Extract __percpu_down_read_trylock()
    
    In preparation for removing the embedded rwsem and building a custom
    lock, extract the read-trylock primitive.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Davidlohr Bueso <dbueso@suse.de>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Tested-by: Juri Lelli <juri.lelli@redhat.com>
    Link: https://lkml.kernel.org/r/20200131151540.098485539@infradead.org

commit a0509313d5dea0a27a5968f04bd556d05e6349fd
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Mon Jun 3 12:11:22 2019 +0530

    arm64/mm: Drop mmap_sem before calling __do_kernel_fault()
    
    There is an inconsistency between down_read_trylock() success and failure
    paths while dealing with kernel access for non exception table areas where
    it calls __do_kernel_fault(). In case of failure it just bails out without
    holding mmap_sem but when it succeeds it does so while holding mmap_sem.
    Fix this inconsistency by just dropping mmap_sem in success path as well.
    
    __do_kernel_fault() calls die_kernel_fault() which then calls show_pte().
    show_pte() in this path might become bit more unreliable without holding
    mmap_sem. But there are already instances [1] in do_page_fault() where
    die_kernel_fault() gets called without holding mmap_sem. show_pte() can
    be made more robust independently but in a later patch.
    
    [1] Conditional block for (is_ttbr0_addr && is_el1_permission_fault)
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

commit f6e0937e8f6cb40b548c1a62698c4267cea3c511
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Nov 5 18:49:33 2015 -0800

    mm: rmap use pte lock not mmap_sem to set PageMlocked
    
    commit b87537d9e2feb30f6a962f27eb32768682698d3b upstream.
    
    KernelThreadSanitizer (ktsan) has shown that the down_read_trylock() of
    mmap_sem in try_to_unmap_one() (when going to set PageMlocked on a page
    found mapped in a VM_LOCKED vma) is ineffective against races with
    exit_mmap()'s munlock_vma_pages_all(), because mmap_sem is not held when
    tearing down an mm.
    
    But that's okay, those races are benign; and although we've believed for
    years in that ugly down_read_trylock(), it's unsuitable for the job, and
    frustrates the good intention of setting PageMlocked when it fails.
    
    It just doesn't matter if here we read vm_flags an instant before or after
    a racing mlock() or munlock() or exit_mmap() sets or clears VM_LOCKED: the
    syscalls (or exit) work their way up the address space (taking pt locks
    after updating vm_flags) to establish the final state.
    
    We do still need to be careful never to mark a page Mlocked (hence
    unevictable) by any race that will not be corrected shortly after.  The
    page lock protects from many of the races, but not all (a page is not
    necessarily locked when it's unmapped).  But the pte lock we just dropped
    is good to cover the rest (and serializes even with
    munlock_vma_pages_all(), so no special barriers required): now hold on to
    the pte lock while calling mlock_vma_page().  Is that lock ordering safe?
    Yes, that's how follow_page_pte() calls it, and how page_remove_rmap()
    calls the complementary clear_page_mlock().
    
    This fixes the following case (though not a case which anyone has
    complained of), which mmap_sem did not: truncation's preliminary
    unmap_mapping_range() is supposed to remove even the anonymous COWs of
    filecache pages, and that might race with try_to_unmap_one() on a
    VM_LOCKED vma, so that mlock_vma_page() sets PageMlocked just after
    zap_pte_range() unmaps the page, causing "Bad page state (mlocked)" when
    freed.  The pte lock protects against this.
    
    You could say that it also protects against the more ordinary case, racing
    with the preliminary unmapping of a filecache page itself: but in our
    current tree, that's independently protected by i_mmap_rwsem; and that
    race would be why "Bad page state (mlocked)" was seen before commit
    48ec833b7851 ("Revert mm/memory.c: share the i_mmap_rwsem").
    
    Vlastimil Babka points out another race which this patch protects against.
     try_to_unmap_one() might reach its mlock_vma_page() TestSetPageMlocked a
    moment after munlock_vma_pages_all() did its Phase 1 TestClearPageMlocked:
    leaving PageMlocked and unevictable when it should be evictable.  mmap_sem
    is ineffective because exit_mmap() does not hold it; page lock ineffective
    because __munlock_pagevec() only takes it afterwards, in Phase 2; pte lock
    is effective because __munlock_pagevec_fill() takes it to get the page,
    after VM_LOCKED was cleared from vm_flags, so visible to try_to_unmap_one.
    
    Kirill Shutemov points out that if the compiler chooses to implement a
    "vma->vm_flags &= VM_WHATEVER" or "vma->vm_flags |= VM_WHATEVER" operation
    with an intermediate store of unrelated bits set, since I'm here foregoing
    its usual protection by mmap_sem, try_to_unmap_one() might catch sight of
    a spurious VM_LOCKED in vm_flags, and make the wrong decision.  This does
    not appear to be an immediate problem, but we may want to define vm_flags
    accessors in future, to guard against such a possibility.
    
    While we're here, make a related optimization in try_to_munmap_one(): if
    it's doing TTU_MUNLOCK, then there's no point at all in descending the
    page tables and getting the pt lock, unless the vma is VM_LOCKED.  Yes,
    that can change racily, but it can change racily even without the
    optimization: it's not critical.  Far better not to waste time here.
    
    Stopped short of separating try_to_munlock_one() from try_to_munmap_one()
    on this occasion, but that's probably the sensible next step - with a
    rename, given that try_to_munlock()'s business is to try to set Mlocked.
    
    Updated the unevictable-lru Documentation, to remove its reference to mmap
    semaphore, but found a few more updates needed in just that area.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Rik van Riel <riel@redhat.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    [bwh: Backported to 3.16 in preparation for commit 017b1660df89
     "mm: migration: fix migration of huge PMD shared pages". Adjusted context.]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit ddb20d1d3aed8f130519c0a29cd5392efcc067b8
Author: Waiman Long <longman@redhat.com>
Date:   Fri Mar 22 10:30:08 2019 -0400

    locking/rwsem: Optimize down_read_trylock()
    
    Modify __down_read_trylock() to optimize for an unlocked rwsem and make
    it generate slightly better code.
    
    Before this patch, down_read_trylock:
    
       0x0000000000000000 <+0>:     callq  0x5 <down_read_trylock+5>
       0x0000000000000005 <+5>:     jmp    0x18 <down_read_trylock+24>
       0x0000000000000007 <+7>:     lea    0x1(%rdx),%rcx
       0x000000000000000b <+11>:    mov    %rdx,%rax
       0x000000000000000e <+14>:    lock cmpxchg %rcx,(%rdi)
       0x0000000000000013 <+19>:    cmp    %rax,%rdx
       0x0000000000000016 <+22>:    je     0x23 <down_read_trylock+35>
       0x0000000000000018 <+24>:    mov    (%rdi),%rdx
       0x000000000000001b <+27>:    test   %rdx,%rdx
       0x000000000000001e <+30>:    jns    0x7 <down_read_trylock+7>
       0x0000000000000020 <+32>:    xor    %eax,%eax
       0x0000000000000022 <+34>:    retq
       0x0000000000000023 <+35>:    mov    %gs:0x0,%rax
       0x000000000000002c <+44>:    or     $0x3,%rax
       0x0000000000000030 <+48>:    mov    %rax,0x20(%rdi)
       0x0000000000000034 <+52>:    mov    $0x1,%eax
       0x0000000000000039 <+57>:    retq
    
    After patch, down_read_trylock:
    
       0x0000000000000000 <+0>:     callq  0x5 <down_read_trylock+5>
       0x0000000000000005 <+5>:     xor    %eax,%eax
       0x0000000000000007 <+7>:     lea    0x1(%rax),%rdx
       0x000000000000000b <+11>:    lock cmpxchg %rdx,(%rdi)
       0x0000000000000010 <+16>:    jne    0x29 <down_read_trylock+41>
       0x0000000000000012 <+18>:    mov    %gs:0x0,%rax
       0x000000000000001b <+27>:    or     $0x3,%rax
       0x000000000000001f <+31>:    mov    %rax,0x20(%rdi)
       0x0000000000000023 <+35>:    mov    $0x1,%eax
       0x0000000000000028 <+40>:    retq
       0x0000000000000029 <+41>:    test   %rax,%rax
       0x000000000000002c <+44>:    jns    0x7 <down_read_trylock+7>
       0x000000000000002e <+46>:    xor    %eax,%eax
       0x0000000000000030 <+48>:    retq
    
    By using a rwsem microbenchmark, the down_read_trylock() rate (with a
    load of 10 to lengthen the lock critical section) on a x86-64 system
    before and after the patch were:
    
                     Before Patch    After Patch
       # of Threads     rlock           rlock
       ------------     -----           -----
            1           14,496          14,716
            2            8,644           8,453
            4            6,799           6,983
            8            5,664           7,190
    
    On a ARM64 system, the performance results were:
    
                     Before Patch    After Patch
       # of Threads     rlock           rlock
       ------------     -----           -----
            1           23,676          24,488
            2            7,697           9,502
            4            4,945           3,440
            8            2,641           1,603
    
    For the uncontended case (1 thread), the new down_read_trylock() is a
    little bit faster. For the contended cases, the new down_read_trylock()
    perform pretty well in x86-64, but performance degrades at high
    contention level on ARM64.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-m68k@lists.linux-m68k.org
    Cc: linux-riscv@lists.infradead.org
    Cc: linux-um@lists.infradead.org
    Cc: linux-xtensa@linux-xtensa.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: nios2-dev@lists.rocketboards.org
    Cc: openrisc@lists.librecores.org
    Cc: uclinux-h8-devel@lists.sourceforge.jp
    Link: https://lkml.kernel.org/r/20190322143008.21313-4-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 3032f0c9008088a3effdc2622ce16c3e1bcb13a2
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Thu Mar 7 13:29:59 2019 -0800

    ARCv2: spinlock: remove the extra smp_mb before lock, after unlock
    
     - ARCv2 LLSC spinlocks have smp_mb() both before and after the LLSC
       instructions, which is not required per lkmm ACQ/REL semantics.
       smp_mb() is only needed _after_ lock and _before_ unlock.
       So remove the extra barriers.
       The reason they were there was mainly historical. At the time of
       initial SMP Linux bringup on HS38 cores, I was too conservative,
       given the fluidity of both hw and sw. The last attempt to ditch the
       extra barrier showed some hackbench regression which is apparently
       not the case now (atleast for LLSC case, read on...)
    
     - EX based spinlocks (!CONFIG_ARC_HAS_LLSC) still needs the extra
       smp_mb(), not due to lkmm, but due to some hardware shenanigans.
       W/o that, hackbench triggers RCU stall splat so extra DMB is retained
       !LLSC based systems are not realistic Linux sstem anyways so they can
       afford to be a nit suboptimal ;-)
    
       | [ARCLinux]# for i in (seq 1 1 5) ; do hackbench; done
       | Running with 10 groups 400 process
       | INFO: task hackbench:158 blocked for more than 10 seconds.
       |       Not tainted 4.20.0-00005-g96b18288a88e-dirty #117
       | "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
       | hackbench       D    0   158    135 0x00000000
       |
       | Stack Trace:
       | watchdog: BUG: soft lockup - CPU#3 stuck for 59s! [hackbench:469]
       | Modules linked in:
       | Path: (null)
       | CPU: 3 PID: 469 Comm: hackbench Not tainted 4.20.0-00005-g96b18288a88e-dirty
       |
       | [ECR   ]: 0x00000000 => Check Programmer's Manual
       | [EFA   ]: 0x00000000
       | [BLINK ]: do_exit+0x4a6/0x7d0
       | [ERET  ]: _raw_write_unlock_irq+0x44/0x5c
    
     - And while at it, remove the extar smp_mb() from EX based
       arch_read_trylock() since the spin lock there guarantees a full
       barrier anyways
    
     - For LLSC case, hackbench threads improves with this patch (HAPS @ 50MHz)
    
       ---- before ----
       |
       | [ARCLinux]# for i in 1 2 3 4 5; do hackbench 10 thread; done
       | Running with 10 groups 400 threads
       | Time: 16.253
       | Time: 16.445
       | Time: 16.590
       | Time: 16.721
       | Time: 16.544
    
       ---- after ----
       |
       | [ARCLinux]# for i in 1 2 3 4 5; do hackbench 10 thread; done
       | Running with 10 groups 400 threads
       | Time: 15.638
       | Time: 15.730
       | Time: 15.870
       | Time: 15.842
       | Time: 15.729
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

commit f90280d6b7963fa8925258ed66b4f567fe73dfea
Author: Kirill Tkhai <tkhai@ya.ru>
Date:   Fri Aug 17 15:48:25 2018 -0700

    mm/vmscan.c: clear shrinker bit if there are no objects related to memcg
    
    To avoid further unneed calls of do_shrink_slab() for shrinkers, which
    already do not have any charged objects in a memcg, their bits have to
    be cleared.
    
    This patch introduces a lockless mechanism to do that without races
    without parallel list lru add.  After do_shrink_slab() returns
    SHRINK_EMPTY the first time, we clear the bit and call it once again.
    Then we restore the bit, if the new return value is different.
    
    Note, that single smp_mb__after_atomic() in shrink_slab_memcg() covers
    two situations:
    
    1)list_lru_add()     shrink_slab_memcg
        list_add_tail()    for_each_set_bit() <--- read bit
                             do_shrink_slab() <--- missed list update (no barrier)
        <MB>                 <MB>
        set_bit()            do_shrink_slab() <--- seen list update
    
    This situation, when the first do_shrink_slab() sees set bit, but it
    doesn't see list update (i.e., race with the first element queueing), is
    rare.  So we don't add <MB> before the first call of do_shrink_slab()
    instead of this to do not slow down generic case.  Also, it's need the
    second call as seen in below in (2).
    
    2)list_lru_add()      shrink_slab_memcg()
        list_add_tail()     ...
        set_bit()           ...
      ...                   for_each_set_bit()
      do_shrink_slab()        do_shrink_slab()
        clear_bit()           ...
      ...                     ...
      list_lru_add()          ...
        list_add_tail()       clear_bit()
        <MB>                  <MB>
        set_bit()             do_shrink_slab()
    
    The barriers guarantee that the second do_shrink_slab() in the right
    side task sees list update if really cleared the bit.  This case is
    drawn in the code comment.
    
    [Results/performance of the patchset]
    
    After the whole patchset applied the below test shows signify increase
    of performance:
    
      $echo 1 > /sys/fs/cgroup/memory/memory.use_hierarchy
      $mkdir /sys/fs/cgroup/memory/ct
      $echo 4000M > /sys/fs/cgroup/memory/ct/memory.kmem.limit_in_bytes
          $for i in `seq 0 4000`; do mkdir /sys/fs/cgroup/memory/ct/$i;
                                echo $$ > /sys/fs/cgroup/memory/ct/$i/cgroup.procs;
                                mkdir -p s/$i; mount -t tmpfs $i s/$i;
                                touch s/$i/file; done
    
    Then, 5 sequential calls of drop caches:
    
      $time echo 3 > /proc/sys/vm/drop_caches
    
    1)Before:
      0.00user 13.78system 0:13.78elapsed 99%CPU
      0.00user 5.59system 0:05.60elapsed 99%CPU
      0.00user 5.48system 0:05.48elapsed 99%CPU
      0.00user 8.35system 0:08.35elapsed 99%CPU
      0.00user 8.34system 0:08.35elapsed 99%CPU
    
    2)After
      0.00user 1.10system 0:01.10elapsed 99%CPU
      0.00user 0.00system 0:00.01elapsed 64%CPU
      0.00user 0.01system 0:00.01elapsed 82%CPU
      0.00user 0.00system 0:00.01elapsed 64%CPU
      0.00user 0.01system 0:00.01elapsed 82%CPU
    
    The results show the performance increases at least in 548 times.
    
    Shakeel Butt tested this patchset with fork-bomb on his configuration:
    
     > I created 255 memcgs, 255 ext4 mounts and made each memcg create a
     > file containing few KiBs on corresponding mount. Then in a separate
     > memcg of 200 MiB limit ran a fork-bomb.
     >
     > I ran the "perf record -ag -- sleep 60" and below are the results:
     >
     > Without the patch series:
     > Samples: 4M of event 'cycles', Event count (approx.): 3279403076005
     > +  36.40%            fb.sh  [kernel.kallsyms]    [k] shrink_slab
     > +  18.97%            fb.sh  [kernel.kallsyms]    [k] list_lru_count_one
     > +   6.75%            fb.sh  [kernel.kallsyms]    [k] super_cache_count
     > +   0.49%            fb.sh  [kernel.kallsyms]    [k] down_read_trylock
     > +   0.44%            fb.sh  [kernel.kallsyms]    [k] mem_cgroup_iter
     > +   0.27%            fb.sh  [kernel.kallsyms]    [k] up_read
     > +   0.21%            fb.sh  [kernel.kallsyms]    [k] osq_lock
     > +   0.13%            fb.sh  [kernel.kallsyms]    [k] shmem_unused_huge_count
     > +   0.08%            fb.sh  [kernel.kallsyms]    [k] shrink_node_memcg
     > +   0.08%            fb.sh  [kernel.kallsyms]    [k] shrink_node
     >
     > With the patch series:
     > Samples: 4M of event 'cycles', Event count (approx.): 2756866824946
     > +  47.49%            fb.sh  [kernel.kallsyms]    [k] down_read_trylock
     > +  30.72%            fb.sh  [kernel.kallsyms]    [k] up_read
     > +   9.51%            fb.sh  [kernel.kallsyms]    [k] mem_cgroup_iter
     > +   1.69%            fb.sh  [kernel.kallsyms]    [k] shrink_node_memcg
     > +   1.35%            fb.sh  [kernel.kallsyms]    [k] mem_cgroup_protected
     > +   1.05%            fb.sh  [kernel.kallsyms]    [k] queued_spin_lock_slowpath
     > +   0.85%            fb.sh  [kernel.kallsyms]    [k] _raw_spin_lock
     > +   0.78%            fb.sh  [kernel.kallsyms]    [k] lruvec_lru_size
     > +   0.57%            fb.sh  [kernel.kallsyms]    [k] shrink_node
     > +   0.54%            fb.sh  [kernel.kallsyms]    [k] queue_work_on
     > +   0.46%            fb.sh  [kernel.kallsyms]    [k] shrink_slab_memcg
    
    [ktkhai@virtuozzo.com: v9]
      Link: http://lkml.kernel.org/r/153112561772.4097.11011071937553113003.stgit@localhost.localdomain
    Link: http://lkml.kernel.org/r/153063070859.1818.11870882950920963480.stgit@localhost.localdomain
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Tested-by: Shakeel Butt <shakeelb@google.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: "Huang, Ying" <ying.huang@intel.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Josef Bacik <jbacik@fb.com>
    Cc: Li RongQing <lirongqing@baidu.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matthias Kaehlcke <mka@chromium.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Sahitya Tummala <stummala@codeaurora.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b018fc9800557bd14a40d69501e19c340eb2c521
Merge: c07b3682cd12 7425ecd5e3e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 13:12:24 2018 -0700

    Merge tag 'pm-4.19-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "These add a new framework for CPU idle time injection, to be used by
      all of the idle injection code in the kernel in the future, fix some
      issues and add a number of relatively small extensions in multiple
      places.
    
      Specifics:
    
       - Add a new framework for CPU idle time injection (Daniel Lezcano).
    
       - Add AVS support to the armada-37xx cpufreq driver (Gregory
         CLEMENT).
    
       - Add support for current CPU frequency reporting to the ACPI CPPC
         cpufreq driver (George Cherian).
    
       - Rework the cooling device registration in the imx6q/thermal driver
         (Bastian Stender).
    
       - Make the pcc-cpufreq driver refuse to work with dynamic scaling
         governors on systems with many CPUs to avoid scalability issues
         with it (Rafael Wysocki).
    
       - Fix the intel_pstate driver to report different maximum CPU
         frequencies on systems where they really are different and to
         ignore the turbo active ratio if hardware-managend P-states (HWP)
         are in use; make it use the match_string() helper (Xie Yisheng,
         Srinivas Pandruvada).
    
       - Fix a minor deferred probe issue in the qcom-kryo cpufreq driver
         (Niklas Cassel).
    
       - Add a tracepoint for the tracking of frequency limits changes (from
         Andriod) to the cpufreq core (Ruchi Kandoi).
    
       - Fix a circular lock dependency between CPU hotplug and sysfs
         locking in the cpufreq core reported by lockdep (Waiman Long).
    
       - Avoid excessive error reports on driver registration failures in
         the ARM cpuidle driver (Sudeep Holla).
    
       - Add a new device links flag to the driver core to make links go
         away automatically on supplier driver removal (Vivek Gautam).
    
       - Eliminate potential race condition between system-wide power
         management transitions and system shutdown (Pingfan Liu).
    
       - Add a quirk to save NVS memory on system suspend for the ASUS 1025C
         laptop (Willy Tarreau).
    
       - Make more systems use suspend-to-idle (instead of ACPI S3) by
         default (Tristian Celestin).
    
       - Get rid of stack VLA usage in the low-level hibernation code on
         64-bit x86 (Kees Cook).
    
       - Fix error handling in the hibernation core and mark an expected
         fall-through switch in it (Chengguang Xu, Gustavo Silva).
    
       - Extend the generic power domains (genpd) framework to support
         attaching a device to a power domain by name (Ulf Hansson).
    
       - Fix device reference counting and user limits initialization in the
         devfreq core (Arvind Yadav, Matthias Kaehlcke).
    
       - Fix a few issues in the rk3399_dmc devfreq driver and improve its
         documentation (Enric Balletbo i Serra, Lin Huang, Nick Milner).
    
       - Drop a redundant error message from the exynos-ppmu devfreq driver
         (Markus Elfring)"
    
    * tag 'pm-4.19-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (35 commits)
      PM / reboot: Eliminate race between reboot and suspend
      PM / hibernate: Mark expected switch fall-through
      cpufreq: intel_pstate: Ignore turbo active ratio in HWP
      cpufreq: Fix a circular lock dependency problem
      cpu/hotplug: Add a cpus_read_trylock() function
      x86/power/hibernate_64: Remove VLA usage
      cpufreq: trace frequency limits change
      cpufreq: intel_pstate: Show different max frequency with turbo 3 and HWP
      cpufreq: pcc-cpufreq: Disable dynamic scaling on many-CPU systems
      cpufreq: qcom-kryo: Silently error out on EPROBE_DEFER
      cpufreq / CPPC: Add cpuinfo_cur_freq support for CPPC
      cpufreq: armada-37xx: Add AVS support
      dt-bindings: marvell: Add documentation for the Armada 3700 AVS binding
      PM / devfreq: rk3399_dmc: Fix duplicated opp table on reload.
      PM / devfreq: Init user limits from OPP limits, not viceversa
      PM / devfreq: rk3399_dmc: fix spelling mistakes.
      PM / devfreq: rk3399_dmc: do not print error when get supply and clk defer.
      dt-bindings: devfreq: rk3399_dmc: move interrupts to be optional.
      PM / devfreq: rk3399_dmc: remove wait for dcf irq event.
      dt-bindings: clock: add rk3399 DDR3 standard speed bins.
      ...

commit 7425ecd5e3e8c9d84f399a102282a23a90a19278
Merge: 8f0f2b621198 d3264f752a1a
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Aug 14 09:57:34 2018 +0200

    Merge branch 'pm-cpufreq'
    
    Merge cpufreq changes for 4.19.
    
    These are driver extensions, some driver and core fixes and a new
    tracepoint for the tracking of frequency limits changes (coming from
    Android).
    
    * pm-cpufreq:
      cpufreq: intel_pstate: Ignore turbo active ratio in HWP
      cpufreq: Fix a circular lock dependency problem
      cpu/hotplug: Add a cpus_read_trylock() function
      cpufreq: trace frequency limits change
      cpufreq: intel_pstate: Show different max frequency with turbo 3 and HWP
      cpufreq: pcc-cpufreq: Disable dynamic scaling on many-CPU systems
      cpufreq: qcom-kryo: Silently error out on EPROBE_DEFER
      cpufreq / CPPC: Add cpuinfo_cur_freq support for CPPC
      cpufreq: armada-37xx: Add AVS support
      dt-bindings: marvell: Add documentation for the Armada 3700 AVS binding
      cpufreq: imx6q/thermal: imx: register cooling device depending on OF
      cpufreq: intel_pstate: use match_string() helper

commit 9b3d9bb3e4deef41095e513c2ffbebab20f9a982
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jul 24 14:26:05 2018 -0400

    cpufreq: Fix a circular lock dependency problem
    
    With lockdep turned on, the following circular lock dependency problem
    was reported:
    
    [   57.470040] ======================================================
    [   57.502900] WARNING: possible circular locking dependency detected
    [   57.535208] 4.18.0-0.rc3.1.el8+7.x86_64+debug #1 Tainted: G
    [   57.577761] ------------------------------------------------------
    [   57.609714] tuned/1505 is trying to acquire lock:
    [   57.633808] 00000000559deec5 (cpu_hotplug_lock.rw_sem){++++}, at: store+0x27/0x120
    [   57.672880]
    [   57.672880] but task is already holding lock:
    [   57.702184] 000000002136ca64 (kn->count#118){++++}, at: kernfs_fop_write+0x1d0/0x410
    [   57.742176]
    [   57.742176] which lock already depends on the new lock.
    [   57.742176]
    [   57.785220]
    [   57.785220] the existing dependency chain (in reverse order) is:
        :
    [   58.932512] other info that might help us debug this:
    [   58.932512]
    [   58.973344] Chain exists of:
    [   58.973344]   cpu_hotplug_lock.rw_sem --> subsys mutex#5 --> kn->count#118
    [   58.973344]
    [   59.030795]  Possible unsafe locking scenario:
    [   59.030795]
    [   59.061248]        CPU0                    CPU1
    [   59.085377]        ----                    ----
    [   59.108160]   lock(kn->count#118);
    [   59.124935]                                lock(subsys mutex#5);
    [   59.156330]                                lock(kn->count#118);
    [   59.186088]   lock(cpu_hotplug_lock.rw_sem);
    [   59.208541]
    [   59.208541]  *** DEADLOCK ***
    
    In the cpufreq_register_driver() function, the lock sequence is:
    
      cpus_read_lock --> kn->count
    
    For the cpufreq sysfs store method, the lock sequence is:
    
      kn->count --> cpus_read_lock
    
    These sequences are actually safe as they are taking a share lock on
    cpu_hotplug_lock. However, the current lockdep code doesn't check for
    share locking when detecting circular lock dependency.  Fixing that
    could be a substantial effort.
    
    Instead, we can work around this problem by using cpus_read_trylock()
    in the store method which is much simpler. The chance of not getting
    the read lock is very small. If that happens, the userspace application
    that writes the sysfs file will get an error.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 6f4ceee9305dc3fe74099159b460f4b56b506f1d
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jul 24 14:26:04 2018 -0400

    cpu/hotplug: Add a cpus_read_trylock() function
    
    There are use cases where it can be useful to have a cpus_read_trylock()
    function to work around circular lock dependency problem involving
    the cpu_hotplug_lock.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit e56d3700cff0839fb22a7890f78b370757c42ba7
Author: Yang Shi <yang.s@alibaba-inc.com>
Date:   Wed Jan 31 16:18:28 2018 -0800

    mm: thp: use down_read_trylock() in khugepaged to avoid long block
    
    [ Upstream commit 3b454ad35043dfbd3b5d2bb92b0991d6342afb44 ]
    
    In the current design, khugepaged needs to acquire mmap_sem before
    scanning an mm.  But in some corner cases, khugepaged may scan a process
    which is modifying its memory mapping, so khugepaged blocks in
    uninterruptible state.  But the process might hold the mmap_sem for a
    long time when modifying a huge memory space and it may trigger the
    below khugepaged hung issue:
    
      INFO: task khugepaged:270 blocked for more than 120 seconds.
      Tainted: G E 4.9.65-006.ali3000.alios7.x86_64 #1
      "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      khugepaged D 0 270 2 0x00000000 
      ffff883f3deae4c0 0000000000000000 ffff883f610596c0 ffff883f7d359440
      ffff883f63818000 ffffc90019adfc78 ffffffff817079a5 d67e5aa8c1860a64
      0000000000000246 ffff883f7d359440 ffffc90019adfc88 ffff883f610596c0
      Call Trace:
        schedule+0x36/0x80
        rwsem_down_read_failed+0xf0/0x150
        call_rwsem_down_read_failed+0x18/0x30
        down_read+0x20/0x40
        khugepaged+0x476/0x11d0
        kthread+0xe6/0x100
        ret_from_fork+0x25/0x30
    
    So it sounds pointless to just block khugepaged waiting for the
    semaphore so replace down_read() with down_read_trylock() to move to
    scan the next mm quickly instead of just blocking on the semaphore so
    that other processes can get more chances to install THP.  Then
    khugepaged can come back to scan the skipped mm when it has finished the
    current round full_scan.
    
    And it appears that the change can improve khugepaged efficiency a
    little bit.
    
    Below is the test result when running LTP on a 24 cores 4GB memory 2
    nodes NUMA VM:
    
                                        pristine          w/ trylock
      full_scan                         197               187
      pages_collapsed                   21                26
      thp_fault_alloc                   40818             44466
      thp_fault_fallback                18413             16679
      thp_collapse_alloc                21                150
      thp_collapse_alloc_failed         14                16
      thp_file_alloc                    369               369
    
    [akpm@linux-foundation.org: coding-style fixes]
    [akpm@linux-foundation.org: tweak comment]
    [arnd@arndb.de: avoid uninitialized variable use]
      Link: http://lkml.kernel.org/r/20171215125129.2948634-1-arnd@arndb.de
    Link: http://lkml.kernel.org/r/1513281203-54878-1-git-send-email-yang.s@alibaba-inc.com
    Signed-off-by: Yang Shi <yang.s@alibaba-inc.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8054b87fccd4fe9c67bfc164462d81b38ea56af4
Author: Yang Shi <yang.s@alibaba-inc.com>
Date:   Wed Jan 31 16:18:28 2018 -0800

    mm: thp: use down_read_trylock() in khugepaged to avoid long block
    
    
    [ Upstream commit 3b454ad35043dfbd3b5d2bb92b0991d6342afb44 ]
    
    In the current design, khugepaged needs to acquire mmap_sem before
    scanning an mm.  But in some corner cases, khugepaged may scan a process
    which is modifying its memory mapping, so khugepaged blocks in
    uninterruptible state.  But the process might hold the mmap_sem for a
    long time when modifying a huge memory space and it may trigger the
    below khugepaged hung issue:
    
      INFO: task khugepaged:270 blocked for more than 120 seconds.
      Tainted: G E 4.9.65-006.ali3000.alios7.x86_64 #1
      "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      khugepaged D 0 270 2 0x00000000 
      ffff883f3deae4c0 0000000000000000 ffff883f610596c0 ffff883f7d359440
      ffff883f63818000 ffffc90019adfc78 ffffffff817079a5 d67e5aa8c1860a64
      0000000000000246 ffff883f7d359440 ffffc90019adfc88 ffff883f610596c0
      Call Trace:
        schedule+0x36/0x80
        rwsem_down_read_failed+0xf0/0x150
        call_rwsem_down_read_failed+0x18/0x30
        down_read+0x20/0x40
        khugepaged+0x476/0x11d0
        kthread+0xe6/0x100
        ret_from_fork+0x25/0x30
    
    So it sounds pointless to just block khugepaged waiting for the
    semaphore so replace down_read() with down_read_trylock() to move to
    scan the next mm quickly instead of just blocking on the semaphore so
    that other processes can get more chances to install THP.  Then
    khugepaged can come back to scan the skipped mm when it has finished the
    current round full_scan.
    
    And it appears that the change can improve khugepaged efficiency a
    little bit.
    
    Below is the test result when running LTP on a 24 cores 4GB memory 2
    nodes NUMA VM:
    
                                        pristine          w/ trylock
      full_scan                         197               187
      pages_collapsed                   21                26
      thp_fault_alloc                   40818             44466
      thp_fault_fallback                18413             16679
      thp_collapse_alloc                21                150
      thp_collapse_alloc_failed         14                16
      thp_file_alloc                    369               369
    
    [akpm@linux-foundation.org: coding-style fixes]
    [akpm@linux-foundation.org: tweak comment]
    [arnd@arndb.de: avoid uninitialized variable use]
      Link: http://lkml.kernel.org/r/20171215125129.2948634-1-arnd@arndb.de
    Link: http://lkml.kernel.org/r/1513281203-54878-1-git-send-email-yang.s@alibaba-inc.com
    Signed-off-by: Yang Shi <yang.s@alibaba-inc.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a09881cfb7135787375a3975cbd05bc94e07a0d0
Author: Sheng Yong <shengyong1@huawei.com>
Date:   Wed Jan 17 12:11:31 2018 +0800

    f2fs: avoid hungtask when GC encrypted block if io_bits is set
    
    
    [ Upstream commit a9d572c7550044d5b217b5287d99a2e6d34b97b0 ]
    
    When io_bits is set, GCing encrypted block may hit the following hungtask.
    Since io_bits requires aligned block address, f2fs_submit_page_write may
    return -EAGAIN if new_blkaddr does not satisify io_bits alignment. As a
    result, the encrypted page will never be writtenback.
    
    This patch makes move_data_block aware the EAGAIN error and cancel the
    writeback.
    
    [  246.751371] INFO: task kworker/u4:4:797 blocked for more than 90 seconds.
    [  246.752423]       Not tainted 4.15.0-rc4+ #11
    [  246.754176] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    [  246.755336] kworker/u4:4    D25448   797      2 0x80000000
    [  246.755597] Workqueue: writeback wb_workfn (flush-7:0)
    [  246.755616] Call Trace:
    [  246.755695]  ? __schedule+0x322/0xa90
    [  246.755761]  ? blk_init_request_from_bio+0x120/0x120
    [  246.755773]  ? pci_mmcfg_check_reserved+0xb0/0xb0
    [  246.755801]  ? __radix_tree_create+0x19e/0x200
    [  246.755813]  ? delete_node+0x136/0x370
    [  246.755838]  schedule+0x43/0xc0
    [  246.755904]  io_schedule+0x17/0x40
    [  246.755939]  wait_on_page_bit_common+0x17b/0x240
    [  246.755950]  ? wake_page_function+0xa0/0xa0
    [  246.755961]  ? add_to_page_cache_lru+0x160/0x160
    [  246.755972]  ? page_cache_tree_insert+0x170/0x170
    [  246.755983]  ? __lru_cache_add+0x96/0xb0
    [  246.756086]  __filemap_fdatawait_range+0x14f/0x1c0
    [  246.756097]  ? wait_on_page_bit_common+0x240/0x240
    [  246.756120]  ? __wake_up_locked_key_bookmark+0x20/0x20
    [  246.756167]  ? wait_on_all_pages_writeback+0xc9/0x100
    [  246.756179]  ? __remove_ino_entry+0x120/0x120
    [  246.756192]  ? wait_woken+0x100/0x100
    [  246.756204]  filemap_fdatawait_range+0x9/0x20
    [  246.756216]  write_checkpoint+0x18a1/0x1f00
    [  246.756254]  ? blk_get_request+0x10/0x10
    [  246.756265]  ? cpumask_next_and+0x43/0x60
    [  246.756279]  ? f2fs_sync_inode_meta+0x160/0x160
    [  246.756289]  ? remove_element.isra.4+0xa0/0xa0
    [  246.756300]  ? __put_compound_page+0x40/0x40
    [  246.756310]  ? f2fs_sync_fs+0xec/0x1c0
    [  246.756320]  ? f2fs_sync_fs+0x120/0x1c0
    [  246.756329]  f2fs_sync_fs+0x120/0x1c0
    [  246.756357]  ? trace_event_raw_event_f2fs__page+0x260/0x260
    [  246.756393]  ? ata_build_rw_tf+0x173/0x410
    [  246.756397]  f2fs_balance_fs_bg+0x198/0x390
    [  246.756405]  ? drop_inmem_page+0x230/0x230
    [  246.756415]  ? ahci_qc_prep+0x1bb/0x2e0
    [  246.756418]  ? ahci_qc_issue+0x1df/0x290
    [  246.756422]  ? __accumulate_pelt_segments+0x42/0xd0
    [  246.756426]  ? f2fs_write_node_pages+0xd1/0x380
    [  246.756429]  f2fs_write_node_pages+0xd1/0x380
    [  246.756437]  ? sync_node_pages+0x8f0/0x8f0
    [  246.756440]  ? update_curr+0x53/0x220
    [  246.756444]  ? __accumulate_pelt_segments+0xa2/0xd0
    [  246.756448]  ? __update_load_avg_se.isra.39+0x349/0x360
    [  246.756452]  ? do_writepages+0x2a/0xa0
    [  246.756456]  do_writepages+0x2a/0xa0
    [  246.756460]  __writeback_single_inode+0x70/0x490
    [  246.756463]  ? check_preempt_wakeup+0x199/0x310
    [  246.756467]  writeback_sb_inodes+0x2a2/0x660
    [  246.756471]  ? is_empty_dir_inode+0x40/0x40
    [  246.756474]  ? __writeback_single_inode+0x490/0x490
    [  246.756477]  ? string+0xbf/0xf0
    [  246.756480]  ? down_read_trylock+0x35/0x60
    [  246.756484]  __writeback_inodes_wb+0x9f/0xf0
    [  246.756488]  wb_writeback+0x41d/0x4b0
    [  246.756492]  ? writeback_inodes_wb.constprop.55+0x150/0x150
    [  246.756498]  ? set_worker_desc+0xf7/0x130
    [  246.756502]  ? current_is_workqueue_rescuer+0x60/0x60
    [  246.756511]  ? _find_next_bit+0x2c/0xa0
    [  246.756514]  ? wb_workfn+0x400/0x5d0
    [  246.756518]  wb_workfn+0x400/0x5d0
    [  246.756521]  ? finish_task_switch+0xdf/0x2a0
    [  246.756525]  ? inode_wait_for_writeback+0x30/0x30
    [  246.756529]  process_one_work+0x3a7/0x6f0
    [  246.756533]  worker_thread+0x82/0x750
    [  246.756537]  kthread+0x16f/0x1c0
    [  246.756541]  ? trace_event_raw_event_workqueue_work+0x110/0x110
    [  246.756544]  ? kthread_create_worker_on_cpu+0xb0/0xb0
    [  246.756548]  ret_from_fork+0x1f/0x30
    
    Signed-off-by: Sheng Yong <shengyong1@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d5367b8982446d191d60e57f1335726958c6242f
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Mon May 15 15:13:16 2017 +0200

    sched/numa: Use down_read_trylock() for the mmap_sem
    
    
    [ Upstream commit 8655d5497735b288f8a9b458bd22e7d1bf95bb61 ]
    
    A customer has reported a soft-lockup when running an intensive
    memory stress test, where the trace on multiple CPU's looks like this:
    
     RIP: 0010:[<ffffffff810c53fe>]
      [<ffffffff810c53fe>] native_queued_spin_lock_slowpath+0x10e/0x190
    ...
     Call Trace:
      [<ffffffff81182d07>] queued_spin_lock_slowpath+0x7/0xa
      [<ffffffff811bc331>] change_protection_range+0x3b1/0x930
      [<ffffffff811d4be8>] change_prot_numa+0x18/0x30
      [<ffffffff810adefe>] task_numa_work+0x1fe/0x310
      [<ffffffff81098322>] task_work_run+0x72/0x90
    
    Further investigation showed that the lock contention here is pmd_lock().
    
    The task_numa_work() function makes sure that only one thread is let to perform
    the work in a single scan period (via cmpxchg), but if there's a thread with
    mmap_sem locked for writing for several periods, multiple threads in
    task_numa_work() can build up a convoy waiting for mmap_sem for read and then
    all get unblocked at once.
    
    This patch changes the down_read() to the trylock version, which prevents the
    build up. For a workload experiencing mmap_sem contention, it's probably better
    to postpone the NUMA balancing work anyway. This seems to have fixed the soft
    lockups involving pmd_lock(), which is in line with the convoy theory.
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170515131316.21909-1-vbabka@suse.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a1e7a9e2e3c992574a19493566aff6580a2d5ad5
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Mon May 15 15:13:16 2017 +0200

    sched/numa: Use down_read_trylock() for the mmap_sem
    
    
    [ Upstream commit 8655d5497735b288f8a9b458bd22e7d1bf95bb61 ]
    
    A customer has reported a soft-lockup when running an intensive
    memory stress test, where the trace on multiple CPU's looks like this:
    
     RIP: 0010:[<ffffffff810c53fe>]
      [<ffffffff810c53fe>] native_queued_spin_lock_slowpath+0x10e/0x190
    ...
     Call Trace:
      [<ffffffff81182d07>] queued_spin_lock_slowpath+0x7/0xa
      [<ffffffff811bc331>] change_protection_range+0x3b1/0x930
      [<ffffffff811d4be8>] change_prot_numa+0x18/0x30
      [<ffffffff810adefe>] task_numa_work+0x1fe/0x310
      [<ffffffff81098322>] task_work_run+0x72/0x90
    
    Further investigation showed that the lock contention here is pmd_lock().
    
    The task_numa_work() function makes sure that only one thread is let to perform
    the work in a single scan period (via cmpxchg), but if there's a thread with
    mmap_sem locked for writing for several periods, multiple threads in
    task_numa_work() can build up a convoy waiting for mmap_sem for read and then
    all get unblocked at once.
    
    This patch changes the down_read() to the trylock version, which prevents the
    build up. For a workload experiencing mmap_sem contention, it's probably better
    to postpone the NUMA balancing work anyway. This seems to have fixed the soft
    lockups involving pmd_lock(), which is in line with the convoy theory.
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170515131316.21909-1-vbabka@suse.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0c7c1bed7e13dbb545375c231e6ba1dca5e8d725
Author: Kirill Tkhai <tkhai@ya.ru>
Date:   Thu Apr 5 16:25:08 2018 -0700

    mm: make counting of list_lru_one::nr_items lockless
    
    During the reclaiming slab of a memcg, shrink_slab iterates over all
    registered shrinkers in the system, and tries to count and consume
    objects related to the cgroup.  In case of memory pressure, this behaves
    bad: I observe high system time and time spent in list_lru_count_one()
    for many processes on RHEL7 kernel.
    
    This patch makes list_lru_node::memcg_lrus rcu protected, that allows to
    skip taking spinlock in list_lru_count_one().
    
    Shakeel Butt with the patch observes significant perf graph change.  He
    says:
    
    ========================================================================
    Setup: running a fork-bomb in a memcg of 200MiB on a 8GiB and 4 vcpu
    VM and recording the trace with 'perf record -g -a'.
    
    The trace without the patch:
    
    +  34.19%     fb.sh  [kernel.kallsyms]  [k] queued_spin_lock_slowpath
    +  30.77%     fb.sh  [kernel.kallsyms]  [k] _raw_spin_lock
    +   3.53%     fb.sh  [kernel.kallsyms]  [k] list_lru_count_one
    +   2.26%     fb.sh  [kernel.kallsyms]  [k] super_cache_count
    +   1.68%     fb.sh  [kernel.kallsyms]  [k] shrink_slab
    +   0.59%     fb.sh  [kernel.kallsyms]  [k] down_read_trylock
    +   0.48%     fb.sh  [kernel.kallsyms]  [k] _raw_spin_unlock_irqrestore
    +   0.38%     fb.sh  [kernel.kallsyms]  [k] shrink_node_memcg
    +   0.32%     fb.sh  [kernel.kallsyms]  [k] queue_work_on
    +   0.26%     fb.sh  [kernel.kallsyms]  [k] count_shadow_nodes
    
    With the patch:
    
    +   0.16%     swapper  [kernel.kallsyms]    [k] default_idle
    +   0.13%     oom_reaper  [kernel.kallsyms]    [k] mutex_spin_on_owner
    +   0.05%     perf  [kernel.kallsyms]    [k] copy_user_generic_string
    +   0.05%     init.real  [kernel.kallsyms]    [k] wait_consider_task
    +   0.05%     kworker/0:0  [kernel.kallsyms]    [k] finish_task_switch
    +   0.04%     kworker/2:1  [kernel.kallsyms]    [k] finish_task_switch
    +   0.04%     kworker/3:1  [kernel.kallsyms]    [k] finish_task_switch
    +   0.04%     kworker/1:0  [kernel.kallsyms]    [k] finish_task_switch
    +   0.03%     binary  [kernel.kallsyms]    [k] copy_page
    ========================================================================
    
    Thanks Shakeel for the testing.
    
    [ktkhai@virtuozzo.com: v2]
      Link: http://lkml.kernel.org/r/151203869520.3915.2587549826865799173.stgit@localhost.localdomain
    Link: http://lkml.kernel.org/r/150583358557.26700.8490036563698102569.stgit@localhost.localdomain
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Tested-by: Shakeel Butt <shakeelb@google.com>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 71b009c13528b4f77ed16e5980a6f16f926de1a2
Author: Sahara <keun-o.park@darkmatter.ae>
Date:   Wed Dec 13 09:10:48 2017 +0400

    pty: cancel pty slave port buf's work in tty_release
    
    
    [ Upstream commit 2b022ab7542df60021ab57854b3faaaf42552eaf ]
    
    In case that CONFIG_SLUB_DEBUG is on and pty is used, races between
    release_one_tty and flush_to_ldisc work threads may happen and lead
    to use-after-free condition on tty->link->port. Because SLUB_DEBUG
    is turned on, freed tty->link->port is filled with POISON_FREE value.
    So far without SLUB_DEBUG, port was filled with zero and flush_to_ldisc
    could return without a problem by checking if tty is NULL.
    
    CPU 0                                 CPU 1
    -----                                 -----
    release_tty                           pty_write
       cancel_work_sync(tty)                 to = tty->link
       tty_kref_put(tty->link)               tty_schedule_flip(to->port)
          << workqueue >>                 ...
          release_one_tty                 ...
             pty_cleanup                  ...
                kfree(tty->link->port)       << workqueue >>
                                             flush_to_ldisc
                                                tty = READ_ONCE(port->itty)
                                                tty is 0x6b6b6b6b6b6b6b6b
                                                !!PANIC!! access tty->ldisc
    
     Unable to handle kernel paging request at virtual address 6b6b6b6b6b6b6b93
     pgd = ffffffc0eb1c3000
     [6b6b6b6b6b6b6b93] *pgd=0000000000000000, *pud=0000000000000000
     ------------[ cut here ]------------
     Kernel BUG at ffffff800851154c [verbose debug info unavailable]
     Internal error: Oops - BUG: 96000004 [#1] PREEMPT SMP
     CPU: 3 PID: 265 Comm: kworker/u8:9 Tainted: G        W 3.18.31-g0a58eeb #1
     Hardware name: Qualcomm Technologies, Inc. MSM 8996pro v1.1 + PMI8996 Carbide (DT)
     Workqueue: events_unbound flush_to_ldisc
     task: ffffffc0ed610ec0 ti: ffffffc0ed624000 task.ti: ffffffc0ed624000
     PC is at ldsem_down_read_trylock+0x0/0x4c
     LR is at tty_ldisc_ref+0x24/0x4c
     pc : [<ffffff800851154c>] lr : [<ffffff800850f6c0>] pstate: 80400145
     sp : ffffffc0ed627cd0
     x29: ffffffc0ed627cd0 x28: 0000000000000000
     x27: ffffff8009e05000 x26: ffffffc0d382cfa0
     x25: 0000000000000000 x24: ffffff800a012f08
     x23: 0000000000000000 x22: ffffffc0703fbc88
     x21: 6b6b6b6b6b6b6b6b x20: 6b6b6b6b6b6b6b93
     x19: 0000000000000000 x18: 0000000000000001
     x17: 00e80000f80d6f53 x16: 0000000000000001
     x15: 0000007f7d826fff x14: 00000000000000a0
     x13: 0000000000000000 x12: 0000000000000109
     x11: 0000000000000000 x10: 0000000000000000
     x9 : ffffffc0ed624000 x8 : ffffffc0ed611580
     x7 : 0000000000000000 x6 : ffffff800a42e000
     x5 : 00000000000003fc x4 : 0000000003bd1201
     x3 : 0000000000000001 x2 : 0000000000000001
     x1 : ffffff800851004c x0 : 6b6b6b6b6b6b6b93
    
    Signed-off-by: Sahara <keun-o.park@darkmatter.ae>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f16a65befe744c3c32bc76d817b34d9c368a136e
Author: Sahara <keun-o.park@darkmatter.ae>
Date:   Wed Dec 13 09:10:48 2017 +0400

    pty: cancel pty slave port buf's work in tty_release
    
    
    [ Upstream commit 2b022ab7542df60021ab57854b3faaaf42552eaf ]
    
    In case that CONFIG_SLUB_DEBUG is on and pty is used, races between
    release_one_tty and flush_to_ldisc work threads may happen and lead
    to use-after-free condition on tty->link->port. Because SLUB_DEBUG
    is turned on, freed tty->link->port is filled with POISON_FREE value.
    So far without SLUB_DEBUG, port was filled with zero and flush_to_ldisc
    could return without a problem by checking if tty is NULL.
    
    CPU 0                                 CPU 1
    -----                                 -----
    release_tty                           pty_write
       cancel_work_sync(tty)                 to = tty->link
       tty_kref_put(tty->link)               tty_schedule_flip(to->port)
          << workqueue >>                 ...
          release_one_tty                 ...
             pty_cleanup                  ...
                kfree(tty->link->port)       << workqueue >>
                                             flush_to_ldisc
                                                tty = READ_ONCE(port->itty)
                                                tty is 0x6b6b6b6b6b6b6b6b
                                                !!PANIC!! access tty->ldisc
    
     Unable to handle kernel paging request at virtual address 6b6b6b6b6b6b6b93
     pgd = ffffffc0eb1c3000
     [6b6b6b6b6b6b6b93] *pgd=0000000000000000, *pud=0000000000000000
     ------------[ cut here ]------------
     Kernel BUG at ffffff800851154c [verbose debug info unavailable]
     Internal error: Oops - BUG: 96000004 [#1] PREEMPT SMP
     CPU: 3 PID: 265 Comm: kworker/u8:9 Tainted: G        W 3.18.31-g0a58eeb #1
     Hardware name: Qualcomm Technologies, Inc. MSM 8996pro v1.1 + PMI8996 Carbide (DT)
     Workqueue: events_unbound flush_to_ldisc
     task: ffffffc0ed610ec0 ti: ffffffc0ed624000 task.ti: ffffffc0ed624000
     PC is at ldsem_down_read_trylock+0x0/0x4c
     LR is at tty_ldisc_ref+0x24/0x4c
     pc : [<ffffff800851154c>] lr : [<ffffff800850f6c0>] pstate: 80400145
     sp : ffffffc0ed627cd0
     x29: ffffffc0ed627cd0 x28: 0000000000000000
     x27: ffffff8009e05000 x26: ffffffc0d382cfa0
     x25: 0000000000000000 x24: ffffff800a012f08
     x23: 0000000000000000 x22: ffffffc0703fbc88
     x21: 6b6b6b6b6b6b6b6b x20: 6b6b6b6b6b6b6b93
     x19: 0000000000000000 x18: 0000000000000001
     x17: 00e80000f80d6f53 x16: 0000000000000001
     x15: 0000007f7d826fff x14: 00000000000000a0
     x13: 0000000000000000 x12: 0000000000000109
     x11: 0000000000000000 x10: 0000000000000000
     x9 : ffffffc0ed624000 x8 : ffffffc0ed611580
     x7 : 0000000000000000 x6 : ffffff800a42e000
     x5 : 00000000000003fc x4 : 0000000003bd1201
     x3 : 0000000000000001 x2 : 0000000000000001
     x1 : ffffff800851004c x0 : 6b6b6b6b6b6b6b93
    
    Signed-off-by: Sahara <keun-o.park@darkmatter.ae>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d69cf8561fb995be0e72da6c92e206f342707d67
Author: Sahara <keun-o.park@darkmatter.ae>
Date:   Wed Dec 13 09:10:48 2017 +0400

    pty: cancel pty slave port buf's work in tty_release
    
    
    [ Upstream commit 2b022ab7542df60021ab57854b3faaaf42552eaf ]
    
    In case that CONFIG_SLUB_DEBUG is on and pty is used, races between
    release_one_tty and flush_to_ldisc work threads may happen and lead
    to use-after-free condition on tty->link->port. Because SLUB_DEBUG
    is turned on, freed tty->link->port is filled with POISON_FREE value.
    So far without SLUB_DEBUG, port was filled with zero and flush_to_ldisc
    could return without a problem by checking if tty is NULL.
    
    CPU 0                                 CPU 1
    -----                                 -----
    release_tty                           pty_write
       cancel_work_sync(tty)                 to = tty->link
       tty_kref_put(tty->link)               tty_schedule_flip(to->port)
          << workqueue >>                 ...
          release_one_tty                 ...
             pty_cleanup                  ...
                kfree(tty->link->port)       << workqueue >>
                                             flush_to_ldisc
                                                tty = READ_ONCE(port->itty)
                                                tty is 0x6b6b6b6b6b6b6b6b
                                                !!PANIC!! access tty->ldisc
    
     Unable to handle kernel paging request at virtual address 6b6b6b6b6b6b6b93
     pgd = ffffffc0eb1c3000
     [6b6b6b6b6b6b6b93] *pgd=0000000000000000, *pud=0000000000000000
     ------------[ cut here ]------------
     Kernel BUG at ffffff800851154c [verbose debug info unavailable]
     Internal error: Oops - BUG: 96000004 [#1] PREEMPT SMP
     CPU: 3 PID: 265 Comm: kworker/u8:9 Tainted: G        W 3.18.31-g0a58eeb #1
     Hardware name: Qualcomm Technologies, Inc. MSM 8996pro v1.1 + PMI8996 Carbide (DT)
     Workqueue: events_unbound flush_to_ldisc
     task: ffffffc0ed610ec0 ti: ffffffc0ed624000 task.ti: ffffffc0ed624000
     PC is at ldsem_down_read_trylock+0x0/0x4c
     LR is at tty_ldisc_ref+0x24/0x4c
     pc : [<ffffff800851154c>] lr : [<ffffff800850f6c0>] pstate: 80400145
     sp : ffffffc0ed627cd0
     x29: ffffffc0ed627cd0 x28: 0000000000000000
     x27: ffffff8009e05000 x26: ffffffc0d382cfa0
     x25: 0000000000000000 x24: ffffff800a012f08
     x23: 0000000000000000 x22: ffffffc0703fbc88
     x21: 6b6b6b6b6b6b6b6b x20: 6b6b6b6b6b6b6b93
     x19: 0000000000000000 x18: 0000000000000001
     x17: 00e80000f80d6f53 x16: 0000000000000001
     x15: 0000007f7d826fff x14: 00000000000000a0
     x13: 0000000000000000 x12: 0000000000000109
     x11: 0000000000000000 x10: 0000000000000000
     x9 : ffffffc0ed624000 x8 : ffffffc0ed611580
     x7 : 0000000000000000 x6 : ffffff800a42e000
     x5 : 00000000000003fc x4 : 0000000003bd1201
     x3 : 0000000000000001 x2 : 0000000000000001
     x1 : ffffff800851004c x0 : 6b6b6b6b6b6b6b93
    
    Signed-off-by: Sahara <keun-o.park@darkmatter.ae>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d06bff351201161fcbaf0907add9fbc5ee954db6
Author: Sahara <keun-o.park@darkmatter.ae>
Date:   Wed Dec 13 09:10:48 2017 +0400

    pty: cancel pty slave port buf's work in tty_release
    
    
    [ Upstream commit 2b022ab7542df60021ab57854b3faaaf42552eaf ]
    
    In case that CONFIG_SLUB_DEBUG is on and pty is used, races between
    release_one_tty and flush_to_ldisc work threads may happen and lead
    to use-after-free condition on tty->link->port. Because SLUB_DEBUG
    is turned on, freed tty->link->port is filled with POISON_FREE value.
    So far without SLUB_DEBUG, port was filled with zero and flush_to_ldisc
    could return without a problem by checking if tty is NULL.
    
    CPU 0                                 CPU 1
    -----                                 -----
    release_tty                           pty_write
       cancel_work_sync(tty)                 to = tty->link
       tty_kref_put(tty->link)               tty_schedule_flip(to->port)
          << workqueue >>                 ...
          release_one_tty                 ...
             pty_cleanup                  ...
                kfree(tty->link->port)       << workqueue >>
                                             flush_to_ldisc
                                                tty = READ_ONCE(port->itty)
                                                tty is 0x6b6b6b6b6b6b6b6b
                                                !!PANIC!! access tty->ldisc
    
     Unable to handle kernel paging request at virtual address 6b6b6b6b6b6b6b93
     pgd = ffffffc0eb1c3000
     [6b6b6b6b6b6b6b93] *pgd=0000000000000000, *pud=0000000000000000
     ------------[ cut here ]------------
     Kernel BUG at ffffff800851154c [verbose debug info unavailable]
     Internal error: Oops - BUG: 96000004 [#1] PREEMPT SMP
     CPU: 3 PID: 265 Comm: kworker/u8:9 Tainted: G        W 3.18.31-g0a58eeb #1
     Hardware name: Qualcomm Technologies, Inc. MSM 8996pro v1.1 + PMI8996 Carbide (DT)
     Workqueue: events_unbound flush_to_ldisc
     task: ffffffc0ed610ec0 ti: ffffffc0ed624000 task.ti: ffffffc0ed624000
     PC is at ldsem_down_read_trylock+0x0/0x4c
     LR is at tty_ldisc_ref+0x24/0x4c
     pc : [<ffffff800851154c>] lr : [<ffffff800850f6c0>] pstate: 80400145
     sp : ffffffc0ed627cd0
     x29: ffffffc0ed627cd0 x28: 0000000000000000
     x27: ffffff8009e05000 x26: ffffffc0d382cfa0
     x25: 0000000000000000 x24: ffffff800a012f08
     x23: 0000000000000000 x22: ffffffc0703fbc88
     x21: 6b6b6b6b6b6b6b6b x20: 6b6b6b6b6b6b6b93
     x19: 0000000000000000 x18: 0000000000000001
     x17: 00e80000f80d6f53 x16: 0000000000000001
     x15: 0000007f7d826fff x14: 00000000000000a0
     x13: 0000000000000000 x12: 0000000000000109
     x11: 0000000000000000 x10: 0000000000000000
     x9 : ffffffc0ed624000 x8 : ffffffc0ed611580
     x7 : 0000000000000000 x6 : ffffff800a42e000
     x5 : 00000000000003fc x4 : 0000000003bd1201
     x3 : 0000000000000001 x2 : 0000000000000001
     x1 : ffffff800851004c x0 : 6b6b6b6b6b6b6b93
    
    Signed-off-by: Sahara <keun-o.park@darkmatter.ae>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 294975841483c08e84572713f348cd51b8408021
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Feb 5 22:18:11 2018 -0500

    tracing: Fix parsing of globs with a wildcard at the beginning
    
    commit 07234021410bbc27b7c86c18de98616c29fbe667 upstream.
    
    Al Viro reported:
    
        For substring - sure, but what about something like "*a*b" and "a*b"?
        AFAICS, filter_parse_regex() ends up with identical results in both
        cases - MATCH_GLOB and *search = "a*b".  And no way for the caller
        to tell one from another.
    
    Testing this with the following:
    
     # cd /sys/kernel/tracing
     # echo '*raw*lock' > set_ftrace_filter
     bash: echo: write error: Invalid argument
    
    With this patch:
    
     # echo '*raw*lock' > set_ftrace_filter
     # cat set_ftrace_filter
    _raw_read_trylock
    _raw_write_trylock
    _raw_read_unlock
    _raw_spin_unlock
    _raw_write_unlock
    _raw_spin_trylock
    _raw_spin_lock
    _raw_write_lock
    _raw_read_lock
    
    Al recommended not setting the search buffer to skip the first '*' unless we
    know we are not using MATCH_GLOB. This implements his suggested logic.
    
    Link: http://lkml.kernel.org/r/20180127170748.GF13338@ZenIV.linux.org.uk
    
    Cc: stable@vger.kernel.org
    Fixes: 60f1d5e3bac44 ("ftrace: Support full glob matching")
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reported-by: Al Viro <viro@ZenIV.linux.org.uk>
    Suggsted-by: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 95f92d0a0ca9dd0f4a92e9eb02b2b7b3d257d46f
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Feb 5 22:18:11 2018 -0500

    tracing: Fix parsing of globs with a wildcard at the beginning
    
    commit 07234021410bbc27b7c86c18de98616c29fbe667 upstream.
    
    Al Viro reported:
    
        For substring - sure, but what about something like "*a*b" and "a*b"?
        AFAICS, filter_parse_regex() ends up with identical results in both
        cases - MATCH_GLOB and *search = "a*b".  And no way for the caller
        to tell one from another.
    
    Testing this with the following:
    
     # cd /sys/kernel/tracing
     # echo '*raw*lock' > set_ftrace_filter
     bash: echo: write error: Invalid argument
    
    With this patch:
    
     # echo '*raw*lock' > set_ftrace_filter
     # cat set_ftrace_filter
    _raw_read_trylock
    _raw_write_trylock
    _raw_read_unlock
    _raw_spin_unlock
    _raw_write_unlock
    _raw_spin_trylock
    _raw_spin_lock
    _raw_write_lock
    _raw_read_lock
    
    Al recommended not setting the search buffer to skip the first '*' unless we
    know we are not using MATCH_GLOB. This implements his suggested logic.
    
    Link: http://lkml.kernel.org/r/20180127170748.GF13338@ZenIV.linux.org.uk
    
    Cc: stable@vger.kernel.org
    Fixes: 60f1d5e3bac44 ("ftrace: Support full glob matching")
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reported-by: Al Viro <viro@ZenIV.linux.org.uk>
    Suggsted-by: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 07234021410bbc27b7c86c18de98616c29fbe667
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Feb 5 22:18:11 2018 -0500

    tracing: Fix parsing of globs with a wildcard at the beginning
    
    Al Viro reported:
    
        For substring - sure, but what about something like "*a*b" and "a*b"?
        AFAICS, filter_parse_regex() ends up with identical results in both
        cases - MATCH_GLOB and *search = "a*b".  And no way for the caller
        to tell one from another.
    
    Testing this with the following:
    
     # cd /sys/kernel/tracing
     # echo '*raw*lock' > set_ftrace_filter
     bash: echo: write error: Invalid argument
    
    With this patch:
    
     # echo '*raw*lock' > set_ftrace_filter
     # cat set_ftrace_filter
    _raw_read_trylock
    _raw_write_trylock
    _raw_read_unlock
    _raw_spin_unlock
    _raw_write_unlock
    _raw_spin_trylock
    _raw_spin_lock
    _raw_write_lock
    _raw_read_lock
    
    Al recommended not setting the search buffer to skip the first '*' unless we
    know we are not using MATCH_GLOB. This implements his suggested logic.
    
    Link: http://lkml.kernel.org/r/20180127170748.GF13338@ZenIV.linux.org.uk
    
    Cc: stable@vger.kernel.org
    Fixes: 60f1d5e3bac44 ("ftrace: Support full glob matching")
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reported-by: Al Viro <viro@ZenIV.linux.org.uk>
    Suggsted-by: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

commit 3b454ad35043dfbd3b5d2bb92b0991d6342afb44
Author: Yang Shi <yang.s@alibaba-inc.com>
Date:   Wed Jan 31 16:18:28 2018 -0800

    mm: thp: use down_read_trylock() in khugepaged to avoid long block
    
    In the current design, khugepaged needs to acquire mmap_sem before
    scanning an mm.  But in some corner cases, khugepaged may scan a process
    which is modifying its memory mapping, so khugepaged blocks in
    uninterruptible state.  But the process might hold the mmap_sem for a
    long time when modifying a huge memory space and it may trigger the
    below khugepaged hung issue:
    
      INFO: task khugepaged:270 blocked for more than 120 seconds.
      Tainted: G E 4.9.65-006.ali3000.alios7.x86_64 #1
      "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
      khugepaged D 0 270 2 0x00000000 
      ffff883f3deae4c0 0000000000000000 ffff883f610596c0 ffff883f7d359440
      ffff883f63818000 ffffc90019adfc78 ffffffff817079a5 d67e5aa8c1860a64
      0000000000000246 ffff883f7d359440 ffffc90019adfc88 ffff883f610596c0
      Call Trace:
        schedule+0x36/0x80
        rwsem_down_read_failed+0xf0/0x150
        call_rwsem_down_read_failed+0x18/0x30
        down_read+0x20/0x40
        khugepaged+0x476/0x11d0
        kthread+0xe6/0x100
        ret_from_fork+0x25/0x30
    
    So it sounds pointless to just block khugepaged waiting for the
    semaphore so replace down_read() with down_read_trylock() to move to
    scan the next mm quickly instead of just blocking on the semaphore so
    that other processes can get more chances to install THP.  Then
    khugepaged can come back to scan the skipped mm when it has finished the
    current round full_scan.
    
    And it appears that the change can improve khugepaged efficiency a
    little bit.
    
    Below is the test result when running LTP on a 24 cores 4GB memory 2
    nodes NUMA VM:
    
                                        pristine          w/ trylock
      full_scan                         197               187
      pages_collapsed                   21                26
      thp_fault_alloc                   40818             44466
      thp_fault_fallback                18413             16679
      thp_collapse_alloc                21                150
      thp_collapse_alloc_failed         14                16
      thp_file_alloc                    369               369
    
    [akpm@linux-foundation.org: coding-style fixes]
    [akpm@linux-foundation.org: tweak comment]
    [arnd@arndb.de: avoid uninitialized variable use]
      Link: http://lkml.kernel.org/r/20171215125129.2948634-1-arnd@arndb.de
    Link: http://lkml.kernel.org/r/1513281203-54878-1-git-send-email-yang.s@alibaba-inc.com
    Signed-off-by: Yang Shi <yang.s@alibaba-inc.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit a9d572c7550044d5b217b5287d99a2e6d34b97b0
Author: Sheng Yong <shengyong1@huawei.com>
Date:   Wed Jan 17 12:11:31 2018 +0800

    f2fs: avoid hungtask when GC encrypted block if io_bits is set
    
    When io_bits is set, GCing encrypted block may hit the following hungtask.
    Since io_bits requires aligned block address, f2fs_submit_page_write may
    return -EAGAIN if new_blkaddr does not satisify io_bits alignment. As a
    result, the encrypted page will never be writtenback.
    
    This patch makes move_data_block aware the EAGAIN error and cancel the
    writeback.
    
    [  246.751371] INFO: task kworker/u4:4:797 blocked for more than 90 seconds.
    [  246.752423]       Not tainted 4.15.0-rc4+ #11
    [  246.754176] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    [  246.755336] kworker/u4:4    D25448   797      2 0x80000000
    [  246.755597] Workqueue: writeback wb_workfn (flush-7:0)
    [  246.755616] Call Trace:
    [  246.755695]  ? __schedule+0x322/0xa90
    [  246.755761]  ? blk_init_request_from_bio+0x120/0x120
    [  246.755773]  ? pci_mmcfg_check_reserved+0xb0/0xb0
    [  246.755801]  ? __radix_tree_create+0x19e/0x200
    [  246.755813]  ? delete_node+0x136/0x370
    [  246.755838]  schedule+0x43/0xc0
    [  246.755904]  io_schedule+0x17/0x40
    [  246.755939]  wait_on_page_bit_common+0x17b/0x240
    [  246.755950]  ? wake_page_function+0xa0/0xa0
    [  246.755961]  ? add_to_page_cache_lru+0x160/0x160
    [  246.755972]  ? page_cache_tree_insert+0x170/0x170
    [  246.755983]  ? __lru_cache_add+0x96/0xb0
    [  246.756086]  __filemap_fdatawait_range+0x14f/0x1c0
    [  246.756097]  ? wait_on_page_bit_common+0x240/0x240
    [  246.756120]  ? __wake_up_locked_key_bookmark+0x20/0x20
    [  246.756167]  ? wait_on_all_pages_writeback+0xc9/0x100
    [  246.756179]  ? __remove_ino_entry+0x120/0x120
    [  246.756192]  ? wait_woken+0x100/0x100
    [  246.756204]  filemap_fdatawait_range+0x9/0x20
    [  246.756216]  write_checkpoint+0x18a1/0x1f00
    [  246.756254]  ? blk_get_request+0x10/0x10
    [  246.756265]  ? cpumask_next_and+0x43/0x60
    [  246.756279]  ? f2fs_sync_inode_meta+0x160/0x160
    [  246.756289]  ? remove_element.isra.4+0xa0/0xa0
    [  246.756300]  ? __put_compound_page+0x40/0x40
    [  246.756310]  ? f2fs_sync_fs+0xec/0x1c0
    [  246.756320]  ? f2fs_sync_fs+0x120/0x1c0
    [  246.756329]  f2fs_sync_fs+0x120/0x1c0
    [  246.756357]  ? trace_event_raw_event_f2fs__page+0x260/0x260
    [  246.756393]  ? ata_build_rw_tf+0x173/0x410
    [  246.756397]  f2fs_balance_fs_bg+0x198/0x390
    [  246.756405]  ? drop_inmem_page+0x230/0x230
    [  246.756415]  ? ahci_qc_prep+0x1bb/0x2e0
    [  246.756418]  ? ahci_qc_issue+0x1df/0x290
    [  246.756422]  ? __accumulate_pelt_segments+0x42/0xd0
    [  246.756426]  ? f2fs_write_node_pages+0xd1/0x380
    [  246.756429]  f2fs_write_node_pages+0xd1/0x380
    [  246.756437]  ? sync_node_pages+0x8f0/0x8f0
    [  246.756440]  ? update_curr+0x53/0x220
    [  246.756444]  ? __accumulate_pelt_segments+0xa2/0xd0
    [  246.756448]  ? __update_load_avg_se.isra.39+0x349/0x360
    [  246.756452]  ? do_writepages+0x2a/0xa0
    [  246.756456]  do_writepages+0x2a/0xa0
    [  246.756460]  __writeback_single_inode+0x70/0x490
    [  246.756463]  ? check_preempt_wakeup+0x199/0x310
    [  246.756467]  writeback_sb_inodes+0x2a2/0x660
    [  246.756471]  ? is_empty_dir_inode+0x40/0x40
    [  246.756474]  ? __writeback_single_inode+0x490/0x490
    [  246.756477]  ? string+0xbf/0xf0
    [  246.756480]  ? down_read_trylock+0x35/0x60
    [  246.756484]  __writeback_inodes_wb+0x9f/0xf0
    [  246.756488]  wb_writeback+0x41d/0x4b0
    [  246.756492]  ? writeback_inodes_wb.constprop.55+0x150/0x150
    [  246.756498]  ? set_worker_desc+0xf7/0x130
    [  246.756502]  ? current_is_workqueue_rescuer+0x60/0x60
    [  246.756511]  ? _find_next_bit+0x2c/0xa0
    [  246.756514]  ? wb_workfn+0x400/0x5d0
    [  246.756518]  wb_workfn+0x400/0x5d0
    [  246.756521]  ? finish_task_switch+0xdf/0x2a0
    [  246.756525]  ? inode_wait_for_writeback+0x30/0x30
    [  246.756529]  process_one_work+0x3a7/0x6f0
    [  246.756533]  worker_thread+0x82/0x750
    [  246.756537]  kthread+0x16f/0x1c0
    [  246.756541]  ? trace_event_raw_event_workqueue_work+0x110/0x110
    [  246.756544]  ? kthread_create_worker_on_cpu+0xb0/0xb0
    [  246.756548]  ret_from_fork+0x1f/0x30
    
    Signed-off-by: Sheng Yong <shengyong1@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

commit 2b022ab7542df60021ab57854b3faaaf42552eaf
Author: Sahara <keun-o.park@darkmatter.ae>
Date:   Wed Dec 13 09:10:48 2017 +0400

    pty: cancel pty slave port buf's work in tty_release
    
    In case that CONFIG_SLUB_DEBUG is on and pty is used, races between
    release_one_tty and flush_to_ldisc work threads may happen and lead
    to use-after-free condition on tty->link->port. Because SLUB_DEBUG
    is turned on, freed tty->link->port is filled with POISON_FREE value.
    So far without SLUB_DEBUG, port was filled with zero and flush_to_ldisc
    could return without a problem by checking if tty is NULL.
    
    CPU 0                                 CPU 1
    -----                                 -----
    release_tty                           pty_write
       cancel_work_sync(tty)                 to = tty->link
       tty_kref_put(tty->link)               tty_schedule_flip(to->port)
          << workqueue >>                 ...
          release_one_tty                 ...
             pty_cleanup                  ...
                kfree(tty->link->port)       << workqueue >>
                                             flush_to_ldisc
                                                tty = READ_ONCE(port->itty)
                                                tty is 0x6b6b6b6b6b6b6b6b
                                                !!PANIC!! access tty->ldisc
    
     Unable to handle kernel paging request at virtual address 6b6b6b6b6b6b6b93
     pgd = ffffffc0eb1c3000
     [6b6b6b6b6b6b6b93] *pgd=0000000000000000, *pud=0000000000000000
     ------------[ cut here ]------------
     Kernel BUG at ffffff800851154c [verbose debug info unavailable]
     Internal error: Oops - BUG: 96000004 [#1] PREEMPT SMP
     CPU: 3 PID: 265 Comm: kworker/u8:9 Tainted: G        W 3.18.31-g0a58eeb #1
     Hardware name: Qualcomm Technologies, Inc. MSM 8996pro v1.1 + PMI8996 Carbide (DT)
     Workqueue: events_unbound flush_to_ldisc
     task: ffffffc0ed610ec0 ti: ffffffc0ed624000 task.ti: ffffffc0ed624000
     PC is at ldsem_down_read_trylock+0x0/0x4c
     LR is at tty_ldisc_ref+0x24/0x4c
     pc : [<ffffff800851154c>] lr : [<ffffff800850f6c0>] pstate: 80400145
     sp : ffffffc0ed627cd0
     x29: ffffffc0ed627cd0 x28: 0000000000000000
     x27: ffffff8009e05000 x26: ffffffc0d382cfa0
     x25: 0000000000000000 x24: ffffff800a012f08
     x23: 0000000000000000 x22: ffffffc0703fbc88
     x21: 6b6b6b6b6b6b6b6b x20: 6b6b6b6b6b6b6b93
     x19: 0000000000000000 x18: 0000000000000001
     x17: 00e80000f80d6f53 x16: 0000000000000001
     x15: 0000007f7d826fff x14: 00000000000000a0
     x13: 0000000000000000 x12: 0000000000000109
     x11: 0000000000000000 x10: 0000000000000000
     x9 : ffffffc0ed624000 x8 : ffffffc0ed611580
     x7 : 0000000000000000 x6 : ffffff800a42e000
     x5 : 00000000000003fc x4 : 0000000003bd1201
     x3 : 0000000000000001 x2 : 0000000000000001
     x1 : ffffff800851004c x0 : 6b6b6b6b6b6b6b93
    
    Signed-off-by: Sahara <keun-o.park@darkmatter.ae>
    
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7084a918af5c4e934e2993c24b479673c54b31a8
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Mon May 15 15:13:16 2017 +0200

    sched/numa: Use down_read_trylock() for the mmap_sem
    
    commit 8655d5497735b288f8a9b458bd22e7d1bf95bb61 upstream.
    
    A customer has reported a soft-lockup when running an intensive
    memory stress test, where the trace on multiple CPU's looks like this:
    
     RIP: 0010:[<ffffffff810c53fe>]
      [<ffffffff810c53fe>] native_queued_spin_lock_slowpath+0x10e/0x190
    ...
     Call Trace:
      [<ffffffff81182d07>] queued_spin_lock_slowpath+0x7/0xa
      [<ffffffff811bc331>] change_protection_range+0x3b1/0x930
      [<ffffffff811d4be8>] change_prot_numa+0x18/0x30
      [<ffffffff810adefe>] task_numa_work+0x1fe/0x310
      [<ffffffff81098322>] task_work_run+0x72/0x90
    
    Further investigation showed that the lock contention here is pmd_lock().
    
    The task_numa_work() function makes sure that only one thread is let to perform
    the work in a single scan period (via cmpxchg), but if there's a thread with
    mmap_sem locked for writing for several periods, multiple threads in
    task_numa_work() can build up a convoy waiting for mmap_sem for read and then
    all get unblocked at once.
    
    This patch changes the down_read() to the trylock version, which prevents the
    build up. For a workload experiencing mmap_sem contention, it's probably better
    to postpone the NUMA balancing work anyway. This seems to have fixed the soft
    lockups involving pmd_lock(), which is in line with the convoy theory.
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170515131316.21909-1-vbabka@suse.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1096c35aa821cc4789a64232a0e210bb87a0e5e8
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Sun Jun 18 23:23:02 2017 +0900

    perf ftrace: Add -D option for depth filter
    
    The -D/--graph-depth option is to set max graph depth.  The following
    example traces max 2-depth of page fault handler.
    
      $ sudo perf ftrace -G __do_page_fault -D 2 -- hello
       ...
       0)               |  __do_page_fault() {
       0)   0.063 us    |    down_read_trylock();
       0)   0.251 us    |    find_vma();
       0)   5.374 us    |    handle_mm_fault();
       0)   0.054 us    |    up_read();
       0)   7.463 us    |  }
       ...
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: kernel-team@lge.com
    Link: http://lkml.kernel.org/r/20170618142302.25390-4-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

commit 78b83e8b12b4467540ca501c7c019e9d46051957
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Sun Jun 18 23:23:01 2017 +0900

    perf ftrace: Add option for function filtering
    
    The -T/--trace-funcs and -N/--notrace-funcs options are to specify
    functions to enable/disable tracing dynamically.
    
    The -G/--graph-funcs and -g/--nograph-funcs options are to set filters
    for function graph tracer.
    
    For example, to trace fault handling functions only:
    
      $ sudo perf ftrace -T *fault hello
       0)               |  __do_page_fault() {
       0)               |    handle_mm_fault() {
       0)   2.117 us    |      __handle_mm_fault();
       0)   3.627 us    |    }
       0)   7.811 us    |  }
       0)               |  __do_page_fault() {
       0)               |    handle_mm_fault() {
       0)   2.014 us    |      __handle_mm_fault();
       0)   2.424 us    |    }
       0)   2.951 us    |  }
       ...
    
    To trace all functions executed in __do_page_fault:
    
      $ sudo perf ftrace -G __do_page_fault hello
       2)               |  __do_page_fault() {
       3)   0.060 us    |    down_read_trylock();
       3)               |    find_vma() {
       3)   0.075 us    |      vmacache_find();
       3)   0.053 us    |      vmacache_update();
       3)   1.246 us    |    }
       3)               |    handle_mm_fault() {
       3)   0.063 us    |      __rcu_read_lock();
       3)   0.056 us    |      mem_cgroup_from_task();
       3)   0.057 us    |      __rcu_read_unlock();
       3)               |      __handle_mm_fault() {
       3)               |        filemap_map_pages() {
       3)   0.058 us    |          __rcu_read_lock();
       3)               |          alloc_set_pte() {
       ...
    
    But don't want to show details in handle_mm_fault:
    
      $ sudo perf ftrace -G __do_page_fault -g handle_mm_fault hello
       3)               |  __do_page_fault() {
       3)   0.049 us    |    down_read_trylock();
       3)               |    find_vma() {
       3)   0.048 us    |      vmacache_find();
       3)   0.041 us    |      vmacache_update();
       3)   0.680 us    |    }
       3)   0.036 us    |    up_read();
       3)   4.547 us    |  } /* __do_page_fault */
       ...
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: kernel-team@lge.com
    Link: http://lkml.kernel.org/r/20170618142302.25390-3-namhyung@kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

commit 8655d5497735b288f8a9b458bd22e7d1bf95bb61
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Mon May 15 15:13:16 2017 +0200

    sched/numa: Use down_read_trylock() for the mmap_sem
    
    A customer has reported a soft-lockup when running an intensive
    memory stress test, where the trace on multiple CPU's looks like this:
    
     RIP: 0010:[<ffffffff810c53fe>]
      [<ffffffff810c53fe>] native_queued_spin_lock_slowpath+0x10e/0x190
    ...
     Call Trace:
      [<ffffffff81182d07>] queued_spin_lock_slowpath+0x7/0xa
      [<ffffffff811bc331>] change_protection_range+0x3b1/0x930
      [<ffffffff811d4be8>] change_prot_numa+0x18/0x30
      [<ffffffff810adefe>] task_numa_work+0x1fe/0x310
      [<ffffffff81098322>] task_work_run+0x72/0x90
    
    Further investigation showed that the lock contention here is pmd_lock().
    
    The task_numa_work() function makes sure that only one thread is let to perform
    the work in a single scan period (via cmpxchg), but if there's a thread with
    mmap_sem locked for writing for several periods, multiple threads in
    task_numa_work() can build up a convoy waiting for mmap_sem for read and then
    all get unblocked at once.
    
    This patch changes the down_read() to the trylock version, which prevents the
    build up. For a workload experiencing mmap_sem contention, it's probably better
    to postpone the NUMA balancing work anyway. This seems to have fixed the soft
    lockups involving pmd_lock(), which is in line with the convoy theory.
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170515131316.21909-1-vbabka@suse.cz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e22834f0248d0fa841ead6436d6c19f65539dc9c
Author: Dmitry Monakhov <dmonakhov@openvz.org>
Date:   Mon Sep 5 23:38:36 2016 -0400

    ext4: improve ext4lazyinit scalability
    
    ext4lazyinit is a global thread. This thread performs itable
    initalization under li_list_mtx mutex.
    
    It basically does the following:
    ext4_lazyinit_thread
      ->mutex_lock(&eli->li_list_mtx);
      ->ext4_run_li_request(elr)
        ->ext4_init_inode_table-> Do a lot of IO if the list is large
    
    And when new mount/umount arrive they have to block on ->li_list_mtx
    because  lazy_thread holds it during full walk procedure.
    ext4_fill_super
     ->ext4_register_li_request
       ->mutex_lock(&ext4_li_info->li_list_mtx);
       ->list_add(&elr->lr_request, &ext4_li_info >li_request_list);
    In my case mount takes 40minutes on server with 36 * 4Tb HDD.
    Common user may face this in case of very slow dev ( /dev/mmcblkXXX)
    Even more. If one of filesystems was frozen lazyinit_thread will simply
    block on sb_start_write() so other mount/umount will be stuck forever.
    
    This patch changes logic like follows:
    - grab ->s_umount read sem before processing new li_request.
      After that it is safe to drop li_list_mtx because all callers of
      li_remove_request are holding ->s_umount for write.
    - li_thread skips frozen SB's
    
    Locking order:
    Mh KOrder is asserted by umount path like follows: s_umount ->li_list_mtx so
    the only way to to grab ->s_mount inside li_thread is via down_read_trylock
    
    xfstests:ext4/023
    #PSBM-49658
    
    Signed-off-by: Dmitry Monakhov <dmonakhov@openvz.org>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

commit e5e3f4c4f0e95ecbad2f8d2f4f6a29bb8a90226b
Author: Michal Hocko <mhocko@suse.com>
Date:   Tue Jul 26 15:24:50 2016 -0700

    mm, oom_reaper: make sure that mmput_async is called only when memory was reaped
    
    Tetsuo is worried that mmput_async might still lead to a premature new
    oom victim selection due to the following race:
    
    __oom_reap_task                         exit_mm
      find_lock_task_mm
      atomic_inc(mm->mm_users) # = 2
      task_unlock
                                              task_lock
                                              task->mm = NULL
                                              up_read(&mm->mmap_sem)
                    < somebody write locks mmap_sem >
                                              task_unlock
                                              mmput
                                                atomic_dec_and_test # = 1
                                              exit_oom_victim
      down_read_trylock # failed - no reclaim
      mmput_async # Takes unpredictable amount of time
                    < new OOM situation >
    
    the final __mmput will be executed in the delayed context which might
    happen far in the future.  Such a race is highly unlikely because the
    write holder of mmap_sem would have to be an external task (all direct
    holders are already killed or exiting) and it usually have to pin
    mm_users in order to do anything reasonable.
    
    We can, however, make sure that the mmput_async is only called when we
    do not back off and reap some memory.  That would reduce the impact of
    the delayed __mmput because the real content would be already freed.
    Pin mm_count to keep it alive after we drop task_lock and before we try
    to get mmap_sem.  If the mmap_sem succeeds we can try to grab mm_users
    reference and then go on with unmapping the address space.
    
    It is not clear whether this race is possible at all but it is better to
    be more robust and do not pin mm_users unless we are sure we are
    actually doing some real work during __oom_reap_task.
    
    Link: http://lkml.kernel.org/r/1465306987-30297-1-git-send-email-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b094f89ca42fbb8ce40174d5f85ca8430e499da6
Author: Jens Axboe <axboe@fb.com>
Date:   Fri Nov 20 20:29:45 2015 -0700

    blk-mq: fix calling unplug callbacks with preempt disabled
    
    Liu reported that running certain parts of xfstests threw the
    following error:
    
    BUG: sleeping function called from invalid context at mm/page_alloc.c:3190
    in_atomic(): 1, irqs_disabled(): 0, pid: 6, name: kworker/u16:0
    3 locks held by kworker/u16:0/6:
     #0:  ("writeback"){++++.+}, at: [<ffffffff8107f083>] process_one_work+0x173/0x730
     #1:  ((&(&wb->dwork)->work)){+.+.+.}, at: [<ffffffff8107f083>] process_one_work+0x173/0x730
     #2:  (&type->s_umount_key#44){+++++.}, at: [<ffffffff811e6805>] trylock_super+0x25/0x60
    CPU: 5 PID: 6 Comm: kworker/u16:0 Tainted: G           OE   4.3.0+ #3
    Hardware name: Red Hat KVM, BIOS Bochs 01/01/2011
    Workqueue: writeback wb_workfn (flush-btrfs-108)
     ffffffff81a3abab ffff88042e282ba8 ffffffff8130191b ffffffff81a3abab
     0000000000000c76 ffff88042e282ba8 ffff88042e27c180 ffff88042e282bd8
     ffffffff8108ed95 ffff880400000004 0000000000000000 0000000000000c76
    Call Trace:
     [<ffffffff8130191b>] dump_stack+0x4f/0x74
     [<ffffffff8108ed95>] ___might_sleep+0x185/0x240
     [<ffffffff8108eea2>] __might_sleep+0x52/0x90
     [<ffffffff811817e8>] __alloc_pages_nodemask+0x268/0x410
     [<ffffffff8109a43c>] ? sched_clock_local+0x1c/0x90
     [<ffffffff8109a6d1>] ? local_clock+0x21/0x40
     [<ffffffff810b9eb0>] ? __lock_release+0x420/0x510
     [<ffffffff810b534c>] ? __lock_acquired+0x16c/0x3c0
     [<ffffffff811ca265>] alloc_pages_current+0xc5/0x210
     [<ffffffffa0577105>] ? rbio_is_full+0x55/0x70 [btrfs]
     [<ffffffff810b7ed8>] ? mark_held_locks+0x78/0xa0
     [<ffffffff81666d50>] ? _raw_spin_unlock_irqrestore+0x40/0x60
     [<ffffffffa0578c0a>] full_stripe_write+0x5a/0xc0 [btrfs]
     [<ffffffffa0578ca9>] __raid56_parity_write+0x39/0x60 [btrfs]
     [<ffffffffa0578deb>] run_plug+0x11b/0x140 [btrfs]
     [<ffffffffa0578e33>] btrfs_raid_unplug+0x23/0x70 [btrfs]
     [<ffffffff812d36c2>] blk_flush_plug_list+0x82/0x1f0
     [<ffffffff812e0349>] blk_sq_make_request+0x1f9/0x740
     [<ffffffff812ceba2>] ? generic_make_request_checks+0x222/0x7c0
     [<ffffffff812cf264>] ? blk_queue_enter+0x124/0x310
     [<ffffffff812cf1d2>] ? blk_queue_enter+0x92/0x310
     [<ffffffff812d0ae2>] generic_make_request+0x172/0x2c0
     [<ffffffff812d0ad4>] ? generic_make_request+0x164/0x2c0
     [<ffffffff812d0ca0>] submit_bio+0x70/0x140
     [<ffffffffa0577b29>] ? rbio_add_io_page+0x99/0x150 [btrfs]
     [<ffffffffa0578a89>] finish_rmw+0x4d9/0x600 [btrfs]
     [<ffffffffa0578c4c>] full_stripe_write+0x9c/0xc0 [btrfs]
     [<ffffffffa057ab7f>] raid56_parity_write+0xef/0x160 [btrfs]
     [<ffffffffa052bd83>] btrfs_map_bio+0xe3/0x2d0 [btrfs]
     [<ffffffffa04fbd6d>] btrfs_submit_bio_hook+0x8d/0x1d0 [btrfs]
     [<ffffffffa05173c4>] submit_one_bio+0x74/0xb0 [btrfs]
     [<ffffffffa0517f55>] submit_extent_page+0xe5/0x1c0 [btrfs]
     [<ffffffffa0519b18>] __extent_writepage_io+0x408/0x4c0 [btrfs]
     [<ffffffffa05179c0>] ? alloc_dummy_extent_buffer+0x140/0x140 [btrfs]
     [<ffffffffa051dc88>] __extent_writepage+0x218/0x3a0 [btrfs]
     [<ffffffff810b7ed8>] ? mark_held_locks+0x78/0xa0
     [<ffffffffa051e2c9>] extent_write_cache_pages.clone.0+0x2f9/0x400 [btrfs]
     [<ffffffffa051e422>] extent_writepages+0x52/0x70 [btrfs]
     [<ffffffffa05001f0>] ? btrfs_set_inode_index+0x70/0x70 [btrfs]
     [<ffffffffa04fcc17>] btrfs_writepages+0x27/0x30 [btrfs]
     [<ffffffff81184df3>] do_writepages+0x23/0x40
     [<ffffffff81212229>] __writeback_single_inode+0x89/0x4d0
     [<ffffffff81212a60>] ? writeback_sb_inodes+0x260/0x480
     [<ffffffff81212a60>] ? writeback_sb_inodes+0x260/0x480
     [<ffffffff8121295f>] ? writeback_sb_inodes+0x15f/0x480
     [<ffffffff81212ad2>] writeback_sb_inodes+0x2d2/0x480
     [<ffffffff810b1397>] ? down_read_trylock+0x57/0x60
     [<ffffffff811e6805>] ? trylock_super+0x25/0x60
     [<ffffffff810d629f>] ? rcu_read_lock_sched_held+0x4f/0x90
     [<ffffffff81212d0c>] __writeback_inodes_wb+0x8c/0xc0
     [<ffffffff812130b5>] wb_writeback+0x2b5/0x500
     [<ffffffff810b7ed8>] ? mark_held_locks+0x78/0xa0
     [<ffffffff810660a8>] ? __local_bh_enable_ip+0x68/0xc0
     [<ffffffff81213362>] ? wb_do_writeback+0x62/0x310
     [<ffffffff812133c1>] wb_do_writeback+0xc1/0x310
     [<ffffffff8107c3d9>] ? set_worker_desc+0x79/0x90
     [<ffffffff81213842>] wb_workfn+0x92/0x330
     [<ffffffff8107f133>] process_one_work+0x223/0x730
     [<ffffffff8107f083>] ? process_one_work+0x173/0x730
     [<ffffffff8108035f>] ? worker_thread+0x18f/0x430
     [<ffffffff810802ed>] worker_thread+0x11d/0x430
     [<ffffffff810801d0>] ? maybe_create_worker+0xf0/0xf0
     [<ffffffff810801d0>] ? maybe_create_worker+0xf0/0xf0
     [<ffffffff810858df>] kthread+0xef/0x110
     [<ffffffff8108f74e>] ? schedule_tail+0x1e/0xd0
     [<ffffffff810857f0>] ? __init_kthread_worker+0x70/0x70
     [<ffffffff816673bf>] ret_from_fork+0x3f/0x70
     [<ffffffff810857f0>] ? __init_kthread_worker+0x70/0x70
    
    The issue is that we've got the software context pinned while
    calling blk_flush_plug_list(), which flushes callbacks that
    are allowed to sleep. btrfs and raid has such callbacks.
    
    Flip the checks around a bit, so we can enable preempt a bit
    earlier and flush plugs without having preempt disabled.
    
    This only affects blk-mq driven devices, and only those that
    register a single queue.
    
    Reported-by: Liu Bo <bo.li.liu@oracle.com>
    Tested-by: Liu Bo <bo.li.liu@oracle.com>
    Cc: stable@kernel.org
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit b87537d9e2feb30f6a962f27eb32768682698d3b
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Nov 5 18:49:33 2015 -0800

    mm: rmap use pte lock not mmap_sem to set PageMlocked
    
    KernelThreadSanitizer (ktsan) has shown that the down_read_trylock() of
    mmap_sem in try_to_unmap_one() (when going to set PageMlocked on a page
    found mapped in a VM_LOCKED vma) is ineffective against races with
    exit_mmap()'s munlock_vma_pages_all(), because mmap_sem is not held when
    tearing down an mm.
    
    But that's okay, those races are benign; and although we've believed for
    years in that ugly down_read_trylock(), it's unsuitable for the job, and
    frustrates the good intention of setting PageMlocked when it fails.
    
    It just doesn't matter if here we read vm_flags an instant before or after
    a racing mlock() or munlock() or exit_mmap() sets or clears VM_LOCKED: the
    syscalls (or exit) work their way up the address space (taking pt locks
    after updating vm_flags) to establish the final state.
    
    We do still need to be careful never to mark a page Mlocked (hence
    unevictable) by any race that will not be corrected shortly after.  The
    page lock protects from many of the races, but not all (a page is not
    necessarily locked when it's unmapped).  But the pte lock we just dropped
    is good to cover the rest (and serializes even with
    munlock_vma_pages_all(), so no special barriers required): now hold on to
    the pte lock while calling mlock_vma_page().  Is that lock ordering safe?
    Yes, that's how follow_page_pte() calls it, and how page_remove_rmap()
    calls the complementary clear_page_mlock().
    
    This fixes the following case (though not a case which anyone has
    complained of), which mmap_sem did not: truncation's preliminary
    unmap_mapping_range() is supposed to remove even the anonymous COWs of
    filecache pages, and that might race with try_to_unmap_one() on a
    VM_LOCKED vma, so that mlock_vma_page() sets PageMlocked just after
    zap_pte_range() unmaps the page, causing "Bad page state (mlocked)" when
    freed.  The pte lock protects against this.
    
    You could say that it also protects against the more ordinary case, racing
    with the preliminary unmapping of a filecache page itself: but in our
    current tree, that's independently protected by i_mmap_rwsem; and that
    race would be why "Bad page state (mlocked)" was seen before commit
    48ec833b7851 ("Revert mm/memory.c: share the i_mmap_rwsem").
    
    Vlastimil Babka points out another race which this patch protects against.
     try_to_unmap_one() might reach its mlock_vma_page() TestSetPageMlocked a
    moment after munlock_vma_pages_all() did its Phase 1 TestClearPageMlocked:
    leaving PageMlocked and unevictable when it should be evictable.  mmap_sem
    is ineffective because exit_mmap() does not hold it; page lock ineffective
    because __munlock_pagevec() only takes it afterwards, in Phase 2; pte lock
    is effective because __munlock_pagevec_fill() takes it to get the page,
    after VM_LOCKED was cleared from vm_flags, so visible to try_to_unmap_one.
    
    Kirill Shutemov points out that if the compiler chooses to implement a
    "vma->vm_flags &= VM_WHATEVER" or "vma->vm_flags |= VM_WHATEVER" operation
    with an intermediate store of unrelated bits set, since I'm here foregoing
    its usual protection by mmap_sem, try_to_unmap_one() might catch sight of
    a spurious VM_LOCKED in vm_flags, and make the wrong decision.  This does
    not appear to be an immediate problem, but we may want to define vm_flags
    accessors in future, to guard against such a possibility.
    
    While we're here, make a related optimization in try_to_munmap_one(): if
    it's doing TTU_MUNLOCK, then there's no point at all in descending the
    page tables and getting the pt lock, unless the vma is VM_LOCKED.  Yes,
    that can change racily, but it can change racily even without the
    optimization: it's not critical.  Far better not to waste time here.
    
    Stopped short of separating try_to_munlock_one() from try_to_munmap_one()
    on this occasion, but that's probably the sensible next step - with a
    rename, given that try_to_munlock()'s business is to try to set Mlocked.
    
    Updated the unevictable-lru Documentation, to remove its reference to mmap
    semaphore, but found a few more updates needed in just that area.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Rik van Riel <riel@redhat.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit bf248ca1f5c7ba1e535ba4bd517a15a1ae965c69
Author: Daniel Stone <daniels@collabora.com>
Date:   Tue Nov 3 21:42:31 2015 +0000

    drm/i915: Fix locking around GuC firmware load
    
    The GuC firmware load requires struct_mutex to create a GEM object,
    but this collides badly with request_firmware. Move struct_mutex
    locking down into the loader itself, so we don't hold it across the
    entire load process, including request_firmware.
    
    [   20.451400] ======================================================
    [   20.451420] [ INFO: possible circular locking dependency detected ]
    [   20.451441] 4.3.0-rc5+ #1 Tainted: G        W
    [   20.451457] -------------------------------------------------------
    [   20.451477] plymouthd/371 is trying to acquire lock:
    [   20.451494]  (&dev->struct_mutex){+.+.+.}, at: [<ffffffffa0093c62>]
    drm_gem_mmap+0x112/0x290 [drm]
    [   20.451538]
                   but task is already holding lock:
    [   20.451557]  (&mm->mmap_sem){++++++}, at: [<ffffffff811fd9ac>]
    vm_mmap_pgoff+0x8c/0xf0
    [   20.451591]
                   which lock already depends on the new lock.
    
    [   20.451617]
                   the existing dependency chain (in reverse order) is:
    [   20.451640]
                   -> #3 (&mm->mmap_sem){++++++}:
    [   20.451661]        [<ffffffff8110644e>] lock_acquire+0xce/0x1c0
    [   20.451683]        [<ffffffff8120ec9a>] __might_fault+0x7a/0xa0
    [   20.451705]        [<ffffffff8127e34e>] filldir+0x9e/0x130
    [   20.451726]        [<ffffffff81295b86>] dcache_readdir+0x186/0x230
    [   20.451748]        [<ffffffff8127e117>] iterate_dir+0x97/0x130
    [   20.451769]        [<ffffffff8127e66a>] SyS_getdents+0x9a/0x130
    [   20.451790]        [<ffffffff8184f2f2>] entry_SYSCALL_64_fastpath+0x12/0x76
    [   20.451829]
                   -> #2 (&sb->s_type->i_mutex_key#2){+.+.+.}:
    [   20.451852]        [<ffffffff8110644e>] lock_acquire+0xce/0x1c0
    [   20.451872]        [<ffffffff8184b516>] mutex_lock_nested+0x86/0x400
    [   20.451893]        [<ffffffff81277790>] walk_component+0x1d0/0x2a0
    [   20.451914]        [<ffffffff812779f0>] link_path_walk+0x190/0x5a0
    [   20.451935]        [<ffffffff8127803b>] path_openat+0xab/0x1260
    [   20.451955]        [<ffffffff8127a651>] do_filp_open+0x91/0x100
    [   20.451975]        [<ffffffff81267e67>] file_open_name+0xf7/0x150
    [   20.451995]        [<ffffffff81267ef3>] filp_open+0x33/0x60
    [   20.452014]        [<ffffffff8157e1e7>] _request_firmware+0x277/0x880
    [   20.452038]        [<ffffffff8157e9e4>] request_firmware_work_func+0x34/0x80
    [   20.452060]        [<ffffffff810c7020>] process_one_work+0x230/0x680
    [   20.452082]        [<ffffffff810c74be>] worker_thread+0x4e/0x450
    [   20.452102]        [<ffffffff810ce511>] kthread+0x101/0x120
    [   20.452121]        [<ffffffff8184f66f>] ret_from_fork+0x3f/0x70
    [   20.452140]
                   -> #1 (umhelper_sem){++++.+}:
    [   20.452159]        [<ffffffff8110644e>] lock_acquire+0xce/0x1c0
    [   20.452178]        [<ffffffff8184c5c1>] down_read+0x51/0xa0
    [   20.452197]        [<ffffffff810c203b>]
    usermodehelper_read_trylock+0x5b/0x130
    [   20.452221]        [<ffffffff8157e147>] _request_firmware+0x1d7/0x880
    [   20.452242]        [<ffffffff8157e821>] request_firmware+0x31/0x50
    [   20.452262]        [<ffffffffa01b54a4>]
    intel_guc_ucode_init+0xf4/0x400 [i915]
    [   20.452305]        [<ffffffffa0213913>] i915_driver_load+0xd63/0x16e0 [i915]
    [   20.452343]        [<ffffffffa00987d9>] drm_dev_register+0xa9/0xc0 [drm]
    [   20.452369]        [<ffffffffa009ae3d>] drm_get_pci_dev+0x8d/0x1e0 [drm]
    [   20.452396]        [<ffffffffa01521e4>] i915_pci_probe+0x34/0x50 [i915]
    [   20.452421]        [<ffffffff81464675>] local_pci_probe+0x45/0xa0
    [   20.452443]        [<ffffffff81465a6d>] pci_device_probe+0xfd/0x140
    [   20.452464]        [<ffffffff8156a2e4>] driver_probe_device+0x224/0x480
    [   20.452486]        [<ffffffff8156a5c8>] __driver_attach+0x88/0x90
    [   20.452505]        [<ffffffff81567cf3>] bus_for_each_dev+0x73/0xc0
    [   20.452526]        [<ffffffff81569a7e>] driver_attach+0x1e/0x20
    [   20.452546]        [<ffffffff815695ae>] bus_add_driver+0x1ee/0x280
    [   20.452566]        [<ffffffff8156b100>] driver_register+0x60/0xe0
    [   20.453197]        [<ffffffff81464050>] __pci_register_driver+0x60/0x70
    [   20.453845]        [<ffffffffa009b070>] drm_pci_init+0xe0/0x110 [drm]
    [   20.454497]        [<ffffffffa027f092>] 0xffffffffa027f092
    [   20.455156]        [<ffffffff81002123>] do_one_initcall+0xb3/0x200
    [   20.455796]        [<ffffffff811d8c01>] do_init_module+0x5f/0x1e7
    [   20.456434]        [<ffffffff8114c4e6>] load_module+0x2126/0x27d0
    [   20.457071]        [<ffffffff8114cdf9>] SyS_finit_module+0xb9/0xf0
    [   20.457738]        [<ffffffff8184f2f2>] entry_SYSCALL_64_fastpath+0x12/0x76
    [   20.458370]
                   -> #0 (&dev->struct_mutex){+.+.+.}:
    [   20.459773]        [<ffffffff8110584f>] __lock_acquire+0x191f/0x1ba0
    [   20.460451]        [<ffffffff8110644e>] lock_acquire+0xce/0x1c0
    [   20.461074]        [<ffffffffa0093c88>] drm_gem_mmap+0x138/0x290 [drm]
    [   20.461693]        [<ffffffff8121a5ec>] mmap_region+0x3ec/0x670
    [   20.462298]        [<ffffffff8121abb2>] do_mmap+0x342/0x420
    [   20.462901]        [<ffffffff811fd9d2>] vm_mmap_pgoff+0xb2/0xf0
    [   20.463532]        [<ffffffff81218f62>] SyS_mmap_pgoff+0x1f2/0x290
    [   20.464118]        [<ffffffff8102187b>] SyS_mmap+0x1b/0x30
    [   20.464702]        [<ffffffff8184f2f2>] entry_SYSCALL_64_fastpath+0x12/0x76
    [   20.465289]
                   other info that might help us debug this:
    
    [   20.467179] Chain exists of:
                     &dev->struct_mutex --> &sb->s_type->i_mutex_key#2 -->
    &mm->mmap_sem
    
    [   20.468928]  Possible unsafe locking scenario:
    
    [   20.470161]        CPU0                    CPU1
    [   20.470745]        ----                    ----
    [   20.471325]   lock(&mm->mmap_sem);
    [   20.471902]                                lock(&sb->s_type->i_mutex_key#2);
    [   20.472538]                                lock(&mm->mmap_sem);
    [   20.473118]   lock(&dev->struct_mutex);
    [   20.473704]
                    *** DEADLOCK ***
    
    Signed-off-by: Daniel Stone <daniels@collabora.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

commit 7d9071a095023cd1db8fa18fa0d648dc1a5210e0
Merge: bd779669945e 397d425dc26d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 5 20:34:28 2015 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs updates from Al Viro:
     "In this one:
    
       - d_move fixes (Eric Biederman)
    
       - UFS fixes (me; locking is mostly sane now, a bunch of bugs in error
         handling ought to be fixed)
    
       - switch of sb_writers to percpu rwsem (Oleg Nesterov)
    
       - superblock scalability (Josef Bacik and Dave Chinner)
    
       - swapon(2) race fix (Hugh Dickins)"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (65 commits)
      vfs: Test for and handle paths that are unreachable from their mnt_root
      dcache: Reduce the scope of i_lock in d_splice_alias
      dcache: Handle escaped paths in prepend_path
      mm: fix potential data race in SyS_swapon
      inode: don't softlockup when evicting inodes
      inode: rename i_wb_list to i_io_list
      sync: serialise per-superblock sync operations
      inode: convert inode_sb_list_lock to per-sb
      inode: add hlist_fake to avoid the inode hash lock in evict
      writeback: plug writeback at a high level
      change sb_writers to use percpu_rw_semaphore
      shift percpu_counter_destroy() into destroy_super_work()
      percpu-rwsem: kill CONFIG_PERCPU_RWSEM
      percpu-rwsem: introduce percpu_rwsem_release() and percpu_rwsem_acquire()
      percpu-rwsem: introduce percpu_down_read_trylock()
      document rwsem_release() in sb_wait_write()
      fix the broken lockdep logic in __sb_start_write()
      introduce __sb_writers_{acquired,release}() helpers
      ufs_inode_get{frag,block}(): get rid of 'phys' argument
      ufs_getfrag_block(): tidy up a bit
      ...

commit 9287f6925ad9d8fb8c6283066b4f77fd87f123a9
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Jul 21 17:45:57 2015 +0200

    percpu-rwsem: introduce percpu_down_read_trylock()
    
    Add percpu_down_read_trylock(), it will have the user soon.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

commit 6d209d9ada74279a3fb64b0ab873fe8e4a11f853
Author: Daniel J Blueman <daniel@numascale.com>
Date:   Tue Feb 17 11:34:38 2015 +0800

    EDAC, amd64_edac: Prevent OOPS with >16 memory controllers
    
    commit 0c510cc83bdbaac8406f4f7caef34f4da0ba35ea upstream.
    
    When DRAM errors occur on memory controllers after EDAC_MAX_MCS (16),
    the kernel fatally dereferences unallocated structures, see splat below;
    this occurs on at least NumaConnect systems.
    
    Fix by checking if a memory controller info structure was found.
    
    BUG: unable to handle kernel NULL pointer dereference at 0000000000000320
    IP: [<ffffffff819f714f>] decode_bus_error+0x2f/0x2b0
    PGD 2f8b5a3067 PUD 2f8b5a2067 PMD 0
    Oops: 0000 [#2] SMP
    Modules linked in:
    CPU: 224 PID: 11930 Comm: stream_c.exe.gn Tainted: G   D    3.19.0 #1
    Hardware name: Supermicro H8QGL/H8QGL, BIOS 3.5b    01/28/2015
    task: ffff8807dbfb8c00 ti: ffff8807dd16c000 task.ti: ffff8807dd16c000
    RIP: 0010:[<ffffffff819f714f>] [<ffffffff819f714f>] decode_bus_error+0x2f/0x2b0
    RSP: 0000:ffff8907dfc03c48 EFLAGS: 00010297
    RAX: 0000000000000001 RBX: 9c67400010080a13 RCX: 0000000000001dc6
    RDX: 000000001dc61dc6 RSI: ffff8907dfc03df0 RDI: 000000000000001c
    RBP: ffff8907dfc03ce8 R08: 0000000000000000 R09: 0000000000000022
    R10: ffff891fffa30380 R11: 00000000001cfc90 R12: 0000000000000008
    R13: 0000000000000000 R14: 000000000000001c R15: 00009c6740001000
    FS: 00007fa97ee18700(0000) GS:ffff8907dfc00000(0000) knlGS:0000000000000000
    CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000000000320 CR3: 0000003f889b8000 CR4: 00000000000407e0
    Stack:
     0000000000000000 ffff8907dfc03df0 0000000000000008 9c67400010080a13
     000000000000001c 00009c6740001000 ffff8907dfc03c88 ffffffff810e4f9a
     ffff8907dfc03ce8 ffffffff81b375b9 0000000000000000 0000000000000010
    Call Trace:
     <IRQ>
     ? vprintk_default
     ? printk
     amd_decode_mce
     notifier_call_chain
     atomic_notifier_call_chain
     mce_log
     machine_check_poll
     mce_timer_fn
     ? mce_cpu_restart
     call_timer_fn.isra.29
     run_timer_softirq
     __do_softirq
     irq_exit
     smp_apic_timer_interrupt
     apic_timer_interrupt
     <EOI>
     ? down_read_trylock
     __do_page_fault
     ? __schedule
     do_page_fault
     page_fault
    
    Signed-off-by: Daniel J Blueman <daniel@numascale.com>
    Link: http://lkml.kernel.org/r/1424144078-24589-1-git-send-email-daniel@numascale.com
    [ Boris: massage commit message ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6a35db9924ee513fe5d3edac99f563f4a447e957
Author: Daniel J Blueman <daniel@numascale.com>
Date:   Tue Feb 17 11:34:38 2015 +0800

    EDAC, amd64_edac: Prevent OOPS with >16 memory controllers
    
    commit 0c510cc83bdbaac8406f4f7caef34f4da0ba35ea upstream.
    
    When DRAM errors occur on memory controllers after EDAC_MAX_MCS (16),
    the kernel fatally dereferences unallocated structures, see splat below;
    this occurs on at least NumaConnect systems.
    
    Fix by checking if a memory controller info structure was found.
    
    BUG: unable to handle kernel NULL pointer dereference at 0000000000000320
    IP: [<ffffffff819f714f>] decode_bus_error+0x2f/0x2b0
    PGD 2f8b5a3067 PUD 2f8b5a2067 PMD 0
    Oops: 0000 [#2] SMP
    Modules linked in:
    CPU: 224 PID: 11930 Comm: stream_c.exe.gn Tainted: G   D    3.19.0 #1
    Hardware name: Supermicro H8QGL/H8QGL, BIOS 3.5b    01/28/2015
    task: ffff8807dbfb8c00 ti: ffff8807dd16c000 task.ti: ffff8807dd16c000
    RIP: 0010:[<ffffffff819f714f>] [<ffffffff819f714f>] decode_bus_error+0x2f/0x2b0
    RSP: 0000:ffff8907dfc03c48 EFLAGS: 00010297
    RAX: 0000000000000001 RBX: 9c67400010080a13 RCX: 0000000000001dc6
    RDX: 000000001dc61dc6 RSI: ffff8907dfc03df0 RDI: 000000000000001c
    RBP: ffff8907dfc03ce8 R08: 0000000000000000 R09: 0000000000000022
    R10: ffff891fffa30380 R11: 00000000001cfc90 R12: 0000000000000008
    R13: 0000000000000000 R14: 000000000000001c R15: 00009c6740001000
    FS: 00007fa97ee18700(0000) GS:ffff8907dfc00000(0000) knlGS:0000000000000000
    CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000000000320 CR3: 0000003f889b8000 CR4: 00000000000407e0
    Stack:
     0000000000000000 ffff8907dfc03df0 0000000000000008 9c67400010080a13
     000000000000001c 00009c6740001000 ffff8907dfc03c88 ffffffff810e4f9a
     ffff8907dfc03ce8 ffffffff81b375b9 0000000000000000 0000000000000010
    Call Trace:
     <IRQ>
     ? vprintk_default
     ? printk
     amd_decode_mce
     notifier_call_chain
     atomic_notifier_call_chain
     mce_log
     machine_check_poll
     mce_timer_fn
     ? mce_cpu_restart
     call_timer_fn.isra.29
     run_timer_softirq
     __do_softirq
     irq_exit
     smp_apic_timer_interrupt
     apic_timer_interrupt
     <EOI>
     ? down_read_trylock
     __do_page_fault
     ? __schedule
     do_page_fault
     page_fault
    
    Signed-off-by: Daniel J Blueman <daniel@numascale.com>
    Link: http://lkml.kernel.org/r/1424144078-24589-1-git-send-email-daniel@numascale.com
    [ Boris: massage commit message ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a36a251710af8e53fdf252f44dba5b0fdb478d30
Author: Daniel J Blueman <daniel@numascale.com>
Date:   Tue Feb 17 11:34:38 2015 +0800

    EDAC, amd64_edac: Prevent OOPS with >16 memory controllers
    
    commit 0c510cc83bdbaac8406f4f7caef34f4da0ba35ea upstream.
    
    When DRAM errors occur on memory controllers after EDAC_MAX_MCS (16),
    the kernel fatally dereferences unallocated structures, see splat below;
    this occurs on at least NumaConnect systems.
    
    Fix by checking if a memory controller info structure was found.
    
    BUG: unable to handle kernel NULL pointer dereference at 0000000000000320
    IP: [<ffffffff819f714f>] decode_bus_error+0x2f/0x2b0
    PGD 2f8b5a3067 PUD 2f8b5a2067 PMD 0
    Oops: 0000 [#2] SMP
    Modules linked in:
    CPU: 224 PID: 11930 Comm: stream_c.exe.gn Tainted: G   D    3.19.0 #1
    Hardware name: Supermicro H8QGL/H8QGL, BIOS 3.5b    01/28/2015
    task: ffff8807dbfb8c00 ti: ffff8807dd16c000 task.ti: ffff8807dd16c000
    RIP: 0010:[<ffffffff819f714f>] [<ffffffff819f714f>] decode_bus_error+0x2f/0x2b0
    RSP: 0000:ffff8907dfc03c48 EFLAGS: 00010297
    RAX: 0000000000000001 RBX: 9c67400010080a13 RCX: 0000000000001dc6
    RDX: 000000001dc61dc6 RSI: ffff8907dfc03df0 RDI: 000000000000001c
    RBP: ffff8907dfc03ce8 R08: 0000000000000000 R09: 0000000000000022
    R10: ffff891fffa30380 R11: 00000000001cfc90 R12: 0000000000000008
    R13: 0000000000000000 R14: 000000000000001c R15: 00009c6740001000
    FS: 00007fa97ee18700(0000) GS:ffff8907dfc00000(0000) knlGS:0000000000000000
    CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000000000320 CR3: 0000003f889b8000 CR4: 00000000000407e0
    Stack:
     0000000000000000 ffff8907dfc03df0 0000000000000008 9c67400010080a13
     000000000000001c 00009c6740001000 ffff8907dfc03c88 ffffffff810e4f9a
     ffff8907dfc03ce8 ffffffff81b375b9 0000000000000000 0000000000000010
    Call Trace:
     <IRQ>
     ? vprintk_default
     ? printk
     amd_decode_mce
     notifier_call_chain
     atomic_notifier_call_chain
     mce_log
     machine_check_poll
     mce_timer_fn
     ? mce_cpu_restart
     call_timer_fn.isra.29
     run_timer_softirq
     __do_softirq
     irq_exit
     smp_apic_timer_interrupt
     apic_timer_interrupt
     <EOI>
     ? down_read_trylock
     __do_page_fault
     ? __schedule
     do_page_fault
     page_fault
    
    Signed-off-by: Daniel J Blueman <daniel@numascale.com>
    Link: http://lkml.kernel.org/r/1424144078-24589-1-git-send-email-daniel@numascale.com
    [ Boris: massage commit message ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9a8120f5c8ff17c2bc97d615a7006da7aec67ebe
Author: Daniel J Blueman <daniel@numascale.com>
Date:   Tue Feb 17 11:34:38 2015 +0800

    EDAC, amd64_edac: Prevent OOPS with >16 memory controllers
    
    commit 0c510cc83bdbaac8406f4f7caef34f4da0ba35ea upstream.
    
    When DRAM errors occur on memory controllers after EDAC_MAX_MCS (16),
    the kernel fatally dereferences unallocated structures, see splat below;
    this occurs on at least NumaConnect systems.
    
    Fix by checking if a memory controller info structure was found.
    
    BUG: unable to handle kernel NULL pointer dereference at 0000000000000320
    IP: [<ffffffff819f714f>] decode_bus_error+0x2f/0x2b0
    PGD 2f8b5a3067 PUD 2f8b5a2067 PMD 0
    Oops: 0000 [#2] SMP
    Modules linked in:
    CPU: 224 PID: 11930 Comm: stream_c.exe.gn Tainted: G   D    3.19.0 #1
    Hardware name: Supermicro H8QGL/H8QGL, BIOS 3.5b    01/28/2015
    task: ffff8807dbfb8c00 ti: ffff8807dd16c000 task.ti: ffff8807dd16c000
    RIP: 0010:[<ffffffff819f714f>] [<ffffffff819f714f>] decode_bus_error+0x2f/0x2b0
    RSP: 0000:ffff8907dfc03c48 EFLAGS: 00010297
    RAX: 0000000000000001 RBX: 9c67400010080a13 RCX: 0000000000001dc6
    RDX: 000000001dc61dc6 RSI: ffff8907dfc03df0 RDI: 000000000000001c
    RBP: ffff8907dfc03ce8 R08: 0000000000000000 R09: 0000000000000022
    R10: ffff891fffa30380 R11: 00000000001cfc90 R12: 0000000000000008
    R13: 0000000000000000 R14: 000000000000001c R15: 00009c6740001000
    FS: 00007fa97ee18700(0000) GS:ffff8907dfc00000(0000) knlGS:0000000000000000
    CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000000000320 CR3: 0000003f889b8000 CR4: 00000000000407e0
    Stack:
     0000000000000000 ffff8907dfc03df0 0000000000000008 9c67400010080a13
     000000000000001c 00009c6740001000 ffff8907dfc03c88 ffffffff810e4f9a
     ffff8907dfc03ce8 ffffffff81b375b9 0000000000000000 0000000000000010
    Call Trace:
     <IRQ>
     ? vprintk_default
     ? printk
     amd_decode_mce
     notifier_call_chain
     atomic_notifier_call_chain
     mce_log
     machine_check_poll
     mce_timer_fn
     ? mce_cpu_restart
     call_timer_fn.isra.29
     run_timer_softirq
     __do_softirq
     irq_exit
     smp_apic_timer_interrupt
     apic_timer_interrupt
     <EOI>
     ? down_read_trylock
     __do_page_fault
     ? __schedule
     do_page_fault
     page_fault
    
    Signed-off-by: Daniel J Blueman <daniel@numascale.com>
    Link: http://lkml.kernel.org/r/1424144078-24589-1-git-send-email-daniel@numascale.com
    [ Boris: massage commit message ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz> [backport to 3.12]

commit 0c24a6c4cc81f49a11389f480e4526cbf1e13d40
Author: Daniel J Blueman <daniel@numascale.com>
Date:   Tue Feb 17 11:34:38 2015 +0800

    EDAC, amd64_edac: Prevent OOPS with >16 memory controllers
    
    commit 0c510cc83bdbaac8406f4f7caef34f4da0ba35ea upstream.
    
    When DRAM errors occur on memory controllers after EDAC_MAX_MCS (16),
    the kernel fatally dereferences unallocated structures, see splat below;
    this occurs on at least NumaConnect systems.
    
    Fix by checking if a memory controller info structure was found.
    
    BUG: unable to handle kernel NULL pointer dereference at 0000000000000320
    IP: [<ffffffff819f714f>] decode_bus_error+0x2f/0x2b0
    PGD 2f8b5a3067 PUD 2f8b5a2067 PMD 0
    Oops: 0000 [#2] SMP
    Modules linked in:
    CPU: 224 PID: 11930 Comm: stream_c.exe.gn Tainted: G   D    3.19.0 #1
    Hardware name: Supermicro H8QGL/H8QGL, BIOS 3.5b    01/28/2015
    task: ffff8807dbfb8c00 ti: ffff8807dd16c000 task.ti: ffff8807dd16c000
    RIP: 0010:[<ffffffff819f714f>] [<ffffffff819f714f>] decode_bus_error+0x2f/0x2b0
    RSP: 0000:ffff8907dfc03c48 EFLAGS: 00010297
    RAX: 0000000000000001 RBX: 9c67400010080a13 RCX: 0000000000001dc6
    RDX: 000000001dc61dc6 RSI: ffff8907dfc03df0 RDI: 000000000000001c
    RBP: ffff8907dfc03ce8 R08: 0000000000000000 R09: 0000000000000022
    R10: ffff891fffa30380 R11: 00000000001cfc90 R12: 0000000000000008
    R13: 0000000000000000 R14: 000000000000001c R15: 00009c6740001000
    FS: 00007fa97ee18700(0000) GS:ffff8907dfc00000(0000) knlGS:0000000000000000
    CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000000000320 CR3: 0000003f889b8000 CR4: 00000000000407e0
    Stack:
     0000000000000000 ffff8907dfc03df0 0000000000000008 9c67400010080a13
     000000000000001c 00009c6740001000 ffff8907dfc03c88 ffffffff810e4f9a
     ffff8907dfc03ce8 ffffffff81b375b9 0000000000000000 0000000000000010
    Call Trace:
     <IRQ>
     ? vprintk_default
     ? printk
     amd_decode_mce
     notifier_call_chain
     atomic_notifier_call_chain
     mce_log
     machine_check_poll
     mce_timer_fn
     ? mce_cpu_restart
     call_timer_fn.isra.29
     run_timer_softirq
     __do_softirq
     irq_exit
     smp_apic_timer_interrupt
     apic_timer_interrupt
     <EOI>
     ? down_read_trylock
     __do_page_fault
     ? __schedule
     do_page_fault
     page_fault
    
    Signed-off-by: Daniel J Blueman <daniel@numascale.com>
    Link: http://lkml.kernel.org/r/1424144078-24589-1-git-send-email-daniel@numascale.com
    [ Boris: massage commit message ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit 0c510cc83bdbaac8406f4f7caef34f4da0ba35ea
Author: Daniel J Blueman <daniel@numascale.com>
Date:   Tue Feb 17 11:34:38 2015 +0800

    EDAC, amd64_edac: Prevent OOPS with >16 memory controllers
    
    When DRAM errors occur on memory controllers after EDAC_MAX_MCS (16),
    the kernel fatally dereferences unallocated structures, see splat below;
    this occurs on at least NumaConnect systems.
    
    Fix by checking if a memory controller info structure was found.
    
    BUG: unable to handle kernel NULL pointer dereference at 0000000000000320
    IP: [<ffffffff819f714f>] decode_bus_error+0x2f/0x2b0
    PGD 2f8b5a3067 PUD 2f8b5a2067 PMD 0
    Oops: 0000 [#2] SMP
    Modules linked in:
    CPU: 224 PID: 11930 Comm: stream_c.exe.gn Tainted: G   D    3.19.0 #1
    Hardware name: Supermicro H8QGL/H8QGL, BIOS 3.5b    01/28/2015
    task: ffff8807dbfb8c00 ti: ffff8807dd16c000 task.ti: ffff8807dd16c000
    RIP: 0010:[<ffffffff819f714f>] [<ffffffff819f714f>] decode_bus_error+0x2f/0x2b0
    RSP: 0000:ffff8907dfc03c48 EFLAGS: 00010297
    RAX: 0000000000000001 RBX: 9c67400010080a13 RCX: 0000000000001dc6
    RDX: 000000001dc61dc6 RSI: ffff8907dfc03df0 RDI: 000000000000001c
    RBP: ffff8907dfc03ce8 R08: 0000000000000000 R09: 0000000000000022
    R10: ffff891fffa30380 R11: 00000000001cfc90 R12: 0000000000000008
    R13: 0000000000000000 R14: 000000000000001c R15: 00009c6740001000
    FS: 00007fa97ee18700(0000) GS:ffff8907dfc00000(0000) knlGS:0000000000000000
    CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000000000320 CR3: 0000003f889b8000 CR4: 00000000000407e0
    Stack:
     0000000000000000 ffff8907dfc03df0 0000000000000008 9c67400010080a13
     000000000000001c 00009c6740001000 ffff8907dfc03c88 ffffffff810e4f9a
     ffff8907dfc03ce8 ffffffff81b375b9 0000000000000000 0000000000000010
    Call Trace:
     <IRQ>
     ? vprintk_default
     ? printk
     amd_decode_mce
     notifier_call_chain
     atomic_notifier_call_chain
     mce_log
     machine_check_poll
     mce_timer_fn
     ? mce_cpu_restart
     call_timer_fn.isra.29
     run_timer_softirq
     __do_softirq
     irq_exit
     smp_apic_timer_interrupt
     apic_timer_interrupt
     <EOI>
     ? down_read_trylock
     __do_page_fault
     ? __schedule
     do_page_fault
     page_fault
    
    Signed-off-by: Daniel J Blueman <daniel@numascale.com>
    Link: http://lkml.kernel.org/r/1424144078-24589-1-git-send-email-daniel@numascale.com
    Cc: stable@vger.kernel.org
    [ Boris: massage commit message ]
    Signed-off-by: Borislav Petkov <bp@suse.de>

commit 56dfc960370c6b48c69394fa088935a79c7ab3e4
Author: Johan Hovold <johan@kernel.org>
Date:   Mon Jan 5 16:04:12 2015 +0100

    USB: console: fix uninitialised ldisc semaphore
    
    commit d269d4434c72ed0da3a9b1230c30da82c4918c63 upstream.
    
    The USB console currently allocates a temporary fake tty which is used
    to pass terminal settings to the underlying serial driver.
    
    The tty struct is not fully initialised, something which can lead to a
    lockdep warning (or worse) if a serial driver tries to acquire a
    line-discipline reference:
    
            usbserial: USB Serial support registered for pl2303
            pl2303 1-2.1:1.0: pl2303 converter detected
            usb 1-2.1: pl2303 converter now attached to ttyUSB0
            INFO: trying to register non-static key.
            the code is fine but needs lockdep annotation.
            turning off the locking correctness validator.
            CPU: 0 PID: 68 Comm: udevd Tainted: G        W      3.18.0-rc5 #10
            [<c0016f04>] (unwind_backtrace) from [<c0013978>] (show_stack+0x20/0x24)
            [<c0013978>] (show_stack) from [<c0449794>] (dump_stack+0x24/0x28)
            [<c0449794>] (dump_stack) from [<c006f730>] (__lock_acquire+0x1e50/0x2004)
            [<c006f730>] (__lock_acquire) from [<c0070128>] (lock_acquire+0xe4/0x18c)
            [<c0070128>] (lock_acquire) from [<c027c6f8>] (ldsem_down_read_trylock+0x78/0x90)
            [<c027c6f8>] (ldsem_down_read_trylock) from [<c027a1cc>] (tty_ldisc_ref+0x24/0x58)
            [<c027a1cc>] (tty_ldisc_ref) from [<c0340760>] (usb_serial_handle_dcd_change+0x48/0xe8)
            [<c0340760>] (usb_serial_handle_dcd_change) from [<bf000484>] (pl2303_read_int_callback+0x210/0x220 [pl2303])
            [<bf000484>] (pl2303_read_int_callback [pl2303]) from [<c031624c>] (__usb_hcd_giveback_urb+0x80/0x140)
            [<c031624c>] (__usb_hcd_giveback_urb) from [<c0316fc0>] (usb_giveback_urb_bh+0x98/0xd4)
            [<c0316fc0>] (usb_giveback_urb_bh) from [<c0042e44>] (tasklet_hi_action+0x9c/0x108)
            [<c0042e44>] (tasklet_hi_action) from [<c0042380>] (__do_softirq+0x148/0x42c)
            [<c0042380>] (__do_softirq) from [<c00429cc>] (irq_exit+0xd8/0x114)
            [<c00429cc>] (irq_exit) from [<c007ae58>] (__handle_domain_irq+0x84/0xdc)
            [<c007ae58>] (__handle_domain_irq) from [<c000879c>] (omap_intc_handle_irq+0xd8/0xe0)
            [<c000879c>] (omap_intc_handle_irq) from [<c0014544>] (__irq_svc+0x44/0x7c)
            Exception stack(0xdf4e7f08 to 0xdf4e7f50)
            7f00:                   debc0b80 df4e7f5c 00000000 00000000 debc0b80 be8da96c
            7f20: 00000000 00000128 c000fc84 df4e6000 00000000 df4e7f94 00000004 df4e7f50
            7f40: c038ebc0 c038d74c 600f0013 ffffffff
            [<c0014544>] (__irq_svc) from [<c038d74c>] (___sys_sendmsg.part.29+0x0/0x2e0)
            [<c038d74c>] (___sys_sendmsg.part.29) from [<c038ec08>] (SyS_sendmsg+0x18/0x1c)
            [<c038ec08>] (SyS_sendmsg) from [<c000fa00>] (ret_fast_syscall+0x0/0x48)
            console [ttyUSB0] enabled
    
    Fixes: 36697529b5bb ("tty: Replace ldisc locking with ldisc_sem")
    Signed-off-by: Johan Hovold <johan@kernel.org>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit 68d91b4c79481099b0fe0b37c7eafeabd12813ae
Author: Johan Hovold <johan@kernel.org>
Date:   Mon Jan 5 16:04:12 2015 +0100

    USB: console: fix uninitialised ldisc semaphore
    
    commit d269d4434c72ed0da3a9b1230c30da82c4918c63 upstream.
    
    The USB console currently allocates a temporary fake tty which is used
    to pass terminal settings to the underlying serial driver.
    
    The tty struct is not fully initialised, something which can lead to a
    lockdep warning (or worse) if a serial driver tries to acquire a
    line-discipline reference:
    
            usbserial: USB Serial support registered for pl2303
            pl2303 1-2.1:1.0: pl2303 converter detected
            usb 1-2.1: pl2303 converter now attached to ttyUSB0
            INFO: trying to register non-static key.
            the code is fine but needs lockdep annotation.
            turning off the locking correctness validator.
            CPU: 0 PID: 68 Comm: udevd Tainted: G        W      3.18.0-rc5 #10
            [<c0016f04>] (unwind_backtrace) from [<c0013978>] (show_stack+0x20/0x24)
            [<c0013978>] (show_stack) from [<c0449794>] (dump_stack+0x24/0x28)
            [<c0449794>] (dump_stack) from [<c006f730>] (__lock_acquire+0x1e50/0x2004)
            [<c006f730>] (__lock_acquire) from [<c0070128>] (lock_acquire+0xe4/0x18c)
            [<c0070128>] (lock_acquire) from [<c027c6f8>] (ldsem_down_read_trylock+0x78/0x90)
            [<c027c6f8>] (ldsem_down_read_trylock) from [<c027a1cc>] (tty_ldisc_ref+0x24/0x58)
            [<c027a1cc>] (tty_ldisc_ref) from [<c0340760>] (usb_serial_handle_dcd_change+0x48/0xe8)
            [<c0340760>] (usb_serial_handle_dcd_change) from [<bf000484>] (pl2303_read_int_callback+0x210/0x220 [pl2303])
            [<bf000484>] (pl2303_read_int_callback [pl2303]) from [<c031624c>] (__usb_hcd_giveback_urb+0x80/0x140)
            [<c031624c>] (__usb_hcd_giveback_urb) from [<c0316fc0>] (usb_giveback_urb_bh+0x98/0xd4)
            [<c0316fc0>] (usb_giveback_urb_bh) from [<c0042e44>] (tasklet_hi_action+0x9c/0x108)
            [<c0042e44>] (tasklet_hi_action) from [<c0042380>] (__do_softirq+0x148/0x42c)
            [<c0042380>] (__do_softirq) from [<c00429cc>] (irq_exit+0xd8/0x114)
            [<c00429cc>] (irq_exit) from [<c007ae58>] (__handle_domain_irq+0x84/0xdc)
            [<c007ae58>] (__handle_domain_irq) from [<c000879c>] (omap_intc_handle_irq+0xd8/0xe0)
            [<c000879c>] (omap_intc_handle_irq) from [<c0014544>] (__irq_svc+0x44/0x7c)
            Exception stack(0xdf4e7f08 to 0xdf4e7f50)
            7f00:                   debc0b80 df4e7f5c 00000000 00000000 debc0b80 be8da96c
            7f20: 00000000 00000128 c000fc84 df4e6000 00000000 df4e7f94 00000004 df4e7f50
            7f40: c038ebc0 c038d74c 600f0013 ffffffff
            [<c0014544>] (__irq_svc) from [<c038d74c>] (___sys_sendmsg.part.29+0x0/0x2e0)
            [<c038d74c>] (___sys_sendmsg.part.29) from [<c038ec08>] (SyS_sendmsg+0x18/0x1c)
            [<c038ec08>] (SyS_sendmsg) from [<c000fa00>] (ret_fast_syscall+0x0/0x48)
            console [ttyUSB0] enabled
    
    Fixes: 36697529b5bb ("tty: Replace ldisc locking with ldisc_sem")
    Signed-off-by: Johan Hovold <johan@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8438d81444fe8338ccf8b43538ce6aacf4cd3c39
Author: Johan Hovold <johan@kernel.org>
Date:   Mon Jan 5 16:04:12 2015 +0100

    USB: console: fix uninitialised ldisc semaphore
    
    commit d269d4434c72ed0da3a9b1230c30da82c4918c63 upstream.
    
    The USB console currently allocates a temporary fake tty which is used
    to pass terminal settings to the underlying serial driver.
    
    The tty struct is not fully initialised, something which can lead to a
    lockdep warning (or worse) if a serial driver tries to acquire a
    line-discipline reference:
    
            usbserial: USB Serial support registered for pl2303
            pl2303 1-2.1:1.0: pl2303 converter detected
            usb 1-2.1: pl2303 converter now attached to ttyUSB0
            INFO: trying to register non-static key.
            the code is fine but needs lockdep annotation.
            turning off the locking correctness validator.
            CPU: 0 PID: 68 Comm: udevd Tainted: G        W      3.18.0-rc5 #10
            [<c0016f04>] (unwind_backtrace) from [<c0013978>] (show_stack+0x20/0x24)
            [<c0013978>] (show_stack) from [<c0449794>] (dump_stack+0x24/0x28)
            [<c0449794>] (dump_stack) from [<c006f730>] (__lock_acquire+0x1e50/0x2004)
            [<c006f730>] (__lock_acquire) from [<c0070128>] (lock_acquire+0xe4/0x18c)
            [<c0070128>] (lock_acquire) from [<c027c6f8>] (ldsem_down_read_trylock+0x78/0x90)
            [<c027c6f8>] (ldsem_down_read_trylock) from [<c027a1cc>] (tty_ldisc_ref+0x24/0x58)
            [<c027a1cc>] (tty_ldisc_ref) from [<c0340760>] (usb_serial_handle_dcd_change+0x48/0xe8)
            [<c0340760>] (usb_serial_handle_dcd_change) from [<bf000484>] (pl2303_read_int_callback+0x210/0x220 [pl2303])
            [<bf000484>] (pl2303_read_int_callback [pl2303]) from [<c031624c>] (__usb_hcd_giveback_urb+0x80/0x140)
            [<c031624c>] (__usb_hcd_giveback_urb) from [<c0316fc0>] (usb_giveback_urb_bh+0x98/0xd4)
            [<c0316fc0>] (usb_giveback_urb_bh) from [<c0042e44>] (tasklet_hi_action+0x9c/0x108)
            [<c0042e44>] (tasklet_hi_action) from [<c0042380>] (__do_softirq+0x148/0x42c)
            [<c0042380>] (__do_softirq) from [<c00429cc>] (irq_exit+0xd8/0x114)
            [<c00429cc>] (irq_exit) from [<c007ae58>] (__handle_domain_irq+0x84/0xdc)
            [<c007ae58>] (__handle_domain_irq) from [<c000879c>] (omap_intc_handle_irq+0xd8/0xe0)
            [<c000879c>] (omap_intc_handle_irq) from [<c0014544>] (__irq_svc+0x44/0x7c)
            Exception stack(0xdf4e7f08 to 0xdf4e7f50)
            7f00:                   debc0b80 df4e7f5c 00000000 00000000 debc0b80 be8da96c
            7f20: 00000000 00000128 c000fc84 df4e6000 00000000 df4e7f94 00000004 df4e7f50
            7f40: c038ebc0 c038d74c 600f0013 ffffffff
            [<c0014544>] (__irq_svc) from [<c038d74c>] (___sys_sendmsg.part.29+0x0/0x2e0)
            [<c038d74c>] (___sys_sendmsg.part.29) from [<c038ec08>] (SyS_sendmsg+0x18/0x1c)
            [<c038ec08>] (SyS_sendmsg) from [<c000fa00>] (ret_fast_syscall+0x0/0x48)
            console [ttyUSB0] enabled
    
    Fixes: 36697529b5bb ("tty: Replace ldisc locking with ldisc_sem")
    Signed-off-by: Johan Hovold <johan@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6b03e26e122ee60ae2c63912ce13b3a10d86feeb
Author: Johan Hovold <johan@kernel.org>
Date:   Mon Jan 5 16:04:12 2015 +0100

    USB: console: fix uninitialised ldisc semaphore
    
    commit d269d4434c72ed0da3a9b1230c30da82c4918c63 upstream.
    
    The USB console currently allocates a temporary fake tty which is used
    to pass terminal settings to the underlying serial driver.
    
    The tty struct is not fully initialised, something which can lead to a
    lockdep warning (or worse) if a serial driver tries to acquire a
    line-discipline reference:
    
            usbserial: USB Serial support registered for pl2303
            pl2303 1-2.1:1.0: pl2303 converter detected
            usb 1-2.1: pl2303 converter now attached to ttyUSB0
            INFO: trying to register non-static key.
            the code is fine but needs lockdep annotation.
            turning off the locking correctness validator.
            CPU: 0 PID: 68 Comm: udevd Tainted: G        W      3.18.0-rc5 #10
            [<c0016f04>] (unwind_backtrace) from [<c0013978>] (show_stack+0x20/0x24)
            [<c0013978>] (show_stack) from [<c0449794>] (dump_stack+0x24/0x28)
            [<c0449794>] (dump_stack) from [<c006f730>] (__lock_acquire+0x1e50/0x2004)
            [<c006f730>] (__lock_acquire) from [<c0070128>] (lock_acquire+0xe4/0x18c)
            [<c0070128>] (lock_acquire) from [<c027c6f8>] (ldsem_down_read_trylock+0x78/0x90)
            [<c027c6f8>] (ldsem_down_read_trylock) from [<c027a1cc>] (tty_ldisc_ref+0x24/0x58)
            [<c027a1cc>] (tty_ldisc_ref) from [<c0340760>] (usb_serial_handle_dcd_change+0x48/0xe8)
            [<c0340760>] (usb_serial_handle_dcd_change) from [<bf000484>] (pl2303_read_int_callback+0x210/0x220 [pl2303])
            [<bf000484>] (pl2303_read_int_callback [pl2303]) from [<c031624c>] (__usb_hcd_giveback_urb+0x80/0x140)
            [<c031624c>] (__usb_hcd_giveback_urb) from [<c0316fc0>] (usb_giveback_urb_bh+0x98/0xd4)
            [<c0316fc0>] (usb_giveback_urb_bh) from [<c0042e44>] (tasklet_hi_action+0x9c/0x108)
            [<c0042e44>] (tasklet_hi_action) from [<c0042380>] (__do_softirq+0x148/0x42c)
            [<c0042380>] (__do_softirq) from [<c00429cc>] (irq_exit+0xd8/0x114)
            [<c00429cc>] (irq_exit) from [<c007ae58>] (__handle_domain_irq+0x84/0xdc)
            [<c007ae58>] (__handle_domain_irq) from [<c000879c>] (omap_intc_handle_irq+0xd8/0xe0)
            [<c000879c>] (omap_intc_handle_irq) from [<c0014544>] (__irq_svc+0x44/0x7c)
            Exception stack(0xdf4e7f08 to 0xdf4e7f50)
            7f00:                   debc0b80 df4e7f5c 00000000 00000000 debc0b80 be8da96c
            7f20: 00000000 00000128 c000fc84 df4e6000 00000000 df4e7f94 00000004 df4e7f50
            7f40: c038ebc0 c038d74c 600f0013 ffffffff
            [<c0014544>] (__irq_svc) from [<c038d74c>] (___sys_sendmsg.part.29+0x0/0x2e0)
            [<c038d74c>] (___sys_sendmsg.part.29) from [<c038ec08>] (SyS_sendmsg+0x18/0x1c)
            [<c038ec08>] (SyS_sendmsg) from [<c000fa00>] (ret_fast_syscall+0x0/0x48)
            console [ttyUSB0] enabled
    
    Fixes: 36697529b5bb ("tty: Replace ldisc locking with ldisc_sem")
    Signed-off-by: Johan Hovold <johan@kernel.org>
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit d269d4434c72ed0da3a9b1230c30da82c4918c63
Author: Johan Hovold <johan@kernel.org>
Date:   Mon Jan 5 16:04:12 2015 +0100

    USB: console: fix uninitialised ldisc semaphore
    
    The USB console currently allocates a temporary fake tty which is used
    to pass terminal settings to the underlying serial driver.
    
    The tty struct is not fully initialised, something which can lead to a
    lockdep warning (or worse) if a serial driver tries to acquire a
    line-discipline reference:
    
            usbserial: USB Serial support registered for pl2303
            pl2303 1-2.1:1.0: pl2303 converter detected
            usb 1-2.1: pl2303 converter now attached to ttyUSB0
            INFO: trying to register non-static key.
            the code is fine but needs lockdep annotation.
            turning off the locking correctness validator.
            CPU: 0 PID: 68 Comm: udevd Tainted: G        W      3.18.0-rc5 #10
            [<c0016f04>] (unwind_backtrace) from [<c0013978>] (show_stack+0x20/0x24)
            [<c0013978>] (show_stack) from [<c0449794>] (dump_stack+0x24/0x28)
            [<c0449794>] (dump_stack) from [<c006f730>] (__lock_acquire+0x1e50/0x2004)
            [<c006f730>] (__lock_acquire) from [<c0070128>] (lock_acquire+0xe4/0x18c)
            [<c0070128>] (lock_acquire) from [<c027c6f8>] (ldsem_down_read_trylock+0x78/0x90)
            [<c027c6f8>] (ldsem_down_read_trylock) from [<c027a1cc>] (tty_ldisc_ref+0x24/0x58)
            [<c027a1cc>] (tty_ldisc_ref) from [<c0340760>] (usb_serial_handle_dcd_change+0x48/0xe8)
            [<c0340760>] (usb_serial_handle_dcd_change) from [<bf000484>] (pl2303_read_int_callback+0x210/0x220 [pl2303])
            [<bf000484>] (pl2303_read_int_callback [pl2303]) from [<c031624c>] (__usb_hcd_giveback_urb+0x80/0x140)
            [<c031624c>] (__usb_hcd_giveback_urb) from [<c0316fc0>] (usb_giveback_urb_bh+0x98/0xd4)
            [<c0316fc0>] (usb_giveback_urb_bh) from [<c0042e44>] (tasklet_hi_action+0x9c/0x108)
            [<c0042e44>] (tasklet_hi_action) from [<c0042380>] (__do_softirq+0x148/0x42c)
            [<c0042380>] (__do_softirq) from [<c00429cc>] (irq_exit+0xd8/0x114)
            [<c00429cc>] (irq_exit) from [<c007ae58>] (__handle_domain_irq+0x84/0xdc)
            [<c007ae58>] (__handle_domain_irq) from [<c000879c>] (omap_intc_handle_irq+0xd8/0xe0)
            [<c000879c>] (omap_intc_handle_irq) from [<c0014544>] (__irq_svc+0x44/0x7c)
            Exception stack(0xdf4e7f08 to 0xdf4e7f50)
            7f00:                   debc0b80 df4e7f5c 00000000 00000000 debc0b80 be8da96c
            7f20: 00000000 00000128 c000fc84 df4e6000 00000000 df4e7f94 00000004 df4e7f50
            7f40: c038ebc0 c038d74c 600f0013 ffffffff
            [<c0014544>] (__irq_svc) from [<c038d74c>] (___sys_sendmsg.part.29+0x0/0x2e0)
            [<c038d74c>] (___sys_sendmsg.part.29) from [<c038ec08>] (SyS_sendmsg+0x18/0x1c)
            [<c038ec08>] (SyS_sendmsg) from [<c000fa00>] (ret_fast_syscall+0x0/0x48)
            console [ttyUSB0] enabled
    
    Fixes: 36697529b5bb ("tty: Replace ldisc locking with ldisc_sem")
    Cc: stable <stable@vger.kernel.org>
    Signed-off-by: Johan Hovold <johan@kernel.org>

commit 73d908ca212dfa4a27e6ba4c272a651c979594d4
Author: Kyle McMartin <kyle@redhat.com>
Date:   Wed Nov 12 21:07:44 2014 +0000

    arm64: __clear_user: handle exceptions on strb
    
    commit 97fc15436b36ee3956efad83e22a557991f7d19d upstream.
    
    ARM64 currently doesn't fix up faults on the single-byte (strb) case of
    __clear_user... which means that we can cause a nasty kernel panic as an
    ordinary user with any multiple PAGE_SIZE+1 read from /dev/zero.
    i.e.: dd if=/dev/zero of=foo ibs=1 count=1 (or ibs=65537, etc.)
    
    This is a pretty obscure bug in the general case since we'll only
    __do_kernel_fault (since there's no extable entry for pc) if the
    mmap_sem is contended. However, with CONFIG_DEBUG_VM enabled, we'll
    always fault.
    
    if (!down_read_trylock(&mm->mmap_sem)) {
            if (!user_mode(regs) && !search_exception_tables(regs->pc))
                    goto no_context;
    retry:
            down_read(&mm->mmap_sem);
    } else {
            /*
             * The above down_read_trylock() might have succeeded in
             * which
             * case, we'll have missed the might_sleep() from
             * down_read().
             */
            might_sleep();
            if (!user_mode(regs) && !search_exception_tables(regs->pc))
                    goto no_context;
    }
    
    Fix that by adding an extable entry for the strb instruction, since it
    touches user memory, similar to the other stores in __clear_user.
    
    Signed-off-by: Kyle McMartin <kyle@redhat.com>
    Reported-by: Miloš Prchlík <mprchlik@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit 0000e34c1c04e7b08140768deaa91cf878ab4a19
Author: Kyle McMartin <kyle@redhat.com>
Date:   Wed Nov 12 21:07:44 2014 +0000

    arm64: __clear_user: handle exceptions on strb
    
    commit 97fc15436b36ee3956efad83e22a557991f7d19d upstream.
    
    ARM64 currently doesn't fix up faults on the single-byte (strb) case of
    __clear_user... which means that we can cause a nasty kernel panic as an
    ordinary user with any multiple PAGE_SIZE+1 read from /dev/zero.
    i.e.: dd if=/dev/zero of=foo ibs=1 count=1 (or ibs=65537, etc.)
    
    This is a pretty obscure bug in the general case since we'll only
    __do_kernel_fault (since there's no extable entry for pc) if the
    mmap_sem is contended. However, with CONFIG_DEBUG_VM enabled, we'll
    always fault.
    
    if (!down_read_trylock(&mm->mmap_sem)) {
            if (!user_mode(regs) && !search_exception_tables(regs->pc))
                    goto no_context;
    retry:
            down_read(&mm->mmap_sem);
    } else {
            /*
             * The above down_read_trylock() might have succeeded in
             * which
             * case, we'll have missed the might_sleep() from
             * down_read().
             */
            might_sleep();
            if (!user_mode(regs) && !search_exception_tables(regs->pc))
                    goto no_context;
    }
    
    Fix that by adding an extable entry for the strb instruction, since it
    touches user memory, similar to the other stores in __clear_user.
    
    Signed-off-by: Kyle McMartin <kyle@redhat.com>
    Reported-by: Miloš Prchlík <mprchlik@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e6bed540241ac74b6c30688ba9411b1f7b8d96fa
Author: Dave Chinner <david@fromorbit.com>
Date:   Wed Jun 4 16:10:46 2014 -0700

    fs/superblock: unregister sb shrinker before ->kill_sb()
    
    commit 28f2cd4f6da24a1aa06c226618ed5ad69e13df64 upstream.
    
    This series is aimed at regressions noticed during reclaim activity.  The
    first two patches are shrinker patches that were posted ages ago but never
    merged for reasons that are unclear to me.  I'm posting them again to see
    if there was a reason they were dropped or if they just got lost.  Dave?
    Time?  The last patch adjusts proportional reclaim.  Yuanhan Liu, can you
    retest the vm scalability test cases on a larger machine?  Hugh, does this
    work for you on the memcg test cases?
    
    Based on ext4, I get the following results but unfortunately my larger
    test machines are all unavailable so this is based on a relatively small
    machine.
    
    postmark
                                      3.15.0-rc5            3.15.0-rc5
                                         vanilla       proportion-v1r4
    Ops/sec Transactions         21.00 (  0.00%)       25.00 ( 19.05%)
    Ops/sec FilesCreate          39.00 (  0.00%)       45.00 ( 15.38%)
    Ops/sec CreateTransact       10.00 (  0.00%)       12.00 ( 20.00%)
    Ops/sec FilesDeleted       6202.00 (  0.00%)     6202.00 (  0.00%)
    Ops/sec DeleteTransact       11.00 (  0.00%)       12.00 (  9.09%)
    Ops/sec DataRead/MB          25.97 (  0.00%)       30.02 ( 15.59%)
    Ops/sec DataWrite/MB         49.99 (  0.00%)       57.78 ( 15.58%)
    
    ffsb (mail server simulator)
                                     3.15.0-rc5             3.15.0-rc5
                                        vanilla        proportion-v1r4
    Ops/sec readall           9402.63 (  0.00%)      9805.74 (  4.29%)
    Ops/sec create            4695.45 (  0.00%)      4781.39 (  1.83%)
    Ops/sec delete             173.72 (  0.00%)       177.23 (  2.02%)
    Ops/sec Transactions     14271.80 (  0.00%)     14764.37 (  3.45%)
    Ops/sec Read                37.00 (  0.00%)        38.50 (  4.05%)
    Ops/sec Write               18.20 (  0.00%)        18.50 (  1.65%)
    
    dd of a large file
                                    3.15.0-rc5            3.15.0-rc5
                                       vanilla       proportion-v1r4
    WallTime DownloadTar       75.00 (  0.00%)       61.00 ( 18.67%)
    WallTime DD               423.00 (  0.00%)      401.00 (  5.20%)
    WallTime Delete             2.00 (  0.00%)        5.00 (-150.00%)
    
    stutter (times mmap latency during large amounts of IO)
    
                                3.15.0-rc5            3.15.0-rc5
                                   vanilla       proportion-v1r4
    Unit >5ms Delays  80252.0000 (  0.00%)  81523.0000 ( -1.58%)
    Unit Mmap min         8.2118 (  0.00%)      8.3206 ( -1.33%)
    Unit Mmap mean       17.4614 (  0.00%)     17.2868 (  1.00%)
    Unit Mmap stddev     24.9059 (  0.00%)     34.6771 (-39.23%)
    Unit Mmap max      2811.6433 (  0.00%)   2645.1398 (  5.92%)
    Unit Mmap 90%        20.5098 (  0.00%)     18.3105 ( 10.72%)
    Unit Mmap 93%        22.9180 (  0.00%)     20.1751 ( 11.97%)
    Unit Mmap 95%        25.2114 (  0.00%)     22.4988 ( 10.76%)
    Unit Mmap 99%        46.1430 (  0.00%)     43.5952 (  5.52%)
    Unit Ideal  Tput     85.2623 (  0.00%)     78.8906 (  7.47%)
    Unit Tput min        44.0666 (  0.00%)     43.9609 (  0.24%)
    Unit Tput mean       45.5646 (  0.00%)     45.2009 (  0.80%)
    Unit Tput stddev      0.9318 (  0.00%)      1.1084 (-18.95%)
    Unit Tput max        46.7375 (  0.00%)     46.7539 ( -0.04%)
    
    This patch (of 3):
    
    We will like to unregister the sb shrinker before ->kill_sb().  This will
    allow cached objects to be counted without call to grab_super_passive() to
    update ref count on sb.  We want to avoid locking during memory
    reclamation especially when we are skipping the memory reclaim when we are
    out of cached objects.
    
    This is safe because grab_super_passive does a try-lock on the
    sb->s_umount now, and so if we are in the unmount process, it won't ever
    block.  That means what used to be a deadlock and races we were avoiding
    by using grab_super_passive() is now:
    
            shrinker                        umount
    
            down_read(shrinker_rwsem)
                                            down_write(sb->s_umount)
                                            shrinker_unregister
                                              down_write(shrinker_rwsem)
                                                <blocks>
            grab_super_passive(sb)
              down_read_trylock(sb->s_umount)
                <fails>
            <shrinker aborts>
            ....
            <shrinkers finish running>
            up_read(shrinker_rwsem)
                                              <unblocks>
                                              <removes shrinker>
                                              up_write(shrinker_rwsem)
                                            ->kill_sb()
                                            ....
    
    So it is safe to deregister the shrinker before ->kill_sb().
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Tested-by: Yuanhan Liu <yuanhan.liu@linux.intel.com>
    Cc: Bob Liu <bob.liu@oracle.com>
    Cc: Jan Kara <jack@suse.cz>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c6f8075d3934e493980fe83f8a746d74b98f5e51
Author: Kyle McMartin <kyle@redhat.com>
Date:   Wed Nov 12 21:07:44 2014 +0000

    arm64: __clear_user: handle exceptions on strb
    
    commit 97fc15436b36ee3956efad83e22a557991f7d19d upstream.
    
    ARM64 currently doesn't fix up faults on the single-byte (strb) case of
    __clear_user... which means that we can cause a nasty kernel panic as an
    ordinary user with any multiple PAGE_SIZE+1 read from /dev/zero.
    i.e.: dd if=/dev/zero of=foo ibs=1 count=1 (or ibs=65537, etc.)
    
    This is a pretty obscure bug in the general case since we'll only
    __do_kernel_fault (since there's no extable entry for pc) if the
    mmap_sem is contended. However, with CONFIG_DEBUG_VM enabled, we'll
    always fault.
    
    if (!down_read_trylock(&mm->mmap_sem)) {
            if (!user_mode(regs) && !search_exception_tables(regs->pc))
                    goto no_context;
    retry:
            down_read(&mm->mmap_sem);
    } else {
            /*
             * The above down_read_trylock() might have succeeded in
             * which
             * case, we'll have missed the might_sleep() from
             * down_read().
             */
            might_sleep();
            if (!user_mode(regs) && !search_exception_tables(regs->pc))
                    goto no_context;
    }
    
    Fix that by adding an extable entry for the strb instruction, since it
    touches user memory, similar to the other stores in __clear_user.
    
    Signed-off-by: Kyle McMartin <kyle@redhat.com>
    Reported-by: Miloš Prchlík <mprchlik@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 16640ca660f4980fb5c1f4e4febce19875f4c1b8
Author: Kyle McMartin <kyle@redhat.com>
Date:   Wed Nov 12 21:07:44 2014 +0000

    arm64: __clear_user: handle exceptions on strb
    
    commit 97fc15436b36ee3956efad83e22a557991f7d19d upstream.
    
    ARM64 currently doesn't fix up faults on the single-byte (strb) case of
    __clear_user... which means that we can cause a nasty kernel panic as an
    ordinary user with any multiple PAGE_SIZE+1 read from /dev/zero.
    i.e.: dd if=/dev/zero of=foo ibs=1 count=1 (or ibs=65537, etc.)
    
    This is a pretty obscure bug in the general case since we'll only
    __do_kernel_fault (since there's no extable entry for pc) if the
    mmap_sem is contended. However, with CONFIG_DEBUG_VM enabled, we'll
    always fault.
    
    if (!down_read_trylock(&mm->mmap_sem)) {
            if (!user_mode(regs) && !search_exception_tables(regs->pc))
                    goto no_context;
    retry:
            down_read(&mm->mmap_sem);
    } else {
            /*
             * The above down_read_trylock() might have succeeded in
             * which
             * case, we'll have missed the might_sleep() from
             * down_read().
             */
            might_sleep();
            if (!user_mode(regs) && !search_exception_tables(regs->pc))
                    goto no_context;
    }
    
    Fix that by adding an extable entry for the strb instruction, since it
    touches user memory, similar to the other stores in __clear_user.
    
    Signed-off-by: Kyle McMartin <kyle@redhat.com>
    Reported-by: Miloš Prchlík <mprchlik@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit beb762ba2a940ce1f87a03b1c5820ce2d433db6e
Author: Kyle McMartin <kyle@redhat.com>
Date:   Wed Nov 12 21:07:44 2014 +0000

    arm64: __clear_user: handle exceptions on strb
    
    commit 97fc15436b36ee3956efad83e22a557991f7d19d upstream.
    
    ARM64 currently doesn't fix up faults on the single-byte (strb) case of
    __clear_user... which means that we can cause a nasty kernel panic as an
    ordinary user with any multiple PAGE_SIZE+1 read from /dev/zero.
    i.e.: dd if=/dev/zero of=foo ibs=1 count=1 (or ibs=65537, etc.)
    
    This is a pretty obscure bug in the general case since we'll only
    __do_kernel_fault (since there's no extable entry for pc) if the
    mmap_sem is contended. However, with CONFIG_DEBUG_VM enabled, we'll
    always fault.
    
    if (!down_read_trylock(&mm->mmap_sem)) {
            if (!user_mode(regs) && !search_exception_tables(regs->pc))
                    goto no_context;
    retry:
            down_read(&mm->mmap_sem);
    } else {
            /*
             * The above down_read_trylock() might have succeeded in
             * which
             * case, we'll have missed the might_sleep() from
             * down_read().
             */
            might_sleep();
            if (!user_mode(regs) && !search_exception_tables(regs->pc))
                    goto no_context;
    }
    
    Fix that by adding an extable entry for the strb instruction, since it
    touches user memory, similar to the other stores in __clear_user.
    
    Signed-off-by: Kyle McMartin <kyle@redhat.com>
    Reported-by: Miloš Prchlík <mprchlik@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit 97fc15436b36ee3956efad83e22a557991f7d19d
Author: Kyle McMartin <kyle@redhat.com>
Date:   Wed Nov 12 21:07:44 2014 +0000

    arm64: __clear_user: handle exceptions on strb
    
    ARM64 currently doesn't fix up faults on the single-byte (strb) case of
    __clear_user... which means that we can cause a nasty kernel panic as an
    ordinary user with any multiple PAGE_SIZE+1 read from /dev/zero.
    i.e.: dd if=/dev/zero of=foo ibs=1 count=1 (or ibs=65537, etc.)
    
    This is a pretty obscure bug in the general case since we'll only
    __do_kernel_fault (since there's no extable entry for pc) if the
    mmap_sem is contended. However, with CONFIG_DEBUG_VM enabled, we'll
    always fault.
    
    if (!down_read_trylock(&mm->mmap_sem)) {
            if (!user_mode(regs) && !search_exception_tables(regs->pc))
                    goto no_context;
    retry:
            down_read(&mm->mmap_sem);
    } else {
            /*
             * The above down_read_trylock() might have succeeded in
             * which
             * case, we'll have missed the might_sleep() from
             * down_read().
             */
            might_sleep();
            if (!user_mode(regs) && !search_exception_tables(regs->pc))
                    goto no_context;
    }
    
    Fix that by adding an extable entry for the strb instruction, since it
    touches user memory, similar to the other stores in __clear_user.
    
    Signed-off-by: Kyle McMartin <kyle@redhat.com>
    Reported-by: Miloš Prchlík <mprchlik@redhat.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

commit 2f11e3a821aff1a50c8ebc2fadbeb22c1ea1adbd
Author: Dave Chinner <david@fromorbit.com>
Date:   Thu Aug 28 19:35:11 2014 +0100

    fs/superblock: unregister sb shrinker before ->kill_sb()
    
    commit 28f2cd4f6da24a1aa06c226618ed5ad69e13df64 upstream.
    
    This series is aimed at regressions noticed during reclaim activity.  The
    first two patches are shrinker patches that were posted ages ago but never
    merged for reasons that are unclear to me.  I'm posting them again to see
    if there was a reason they were dropped or if they just got lost.  Dave?
    Time?  The last patch adjusts proportional reclaim.  Yuanhan Liu, can you
    retest the vm scalability test cases on a larger machine?  Hugh, does this
    work for you on the memcg test cases?
    
    Based on ext4, I get the following results but unfortunately my larger
    test machines are all unavailable so this is based on a relatively small
    machine.
    
    postmark
                                      3.15.0-rc5            3.15.0-rc5
                                         vanilla       proportion-v1r4
    Ops/sec Transactions         21.00 (  0.00%)       25.00 ( 19.05%)
    Ops/sec FilesCreate          39.00 (  0.00%)       45.00 ( 15.38%)
    Ops/sec CreateTransact       10.00 (  0.00%)       12.00 ( 20.00%)
    Ops/sec FilesDeleted       6202.00 (  0.00%)     6202.00 (  0.00%)
    Ops/sec DeleteTransact       11.00 (  0.00%)       12.00 (  9.09%)
    Ops/sec DataRead/MB          25.97 (  0.00%)       30.02 ( 15.59%)
    Ops/sec DataWrite/MB         49.99 (  0.00%)       57.78 ( 15.58%)
    
    ffsb (mail server simulator)
                                     3.15.0-rc5             3.15.0-rc5
                                        vanilla        proportion-v1r4
    Ops/sec readall           9402.63 (  0.00%)      9805.74 (  4.29%)
    Ops/sec create            4695.45 (  0.00%)      4781.39 (  1.83%)
    Ops/sec delete             173.72 (  0.00%)       177.23 (  2.02%)
    Ops/sec Transactions     14271.80 (  0.00%)     14764.37 (  3.45%)
    Ops/sec Read                37.00 (  0.00%)        38.50 (  4.05%)
    Ops/sec Write               18.20 (  0.00%)        18.50 (  1.65%)
    
    dd of a large file
                                    3.15.0-rc5            3.15.0-rc5
                                       vanilla       proportion-v1r4
    WallTime DownloadTar       75.00 (  0.00%)       61.00 ( 18.67%)
    WallTime DD               423.00 (  0.00%)      401.00 (  5.20%)
    WallTime Delete             2.00 (  0.00%)        5.00 (-150.00%)
    
    stutter (times mmap latency during large amounts of IO)
    
                                3.15.0-rc5            3.15.0-rc5
                                   vanilla       proportion-v1r4
    Unit >5ms Delays  80252.0000 (  0.00%)  81523.0000 ( -1.58%)
    Unit Mmap min         8.2118 (  0.00%)      8.3206 ( -1.33%)
    Unit Mmap mean       17.4614 (  0.00%)     17.2868 (  1.00%)
    Unit Mmap stddev     24.9059 (  0.00%)     34.6771 (-39.23%)
    Unit Mmap max      2811.6433 (  0.00%)   2645.1398 (  5.92%)
    Unit Mmap 90%        20.5098 (  0.00%)     18.3105 ( 10.72%)
    Unit Mmap 93%        22.9180 (  0.00%)     20.1751 ( 11.97%)
    Unit Mmap 95%        25.2114 (  0.00%)     22.4988 ( 10.76%)
    Unit Mmap 99%        46.1430 (  0.00%)     43.5952 (  5.52%)
    Unit Ideal  Tput     85.2623 (  0.00%)     78.8906 (  7.47%)
    Unit Tput min        44.0666 (  0.00%)     43.9609 (  0.24%)
    Unit Tput mean       45.5646 (  0.00%)     45.2009 (  0.80%)
    Unit Tput stddev      0.9318 (  0.00%)      1.1084 (-18.95%)
    Unit Tput max        46.7375 (  0.00%)     46.7539 ( -0.04%)
    
    This patch (of 3):
    
    We will like to unregister the sb shrinker before ->kill_sb().  This will
    allow cached objects to be counted without call to grab_super_passive() to
    update ref count on sb.  We want to avoid locking during memory
    reclamation especially when we are skipping the memory reclaim when we are
    out of cached objects.
    
    This is safe because grab_super_passive does a try-lock on the
    sb->s_umount now, and so if we are in the unmount process, it won't ever
    block.  That means what used to be a deadlock and races we were avoiding
    by using grab_super_passive() is now:
    
            shrinker                        umount
    
            down_read(shrinker_rwsem)
                                            down_write(sb->s_umount)
                                            shrinker_unregister
                                              down_write(shrinker_rwsem)
                                                <blocks>
            grab_super_passive(sb)
              down_read_trylock(sb->s_umount)
                <fails>
            <shrinker aborts>
            ....
            <shrinkers finish running>
            up_read(shrinker_rwsem)
                                              <unblocks>
                                              <removes shrinker>
                                              up_write(shrinker_rwsem)
                                            ->kill_sb()
                                            ....
    
    So it is safe to deregister the shrinker before ->kill_sb().
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Tested-by: Yuanhan Liu <yuanhan.liu@linux.intel.com>
    Cc: Bob Liu <bob.liu@oracle.com>
    Cc: Jan Kara <jack@suse.cz>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit d5f32654455c75265f46e2c2ff1cdcadd3ebbabb
Author: Takashi Iwai <tiwai@suse.de>
Date:   Tue Jul 15 08:51:27 2014 +0200

    PM / sleep: Fix request_firmware() error at resume
    
    commit 4320f6b1d9db4ca912c5eb6ecb328b2e090e1586 upstream.
    
    The commit [247bc037: PM / Sleep: Mitigate race between the freezer
    and request_firmware()] introduced the finer state control, but it
    also leads to a new bug; for example, a bug report regarding the
    firmware loading of intel BT device at suspend/resume:
      https://bugzilla.novell.com/show_bug.cgi?id=873790
    
    The root cause seems to be a small window between the process resume
    and the clear of usermodehelper lock.  The request_firmware() function
    checks the UMH lock and gives up when it's in UMH_DISABLE state.  This
    is for avoiding the invalid  f/w loading during suspend/resume phase.
    The problem is, however, that usermodehelper_enable() is called at the
    end of thaw_processes().  Thus, a thawed process in between can kick
    off the f/w loader code path (in this case, via btusb_setup_intel())
    even before the call of usermodehelper_enable().  Then
    usermodehelper_read_trylock() returns an error and request_firmware()
    spews WARN_ON() in the end.
    
    This oneliner patch fixes the issue just by setting to UMH_FREEZING
    state again before restarting tasks, so that the call of
    request_firmware() will be blocked until the end of this function
    instead of returning an error.
    
    Fixes: 247bc0374254 (PM / Sleep: Mitigate race between the freezer and request_firmware())
    Link: https://bugzilla.novell.com/show_bug.cgi?id=873790
    Signed-off-by: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit e02f84ab62e11b1d270e24fb0387cd353a3e9601
Author: Takashi Iwai <tiwai@suse.de>
Date:   Tue Jul 15 08:51:27 2014 +0200

    PM / sleep: Fix request_firmware() error at resume
    
    commit 4320f6b1d9db4ca912c5eb6ecb328b2e090e1586 upstream.
    
    The commit [247bc037: PM / Sleep: Mitigate race between the freezer
    and request_firmware()] introduced the finer state control, but it
    also leads to a new bug; for example, a bug report regarding the
    firmware loading of intel BT device at suspend/resume:
      https://bugzilla.novell.com/show_bug.cgi?id=873790
    
    The root cause seems to be a small window between the process resume
    and the clear of usermodehelper lock.  The request_firmware() function
    checks the UMH lock and gives up when it's in UMH_DISABLE state.  This
    is for avoiding the invalid  f/w loading during suspend/resume phase.
    The problem is, however, that usermodehelper_enable() is called at the
    end of thaw_processes().  Thus, a thawed process in between can kick
    off the f/w loader code path (in this case, via btusb_setup_intel())
    even before the call of usermodehelper_enable().  Then
    usermodehelper_read_trylock() returns an error and request_firmware()
    spews WARN_ON() in the end.
    
    This oneliner patch fixes the issue just by setting to UMH_FREEZING
    state again before restarting tasks, so that the call of
    request_firmware() will be blocked until the end of this function
    instead of returning an error.
    
    Fixes: 247bc0374254 (PM / Sleep: Mitigate race between the freezer and request_firmware())
    Link: https://bugzilla.novell.com/show_bug.cgi?id=873790
    Signed-off-by: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9abefe6933601246129da98d6c16936547a5cc8e
Author: Takashi Iwai <tiwai@suse.de>
Date:   Tue Jul 15 08:51:27 2014 +0200

    PM / sleep: Fix request_firmware() error at resume
    
    commit 4320f6b1d9db4ca912c5eb6ecb328b2e090e1586 upstream.
    
    The commit [247bc037: PM / Sleep: Mitigate race between the freezer
    and request_firmware()] introduced the finer state control, but it
    also leads to a new bug; for example, a bug report regarding the
    firmware loading of intel BT device at suspend/resume:
      https://bugzilla.novell.com/show_bug.cgi?id=873790
    
    The root cause seems to be a small window between the process resume
    and the clear of usermodehelper lock.  The request_firmware() function
    checks the UMH lock and gives up when it's in UMH_DISABLE state.  This
    is for avoiding the invalid  f/w loading during suspend/resume phase.
    The problem is, however, that usermodehelper_enable() is called at the
    end of thaw_processes().  Thus, a thawed process in between can kick
    off the f/w loader code path (in this case, via btusb_setup_intel())
    even before the call of usermodehelper_enable().  Then
    usermodehelper_read_trylock() returns an error and request_firmware()
    spews WARN_ON() in the end.
    
    This oneliner patch fixes the issue just by setting to UMH_FREEZING
    state again before restarting tasks, so that the call of
    request_firmware() will be blocked until the end of this function
    instead of returning an error.
    
    Fixes: 247bc0374254 (PM / Sleep: Mitigate race between the freezer and request_firmware())
    Link: https://bugzilla.novell.com/show_bug.cgi?id=873790
    Signed-off-by: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 804536e8e033d7917a1384b89d1e29a3457ec429
Author: Takashi Iwai <tiwai@suse.de>
Date:   Tue Jul 15 08:51:27 2014 +0200

    PM / sleep: Fix request_firmware() error at resume
    
    commit 4320f6b1d9db4ca912c5eb6ecb328b2e090e1586 upstream.
    
    The commit [247bc037: PM / Sleep: Mitigate race between the freezer
    and request_firmware()] introduced the finer state control, but it
    also leads to a new bug; for example, a bug report regarding the
    firmware loading of intel BT device at suspend/resume:
      https://bugzilla.novell.com/show_bug.cgi?id=873790
    
    The root cause seems to be a small window between the process resume
    and the clear of usermodehelper lock.  The request_firmware() function
    checks the UMH lock and gives up when it's in UMH_DISABLE state.  This
    is for avoiding the invalid  f/w loading during suspend/resume phase.
    The problem is, however, that usermodehelper_enable() is called at the
    end of thaw_processes().  Thus, a thawed process in between can kick
    off the f/w loader code path (in this case, via btusb_setup_intel())
    even before the call of usermodehelper_enable().  Then
    usermodehelper_read_trylock() returns an error and request_firmware()
    spews WARN_ON() in the end.
    
    This oneliner patch fixes the issue just by setting to UMH_FREEZING
    state again before restarting tasks, so that the call of
    request_firmware() will be blocked until the end of this function
    instead of returning an error.
    
    Fixes: 247bc0374254 (PM / Sleep: Mitigate race between the freezer and request_firmware())
    Link: https://bugzilla.novell.com/show_bug.cgi?id=873790
    Signed-off-by: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5dc1c8851364ffb79cb9403f72c712ee83cce755
Author: Takashi Iwai <tiwai@suse.de>
Date:   Tue Jul 15 08:51:27 2014 +0200

    PM / sleep: Fix request_firmware() error at resume
    
    commit 4320f6b1d9db4ca912c5eb6ecb328b2e090e1586 upstream.
    
    The commit [247bc037: PM / Sleep: Mitigate race between the freezer
    and request_firmware()] introduced the finer state control, but it
    also leads to a new bug; for example, a bug report regarding the
    firmware loading of intel BT device at suspend/resume:
      https://bugzilla.novell.com/show_bug.cgi?id=873790
    
    The root cause seems to be a small window between the process resume
    and the clear of usermodehelper lock.  The request_firmware() function
    checks the UMH lock and gives up when it's in UMH_DISABLE state.  This
    is for avoiding the invalid  f/w loading during suspend/resume phase.
    The problem is, however, that usermodehelper_enable() is called at the
    end of thaw_processes().  Thus, a thawed process in between can kick
    off the f/w loader code path (in this case, via btusb_setup_intel())
    even before the call of usermodehelper_enable().  Then
    usermodehelper_read_trylock() returns an error and request_firmware()
    spews WARN_ON() in the end.
    
    This oneliner patch fixes the issue just by setting to UMH_FREEZING
    state again before restarting tasks, so that the call of
    request_firmware() will be blocked until the end of this function
    instead of returning an error.
    
    Fixes: 247bc0374254 (PM / Sleep: Mitigate race between the freezer and request_firmware())
    Link: https://bugzilla.novell.com/show_bug.cgi?id=873790
    Signed-off-by: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4320f6b1d9db4ca912c5eb6ecb328b2e090e1586
Author: Takashi Iwai <tiwai@suse.de>
Date:   Tue Jul 15 08:51:27 2014 +0200

    PM / sleep: Fix request_firmware() error at resume
    
    The commit [247bc037: PM / Sleep: Mitigate race between the freezer
    and request_firmware()] introduced the finer state control, but it
    also leads to a new bug; for example, a bug report regarding the
    firmware loading of intel BT device at suspend/resume:
      https://bugzilla.novell.com/show_bug.cgi?id=873790
    
    The root cause seems to be a small window between the process resume
    and the clear of usermodehelper lock.  The request_firmware() function
    checks the UMH lock and gives up when it's in UMH_DISABLE state.  This
    is for avoiding the invalid  f/w loading during suspend/resume phase.
    The problem is, however, that usermodehelper_enable() is called at the
    end of thaw_processes().  Thus, a thawed process in between can kick
    off the f/w loader code path (in this case, via btusb_setup_intel())
    even before the call of usermodehelper_enable().  Then
    usermodehelper_read_trylock() returns an error and request_firmware()
    spews WARN_ON() in the end.
    
    This oneliner patch fixes the issue just by setting to UMH_FREEZING
    state again before restarting tasks, so that the call of
    request_firmware() will be blocked until the end of this function
    instead of returning an error.
    
    Fixes: 247bc0374254 (PM / Sleep: Mitigate race between the freezer and request_firmware())
    Link: https://bugzilla.novell.com/show_bug.cgi?id=873790
    Cc: 3.4+ <stable@vger.kernel.org> # 3.4+
    Signed-off-by: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 28f2cd4f6da24a1aa06c226618ed5ad69e13df64
Author: Dave Chinner <david@fromorbit.com>
Date:   Wed Jun 4 16:10:46 2014 -0700

    fs/superblock: unregister sb shrinker before ->kill_sb()
    
    This series is aimed at regressions noticed during reclaim activity.  The
    first two patches are shrinker patches that were posted ages ago but never
    merged for reasons that are unclear to me.  I'm posting them again to see
    if there was a reason they were dropped or if they just got lost.  Dave?
    Time?  The last patch adjusts proportional reclaim.  Yuanhan Liu, can you
    retest the vm scalability test cases on a larger machine?  Hugh, does this
    work for you on the memcg test cases?
    
    Based on ext4, I get the following results but unfortunately my larger
    test machines are all unavailable so this is based on a relatively small
    machine.
    
    postmark
                                      3.15.0-rc5            3.15.0-rc5
                                         vanilla       proportion-v1r4
    Ops/sec Transactions         21.00 (  0.00%)       25.00 ( 19.05%)
    Ops/sec FilesCreate          39.00 (  0.00%)       45.00 ( 15.38%)
    Ops/sec CreateTransact       10.00 (  0.00%)       12.00 ( 20.00%)
    Ops/sec FilesDeleted       6202.00 (  0.00%)     6202.00 (  0.00%)
    Ops/sec DeleteTransact       11.00 (  0.00%)       12.00 (  9.09%)
    Ops/sec DataRead/MB          25.97 (  0.00%)       30.02 ( 15.59%)
    Ops/sec DataWrite/MB         49.99 (  0.00%)       57.78 ( 15.58%)
    
    ffsb (mail server simulator)
                                     3.15.0-rc5             3.15.0-rc5
                                        vanilla        proportion-v1r4
    Ops/sec readall           9402.63 (  0.00%)      9805.74 (  4.29%)
    Ops/sec create            4695.45 (  0.00%)      4781.39 (  1.83%)
    Ops/sec delete             173.72 (  0.00%)       177.23 (  2.02%)
    Ops/sec Transactions     14271.80 (  0.00%)     14764.37 (  3.45%)
    Ops/sec Read                37.00 (  0.00%)        38.50 (  4.05%)
    Ops/sec Write               18.20 (  0.00%)        18.50 (  1.65%)
    
    dd of a large file
                                    3.15.0-rc5            3.15.0-rc5
                                       vanilla       proportion-v1r4
    WallTime DownloadTar       75.00 (  0.00%)       61.00 ( 18.67%)
    WallTime DD               423.00 (  0.00%)      401.00 (  5.20%)
    WallTime Delete             2.00 (  0.00%)        5.00 (-150.00%)
    
    stutter (times mmap latency during large amounts of IO)
    
                                3.15.0-rc5            3.15.0-rc5
                                   vanilla       proportion-v1r4
    Unit >5ms Delays  80252.0000 (  0.00%)  81523.0000 ( -1.58%)
    Unit Mmap min         8.2118 (  0.00%)      8.3206 ( -1.33%)
    Unit Mmap mean       17.4614 (  0.00%)     17.2868 (  1.00%)
    Unit Mmap stddev     24.9059 (  0.00%)     34.6771 (-39.23%)
    Unit Mmap max      2811.6433 (  0.00%)   2645.1398 (  5.92%)
    Unit Mmap 90%        20.5098 (  0.00%)     18.3105 ( 10.72%)
    Unit Mmap 93%        22.9180 (  0.00%)     20.1751 ( 11.97%)
    Unit Mmap 95%        25.2114 (  0.00%)     22.4988 ( 10.76%)
    Unit Mmap 99%        46.1430 (  0.00%)     43.5952 (  5.52%)
    Unit Ideal  Tput     85.2623 (  0.00%)     78.8906 (  7.47%)
    Unit Tput min        44.0666 (  0.00%)     43.9609 (  0.24%)
    Unit Tput mean       45.5646 (  0.00%)     45.2009 (  0.80%)
    Unit Tput stddev      0.9318 (  0.00%)      1.1084 (-18.95%)
    Unit Tput max        46.7375 (  0.00%)     46.7539 ( -0.04%)
    
    This patch (of 3):
    
    We will like to unregister the sb shrinker before ->kill_sb().  This will
    allow cached objects to be counted without call to grab_super_passive() to
    update ref count on sb.  We want to avoid locking during memory
    reclamation especially when we are skipping the memory reclaim when we are
    out of cached objects.
    
    This is safe because grab_super_passive does a try-lock on the
    sb->s_umount now, and so if we are in the unmount process, it won't ever
    block.  That means what used to be a deadlock and races we were avoiding
    by using grab_super_passive() is now:
    
            shrinker                        umount
    
            down_read(shrinker_rwsem)
                                            down_write(sb->s_umount)
                                            shrinker_unregister
                                              down_write(shrinker_rwsem)
                                                <blocks>
            grab_super_passive(sb)
              down_read_trylock(sb->s_umount)
                <fails>
            <shrinker aborts>
            ....
            <shrinkers finish running>
            up_read(shrinker_rwsem)
                                              <unblocks>
                                              <removes shrinker>
                                              up_write(shrinker_rwsem)
                                            ->kill_sb()
                                            ....
    
    So it is safe to deregister the shrinker before ->kill_sb().
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Tested-by: Yuanhan Liu <yuanhan.liu@linux.intel.com>
    Cc: Bob Liu <bob.liu@oracle.com>
    Cc: Jan Kara <jack@suse.cz>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 5162fef0ca711d3d467b6cf181219e43100d7122
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Feb 11 14:50:01 2014 -0500

    ftrace: Have function graph only trace based on global_ops filters
    
    commit 23a8e8441a0a74dd612edf81dc89d1600bc0a3d1 upstream.
    
    Doing some different tests, I discovered that function graph tracing, when
    filtered via the set_ftrace_filter and set_ftrace_notrace files, does
    not always keep with them if another function ftrace_ops is registered
    to trace functions.
    
    The reason is that function graph just happens to trace all functions
    that the function tracer enables. When there was only one user of
    function tracing, the function graph tracer did not need to worry about
    being called by functions that it did not want to trace. But now that there
    are other users, this becomes a problem.
    
    For example, one just needs to do the following:
    
     # cd /sys/kernel/debug/tracing
     # echo schedule > set_ftrace_filter
     # echo function_graph > current_tracer
     # cat trace
    [..]
     0)               |  schedule() {
     ------------------------------------------
     0)    <idle>-0    =>   rcu_pre-7
     ------------------------------------------
    
     0) ! 2980.314 us |  }
     0)               |  schedule() {
     ------------------------------------------
     0)   rcu_pre-7    =>    <idle>-0
     ------------------------------------------
    
     0) + 20.701 us   |  }
    
     # echo 1 > /proc/sys/kernel/stack_tracer_enabled
     # cat trace
    [..]
     1) + 20.825 us   |      }
     1) + 21.651 us   |    }
     1) + 30.924 us   |  } /* SyS_ioctl */
     1)               |  do_page_fault() {
     1)               |    __do_page_fault() {
     1)   0.274 us    |      down_read_trylock();
     1)   0.098 us    |      find_vma();
     1)               |      handle_mm_fault() {
     1)               |        _raw_spin_lock() {
     1)   0.102 us    |          preempt_count_add();
     1)   0.097 us    |          do_raw_spin_lock();
     1)   2.173 us    |        }
     1)               |        do_wp_page() {
     1)   0.079 us    |          vm_normal_page();
     1)   0.086 us    |          reuse_swap_page();
     1)   0.076 us    |          page_move_anon_rmap();
     1)               |          unlock_page() {
     1)   0.082 us    |            page_waitqueue();
     1)   0.086 us    |            __wake_up_bit();
     1)   1.801 us    |          }
     1)   0.075 us    |          ptep_set_access_flags();
     1)               |          _raw_spin_unlock() {
     1)   0.098 us    |            do_raw_spin_unlock();
     1)   0.105 us    |            preempt_count_sub();
     1)   1.884 us    |          }
     1)   9.149 us    |        }
     1) + 13.083 us   |      }
     1)   0.146 us    |      up_read();
    
    When the stack tracer was enabled, it enabled all functions to be traced, which
    now the function graph tracer also traces. This is a side effect that should
    not occur.
    
    To fix this a test is added when the function tracing is changed, as well as when
    the graph tracer is enabled, to see if anything other than the ftrace global_ops
    function tracer is enabled. If so, then the graph tracer calls a test trampoline
    that will look at the function that is being traced and compare it with the
    filters defined by the global_ops.
    
    As an optimization, if there's no other function tracers registered, or if
    the only registered function tracers also use the global ops, the function
    graph infrastructure will call the registered function graph callback directly
    and not go through the test trampoline.
    
    Fixes: d2d45c7a03a2 "tracing: Have stack_tracer use a separate list of functions"
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 1c2bd0db1189643691557ff34406906b053cef92
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Feb 11 14:50:01 2014 -0500

    ftrace: Have function graph only trace based on global_ops filters
    
    commit 23a8e8441a0a74dd612edf81dc89d1600bc0a3d1 upstream.
    
    Doing some different tests, I discovered that function graph tracing, when
    filtered via the set_ftrace_filter and set_ftrace_notrace files, does
    not always keep with them if another function ftrace_ops is registered
    to trace functions.
    
    The reason is that function graph just happens to trace all functions
    that the function tracer enables. When there was only one user of
    function tracing, the function graph tracer did not need to worry about
    being called by functions that it did not want to trace. But now that there
    are other users, this becomes a problem.
    
    For example, one just needs to do the following:
    
     # cd /sys/kernel/debug/tracing
     # echo schedule > set_ftrace_filter
     # echo function_graph > current_tracer
     # cat trace
    [..]
     0)               |  schedule() {
     ------------------------------------------
     0)    <idle>-0    =>   rcu_pre-7
     ------------------------------------------
    
     0) ! 2980.314 us |  }
     0)               |  schedule() {
     ------------------------------------------
     0)   rcu_pre-7    =>    <idle>-0
     ------------------------------------------
    
     0) + 20.701 us   |  }
    
     # echo 1 > /proc/sys/kernel/stack_tracer_enabled
     # cat trace
    [..]
     1) + 20.825 us   |      }
     1) + 21.651 us   |    }
     1) + 30.924 us   |  } /* SyS_ioctl */
     1)               |  do_page_fault() {
     1)               |    __do_page_fault() {
     1)   0.274 us    |      down_read_trylock();
     1)   0.098 us    |      find_vma();
     1)               |      handle_mm_fault() {
     1)               |        _raw_spin_lock() {
     1)   0.102 us    |          preempt_count_add();
     1)   0.097 us    |          do_raw_spin_lock();
     1)   2.173 us    |        }
     1)               |        do_wp_page() {
     1)   0.079 us    |          vm_normal_page();
     1)   0.086 us    |          reuse_swap_page();
     1)   0.076 us    |          page_move_anon_rmap();
     1)               |          unlock_page() {
     1)   0.082 us    |            page_waitqueue();
     1)   0.086 us    |            __wake_up_bit();
     1)   1.801 us    |          }
     1)   0.075 us    |          ptep_set_access_flags();
     1)               |          _raw_spin_unlock() {
     1)   0.098 us    |            do_raw_spin_unlock();
     1)   0.105 us    |            preempt_count_sub();
     1)   1.884 us    |          }
     1)   9.149 us    |        }
     1) + 13.083 us   |      }
     1)   0.146 us    |      up_read();
    
    When the stack tracer was enabled, it enabled all functions to be traced, which
    now the function graph tracer also traces. This is a side effect that should
    not occur.
    
    To fix this a test is added when the function tracing is changed, as well as when
    the graph tracer is enabled, to see if anything other than the ftrace global_ops
    function tracer is enabled. If so, then the graph tracer calls a test trampoline
    that will look at the function that is being traced and compare it with the
    filters defined by the global_ops.
    
    As an optimization, if there's no other function tracers registered, or if
    the only registered function tracers also use the global ops, the function
    graph infrastructure will call the registered function graph callback directly
    and not go through the test trampoline.
    
    Fixes: d2d45c7a03a2 "tracing: Have stack_tracer use a separate list of functions"
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 344adf8bccdd92ff5c23d7d01907ac7ced52138d
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Jan 13 10:30:23 2014 -0500

    ftrace: Have function graph only trace based on global_ops filters
    
    commit 23a8e8441a0a74dd612edf81dc89d1600bc0a3d1 upstream.
    
    Doing some different tests, I discovered that function graph tracing, when
    filtered via the set_ftrace_filter and set_ftrace_notrace files, does
    not always keep with them if another function ftrace_ops is registered
    to trace functions.
    
    The reason is that function graph just happens to trace all functions
    that the function tracer enables. When there was only one user of
    function tracing, the function graph tracer did not need to worry about
    being called by functions that it did not want to trace. But now that there
    are other users, this becomes a problem.
    
    For example, one just needs to do the following:
    
     # cd /sys/kernel/debug/tracing
     # echo schedule > set_ftrace_filter
     # echo function_graph > current_tracer
     # cat trace
    [..]
     0)               |  schedule() {
     ------------------------------------------
     0)    <idle>-0    =>   rcu_pre-7
     ------------------------------------------
    
     0) ! 2980.314 us |  }
     0)               |  schedule() {
     ------------------------------------------
     0)   rcu_pre-7    =>    <idle>-0
     ------------------------------------------
    
     0) + 20.701 us   |  }
    
     # echo 1 > /proc/sys/kernel/stack_tracer_enabled
     # cat trace
    [..]
     1) + 20.825 us   |      }
     1) + 21.651 us   |    }
     1) + 30.924 us   |  } /* SyS_ioctl */
     1)               |  do_page_fault() {
     1)               |    __do_page_fault() {
     1)   0.274 us    |      down_read_trylock();
     1)   0.098 us    |      find_vma();
     1)               |      handle_mm_fault() {
     1)               |        _raw_spin_lock() {
     1)   0.102 us    |          preempt_count_add();
     1)   0.097 us    |          do_raw_spin_lock();
     1)   2.173 us    |        }
     1)               |        do_wp_page() {
     1)   0.079 us    |          vm_normal_page();
     1)   0.086 us    |          reuse_swap_page();
     1)   0.076 us    |          page_move_anon_rmap();
     1)               |          unlock_page() {
     1)   0.082 us    |            page_waitqueue();
     1)   0.086 us    |            __wake_up_bit();
     1)   1.801 us    |          }
     1)   0.075 us    |          ptep_set_access_flags();
     1)               |          _raw_spin_unlock() {
     1)   0.098 us    |            do_raw_spin_unlock();
     1)   0.105 us    |            preempt_count_sub();
     1)   1.884 us    |          }
     1)   9.149 us    |        }
     1) + 13.083 us   |      }
     1)   0.146 us    |      up_read();
    
    When the stack tracer was enabled, it enabled all functions to be traced, which
    now the function graph tracer also traces. This is a side effect that should
    not occur.
    
    To fix this a test is added when the function tracing is changed, as well as when
    the graph tracer is enabled, to see if anything other than the ftrace global_ops
    function tracer is enabled. If so, then the graph tracer calls a test trampoline
    that will look at the function that is being traced and compare it with the
    filters defined by the global_ops.
    
    As an optimization, if there's no other function tracers registered, or if
    the only registered function tracers also use the global ops, the function
    graph infrastructure will call the registered function graph callback directly
    and not go through the test trampoline.
    
    Fixes: d2d45c7a03a2 "tracing: Have stack_tracer use a separate list of functions"
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 90eecc4b9e40c13da7c04def499bfb103b966577
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Jan 13 10:30:23 2014 -0500

    ftrace: Have function graph only trace based on global_ops filters
    
    commit 23a8e8441a0a74dd612edf81dc89d1600bc0a3d1 upstream.
    
    Doing some different tests, I discovered that function graph tracing, when
    filtered via the set_ftrace_filter and set_ftrace_notrace files, does
    not always keep with them if another function ftrace_ops is registered
    to trace functions.
    
    The reason is that function graph just happens to trace all functions
    that the function tracer enables. When there was only one user of
    function tracing, the function graph tracer did not need to worry about
    being called by functions that it did not want to trace. But now that there
    are other users, this becomes a problem.
    
    For example, one just needs to do the following:
    
     # cd /sys/kernel/debug/tracing
     # echo schedule > set_ftrace_filter
     # echo function_graph > current_tracer
     # cat trace
    [..]
     0)               |  schedule() {
     ------------------------------------------
     0)    <idle>-0    =>   rcu_pre-7
     ------------------------------------------
    
     0) ! 2980.314 us |  }
     0)               |  schedule() {
     ------------------------------------------
     0)   rcu_pre-7    =>    <idle>-0
     ------------------------------------------
    
     0) + 20.701 us   |  }
    
     # echo 1 > /proc/sys/kernel/stack_tracer_enabled
     # cat trace
    [..]
     1) + 20.825 us   |      }
     1) + 21.651 us   |    }
     1) + 30.924 us   |  } /* SyS_ioctl */
     1)               |  do_page_fault() {
     1)               |    __do_page_fault() {
     1)   0.274 us    |      down_read_trylock();
     1)   0.098 us    |      find_vma();
     1)               |      handle_mm_fault() {
     1)               |        _raw_spin_lock() {
     1)   0.102 us    |          preempt_count_add();
     1)   0.097 us    |          do_raw_spin_lock();
     1)   2.173 us    |        }
     1)               |        do_wp_page() {
     1)   0.079 us    |          vm_normal_page();
     1)   0.086 us    |          reuse_swap_page();
     1)   0.076 us    |          page_move_anon_rmap();
     1)               |          unlock_page() {
     1)   0.082 us    |            page_waitqueue();
     1)   0.086 us    |            __wake_up_bit();
     1)   1.801 us    |          }
     1)   0.075 us    |          ptep_set_access_flags();
     1)               |          _raw_spin_unlock() {
     1)   0.098 us    |            do_raw_spin_unlock();
     1)   0.105 us    |            preempt_count_sub();
     1)   1.884 us    |          }
     1)   9.149 us    |        }
     1) + 13.083 us   |      }
     1)   0.146 us    |      up_read();
    
    When the stack tracer was enabled, it enabled all functions to be traced, which
    now the function graph tracer also traces. This is a side effect that should
    not occur.
    
    To fix this a test is added when the function tracing is changed, as well as when
    the graph tracer is enabled, to see if anything other than the ftrace global_ops
    function tracer is enabled. If so, then the graph tracer calls a test trampoline
    that will look at the function that is being traced and compare it with the
    filters defined by the global_ops.
    
    As an optimization, if there's no other function tracers registered, or if
    the only registered function tracers also use the global ops, the function
    graph infrastructure will call the registered function graph callback directly
    and not go through the test trampoline.
    
    Fixes: d2d45c7a03a2 "tracing: Have stack_tracer use a separate list of functions"
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1499a3eb0473cad61fb2ed8f2e5247f4568a5786
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Feb 7 14:42:35 2014 -0500

    ftrace: Have function graph only trace based on global_ops filters
    
    commit 23a8e8441a0a74dd612edf81dc89d1600bc0a3d1 upstream.
    
    Doing some different tests, I discovered that function graph tracing, when
    filtered via the set_ftrace_filter and set_ftrace_notrace files, does
    not always keep with them if another function ftrace_ops is registered
    to trace functions.
    
    The reason is that function graph just happens to trace all functions
    that the function tracer enables. When there was only one user of
    function tracing, the function graph tracer did not need to worry about
    being called by functions that it did not want to trace. But now that there
    are other users, this becomes a problem.
    
    For example, one just needs to do the following:
    
     # cd /sys/kernel/debug/tracing
     # echo schedule > set_ftrace_filter
     # echo function_graph > current_tracer
     # cat trace
    [..]
     0)               |  schedule() {
     ------------------------------------------
     0)    <idle>-0    =>   rcu_pre-7
     ------------------------------------------
    
     0) ! 2980.314 us |  }
     0)               |  schedule() {
     ------------------------------------------
     0)   rcu_pre-7    =>    <idle>-0
     ------------------------------------------
    
     0) + 20.701 us   |  }
    
     # echo 1 > /proc/sys/kernel/stack_tracer_enabled
     # cat trace
    [..]
     1) + 20.825 us   |      }
     1) + 21.651 us   |    }
     1) + 30.924 us   |  } /* SyS_ioctl */
     1)               |  do_page_fault() {
     1)               |    __do_page_fault() {
     1)   0.274 us    |      down_read_trylock();
     1)   0.098 us    |      find_vma();
     1)               |      handle_mm_fault() {
     1)               |        _raw_spin_lock() {
     1)   0.102 us    |          preempt_count_add();
     1)   0.097 us    |          do_raw_spin_lock();
     1)   2.173 us    |        }
     1)               |        do_wp_page() {
     1)   0.079 us    |          vm_normal_page();
     1)   0.086 us    |          reuse_swap_page();
     1)   0.076 us    |          page_move_anon_rmap();
     1)               |          unlock_page() {
     1)   0.082 us    |            page_waitqueue();
     1)   0.086 us    |            __wake_up_bit();
     1)   1.801 us    |          }
     1)   0.075 us    |          ptep_set_access_flags();
     1)               |          _raw_spin_unlock() {
     1)   0.098 us    |            do_raw_spin_unlock();
     1)   0.105 us    |            preempt_count_sub();
     1)   1.884 us    |          }
     1)   9.149 us    |        }
     1) + 13.083 us   |      }
     1)   0.146 us    |      up_read();
    
    When the stack tracer was enabled, it enabled all functions to be traced, which
    now the function graph tracer also traces. This is a side effect that should
    not occur.
    
    To fix this a test is added when the function tracing is changed, as well as when
    the graph tracer is enabled, to see if anything other than the ftrace global_ops
    function tracer is enabled. If so, then the graph tracer calls a test trampoline
    that will look at the function that is being traced and compare it with the
    filters defined by the global_ops.
    
    As an optimization, if there's no other function tracers registered, or if
    the only registered function tracers also use the global ops, the function
    graph infrastructure will call the registered function graph callback directly
    and not go through the test trampoline.
    
    Fixes: d2d45c7a03a2 "tracing: Have stack_tracer use a separate list of functions"
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 23a8e8441a0a74dd612edf81dc89d1600bc0a3d1
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Jan 13 10:30:23 2014 -0500

    ftrace: Have function graph only trace based on global_ops filters
    
    Doing some different tests, I discovered that function graph tracing, when
    filtered via the set_ftrace_filter and set_ftrace_notrace files, does
    not always keep with them if another function ftrace_ops is registered
    to trace functions.
    
    The reason is that function graph just happens to trace all functions
    that the function tracer enables. When there was only one user of
    function tracing, the function graph tracer did not need to worry about
    being called by functions that it did not want to trace. But now that there
    are other users, this becomes a problem.
    
    For example, one just needs to do the following:
    
     # cd /sys/kernel/debug/tracing
     # echo schedule > set_ftrace_filter
     # echo function_graph > current_tracer
     # cat trace
    [..]
     0)               |  schedule() {
     ------------------------------------------
     0)    <idle>-0    =>   rcu_pre-7
     ------------------------------------------
    
     0) ! 2980.314 us |  }
     0)               |  schedule() {
     ------------------------------------------
     0)   rcu_pre-7    =>    <idle>-0
     ------------------------------------------
    
     0) + 20.701 us   |  }
    
     # echo 1 > /proc/sys/kernel/stack_tracer_enabled
     # cat trace
    [..]
     1) + 20.825 us   |      }
     1) + 21.651 us   |    }
     1) + 30.924 us   |  } /* SyS_ioctl */
     1)               |  do_page_fault() {
     1)               |    __do_page_fault() {
     1)   0.274 us    |      down_read_trylock();
     1)   0.098 us    |      find_vma();
     1)               |      handle_mm_fault() {
     1)               |        _raw_spin_lock() {
     1)   0.102 us    |          preempt_count_add();
     1)   0.097 us    |          do_raw_spin_lock();
     1)   2.173 us    |        }
     1)               |        do_wp_page() {
     1)   0.079 us    |          vm_normal_page();
     1)   0.086 us    |          reuse_swap_page();
     1)   0.076 us    |          page_move_anon_rmap();
     1)               |          unlock_page() {
     1)   0.082 us    |            page_waitqueue();
     1)   0.086 us    |            __wake_up_bit();
     1)   1.801 us    |          }
     1)   0.075 us    |          ptep_set_access_flags();
     1)               |          _raw_spin_unlock() {
     1)   0.098 us    |            do_raw_spin_unlock();
     1)   0.105 us    |            preempt_count_sub();
     1)   1.884 us    |          }
     1)   9.149 us    |        }
     1) + 13.083 us   |      }
     1)   0.146 us    |      up_read();
    
    When the stack tracer was enabled, it enabled all functions to be traced, which
    now the function graph tracer also traces. This is a side effect that should
    not occur.
    
    To fix this a test is added when the function tracing is changed, as well as when
    the graph tracer is enabled, to see if anything other than the ftrace global_ops
    function tracer is enabled. If so, then the graph tracer calls a test trampoline
    that will look at the function that is being traced and compare it with the
    filters defined by the global_ops.
    
    As an optimization, if there's no other function tracers registered, or if
    the only registered function tracers also use the global ops, the function
    graph infrastructure will call the registered function graph callback directly
    and not go through the test trampoline.
    
    Cc: stable@vger.kernel.org # 3.3+
    Fixes: d2d45c7a03a2 "tracing: Have stack_tracer use a separate list of functions"
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b108c975f4d210be89eacdec93bf8d34e7edbcc6
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Mon Aug 12 16:02:24 2013 +0400

    sparc64: Remove RWSEM export leftovers
    
    [ Upstream commit 61d9b9355b0d427bd1e732bd54628ff9103e496f ]
    
    The functions
    
                            __down_read
                            __down_read_trylock
                            __down_write
                            __down_write_trylock
                            __up_read
                            __up_write
                            __downgrade_write
    
    are implemented inline, so remove corresponding EXPORT_SYMBOLs
    (They lead to compile errors on RT kernel).
    
    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    CC: David Miller <davem@davemloft.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 29ad23b00474c34e3b5040dda508c78d33a1a3eb
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Mon Oct 14 17:24:26 2013 +0900

    ftrace: Add set_graph_notrace filter
    
    The set_graph_notrace filter is analogous to set_ftrace_notrace and
    can be used for eliminating uninteresting part of function graph trace
    output.  It also works with set_graph_function nicely.
    
      # cd /sys/kernel/debug/tracing/
      # echo do_page_fault > set_graph_function
      # perf ftrace live true
       2)               |  do_page_fault() {
       2)               |    __do_page_fault() {
       2)   0.381 us    |      down_read_trylock();
       2)   0.055 us    |      __might_sleep();
       2)   0.696 us    |      find_vma();
       2)               |      handle_mm_fault() {
       2)               |        handle_pte_fault() {
       2)               |          __do_fault() {
       2)               |            filemap_fault() {
       2)               |              find_get_page() {
       2)   0.033 us    |                __rcu_read_lock();
       2)   0.035 us    |                __rcu_read_unlock();
       2)   1.696 us    |              }
       2)   0.031 us    |              __might_sleep();
       2)   2.831 us    |            }
       2)               |            _raw_spin_lock() {
       2)   0.046 us    |              add_preempt_count();
       2)   0.841 us    |            }
       2)   0.033 us    |            page_add_file_rmap();
       2)               |            _raw_spin_unlock() {
       2)   0.057 us    |              sub_preempt_count();
       2)   0.568 us    |            }
       2)               |            unlock_page() {
       2)   0.084 us    |              page_waitqueue();
       2)   0.126 us    |              __wake_up_bit();
       2)   1.117 us    |            }
       2)   7.729 us    |          }
       2)   8.397 us    |        }
       2)   8.956 us    |      }
       2)   0.085 us    |      up_read();
       2) + 12.745 us   |    }
       2) + 13.401 us   |  }
      ...
    
      # echo handle_mm_fault > set_graph_notrace
      # perf ftrace live true
       1)               |  do_page_fault() {
       1)               |    __do_page_fault() {
       1)   0.205 us    |      down_read_trylock();
       1)   0.041 us    |      __might_sleep();
       1)   0.344 us    |      find_vma();
       1)   0.069 us    |      up_read();
       1)   4.692 us    |    }
       1)   5.311 us    |  }
      ...
    
    Link: http://lkml.kernel.org/r/1381739066-7531-5-git-send-email-namhyung@kernel.org
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 47da75cd1eccef5e203c326bcf74a70701d4f158
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Mon Aug 12 16:02:24 2013 +0400

    sparc64: Remove RWSEM export leftovers
    
    [ Upstream commit 61d9b9355b0d427bd1e732bd54628ff9103e496f ]
    
    The functions
    
                            __down_read
                            __down_read_trylock
                            __down_write
                            __down_write_trylock
                            __up_read
                            __up_write
                            __downgrade_write
    
    are implemented inline, so remove corresponding EXPORT_SYMBOLs
    (They lead to compile errors on RT kernel).
    
    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    CC: David Miller <davem@davemloft.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 32f0ba8e21166ec3ac11d692852d6d3e768942a4
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Mon Aug 12 16:02:24 2013 +0400

    sparc64: Remove RWSEM export leftovers
    
    [ Upstream commit 61d9b9355b0d427bd1e732bd54628ff9103e496f ]
    
    The functions
    
                            __down_read
                            __down_read_trylock
                            __down_write
                            __down_write_trylock
                            __up_read
                            __up_write
                            __downgrade_write
    
    are implemented inline, so remove corresponding EXPORT_SYMBOLs
    (They lead to compile errors on RT kernel).
    
    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    CC: David Miller <davem@davemloft.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit cf818489ca512369e2c6ac57307284e171a1d588
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Mon Aug 12 16:02:24 2013 +0400

    sparc64: Remove RWSEM export leftovers
    
    [ Upstream commit 61d9b9355b0d427bd1e732bd54628ff9103e496f ]
    
    The functions
    
                            __down_read
                            __down_read_trylock
                            __down_write
                            __down_write_trylock
                            __up_read
                            __up_write
                            __downgrade_write
    
    are implemented inline, so remove corresponding EXPORT_SYMBOLs
    (They lead to compile errors on RT kernel).
    
    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    CC: David Miller <davem@davemloft.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ca0bd2082f83ccf6abbb2db2e4475bb81b415118
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Mon Aug 12 16:02:24 2013 +0400

    sparc64: Remove RWSEM export leftovers
    
    [ Upstream commit 61d9b9355b0d427bd1e732bd54628ff9103e496f ]
    
    The functions
    
                            __down_read
                            __down_read_trylock
                            __down_write
                            __down_write_trylock
                            __up_read
                            __up_write
                            __downgrade_write
    
    are implemented inline, so remove corresponding EXPORT_SYMBOLs
    (They lead to compile errors on RT kernel).
    
    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    CC: David Miller <davem@davemloft.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b20204310ad18516146264830bc7b840b6052d57
Author: Jesper Juhl <jj@chaosbits.net>
Date:   Mon Sep 30 22:44:37 2013 +0200

    staging: lustre: Don't leak 'buffer' in cfs_get_environ()
    
    If 'down_read_trylock' fails we'll curently leak the memory allocated to 'buffer'.
    Fix the leak by simply kfree'ing 'buffer' before returning '-EDEADLK'.
    
    Signed-off-by: Jesper Juhl <jj@chaosbits.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 61d9b9355b0d427bd1e732bd54628ff9103e496f
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Mon Aug 12 16:02:24 2013 +0400

    sparc64: Remove RWSEM export leftovers
    
    The functions
    
                            __down_read
                            __down_read_trylock
                            __down_write
                            __down_write_trylock
                            __up_read
                            __up_write
                            __downgrade_write
    
    are implemented inline, so remove corresponding EXPORT_SYMBOLs
    (They lead to compile errors on RT kernel).
    
    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    CC: David Miller <davem@davemloft.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit ebc9baed42e42f9b51cf61672b7afb72f068d523
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Mon Mar 11 16:44:40 2013 -0400

    tty: Separate release semantics of ldisc reference
    
    tty_ldisc_ref()/tty_ldisc_unref() have usage semantics
    equivalent to down_read_trylock()/up_read(). Only
    callers of tty_ldisc_put() are performing the additional
    operations necessary for proper ldisc teardown, and then only
    after ensuring no outstanding 'read lock' remains.
    
    Thus, tty_ldisc_unref() should never be the last reference;
    WARN if it is. Conversely, tty_ldisc_put() should never be
    destructing if the use count != 1.
    
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit be871b7e54711479d3b9d3617d49898770830db2
Author: Michal Hocko <mhocko@suse.cz>
Date:   Tue Mar 12 17:21:19 2013 +0100

    device: separate all subsys mutexes
    
    ca22e56d (driver-core: implement 'sysdev' functionality for regular
    devices and buses) has introduced bus_register macro with a static
    key to distinguish different subsys mutex classes.
    
    This however doesn't work for different subsys which use a common
    registering function. One example is subsys_system_register (and
    mce_device and cpu_device).
    
    In the end this leads to the following lockdep splat:
    [  207.271924] ======================================================
    [  207.271932] [ INFO: possible circular locking dependency detected ]
    [  207.271942] 3.9.0-rc1-0.7-default+ #34 Not tainted
    [  207.271948] -------------------------------------------------------
    [  207.271957] bash/10493 is trying to acquire lock:
    [  207.271963]  (subsys mutex){+.+.+.}, at: [<ffffffff8134af27>] bus_remove_device+0x37/0x1c0
    [  207.271987]
    [  207.271987] but task is already holding lock:
    [  207.271995]  (cpu_hotplug.lock){+.+.+.}, at: [<ffffffff81046ccf>] cpu_hotplug_begin+0x2f/0x60
    [  207.272012]
    [  207.272012] which lock already depends on the new lock.
    [  207.272012]
    [  207.272023]
    [  207.272023] the existing dependency chain (in reverse order) is:
    [  207.272033]
    [  207.272033] -> #4 (cpu_hotplug.lock){+.+.+.}:
    [  207.272044]        [<ffffffff810ae329>] lock_acquire+0xe9/0x120
    [  207.272056]        [<ffffffff814ad807>] mutex_lock_nested+0x37/0x360
    [  207.272069]        [<ffffffff81046ba9>] get_online_cpus+0x29/0x40
    [  207.272082]        [<ffffffff81185210>] drain_all_stock+0x30/0x150
    [  207.272094]        [<ffffffff811853da>] mem_cgroup_reclaim+0xaa/0xe0
    [  207.272104]        [<ffffffff8118775e>] __mem_cgroup_try_charge+0x51e/0xcf0
    [  207.272114]        [<ffffffff81188486>] mem_cgroup_charge_common+0x36/0x60
    [  207.272125]        [<ffffffff811884da>] mem_cgroup_newpage_charge+0x2a/0x30
    [  207.272135]        [<ffffffff81150531>] do_wp_page+0x231/0x830
    [  207.272147]        [<ffffffff8115151e>] handle_pte_fault+0x19e/0x8d0
    [  207.272157]        [<ffffffff81151da8>] handle_mm_fault+0x158/0x1e0
    [  207.272166]        [<ffffffff814b6153>] do_page_fault+0x2a3/0x4e0
    [  207.272178]        [<ffffffff814b2578>] page_fault+0x28/0x30
    [  207.272189]
    [  207.272189] -> #3 (&mm->mmap_sem){++++++}:
    [  207.272199]        [<ffffffff810ae329>] lock_acquire+0xe9/0x120
    [  207.272208]        [<ffffffff8114c5ad>] might_fault+0x6d/0x90
    [  207.272218]        [<ffffffff811a11e3>] filldir64+0xb3/0x120
    [  207.272229]        [<ffffffffa013fc19>] call_filldir+0x89/0x130 [ext3]
    [  207.272248]        [<ffffffffa0140377>] ext3_readdir+0x6b7/0x7e0 [ext3]
    [  207.272263]        [<ffffffff811a1519>] vfs_readdir+0xa9/0xc0
    [  207.272273]        [<ffffffff811a15cb>] sys_getdents64+0x9b/0x110
    [  207.272284]        [<ffffffff814bb599>] system_call_fastpath+0x16/0x1b
    [  207.272296]
    [  207.272296] -> #2 (&type->i_mutex_dir_key#3){+.+.+.}:
    [  207.272309]        [<ffffffff810ae329>] lock_acquire+0xe9/0x120
    [  207.272319]        [<ffffffff814ad807>] mutex_lock_nested+0x37/0x360
    [  207.272329]        [<ffffffff8119c254>] link_path_walk+0x6f4/0x9a0
    [  207.272339]        [<ffffffff8119e7fa>] path_openat+0xba/0x470
    [  207.272349]        [<ffffffff8119ecf8>] do_filp_open+0x48/0xa0
    [  207.272358]        [<ffffffff8118d81c>] file_open_name+0xdc/0x110
    [  207.272369]        [<ffffffff8118d885>] filp_open+0x35/0x40
    [  207.272378]        [<ffffffff8135c76e>] _request_firmware+0x52e/0xb20
    [  207.272389]        [<ffffffff8135cdd6>] request_firmware+0x16/0x20
    [  207.272399]        [<ffffffffa03bdb91>] request_microcode_fw+0x61/0xd0 [microcode]
    [  207.272416]        [<ffffffffa03bd554>] microcode_init_cpu+0x104/0x150 [microcode]
    [  207.272431]        [<ffffffffa03bd61c>] mc_device_add+0x7c/0xb0 [microcode]
    [  207.272444]        [<ffffffff8134a419>] subsys_interface_register+0xc9/0x100
    [  207.272457]        [<ffffffffa04fc0f4>] 0xffffffffa04fc0f4
    [  207.272472]        [<ffffffff81000202>] do_one_initcall+0x42/0x180
    [  207.272485]        [<ffffffff810bbeff>] load_module+0x19df/0x1b70
    [  207.272499]        [<ffffffff810bc376>] sys_init_module+0xe6/0x130
    [  207.272511]        [<ffffffff814bb599>] system_call_fastpath+0x16/0x1b
    [  207.272523]
    [  207.272523] -> #1 (umhelper_sem){++++.+}:
    [  207.272537]        [<ffffffff810ae329>] lock_acquire+0xe9/0x120
    [  207.272548]        [<ffffffff814ae9c4>] down_read+0x34/0x50
    [  207.272559]        [<ffffffff81062bff>] usermodehelper_read_trylock+0x4f/0x100
    [  207.272575]        [<ffffffff8135c7dd>] _request_firmware+0x59d/0xb20
    [  207.272587]        [<ffffffff8135cdd6>] request_firmware+0x16/0x20
    [  207.272599]        [<ffffffffa03bdb91>] request_microcode_fw+0x61/0xd0 [microcode]
    [  207.272613]        [<ffffffffa03bd554>] microcode_init_cpu+0x104/0x150 [microcode]
    [  207.272627]        [<ffffffffa03bd61c>] mc_device_add+0x7c/0xb0 [microcode]
    [  207.272641]        [<ffffffff8134a419>] subsys_interface_register+0xc9/0x100
    [  207.272654]        [<ffffffffa04fc0f4>] 0xffffffffa04fc0f4
    [  207.272666]        [<ffffffff81000202>] do_one_initcall+0x42/0x180
    [  207.272678]        [<ffffffff810bbeff>] load_module+0x19df/0x1b70
    [  207.272690]        [<ffffffff810bc376>] sys_init_module+0xe6/0x130
    [  207.272702]        [<ffffffff814bb599>] system_call_fastpath+0x16/0x1b
    [  207.272715]
    [  207.272715] -> #0 (subsys mutex){+.+.+.}:
    [  207.272729]        [<ffffffff810ae002>] __lock_acquire+0x13b2/0x15f0
    [  207.272740]        [<ffffffff810ae329>] lock_acquire+0xe9/0x120
    [  207.272751]        [<ffffffff814ad807>] mutex_lock_nested+0x37/0x360
    [  207.272763]        [<ffffffff8134af27>] bus_remove_device+0x37/0x1c0
    [  207.272775]        [<ffffffff81349114>] device_del+0x134/0x1f0
    [  207.272786]        [<ffffffff813491f2>] device_unregister+0x22/0x60
    [  207.272798]        [<ffffffff814a24ea>] mce_cpu_callback+0x15e/0x1ad
    [  207.272812]        [<ffffffff814b6402>] notifier_call_chain+0x72/0x130
    [  207.272824]        [<ffffffff81073d6e>] __raw_notifier_call_chain+0xe/0x10
    [  207.272839]        [<ffffffff81498f76>] _cpu_down+0x1d6/0x350
    [  207.272851]        [<ffffffff81499130>] cpu_down+0x40/0x60
    [  207.272862]        [<ffffffff8149cc55>] store_online+0x75/0xe0
    [  207.272874]        [<ffffffff813474a0>] dev_attr_store+0x20/0x30
    [  207.272886]        [<ffffffff812090d9>] sysfs_write_file+0xd9/0x150
    [  207.272900]        [<ffffffff8118e10b>] vfs_write+0xcb/0x130
    [  207.272911]        [<ffffffff8118e924>] sys_write+0x64/0xa0
    [  207.272923]        [<ffffffff814bb599>] system_call_fastpath+0x16/0x1b
    [  207.272936]
    [  207.272936] other info that might help us debug this:
    [  207.272936]
    [  207.272952] Chain exists of:
    [  207.272952]   subsys mutex --> &mm->mmap_sem --> cpu_hotplug.lock
    [  207.272952]
    [  207.272973]  Possible unsafe locking scenario:
    [  207.272973]
    [  207.272984]        CPU0                    CPU1
    [  207.272992]        ----                    ----
    [  207.273000]   lock(cpu_hotplug.lock);
    [  207.273009]                                lock(&mm->mmap_sem);
    [  207.273020]                                lock(cpu_hotplug.lock);
    [  207.273031]   lock(subsys mutex);
    [  207.273040]
    [  207.273040]  *** DEADLOCK ***
    [  207.273040]
    [  207.273055] 5 locks held by bash/10493:
    [  207.273062]  #0:  (&buffer->mutex){+.+.+.}, at: [<ffffffff81209049>] sysfs_write_file+0x49/0x150
    [  207.273080]  #1:  (s_active#150){.+.+.+}, at: [<ffffffff812090c2>] sysfs_write_file+0xc2/0x150
    [  207.273099]  #2:  (x86_cpu_hotplug_driver_mutex){+.+.+.}, at: [<ffffffff81027557>] cpu_hotplug_driver_lock+0x17/0x20
    [  207.273121]  #3:  (cpu_add_remove_lock){+.+.+.}, at: [<ffffffff8149911c>] cpu_down+0x2c/0x60
    [  207.273140]  #4:  (cpu_hotplug.lock){+.+.+.}, at: [<ffffffff81046ccf>] cpu_hotplug_begin+0x2f/0x60
    [  207.273158]
    [  207.273158] stack backtrace:
    [  207.273170] Pid: 10493, comm: bash Not tainted 3.9.0-rc1-0.7-default+ #34
    [  207.273180] Call Trace:
    [  207.273192]  [<ffffffff810ab373>] print_circular_bug+0x223/0x310
    [  207.273204]  [<ffffffff810ae002>] __lock_acquire+0x13b2/0x15f0
    [  207.273216]  [<ffffffff812086b0>] ? sysfs_hash_and_remove+0x60/0xc0
    [  207.273227]  [<ffffffff810ae329>] lock_acquire+0xe9/0x120
    [  207.273239]  [<ffffffff8134af27>] ? bus_remove_device+0x37/0x1c0
    [  207.273251]  [<ffffffff814ad807>] mutex_lock_nested+0x37/0x360
    [  207.273263]  [<ffffffff8134af27>] ? bus_remove_device+0x37/0x1c0
    [  207.273274]  [<ffffffff812086b0>] ? sysfs_hash_and_remove+0x60/0xc0
    [  207.273286]  [<ffffffff8134af27>] bus_remove_device+0x37/0x1c0
    [  207.273298]  [<ffffffff81349114>] device_del+0x134/0x1f0
    [  207.273309]  [<ffffffff813491f2>] device_unregister+0x22/0x60
    [  207.273321]  [<ffffffff814a24ea>] mce_cpu_callback+0x15e/0x1ad
    [  207.273332]  [<ffffffff814b6402>] notifier_call_chain+0x72/0x130
    [  207.273344]  [<ffffffff81073d6e>] __raw_notifier_call_chain+0xe/0x10
    [  207.273356]  [<ffffffff81498f76>] _cpu_down+0x1d6/0x350
    [  207.273368]  [<ffffffff81027557>] ? cpu_hotplug_driver_lock+0x17/0x20
    [  207.273380]  [<ffffffff81499130>] cpu_down+0x40/0x60
    [  207.273391]  [<ffffffff8149cc55>] store_online+0x75/0xe0
    [  207.273402]  [<ffffffff813474a0>] dev_attr_store+0x20/0x30
    [  207.273413]  [<ffffffff812090d9>] sysfs_write_file+0xd9/0x150
    [  207.273425]  [<ffffffff8118e10b>] vfs_write+0xcb/0x130
    [  207.273436]  [<ffffffff8118e924>] sys_write+0x64/0xa0
    [  207.273447]  [<ffffffff814bb599>] system_call_fastpath+0x16/0x1b
    
    Which reports a false possitive deadlock because it sees:
    1) load_module -> subsys_interface_register -> mc_deveice_add (*) -> subsys->p->mutex -> link_path_walk -> lookup_slow -> i_mutex
    2) sys_write -> _cpu_down -> cpu_hotplug_begin -> cpu_hotplug.lock -> mce_cpu_callback -> mce_device_remove(**) -> device_unregister -> bus_remove_device -> subsys mutex
    3) vfs_readdir -> i_mutex -> filldir64 -> might_fault -> might_lock_read(mmap_sem) -> page_fault -> mmap_sem -> drain_all_stock -> cpu_hotplug.lock
    
    but
    1) takes cpu_subsys subsys (*) but 2) takes mce_device subsys (**) so
    the deadlock is not possible AFAICS.
    
    The fix is quite simple. We can pull the key inside bus_type structure
    because they are defined per device so the pointer will be unique as
    well. bus_register doesn't need to be a macro anymore so change it
    to the inline. We could get rid of __bus_register as there is no other
    caller but maybe somebody will want to use a different key so keep it
    around for now.
    
    Reported-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit de1a2262b006220dae2561a299a6ea128c46f4fe
Merge: f042fea0da78 ed84825b785c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 28 13:21:44 2013 -0800

    Merge tag 'writeback-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux
    
    Pull writeback fixes from Wu Fengguang:
     "Two writeback fixes
    
       - fix negative (setpoint - dirty) in 32bit archs
    
       - use down_read_trylock() in writeback_inodes_sb(_nr)_if_idle()"
    
    * tag 'writeback-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux:
      Negative (setpoint-dirty) in bdi_position_ratio()
      vfs: re-implement writeback_inodes_sb(_nr)_if_idle() and rename them

commit 10ee27a06cc8eb57f83342a8eabcb75deb872d52
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Thu Jan 10 13:47:57 2013 +0800

    vfs: re-implement writeback_inodes_sb(_nr)_if_idle() and rename them
    
    writeback_inodes_sb(_nr)_if_idle() is re-implemented by replacing down_read()
    with down_read_trylock() because
    
    - If ->s_umount is write locked, then the sb is not idle. That is
      writeback_inodes_sb(_nr)_if_idle() needn't wait for the lock.
    
    - writeback_inodes_sb(_nr)_if_idle() grabs s_umount lock when it want to start
      writeback, it may bring us deadlock problem when doing umount. In order to
      fix the problem, ext4 and btrfs implemented their own writeback functions
      instead of writeback_inodes_sb(_nr)_if_idle(), but it introduced the redundant
      code, it is better to implement a new writeback_inodes_sb(_nr)_if_idle().
    
    The name of these two functions is cumbersome, so rename them to
    try_to_writeback_inodes_sb(_nr).
    
    This idea came from Christoph Hellwig.
    Some code is from the patch of Kamal Mostafa.
    
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>

commit 0c5445015c8e37d4f79c16fa51be398c4cb0e46c
Merge: 1ffab3d41395 77c8006d8df4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 20 07:24:17 2012 -0800

    Merge tag 'cris-for-linus-3.8' of git://jni.nu/cris
    
    Pull CRIS changes from Jesper Nilsson.
    
    ... mainly the UAPI disintegration.
    
    * tag 'cris-for-linus-3.8' of git://jni.nu/cris:
      UAPI: Fix up empty files in arch/cris/
      CRIS: locking: fix the return value of arch_read_trylock()
      CRIS: use kbuild.h instead of defining macros in asm-offset.c
      UAPI: (Scripted) Disintegrate arch/cris/include/asm
      UAPI: (Scripted) Disintegrate arch/cris/include/arch-v32/arch
      UAPI: (Scripted) Disintegrate arch/cris/include/arch-v10/arch

commit 00addd1a2de8d83dc3a5d6bb926911268ec716c9
Author: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
Date:   Wed Oct 17 16:54:27 2012 +0200

    CRIS: locking: fix the return value of arch_read_trylock()
    
    arch_write_trylock() should return 'ret' instead of always
    return 1.
    
    dpatch engine is used to auto generate this patch.
    (https://github.com/weiyj/dpatch)
    
    Signed-off-by: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
    Signed-off-by: Jesper Nilsson <jesper.nilsson@axis.com>

commit 9aa05000f2b7cab4be582afba64af10b2d74727e
Author: Dave Chinner <dchinner@redhat.com>
Date:   Mon Oct 8 21:56:04 2012 +1100

    xfs: xfs_sync_data is redundant.
    
    We don't do any data writeback from XFS any more - the VFS is
    completely responsible for that, including for freeze. We can
    replace the remaining caller with a VFS level function that
    achieves the same thing, but without conflicting with current
    writeback work.
    
    This means we can remove the flush_work and xfs_flush_inodes() - the
    VFS functionality completely replaces the internal flush queue for
    doing this writeback work in a separate context to avoid stack
    overruns.
    
    This does have one complication - it cannot be called with page
    locks held.  Hence move the flushing of delalloc space when ENOSPC
    occurs back up into xfs_file_aio_buffered_write when we don't hold
    any locks that will stall writeback.
    
    Unfortunately, writeback_inodes_sb_if_idle() is not sufficient to
    trigger delalloc conversion fast enough to prevent spurious ENOSPC
    whent here are hundreds of writers, thousands of small files and GBs
    of free RAM.  Hence we need to use sync_sb_inodes() to block callers
    while we wait for writeback like the previous xfs_flush_inodes
    implementation did.
    
    That means we have to hold the s_umount lock here, but because this
    call can nest inside i_mutex (the parent directory in the create
    case, held by the VFS), we have to use down_read_trylock() to avoid
    potential deadlocks. In practice, this trylock will succeed on
    almost every attempt as unmount/remount type operations are
    exceedingly rare.
    
    Note: we always need to pass a count of zero to
    generic_file_buffered_write() as the previously written byte count.
    We only do this by accident before this patch by the virtue of ret
    always being zero when there are no errors. Make this explicit
    rather than needing to specifically zero ret in the ENOSPC retry
    case.
    
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Tested-by: Brian Foster <bfoster@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ben Myers <bpm@sgi.com>

commit dff2f8b03d16e976aac685dd8b7adbfa4cc730a6
Author: Rafael J. Wysocki <rjw@rjwysocki.net>
Date:   Wed Mar 28 23:30:28 2012 +0200

    PM / Sleep: Mitigate race between the freezer and request_firmware()
    
    commit 247bc03742545fec2f79939a3b9f738392a0f7b4 upstream.
    
    There is a race condition between the freezer and request_firmware()
    such that if request_firmware() is run on one CPU and
    freeze_processes() is run on another CPU and usermodehelper_disable()
    called by it succeeds to grab umhelper_sem for writing before
    usermodehelper_read_trylock() called from request_firmware()
    acquires it for reading, the request_firmware() will fail and
    trigger a WARN_ON() complaining that it was called at a wrong time.
    However, in fact, it wasn't called at a wrong time and
    freeze_processes() simply happened to be executed simultaneously.
    
    To avoid this race, at least in some cases, modify
    usermodehelper_read_trylock() so that it doesn't fail if the
    freezing of tasks has just started and hasn't been completed yet.
    Instead, during the freezing of tasks, it will try to freeze the
    task that has called it so that it can wait until user space is
    thawed without triggering the scary warning.
    
    For this purpose, change usermodehelper_disabled so that it can
    take three different values, UMH_ENABLED (0), UMH_FREEZING and
    UMH_DISABLED.  The first one means that usermode helpers are
    enabled, the last one means "hard disable" (i.e. the system is not
    ready for usermode helpers to be used) and the second one
    is reserved for the freezer.  Namely, when freeze_processes() is
    started, it sets usermodehelper_disabled to UMH_FREEZING which
    tells usermodehelper_read_trylock() that it shouldn't fail just
    yet and should call try_to_freeze() if woken up and cannot
    return immediately.  This way all freezable tasks that happen
    to call request_firmware() right before freeze_processes() is
    started and lose the race for umhelper_sem with it will be
    frozen and will sleep until thaw_processes() unsets
    usermodehelper_disabled.  [For the non-freezable callers of
    request_firmware() the race for umhelper_sem against
    freeze_processes() is unfortunately unavoidable.]
    
    Reported-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 91cb632cd8d0a1649d088a50e752459d1e8b3d79
Author: Rafael J. Wysocki <rjw@rjwysocki.net>
Date:   Wed Mar 28 23:30:02 2012 +0200

    firmware_class: Do not warn that system is not ready from async loads
    
    commit 9b78c1da60b3c62ccdd1509f0902ad19ceaf776b upstream.
    
    If firmware is requested asynchronously, by calling
    request_firmware_nowait(), there is no reason to fail the request
    (and warn the user) when the system is (presumably temporarily)
    unready to handle it (because user space is not available yet or
    frozen).  For this reason, introduce an alternative routine for
    read-locking umhelper_sem, usermodehelper_read_lock_wait(), that
    will wait for usermodehelper_disabled to be unset (possibly with
    a timeout) and make request_firmware_work_func() use it instead of
    usermodehelper_read_trylock().
    
    Accordingly, modify request_firmware() so that it uses
    usermodehelper_read_trylock() to acquire umhelper_sem and remove
    the code related to that lock from _request_firmware().
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1cbfac52d7ccb45e3a3ea1941ed626997143456c
Author: Rafael J. Wysocki <rjw@rjwysocki.net>
Date:   Wed Mar 28 23:29:45 2012 +0200

    firmware_class: Rework usermodehelper check
    
    commit fe2e39d8782d885755139304d8dba0b3e5bfa878 upstream.
    
    Instead of two functions, read_lock_usermodehelper() and
    usermodehelper_is_disabled(), used in combination, introduce
    usermodehelper_read_trylock() that will only return with umhelper_sem
    held if usermodehelper_disabled is unset (and will return -EAGAIN
    otherwise) and make _request_firmware() use it.
    
    Rename read_unlock_usermodehelper() to
    usermodehelper_read_unlock() to follow the naming convention of the
    new function.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4a1d704194a441bf83c636004a479e01360ec850
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Wed Mar 21 16:33:42 2012 -0700

    mm: thp: fix pmd_bad() triggering in code paths holding mmap_sem read mode
    
    commit 1a5a9906d4e8d1976b701f889d8f35d54b928f25 upstream.
    
    In some cases it may happen that pmd_none_or_clear_bad() is called with
    the mmap_sem hold in read mode.  In those cases the huge page faults can
    allocate hugepmds under pmd_none_or_clear_bad() and that can trigger a
    false positive from pmd_bad() that will not like to see a pmd
    materializing as trans huge.
    
    It's not khugepaged causing the problem, khugepaged holds the mmap_sem
    in write mode (and all those sites must hold the mmap_sem in read mode
    to prevent pagetables to go away from under them, during code review it
    seems vm86 mode on 32bit kernels requires that too unless it's
    restricted to 1 thread per process or UP builds).  The race is only with
    the huge pagefaults that can convert a pmd_none() into a
    pmd_trans_huge().
    
    Effectively all these pmd_none_or_clear_bad() sites running with
    mmap_sem in read mode are somewhat speculative with the page faults, and
    the result is always undefined when they run simultaneously.  This is
    probably why it wasn't common to run into this.  For example if the
    madvise(MADV_DONTNEED) runs zap_page_range() shortly before the page
    fault, the hugepage will not be zapped, if the page fault runs first it
    will be zapped.
    
    Altering pmd_bad() not to error out if it finds hugepmds won't be enough
    to fix this, because zap_pmd_range would then proceed to call
    zap_pte_range (which would be incorrect if the pmd become a
    pmd_trans_huge()).
    
    The simplest way to fix this is to read the pmd in the local stack
    (regardless of what we read, no need of actual CPU barriers, only
    compiler barrier needed), and be sure it is not changing under the code
    that computes its value.  Even if the real pmd is changing under the
    value we hold on the stack, we don't care.  If we actually end up in
    zap_pte_range it means the pmd was not none already and it was not huge,
    and it can't become huge from under us (khugepaged locking explained
    above).
    
    All we need is to enforce that there is no way anymore that in a code
    path like below, pmd_trans_huge can be false, but pmd_none_or_clear_bad
    can run into a hugepmd.  The overhead of a barrier() is just a compiler
    tweak and should not be measurable (I only added it for THP builds).  I
    don't exclude different compiler versions may have prevented the race
    too by caching the value of *pmd on the stack (that hasn't been
    verified, but it wouldn't be impossible considering
    pmd_none_or_clear_bad, pmd_bad, pmd_trans_huge, pmd_none are all inlines
    and there's no external function called in between pmd_trans_huge and
    pmd_none_or_clear_bad).
    
                    if (pmd_trans_huge(*pmd)) {
                            if (next-addr != HPAGE_PMD_SIZE) {
                                    VM_BUG_ON(!rwsem_is_locked(&tlb->mm->mmap_sem));
                                    split_huge_page_pmd(vma->vm_mm, pmd);
                            } else if (zap_huge_pmd(tlb, vma, pmd, addr))
                                    continue;
                            /* fall through */
                    }
                    if (pmd_none_or_clear_bad(pmd))
    
    Because this race condition could be exercised without special
    privileges this was reported in CVE-2012-1179.
    
    The race was identified and fully explained by Ulrich who debugged it.
    I'm quoting his accurate explanation below, for reference.
    
    ====== start quote =======
          mapcount 0 page_mapcount 1
          kernel BUG at mm/huge_memory.c:1384!
    
        At some point prior to the panic, a "bad pmd ..." message similar to the
        following is logged on the console:
    
          mm/memory.c:145: bad pmd ffff8800376e1f98(80000000314000e7).
    
        The "bad pmd ..." message is logged by pmd_clear_bad() before it clears
        the page's PMD table entry.
    
            143 void pmd_clear_bad(pmd_t *pmd)
            144 {
        ->  145         pmd_ERROR(*pmd);
            146         pmd_clear(pmd);
            147 }
    
        After the PMD table entry has been cleared, there is an inconsistency
        between the actual number of PMD table entries that are mapping the page
        and the page's map count (_mapcount field in struct page). When the page
        is subsequently reclaimed, __split_huge_page() detects this inconsistency.
    
           1381         if (mapcount != page_mapcount(page))
           1382                 printk(KERN_ERR "mapcount %d page_mapcount %d\n",
           1383                        mapcount, page_mapcount(page));
        -> 1384         BUG_ON(mapcount != page_mapcount(page));
    
        The root cause of the problem is a race of two threads in a multithreaded
        process. Thread B incurs a page fault on a virtual address that has never
        been accessed (PMD entry is zero) while Thread A is executing an madvise()
        system call on a virtual address within the same 2 MB (huge page) range.
    
                   virtual address space
                  .---------------------.
                  |                     |
                  |                     |
                .-|---------------------|
                | |                     |
                | |                     |<-- B(fault)
                | |                     |
          2 MB  | |/////////////////////|-.
          huge <  |/////////////////////|  > A(range)
          page  | |/////////////////////|-'
                | |                     |
                | |                     |
                '-|---------------------|
                  |                     |
                  |                     |
                  '---------------------'
    
        - Thread A is executing an madvise(..., MADV_DONTNEED) system call
          on the virtual address range "A(range)" shown in the picture.
    
        sys_madvise
          // Acquire the semaphore in shared mode.
          down_read(&current->mm->mmap_sem)
          ...
          madvise_vma
            switch (behavior)
            case MADV_DONTNEED:
                 madvise_dontneed
                   zap_page_range
                     unmap_vmas
                       unmap_page_range
                         zap_pud_range
                           zap_pmd_range
                             //
                             // Assume that this huge page has never been accessed.
                             // I.e. content of the PMD entry is zero (not mapped).
                             //
                             if (pmd_trans_huge(*pmd)) {
                                 // We don't get here due to the above assumption.
                             }
                             //
                             // Assume that Thread B incurred a page fault and
                 .---------> // sneaks in here as shown below.
                 |           //
                 |           if (pmd_none_or_clear_bad(pmd))
                 |               {
                 |                 if (unlikely(pmd_bad(*pmd)))
                 |                     pmd_clear_bad
                 |                     {
                 |                       pmd_ERROR
                 |                         // Log "bad pmd ..." message here.
                 |                       pmd_clear
                 |                         // Clear the page's PMD entry.
                 |                         // Thread B incremented the map count
                 |                         // in page_add_new_anon_rmap(), but
                 |                         // now the page is no longer mapped
                 |                         // by a PMD entry (-> inconsistency).
                 |                     }
                 |               }
                 |
                 v
        - Thread B is handling a page fault on virtual address "B(fault)" shown
          in the picture.
    
        ...
        do_page_fault
          __do_page_fault
            // Acquire the semaphore in shared mode.
            down_read_trylock(&mm->mmap_sem)
            ...
            handle_mm_fault
              if (pmd_none(*pmd) && transparent_hugepage_enabled(vma))
                  // We get here due to the above assumption (PMD entry is zero).
                  do_huge_pmd_anonymous_page
                    alloc_hugepage_vma
                      // Allocate a new transparent huge page here.
                    ...
                    __do_huge_pmd_anonymous_page
                      ...
                      spin_lock(&mm->page_table_lock)
                      ...
                      page_add_new_anon_rmap
                        // Here we increment the page's map count (starts at -1).
                        atomic_set(&page->_mapcount, 0)
                      set_pmd_at
                        // Here we set the page's PMD entry which will be cleared
                        // when Thread A calls pmd_clear_bad().
                      ...
                      spin_unlock(&mm->page_table_lock)
    
        The mmap_sem does not prevent the race because both threads are acquiring
        it in shared mode (down_read).  Thread B holds the page_table_lock while
        the page's map count and PMD table entry are updated.  However, Thread A
        does not synchronize on that lock.
    
    ====== end quote =======
    
    [akpm@linux-foundation.org: checkpatch fixes]
    Reported-by: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Jones <davej@redhat.com>
    Acked-by: Larry Woodman <lwoodman@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Mark Salter <msalter@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c6cf24ba30c7225667827245cfd2bc98f7f5ed2b
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Wed Mar 21 16:33:42 2012 -0700

    mm: thp: fix pmd_bad() triggering in code paths holding mmap_sem read mode
    
    commit 1a5a9906d4e8d1976b701f889d8f35d54b928f25 upstream.
    
    In some cases it may happen that pmd_none_or_clear_bad() is called with
    the mmap_sem hold in read mode.  In those cases the huge page faults can
    allocate hugepmds under pmd_none_or_clear_bad() and that can trigger a
    false positive from pmd_bad() that will not like to see a pmd
    materializing as trans huge.
    
    It's not khugepaged causing the problem, khugepaged holds the mmap_sem
    in write mode (and all those sites must hold the mmap_sem in read mode
    to prevent pagetables to go away from under them, during code review it
    seems vm86 mode on 32bit kernels requires that too unless it's
    restricted to 1 thread per process or UP builds).  The race is only with
    the huge pagefaults that can convert a pmd_none() into a
    pmd_trans_huge().
    
    Effectively all these pmd_none_or_clear_bad() sites running with
    mmap_sem in read mode are somewhat speculative with the page faults, and
    the result is always undefined when they run simultaneously.  This is
    probably why it wasn't common to run into this.  For example if the
    madvise(MADV_DONTNEED) runs zap_page_range() shortly before the page
    fault, the hugepage will not be zapped, if the page fault runs first it
    will be zapped.
    
    Altering pmd_bad() not to error out if it finds hugepmds won't be enough
    to fix this, because zap_pmd_range would then proceed to call
    zap_pte_range (which would be incorrect if the pmd become a
    pmd_trans_huge()).
    
    The simplest way to fix this is to read the pmd in the local stack
    (regardless of what we read, no need of actual CPU barriers, only
    compiler barrier needed), and be sure it is not changing under the code
    that computes its value.  Even if the real pmd is changing under the
    value we hold on the stack, we don't care.  If we actually end up in
    zap_pte_range it means the pmd was not none already and it was not huge,
    and it can't become huge from under us (khugepaged locking explained
    above).
    
    All we need is to enforce that there is no way anymore that in a code
    path like below, pmd_trans_huge can be false, but pmd_none_or_clear_bad
    can run into a hugepmd.  The overhead of a barrier() is just a compiler
    tweak and should not be measurable (I only added it for THP builds).  I
    don't exclude different compiler versions may have prevented the race
    too by caching the value of *pmd on the stack (that hasn't been
    verified, but it wouldn't be impossible considering
    pmd_none_or_clear_bad, pmd_bad, pmd_trans_huge, pmd_none are all inlines
    and there's no external function called in between pmd_trans_huge and
    pmd_none_or_clear_bad).
    
                    if (pmd_trans_huge(*pmd)) {
                            if (next-addr != HPAGE_PMD_SIZE) {
                                    VM_BUG_ON(!rwsem_is_locked(&tlb->mm->mmap_sem));
                                    split_huge_page_pmd(vma->vm_mm, pmd);
                            } else if (zap_huge_pmd(tlb, vma, pmd, addr))
                                    continue;
                            /* fall through */
                    }
                    if (pmd_none_or_clear_bad(pmd))
    
    Because this race condition could be exercised without special
    privileges this was reported in CVE-2012-1179.
    
    The race was identified and fully explained by Ulrich who debugged it.
    I'm quoting his accurate explanation below, for reference.
    
    ====== start quote =======
          mapcount 0 page_mapcount 1
          kernel BUG at mm/huge_memory.c:1384!
    
        At some point prior to the panic, a "bad pmd ..." message similar to the
        following is logged on the console:
    
          mm/memory.c:145: bad pmd ffff8800376e1f98(80000000314000e7).
    
        The "bad pmd ..." message is logged by pmd_clear_bad() before it clears
        the page's PMD table entry.
    
            143 void pmd_clear_bad(pmd_t *pmd)
            144 {
        ->  145         pmd_ERROR(*pmd);
            146         pmd_clear(pmd);
            147 }
    
        After the PMD table entry has been cleared, there is an inconsistency
        between the actual number of PMD table entries that are mapping the page
        and the page's map count (_mapcount field in struct page). When the page
        is subsequently reclaimed, __split_huge_page() detects this inconsistency.
    
           1381         if (mapcount != page_mapcount(page))
           1382                 printk(KERN_ERR "mapcount %d page_mapcount %d\n",
           1383                        mapcount, page_mapcount(page));
        -> 1384         BUG_ON(mapcount != page_mapcount(page));
    
        The root cause of the problem is a race of two threads in a multithreaded
        process. Thread B incurs a page fault on a virtual address that has never
        been accessed (PMD entry is zero) while Thread A is executing an madvise()
        system call on a virtual address within the same 2 MB (huge page) range.
    
                   virtual address space
                  .---------------------.
                  |                     |
                  |                     |
                .-|---------------------|
                | |                     |
                | |                     |<-- B(fault)
                | |                     |
          2 MB  | |/////////////////////|-.
          huge <  |/////////////////////|  > A(range)
          page  | |/////////////////////|-'
                | |                     |
                | |                     |
                '-|---------------------|
                  |                     |
                  |                     |
                  '---------------------'
    
        - Thread A is executing an madvise(..., MADV_DONTNEED) system call
          on the virtual address range "A(range)" shown in the picture.
    
        sys_madvise
          // Acquire the semaphore in shared mode.
          down_read(&current->mm->mmap_sem)
          ...
          madvise_vma
            switch (behavior)
            case MADV_DONTNEED:
                 madvise_dontneed
                   zap_page_range
                     unmap_vmas
                       unmap_page_range
                         zap_pud_range
                           zap_pmd_range
                             //
                             // Assume that this huge page has never been accessed.
                             // I.e. content of the PMD entry is zero (not mapped).
                             //
                             if (pmd_trans_huge(*pmd)) {
                                 // We don't get here due to the above assumption.
                             }
                             //
                             // Assume that Thread B incurred a page fault and
                 .---------> // sneaks in here as shown below.
                 |           //
                 |           if (pmd_none_or_clear_bad(pmd))
                 |               {
                 |                 if (unlikely(pmd_bad(*pmd)))
                 |                     pmd_clear_bad
                 |                     {
                 |                       pmd_ERROR
                 |                         // Log "bad pmd ..." message here.
                 |                       pmd_clear
                 |                         // Clear the page's PMD entry.
                 |                         // Thread B incremented the map count
                 |                         // in page_add_new_anon_rmap(), but
                 |                         // now the page is no longer mapped
                 |                         // by a PMD entry (-> inconsistency).
                 |                     }
                 |               }
                 |
                 v
        - Thread B is handling a page fault on virtual address "B(fault)" shown
          in the picture.
    
        ...
        do_page_fault
          __do_page_fault
            // Acquire the semaphore in shared mode.
            down_read_trylock(&mm->mmap_sem)
            ...
            handle_mm_fault
              if (pmd_none(*pmd) && transparent_hugepage_enabled(vma))
                  // We get here due to the above assumption (PMD entry is zero).
                  do_huge_pmd_anonymous_page
                    alloc_hugepage_vma
                      // Allocate a new transparent huge page here.
                    ...
                    __do_huge_pmd_anonymous_page
                      ...
                      spin_lock(&mm->page_table_lock)
                      ...
                      page_add_new_anon_rmap
                        // Here we increment the page's map count (starts at -1).
                        atomic_set(&page->_mapcount, 0)
                      set_pmd_at
                        // Here we set the page's PMD entry which will be cleared
                        // when Thread A calls pmd_clear_bad().
                      ...
                      spin_unlock(&mm->page_table_lock)
    
        The mmap_sem does not prevent the race because both threads are acquiring
        it in shared mode (down_read).  Thread B holds the page_table_lock while
        the page's map count and PMD table entry are updated.  However, Thread A
        does not synchronize on that lock.
    
    ====== end quote =======
    
    [akpm@linux-foundation.org: checkpatch fixes]
    Reported-by: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Jones <davej@redhat.com>
    Acked-by: Larry Woodman <lwoodman@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Mark Salter <msalter@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5a3e1f550cfc86a68729770bcfa28f36b238b34d
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Wed Mar 21 16:33:42 2012 -0700

    mm: thp: fix pmd_bad() triggering in code paths holding mmap_sem read mode
    
    commit 1a5a9906d4e8d1976b701f889d8f35d54b928f25 upstream.
    
    In some cases it may happen that pmd_none_or_clear_bad() is called with
    the mmap_sem hold in read mode.  In those cases the huge page faults can
    allocate hugepmds under pmd_none_or_clear_bad() and that can trigger a
    false positive from pmd_bad() that will not like to see a pmd
    materializing as trans huge.
    
    It's not khugepaged causing the problem, khugepaged holds the mmap_sem
    in write mode (and all those sites must hold the mmap_sem in read mode
    to prevent pagetables to go away from under them, during code review it
    seems vm86 mode on 32bit kernels requires that too unless it's
    restricted to 1 thread per process or UP builds).  The race is only with
    the huge pagefaults that can convert a pmd_none() into a
    pmd_trans_huge().
    
    Effectively all these pmd_none_or_clear_bad() sites running with
    mmap_sem in read mode are somewhat speculative with the page faults, and
    the result is always undefined when they run simultaneously.  This is
    probably why it wasn't common to run into this.  For example if the
    madvise(MADV_DONTNEED) runs zap_page_range() shortly before the page
    fault, the hugepage will not be zapped, if the page fault runs first it
    will be zapped.
    
    Altering pmd_bad() not to error out if it finds hugepmds won't be enough
    to fix this, because zap_pmd_range would then proceed to call
    zap_pte_range (which would be incorrect if the pmd become a
    pmd_trans_huge()).
    
    The simplest way to fix this is to read the pmd in the local stack
    (regardless of what we read, no need of actual CPU barriers, only
    compiler barrier needed), and be sure it is not changing under the code
    that computes its value.  Even if the real pmd is changing under the
    value we hold on the stack, we don't care.  If we actually end up in
    zap_pte_range it means the pmd was not none already and it was not huge,
    and it can't become huge from under us (khugepaged locking explained
    above).
    
    All we need is to enforce that there is no way anymore that in a code
    path like below, pmd_trans_huge can be false, but pmd_none_or_clear_bad
    can run into a hugepmd.  The overhead of a barrier() is just a compiler
    tweak and should not be measurable (I only added it for THP builds).  I
    don't exclude different compiler versions may have prevented the race
    too by caching the value of *pmd on the stack (that hasn't been
    verified, but it wouldn't be impossible considering
    pmd_none_or_clear_bad, pmd_bad, pmd_trans_huge, pmd_none are all inlines
    and there's no external function called in between pmd_trans_huge and
    pmd_none_or_clear_bad).
    
                    if (pmd_trans_huge(*pmd)) {
                            if (next-addr != HPAGE_PMD_SIZE) {
                                    VM_BUG_ON(!rwsem_is_locked(&tlb->mm->mmap_sem));
                                    split_huge_page_pmd(vma->vm_mm, pmd);
                            } else if (zap_huge_pmd(tlb, vma, pmd, addr))
                                    continue;
                            /* fall through */
                    }
                    if (pmd_none_or_clear_bad(pmd))
    
    Because this race condition could be exercised without special
    privileges this was reported in CVE-2012-1179.
    
    The race was identified and fully explained by Ulrich who debugged it.
    I'm quoting his accurate explanation below, for reference.
    
    ====== start quote =======
          mapcount 0 page_mapcount 1
          kernel BUG at mm/huge_memory.c:1384!
    
        At some point prior to the panic, a "bad pmd ..." message similar to the
        following is logged on the console:
    
          mm/memory.c:145: bad pmd ffff8800376e1f98(80000000314000e7).
    
        The "bad pmd ..." message is logged by pmd_clear_bad() before it clears
        the page's PMD table entry.
    
            143 void pmd_clear_bad(pmd_t *pmd)
            144 {
        ->  145         pmd_ERROR(*pmd);
            146         pmd_clear(pmd);
            147 }
    
        After the PMD table entry has been cleared, there is an inconsistency
        between the actual number of PMD table entries that are mapping the page
        and the page's map count (_mapcount field in struct page). When the page
        is subsequently reclaimed, __split_huge_page() detects this inconsistency.
    
           1381         if (mapcount != page_mapcount(page))
           1382                 printk(KERN_ERR "mapcount %d page_mapcount %d\n",
           1383                        mapcount, page_mapcount(page));
        -> 1384         BUG_ON(mapcount != page_mapcount(page));
    
        The root cause of the problem is a race of two threads in a multithreaded
        process. Thread B incurs a page fault on a virtual address that has never
        been accessed (PMD entry is zero) while Thread A is executing an madvise()
        system call on a virtual address within the same 2 MB (huge page) range.
    
                   virtual address space
                  .---------------------.
                  |                     |
                  |                     |
                .-|---------------------|
                | |                     |
                | |                     |<-- B(fault)
                | |                     |
          2 MB  | |/////////////////////|-.
          huge <  |/////////////////////|  > A(range)
          page  | |/////////////////////|-'
                | |                     |
                | |                     |
                '-|---------------------|
                  |                     |
                  |                     |
                  '---------------------'
    
        - Thread A is executing an madvise(..., MADV_DONTNEED) system call
          on the virtual address range "A(range)" shown in the picture.
    
        sys_madvise
          // Acquire the semaphore in shared mode.
          down_read(&current->mm->mmap_sem)
          ...
          madvise_vma
            switch (behavior)
            case MADV_DONTNEED:
                 madvise_dontneed
                   zap_page_range
                     unmap_vmas
                       unmap_page_range
                         zap_pud_range
                           zap_pmd_range
                             //
                             // Assume that this huge page has never been accessed.
                             // I.e. content of the PMD entry is zero (not mapped).
                             //
                             if (pmd_trans_huge(*pmd)) {
                                 // We don't get here due to the above assumption.
                             }
                             //
                             // Assume that Thread B incurred a page fault and
                 .---------> // sneaks in here as shown below.
                 |           //
                 |           if (pmd_none_or_clear_bad(pmd))
                 |               {
                 |                 if (unlikely(pmd_bad(*pmd)))
                 |                     pmd_clear_bad
                 |                     {
                 |                       pmd_ERROR
                 |                         // Log "bad pmd ..." message here.
                 |                       pmd_clear
                 |                         // Clear the page's PMD entry.
                 |                         // Thread B incremented the map count
                 |                         // in page_add_new_anon_rmap(), but
                 |                         // now the page is no longer mapped
                 |                         // by a PMD entry (-> inconsistency).
                 |                     }
                 |               }
                 |
                 v
        - Thread B is handling a page fault on virtual address "B(fault)" shown
          in the picture.
    
        ...
        do_page_fault
          __do_page_fault
            // Acquire the semaphore in shared mode.
            down_read_trylock(&mm->mmap_sem)
            ...
            handle_mm_fault
              if (pmd_none(*pmd) && transparent_hugepage_enabled(vma))
                  // We get here due to the above assumption (PMD entry is zero).
                  do_huge_pmd_anonymous_page
                    alloc_hugepage_vma
                      // Allocate a new transparent huge page here.
                    ...
                    __do_huge_pmd_anonymous_page
                      ...
                      spin_lock(&mm->page_table_lock)
                      ...
                      page_add_new_anon_rmap
                        // Here we increment the page's map count (starts at -1).
                        atomic_set(&page->_mapcount, 0)
                      set_pmd_at
                        // Here we set the page's PMD entry which will be cleared
                        // when Thread A calls pmd_clear_bad().
                      ...
                      spin_unlock(&mm->page_table_lock)
    
        The mmap_sem does not prevent the race because both threads are acquiring
        it in shared mode (down_read).  Thread B holds the page_table_lock while
        the page's map count and PMD table entry are updated.  However, Thread A
        does not synchronize on that lock.
    
    ====== end quote =======
    
    [akpm@linux-foundation.org: checkpatch fixes]
    Reported-by: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Jones <davej@redhat.com>
    Acked-by: Larry Woodman <lwoodman@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Mark Salter <msalter@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 247bc03742545fec2f79939a3b9f738392a0f7b4
Author: Rafael J. Wysocki <rjw@rjwysocki.net>
Date:   Wed Mar 28 23:30:28 2012 +0200

    PM / Sleep: Mitigate race between the freezer and request_firmware()
    
    There is a race condition between the freezer and request_firmware()
    such that if request_firmware() is run on one CPU and
    freeze_processes() is run on another CPU and usermodehelper_disable()
    called by it succeeds to grab umhelper_sem for writing before
    usermodehelper_read_trylock() called from request_firmware()
    acquires it for reading, the request_firmware() will fail and
    trigger a WARN_ON() complaining that it was called at a wrong time.
    However, in fact, it wasn't called at a wrong time and
    freeze_processes() simply happened to be executed simultaneously.
    
    To avoid this race, at least in some cases, modify
    usermodehelper_read_trylock() so that it doesn't fail if the
    freezing of tasks has just started and hasn't been completed yet.
    Instead, during the freezing of tasks, it will try to freeze the
    task that has called it so that it can wait until user space is
    thawed without triggering the scary warning.
    
    For this purpose, change usermodehelper_disabled so that it can
    take three different values, UMH_ENABLED (0), UMH_FREEZING and
    UMH_DISABLED.  The first one means that usermode helpers are
    enabled, the last one means "hard disable" (i.e. the system is not
    ready for usermode helpers to be used) and the second one
    is reserved for the freezer.  Namely, when freeze_processes() is
    started, it sets usermodehelper_disabled to UMH_FREEZING which
    tells usermodehelper_read_trylock() that it shouldn't fail just
    yet and should call try_to_freeze() if woken up and cannot
    return immediately.  This way all freezable tasks that happen
    to call request_firmware() right before freeze_processes() is
    started and lose the race for umhelper_sem with it will be
    frozen and will sleep until thaw_processes() unsets
    usermodehelper_disabled.  [For the non-freezable callers of
    request_firmware() the race for umhelper_sem against
    freeze_processes() is unfortunately unavoidable.]
    
    Reported-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: stable@vger.kernel.org

commit 9b78c1da60b3c62ccdd1509f0902ad19ceaf776b
Author: Rafael J. Wysocki <rjw@rjwysocki.net>
Date:   Wed Mar 28 23:30:02 2012 +0200

    firmware_class: Do not warn that system is not ready from async loads
    
    If firmware is requested asynchronously, by calling
    request_firmware_nowait(), there is no reason to fail the request
    (and warn the user) when the system is (presumably temporarily)
    unready to handle it (because user space is not available yet or
    frozen).  For this reason, introduce an alternative routine for
    read-locking umhelper_sem, usermodehelper_read_lock_wait(), that
    will wait for usermodehelper_disabled to be unset (possibly with
    a timeout) and make request_firmware_work_func() use it instead of
    usermodehelper_read_trylock().
    
    Accordingly, modify request_firmware() so that it uses
    usermodehelper_read_trylock() to acquire umhelper_sem and remove
    the code related to that lock from _request_firmware().
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: stable@vger.kernel.org

commit fe2e39d8782d885755139304d8dba0b3e5bfa878
Author: Rafael J. Wysocki <rjw@rjwysocki.net>
Date:   Wed Mar 28 23:29:45 2012 +0200

    firmware_class: Rework usermodehelper check
    
    Instead of two functions, read_lock_usermodehelper() and
    usermodehelper_is_disabled(), used in combination, introduce
    usermodehelper_read_trylock() that will only return with umhelper_sem
    held if usermodehelper_disabled is unset (and will return -EAGAIN
    otherwise) and make _request_firmware() use it.
    
    Rename read_unlock_usermodehelper() to
    usermodehelper_read_unlock() to follow the naming convention of the
    new function.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: stable@vger.kernel.org

commit 1a5a9906d4e8d1976b701f889d8f35d54b928f25
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Wed Mar 21 16:33:42 2012 -0700

    mm: thp: fix pmd_bad() triggering in code paths holding mmap_sem read mode
    
    In some cases it may happen that pmd_none_or_clear_bad() is called with
    the mmap_sem hold in read mode.  In those cases the huge page faults can
    allocate hugepmds under pmd_none_or_clear_bad() and that can trigger a
    false positive from pmd_bad() that will not like to see a pmd
    materializing as trans huge.
    
    It's not khugepaged causing the problem, khugepaged holds the mmap_sem
    in write mode (and all those sites must hold the mmap_sem in read mode
    to prevent pagetables to go away from under them, during code review it
    seems vm86 mode on 32bit kernels requires that too unless it's
    restricted to 1 thread per process or UP builds).  The race is only with
    the huge pagefaults that can convert a pmd_none() into a
    pmd_trans_huge().
    
    Effectively all these pmd_none_or_clear_bad() sites running with
    mmap_sem in read mode are somewhat speculative with the page faults, and
    the result is always undefined when they run simultaneously.  This is
    probably why it wasn't common to run into this.  For example if the
    madvise(MADV_DONTNEED) runs zap_page_range() shortly before the page
    fault, the hugepage will not be zapped, if the page fault runs first it
    will be zapped.
    
    Altering pmd_bad() not to error out if it finds hugepmds won't be enough
    to fix this, because zap_pmd_range would then proceed to call
    zap_pte_range (which would be incorrect if the pmd become a
    pmd_trans_huge()).
    
    The simplest way to fix this is to read the pmd in the local stack
    (regardless of what we read, no need of actual CPU barriers, only
    compiler barrier needed), and be sure it is not changing under the code
    that computes its value.  Even if the real pmd is changing under the
    value we hold on the stack, we don't care.  If we actually end up in
    zap_pte_range it means the pmd was not none already and it was not huge,
    and it can't become huge from under us (khugepaged locking explained
    above).
    
    All we need is to enforce that there is no way anymore that in a code
    path like below, pmd_trans_huge can be false, but pmd_none_or_clear_bad
    can run into a hugepmd.  The overhead of a barrier() is just a compiler
    tweak and should not be measurable (I only added it for THP builds).  I
    don't exclude different compiler versions may have prevented the race
    too by caching the value of *pmd on the stack (that hasn't been
    verified, but it wouldn't be impossible considering
    pmd_none_or_clear_bad, pmd_bad, pmd_trans_huge, pmd_none are all inlines
    and there's no external function called in between pmd_trans_huge and
    pmd_none_or_clear_bad).
    
                    if (pmd_trans_huge(*pmd)) {
                            if (next-addr != HPAGE_PMD_SIZE) {
                                    VM_BUG_ON(!rwsem_is_locked(&tlb->mm->mmap_sem));
                                    split_huge_page_pmd(vma->vm_mm, pmd);
                            } else if (zap_huge_pmd(tlb, vma, pmd, addr))
                                    continue;
                            /* fall through */
                    }
                    if (pmd_none_or_clear_bad(pmd))
    
    Because this race condition could be exercised without special
    privileges this was reported in CVE-2012-1179.
    
    The race was identified and fully explained by Ulrich who debugged it.
    I'm quoting his accurate explanation below, for reference.
    
    ====== start quote =======
          mapcount 0 page_mapcount 1
          kernel BUG at mm/huge_memory.c:1384!
    
        At some point prior to the panic, a "bad pmd ..." message similar to the
        following is logged on the console:
    
          mm/memory.c:145: bad pmd ffff8800376e1f98(80000000314000e7).
    
        The "bad pmd ..." message is logged by pmd_clear_bad() before it clears
        the page's PMD table entry.
    
            143 void pmd_clear_bad(pmd_t *pmd)
            144 {
        ->  145         pmd_ERROR(*pmd);
            146         pmd_clear(pmd);
            147 }
    
        After the PMD table entry has been cleared, there is an inconsistency
        between the actual number of PMD table entries that are mapping the page
        and the page's map count (_mapcount field in struct page). When the page
        is subsequently reclaimed, __split_huge_page() detects this inconsistency.
    
           1381         if (mapcount != page_mapcount(page))
           1382                 printk(KERN_ERR "mapcount %d page_mapcount %d\n",
           1383                        mapcount, page_mapcount(page));
        -> 1384         BUG_ON(mapcount != page_mapcount(page));
    
        The root cause of the problem is a race of two threads in a multithreaded
        process. Thread B incurs a page fault on a virtual address that has never
        been accessed (PMD entry is zero) while Thread A is executing an madvise()
        system call on a virtual address within the same 2 MB (huge page) range.
    
                   virtual address space
                  .---------------------.
                  |                     |
                  |                     |
                .-|---------------------|
                | |                     |
                | |                     |<-- B(fault)
                | |                     |
          2 MB  | |/////////////////////|-.
          huge <  |/////////////////////|  > A(range)
          page  | |/////////////////////|-'
                | |                     |
                | |                     |
                '-|---------------------|
                  |                     |
                  |                     |
                  '---------------------'
    
        - Thread A is executing an madvise(..., MADV_DONTNEED) system call
          on the virtual address range "A(range)" shown in the picture.
    
        sys_madvise
          // Acquire the semaphore in shared mode.
          down_read(&current->mm->mmap_sem)
          ...
          madvise_vma
            switch (behavior)
            case MADV_DONTNEED:
                 madvise_dontneed
                   zap_page_range
                     unmap_vmas
                       unmap_page_range
                         zap_pud_range
                           zap_pmd_range
                             //
                             // Assume that this huge page has never been accessed.
                             // I.e. content of the PMD entry is zero (not mapped).
                             //
                             if (pmd_trans_huge(*pmd)) {
                                 // We don't get here due to the above assumption.
                             }
                             //
                             // Assume that Thread B incurred a page fault and
                 .---------> // sneaks in here as shown below.
                 |           //
                 |           if (pmd_none_or_clear_bad(pmd))
                 |               {
                 |                 if (unlikely(pmd_bad(*pmd)))
                 |                     pmd_clear_bad
                 |                     {
                 |                       pmd_ERROR
                 |                         // Log "bad pmd ..." message here.
                 |                       pmd_clear
                 |                         // Clear the page's PMD entry.
                 |                         // Thread B incremented the map count
                 |                         // in page_add_new_anon_rmap(), but
                 |                         // now the page is no longer mapped
                 |                         // by a PMD entry (-> inconsistency).
                 |                     }
                 |               }
                 |
                 v
        - Thread B is handling a page fault on virtual address "B(fault)" shown
          in the picture.
    
        ...
        do_page_fault
          __do_page_fault
            // Acquire the semaphore in shared mode.
            down_read_trylock(&mm->mmap_sem)
            ...
            handle_mm_fault
              if (pmd_none(*pmd) && transparent_hugepage_enabled(vma))
                  // We get here due to the above assumption (PMD entry is zero).
                  do_huge_pmd_anonymous_page
                    alloc_hugepage_vma
                      // Allocate a new transparent huge page here.
                    ...
                    __do_huge_pmd_anonymous_page
                      ...
                      spin_lock(&mm->page_table_lock)
                      ...
                      page_add_new_anon_rmap
                        // Here we increment the page's map count (starts at -1).
                        atomic_set(&page->_mapcount, 0)
                      set_pmd_at
                        // Here we set the page's PMD entry which will be cleared
                        // when Thread A calls pmd_clear_bad().
                      ...
                      spin_unlock(&mm->page_table_lock)
    
        The mmap_sem does not prevent the race because both threads are acquiring
        it in shared mode (down_read).  Thread B holds the page_table_lock while
        the page's map count and PMD table entry are updated.  However, Thread A
        does not synchronize on that lock.
    
    ====== end quote =======
    
    [akpm@linux-foundation.org: checkpatch fixes]
    Reported-by: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Jones <davej@redhat.com>
    Acked-by: Larry Woodman <lwoodman@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: <stable@vger.kernel.org>            [2.6.38+]
    Cc: Mark Salter <msalter@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 3d470fc385defa60d9af610f05db8e7f8b4f2f5e
Author: Hugh Dickins <hughd@google.com>
Date:   Mon Oct 31 17:09:43 2011 -0700

    mm: munlock use mapcount to avoid terrible overhead
    
    A process spent 30 minutes exiting, just munlocking the pages of a large
    anonymous area that had been alternately mprotected into page-sized vmas:
    for every single page there's an anon_vma walk through all the other
    little vmas to find the right one.
    
    A general fix to that would be a lot more complicated (use prio_tree on
    anon_vma?), but there's one very simple thing we can do to speed up the
    common case: if a page to be munlocked is mapped only once, then it is our
    vma that it is mapped into, and there's no need whatever to walk through
    all the others.
    
    Okay, there is a very remote race in munlock_vma_pages_range(), if between
    its follow_page() and lock_page(), another process were to munlock the
    same page, then page reclaim remove it from our vma, then another process
    mlock it again.  We would find it with page_mapcount 1, yet it's still
    mlocked in another process.  But never mind, that's much less likely than
    the down_read_trylock() failure which munlocking already tolerates (in
    try_to_unmap_one()): in due course page reclaim will discover and move the
    page to unevictable instead.
    
    [akpm@linux-foundation.org: add comment]
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 3c5ead52ed68406c0ee789024c4ae581be8bcee4
Author: Chris Metcalf <cmetcalf@tilera.com>
Date:   Tue Mar 1 13:30:15 2011 -0500

    arch/tile: fix deadlock bugs in rwlock implementation
    
    The first issue fixed in this patch is that pending rwlock write locks
    could lock out new readers; this could cause a deadlock if a read lock was
    held on cpu 1, a write lock was then attempted on cpu 2 and was pending,
    and cpu 1 was interrupted and attempted to re-acquire a read lock.
    The write lock code was modified to not lock out new readers.
    
    The second issue fixed is that there was a narrow race window where a tns
    instruction had been issued (setting the lock value to "1") and the store
    instruction to reset the lock value correctly had not yet been issued.
    In this case, if an interrupt occurred and the same cpu then tried to
    manipulate the lock, it would find the lock value set to "1" and spin
    forever, assuming some other cpu was partway through updating it.  The fix
    is to enforce an interrupt critical section around the tns/store pair.
    
    In addition, this change now arranges to always validate that after
    a readlock we have not wrapped around the count of readers, which
    is only eight bits.
    
    Since these changes make the rwlock "fast path" code heavier weight,
    I decided to move all the rwlock code all out of line, leaving only the
    conventional spinlock code with fastpath inlines.  Since the read_lock
    and read_trylock implementations ended up very similar, I just expressed
    read_lock in terms of read_trylock.
    
    As part of this change I also eliminate support for the now-obsolete
    tns_atomic mode.
    
    Signed-off-by: Chris Metcalf <cmetcalf@tilera.com>

commit dfe076b0971a783469bc2066e85d46e23c8acb1c
Author: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
Date:   Thu Jan 13 15:47:41 2011 -0800

    memcg: fix deadlock between cpuset and memcg
    
    Commit b1dd693e ("memcg: avoid deadlock between move charge and
    try_charge()") can cause another deadlock about mmap_sem on task migration
    if cpuset and memcg are mounted onto the same mount point.
    
    After the commit, cgroup_attach_task() has sequence like:
    
    cgroup_attach_task()
      ss->can_attach()
        cpuset_can_attach()
        mem_cgroup_can_attach()
          down_read(&mmap_sem)        (1)
      ss->attach()
        cpuset_attach()
          mpol_rebind_mm()
            down_write(&mmap_sem)     (2)
            up_write(&mmap_sem)
          cpuset_migrate_mm()
            do_migrate_pages()
              down_read(&mmap_sem)
              up_read(&mmap_sem)
        mem_cgroup_move_task()
          mem_cgroup_clear_mc()
            up_read(&mmap_sem)
    
    We can cause deadlock at (2) because we've already aquire the mmap_sem at (1).
    
    But the commit itself is necessary to fix deadlocks which have existed
    before the commit like:
    
    Ex.1)
                    move charge             |        try charge
      --------------------------------------+------------------------------
        mem_cgroup_can_attach()             |  down_write(&mmap_sem)
          mc.moving_task = current          |    ..
          mem_cgroup_precharge_mc()         |  __mem_cgroup_try_charge()
            mem_cgroup_count_precharge()    |    prepare_to_wait()
              down_read(&mmap_sem)          |    if (mc.moving_task)
              -> cannot aquire the lock     |    -> true
                                            |      schedule()
                                            |      -> move charge should wake it up
    
    Ex.2)
                    move charge             |        try charge
      --------------------------------------+------------------------------
        mem_cgroup_can_attach()             |
          mc.moving_task = current          |
          mem_cgroup_precharge_mc()         |
            mem_cgroup_count_precharge()    |
              down_read(&mmap_sem)          |
              ..                            |
              up_read(&mmap_sem)            |
                                            |  down_write(&mmap_sem)
        mem_cgroup_move_task()              |    ..
          mem_cgroup_move_charge()          |  __mem_cgroup_try_charge()
            down_read(&mmap_sem)            |    prepare_to_wait()
            -> cannot aquire the lock       |    if (mc.moving_task)
                                            |    -> true
                                            |      schedule()
                                            |      -> move charge should wake it up
    
    This patch fixes all of these problems by:
    1. revert the commit.
    2. To fix the Ex.1, we set mc.moving_task after mem_cgroup_count_precharge()
       has released the mmap_sem.
    3. To fix the Ex.2, we use down_read_trylock() instead of down_read() in
       mem_cgroup_move_charge() and, if it has failed to aquire the lock, cancel
       all extra charges, wake up all waiters, and retry trylock.
    
    Signed-off-by: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Reported-by: Ben Blum <bblum@andrew.cmu.edu>
    Cc: Miao Xie <miaox@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Paul Menage <menage@google.com>
    Cc: Hiroyuki Kamezawa <kamezawa.hiroyuki@gmail.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit f75ca962037ffd639a44fd88933cd9b84c4c4411
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Aug 10 18:03:02 2010 -0700

    memcg: avoid css_get()
    
    Now, memory cgroup increments css(cgroup subsys state)'s reference count
    per a charged page.  And the reference count is kept until the page is
    uncharged.  But this has 2 bad effect.
    
     1. Because css_get/put calls atomic_inc()/dec, heavy call of them
        on large smp will not scale well.
     2. Because css's refcnt cannot be in a state as "ready-to-release",
        cgroup's notify_on_release handler can't work with memcg.
     3. css's refcnt is atomic_t, it means smaller than 32bit. Maybe too small.
    
    This has been a problem since the 1st merge of memcg.
    
    This is a trial to remove css's refcnt per a page. Even if we remove
    refcnt, pre_destroy() does enough synchronization as
      - check res->usage == 0.
      - check no pages on LRU.
    
    This patch removes css's refcnt per page.  Even after this patch, at the
    1st look, it seems css_get() is still called in try_charge().
    
    But the logic is.
    
      - If a memcg of mm->owner is cached one, consume_stock() will work.
        At success, return immediately.
      - If consume_stock returns false, css_get() is called and go to
        slow path which may be blocked. At the end of slow path,
        css_put() is called and restart from the start if necessary.
    
    So, in the fast path, we don't call css_get() and can avoid access to
    shared counter. This patch can make the most possible case fast.
    
    Here is a result of multi-threaded page fault benchmark.
    
    [Before]
        25.32%  multi-fault-all  [kernel.kallsyms]      [k] clear_page_c
         9.30%  multi-fault-all  [kernel.kallsyms]      [k] _raw_spin_lock_irqsave
         8.02%  multi-fault-all  [kernel.kallsyms]      [k] try_get_mem_cgroup_from_mm <=====(*)
         7.83%  multi-fault-all  [kernel.kallsyms]      [k] down_read_trylock
         5.38%  multi-fault-all  [kernel.kallsyms]      [k] __css_put
         5.29%  multi-fault-all  [kernel.kallsyms]      [k] __alloc_pages_nodemask
         4.92%  multi-fault-all  [kernel.kallsyms]      [k] _raw_spin_lock_irq
         4.24%  multi-fault-all  [kernel.kallsyms]      [k] up_read
         3.53%  multi-fault-all  [kernel.kallsyms]      [k] css_put
         2.11%  multi-fault-all  [kernel.kallsyms]      [k] handle_mm_fault
         1.76%  multi-fault-all  [kernel.kallsyms]      [k] __rmqueue
         1.64%  multi-fault-all  [kernel.kallsyms]      [k] __mem_cgroup_commit_charge
    
    [After]
        28.41%  multi-fault-all  [kernel.kallsyms]      [k] clear_page_c
        10.08%  multi-fault-all  [kernel.kallsyms]      [k] _raw_spin_lock_irq
         9.58%  multi-fault-all  [kernel.kallsyms]      [k] down_read_trylock
         9.38%  multi-fault-all  [kernel.kallsyms]      [k] _raw_spin_lock_irqsave
         5.86%  multi-fault-all  [kernel.kallsyms]      [k] __alloc_pages_nodemask
         5.65%  multi-fault-all  [kernel.kallsyms]      [k] up_read
         2.82%  multi-fault-all  [kernel.kallsyms]      [k] handle_mm_fault
         2.64%  multi-fault-all  [kernel.kallsyms]      [k] mem_cgroup_add_lru_list
         2.48%  multi-fault-all  [kernel.kallsyms]      [k] __mem_cgroup_commit_charge
    
    Then, 8.02% of try_get_mem_cgroup_from_mm() disappears because this patch
    removes css_tryget() in it. (But yes, this is an extreme case.)
    
    Signed-off-by: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 846f99749ab68bbc7f75c74fec305de675b1a1bf
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Jan 2 13:37:12 2010 -0800

    sysfs: Add lockdep annotations for the sysfs active reference
    
    Holding locks over device_del -> kobject_del -> sysfs_deactivate can
    cause deadlocks if those same locks are grabbed in sysfs show or store
    methods.
    
    The I model s_active count + completion as a sleeping read/write lock.
    I describe to lockdep sysfs_get_active as a read_trylock,
    sysfs_put_active as a read_unlock, and sysfs_deactivate as a
    write_lock and write_unlock pair.  This seems to capture the essence
    for purposes of finding deadlocks, and in my testing gives finds real
    issues and ignores non-issues.
    
    This brings us back to holding locks over kobject_del is a problem
    that ideally we should find a way of addressing, but at least lockdep
    can tell us about the problems instead of requiring developers to debug
    rare strange system deadlocks, that happen when sysfs files are removed
    while being written to.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit c9286b7e293a1ea054e857ff3f5a23d0ad8d4f36
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 16 19:50:38 2009 +0100

    locking: Remove unused prototype
    
    commit 910067d1(remove generic__raw_read_trylock()) removed the
    implementation but left the prototype around. Remove it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 1b290d670ffa883b7e062177463a8efd00eaa2c1
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Nov 23 15:42:35 2009 +0100

    perf tools: Add support for breakpoint events in perf tools
    
    Add the breakpoint events support with this new sysnopsis:
    
      mem:addr[:access]
    
    Where addr is a raw addr value in the kernel and access can be
    either [r][w][x]
    
    Example to profile tasklist_lock:
    
            $ grep tasklist_lock /proc/kallsyms
            ffffffff8189c000 D tasklist_lock
    
            $ perf record -e mem:0xffffffff8189c000:rw -a -f -c 1
            $ perf report
    
            # Samples: 62
            #
            # Overhead          Command  Shared Object  Symbol
            # ........  ...............  .............  ......
            #
                29.03%          swapper  [kernel]       [k] _raw_read_trylock
                29.03%          swapper  [kernel]       [k] _raw_read_unlock
                19.35%             init  [kernel]       [k] _raw_read_trylock
                19.35%             init  [kernel]       [k] _raw_read_unlock
                 1.61%         events/0  [kernel]       [k] _raw_read_trylock
                 1.61%         events/0  [kernel]       [k] _raw_read_unlock
    
    Coming soon:
    
     - Support for symbols in the event definition.
    
     - Default period to 1 for breakpoint events because these are
       not high frequency events. The same thing is needed for trace
       events.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    LKML-Reference: <1258987355-8751-4-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Prasad <prasad@linux.vnet.ibm.com>

commit 1d2127123db9b1821959c2b8b7473dd7ffcdf527
Author: Imre Deak <imre.deak@nokia.com>
Date:   Mon Oct 5 13:40:44 2009 +0100

    ARM: 5742/1: ARM: add debug check for invalid kernel page faults
    
    According to the following in arch/arm/mm/fault.c page faults from
    kernel mode are invalid if mmap_sem is already held and there is
    no exception handler defined for the faulting instruction:
    
    /*
     * As per x86, we may deadlock here.  However, since the kernel only
     * validly references user space from well defined areas of the code,
     * we can bug out early if this is from code which shouldn't.
     */
    if (!down_read_trylock(&mm->mmap_sem)) {
            if (!user_mode(regs) && !search_exception_tables(regs->ARM_pc))
                    goto no_context;
    
    Since mmap_sem can be held at arbitrary times by another thread this
    also means that any page faults from kernel mode are invalid if no
    exception handler is defined for them, regardless whether mmap_sem is
    held at the time of fault.
    
    To easier detect code that can trigger the above error, add a check
    also for the case where mmap_sem is acquired. As this has an overhead
    make it a VM debug check.
    
    Signed-off-by: Imre Deak <imre.deak@nokia.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 47eb6b9c8fa963c9f49967ad1d9d7ec947d15b68
Author: Ryusuke Konishi <konishi.ryusuke@lab.ntt.co.jp>
Date:   Thu Apr 30 02:21:00 2009 +0900

    nilfs2: fix possible circular locking for get information ioctls
    
    This is one of two patches which are to correct possible circular
    locking between mm->mmap_sem and nilfs->ns_segctor_sem.
    
    The problem was detected by lockdep check as follows:
    
     =======================================================
     [ INFO: possible circular locking dependency detected ]
     2.6.30-rc3-nilfs-00002-g3552613 #6
     -------------------------------------------------------
     mmap/5418 is trying to acquire lock:
     (&nilfs->ns_segctor_sem){++++.+}, at: [<d0d0e852>] nilfs_transaction_begin+0xb6/0x10c [nilfs2]
    
     but task is already holding lock:
     (&mm->mmap_sem){++++++}, at: [<c043700a>] do_page_fault+0x1d8/0x30a
    
     which lock already depends on the new lock.
    
     the existing dependency chain (in reverse order) is:
    
     -> #1 (&mm->mmap_sem){++++++}:
     [<c01470a5>] __lock_acquire+0x1066/0x13b0
     [<c01474a9>] lock_acquire+0xba/0xdd
     [<c01836bc>] might_fault+0x68/0x88
     [<c023c730>] copy_to_user+0x2c/0xfc
     [<d0d11b4f>] nilfs_ioctl_wrap_copy+0x103/0x160 [nilfs2]
     [<d0d11fa9>] nilfs_ioctl+0x30a/0x3b0 [nilfs2]
     [<c01a3be7>] vfs_ioctl+0x22/0x69
     [<c01a408e>] do_vfs_ioctl+0x460/0x499
     [<c01a4107>] sys_ioctl+0x40/0x5a
     [<c01031a4>] sysenter_do_call+0x12/0x38
     [<ffffffff>] 0xffffffff
    
     -> #0 (&nilfs->ns_segctor_sem){++++.+}:
     [<c0146e0b>] __lock_acquire+0xdcc/0x13b0
     [<c01474a9>] lock_acquire+0xba/0xdd
     [<c0433f1d>] down_read+0x2a/0x3e
     [<d0d0e852>] nilfs_transaction_begin+0xb6/0x10c [nilfs2]
     [<d0cfe0e5>] nilfs_page_mkwrite+0xe7/0x154 [nilfs2]
     [<c0183b0b>] __do_fault+0x165/0x376
     [<c01855cd>] handle_mm_fault+0x287/0x5d1
     [<c043712d>] do_page_fault+0x2fb/0x30a
     [<c0435462>] error_code+0x72/0x78
     [<ffffffff>] 0xffffffff
    
     other info that might help us debug this:
    
     1 lock held by mmap/5418:
     #0:  (&mm->mmap_sem){++++++}, at: [<c043700a>] do_page_fault+0x1d8/0x30a
    
     stack backtrace:
     Pid: 5418, comm: mmap Not tainted 2.6.30-rc3-nilfs-00002-g3552613 #6
     Call Trace:
     [<c0432145>] ? printk+0xf/0x12
     [<c0145c48>] print_circular_bug_tail+0xaa/0xb5
     [<c0146e0b>] __lock_acquire+0xdcc/0x13b0
     [<d0d10149>] ? nilfs_sufile_get_stat+0x1e/0x105 [nilfs2]
     [<c013b59a>] ? up_read+0x16/0x2c
     [<d0d10225>] ? nilfs_sufile_get_stat+0xfa/0x105 [nilfs2]
     [<c01474a9>] lock_acquire+0xba/0xdd
     [<d0d0e852>] ? nilfs_transaction_begin+0xb6/0x10c [nilfs2]
     [<c0433f1d>] down_read+0x2a/0x3e
     [<d0d0e852>] ? nilfs_transaction_begin+0xb6/0x10c [nilfs2]
     [<d0d0e852>] nilfs_transaction_begin+0xb6/0x10c [nilfs2]
     [<d0cfe0e5>] nilfs_page_mkwrite+0xe7/0x154 [nilfs2]
     [<c0183b0b>] __do_fault+0x165/0x376
     [<c01855cd>] handle_mm_fault+0x287/0x5d1
     [<c043700a>] ? do_page_fault+0x1d8/0x30a
     [<c013b54f>] ? down_read_trylock+0x39/0x43
     [<c043712d>] do_page_fault+0x2fb/0x30a
     [<c0436e32>] ? do_page_fault+0x0/0x30a
     [<c0435462>] error_code+0x72/0x78
     [<c0436e32>] ? do_page_fault+0x0/0x30a
    
    This makes the lock granularity of nilfs->ns_segctor_sem finer than
    that of the mmap semaphore for ioctl commands except
    nilfs_clean_segments().
    
    The successive patch ("nilfs2: fix lock order reversal in
    nilfs_clean_segments ioctl") is required to fully resolve the problem.
    
    Signed-off-by: Ryusuke Konishi <konishi.ryusuke@lab.ntt.co.jp>

commit efed792d6738964f399a508ef9e831cd60fa4657
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Mar 4 12:32:55 2009 +0100

    tracing: add lockdep tracepoints for lock acquire/release
    
    Augment the traces with lock names when lockdep is available:
    
     1)               |  down_read_trylock() {
     1)               |    _spin_lock_irqsave() {
     1)               |      /* lock_acquire: &sem->wait_lock */
     1)   4.201 us    |    }
     1)               |    _spin_unlock_irqrestore() {
     1)               |      /* lock_release: &sem->wait_lock */
     1)   3.523 us    |    }
     1)               |  /* lock_acquire: try read &mm->mmap_sem */
     1) + 13.386 us   |  }
     1)   1.635 us    |  find_vma();
     1)               |  handle_mm_fault() {
     1)               |    __do_fault() {
     1)               |      filemap_fault() {
     1)               |        find_lock_page() {
     1)               |          find_get_page() {
     1)               |            /* lock_acquire: read rcu_read_lock */
     1)               |            /* lock_release: rcu_read_lock */
     1)   5.697 us    |          }
     1)   8.158 us    |        }
     1) + 11.079 us   |      }
     1)               |      _spin_lock() {
     1)               |        /* lock_acquire: __pte_lockptr(page) */
     1)   3.949 us    |      }
     1)   1.460 us    |      page_add_file_rmap();
     1)               |      _spin_unlock() {
     1)               |        /* lock_release: __pte_lockptr(page) */
     1)   3.115 us    |      }
     1)               |      unlock_page() {
     1)   1.421 us    |        page_waitqueue();
     1)   1.220 us    |        __wake_up_bit();
     1)   6.519 us    |      }
     1) + 34.328 us   |    }
     1) + 37.452 us   |  }
     1)               |  up_read() {
     1)               |  /* lock_release: &mm->mmap_sem */
     1)               |    _spin_lock_irqsave() {
     1)               |      /* lock_acquire: &sem->wait_lock */
     1)   3.865 us    |    }
     1)               |    _spin_unlock_irqrestore() {
     1)               |      /* lock_release: &sem->wait_lock */
     1)   8.562 us    |    }
     1) + 17.370 us   |  }
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: =?ISO-8859-1?Q?T=F6r=F6k?= Edwin <edwintorok@gmail.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1236166375.5330.7209.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 16fd8be997245025ed5252f12306fff28801335b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jan 23 17:37:49 2009 +0100

    x86, mm: fix pte_free()
    
    commit 42ef73fe134732b2e91c0326df5fd568da17c4b2 upstream.
    
    On -rt we were seeing spurious bad page states like:
    
    Bad page state in process 'firefox'
    page:c1bc2380 flags:0x40000000 mapping:c1bc2390 mapcount:0 count:0
    Trying to fix it up, but a reboot is needed
    Backtrace:
    Pid: 503, comm: firefox Not tainted 2.6.26.8-rt13 #3
    [<c043d0f3>] ? printk+0x14/0x19
    [<c0272d4e>] bad_page+0x4e/0x79
    [<c0273831>] free_hot_cold_page+0x5b/0x1d3
    [<c02739f6>] free_hot_page+0xf/0x11
    [<c0273a18>] __free_pages+0x20/0x2b
    [<c027d170>] __pte_alloc+0x87/0x91
    [<c027d25e>] handle_mm_fault+0xe4/0x733
    [<c043f680>] ? rt_mutex_down_read_trylock+0x57/0x63
    [<c043f680>] ? rt_mutex_down_read_trylock+0x57/0x63
    [<c0218875>] do_page_fault+0x36f/0x88a
    
    This is the case where a concurrent fault already installed the PTE and
    we get to free the newly allocated one.
    
    This is due to pgtable_page_ctor() doing the spin_lock_init(&page->ptl)
    which is overlaid with the {private, mapping} struct.
    
    union {
        struct {
            unsigned long private;
            struct address_space *mapping;
        };
        spinlock_t ptl;
        struct kmem_cache *slab;
        struct page *first_page;
    };
    
    Normally the spinlock is small enough to not stomp on page->mapping, but
    PREEMPT_RT=y has huge 'spin'locks.
    
    But lockdep kernels should also be able to trigger this splat, as the
    lock tracking code grows the spinlock to cover page->mapping.
    
    The obvious fix is calling pgtable_page_dtor() like the regular pte free
    path __pte_free_tlb() does.
    
    It seems all architectures except x86 and nm10300 already do this, and
    nm10300 doesn't seem to use pgtable_page_ctor(), which suggests it
    doesn't do SMP or simply doesnt do MMU at all or something.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlsta@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 8ca2918f99b5861359de1805f27b08023c82abd2
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jan 23 17:37:49 2009 +0100

    x86, mm: fix pte_free()
    
    commit 42ef73fe134732b2e91c0326df5fd568da17c4b2 upstream.
    
    On -rt we were seeing spurious bad page states like:
    
    Bad page state in process 'firefox'
    page:c1bc2380 flags:0x40000000 mapping:c1bc2390 mapcount:0 count:0
    Trying to fix it up, but a reboot is needed
    Backtrace:
    Pid: 503, comm: firefox Not tainted 2.6.26.8-rt13 #3
    [<c043d0f3>] ? printk+0x14/0x19
    [<c0272d4e>] bad_page+0x4e/0x79
    [<c0273831>] free_hot_cold_page+0x5b/0x1d3
    [<c02739f6>] free_hot_page+0xf/0x11
    [<c0273a18>] __free_pages+0x20/0x2b
    [<c027d170>] __pte_alloc+0x87/0x91
    [<c027d25e>] handle_mm_fault+0xe4/0x733
    [<c043f680>] ? rt_mutex_down_read_trylock+0x57/0x63
    [<c043f680>] ? rt_mutex_down_read_trylock+0x57/0x63
    [<c0218875>] do_page_fault+0x36f/0x88a
    
    This is the case where a concurrent fault already installed the PTE and
    we get to free the newly allocated one.
    
    This is due to pgtable_page_ctor() doing the spin_lock_init(&page->ptl)
    which is overlaid with the {private, mapping} struct.
    
    union {
        struct {
            unsigned long private;
            struct address_space *mapping;
        };
        spinlock_t ptl;
        struct kmem_cache *slab;
        struct page *first_page;
    };
    
    Normally the spinlock is small enough to not stomp on page->mapping, but
    PREEMPT_RT=y has huge 'spin'locks.
    
    But lockdep kernels should also be able to trigger this splat, as the
    lock tracking code grows the spinlock to cover page->mapping.
    
    The obvious fix is calling pgtable_page_dtor() like the regular pte free
    path __pte_free_tlb() does.
    
    It seems all architectures except x86 and nm10300 already do this, and
    nm10300 doesn't seem to use pgtable_page_ctor(), which suggests it
    doesn't do SMP or simply doesnt do MMU at all or something.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlsta@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 2d4d57db692ea790e185656516e6ebe8791f1788
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Jan 25 12:50:13 2009 -0800

    x86: micro-optimize __raw_read_trylock()
    
    The current version of __raw_read_trylock starts with decrementing the lock
    and read its new value as a separate operation after that.
    
    That makes 3 dereferences (read, write (after sub), read) whereas
    a single atomic_dec_return does only two pointers dereferences (read, write).
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 42ef73fe134732b2e91c0326df5fd568da17c4b2
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jan 23 17:37:49 2009 +0100

    x86, mm: fix pte_free()
    
    On -rt we were seeing spurious bad page states like:
    
    Bad page state in process 'firefox'
    page:c1bc2380 flags:0x40000000 mapping:c1bc2390 mapcount:0 count:0
    Trying to fix it up, but a reboot is needed
    Backtrace:
    Pid: 503, comm: firefox Not tainted 2.6.26.8-rt13 #3
    [<c043d0f3>] ? printk+0x14/0x19
    [<c0272d4e>] bad_page+0x4e/0x79
    [<c0273831>] free_hot_cold_page+0x5b/0x1d3
    [<c02739f6>] free_hot_page+0xf/0x11
    [<c0273a18>] __free_pages+0x20/0x2b
    [<c027d170>] __pte_alloc+0x87/0x91
    [<c027d25e>] handle_mm_fault+0xe4/0x733
    [<c043f680>] ? rt_mutex_down_read_trylock+0x57/0x63
    [<c043f680>] ? rt_mutex_down_read_trylock+0x57/0x63
    [<c0218875>] do_page_fault+0x36f/0x88a
    
    This is the case where a concurrent fault already installed the PTE and
    we get to free the newly allocated one.
    
    This is due to pgtable_page_ctor() doing the spin_lock_init(&page->ptl)
    which is overlaid with the {private, mapping} struct.
    
    union {
        struct {
            unsigned long private;
            struct address_space *mapping;
        };
        spinlock_t ptl;
        struct kmem_cache *slab;
        struct page *first_page;
    };
    
    Normally the spinlock is small enough to not stomp on page->mapping, but
    PREEMPT_RT=y has huge 'spin'locks.
    
    But lockdep kernels should also be able to trigger this splat, as the
    lock tracking code grows the spinlock to cover page->mapping.
    
    The obvious fix is calling pgtable_page_dtor() like the regular pte free
    path __pte_free_tlb() does.
    
    It seems all architectures except x86 and nm10300 already do this, and
    nm10300 doesn't seem to use pgtable_page_ctor(), which suggests it
    doesn't do SMP or simply doesnt do MMU at all or something.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlsta@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: <stable@kernel.org>

commit a309720c876d7ad2e224bfd1982c92ae4364c82e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 7 22:36:02 2008 -0500

    ftrace: display start of CPU buffer in trace output
    
    Impact: change in trace output
    
    Because the trace buffers are per cpu ring buffers, the start of
    the trace can be confusing. If one CPU is very active at the
    end of the trace, its history will not go as far back as the
    other CPU traces.  This means that output for a particular CPU
    may not appear for the first part of a trace.
    
    To help annotate what is happening, and to prevent any more
    confusion, this patch adds a line that annotates the start of
    a CPU buffer output.
    
    For example:
    
           automount-3495  [001]   184.596443: dnotify_parent <-vfs_write
    [...]
           automount-3495  [001]   184.596449: dput <-path_put
           automount-3496  [002]   184.596450: down_read_trylock <-do_page_fault
    [...]
               sshd-3497  [001]   184.597069: up_read <-do_page_fault
              <idle>-0     [000]   184.597074: __exit_idle <-exit_idle
    [...]
           automount-3496  [002]   184.597257: filemap_fault <-__do_fault
              <idle>-0     [003]   184.597261: exit_idle <-smp_apic_timer_interrupt
    
    Note, parsers of a trace output should always ignore any lines that
    start with a '#'.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit b1e0349e2d6a5cb63467ea848ae4f1df095b79d0
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Jul 23 11:43:39 2008 +0300

    KVM: mmu_shrink: kvm_mmu_zap_page requires slots_lock to be held
    
    Original-Commit-Hash: 64f6a0c041bd8fc100a0d655058bdbc31feda03c
    
    kvm_mmu_zap_page() needs slots lock held (rmap_remove->gfn_to_memslot,
    for example).
    
    Since kvm_lock spinlock is held in mmu_shrink(), do a non-blocking
    down_read_trylock().
    
    Untested.
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 5a4c92880493945678315a6df810f7a21f55b985
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Jul 3 18:33:02 2008 -0300

    KVM: mmu_shrink: kvm_mmu_zap_page requires slots_lock to be held
    
    kvm_mmu_zap_page() needs slots lock held (rmap_remove->gfn_to_memslot,
    for example).
    
    Since kvm_lock spinlock is held in mmu_shrink(), do a non-blocking
    down_read_trylock().
    
    Untested.
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>

commit 0ec160dd48b666ddef39d639323d0da26d0b710d
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Mon Jan 21 17:18:24 2008 -0800

    hrtimer: fix section mismatch
    
    Fix section mismatch in hrtimer.c:
    
    WARNING: vmlinux.o(.text+0x50c61): Section mismatch: reference to .init.text: (between 'hrtimer_cpu_notify' and 'down_read_trylock')
    
    Noticed by Johannes Berg and confirmed by Sam Ravnborg.
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@akpm@linux-foundation.org>

commit c3e2a8e64cb2282a406ff6e63f7bd09eb6f61a1d
Author: Jiri Slaby <jirislaby@kernel.org>
Date:   Tue Oct 9 22:15:27 2007 -0300

    V4L/DVB (6308): V4L: zc0301, remove bad usage of ERESTARTSYS
    
    zc0301, remove bad usage of ERESTARTSYS
    
    down_read_trylock can't be interrupted and so ERESTARTSYS would reach
    userspace, which is not permitted. Change it to EAGAIN
    
    Signed-off-by: Jiri Slaby <jirislaby@gmail.com>
    Acked-by: Luca Risolia <luca.risolia@studio.unibo.it>
    Signed-off-by: Mauro Carvalho Chehab <mchehab@infradead.org>

commit 3abff557d5eed7d6fd03aa11353a1c0f329cac2b
Author: Jiri Slaby <jirislaby@kernel.org>
Date:   Tue Oct 9 22:03:12 2007 -0300

    V4L/DVB (6307): V4L: w9968cf, remove bad usage of ERESTARTSYS
    
    w9968cf, remove bad usage of ERESTARTSYS
    
    down_read_trylock can't be interrupted and so ERESTARTSYS would reach
    userspace, which is not permitted. Change it to EAGAIN
    
    Signed-off-by: Jiri Slaby <jirislaby@gmail.com>
    Acked-by: Luca Risolia <luca.risolia@studio.unibo.it>
    Signed-off-by: Mauro Carvalho Chehab <mchehab@infradead.org>

commit e9dfc0b2bc42761410e8db6c252c6c5889e178b8
Author: Mark Fasheh <mark.fasheh@oracle.com>
Date:   Mon May 14 11:38:51 2007 -0700

    ocfs2: trylock in ocfs2_readpage()
    
    Similarly to the page lock / cluster lock inversion in ocfs2_readpage, we
    can deadlock on ip_alloc_sem. We can down_read_trylock() instead and just
    return AOP_TRUNCATED_PAGE if the operation fails.
    
    Signed-off-by: Mark Fasheh <mark.fasheh@oracle.com>

commit 185d84b4e1f6febebbe30d785fe31310dcf9632a
Merge: 03154a271012 063ea774b021
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Tue Mar 6 18:02:46 2007 -0800

    Merge branch 'upstream' of git://ftp.linux-mips.org/pub/scm/upstream-linus
    
    * 'upstream' of git://ftp.linux-mips.org/pub/scm/upstream-linus:
      [MIPS] IP27: Build fix
      [MIPS] Wire up ioprio_set and ioprio_get.
      [MIPS] Fix __raw_read_trylock() to allow multiple readers
      [MIPS] Export __copy_user_inatomic.
      [MIPS] R2 bitops compile fix for gcc < 4.0.
      [MIPS] TX39: Remove redundant tx39_blast_icache() calls
      [MIPS] Cobalt: Fix early printk
      [MIPS] SMTC: De-obscure Malta hooks.
      [MIPS] SMTC: Add fordward declarations for mm_struct and task_struct.
      [MIPS] SMTC: <asm/mips_mt.h> must include <linux/cpumask.h>
      [MIPS] SMTC: <asm/smtc_ipi.h> must include <linux/spinlock.h>
      [MIPS] Atlas, Malta: Fix build warning.

commit d52c2d5a626a2cb1848fa7063b3ab79e2752dac7
Author: Dave Johnson <djohnson+linux-mips@sw.starentnetworks.com>
Date:   Mon Mar 5 20:50:27 2007 -0500

    [MIPS] Fix __raw_read_trylock() to allow multiple readers
    
    A deadlock can occur for mixed irq and non-irq rwlock readers if a 2nd
    reader attempts to take lock by looping around __raw_read_trylock().
    
    Signed-off-by: Dave Johnson <djohnson+linux-mips@sw.starentnetworks.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 7a39f52202a70ff6834e37053e2ee55c7d351621
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Sun Oct 8 14:32:15 2006 +0100

    [PATCH] sparc32 rwlock fix
    
    read_trylock() is broken on sparc32 (doesn't build and didn't work
    right, actually).  Proposed fix:
    
     - make "writer holds lock" distinguishable from "reader tries to grab
       lock"
    
     - have __raw_read_trylock() try to acquire the mutex (in LSB of lock),
       terminating spin if we see that there's writer holding it.  Then do
       the rest as we do in read_lock().
    
    Thanks to Ingo for discussion...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 6ebfc0e20b409f13e62bbb84ce70102b49945cfd
Merge: 1e9abb5b1dc9 fac99d97469e
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Tue Oct 3 08:54:28 2006 -0700

    Merge master.kernel.org:/pub/scm/linux/kernel/git/lethal/sh-2.6
    
    * master.kernel.org:/pub/scm/linux/kernel/git/lethal/sh-2.6:
      sh: Fixup __raw_read_trylock().
      sh: Kill off remaining config.h references.
      sh: Initial gitignore list
      sh: build fixes for defconfigs.
      sh: Kill off more dead headers.
      sh: Set pclk default for SH7705.
      sh: defconfig updates.

commit fac99d97469e7f91102f97101bea620e5f073196
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Tue Oct 3 14:13:09 2006 +0900

    sh: Fixup __raw_read_trylock().
    
    generic__raw_read_trylock() was broken, fix up the __raw_read_trylock()
    implementation for something sensible. Taken from m32r, which has the
    same use cases.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

commit ef6edc9746dc2bfdacf44eefd5f881179971c478
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Sat Sep 30 23:27:43 2006 -0700

    [PATCH] Directed yield: cpu_relax variants for spinlocks and rw-locks
    
    On systems running with virtual cpus there is optimization potential in
    regard to spinlocks and rw-locks.  If the virtual cpu that has taken a lock
    is known to a cpu that wants to acquire the same lock it is beneficial to
    yield the timeslice of the virtual cpu in favour of the cpu that has the
    lock (directed yield).
    
    With CONFIG_PREEMPT="n" this can be implemented by the architecture without
    common code changes.  Powerpc already does this.
    
    With CONFIG_PREEMPT="y" the lock loops are coded with _raw_spin_trylock,
    _raw_read_trylock and _raw_write_trylock in kernel/spinlock.c.  If the lock
    could not be taken cpu_relax is called.  A directed yield is not possible
    because cpu_relax doesn't know anything about the lock.  To be able to
    yield the lock in favour of the current lock holder variants of cpu_relax
    for spinlocks and rw-locks are needed.  The new _raw_spin_relax,
    _raw_read_relax and _raw_write_relax primitives differ from cpu_relax
    insofar that they have an argument: a pointer to the lock structure.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 910067d188d56d80801b71b0ca1f73aa400c7b8c
Author: Matthew Wilcox <willy@infradead.org>
Date:   Fri Sep 29 01:58:36 2006 -0700

    [PATCH] remove generic__raw_read_trylock()
    
    If the cpu has the lock held for write, is interrupted, and the interrupt
    handler calls read_trylock(), it's an instant deadlock.
    
    Now, Dave Miller has subsequently pointed out that we don't have any
    situations where this can occur.  Nevertheless, we should delete
    generic__raw_read_lock (and its associated EXPORT to make Arjan happy) so that
    nobody thinks they can use it.
    
    Acked-by: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 85f651794c46e8e3faf204a767d1caa7f9f278f0
Author: Hirokazu Takata <takata.hirokazu@renesas.com>
Date:   Wed Sep 27 01:50:24 2006 -0700

    [PATCH] m32r: revise __raw_read_trylock()
    
    Signed-off-by: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Matthew Wilcox <matthew@wil.cx>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 65316fd13ad9d82560edbad0a940d684380f7461
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Thu Aug 31 14:16:06 2006 +0100

    [MIPS] Replace generic__raw_read_trylock usage
    
    generic__raw_read_trylock() is a defect generic function actually doing
    a __raw_read_lock ...
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 5a05e5bf204e729b79e9462785f2fa09d5126855
Merge: 08a55c01b504 d17f901defef
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Mon Sep 11 07:55:39 2006 -0700

    Merge master.kernel.org:/home/rmk/linux-2.6-arm
    
    * master.kernel.org:/home/rmk/linux-2.6-arm:
      [ARM] 3778/1: S3C24XX: remove changelogs from include/asm-arm/arch-s3c2410 [simtec]
      [ARM] 3783/1: S3C2412: fix IRQ_EINT0 to IRQ_EINT3 handling
      [ARM] 3779/1: S3C24XX: remove changelogs from include/asm-arm/arch-s3c2410 [left]
      [ARM] 3777/1: S3C24XX:  remove changelogs from include/asm-arm/arch-s3c2410 [regs-*.h]
      [ARM] 3776/1: S3C24XX: remove changelogs from include/asm-arm/arch-s3c2410
      [ARM] 3775/1: S3C24XX: do not add same sysdev_driver to two classes
      [ARM] 3774/1: S3C24XX: SMDK2413 has two machine IDs
      [ARM] 3773/1: Add the HWCAP_VFP bit for the ARM926 CPUs
      [ARM] 3772/1: Fix compilation error in mach-ixp4xx/nslu2*
      [ARM] 3767/1: S3C24XX: remove changelog comments from arch/arm/mach-s3c2410
      [ARM] 3766/1: Fix typo in ARM _raw_read_trylock

commit e89bc81103bb8798daae2c1871229620ed725657
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Sep 6 19:03:14 2006 +0100

    [ARM] 3766/1: Fix typo in ARM _raw_read_trylock
    
    Patch from Catalin Marinas
    
    A comma was missing between tmp and tmp2.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit e694420258cb1af5eb5a06e4b1a027e8c917d027
Merge: 3b6362b833b9 a188ad2bc7db
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Sat Sep 2 14:51:45 2006 -0700

    Merge master.kernel.org:/home/rmk/linux-2.6-arm
    
    * master.kernel.org:/home/rmk/linux-2.6-arm:
      [ARM] 3762/1: Fix ptrace cache coherency bug for ARM1136 VIPT nonaliasing Harvard caches
      [ARM] 3765/1: S3C24XX: cleanup include/asm-arm/arch-s3c2410/dma.h
      [ARM] 3764/1: S3C24XX: change type naming to kernel style
      [ARM] 3763/1: add both rtcs to csb337 defconfig
      [ARM] Fix ARM __raw_read_trylock() implementation
      [ARM] 3750/3: Fix double VFP emulation for EABI kernels

commit 8e34703b9315688305306d26148088b0a8292563
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Aug 31 15:09:30 2006 +0100

    [ARM] Fix ARM __raw_read_trylock() implementation
    
    Matthew Wilcox pointed out that the generic implementation
    of this is unfit for use.  Here's an ARM optimised version
    instead.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 0feae5c47aabdde59cbbec32d150e17102de37f0
Author: NeilBrown <neilb@suse.de>
Date:   Thu Jun 22 14:47:28 2006 -0700

    [PATCH] Fix dcache race during umount
    
    The race is that the shrink_dcache_memory shrinker could get called while a
    filesystem is being unmounted, and could try to prune a dentry belonging to
    that filesystem.
    
    If it does, then it will call in to iput on the inode while the dentry is
    no longer able to be found by the umounting process.  If iput takes a
    while, generic_shutdown_super could get all the way though
    shrink_dcache_parent and shrink_dcache_anon and invalidate_inodes without
    ever waiting on this particular inode.
    
    Eventually the superblock gets freed anyway and if the iput tried to touch
    it (which some filesystems certainly do), it will lose.  The promised
    "Self-destruct in 5 seconds" doesn't lead to a nice day.
    
    The race is closed by holding s_umount while calling prune_one_dentry on
    someone else's dentry.  As a down_read_trylock is used,
    shrink_dcache_memory will no longer try to prune the dentry of a filesystem
    that is being unmounted, and unmount will not be able to start until any
    such active prune_one_dentry completes.
    
    This requires that prune_dcache *knows* which filesystem (if any) it is
    doing the prune on behalf of so that it can be careful of other
    filesystems.  shrink_dcache_memory isn't called it on behalf of any
    filesystem, and so is careful of everything.
    
    shrink_dcache_anon is now passed a super_block rather than the s_anon list
    out of the superblock, so it can get the s_anon list itself, and can pass
    the superblock down to prune_dcache.
    
    If prune_dcache finds a dentry that it cannot free, it leaves it where it
    is (at the tail of the list) and exits, on the assumption that some other
    thread will be removing that dentry soon.  To try to make sure that some
    work gets done, a limited number of dnetries which are untouchable are
    skipped over while choosing the dentry to work on.
    
    I believe this race was first found by Kirill Korotaev.
    
    Cc: Jan Blunck <jblunck@suse.de>
    Acked-by: Kirill Korotaev <dev@openvz.org>
    Cc: Olaf Hering <olh@suse.de>
    Acked-by: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Neil Brown <neilb@suse.de>
    Signed-off-by: Balbir Singh <balbir@in.ibm.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit a9ba9a3b3897561d01e04cd21433746df46548c0
Author: Arjan van de Ven <arjan@intel.linux.com>
Date:   Sat Mar 25 16:30:10 2006 +0100

    [PATCH] x86_64: prefetch the mmap_sem in the fault path
    
    In a micro-benchmark that stresses the pagefault path, the down_read_trylock
    on the mmap_sem showed up quite high on the profile. Turns out this lock is
    bouncing between cpus quite a bit and thus is cache-cold a lot. This patch
    prefetches the lock (for write) as early as possible (and before some other
    somewhat expensive operations). With this patch, the down_read_trylock
    basically fell out of the top of profile.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit d3ed309a718bc6f79dc485a6d18731990daeee5e
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Mon Jan 23 21:03:56 2006 -0800

    [SPARC64]: Implement __raw_read_trylock()
    
    generic__raw_read_trylock() just does a raw_read_lock() so that
    isn't very useful.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 9ad3d6ccf5eee285e233dbaf186369b8d477a666
Author: Alan Stern <stern@rowland.harvard.edu>
Date:   Thu Nov 17 17:10:32 2005 -0500

    [PATCH] USB: Remove USB private semaphore
    
    This patch (as605) removes the private udev->serialize semaphore,
    relying instead on the locking provided by the embedded struct device's
    semaphore.  The changes are confined to the core, except that the
    usb_trylock_device routine now uses the return convention of
    down_trylock rather than down_read_trylock (they return opposite values
    for no good reason).
    
    A couple of other associated changes are included as well:
    
            Now that we aren't concerned about HCDs that avoid using the
            hcd glue layer, usb_disconnect no longer needs to acquire the
            usb_bus_lock -- that can be done by usb_remove_hcd where it
            belongs.
    
            Devices aren't locked over the same scope of code in
            usb_new_device and hub_port_connect_change as they used to be.
            This shouldn't cause any trouble.
    
    Along with the preceding driver core patch, this needs a lot of testing.
    
    Signed-off-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit bf7ececa9b68f4720f1ce344f442435660bcdae7
Author: Keith Owens <kaos@sgi.com>
Date:   Sat Dec 10 14:24:28 2005 +1100

    [IA64] Define an ia64 version of __raw_read_trylock
    
    IA64 is using the generic version of __raw_read_trylock, which always
    waits for the lock to be free instead of returning when the lock is in
    use.  Define an ia64 version of __raw_read_trylock which behaves
    correctly, and drop the generic one.
    
    Signed-off-by: Keith Owens <kaos@sgi.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>
