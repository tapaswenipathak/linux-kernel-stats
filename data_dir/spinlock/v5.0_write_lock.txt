commit b9fb10d131b8c84af9bb14e2078d5c63600c7dea
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Jan 17 11:01:31 2023 +0000

    l2tp: prevent lockdep issue in l2tp_tunnel_register()
    
    lockdep complains with the following lock/unlock sequence:
    
         lock_sock(sk);
         write_lock_bh(&sk->sk_callback_lock);
    [1]  release_sock(sk);
    [2]  write_unlock_bh(&sk->sk_callback_lock);
    
    We need to swap [1] and [2] to fix this issue.
    
    Fixes: 0b2c59720e65 ("l2tp: close all race conditions in l2tp_tunnel_register()")
    Reported-by: syzbot+bbd35b345c7cab0d9a08@syzkaller.appspotmail.com
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Link: https://lore.kernel.org/netdev/20230114030137.672706-1-xiyou.wangcong@gmail.com/T/#m1164ff20628671b0f326a24cb106ab3239c70ce3
    Cc: Cong Wang <cong.wang@bytedance.com>
    Cc: Guillaume Nault <gnault@redhat.com>
    Reviewed-by: Guillaume Nault <gnault@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 68a37dc77e2fb53530749ac982bd11f3fa2c9028
Author: Xin Long <lucien.xin@gmail.com>
Date:   Sat Dec 3 18:37:21 2022 -0500

    tipc: call tipc_lxc_xmit without holding node_read_lock
    
    commit 88956177db179e4eba7cd590971961857d1565b8 upstream.
    
    When sending packets between nodes in netns, it calls tipc_lxc_xmit() for
    peer node to receive the packets where tipc_sk_mcast_rcv()/tipc_sk_rcv()
    might be called, and it's pretty much like in tipc_rcv().
    
    Currently the local 'node rw lock' is held during calling tipc_lxc_xmit()
    to protect the peer_net not being freed by another thread. However, when
    receiving these packets, tipc_node_add_conn() might be called where the
    peer 'node rw lock' is acquired. Then a dead lock warning is triggered by
    lockdep detector, although it is not a real dead lock:
    
        WARNING: possible recursive locking detected
        --------------------------------------------
        conn_server/1086 is trying to acquire lock:
        ffff8880065cb020 (&n->lock#2){++--}-{2:2}, \
                         at: tipc_node_add_conn.cold.76+0xaa/0x211 [tipc]
    
        but task is already holding lock:
        ffff8880065cd020 (&n->lock#2){++--}-{2:2}, \
                         at: tipc_node_xmit+0x285/0xb30 [tipc]
    
        other info that might help us debug this:
         Possible unsafe locking scenario:
    
               CPU0
               ----
          lock(&n->lock#2);
          lock(&n->lock#2);
    
         *** DEADLOCK ***
    
         May be due to missing lock nesting notation
    
        4 locks held by conn_server/1086:
         #0: ffff8880036d1e40 (sk_lock-AF_TIPC){+.+.}-{0:0}, \
                              at: tipc_accept+0x9c0/0x10b0 [tipc]
         #1: ffff8880036d5f80 (sk_lock-AF_TIPC/1){+.+.}-{0:0}, \
                              at: tipc_accept+0x363/0x10b0 [tipc]
         #2: ffff8880065cd020 (&n->lock#2){++--}-{2:2}, \
                              at: tipc_node_xmit+0x285/0xb30 [tipc]
         #3: ffff888012e13370 (slock-AF_TIPC){+...}-{2:2}, \
                              at: tipc_sk_rcv+0x2da/0x1b40 [tipc]
    
        Call Trace:
         <TASK>
         dump_stack_lvl+0x44/0x5b
         __lock_acquire.cold.77+0x1f2/0x3d7
         lock_acquire+0x1d2/0x610
         _raw_write_lock_bh+0x38/0x80
         tipc_node_add_conn.cold.76+0xaa/0x211 [tipc]
         tipc_sk_finish_conn+0x21e/0x640 [tipc]
         tipc_sk_filter_rcv+0x147b/0x3030 [tipc]
         tipc_sk_rcv+0xbb4/0x1b40 [tipc]
         tipc_lxc_xmit+0x225/0x26b [tipc]
         tipc_node_xmit.cold.82+0x4a/0x102 [tipc]
         __tipc_sendstream+0x879/0xff0 [tipc]
         tipc_accept+0x966/0x10b0 [tipc]
         do_accept+0x37d/0x590
    
    This patch avoids this warning by not holding the 'node rw lock' before
    calling tipc_lxc_xmit(). As to protect the 'peer_net', rcu_read_lock()
    should be enough, as in cleanup_net() when freeing the netns, it calls
    synchronize_rcu() before the free is continued.
    
    Also since tipc_lxc_xmit() is like the RX path in tipc_rcv(), it makes
    sense to call it under rcu_read_lock(). Note that the right lock order
    must be:
    
       rcu_read_lock();
       tipc_node_read_lock(n);
       tipc_node_read_unlock(n);
       tipc_lxc_xmit();
       rcu_read_unlock();
    
    instead of:
    
       tipc_node_read_lock(n);
       rcu_read_lock();
       tipc_node_read_unlock(n);
       tipc_lxc_xmit();
       rcu_read_unlock();
    
    and we have to call tipc_node_read_lock/unlock() twice in
    tipc_node_xmit().
    
    Fixes: f73b12812a3d ("tipc: improve throughput between nodes in netns")
    Reported-by: Shuang Li <shuali@redhat.com>
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Link: https://lore.kernel.org/r/5bdd1f8fee9db695cfff4528a48c9b9d0523fb00.1670110641.git.lucien.xin@gmail.com
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9b83ec63d0de7b1f379daa1571e128bc7b9570f8
Author: Frederick Lawler <fred@cloudflare.com>
Date:   Mon Jan 9 10:39:06 2023 -0600

    net: sched: disallow noqueue for qdisc classes
    
    commit 96398560f26aa07e8f2969d73c8197e6a6d10407 upstream.
    
    While experimenting with applying noqueue to a classful queue discipline,
    we discovered a NULL pointer dereference in the __dev_queue_xmit()
    path that generates a kernel OOPS:
    
        # dev=enp0s5
        # tc qdisc replace dev $dev root handle 1: htb default 1
        # tc class add dev $dev parent 1: classid 1:1 htb rate 10mbit
        # tc qdisc add dev $dev parent 1:1 handle 10: noqueue
        # ping -I $dev -w 1 -c 1 1.1.1.1
    
    [    2.172856] BUG: kernel NULL pointer dereference, address: 0000000000000000
    [    2.173217] #PF: supervisor instruction fetch in kernel mode
    ...
    [    2.178451] Call Trace:
    [    2.178577]  <TASK>
    [    2.178686]  htb_enqueue+0x1c8/0x370
    [    2.178880]  dev_qdisc_enqueue+0x15/0x90
    [    2.179093]  __dev_queue_xmit+0x798/0xd00
    [    2.179305]  ? _raw_write_lock_bh+0xe/0x30
    [    2.179522]  ? __local_bh_enable_ip+0x32/0x70
    [    2.179759]  ? ___neigh_create+0x610/0x840
    [    2.179968]  ? eth_header+0x21/0xc0
    [    2.180144]  ip_finish_output2+0x15e/0x4f0
    [    2.180348]  ? dst_output+0x30/0x30
    [    2.180525]  ip_push_pending_frames+0x9d/0xb0
    [    2.180739]  raw_sendmsg+0x601/0xcb0
    [    2.180916]  ? _raw_spin_trylock+0xe/0x50
    [    2.181112]  ? _raw_spin_unlock_irqrestore+0x16/0x30
    [    2.181354]  ? get_page_from_freelist+0xcd6/0xdf0
    [    2.181594]  ? sock_sendmsg+0x56/0x60
    [    2.181781]  sock_sendmsg+0x56/0x60
    [    2.181958]  __sys_sendto+0xf7/0x160
    [    2.182139]  ? handle_mm_fault+0x6e/0x1d0
    [    2.182366]  ? do_user_addr_fault+0x1e1/0x660
    [    2.182627]  __x64_sys_sendto+0x1b/0x30
    [    2.182881]  do_syscall_64+0x38/0x90
    [    2.183085]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
    ...
    [    2.187402]  </TASK>
    
    Previously in commit d66d6c3152e8 ("net: sched: register noqueue
    qdisc"), NULL was set for the noqueue discipline on noqueue init
    so that __dev_queue_xmit() falls through for the noqueue case. This
    also sets a bypass of the enqueue NULL check in the
    register_qdisc() function for the struct noqueue_disc_ops.
    
    Classful queue disciplines make it past the NULL check in
    __dev_queue_xmit() because the discipline is set to htb (in this case),
    and then in the call to __dev_xmit_skb(), it calls into htb_enqueue()
    which grabs a leaf node for a class and then calls qdisc_enqueue() by
    passing in a queue discipline which assumes ->enqueue() is not set to NULL.
    
    Fix this by not allowing classes to be assigned to the noqueue
    discipline. Linux TC Notes states that classes cannot be set to
    the noqueue discipline. [1] Let's enforce that here.
    
    Links:
    1. https://linux-tc-notes.sourceforge.net/tc/doc/sch_noqueue.txt
    
    Fixes: d66d6c3152e8 ("net: sched: register noqueue qdisc")
    Cc: stable@vger.kernel.org
    Signed-off-by: Frederick Lawler <fred@cloudflare.com>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Link: https://lore.kernel.org/r/20230109163906.706000-1-fred@cloudflare.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 200aa33b5d781e7c0fa6c0c7db9dbcc3f574ce8f
Author: Zhihao Cheng <chengzhihao1@huawei.com>
Date:   Wed Nov 30 21:31:34 2022 +0800

    dm thin: Fix ABBA deadlock between shrink_slab and dm_pool_abort_metadata
    
    commit 8111964f1b8524c4bb56b02cd9c7a37725ea21fd upstream.
    
    Following concurrent processes:
    
              P1(drop cache)                P2(kworker)
    drop_caches_sysctl_handler
     drop_slab
      shrink_slab
       down_read(&shrinker_rwsem)  - LOCK A
       do_shrink_slab
        super_cache_scan
         prune_icache_sb
          dispose_list
           evict
            ext4_evict_inode
             ext4_clear_inode
              ext4_discard_preallocations
               ext4_mb_load_buddy_gfp
                ext4_mb_init_cache
                 ext4_read_block_bitmap_nowait
                  ext4_read_bh_nowait
                   submit_bh
                    dm_submit_bio
                                     do_worker
                                      process_deferred_bios
                                       commit
                                        metadata_operation_failed
                                         dm_pool_abort_metadata
                                          down_write(&pmd->root_lock) - LOCK B
                                          __destroy_persistent_data_objects
                                           dm_block_manager_destroy
                                            dm_bufio_client_destroy
                                             unregister_shrinker
                                              down_write(&shrinker_rwsem)
                     thin_map                            |
                      dm_thin_find_block                 ↓
                       down_read(&pmd->root_lock) --> ABBA deadlock
    
    , which triggers hung task:
    
    [   76.974820] INFO: task kworker/u4:3:63 blocked for more than 15 seconds.
    [   76.976019]       Not tainted 6.1.0-rc4-00011-g8f17dd350364-dirty #910
    [   76.978521] task:kworker/u4:3    state:D stack:0     pid:63    ppid:2
    [   76.978534] Workqueue: dm-thin do_worker
    [   76.978552] Call Trace:
    [   76.978564]  __schedule+0x6ba/0x10f0
    [   76.978582]  schedule+0x9d/0x1e0
    [   76.978588]  rwsem_down_write_slowpath+0x587/0xdf0
    [   76.978600]  down_write+0xec/0x110
    [   76.978607]  unregister_shrinker+0x2c/0xf0
    [   76.978616]  dm_bufio_client_destroy+0x116/0x3d0
    [   76.978625]  dm_block_manager_destroy+0x19/0x40
    [   76.978629]  __destroy_persistent_data_objects+0x5e/0x70
    [   76.978636]  dm_pool_abort_metadata+0x8e/0x100
    [   76.978643]  metadata_operation_failed+0x86/0x110
    [   76.978649]  commit+0x6a/0x230
    [   76.978655]  do_worker+0xc6e/0xd90
    [   76.978702]  process_one_work+0x269/0x630
    [   76.978714]  worker_thread+0x266/0x630
    [   76.978730]  kthread+0x151/0x1b0
    [   76.978772] INFO: task test.sh:2646 blocked for more than 15 seconds.
    [   76.979756]       Not tainted 6.1.0-rc4-00011-g8f17dd350364-dirty #910
    [   76.982111] task:test.sh         state:D stack:0     pid:2646  ppid:2459
    [   76.982128] Call Trace:
    [   76.982139]  __schedule+0x6ba/0x10f0
    [   76.982155]  schedule+0x9d/0x1e0
    [   76.982159]  rwsem_down_read_slowpath+0x4f4/0x910
    [   76.982173]  down_read+0x84/0x170
    [   76.982177]  dm_thin_find_block+0x4c/0xd0
    [   76.982183]  thin_map+0x201/0x3d0
    [   76.982188]  __map_bio+0x5b/0x350
    [   76.982195]  dm_submit_bio+0x2b6/0x930
    [   76.982202]  __submit_bio+0x123/0x2d0
    [   76.982209]  submit_bio_noacct_nocheck+0x101/0x3e0
    [   76.982222]  submit_bio_noacct+0x389/0x770
    [   76.982227]  submit_bio+0x50/0xc0
    [   76.982232]  submit_bh_wbc+0x15e/0x230
    [   76.982238]  submit_bh+0x14/0x20
    [   76.982241]  ext4_read_bh_nowait+0xc5/0x130
    [   76.982247]  ext4_read_block_bitmap_nowait+0x340/0xc60
    [   76.982254]  ext4_mb_init_cache+0x1ce/0xdc0
    [   76.982259]  ext4_mb_load_buddy_gfp+0x987/0xfa0
    [   76.982263]  ext4_discard_preallocations+0x45d/0x830
    [   76.982274]  ext4_clear_inode+0x48/0xf0
    [   76.982280]  ext4_evict_inode+0xcf/0xc70
    [   76.982285]  evict+0x119/0x2b0
    [   76.982290]  dispose_list+0x43/0xa0
    [   76.982294]  prune_icache_sb+0x64/0x90
    [   76.982298]  super_cache_scan+0x155/0x210
    [   76.982303]  do_shrink_slab+0x19e/0x4e0
    [   76.982310]  shrink_slab+0x2bd/0x450
    [   76.982317]  drop_slab+0xcc/0x1a0
    [   76.982323]  drop_caches_sysctl_handler+0xb7/0xe0
    [   76.982327]  proc_sys_call_handler+0x1bc/0x300
    [   76.982331]  proc_sys_write+0x17/0x20
    [   76.982334]  vfs_write+0x3d3/0x570
    [   76.982342]  ksys_write+0x73/0x160
    [   76.982347]  __x64_sys_write+0x1e/0x30
    [   76.982352]  do_syscall_64+0x35/0x80
    [   76.982357]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
    
    Function metadata_operation_failed() is called when operations failed
    on dm pool metadata, dm pool will destroy and recreate metadata. So,
    shrinker will be unregistered and registered, which could down write
    shrinker_rwsem under pmd_write_lock.
    
    Fix it by allocating dm_block_manager before locking pmd->root_lock
    and destroying old dm_block_manager after unlocking pmd->root_lock,
    then old dm_block_manager is replaced with new dm_block_manager under
    pmd->root_lock. So, shrinker register/unregister could be done without
    holding pmd->root_lock.
    
    Fetch a reproducer in [Link].
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=216676
    Cc: stable@vger.kernel.org #v5.2+
    Fixes: e49e582965b3 ("dm thin: add read only and fail io modes")
    Signed-off-by: Zhihao Cheng <chengzhihao1@huawei.com>
    Signed-off-by: Mike Snitzer <snitzer@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0195d5ad539382a83e1bfaab51b93b8685f0b7c7
Author: Frederick Lawler <fred@cloudflare.com>
Date:   Mon Jan 9 10:39:06 2023 -0600

    net: sched: disallow noqueue for qdisc classes
    
    commit 96398560f26aa07e8f2969d73c8197e6a6d10407 upstream.
    
    While experimenting with applying noqueue to a classful queue discipline,
    we discovered a NULL pointer dereference in the __dev_queue_xmit()
    path that generates a kernel OOPS:
    
        # dev=enp0s5
        # tc qdisc replace dev $dev root handle 1: htb default 1
        # tc class add dev $dev parent 1: classid 1:1 htb rate 10mbit
        # tc qdisc add dev $dev parent 1:1 handle 10: noqueue
        # ping -I $dev -w 1 -c 1 1.1.1.1
    
    [    2.172856] BUG: kernel NULL pointer dereference, address: 0000000000000000
    [    2.173217] #PF: supervisor instruction fetch in kernel mode
    ...
    [    2.178451] Call Trace:
    [    2.178577]  <TASK>
    [    2.178686]  htb_enqueue+0x1c8/0x370
    [    2.178880]  dev_qdisc_enqueue+0x15/0x90
    [    2.179093]  __dev_queue_xmit+0x798/0xd00
    [    2.179305]  ? _raw_write_lock_bh+0xe/0x30
    [    2.179522]  ? __local_bh_enable_ip+0x32/0x70
    [    2.179759]  ? ___neigh_create+0x610/0x840
    [    2.179968]  ? eth_header+0x21/0xc0
    [    2.180144]  ip_finish_output2+0x15e/0x4f0
    [    2.180348]  ? dst_output+0x30/0x30
    [    2.180525]  ip_push_pending_frames+0x9d/0xb0
    [    2.180739]  raw_sendmsg+0x601/0xcb0
    [    2.180916]  ? _raw_spin_trylock+0xe/0x50
    [    2.181112]  ? _raw_spin_unlock_irqrestore+0x16/0x30
    [    2.181354]  ? get_page_from_freelist+0xcd6/0xdf0
    [    2.181594]  ? sock_sendmsg+0x56/0x60
    [    2.181781]  sock_sendmsg+0x56/0x60
    [    2.181958]  __sys_sendto+0xf7/0x160
    [    2.182139]  ? handle_mm_fault+0x6e/0x1d0
    [    2.182366]  ? do_user_addr_fault+0x1e1/0x660
    [    2.182627]  __x64_sys_sendto+0x1b/0x30
    [    2.182881]  do_syscall_64+0x38/0x90
    [    2.183085]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
    ...
    [    2.187402]  </TASK>
    
    Previously in commit d66d6c3152e8 ("net: sched: register noqueue
    qdisc"), NULL was set for the noqueue discipline on noqueue init
    so that __dev_queue_xmit() falls through for the noqueue case. This
    also sets a bypass of the enqueue NULL check in the
    register_qdisc() function for the struct noqueue_disc_ops.
    
    Classful queue disciplines make it past the NULL check in
    __dev_queue_xmit() because the discipline is set to htb (in this case),
    and then in the call to __dev_xmit_skb(), it calls into htb_enqueue()
    which grabs a leaf node for a class and then calls qdisc_enqueue() by
    passing in a queue discipline which assumes ->enqueue() is not set to NULL.
    
    Fix this by not allowing classes to be assigned to the noqueue
    discipline. Linux TC Notes states that classes cannot be set to
    the noqueue discipline. [1] Let's enforce that here.
    
    Links:
    1. https://linux-tc-notes.sourceforge.net/tc/doc/sch_noqueue.txt
    
    Fixes: d66d6c3152e8 ("net: sched: register noqueue qdisc")
    Cc: stable@vger.kernel.org
    Signed-off-by: Frederick Lawler <fred@cloudflare.com>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Link: https://lore.kernel.org/r/20230109163906.706000-1-fred@cloudflare.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4574e32cbf652d7efcaa6076558752f770b01757
Author: Frederick Lawler <fred@cloudflare.com>
Date:   Mon Jan 9 10:39:06 2023 -0600

    net: sched: disallow noqueue for qdisc classes
    
    commit 96398560f26aa07e8f2969d73c8197e6a6d10407 upstream.
    
    While experimenting with applying noqueue to a classful queue discipline,
    we discovered a NULL pointer dereference in the __dev_queue_xmit()
    path that generates a kernel OOPS:
    
        # dev=enp0s5
        # tc qdisc replace dev $dev root handle 1: htb default 1
        # tc class add dev $dev parent 1: classid 1:1 htb rate 10mbit
        # tc qdisc add dev $dev parent 1:1 handle 10: noqueue
        # ping -I $dev -w 1 -c 1 1.1.1.1
    
    [    2.172856] BUG: kernel NULL pointer dereference, address: 0000000000000000
    [    2.173217] #PF: supervisor instruction fetch in kernel mode
    ...
    [    2.178451] Call Trace:
    [    2.178577]  <TASK>
    [    2.178686]  htb_enqueue+0x1c8/0x370
    [    2.178880]  dev_qdisc_enqueue+0x15/0x90
    [    2.179093]  __dev_queue_xmit+0x798/0xd00
    [    2.179305]  ? _raw_write_lock_bh+0xe/0x30
    [    2.179522]  ? __local_bh_enable_ip+0x32/0x70
    [    2.179759]  ? ___neigh_create+0x610/0x840
    [    2.179968]  ? eth_header+0x21/0xc0
    [    2.180144]  ip_finish_output2+0x15e/0x4f0
    [    2.180348]  ? dst_output+0x30/0x30
    [    2.180525]  ip_push_pending_frames+0x9d/0xb0
    [    2.180739]  raw_sendmsg+0x601/0xcb0
    [    2.180916]  ? _raw_spin_trylock+0xe/0x50
    [    2.181112]  ? _raw_spin_unlock_irqrestore+0x16/0x30
    [    2.181354]  ? get_page_from_freelist+0xcd6/0xdf0
    [    2.181594]  ? sock_sendmsg+0x56/0x60
    [    2.181781]  sock_sendmsg+0x56/0x60
    [    2.181958]  __sys_sendto+0xf7/0x160
    [    2.182139]  ? handle_mm_fault+0x6e/0x1d0
    [    2.182366]  ? do_user_addr_fault+0x1e1/0x660
    [    2.182627]  __x64_sys_sendto+0x1b/0x30
    [    2.182881]  do_syscall_64+0x38/0x90
    [    2.183085]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
    ...
    [    2.187402]  </TASK>
    
    Previously in commit d66d6c3152e8 ("net: sched: register noqueue
    qdisc"), NULL was set for the noqueue discipline on noqueue init
    so that __dev_queue_xmit() falls through for the noqueue case. This
    also sets a bypass of the enqueue NULL check in the
    register_qdisc() function for the struct noqueue_disc_ops.
    
    Classful queue disciplines make it past the NULL check in
    __dev_queue_xmit() because the discipline is set to htb (in this case),
    and then in the call to __dev_xmit_skb(), it calls into htb_enqueue()
    which grabs a leaf node for a class and then calls qdisc_enqueue() by
    passing in a queue discipline which assumes ->enqueue() is not set to NULL.
    
    Fix this by not allowing classes to be assigned to the noqueue
    discipline. Linux TC Notes states that classes cannot be set to
    the noqueue discipline. [1] Let's enforce that here.
    
    Links:
    1. https://linux-tc-notes.sourceforge.net/tc/doc/sch_noqueue.txt
    
    Fixes: d66d6c3152e8 ("net: sched: register noqueue qdisc")
    Cc: stable@vger.kernel.org
    Signed-off-by: Frederick Lawler <fred@cloudflare.com>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Link: https://lore.kernel.org/r/20230109163906.706000-1-fred@cloudflare.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e8988e878af693ac13b0fa80ba2e72d22d68f2dd
Author: Frederick Lawler <fred@cloudflare.com>
Date:   Mon Jan 9 10:39:06 2023 -0600

    net: sched: disallow noqueue for qdisc classes
    
    commit 96398560f26aa07e8f2969d73c8197e6a6d10407 upstream.
    
    While experimenting with applying noqueue to a classful queue discipline,
    we discovered a NULL pointer dereference in the __dev_queue_xmit()
    path that generates a kernel OOPS:
    
        # dev=enp0s5
        # tc qdisc replace dev $dev root handle 1: htb default 1
        # tc class add dev $dev parent 1: classid 1:1 htb rate 10mbit
        # tc qdisc add dev $dev parent 1:1 handle 10: noqueue
        # ping -I $dev -w 1 -c 1 1.1.1.1
    
    [    2.172856] BUG: kernel NULL pointer dereference, address: 0000000000000000
    [    2.173217] #PF: supervisor instruction fetch in kernel mode
    ...
    [    2.178451] Call Trace:
    [    2.178577]  <TASK>
    [    2.178686]  htb_enqueue+0x1c8/0x370
    [    2.178880]  dev_qdisc_enqueue+0x15/0x90
    [    2.179093]  __dev_queue_xmit+0x798/0xd00
    [    2.179305]  ? _raw_write_lock_bh+0xe/0x30
    [    2.179522]  ? __local_bh_enable_ip+0x32/0x70
    [    2.179759]  ? ___neigh_create+0x610/0x840
    [    2.179968]  ? eth_header+0x21/0xc0
    [    2.180144]  ip_finish_output2+0x15e/0x4f0
    [    2.180348]  ? dst_output+0x30/0x30
    [    2.180525]  ip_push_pending_frames+0x9d/0xb0
    [    2.180739]  raw_sendmsg+0x601/0xcb0
    [    2.180916]  ? _raw_spin_trylock+0xe/0x50
    [    2.181112]  ? _raw_spin_unlock_irqrestore+0x16/0x30
    [    2.181354]  ? get_page_from_freelist+0xcd6/0xdf0
    [    2.181594]  ? sock_sendmsg+0x56/0x60
    [    2.181781]  sock_sendmsg+0x56/0x60
    [    2.181958]  __sys_sendto+0xf7/0x160
    [    2.182139]  ? handle_mm_fault+0x6e/0x1d0
    [    2.182366]  ? do_user_addr_fault+0x1e1/0x660
    [    2.182627]  __x64_sys_sendto+0x1b/0x30
    [    2.182881]  do_syscall_64+0x38/0x90
    [    2.183085]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
    ...
    [    2.187402]  </TASK>
    
    Previously in commit d66d6c3152e8 ("net: sched: register noqueue
    qdisc"), NULL was set for the noqueue discipline on noqueue init
    so that __dev_queue_xmit() falls through for the noqueue case. This
    also sets a bypass of the enqueue NULL check in the
    register_qdisc() function for the struct noqueue_disc_ops.
    
    Classful queue disciplines make it past the NULL check in
    __dev_queue_xmit() because the discipline is set to htb (in this case),
    and then in the call to __dev_xmit_skb(), it calls into htb_enqueue()
    which grabs a leaf node for a class and then calls qdisc_enqueue() by
    passing in a queue discipline which assumes ->enqueue() is not set to NULL.
    
    Fix this by not allowing classes to be assigned to the noqueue
    discipline. Linux TC Notes states that classes cannot be set to
    the noqueue discipline. [1] Let's enforce that here.
    
    Links:
    1. https://linux-tc-notes.sourceforge.net/tc/doc/sch_noqueue.txt
    
    Fixes: d66d6c3152e8 ("net: sched: register noqueue qdisc")
    Cc: stable@vger.kernel.org
    Signed-off-by: Frederick Lawler <fred@cloudflare.com>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Link: https://lore.kernel.org/r/20230109163906.706000-1-fred@cloudflare.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 04941c1d5bb59d64165e09813de2947bdf6f4f28
Author: Frederick Lawler <fred@cloudflare.com>
Date:   Mon Jan 9 10:39:06 2023 -0600

    net: sched: disallow noqueue for qdisc classes
    
    commit 96398560f26aa07e8f2969d73c8197e6a6d10407 upstream.
    
    While experimenting with applying noqueue to a classful queue discipline,
    we discovered a NULL pointer dereference in the __dev_queue_xmit()
    path that generates a kernel OOPS:
    
        # dev=enp0s5
        # tc qdisc replace dev $dev root handle 1: htb default 1
        # tc class add dev $dev parent 1: classid 1:1 htb rate 10mbit
        # tc qdisc add dev $dev parent 1:1 handle 10: noqueue
        # ping -I $dev -w 1 -c 1 1.1.1.1
    
    [    2.172856] BUG: kernel NULL pointer dereference, address: 0000000000000000
    [    2.173217] #PF: supervisor instruction fetch in kernel mode
    ...
    [    2.178451] Call Trace:
    [    2.178577]  <TASK>
    [    2.178686]  htb_enqueue+0x1c8/0x370
    [    2.178880]  dev_qdisc_enqueue+0x15/0x90
    [    2.179093]  __dev_queue_xmit+0x798/0xd00
    [    2.179305]  ? _raw_write_lock_bh+0xe/0x30
    [    2.179522]  ? __local_bh_enable_ip+0x32/0x70
    [    2.179759]  ? ___neigh_create+0x610/0x840
    [    2.179968]  ? eth_header+0x21/0xc0
    [    2.180144]  ip_finish_output2+0x15e/0x4f0
    [    2.180348]  ? dst_output+0x30/0x30
    [    2.180525]  ip_push_pending_frames+0x9d/0xb0
    [    2.180739]  raw_sendmsg+0x601/0xcb0
    [    2.180916]  ? _raw_spin_trylock+0xe/0x50
    [    2.181112]  ? _raw_spin_unlock_irqrestore+0x16/0x30
    [    2.181354]  ? get_page_from_freelist+0xcd6/0xdf0
    [    2.181594]  ? sock_sendmsg+0x56/0x60
    [    2.181781]  sock_sendmsg+0x56/0x60
    [    2.181958]  __sys_sendto+0xf7/0x160
    [    2.182139]  ? handle_mm_fault+0x6e/0x1d0
    [    2.182366]  ? do_user_addr_fault+0x1e1/0x660
    [    2.182627]  __x64_sys_sendto+0x1b/0x30
    [    2.182881]  do_syscall_64+0x38/0x90
    [    2.183085]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
    ...
    [    2.187402]  </TASK>
    
    Previously in commit d66d6c3152e8 ("net: sched: register noqueue
    qdisc"), NULL was set for the noqueue discipline on noqueue init
    so that __dev_queue_xmit() falls through for the noqueue case. This
    also sets a bypass of the enqueue NULL check in the
    register_qdisc() function for the struct noqueue_disc_ops.
    
    Classful queue disciplines make it past the NULL check in
    __dev_queue_xmit() because the discipline is set to htb (in this case),
    and then in the call to __dev_xmit_skb(), it calls into htb_enqueue()
    which grabs a leaf node for a class and then calls qdisc_enqueue() by
    passing in a queue discipline which assumes ->enqueue() is not set to NULL.
    
    Fix this by not allowing classes to be assigned to the noqueue
    discipline. Linux TC Notes states that classes cannot be set to
    the noqueue discipline. [1] Let's enforce that here.
    
    Links:
    1. https://linux-tc-notes.sourceforge.net/tc/doc/sch_noqueue.txt
    
    Fixes: d66d6c3152e8 ("net: sched: register noqueue qdisc")
    Cc: stable@vger.kernel.org
    Signed-off-by: Frederick Lawler <fred@cloudflare.com>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Link: https://lore.kernel.org/r/20230109163906.706000-1-fred@cloudflare.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9f7bc28a6b8afc2274e25650511555e93f45470f
Author: Frederick Lawler <fred@cloudflare.com>
Date:   Mon Jan 9 10:39:06 2023 -0600

    net: sched: disallow noqueue for qdisc classes
    
    commit 96398560f26aa07e8f2969d73c8197e6a6d10407 upstream.
    
    While experimenting with applying noqueue to a classful queue discipline,
    we discovered a NULL pointer dereference in the __dev_queue_xmit()
    path that generates a kernel OOPS:
    
        # dev=enp0s5
        # tc qdisc replace dev $dev root handle 1: htb default 1
        # tc class add dev $dev parent 1: classid 1:1 htb rate 10mbit
        # tc qdisc add dev $dev parent 1:1 handle 10: noqueue
        # ping -I $dev -w 1 -c 1 1.1.1.1
    
    [    2.172856] BUG: kernel NULL pointer dereference, address: 0000000000000000
    [    2.173217] #PF: supervisor instruction fetch in kernel mode
    ...
    [    2.178451] Call Trace:
    [    2.178577]  <TASK>
    [    2.178686]  htb_enqueue+0x1c8/0x370
    [    2.178880]  dev_qdisc_enqueue+0x15/0x90
    [    2.179093]  __dev_queue_xmit+0x798/0xd00
    [    2.179305]  ? _raw_write_lock_bh+0xe/0x30
    [    2.179522]  ? __local_bh_enable_ip+0x32/0x70
    [    2.179759]  ? ___neigh_create+0x610/0x840
    [    2.179968]  ? eth_header+0x21/0xc0
    [    2.180144]  ip_finish_output2+0x15e/0x4f0
    [    2.180348]  ? dst_output+0x30/0x30
    [    2.180525]  ip_push_pending_frames+0x9d/0xb0
    [    2.180739]  raw_sendmsg+0x601/0xcb0
    [    2.180916]  ? _raw_spin_trylock+0xe/0x50
    [    2.181112]  ? _raw_spin_unlock_irqrestore+0x16/0x30
    [    2.181354]  ? get_page_from_freelist+0xcd6/0xdf0
    [    2.181594]  ? sock_sendmsg+0x56/0x60
    [    2.181781]  sock_sendmsg+0x56/0x60
    [    2.181958]  __sys_sendto+0xf7/0x160
    [    2.182139]  ? handle_mm_fault+0x6e/0x1d0
    [    2.182366]  ? do_user_addr_fault+0x1e1/0x660
    [    2.182627]  __x64_sys_sendto+0x1b/0x30
    [    2.182881]  do_syscall_64+0x38/0x90
    [    2.183085]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
    ...
    [    2.187402]  </TASK>
    
    Previously in commit d66d6c3152e8 ("net: sched: register noqueue
    qdisc"), NULL was set for the noqueue discipline on noqueue init
    so that __dev_queue_xmit() falls through for the noqueue case. This
    also sets a bypass of the enqueue NULL check in the
    register_qdisc() function for the struct noqueue_disc_ops.
    
    Classful queue disciplines make it past the NULL check in
    __dev_queue_xmit() because the discipline is set to htb (in this case),
    and then in the call to __dev_xmit_skb(), it calls into htb_enqueue()
    which grabs a leaf node for a class and then calls qdisc_enqueue() by
    passing in a queue discipline which assumes ->enqueue() is not set to NULL.
    
    Fix this by not allowing classes to be assigned to the noqueue
    discipline. Linux TC Notes states that classes cannot be set to
    the noqueue discipline. [1] Let's enforce that here.
    
    Links:
    1. https://linux-tc-notes.sourceforge.net/tc/doc/sch_noqueue.txt
    
    Fixes: d66d6c3152e8 ("net: sched: register noqueue qdisc")
    Cc: stable@vger.kernel.org
    Signed-off-by: Frederick Lawler <fred@cloudflare.com>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Link: https://lore.kernel.org/r/20230109163906.706000-1-fred@cloudflare.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7e37578069737b04955c71dd85db8a3bc2709eff
Author: Zhihao Cheng <chengzhihao1@huawei.com>
Date:   Wed Nov 30 21:31:34 2022 +0800

    dm thin: Fix ABBA deadlock between shrink_slab and dm_pool_abort_metadata
    
    commit 8111964f1b8524c4bb56b02cd9c7a37725ea21fd upstream.
    
    Following concurrent processes:
    
              P1(drop cache)                P2(kworker)
    drop_caches_sysctl_handler
     drop_slab
      shrink_slab
       down_read(&shrinker_rwsem)  - LOCK A
       do_shrink_slab
        super_cache_scan
         prune_icache_sb
          dispose_list
           evict
            ext4_evict_inode
             ext4_clear_inode
              ext4_discard_preallocations
               ext4_mb_load_buddy_gfp
                ext4_mb_init_cache
                 ext4_read_block_bitmap_nowait
                  ext4_read_bh_nowait
                   submit_bh
                    dm_submit_bio
                                     do_worker
                                      process_deferred_bios
                                       commit
                                        metadata_operation_failed
                                         dm_pool_abort_metadata
                                          down_write(&pmd->root_lock) - LOCK B
                                          __destroy_persistent_data_objects
                                           dm_block_manager_destroy
                                            dm_bufio_client_destroy
                                             unregister_shrinker
                                              down_write(&shrinker_rwsem)
                     thin_map                            |
                      dm_thin_find_block                 ↓
                       down_read(&pmd->root_lock) --> ABBA deadlock
    
    , which triggers hung task:
    
    [   76.974820] INFO: task kworker/u4:3:63 blocked for more than 15 seconds.
    [   76.976019]       Not tainted 6.1.0-rc4-00011-g8f17dd350364-dirty #910
    [   76.978521] task:kworker/u4:3    state:D stack:0     pid:63    ppid:2
    [   76.978534] Workqueue: dm-thin do_worker
    [   76.978552] Call Trace:
    [   76.978564]  __schedule+0x6ba/0x10f0
    [   76.978582]  schedule+0x9d/0x1e0
    [   76.978588]  rwsem_down_write_slowpath+0x587/0xdf0
    [   76.978600]  down_write+0xec/0x110
    [   76.978607]  unregister_shrinker+0x2c/0xf0
    [   76.978616]  dm_bufio_client_destroy+0x116/0x3d0
    [   76.978625]  dm_block_manager_destroy+0x19/0x40
    [   76.978629]  __destroy_persistent_data_objects+0x5e/0x70
    [   76.978636]  dm_pool_abort_metadata+0x8e/0x100
    [   76.978643]  metadata_operation_failed+0x86/0x110
    [   76.978649]  commit+0x6a/0x230
    [   76.978655]  do_worker+0xc6e/0xd90
    [   76.978702]  process_one_work+0x269/0x630
    [   76.978714]  worker_thread+0x266/0x630
    [   76.978730]  kthread+0x151/0x1b0
    [   76.978772] INFO: task test.sh:2646 blocked for more than 15 seconds.
    [   76.979756]       Not tainted 6.1.0-rc4-00011-g8f17dd350364-dirty #910
    [   76.982111] task:test.sh         state:D stack:0     pid:2646  ppid:2459
    [   76.982128] Call Trace:
    [   76.982139]  __schedule+0x6ba/0x10f0
    [   76.982155]  schedule+0x9d/0x1e0
    [   76.982159]  rwsem_down_read_slowpath+0x4f4/0x910
    [   76.982173]  down_read+0x84/0x170
    [   76.982177]  dm_thin_find_block+0x4c/0xd0
    [   76.982183]  thin_map+0x201/0x3d0
    [   76.982188]  __map_bio+0x5b/0x350
    [   76.982195]  dm_submit_bio+0x2b6/0x930
    [   76.982202]  __submit_bio+0x123/0x2d0
    [   76.982209]  submit_bio_noacct_nocheck+0x101/0x3e0
    [   76.982222]  submit_bio_noacct+0x389/0x770
    [   76.982227]  submit_bio+0x50/0xc0
    [   76.982232]  submit_bh_wbc+0x15e/0x230
    [   76.982238]  submit_bh+0x14/0x20
    [   76.982241]  ext4_read_bh_nowait+0xc5/0x130
    [   76.982247]  ext4_read_block_bitmap_nowait+0x340/0xc60
    [   76.982254]  ext4_mb_init_cache+0x1ce/0xdc0
    [   76.982259]  ext4_mb_load_buddy_gfp+0x987/0xfa0
    [   76.982263]  ext4_discard_preallocations+0x45d/0x830
    [   76.982274]  ext4_clear_inode+0x48/0xf0
    [   76.982280]  ext4_evict_inode+0xcf/0xc70
    [   76.982285]  evict+0x119/0x2b0
    [   76.982290]  dispose_list+0x43/0xa0
    [   76.982294]  prune_icache_sb+0x64/0x90
    [   76.982298]  super_cache_scan+0x155/0x210
    [   76.982303]  do_shrink_slab+0x19e/0x4e0
    [   76.982310]  shrink_slab+0x2bd/0x450
    [   76.982317]  drop_slab+0xcc/0x1a0
    [   76.982323]  drop_caches_sysctl_handler+0xb7/0xe0
    [   76.982327]  proc_sys_call_handler+0x1bc/0x300
    [   76.982331]  proc_sys_write+0x17/0x20
    [   76.982334]  vfs_write+0x3d3/0x570
    [   76.982342]  ksys_write+0x73/0x160
    [   76.982347]  __x64_sys_write+0x1e/0x30
    [   76.982352]  do_syscall_64+0x35/0x80
    [   76.982357]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
    
    Function metadata_operation_failed() is called when operations failed
    on dm pool metadata, dm pool will destroy and recreate metadata. So,
    shrinker will be unregistered and registered, which could down write
    shrinker_rwsem under pmd_write_lock.
    
    Fix it by allocating dm_block_manager before locking pmd->root_lock
    and destroying old dm_block_manager after unlocking pmd->root_lock,
    then old dm_block_manager is replaced with new dm_block_manager under
    pmd->root_lock. So, shrinker register/unregister could be done without
    holding pmd->root_lock.
    
    Fetch a reproducer in [Link].
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=216676
    Cc: stable@vger.kernel.org #v5.2+
    Fixes: e49e582965b3 ("dm thin: add read only and fail io modes")
    Signed-off-by: Zhihao Cheng <chengzhihao1@huawei.com>
    Signed-off-by: Mike Snitzer <snitzer@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f8c26c33fef588ee54852cffa7cbb9f9d9869405
Author: Zhihao Cheng <chengzhihao1@huawei.com>
Date:   Wed Nov 30 21:31:34 2022 +0800

    dm thin: Fix ABBA deadlock between shrink_slab and dm_pool_abort_metadata
    
    commit 8111964f1b8524c4bb56b02cd9c7a37725ea21fd upstream.
    
    Following concurrent processes:
    
              P1(drop cache)                P2(kworker)
    drop_caches_sysctl_handler
     drop_slab
      shrink_slab
       down_read(&shrinker_rwsem)  - LOCK A
       do_shrink_slab
        super_cache_scan
         prune_icache_sb
          dispose_list
           evict
            ext4_evict_inode
             ext4_clear_inode
              ext4_discard_preallocations
               ext4_mb_load_buddy_gfp
                ext4_mb_init_cache
                 ext4_read_block_bitmap_nowait
                  ext4_read_bh_nowait
                   submit_bh
                    dm_submit_bio
                                     do_worker
                                      process_deferred_bios
                                       commit
                                        metadata_operation_failed
                                         dm_pool_abort_metadata
                                          down_write(&pmd->root_lock) - LOCK B
                                          __destroy_persistent_data_objects
                                           dm_block_manager_destroy
                                            dm_bufio_client_destroy
                                             unregister_shrinker
                                              down_write(&shrinker_rwsem)
                     thin_map                            |
                      dm_thin_find_block                 ↓
                       down_read(&pmd->root_lock) --> ABBA deadlock
    
    , which triggers hung task:
    
    [   76.974820] INFO: task kworker/u4:3:63 blocked for more than 15 seconds.
    [   76.976019]       Not tainted 6.1.0-rc4-00011-g8f17dd350364-dirty #910
    [   76.978521] task:kworker/u4:3    state:D stack:0     pid:63    ppid:2
    [   76.978534] Workqueue: dm-thin do_worker
    [   76.978552] Call Trace:
    [   76.978564]  __schedule+0x6ba/0x10f0
    [   76.978582]  schedule+0x9d/0x1e0
    [   76.978588]  rwsem_down_write_slowpath+0x587/0xdf0
    [   76.978600]  down_write+0xec/0x110
    [   76.978607]  unregister_shrinker+0x2c/0xf0
    [   76.978616]  dm_bufio_client_destroy+0x116/0x3d0
    [   76.978625]  dm_block_manager_destroy+0x19/0x40
    [   76.978629]  __destroy_persistent_data_objects+0x5e/0x70
    [   76.978636]  dm_pool_abort_metadata+0x8e/0x100
    [   76.978643]  metadata_operation_failed+0x86/0x110
    [   76.978649]  commit+0x6a/0x230
    [   76.978655]  do_worker+0xc6e/0xd90
    [   76.978702]  process_one_work+0x269/0x630
    [   76.978714]  worker_thread+0x266/0x630
    [   76.978730]  kthread+0x151/0x1b0
    [   76.978772] INFO: task test.sh:2646 blocked for more than 15 seconds.
    [   76.979756]       Not tainted 6.1.0-rc4-00011-g8f17dd350364-dirty #910
    [   76.982111] task:test.sh         state:D stack:0     pid:2646  ppid:2459
    [   76.982128] Call Trace:
    [   76.982139]  __schedule+0x6ba/0x10f0
    [   76.982155]  schedule+0x9d/0x1e0
    [   76.982159]  rwsem_down_read_slowpath+0x4f4/0x910
    [   76.982173]  down_read+0x84/0x170
    [   76.982177]  dm_thin_find_block+0x4c/0xd0
    [   76.982183]  thin_map+0x201/0x3d0
    [   76.982188]  __map_bio+0x5b/0x350
    [   76.982195]  dm_submit_bio+0x2b6/0x930
    [   76.982202]  __submit_bio+0x123/0x2d0
    [   76.982209]  submit_bio_noacct_nocheck+0x101/0x3e0
    [   76.982222]  submit_bio_noacct+0x389/0x770
    [   76.982227]  submit_bio+0x50/0xc0
    [   76.982232]  submit_bh_wbc+0x15e/0x230
    [   76.982238]  submit_bh+0x14/0x20
    [   76.982241]  ext4_read_bh_nowait+0xc5/0x130
    [   76.982247]  ext4_read_block_bitmap_nowait+0x340/0xc60
    [   76.982254]  ext4_mb_init_cache+0x1ce/0xdc0
    [   76.982259]  ext4_mb_load_buddy_gfp+0x987/0xfa0
    [   76.982263]  ext4_discard_preallocations+0x45d/0x830
    [   76.982274]  ext4_clear_inode+0x48/0xf0
    [   76.982280]  ext4_evict_inode+0xcf/0xc70
    [   76.982285]  evict+0x119/0x2b0
    [   76.982290]  dispose_list+0x43/0xa0
    [   76.982294]  prune_icache_sb+0x64/0x90
    [   76.982298]  super_cache_scan+0x155/0x210
    [   76.982303]  do_shrink_slab+0x19e/0x4e0
    [   76.982310]  shrink_slab+0x2bd/0x450
    [   76.982317]  drop_slab+0xcc/0x1a0
    [   76.982323]  drop_caches_sysctl_handler+0xb7/0xe0
    [   76.982327]  proc_sys_call_handler+0x1bc/0x300
    [   76.982331]  proc_sys_write+0x17/0x20
    [   76.982334]  vfs_write+0x3d3/0x570
    [   76.982342]  ksys_write+0x73/0x160
    [   76.982347]  __x64_sys_write+0x1e/0x30
    [   76.982352]  do_syscall_64+0x35/0x80
    [   76.982357]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
    
    Function metadata_operation_failed() is called when operations failed
    on dm pool metadata, dm pool will destroy and recreate metadata. So,
    shrinker will be unregistered and registered, which could down write
    shrinker_rwsem under pmd_write_lock.
    
    Fix it by allocating dm_block_manager before locking pmd->root_lock
    and destroying old dm_block_manager after unlocking pmd->root_lock,
    then old dm_block_manager is replaced with new dm_block_manager under
    pmd->root_lock. So, shrinker register/unregister could be done without
    holding pmd->root_lock.
    
    Fetch a reproducer in [Link].
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=216676
    Cc: stable@vger.kernel.org #v5.2+
    Fixes: e49e582965b3 ("dm thin: add read only and fail io modes")
    Signed-off-by: Zhihao Cheng <chengzhihao1@huawei.com>
    Signed-off-by: Mike Snitzer <snitzer@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 96398560f26aa07e8f2969d73c8197e6a6d10407
Author: Frederick Lawler <fred@cloudflare.com>
Date:   Mon Jan 9 10:39:06 2023 -0600

    net: sched: disallow noqueue for qdisc classes
    
    While experimenting with applying noqueue to a classful queue discipline,
    we discovered a NULL pointer dereference in the __dev_queue_xmit()
    path that generates a kernel OOPS:
    
        # dev=enp0s5
        # tc qdisc replace dev $dev root handle 1: htb default 1
        # tc class add dev $dev parent 1: classid 1:1 htb rate 10mbit
        # tc qdisc add dev $dev parent 1:1 handle 10: noqueue
        # ping -I $dev -w 1 -c 1 1.1.1.1
    
    [    2.172856] BUG: kernel NULL pointer dereference, address: 0000000000000000
    [    2.173217] #PF: supervisor instruction fetch in kernel mode
    ...
    [    2.178451] Call Trace:
    [    2.178577]  <TASK>
    [    2.178686]  htb_enqueue+0x1c8/0x370
    [    2.178880]  dev_qdisc_enqueue+0x15/0x90
    [    2.179093]  __dev_queue_xmit+0x798/0xd00
    [    2.179305]  ? _raw_write_lock_bh+0xe/0x30
    [    2.179522]  ? __local_bh_enable_ip+0x32/0x70
    [    2.179759]  ? ___neigh_create+0x610/0x840
    [    2.179968]  ? eth_header+0x21/0xc0
    [    2.180144]  ip_finish_output2+0x15e/0x4f0
    [    2.180348]  ? dst_output+0x30/0x30
    [    2.180525]  ip_push_pending_frames+0x9d/0xb0
    [    2.180739]  raw_sendmsg+0x601/0xcb0
    [    2.180916]  ? _raw_spin_trylock+0xe/0x50
    [    2.181112]  ? _raw_spin_unlock_irqrestore+0x16/0x30
    [    2.181354]  ? get_page_from_freelist+0xcd6/0xdf0
    [    2.181594]  ? sock_sendmsg+0x56/0x60
    [    2.181781]  sock_sendmsg+0x56/0x60
    [    2.181958]  __sys_sendto+0xf7/0x160
    [    2.182139]  ? handle_mm_fault+0x6e/0x1d0
    [    2.182366]  ? do_user_addr_fault+0x1e1/0x660
    [    2.182627]  __x64_sys_sendto+0x1b/0x30
    [    2.182881]  do_syscall_64+0x38/0x90
    [    2.183085]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
    ...
    [    2.187402]  </TASK>
    
    Previously in commit d66d6c3152e8 ("net: sched: register noqueue
    qdisc"), NULL was set for the noqueue discipline on noqueue init
    so that __dev_queue_xmit() falls through for the noqueue case. This
    also sets a bypass of the enqueue NULL check in the
    register_qdisc() function for the struct noqueue_disc_ops.
    
    Classful queue disciplines make it past the NULL check in
    __dev_queue_xmit() because the discipline is set to htb (in this case),
    and then in the call to __dev_xmit_skb(), it calls into htb_enqueue()
    which grabs a leaf node for a class and then calls qdisc_enqueue() by
    passing in a queue discipline which assumes ->enqueue() is not set to NULL.
    
    Fix this by not allowing classes to be assigned to the noqueue
    discipline. Linux TC Notes states that classes cannot be set to
    the noqueue discipline. [1] Let's enforce that here.
    
    Links:
    1. https://linux-tc-notes.sourceforge.net/tc/doc/sch_noqueue.txt
    
    Fixes: d66d6c3152e8 ("net: sched: register noqueue qdisc")
    Cc: stable@vger.kernel.org
    Signed-off-by: Frederick Lawler <fred@cloudflare.com>
    Reviewed-by: Jakub Sitnicki <jakub@cloudflare.com>
    Link: https://lore.kernel.org/r/20230109163906.706000-1-fred@cloudflare.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>

commit 2d891cc5a1706b6908bceb56af7176a463ee6d62
Author: Zhihao Cheng <chengzhihao1@huawei.com>
Date:   Wed Nov 30 21:31:34 2022 +0800

    dm thin: Fix ABBA deadlock between shrink_slab and dm_pool_abort_metadata
    
    commit 8111964f1b8524c4bb56b02cd9c7a37725ea21fd upstream.
    
    Following concurrent processes:
    
              P1(drop cache)                P2(kworker)
    drop_caches_sysctl_handler
     drop_slab
      shrink_slab
       down_read(&shrinker_rwsem)  - LOCK A
       do_shrink_slab
        super_cache_scan
         prune_icache_sb
          dispose_list
           evict
            ext4_evict_inode
             ext4_clear_inode
              ext4_discard_preallocations
               ext4_mb_load_buddy_gfp
                ext4_mb_init_cache
                 ext4_read_block_bitmap_nowait
                  ext4_read_bh_nowait
                   submit_bh
                    dm_submit_bio
                                     do_worker
                                      process_deferred_bios
                                       commit
                                        metadata_operation_failed
                                         dm_pool_abort_metadata
                                          down_write(&pmd->root_lock) - LOCK B
                                          __destroy_persistent_data_objects
                                           dm_block_manager_destroy
                                            dm_bufio_client_destroy
                                             unregister_shrinker
                                              down_write(&shrinker_rwsem)
                     thin_map                            |
                      dm_thin_find_block                 ↓
                       down_read(&pmd->root_lock) --> ABBA deadlock
    
    , which triggers hung task:
    
    [   76.974820] INFO: task kworker/u4:3:63 blocked for more than 15 seconds.
    [   76.976019]       Not tainted 6.1.0-rc4-00011-g8f17dd350364-dirty #910
    [   76.978521] task:kworker/u4:3    state:D stack:0     pid:63    ppid:2
    [   76.978534] Workqueue: dm-thin do_worker
    [   76.978552] Call Trace:
    [   76.978564]  __schedule+0x6ba/0x10f0
    [   76.978582]  schedule+0x9d/0x1e0
    [   76.978588]  rwsem_down_write_slowpath+0x587/0xdf0
    [   76.978600]  down_write+0xec/0x110
    [   76.978607]  unregister_shrinker+0x2c/0xf0
    [   76.978616]  dm_bufio_client_destroy+0x116/0x3d0
    [   76.978625]  dm_block_manager_destroy+0x19/0x40
    [   76.978629]  __destroy_persistent_data_objects+0x5e/0x70
    [   76.978636]  dm_pool_abort_metadata+0x8e/0x100
    [   76.978643]  metadata_operation_failed+0x86/0x110
    [   76.978649]  commit+0x6a/0x230
    [   76.978655]  do_worker+0xc6e/0xd90
    [   76.978702]  process_one_work+0x269/0x630
    [   76.978714]  worker_thread+0x266/0x630
    [   76.978730]  kthread+0x151/0x1b0
    [   76.978772] INFO: task test.sh:2646 blocked for more than 15 seconds.
    [   76.979756]       Not tainted 6.1.0-rc4-00011-g8f17dd350364-dirty #910
    [   76.982111] task:test.sh         state:D stack:0     pid:2646  ppid:2459
    [   76.982128] Call Trace:
    [   76.982139]  __schedule+0x6ba/0x10f0
    [   76.982155]  schedule+0x9d/0x1e0
    [   76.982159]  rwsem_down_read_slowpath+0x4f4/0x910
    [   76.982173]  down_read+0x84/0x170
    [   76.982177]  dm_thin_find_block+0x4c/0xd0
    [   76.982183]  thin_map+0x201/0x3d0
    [   76.982188]  __map_bio+0x5b/0x350
    [   76.982195]  dm_submit_bio+0x2b6/0x930
    [   76.982202]  __submit_bio+0x123/0x2d0
    [   76.982209]  submit_bio_noacct_nocheck+0x101/0x3e0
    [   76.982222]  submit_bio_noacct+0x389/0x770
    [   76.982227]  submit_bio+0x50/0xc0
    [   76.982232]  submit_bh_wbc+0x15e/0x230
    [   76.982238]  submit_bh+0x14/0x20
    [   76.982241]  ext4_read_bh_nowait+0xc5/0x130
    [   76.982247]  ext4_read_block_bitmap_nowait+0x340/0xc60
    [   76.982254]  ext4_mb_init_cache+0x1ce/0xdc0
    [   76.982259]  ext4_mb_load_buddy_gfp+0x987/0xfa0
    [   76.982263]  ext4_discard_preallocations+0x45d/0x830
    [   76.982274]  ext4_clear_inode+0x48/0xf0
    [   76.982280]  ext4_evict_inode+0xcf/0xc70
    [   76.982285]  evict+0x119/0x2b0
    [   76.982290]  dispose_list+0x43/0xa0
    [   76.982294]  prune_icache_sb+0x64/0x90
    [   76.982298]  super_cache_scan+0x155/0x210
    [   76.982303]  do_shrink_slab+0x19e/0x4e0
    [   76.982310]  shrink_slab+0x2bd/0x450
    [   76.982317]  drop_slab+0xcc/0x1a0
    [   76.982323]  drop_caches_sysctl_handler+0xb7/0xe0
    [   76.982327]  proc_sys_call_handler+0x1bc/0x300
    [   76.982331]  proc_sys_write+0x17/0x20
    [   76.982334]  vfs_write+0x3d3/0x570
    [   76.982342]  ksys_write+0x73/0x160
    [   76.982347]  __x64_sys_write+0x1e/0x30
    [   76.982352]  do_syscall_64+0x35/0x80
    [   76.982357]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
    
    Function metadata_operation_failed() is called when operations failed
    on dm pool metadata, dm pool will destroy and recreate metadata. So,
    shrinker will be unregistered and registered, which could down write
    shrinker_rwsem under pmd_write_lock.
    
    Fix it by allocating dm_block_manager before locking pmd->root_lock
    and destroying old dm_block_manager after unlocking pmd->root_lock,
    then old dm_block_manager is replaced with new dm_block_manager under
    pmd->root_lock. So, shrinker register/unregister could be done without
    holding pmd->root_lock.
    
    Fetch a reproducer in [Link].
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=216676
    Cc: stable@vger.kernel.org #v5.2+
    Fixes: e49e582965b3 ("dm thin: add read only and fail io modes")
    Signed-off-by: Zhihao Cheng <chengzhihao1@huawei.com>
    Signed-off-by: Mike Snitzer <snitzer@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit cdf7a39bcc427febbfe3c3b9fe829825ead96c27
Author: Zhihao Cheng <chengzhihao1@huawei.com>
Date:   Wed Nov 30 21:31:34 2022 +0800

    dm thin: Fix ABBA deadlock between shrink_slab and dm_pool_abort_metadata
    
    commit 8111964f1b8524c4bb56b02cd9c7a37725ea21fd upstream.
    
    Following concurrent processes:
    
              P1(drop cache)                P2(kworker)
    drop_caches_sysctl_handler
     drop_slab
      shrink_slab
       down_read(&shrinker_rwsem)  - LOCK A
       do_shrink_slab
        super_cache_scan
         prune_icache_sb
          dispose_list
           evict
            ext4_evict_inode
             ext4_clear_inode
              ext4_discard_preallocations
               ext4_mb_load_buddy_gfp
                ext4_mb_init_cache
                 ext4_read_block_bitmap_nowait
                  ext4_read_bh_nowait
                   submit_bh
                    dm_submit_bio
                                     do_worker
                                      process_deferred_bios
                                       commit
                                        metadata_operation_failed
                                         dm_pool_abort_metadata
                                          down_write(&pmd->root_lock) - LOCK B
                                          __destroy_persistent_data_objects
                                           dm_block_manager_destroy
                                            dm_bufio_client_destroy
                                             unregister_shrinker
                                              down_write(&shrinker_rwsem)
                     thin_map                            |
                      dm_thin_find_block                 ↓
                       down_read(&pmd->root_lock) --> ABBA deadlock
    
    , which triggers hung task:
    
    [   76.974820] INFO: task kworker/u4:3:63 blocked for more than 15 seconds.
    [   76.976019]       Not tainted 6.1.0-rc4-00011-g8f17dd350364-dirty #910
    [   76.978521] task:kworker/u4:3    state:D stack:0     pid:63    ppid:2
    [   76.978534] Workqueue: dm-thin do_worker
    [   76.978552] Call Trace:
    [   76.978564]  __schedule+0x6ba/0x10f0
    [   76.978582]  schedule+0x9d/0x1e0
    [   76.978588]  rwsem_down_write_slowpath+0x587/0xdf0
    [   76.978600]  down_write+0xec/0x110
    [   76.978607]  unregister_shrinker+0x2c/0xf0
    [   76.978616]  dm_bufio_client_destroy+0x116/0x3d0
    [   76.978625]  dm_block_manager_destroy+0x19/0x40
    [   76.978629]  __destroy_persistent_data_objects+0x5e/0x70
    [   76.978636]  dm_pool_abort_metadata+0x8e/0x100
    [   76.978643]  metadata_operation_failed+0x86/0x110
    [   76.978649]  commit+0x6a/0x230
    [   76.978655]  do_worker+0xc6e/0xd90
    [   76.978702]  process_one_work+0x269/0x630
    [   76.978714]  worker_thread+0x266/0x630
    [   76.978730]  kthread+0x151/0x1b0
    [   76.978772] INFO: task test.sh:2646 blocked for more than 15 seconds.
    [   76.979756]       Not tainted 6.1.0-rc4-00011-g8f17dd350364-dirty #910
    [   76.982111] task:test.sh         state:D stack:0     pid:2646  ppid:2459
    [   76.982128] Call Trace:
    [   76.982139]  __schedule+0x6ba/0x10f0
    [   76.982155]  schedule+0x9d/0x1e0
    [   76.982159]  rwsem_down_read_slowpath+0x4f4/0x910
    [   76.982173]  down_read+0x84/0x170
    [   76.982177]  dm_thin_find_block+0x4c/0xd0
    [   76.982183]  thin_map+0x201/0x3d0
    [   76.982188]  __map_bio+0x5b/0x350
    [   76.982195]  dm_submit_bio+0x2b6/0x930
    [   76.982202]  __submit_bio+0x123/0x2d0
    [   76.982209]  submit_bio_noacct_nocheck+0x101/0x3e0
    [   76.982222]  submit_bio_noacct+0x389/0x770
    [   76.982227]  submit_bio+0x50/0xc0
    [   76.982232]  submit_bh_wbc+0x15e/0x230
    [   76.982238]  submit_bh+0x14/0x20
    [   76.982241]  ext4_read_bh_nowait+0xc5/0x130
    [   76.982247]  ext4_read_block_bitmap_nowait+0x340/0xc60
    [   76.982254]  ext4_mb_init_cache+0x1ce/0xdc0
    [   76.982259]  ext4_mb_load_buddy_gfp+0x987/0xfa0
    [   76.982263]  ext4_discard_preallocations+0x45d/0x830
    [   76.982274]  ext4_clear_inode+0x48/0xf0
    [   76.982280]  ext4_evict_inode+0xcf/0xc70
    [   76.982285]  evict+0x119/0x2b0
    [   76.982290]  dispose_list+0x43/0xa0
    [   76.982294]  prune_icache_sb+0x64/0x90
    [   76.982298]  super_cache_scan+0x155/0x210
    [   76.982303]  do_shrink_slab+0x19e/0x4e0
    [   76.982310]  shrink_slab+0x2bd/0x450
    [   76.982317]  drop_slab+0xcc/0x1a0
    [   76.982323]  drop_caches_sysctl_handler+0xb7/0xe0
    [   76.982327]  proc_sys_call_handler+0x1bc/0x300
    [   76.982331]  proc_sys_write+0x17/0x20
    [   76.982334]  vfs_write+0x3d3/0x570
    [   76.982342]  ksys_write+0x73/0x160
    [   76.982347]  __x64_sys_write+0x1e/0x30
    [   76.982352]  do_syscall_64+0x35/0x80
    [   76.982357]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
    
    Function metadata_operation_failed() is called when operations failed
    on dm pool metadata, dm pool will destroy and recreate metadata. So,
    shrinker will be unregistered and registered, which could down write
    shrinker_rwsem under pmd_write_lock.
    
    Fix it by allocating dm_block_manager before locking pmd->root_lock
    and destroying old dm_block_manager after unlocking pmd->root_lock,
    then old dm_block_manager is replaced with new dm_block_manager under
    pmd->root_lock. So, shrinker register/unregister could be done without
    holding pmd->root_lock.
    
    Fetch a reproducer in [Link].
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=216676
    Cc: stable@vger.kernel.org #v5.2+
    Fixes: e49e582965b3 ("dm thin: add read only and fail io modes")
    Signed-off-by: Zhihao Cheng <chengzhihao1@huawei.com>
    Signed-off-by: Mike Snitzer <snitzer@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1bb4f8be3797a1c12c1708253ca5535cff9afa9c
Author: Xin Long <lucien.xin@gmail.com>
Date:   Sat Dec 3 18:37:21 2022 -0500

    tipc: call tipc_lxc_xmit without holding node_read_lock
    
    [ Upstream commit 88956177db179e4eba7cd590971961857d1565b8 ]
    
    When sending packets between nodes in netns, it calls tipc_lxc_xmit() for
    peer node to receive the packets where tipc_sk_mcast_rcv()/tipc_sk_rcv()
    might be called, and it's pretty much like in tipc_rcv().
    
    Currently the local 'node rw lock' is held during calling tipc_lxc_xmit()
    to protect the peer_net not being freed by another thread. However, when
    receiving these packets, tipc_node_add_conn() might be called where the
    peer 'node rw lock' is acquired. Then a dead lock warning is triggered by
    lockdep detector, although it is not a real dead lock:
    
        WARNING: possible recursive locking detected
        --------------------------------------------
        conn_server/1086 is trying to acquire lock:
        ffff8880065cb020 (&n->lock#2){++--}-{2:2}, \
                         at: tipc_node_add_conn.cold.76+0xaa/0x211 [tipc]
    
        but task is already holding lock:
        ffff8880065cd020 (&n->lock#2){++--}-{2:2}, \
                         at: tipc_node_xmit+0x285/0xb30 [tipc]
    
        other info that might help us debug this:
         Possible unsafe locking scenario:
    
               CPU0
               ----
          lock(&n->lock#2);
          lock(&n->lock#2);
    
         *** DEADLOCK ***
    
         May be due to missing lock nesting notation
    
        4 locks held by conn_server/1086:
         #0: ffff8880036d1e40 (sk_lock-AF_TIPC){+.+.}-{0:0}, \
                              at: tipc_accept+0x9c0/0x10b0 [tipc]
         #1: ffff8880036d5f80 (sk_lock-AF_TIPC/1){+.+.}-{0:0}, \
                              at: tipc_accept+0x363/0x10b0 [tipc]
         #2: ffff8880065cd020 (&n->lock#2){++--}-{2:2}, \
                              at: tipc_node_xmit+0x285/0xb30 [tipc]
         #3: ffff888012e13370 (slock-AF_TIPC){+...}-{2:2}, \
                              at: tipc_sk_rcv+0x2da/0x1b40 [tipc]
    
        Call Trace:
         <TASK>
         dump_stack_lvl+0x44/0x5b
         __lock_acquire.cold.77+0x1f2/0x3d7
         lock_acquire+0x1d2/0x610
         _raw_write_lock_bh+0x38/0x80
         tipc_node_add_conn.cold.76+0xaa/0x211 [tipc]
         tipc_sk_finish_conn+0x21e/0x640 [tipc]
         tipc_sk_filter_rcv+0x147b/0x3030 [tipc]
         tipc_sk_rcv+0xbb4/0x1b40 [tipc]
         tipc_lxc_xmit+0x225/0x26b [tipc]
         tipc_node_xmit.cold.82+0x4a/0x102 [tipc]
         __tipc_sendstream+0x879/0xff0 [tipc]
         tipc_accept+0x966/0x10b0 [tipc]
         do_accept+0x37d/0x590
    
    This patch avoids this warning by not holding the 'node rw lock' before
    calling tipc_lxc_xmit(). As to protect the 'peer_net', rcu_read_lock()
    should be enough, as in cleanup_net() when freeing the netns, it calls
    synchronize_rcu() before the free is continued.
    
    Also since tipc_lxc_xmit() is like the RX path in tipc_rcv(), it makes
    sense to call it under rcu_read_lock(). Note that the right lock order
    must be:
    
       rcu_read_lock();
       tipc_node_read_lock(n);
       tipc_node_read_unlock(n);
       tipc_lxc_xmit();
       rcu_read_unlock();
    
    instead of:
    
       tipc_node_read_lock(n);
       rcu_read_lock();
       tipc_node_read_unlock(n);
       tipc_lxc_xmit();
       rcu_read_unlock();
    
    and we have to call tipc_node_read_lock/unlock() twice in
    tipc_node_xmit().
    
    Fixes: f73b12812a3d ("tipc: improve throughput between nodes in netns")
    Reported-by: Shuang Li <shuali@redhat.com>
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Link: https://lore.kernel.org/r/5bdd1f8fee9db695cfff4528a48c9b9d0523fb00.1670110641.git.lucien.xin@gmail.com
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d962d42d637613e16c7be218a167d42257eac5f4
Author: Xin Long <lucien.xin@gmail.com>
Date:   Sat Dec 3 18:37:21 2022 -0500

    tipc: call tipc_lxc_xmit without holding node_read_lock
    
    [ Upstream commit 88956177db179e4eba7cd590971961857d1565b8 ]
    
    When sending packets between nodes in netns, it calls tipc_lxc_xmit() for
    peer node to receive the packets where tipc_sk_mcast_rcv()/tipc_sk_rcv()
    might be called, and it's pretty much like in tipc_rcv().
    
    Currently the local 'node rw lock' is held during calling tipc_lxc_xmit()
    to protect the peer_net not being freed by another thread. However, when
    receiving these packets, tipc_node_add_conn() might be called where the
    peer 'node rw lock' is acquired. Then a dead lock warning is triggered by
    lockdep detector, although it is not a real dead lock:
    
        WARNING: possible recursive locking detected
        --------------------------------------------
        conn_server/1086 is trying to acquire lock:
        ffff8880065cb020 (&n->lock#2){++--}-{2:2}, \
                         at: tipc_node_add_conn.cold.76+0xaa/0x211 [tipc]
    
        but task is already holding lock:
        ffff8880065cd020 (&n->lock#2){++--}-{2:2}, \
                         at: tipc_node_xmit+0x285/0xb30 [tipc]
    
        other info that might help us debug this:
         Possible unsafe locking scenario:
    
               CPU0
               ----
          lock(&n->lock#2);
          lock(&n->lock#2);
    
         *** DEADLOCK ***
    
         May be due to missing lock nesting notation
    
        4 locks held by conn_server/1086:
         #0: ffff8880036d1e40 (sk_lock-AF_TIPC){+.+.}-{0:0}, \
                              at: tipc_accept+0x9c0/0x10b0 [tipc]
         #1: ffff8880036d5f80 (sk_lock-AF_TIPC/1){+.+.}-{0:0}, \
                              at: tipc_accept+0x363/0x10b0 [tipc]
         #2: ffff8880065cd020 (&n->lock#2){++--}-{2:2}, \
                              at: tipc_node_xmit+0x285/0xb30 [tipc]
         #3: ffff888012e13370 (slock-AF_TIPC){+...}-{2:2}, \
                              at: tipc_sk_rcv+0x2da/0x1b40 [tipc]
    
        Call Trace:
         <TASK>
         dump_stack_lvl+0x44/0x5b
         __lock_acquire.cold.77+0x1f2/0x3d7
         lock_acquire+0x1d2/0x610
         _raw_write_lock_bh+0x38/0x80
         tipc_node_add_conn.cold.76+0xaa/0x211 [tipc]
         tipc_sk_finish_conn+0x21e/0x640 [tipc]
         tipc_sk_filter_rcv+0x147b/0x3030 [tipc]
         tipc_sk_rcv+0xbb4/0x1b40 [tipc]
         tipc_lxc_xmit+0x225/0x26b [tipc]
         tipc_node_xmit.cold.82+0x4a/0x102 [tipc]
         __tipc_sendstream+0x879/0xff0 [tipc]
         tipc_accept+0x966/0x10b0 [tipc]
         do_accept+0x37d/0x590
    
    This patch avoids this warning by not holding the 'node rw lock' before
    calling tipc_lxc_xmit(). As to protect the 'peer_net', rcu_read_lock()
    should be enough, as in cleanup_net() when freeing the netns, it calls
    synchronize_rcu() before the free is continued.
    
    Also since tipc_lxc_xmit() is like the RX path in tipc_rcv(), it makes
    sense to call it under rcu_read_lock(). Note that the right lock order
    must be:
    
       rcu_read_lock();
       tipc_node_read_lock(n);
       tipc_node_read_unlock(n);
       tipc_lxc_xmit();
       rcu_read_unlock();
    
    instead of:
    
       tipc_node_read_lock(n);
       rcu_read_lock();
       tipc_node_read_unlock(n);
       tipc_lxc_xmit();
       rcu_read_unlock();
    
    and we have to call tipc_node_read_lock/unlock() twice in
    tipc_node_xmit().
    
    Fixes: f73b12812a3d ("tipc: improve throughput between nodes in netns")
    Reported-by: Shuang Li <shuali@redhat.com>
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Link: https://lore.kernel.org/r/5bdd1f8fee9db695cfff4528a48c9b9d0523fb00.1670110641.git.lucien.xin@gmail.com
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit cc668fddde4262f608baca2c9d85b9cf333e41c3
Author: Xin Long <lucien.xin@gmail.com>
Date:   Sat Dec 3 18:37:21 2022 -0500

    tipc: call tipc_lxc_xmit without holding node_read_lock
    
    [ Upstream commit 88956177db179e4eba7cd590971961857d1565b8 ]
    
    When sending packets between nodes in netns, it calls tipc_lxc_xmit() for
    peer node to receive the packets where tipc_sk_mcast_rcv()/tipc_sk_rcv()
    might be called, and it's pretty much like in tipc_rcv().
    
    Currently the local 'node rw lock' is held during calling tipc_lxc_xmit()
    to protect the peer_net not being freed by another thread. However, when
    receiving these packets, tipc_node_add_conn() might be called where the
    peer 'node rw lock' is acquired. Then a dead lock warning is triggered by
    lockdep detector, although it is not a real dead lock:
    
        WARNING: possible recursive locking detected
        --------------------------------------------
        conn_server/1086 is trying to acquire lock:
        ffff8880065cb020 (&n->lock#2){++--}-{2:2}, \
                         at: tipc_node_add_conn.cold.76+0xaa/0x211 [tipc]
    
        but task is already holding lock:
        ffff8880065cd020 (&n->lock#2){++--}-{2:2}, \
                         at: tipc_node_xmit+0x285/0xb30 [tipc]
    
        other info that might help us debug this:
         Possible unsafe locking scenario:
    
               CPU0
               ----
          lock(&n->lock#2);
          lock(&n->lock#2);
    
         *** DEADLOCK ***
    
         May be due to missing lock nesting notation
    
        4 locks held by conn_server/1086:
         #0: ffff8880036d1e40 (sk_lock-AF_TIPC){+.+.}-{0:0}, \
                              at: tipc_accept+0x9c0/0x10b0 [tipc]
         #1: ffff8880036d5f80 (sk_lock-AF_TIPC/1){+.+.}-{0:0}, \
                              at: tipc_accept+0x363/0x10b0 [tipc]
         #2: ffff8880065cd020 (&n->lock#2){++--}-{2:2}, \
                              at: tipc_node_xmit+0x285/0xb30 [tipc]
         #3: ffff888012e13370 (slock-AF_TIPC){+...}-{2:2}, \
                              at: tipc_sk_rcv+0x2da/0x1b40 [tipc]
    
        Call Trace:
         <TASK>
         dump_stack_lvl+0x44/0x5b
         __lock_acquire.cold.77+0x1f2/0x3d7
         lock_acquire+0x1d2/0x610
         _raw_write_lock_bh+0x38/0x80
         tipc_node_add_conn.cold.76+0xaa/0x211 [tipc]
         tipc_sk_finish_conn+0x21e/0x640 [tipc]
         tipc_sk_filter_rcv+0x147b/0x3030 [tipc]
         tipc_sk_rcv+0xbb4/0x1b40 [tipc]
         tipc_lxc_xmit+0x225/0x26b [tipc]
         tipc_node_xmit.cold.82+0x4a/0x102 [tipc]
         __tipc_sendstream+0x879/0xff0 [tipc]
         tipc_accept+0x966/0x10b0 [tipc]
         do_accept+0x37d/0x590
    
    This patch avoids this warning by not holding the 'node rw lock' before
    calling tipc_lxc_xmit(). As to protect the 'peer_net', rcu_read_lock()
    should be enough, as in cleanup_net() when freeing the netns, it calls
    synchronize_rcu() before the free is continued.
    
    Also since tipc_lxc_xmit() is like the RX path in tipc_rcv(), it makes
    sense to call it under rcu_read_lock(). Note that the right lock order
    must be:
    
       rcu_read_lock();
       tipc_node_read_lock(n);
       tipc_node_read_unlock(n);
       tipc_lxc_xmit();
       rcu_read_unlock();
    
    instead of:
    
       tipc_node_read_lock(n);
       rcu_read_lock();
       tipc_node_read_unlock(n);
       tipc_lxc_xmit();
       rcu_read_unlock();
    
    and we have to call tipc_node_read_lock/unlock() twice in
    tipc_node_xmit().
    
    Fixes: f73b12812a3d ("tipc: improve throughput between nodes in netns")
    Reported-by: Shuang Li <shuali@redhat.com>
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Link: https://lore.kernel.org/r/5bdd1f8fee9db695cfff4528a48c9b9d0523fb00.1670110641.git.lucien.xin@gmail.com
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 59f9aad22fd743572bdafa37d3e1dd5dc5658e26
Author: Xin Long <lucien.xin@gmail.com>
Date:   Fri Nov 18 16:45:01 2022 -0500

    tipc: add an extra conn_get in tipc_conn_alloc
    
    [ Upstream commit a7b42969d63f47320853a802efd879fbdc4e010e ]
    
    One extra conn_get() is needed in tipc_conn_alloc(), as after
    tipc_conn_alloc() is called, tipc_conn_close() may free this
    con before deferencing it in tipc_topsrv_accept():
    
       tipc_conn_alloc();
       newsk = newsock->sk;
                                     <---- tipc_conn_close();
       write_lock_bh(&sk->sk_callback_lock);
       newsk->sk_data_ready = tipc_conn_data_ready;
    
    Then an uaf issue can be triggered:
    
      BUG: KASAN: use-after-free in tipc_topsrv_accept+0x1e7/0x370 [tipc]
      Call Trace:
       <TASK>
       dump_stack_lvl+0x33/0x46
       print_report+0x178/0x4b0
       kasan_report+0x8c/0x100
       kasan_check_range+0x179/0x1e0
       tipc_topsrv_accept+0x1e7/0x370 [tipc]
       process_one_work+0x6a3/0x1030
       worker_thread+0x8a/0xdf0
    
    This patch fixes it by holding it in tipc_conn_alloc(), then after
    all accessing in tipc_topsrv_accept() releasing it. Note when does
    this in tipc_topsrv_kern_subscr(), as tipc_conn_rcv_sub() returns
    0 or -1 only, we don't need to check for "> 0".
    
    Fixes: c5fa7b3cf3cb ("tipc: introduce new TIPC server infrastructure")
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Acked-by: Jon Maloy <jmaloy@redhat.com>
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f46826a6fce33c3549332c3eb1fbf615dc79be18
Author: Xin Long <lucien.xin@gmail.com>
Date:   Fri Nov 18 16:45:01 2022 -0500

    tipc: add an extra conn_get in tipc_conn_alloc
    
    [ Upstream commit a7b42969d63f47320853a802efd879fbdc4e010e ]
    
    One extra conn_get() is needed in tipc_conn_alloc(), as after
    tipc_conn_alloc() is called, tipc_conn_close() may free this
    con before deferencing it in tipc_topsrv_accept():
    
       tipc_conn_alloc();
       newsk = newsock->sk;
                                     <---- tipc_conn_close();
       write_lock_bh(&sk->sk_callback_lock);
       newsk->sk_data_ready = tipc_conn_data_ready;
    
    Then an uaf issue can be triggered:
    
      BUG: KASAN: use-after-free in tipc_topsrv_accept+0x1e7/0x370 [tipc]
      Call Trace:
       <TASK>
       dump_stack_lvl+0x33/0x46
       print_report+0x178/0x4b0
       kasan_report+0x8c/0x100
       kasan_check_range+0x179/0x1e0
       tipc_topsrv_accept+0x1e7/0x370 [tipc]
       process_one_work+0x6a3/0x1030
       worker_thread+0x8a/0xdf0
    
    This patch fixes it by holding it in tipc_conn_alloc(), then after
    all accessing in tipc_topsrv_accept() releasing it. Note when does
    this in tipc_topsrv_kern_subscr(), as tipc_conn_rcv_sub() returns
    0 or -1 only, we don't need to check for "> 0".
    
    Fixes: c5fa7b3cf3cb ("tipc: introduce new TIPC server infrastructure")
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Acked-by: Jon Maloy <jmaloy@redhat.com>
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 88956177db179e4eba7cd590971961857d1565b8
Author: Xin Long <lucien.xin@gmail.com>
Date:   Sat Dec 3 18:37:21 2022 -0500

    tipc: call tipc_lxc_xmit without holding node_read_lock
    
    When sending packets between nodes in netns, it calls tipc_lxc_xmit() for
    peer node to receive the packets where tipc_sk_mcast_rcv()/tipc_sk_rcv()
    might be called, and it's pretty much like in tipc_rcv().
    
    Currently the local 'node rw lock' is held during calling tipc_lxc_xmit()
    to protect the peer_net not being freed by another thread. However, when
    receiving these packets, tipc_node_add_conn() might be called where the
    peer 'node rw lock' is acquired. Then a dead lock warning is triggered by
    lockdep detector, although it is not a real dead lock:
    
        WARNING: possible recursive locking detected
        --------------------------------------------
        conn_server/1086 is trying to acquire lock:
        ffff8880065cb020 (&n->lock#2){++--}-{2:2}, \
                         at: tipc_node_add_conn.cold.76+0xaa/0x211 [tipc]
    
        but task is already holding lock:
        ffff8880065cd020 (&n->lock#2){++--}-{2:2}, \
                         at: tipc_node_xmit+0x285/0xb30 [tipc]
    
        other info that might help us debug this:
         Possible unsafe locking scenario:
    
               CPU0
               ----
          lock(&n->lock#2);
          lock(&n->lock#2);
    
         *** DEADLOCK ***
    
         May be due to missing lock nesting notation
    
        4 locks held by conn_server/1086:
         #0: ffff8880036d1e40 (sk_lock-AF_TIPC){+.+.}-{0:0}, \
                              at: tipc_accept+0x9c0/0x10b0 [tipc]
         #1: ffff8880036d5f80 (sk_lock-AF_TIPC/1){+.+.}-{0:0}, \
                              at: tipc_accept+0x363/0x10b0 [tipc]
         #2: ffff8880065cd020 (&n->lock#2){++--}-{2:2}, \
                              at: tipc_node_xmit+0x285/0xb30 [tipc]
         #3: ffff888012e13370 (slock-AF_TIPC){+...}-{2:2}, \
                              at: tipc_sk_rcv+0x2da/0x1b40 [tipc]
    
        Call Trace:
         <TASK>
         dump_stack_lvl+0x44/0x5b
         __lock_acquire.cold.77+0x1f2/0x3d7
         lock_acquire+0x1d2/0x610
         _raw_write_lock_bh+0x38/0x80
         tipc_node_add_conn.cold.76+0xaa/0x211 [tipc]
         tipc_sk_finish_conn+0x21e/0x640 [tipc]
         tipc_sk_filter_rcv+0x147b/0x3030 [tipc]
         tipc_sk_rcv+0xbb4/0x1b40 [tipc]
         tipc_lxc_xmit+0x225/0x26b [tipc]
         tipc_node_xmit.cold.82+0x4a/0x102 [tipc]
         __tipc_sendstream+0x879/0xff0 [tipc]
         tipc_accept+0x966/0x10b0 [tipc]
         do_accept+0x37d/0x590
    
    This patch avoids this warning by not holding the 'node rw lock' before
    calling tipc_lxc_xmit(). As to protect the 'peer_net', rcu_read_lock()
    should be enough, as in cleanup_net() when freeing the netns, it calls
    synchronize_rcu() before the free is continued.
    
    Also since tipc_lxc_xmit() is like the RX path in tipc_rcv(), it makes
    sense to call it under rcu_read_lock(). Note that the right lock order
    must be:
    
       rcu_read_lock();
       tipc_node_read_lock(n);
       tipc_node_read_unlock(n);
       tipc_lxc_xmit();
       rcu_read_unlock();
    
    instead of:
    
       tipc_node_read_lock(n);
       rcu_read_lock();
       tipc_node_read_unlock(n);
       tipc_lxc_xmit();
       rcu_read_unlock();
    
    and we have to call tipc_node_read_lock/unlock() twice in
    tipc_node_xmit().
    
    Fixes: f73b12812a3d ("tipc: improve throughput between nodes in netns")
    Reported-by: Shuang Li <shuali@redhat.com>
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Link: https://lore.kernel.org/r/5bdd1f8fee9db695cfff4528a48c9b9d0523fb00.1670110641.git.lucien.xin@gmail.com
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>

commit bd5a4220f3414bb23d389040481f86928adf32e0
Author: Xin Long <lucien.xin@gmail.com>
Date:   Fri Nov 18 16:45:01 2022 -0500

    tipc: add an extra conn_get in tipc_conn_alloc
    
    [ Upstream commit a7b42969d63f47320853a802efd879fbdc4e010e ]
    
    One extra conn_get() is needed in tipc_conn_alloc(), as after
    tipc_conn_alloc() is called, tipc_conn_close() may free this
    con before deferencing it in tipc_topsrv_accept():
    
       tipc_conn_alloc();
       newsk = newsock->sk;
                                     <---- tipc_conn_close();
       write_lock_bh(&sk->sk_callback_lock);
       newsk->sk_data_ready = tipc_conn_data_ready;
    
    Then an uaf issue can be triggered:
    
      BUG: KASAN: use-after-free in tipc_topsrv_accept+0x1e7/0x370 [tipc]
      Call Trace:
       <TASK>
       dump_stack_lvl+0x33/0x46
       print_report+0x178/0x4b0
       kasan_report+0x8c/0x100
       kasan_check_range+0x179/0x1e0
       tipc_topsrv_accept+0x1e7/0x370 [tipc]
       process_one_work+0x6a3/0x1030
       worker_thread+0x8a/0xdf0
    
    This patch fixes it by holding it in tipc_conn_alloc(), then after
    all accessing in tipc_topsrv_accept() releasing it. Note when does
    this in tipc_topsrv_kern_subscr(), as tipc_conn_rcv_sub() returns
    0 or -1 only, we don't need to check for "> 0".
    
    Fixes: c5fa7b3cf3cb ("tipc: introduce new TIPC server infrastructure")
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Acked-by: Jon Maloy <jmaloy@redhat.com>
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 33fb115a76ae6683e34f76f7e07f6f0734b2525f
Author: Xin Long <lucien.xin@gmail.com>
Date:   Fri Nov 18 16:45:01 2022 -0500

    tipc: add an extra conn_get in tipc_conn_alloc
    
    [ Upstream commit a7b42969d63f47320853a802efd879fbdc4e010e ]
    
    One extra conn_get() is needed in tipc_conn_alloc(), as after
    tipc_conn_alloc() is called, tipc_conn_close() may free this
    con before deferencing it in tipc_topsrv_accept():
    
       tipc_conn_alloc();
       newsk = newsock->sk;
                                     <---- tipc_conn_close();
       write_lock_bh(&sk->sk_callback_lock);
       newsk->sk_data_ready = tipc_conn_data_ready;
    
    Then an uaf issue can be triggered:
    
      BUG: KASAN: use-after-free in tipc_topsrv_accept+0x1e7/0x370 [tipc]
      Call Trace:
       <TASK>
       dump_stack_lvl+0x33/0x46
       print_report+0x178/0x4b0
       kasan_report+0x8c/0x100
       kasan_check_range+0x179/0x1e0
       tipc_topsrv_accept+0x1e7/0x370 [tipc]
       process_one_work+0x6a3/0x1030
       worker_thread+0x8a/0xdf0
    
    This patch fixes it by holding it in tipc_conn_alloc(), then after
    all accessing in tipc_topsrv_accept() releasing it. Note when does
    this in tipc_topsrv_kern_subscr(), as tipc_conn_rcv_sub() returns
    0 or -1 only, we don't need to check for "> 0".
    
    Fixes: c5fa7b3cf3cb ("tipc: introduce new TIPC server infrastructure")
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Acked-by: Jon Maloy <jmaloy@redhat.com>
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 4058e3b74ab3eabe0835cee9a0c6deda79e8a295
Author: Xin Long <lucien.xin@gmail.com>
Date:   Fri Nov 18 16:45:01 2022 -0500

    tipc: add an extra conn_get in tipc_conn_alloc
    
    [ Upstream commit a7b42969d63f47320853a802efd879fbdc4e010e ]
    
    One extra conn_get() is needed in tipc_conn_alloc(), as after
    tipc_conn_alloc() is called, tipc_conn_close() may free this
    con before deferencing it in tipc_topsrv_accept():
    
       tipc_conn_alloc();
       newsk = newsock->sk;
                                     <---- tipc_conn_close();
       write_lock_bh(&sk->sk_callback_lock);
       newsk->sk_data_ready = tipc_conn_data_ready;
    
    Then an uaf issue can be triggered:
    
      BUG: KASAN: use-after-free in tipc_topsrv_accept+0x1e7/0x370 [tipc]
      Call Trace:
       <TASK>
       dump_stack_lvl+0x33/0x46
       print_report+0x178/0x4b0
       kasan_report+0x8c/0x100
       kasan_check_range+0x179/0x1e0
       tipc_topsrv_accept+0x1e7/0x370 [tipc]
       process_one_work+0x6a3/0x1030
       worker_thread+0x8a/0xdf0
    
    This patch fixes it by holding it in tipc_conn_alloc(), then after
    all accessing in tipc_topsrv_accept() releasing it. Note when does
    this in tipc_topsrv_kern_subscr(), as tipc_conn_rcv_sub() returns
    0 or -1 only, we don't need to check for "> 0".
    
    Fixes: c5fa7b3cf3cb ("tipc: introduce new TIPC server infrastructure")
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Acked-by: Jon Maloy <jmaloy@redhat.com>
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 8111964f1b8524c4bb56b02cd9c7a37725ea21fd
Author: Zhihao Cheng <chengzhihao1@huawei.com>
Date:   Wed Nov 30 21:31:34 2022 +0800

    dm thin: Fix ABBA deadlock between shrink_slab and dm_pool_abort_metadata
    
    Following concurrent processes:
    
              P1(drop cache)                P2(kworker)
    drop_caches_sysctl_handler
     drop_slab
      shrink_slab
       down_read(&shrinker_rwsem)  - LOCK A
       do_shrink_slab
        super_cache_scan
         prune_icache_sb
          dispose_list
           evict
            ext4_evict_inode
             ext4_clear_inode
              ext4_discard_preallocations
               ext4_mb_load_buddy_gfp
                ext4_mb_init_cache
                 ext4_read_block_bitmap_nowait
                  ext4_read_bh_nowait
                   submit_bh
                    dm_submit_bio
                                     do_worker
                                      process_deferred_bios
                                       commit
                                        metadata_operation_failed
                                         dm_pool_abort_metadata
                                          down_write(&pmd->root_lock) - LOCK B
                                          __destroy_persistent_data_objects
                                           dm_block_manager_destroy
                                            dm_bufio_client_destroy
                                             unregister_shrinker
                                              down_write(&shrinker_rwsem)
                     thin_map                            |
                      dm_thin_find_block                 ↓
                       down_read(&pmd->root_lock) --> ABBA deadlock
    
    , which triggers hung task:
    
    [   76.974820] INFO: task kworker/u4:3:63 blocked for more than 15 seconds.
    [   76.976019]       Not tainted 6.1.0-rc4-00011-g8f17dd350364-dirty #910
    [   76.978521] task:kworker/u4:3    state:D stack:0     pid:63    ppid:2
    [   76.978534] Workqueue: dm-thin do_worker
    [   76.978552] Call Trace:
    [   76.978564]  __schedule+0x6ba/0x10f0
    [   76.978582]  schedule+0x9d/0x1e0
    [   76.978588]  rwsem_down_write_slowpath+0x587/0xdf0
    [   76.978600]  down_write+0xec/0x110
    [   76.978607]  unregister_shrinker+0x2c/0xf0
    [   76.978616]  dm_bufio_client_destroy+0x116/0x3d0
    [   76.978625]  dm_block_manager_destroy+0x19/0x40
    [   76.978629]  __destroy_persistent_data_objects+0x5e/0x70
    [   76.978636]  dm_pool_abort_metadata+0x8e/0x100
    [   76.978643]  metadata_operation_failed+0x86/0x110
    [   76.978649]  commit+0x6a/0x230
    [   76.978655]  do_worker+0xc6e/0xd90
    [   76.978702]  process_one_work+0x269/0x630
    [   76.978714]  worker_thread+0x266/0x630
    [   76.978730]  kthread+0x151/0x1b0
    [   76.978772] INFO: task test.sh:2646 blocked for more than 15 seconds.
    [   76.979756]       Not tainted 6.1.0-rc4-00011-g8f17dd350364-dirty #910
    [   76.982111] task:test.sh         state:D stack:0     pid:2646  ppid:2459
    [   76.982128] Call Trace:
    [   76.982139]  __schedule+0x6ba/0x10f0
    [   76.982155]  schedule+0x9d/0x1e0
    [   76.982159]  rwsem_down_read_slowpath+0x4f4/0x910
    [   76.982173]  down_read+0x84/0x170
    [   76.982177]  dm_thin_find_block+0x4c/0xd0
    [   76.982183]  thin_map+0x201/0x3d0
    [   76.982188]  __map_bio+0x5b/0x350
    [   76.982195]  dm_submit_bio+0x2b6/0x930
    [   76.982202]  __submit_bio+0x123/0x2d0
    [   76.982209]  submit_bio_noacct_nocheck+0x101/0x3e0
    [   76.982222]  submit_bio_noacct+0x389/0x770
    [   76.982227]  submit_bio+0x50/0xc0
    [   76.982232]  submit_bh_wbc+0x15e/0x230
    [   76.982238]  submit_bh+0x14/0x20
    [   76.982241]  ext4_read_bh_nowait+0xc5/0x130
    [   76.982247]  ext4_read_block_bitmap_nowait+0x340/0xc60
    [   76.982254]  ext4_mb_init_cache+0x1ce/0xdc0
    [   76.982259]  ext4_mb_load_buddy_gfp+0x987/0xfa0
    [   76.982263]  ext4_discard_preallocations+0x45d/0x830
    [   76.982274]  ext4_clear_inode+0x48/0xf0
    [   76.982280]  ext4_evict_inode+0xcf/0xc70
    [   76.982285]  evict+0x119/0x2b0
    [   76.982290]  dispose_list+0x43/0xa0
    [   76.982294]  prune_icache_sb+0x64/0x90
    [   76.982298]  super_cache_scan+0x155/0x210
    [   76.982303]  do_shrink_slab+0x19e/0x4e0
    [   76.982310]  shrink_slab+0x2bd/0x450
    [   76.982317]  drop_slab+0xcc/0x1a0
    [   76.982323]  drop_caches_sysctl_handler+0xb7/0xe0
    [   76.982327]  proc_sys_call_handler+0x1bc/0x300
    [   76.982331]  proc_sys_write+0x17/0x20
    [   76.982334]  vfs_write+0x3d3/0x570
    [   76.982342]  ksys_write+0x73/0x160
    [   76.982347]  __x64_sys_write+0x1e/0x30
    [   76.982352]  do_syscall_64+0x35/0x80
    [   76.982357]  entry_SYSCALL_64_after_hwframe+0x63/0xcd
    
    Function metadata_operation_failed() is called when operations failed
    on dm pool metadata, dm pool will destroy and recreate metadata. So,
    shrinker will be unregistered and registered, which could down write
    shrinker_rwsem under pmd_write_lock.
    
    Fix it by allocating dm_block_manager before locking pmd->root_lock
    and destroying old dm_block_manager after unlocking pmd->root_lock,
    then old dm_block_manager is replaced with new dm_block_manager under
    pmd->root_lock. So, shrinker register/unregister could be done without
    holding pmd->root_lock.
    
    Fetch a reproducer in [Link].
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=216676
    Cc: stable@vger.kernel.org #v5.2+
    Fixes: e49e582965b3 ("dm thin: add read only and fail io modes")
    Signed-off-by: Zhihao Cheng <chengzhihao1@huawei.com>
    Signed-off-by: Mike Snitzer <snitzer@kernel.org>

commit a7b42969d63f47320853a802efd879fbdc4e010e
Author: Xin Long <lucien.xin@gmail.com>
Date:   Fri Nov 18 16:45:01 2022 -0500

    tipc: add an extra conn_get in tipc_conn_alloc
    
    One extra conn_get() is needed in tipc_conn_alloc(), as after
    tipc_conn_alloc() is called, tipc_conn_close() may free this
    con before deferencing it in tipc_topsrv_accept():
    
       tipc_conn_alloc();
       newsk = newsock->sk;
                                     <---- tipc_conn_close();
       write_lock_bh(&sk->sk_callback_lock);
       newsk->sk_data_ready = tipc_conn_data_ready;
    
    Then an uaf issue can be triggered:
    
      BUG: KASAN: use-after-free in tipc_topsrv_accept+0x1e7/0x370 [tipc]
      Call Trace:
       <TASK>
       dump_stack_lvl+0x33/0x46
       print_report+0x178/0x4b0
       kasan_report+0x8c/0x100
       kasan_check_range+0x179/0x1e0
       tipc_topsrv_accept+0x1e7/0x370 [tipc]
       process_one_work+0x6a3/0x1030
       worker_thread+0x8a/0xdf0
    
    This patch fixes it by holding it in tipc_conn_alloc(), then after
    all accessing in tipc_topsrv_accept() releasing it. Note when does
    this in tipc_topsrv_kern_subscr(), as tipc_conn_rcv_sub() returns
    0 or -1 only, we don't need to check for "> 0".
    
    Fixes: c5fa7b3cf3cb ("tipc: introduce new TIPC server infrastructure")
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Acked-by: Jon Maloy <jmaloy@redhat.com>
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>

commit ecbd95958c484780afa6f71b73c3dbdc647cc6ca
Author: Zhang Qilong <zhangqilong3@huawei.com>
Date:   Mon Sep 5 12:59:17 2022 +0800

    f2fs: fix race condition on setting FI_NO_EXTENT flag
    
    [ Upstream commit 07725adc55c0a414c10acb5c8c86cea34b95ddef ]
    
    The following scenarios exist.
    process A:               process B:
    ->f2fs_drop_extent_tree  ->f2fs_update_extent_cache_range
                              ->f2fs_update_extent_tree_range
                               ->write_lock
     ->set_inode_flag
                               ->is_inode_flag_set
                               ->__free_extent_tree // Shouldn't
                                                    // have been
                                                    // cleaned up
                                                    // here
      ->write_lock
    
    In this case, the "FI_NO_EXTENT" flag is set between
    f2fs_update_extent_tree_range and is_inode_flag_set
    by other process. it leads to clearing the whole exten
    tree which should not have happened. And we fix it by
    move the setting it to the range of write_lock.
    
    Fixes:5f281fab9b9a3 ("f2fs: disable extent_cache for fcollapse/finsert inodes")
    Signed-off-by: Zhang Qilong <zhangqilong3@huawei.com>
    Reviewed-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit c5ed3a378978feb5e96242f882b993dcde63ecd3
Author: Zhang Qilong <zhangqilong3@huawei.com>
Date:   Mon Sep 5 12:59:17 2022 +0800

    f2fs: fix race condition on setting FI_NO_EXTENT flag
    
    [ Upstream commit 07725adc55c0a414c10acb5c8c86cea34b95ddef ]
    
    The following scenarios exist.
    process A:               process B:
    ->f2fs_drop_extent_tree  ->f2fs_update_extent_cache_range
                              ->f2fs_update_extent_tree_range
                               ->write_lock
     ->set_inode_flag
                               ->is_inode_flag_set
                               ->__free_extent_tree // Shouldn't
                                                    // have been
                                                    // cleaned up
                                                    // here
      ->write_lock
    
    In this case, the "FI_NO_EXTENT" flag is set between
    f2fs_update_extent_tree_range and is_inode_flag_set
    by other process. it leads to clearing the whole exten
    tree which should not have happened. And we fix it by
    move the setting it to the range of write_lock.
    
    Fixes:5f281fab9b9a3 ("f2fs: disable extent_cache for fcollapse/finsert inodes")
    Signed-off-by: Zhang Qilong <zhangqilong3@huawei.com>
    Reviewed-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d967df65e19de75a1acea67aec5f9a347079699f
Author: Zhang Qilong <zhangqilong3@huawei.com>
Date:   Mon Sep 5 12:59:17 2022 +0800

    f2fs: fix race condition on setting FI_NO_EXTENT flag
    
    [ Upstream commit 07725adc55c0a414c10acb5c8c86cea34b95ddef ]
    
    The following scenarios exist.
    process A:               process B:
    ->f2fs_drop_extent_tree  ->f2fs_update_extent_cache_range
                              ->f2fs_update_extent_tree_range
                               ->write_lock
     ->set_inode_flag
                               ->is_inode_flag_set
                               ->__free_extent_tree // Shouldn't
                                                    // have been
                                                    // cleaned up
                                                    // here
      ->write_lock
    
    In this case, the "FI_NO_EXTENT" flag is set between
    f2fs_update_extent_tree_range and is_inode_flag_set
    by other process. it leads to clearing the whole exten
    tree which should not have happened. And we fix it by
    move the setting it to the range of write_lock.
    
    Fixes:5f281fab9b9a3 ("f2fs: disable extent_cache for fcollapse/finsert inodes")
    Signed-off-by: Zhang Qilong <zhangqilong3@huawei.com>
    Reviewed-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2c7772b3f2d50c993e93ccb96fdf0feed9c121b3
Author: Zhang Qilong <zhangqilong3@huawei.com>
Date:   Mon Sep 5 12:59:17 2022 +0800

    f2fs: fix race condition on setting FI_NO_EXTENT flag
    
    [ Upstream commit 07725adc55c0a414c10acb5c8c86cea34b95ddef ]
    
    The following scenarios exist.
    process A:               process B:
    ->f2fs_drop_extent_tree  ->f2fs_update_extent_cache_range
                              ->f2fs_update_extent_tree_range
                               ->write_lock
     ->set_inode_flag
                               ->is_inode_flag_set
                               ->__free_extent_tree // Shouldn't
                                                    // have been
                                                    // cleaned up
                                                    // here
      ->write_lock
    
    In this case, the "FI_NO_EXTENT" flag is set between
    f2fs_update_extent_tree_range and is_inode_flag_set
    by other process. it leads to clearing the whole exten
    tree which should not have happened. And we fix it by
    move the setting it to the range of write_lock.
    
    Fixes:5f281fab9b9a3 ("f2fs: disable extent_cache for fcollapse/finsert inodes")
    Signed-off-by: Zhang Qilong <zhangqilong3@huawei.com>
    Reviewed-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 7534a90f679c68e9e229be109f13abece019e840
Author: Zhang Qilong <zhangqilong3@huawei.com>
Date:   Mon Sep 5 12:59:17 2022 +0800

    f2fs: fix race condition on setting FI_NO_EXTENT flag
    
    [ Upstream commit 07725adc55c0a414c10acb5c8c86cea34b95ddef ]
    
    The following scenarios exist.
    process A:               process B:
    ->f2fs_drop_extent_tree  ->f2fs_update_extent_cache_range
                              ->f2fs_update_extent_tree_range
                               ->write_lock
     ->set_inode_flag
                               ->is_inode_flag_set
                               ->__free_extent_tree // Shouldn't
                                                    // have been
                                                    // cleaned up
                                                    // here
      ->write_lock
    
    In this case, the "FI_NO_EXTENT" flag is set between
    f2fs_update_extent_tree_range and is_inode_flag_set
    by other process. it leads to clearing the whole exten
    tree which should not have happened. And we fix it by
    move the setting it to the range of write_lock.
    
    Fixes:5f281fab9b9a3 ("f2fs: disable extent_cache for fcollapse/finsert inodes")
    Signed-off-by: Zhang Qilong <zhangqilong3@huawei.com>
    Reviewed-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit fb1dcc2a9e4b189b0c74031cb110c1e3b6de39b8
Author: Zhang Qilong <zhangqilong3@huawei.com>
Date:   Mon Sep 5 12:59:17 2022 +0800

    f2fs: fix race condition on setting FI_NO_EXTENT flag
    
    [ Upstream commit 07725adc55c0a414c10acb5c8c86cea34b95ddef ]
    
    The following scenarios exist.
    process A:               process B:
    ->f2fs_drop_extent_tree  ->f2fs_update_extent_cache_range
                              ->f2fs_update_extent_tree_range
                               ->write_lock
     ->set_inode_flag
                               ->is_inode_flag_set
                               ->__free_extent_tree // Shouldn't
                                                    // have been
                                                    // cleaned up
                                                    // here
      ->write_lock
    
    In this case, the "FI_NO_EXTENT" flag is set between
    f2fs_update_extent_tree_range and is_inode_flag_set
    by other process. it leads to clearing the whole exten
    tree which should not have happened. And we fix it by
    move the setting it to the range of write_lock.
    
    Fixes:5f281fab9b9a3 ("f2fs: disable extent_cache for fcollapse/finsert inodes")
    Signed-off-by: Zhang Qilong <zhangqilong3@huawei.com>
    Reviewed-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 1f4d5d31ddfffbbdb24d8b34b012b528e241a1ea
Author: Zhang Qilong <zhangqilong3@huawei.com>
Date:   Mon Sep 5 12:59:17 2022 +0800

    f2fs: fix race condition on setting FI_NO_EXTENT flag
    
    [ Upstream commit 07725adc55c0a414c10acb5c8c86cea34b95ddef ]
    
    The following scenarios exist.
    process A:               process B:
    ->f2fs_drop_extent_tree  ->f2fs_update_extent_cache_range
                              ->f2fs_update_extent_tree_range
                               ->write_lock
     ->set_inode_flag
                               ->is_inode_flag_set
                               ->__free_extent_tree // Shouldn't
                                                    // have been
                                                    // cleaned up
                                                    // here
      ->write_lock
    
    In this case, the "FI_NO_EXTENT" flag is set between
    f2fs_update_extent_tree_range and is_inode_flag_set
    by other process. it leads to clearing the whole exten
    tree which should not have happened. And we fix it by
    move the setting it to the range of write_lock.
    
    Fixes:5f281fab9b9a3 ("f2fs: disable extent_cache for fcollapse/finsert inodes")
    Signed-off-by: Zhang Qilong <zhangqilong3@huawei.com>
    Reviewed-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 3cf47552b2b9284979ce16cd5c81d2123d2c09f3
Author: Zhang Qilong <zhangqilong3@huawei.com>
Date:   Mon Sep 5 12:59:17 2022 +0800

    f2fs: fix race condition on setting FI_NO_EXTENT flag
    
    [ Upstream commit 07725adc55c0a414c10acb5c8c86cea34b95ddef ]
    
    The following scenarios exist.
    process A:               process B:
    ->f2fs_drop_extent_tree  ->f2fs_update_extent_cache_range
                              ->f2fs_update_extent_tree_range
                               ->write_lock
     ->set_inode_flag
                               ->is_inode_flag_set
                               ->__free_extent_tree // Shouldn't
                                                    // have been
                                                    // cleaned up
                                                    // here
      ->write_lock
    
    In this case, the "FI_NO_EXTENT" flag is set between
    f2fs_update_extent_tree_range and is_inode_flag_set
    by other process. it leads to clearing the whole exten
    tree which should not have happened. And we fix it by
    move the setting it to the range of write_lock.
    
    Fixes:5f281fab9b9a3 ("f2fs: disable extent_cache for fcollapse/finsert inodes")
    Signed-off-by: Zhang Qilong <zhangqilong3@huawei.com>
    Reviewed-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 07725adc55c0a414c10acb5c8c86cea34b95ddef
Author: Zhang Qilong <zhangqilong3@huawei.com>
Date:   Mon Sep 5 12:59:17 2022 +0800

    f2fs: fix race condition on setting FI_NO_EXTENT flag
    
    The following scenarios exist.
    process A:               process B:
    ->f2fs_drop_extent_tree  ->f2fs_update_extent_cache_range
                              ->f2fs_update_extent_tree_range
                               ->write_lock
     ->set_inode_flag
                               ->is_inode_flag_set
                               ->__free_extent_tree // Shouldn't
                                                    // have been
                                                    // cleaned up
                                                    // here
      ->write_lock
    
    In this case, the "FI_NO_EXTENT" flag is set between
    f2fs_update_extent_tree_range and is_inode_flag_set
    by other process. it leads to clearing the whole exten
    tree which should not have happened. And we fix it by
    move the setting it to the range of write_lock.
    
    Fixes:5f281fab9b9a3 ("f2fs: disable extent_cache for fcollapse/finsert inodes")
    Signed-off-by: Zhang Qilong <zhangqilong3@huawei.com>
    Reviewed-by: Chao Yu <chao@kernel.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

commit eb75efdec8dd0f01ac85c88feafa6e63b34a2521
Author: Fedor Pchelkin <pchelkin@ispras.ru>
Date:   Tue Sep 6 21:22:12 2022 +0300

    tty: n_gsm: avoid call of sleeping functions from atomic context
    
    commit 902e02ea9385373ce4b142576eef41c642703955 upstream.
    
    Syzkaller reports the following problem:
    
    BUG: sleeping function called from invalid context at kernel/printk/printk.c:2347
    in_atomic(): 1, irqs_disabled(): 1, non_block: 0, pid: 1105, name: syz-executor423
    3 locks held by syz-executor423/1105:
     #0: ffff8881468b9098 (&tty->ldisc_sem){++++}-{0:0}, at: tty_ldisc_ref_wait+0x22/0x90 drivers/tty/tty_ldisc.c:266
     #1: ffff8881468b9130 (&tty->atomic_write_lock){+.+.}-{3:3}, at: tty_write_lock drivers/tty/tty_io.c:952 [inline]
     #1: ffff8881468b9130 (&tty->atomic_write_lock){+.+.}-{3:3}, at: do_tty_write drivers/tty/tty_io.c:975 [inline]
     #1: ffff8881468b9130 (&tty->atomic_write_lock){+.+.}-{3:3}, at: file_tty_write.constprop.0+0x2a8/0x8e0 drivers/tty/tty_io.c:1118
     #2: ffff88801b06c398 (&gsm->tx_lock){....}-{2:2}, at: gsmld_write+0x5e/0x150 drivers/tty/n_gsm.c:2717
    irq event stamp: 3482
    hardirqs last  enabled at (3481): [<ffffffff81d13343>] __get_reqs_available+0x143/0x2f0 fs/aio.c:946
    hardirqs last disabled at (3482): [<ffffffff87d39722>] __raw_spin_lock_irqsave include/linux/spinlock_api_smp.h:108 [inline]
    hardirqs last disabled at (3482): [<ffffffff87d39722>] _raw_spin_lock_irqsave+0x52/0x60 kernel/locking/spinlock.c:159
    softirqs last  enabled at (3408): [<ffffffff87e01002>] asm_call_irq_on_stack+0x12/0x20
    softirqs last disabled at (3401): [<ffffffff87e01002>] asm_call_irq_on_stack+0x12/0x20
    Preemption disabled at:
    [<0000000000000000>] 0x0
    CPU: 2 PID: 1105 Comm: syz-executor423 Not tainted 5.10.137-syzkaller #0
    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-1 04/01/2014
    Call Trace:
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x107/0x167 lib/dump_stack.c:118
     ___might_sleep.cold+0x1e8/0x22e kernel/sched/core.c:7304
     console_lock+0x19/0x80 kernel/printk/printk.c:2347
     do_con_write+0x113/0x1de0 drivers/tty/vt/vt.c:2909
     con_write+0x22/0xc0 drivers/tty/vt/vt.c:3296
     gsmld_write+0xd0/0x150 drivers/tty/n_gsm.c:2720
     do_tty_write drivers/tty/tty_io.c:1028 [inline]
     file_tty_write.constprop.0+0x502/0x8e0 drivers/tty/tty_io.c:1118
     call_write_iter include/linux/fs.h:1903 [inline]
     aio_write+0x355/0x7b0 fs/aio.c:1580
     __io_submit_one fs/aio.c:1952 [inline]
     io_submit_one+0xf45/0x1a90 fs/aio.c:1999
     __do_sys_io_submit fs/aio.c:2058 [inline]
     __se_sys_io_submit fs/aio.c:2028 [inline]
     __x64_sys_io_submit+0x18c/0x2f0 fs/aio.c:2028
     do_syscall_64+0x33/0x40 arch/x86/entry/common.c:46
     entry_SYSCALL_64_after_hwframe+0x61/0xc6
    
    The problem happens in the following control flow:
    
    gsmld_write(...)
    spin_lock_irqsave(&gsm->tx_lock, flags) // taken a spinlock on TX data
     con_write(...)
      do_con_write(...)
       console_lock()
        might_sleep() // -> bug
    
    As far as console_lock() might sleep it should not be called with
    spinlock held.
    
    The patch replaces tx_lock spinlock with mutex in order to avoid the
    problem.
    
    Found by Linux Verification Center (linuxtesting.org) with Syzkaller.
    
    Fixes: 32dd59f ("tty: n_gsm: fix race condition in gsmld_write()")
    Cc: stable <stable@kernel.org>
    Signed-off-by: Fedor Pchelkin <pchelkin@ispras.ru>
    Signed-off-by: Alexey Khoroshilov <khoroshilov@ispras.ru>
    Link: https://lore.kernel.org/r/20220829131640.69254-3-pchelkin@ispras.ru
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 12c1dc8e7441773c74dc62fab76553c24015f6e1
Author: Abel Wu <wuyun.abel@bytedance.com>
Date:   Thu Aug 11 20:41:57 2022 +0800

    mm/mempolicy: fix lock contention on mems_allowed
    
    The mems_allowed field can be modified by other tasks, so it isn't safe to
    access it with alloc_lock unlocked even in the current process context.
    
    Say there are two tasks: A from cpusetA is performing set_mempolicy(2),
    and B is changing cpusetA's cpuset.mems:
    
      A (set_mempolicy)             B (echo xx > cpuset.mems)
      -------------------------------------------------------
      pol = mpol_new();
                                    update_tasks_nodemask(cpusetA) {
                                      foreach t in cpusetA {
                                        cpuset_change_task_nodemask(t) {
      mpol_set_nodemask(pol) {
                                          task_lock(t); // t could be A
        new = f(A->mems_allowed);
                                          update t->mems_allowed;
        pol.create(pol, new);
                                          task_unlock(t);
      }
                                        }
                                      }
                                    }
      task_lock(A);
      A->mempolicy = pol;
      task_unlock(A);
    
    In this case A's pol->nodes is computed by old mems_allowed, and could
    be inconsistent with A's new mems_allowed.
    
    While it is different when replacing vmas' policy: the pol->nodes is
    gone wild only when current_cpuset_is_being_rebound():
    
      A (mbind)                     B (echo xx > cpuset.mems)
      -------------------------------------------------------
      pol = mpol_new();
      mmap_write_lock(A->mm);
                                    cpuset_being_rebound = cpusetA;
                                    update_tasks_nodemask(cpusetA) {
                                      foreach t in cpusetA {
                                        cpuset_change_task_nodemask(t) {
      mpol_set_nodemask(pol) {
                                          task_lock(t); // t could be A
        mask = f(A->mems_allowed);
                                          update t->mems_allowed;
        pol.create(pol, mask);
                                          task_unlock(t);
      }
                                        }
      foreach v in A->mm {
        if (cpuset_being_rebound == cpusetA)
          pol.rebind(pol, cpuset.mems);
        v->vma_policy = pol;
      }
      mmap_write_unlock(A->mm);
                                        mmap_write_lock(t->mm);
                                        mpol_rebind_mm(t->mm);
                                        mmap_write_unlock(t->mm);
                                      }
                                    }
                                    cpuset_being_rebound = NULL;
    
    In this case, the cpuset.mems, which has already done updating, is finally
    used for calculating pol->nodes, rather than A->mems_allowed.  So it is OK
    to call mpol_set_nodemask() with alloc_lock unlocked when doing mbind(2).
    
    Link: https://lkml.kernel.org/r/20220811124157.74888-1-wuyun.abel@bytedance.com
    Fixes: 78b132e9bae9 ("mm/mempolicy: remove or narrow the lock on current")
    Signed-off-by: Abel Wu <wuyun.abel@bytedance.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Wei Yang <richard.weiyang@gmail.com>
    Reviewed-by: Muchun Song <songmuchun@bytedance.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit 132331c1f605eb5911795a6b9115114575594d0a
Author: Fedor Pchelkin <pchelkin@ispras.ru>
Date:   Mon Aug 29 16:16:40 2022 +0300

    tty: n_gsm: avoid call of sleeping functions from atomic context
    
    commit 902e02ea9385373ce4b142576eef41c642703955 upstream.
    
    Syzkaller reports the following problem:
    
    BUG: sleeping function called from invalid context at kernel/printk/printk.c:2347
    in_atomic(): 1, irqs_disabled(): 1, non_block: 0, pid: 1105, name: syz-executor423
    3 locks held by syz-executor423/1105:
     #0: ffff8881468b9098 (&tty->ldisc_sem){++++}-{0:0}, at: tty_ldisc_ref_wait+0x22/0x90 drivers/tty/tty_ldisc.c:266
     #1: ffff8881468b9130 (&tty->atomic_write_lock){+.+.}-{3:3}, at: tty_write_lock drivers/tty/tty_io.c:952 [inline]
     #1: ffff8881468b9130 (&tty->atomic_write_lock){+.+.}-{3:3}, at: do_tty_write drivers/tty/tty_io.c:975 [inline]
     #1: ffff8881468b9130 (&tty->atomic_write_lock){+.+.}-{3:3}, at: file_tty_write.constprop.0+0x2a8/0x8e0 drivers/tty/tty_io.c:1118
     #2: ffff88801b06c398 (&gsm->tx_lock){....}-{2:2}, at: gsmld_write+0x5e/0x150 drivers/tty/n_gsm.c:2717
    irq event stamp: 3482
    hardirqs last  enabled at (3481): [<ffffffff81d13343>] __get_reqs_available+0x143/0x2f0 fs/aio.c:946
    hardirqs last disabled at (3482): [<ffffffff87d39722>] __raw_spin_lock_irqsave include/linux/spinlock_api_smp.h:108 [inline]
    hardirqs last disabled at (3482): [<ffffffff87d39722>] _raw_spin_lock_irqsave+0x52/0x60 kernel/locking/spinlock.c:159
    softirqs last  enabled at (3408): [<ffffffff87e01002>] asm_call_irq_on_stack+0x12/0x20
    softirqs last disabled at (3401): [<ffffffff87e01002>] asm_call_irq_on_stack+0x12/0x20
    Preemption disabled at:
    [<0000000000000000>] 0x0
    CPU: 2 PID: 1105 Comm: syz-executor423 Not tainted 5.10.137-syzkaller #0
    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-1 04/01/2014
    Call Trace:
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x107/0x167 lib/dump_stack.c:118
     ___might_sleep.cold+0x1e8/0x22e kernel/sched/core.c:7304
     console_lock+0x19/0x80 kernel/printk/printk.c:2347
     do_con_write+0x113/0x1de0 drivers/tty/vt/vt.c:2909
     con_write+0x22/0xc0 drivers/tty/vt/vt.c:3296
     gsmld_write+0xd0/0x150 drivers/tty/n_gsm.c:2720
     do_tty_write drivers/tty/tty_io.c:1028 [inline]
     file_tty_write.constprop.0+0x502/0x8e0 drivers/tty/tty_io.c:1118
     call_write_iter include/linux/fs.h:1903 [inline]
     aio_write+0x355/0x7b0 fs/aio.c:1580
     __io_submit_one fs/aio.c:1952 [inline]
     io_submit_one+0xf45/0x1a90 fs/aio.c:1999
     __do_sys_io_submit fs/aio.c:2058 [inline]
     __se_sys_io_submit fs/aio.c:2028 [inline]
     __x64_sys_io_submit+0x18c/0x2f0 fs/aio.c:2028
     do_syscall_64+0x33/0x40 arch/x86/entry/common.c:46
     entry_SYSCALL_64_after_hwframe+0x61/0xc6
    
    The problem happens in the following control flow:
    
    gsmld_write(...)
    spin_lock_irqsave(&gsm->tx_lock, flags) // taken a spinlock on TX data
     con_write(...)
      do_con_write(...)
       console_lock()
        might_sleep() // -> bug
    
    As far as console_lock() might sleep it should not be called with
    spinlock held.
    
    The patch replaces tx_lock spinlock with mutex in order to avoid the
    problem.
    
    Found by Linux Verification Center (linuxtesting.org) with Syzkaller.
    
    Fixes: 32dd59f96924 ("tty: n_gsm: fix race condition in gsmld_write()")
    Cc: stable <stable@kernel.org>
    Signed-off-by: Fedor Pchelkin <pchelkin@ispras.ru>
    Signed-off-by: Alexey Khoroshilov <khoroshilov@ispras.ru>
    Link: https://lore.kernel.org/r/20220829131640.69254-3-pchelkin@ispras.ru
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 357321557920c805de2b14832002465c320eea4f
Author: Waiman Long <longman@redhat.com>
Date:   Fri Aug 12 14:30:33 2022 -0400

    mm/slab_common: Deleting kobject in kmem_cache_destroy() without holding slab_mutex/cpu_hotplug_lock
    
    [ Upstream commit 0495e337b7039191dfce6e03f5f830454b1fae6b ]
    
    A circular locking problem is reported by lockdep due to the following
    circular locking dependency.
    
      +--> cpu_hotplug_lock --> slab_mutex --> kn->active --+
      |                                                     |
      +-----------------------------------------------------+
    
    The forward cpu_hotplug_lock ==> slab_mutex ==> kn->active dependency
    happens in
    
      kmem_cache_destroy(): cpus_read_lock(); mutex_lock(&slab_mutex);
      ==> sysfs_slab_unlink()
          ==> kobject_del()
              ==> kernfs_remove()
                  ==> __kernfs_remove()
                      ==> kernfs_drain(): rwsem_acquire(&kn->dep_map, ...);
    
    The backward kn->active ==> cpu_hotplug_lock dependency happens in
    
      kernfs_fop_write_iter(): kernfs_get_active();
      ==> slab_attr_store()
          ==> cpu_partial_store()
              ==> flush_all(): cpus_read_lock()
    
    One way to break this circular locking chain is to avoid holding
    cpu_hotplug_lock and slab_mutex while deleting the kobject in
    sysfs_slab_unlink() which should be equivalent to doing a write_lock
    and write_unlock pair of the kn->active virtual lock.
    
    Since the kobject structures are not protected by slab_mutex or the
    cpu_hotplug_lock, we can certainly release those locks before doing
    the delete operation.
    
    Move sysfs_slab_unlink() and sysfs_slab_release() to the newly
    created kmem_cache_release() and call it outside the slab_mutex &
    cpu_hotplug_lock critical sections. There will be a slight delay
    in the deletion of sysfs files if kmem_cache_release() is called
    indirectly from a work function.
    
    Fixes: 5a836bf6b09f ("mm: slub: move flush_cpu_slab() invocations __free_slab() invocations out of IRQ context")
    Signed-off-by: Waiman Long <longman@redhat.com>
    Reviewed-by: Hyeonggon Yoo <42.hyeyoo@gmail.com>
    Reviewed-by: Roman Gushchin <roman.gushchin@linux.dev>
    Acked-by: David Rientjes <rientjes@google.com>
    Link: https://lore.kernel.org/all/YwOImVd+nRUsSAga@hyeyoo/
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 0495e337b7039191dfce6e03f5f830454b1fae6b
Author: Waiman Long <longman@redhat.com>
Date:   Fri Aug 12 14:30:33 2022 -0400

    mm/slab_common: Deleting kobject in kmem_cache_destroy() without holding slab_mutex/cpu_hotplug_lock
    
    A circular locking problem is reported by lockdep due to the following
    circular locking dependency.
    
      +--> cpu_hotplug_lock --> slab_mutex --> kn->active --+
      |                                                     |
      +-----------------------------------------------------+
    
    The forward cpu_hotplug_lock ==> slab_mutex ==> kn->active dependency
    happens in
    
      kmem_cache_destroy(): cpus_read_lock(); mutex_lock(&slab_mutex);
      ==> sysfs_slab_unlink()
          ==> kobject_del()
              ==> kernfs_remove()
                  ==> __kernfs_remove()
                      ==> kernfs_drain(): rwsem_acquire(&kn->dep_map, ...);
    
    The backward kn->active ==> cpu_hotplug_lock dependency happens in
    
      kernfs_fop_write_iter(): kernfs_get_active();
      ==> slab_attr_store()
          ==> cpu_partial_store()
              ==> flush_all(): cpus_read_lock()
    
    One way to break this circular locking chain is to avoid holding
    cpu_hotplug_lock and slab_mutex while deleting the kobject in
    sysfs_slab_unlink() which should be equivalent to doing a write_lock
    and write_unlock pair of the kn->active virtual lock.
    
    Since the kobject structures are not protected by slab_mutex or the
    cpu_hotplug_lock, we can certainly release those locks before doing
    the delete operation.
    
    Move sysfs_slab_unlink() and sysfs_slab_release() to the newly
    created kmem_cache_release() and call it outside the slab_mutex &
    cpu_hotplug_lock critical sections. There will be a slight delay
    in the deletion of sysfs files if kmem_cache_release() is called
    indirectly from a work function.
    
    Fixes: 5a836bf6b09f ("mm: slub: move flush_cpu_slab() invocations __free_slab() invocations out of IRQ context")
    Signed-off-by: Waiman Long <longman@redhat.com>
    Reviewed-by: Hyeonggon Yoo <42.hyeyoo@gmail.com>
    Reviewed-by: Roman Gushchin <roman.gushchin@linux.dev>
    Acked-by: David Rientjes <rientjes@google.com>
    Link: https://lore.kernel.org/all/YwOImVd+nRUsSAga@hyeyoo/
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>

commit 1aa262c1d056551dd1246115af8b7e351184deae
Author: Naohiro Aota <naohiro.aota@wdc.com>
Date:   Mon Aug 22 15:07:03 2022 +0900

    btrfs: replace BTRFS_MAX_EXTENT_SIZE with fs_info->max_extent_size
    
    commit f7b12a62f008a3041f42f2426983e59a6a0a3c59 upstream
    
    On zoned filesystem, data write out is limited by max_zone_append_size,
    and a large ordered extent is split according the size of a bio. OTOH,
    the number of extents to be written is calculated using
    BTRFS_MAX_EXTENT_SIZE, and that estimated number is used to reserve the
    metadata bytes to update and/or create the metadata items.
    
    The metadata reservation is done at e.g, btrfs_buffered_write() and then
    released according to the estimation changes. Thus, if the number of extent
    increases massively, the reserved metadata can run out.
    
    The increase of the number of extents easily occurs on zoned filesystem
    if BTRFS_MAX_EXTENT_SIZE > max_zone_append_size. And, it causes the
    following warning on a small RAM environment with disabling metadata
    over-commit (in the following patch).
    
    [75721.498492] ------------[ cut here ]------------
    [75721.505624] BTRFS: block rsv 1 returned -28
    [75721.512230] WARNING: CPU: 24 PID: 2327559 at fs/btrfs/block-rsv.c:537 btrfs_use_block_rsv+0x560/0x760 [btrfs]
    [75721.581854] CPU: 24 PID: 2327559 Comm: kworker/u64:10 Kdump: loaded Tainted: G        W         5.18.0-rc2-BTRFS-ZNS+ #109
    [75721.597200] Hardware name: Supermicro Super Server/H12SSL-NT, BIOS 2.0 02/22/2021
    [75721.607310] Workqueue: btrfs-endio-write btrfs_work_helper [btrfs]
    [75721.616209] RIP: 0010:btrfs_use_block_rsv+0x560/0x760 [btrfs]
    [75721.646649] RSP: 0018:ffffc9000fbdf3e0 EFLAGS: 00010286
    [75721.654126] RAX: 0000000000000000 RBX: 0000000000004000 RCX: 0000000000000000
    [75721.663524] RDX: 0000000000000004 RSI: 0000000000000008 RDI: fffff52001f7be6e
    [75721.672921] RBP: ffffc9000fbdf420 R08: 0000000000000001 R09: ffff889f8d1fc6c7
    [75721.682493] R10: ffffed13f1a3f8d8 R11: 0000000000000001 R12: ffff88980a3c0e28
    [75721.692284] R13: ffff889b66590000 R14: ffff88980a3c0e40 R15: ffff88980a3c0e8a
    [75721.701878] FS:  0000000000000000(0000) GS:ffff889f8d000000(0000) knlGS:0000000000000000
    [75721.712601] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [75721.720726] CR2: 000055d12e05c018 CR3: 0000800193594000 CR4: 0000000000350ee0
    [75721.730499] Call Trace:
    [75721.735166]  <TASK>
    [75721.739886]  btrfs_alloc_tree_block+0x1e1/0x1100 [btrfs]
    [75721.747545]  ? btrfs_alloc_logged_file_extent+0x550/0x550 [btrfs]
    [75721.756145]  ? btrfs_get_32+0xea/0x2d0 [btrfs]
    [75721.762852]  ? btrfs_get_32+0xea/0x2d0 [btrfs]
    [75721.769520]  ? push_leaf_left+0x420/0x620 [btrfs]
    [75721.776431]  ? memcpy+0x4e/0x60
    [75721.781931]  split_leaf+0x433/0x12d0 [btrfs]
    [75721.788392]  ? btrfs_get_token_32+0x580/0x580 [btrfs]
    [75721.795636]  ? push_for_double_split.isra.0+0x420/0x420 [btrfs]
    [75721.803759]  ? leaf_space_used+0x15d/0x1a0 [btrfs]
    [75721.811156]  btrfs_search_slot+0x1bc3/0x2790 [btrfs]
    [75721.818300]  ? lock_downgrade+0x7c0/0x7c0
    [75721.824411]  ? free_extent_buffer.part.0+0x107/0x200 [btrfs]
    [75721.832456]  ? split_leaf+0x12d0/0x12d0 [btrfs]
    [75721.839149]  ? free_extent_buffer.part.0+0x14f/0x200 [btrfs]
    [75721.846945]  ? free_extent_buffer+0x13/0x20 [btrfs]
    [75721.853960]  ? btrfs_release_path+0x4b/0x190 [btrfs]
    [75721.861429]  btrfs_csum_file_blocks+0x85c/0x1500 [btrfs]
    [75721.869313]  ? rcu_read_lock_sched_held+0x16/0x80
    [75721.876085]  ? lock_release+0x552/0xf80
    [75721.881957]  ? btrfs_del_csums+0x8c0/0x8c0 [btrfs]
    [75721.888886]  ? __kasan_check_write+0x14/0x20
    [75721.895152]  ? do_raw_read_unlock+0x44/0x80
    [75721.901323]  ? _raw_write_lock_irq+0x60/0x80
    [75721.907983]  ? btrfs_global_root+0xb9/0xe0 [btrfs]
    [75721.915166]  ? btrfs_csum_root+0x12b/0x180 [btrfs]
    [75721.921918]  ? btrfs_get_global_root+0x820/0x820 [btrfs]
    [75721.929166]  ? _raw_write_unlock+0x23/0x40
    [75721.935116]  ? unpin_extent_cache+0x1e3/0x390 [btrfs]
    [75721.942041]  btrfs_finish_ordered_io.isra.0+0xa0c/0x1dc0 [btrfs]
    [75721.949906]  ? try_to_wake_up+0x30/0x14a0
    [75721.955700]  ? btrfs_unlink_subvol+0xda0/0xda0 [btrfs]
    [75721.962661]  ? rcu_read_lock_sched_held+0x16/0x80
    [75721.969111]  ? lock_acquire+0x41b/0x4c0
    [75721.974982]  finish_ordered_fn+0x15/0x20 [btrfs]
    [75721.981639]  btrfs_work_helper+0x1af/0xa80 [btrfs]
    [75721.988184]  ? _raw_spin_unlock_irq+0x28/0x50
    [75721.994643]  process_one_work+0x815/0x1460
    [75722.000444]  ? pwq_dec_nr_in_flight+0x250/0x250
    [75722.006643]  ? do_raw_spin_trylock+0xbb/0x190
    [75722.013086]  worker_thread+0x59a/0xeb0
    [75722.018511]  kthread+0x2ac/0x360
    [75722.023428]  ? process_one_work+0x1460/0x1460
    [75722.029431]  ? kthread_complete_and_exit+0x30/0x30
    [75722.036044]  ret_from_fork+0x22/0x30
    [75722.041255]  </TASK>
    [75722.045047] irq event stamp: 0
    [75722.049703] hardirqs last  enabled at (0): [<0000000000000000>] 0x0
    [75722.057610] hardirqs last disabled at (0): [<ffffffff8118a94a>] copy_process+0x1c1a/0x66b0
    [75722.067533] softirqs last  enabled at (0): [<ffffffff8118a989>] copy_process+0x1c59/0x66b0
    [75722.077423] softirqs last disabled at (0): [<0000000000000000>] 0x0
    [75722.085335] ---[ end trace 0000000000000000 ]---
    
    To fix the estimation, we need to introduce fs_info->max_extent_size to
    replace BTRFS_MAX_EXTENT_SIZE, which allow setting the different size for
    regular vs zoned filesystem.
    
    Set fs_info->max_extent_size to BTRFS_MAX_EXTENT_SIZE by default. On zoned
    filesystem, it is set to fs_info->max_zone_append_size.
    
    CC: stable@vger.kernel.org # 5.12+
    Fixes: d8e3fb106f39 ("btrfs: zoned: use ZONE_APPEND write for zoned mode")
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Signed-off-by: Naohiro Aota <naohiro.aota@wdc.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 902e02ea9385373ce4b142576eef41c642703955
Author: Fedor Pchelkin <pchelkin@ispras.ru>
Date:   Mon Aug 29 16:16:40 2022 +0300

    tty: n_gsm: avoid call of sleeping functions from atomic context
    
    Syzkaller reports the following problem:
    
    BUG: sleeping function called from invalid context at kernel/printk/printk.c:2347
    in_atomic(): 1, irqs_disabled(): 1, non_block: 0, pid: 1105, name: syz-executor423
    3 locks held by syz-executor423/1105:
     #0: ffff8881468b9098 (&tty->ldisc_sem){++++}-{0:0}, at: tty_ldisc_ref_wait+0x22/0x90 drivers/tty/tty_ldisc.c:266
     #1: ffff8881468b9130 (&tty->atomic_write_lock){+.+.}-{3:3}, at: tty_write_lock drivers/tty/tty_io.c:952 [inline]
     #1: ffff8881468b9130 (&tty->atomic_write_lock){+.+.}-{3:3}, at: do_tty_write drivers/tty/tty_io.c:975 [inline]
     #1: ffff8881468b9130 (&tty->atomic_write_lock){+.+.}-{3:3}, at: file_tty_write.constprop.0+0x2a8/0x8e0 drivers/tty/tty_io.c:1118
     #2: ffff88801b06c398 (&gsm->tx_lock){....}-{2:2}, at: gsmld_write+0x5e/0x150 drivers/tty/n_gsm.c:2717
    irq event stamp: 3482
    hardirqs last  enabled at (3481): [<ffffffff81d13343>] __get_reqs_available+0x143/0x2f0 fs/aio.c:946
    hardirqs last disabled at (3482): [<ffffffff87d39722>] __raw_spin_lock_irqsave include/linux/spinlock_api_smp.h:108 [inline]
    hardirqs last disabled at (3482): [<ffffffff87d39722>] _raw_spin_lock_irqsave+0x52/0x60 kernel/locking/spinlock.c:159
    softirqs last  enabled at (3408): [<ffffffff87e01002>] asm_call_irq_on_stack+0x12/0x20
    softirqs last disabled at (3401): [<ffffffff87e01002>] asm_call_irq_on_stack+0x12/0x20
    Preemption disabled at:
    [<0000000000000000>] 0x0
    CPU: 2 PID: 1105 Comm: syz-executor423 Not tainted 5.10.137-syzkaller #0
    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-1 04/01/2014
    Call Trace:
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x107/0x167 lib/dump_stack.c:118
     ___might_sleep.cold+0x1e8/0x22e kernel/sched/core.c:7304
     console_lock+0x19/0x80 kernel/printk/printk.c:2347
     do_con_write+0x113/0x1de0 drivers/tty/vt/vt.c:2909
     con_write+0x22/0xc0 drivers/tty/vt/vt.c:3296
     gsmld_write+0xd0/0x150 drivers/tty/n_gsm.c:2720
     do_tty_write drivers/tty/tty_io.c:1028 [inline]
     file_tty_write.constprop.0+0x502/0x8e0 drivers/tty/tty_io.c:1118
     call_write_iter include/linux/fs.h:1903 [inline]
     aio_write+0x355/0x7b0 fs/aio.c:1580
     __io_submit_one fs/aio.c:1952 [inline]
     io_submit_one+0xf45/0x1a90 fs/aio.c:1999
     __do_sys_io_submit fs/aio.c:2058 [inline]
     __se_sys_io_submit fs/aio.c:2028 [inline]
     __x64_sys_io_submit+0x18c/0x2f0 fs/aio.c:2028
     do_syscall_64+0x33/0x40 arch/x86/entry/common.c:46
     entry_SYSCALL_64_after_hwframe+0x61/0xc6
    
    The problem happens in the following control flow:
    
    gsmld_write(...)
    spin_lock_irqsave(&gsm->tx_lock, flags) // taken a spinlock on TX data
     con_write(...)
      do_con_write(...)
       console_lock()
        might_sleep() // -> bug
    
    As far as console_lock() might sleep it should not be called with
    spinlock held.
    
    The patch replaces tx_lock spinlock with mutex in order to avoid the
    problem.
    
    Found by Linux Verification Center (linuxtesting.org) with Syzkaller.
    
    Fixes: 32dd59f96924 ("tty: n_gsm: fix race condition in gsmld_write()")
    Cc: stable <stable@kernel.org>
    Signed-off-by: Fedor Pchelkin <pchelkin@ispras.ru>
    Signed-off-by: Alexey Khoroshilov <khoroshilov@ispras.ru>
    Link: https://lore.kernel.org/r/20220829131640.69254-3-pchelkin@ispras.ru
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 01fe8a3f818e1074a9a95d624be4549ee7ea2b2b
Author: Marco Elver <elver@google.com>
Date:   Mon Aug 29 14:47:15 2022 +0200

    locking/percpu-rwsem: Add percpu_is_write_locked() and percpu_is_read_locked()
    
    Implement simple accessors to probe percpu-rwsem's locked state:
    percpu_is_write_locked(), percpu_is_read_locked().
    
    Signed-off-by: Marco Elver <elver@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Acked-by: Ian Rogers <irogers@google.com>
    Link: https://lore.kernel.org/r/20220829124719.675715-11-elver@google.com

commit 79e522101cf40735f1936a10312e17f937b8dcad
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Jul 7 18:10:15 2021 -0700

    mm/mremap: hold the rmap lock in write mode when moving page table entries.
    
    commit 97113eb39fa7972722ff490b947d8af023e1f6a2 upstream.
    
    To avoid a race between rmap walk and mremap, mremap does
    take_rmap_locks().  The lock was taken to ensure that rmap walk don't miss
    a page table entry due to PTE moves via move_pagetables().  The kernel
    does further optimization of this lock such that if we are going to find
    the newly added vma after the old vma, the rmap lock is not taken.  This
    is because rmap walk would find the vmas in the same order and if we don't
    find the page table attached to older vma we would find it with the new
    vma which we would iterate later.
    
    As explained in commit eb66ae030829 ("mremap: properly flush TLB before
    releasing the page") mremap is special in that it doesn't take ownership
    of the page.  The optimized version for PUD/PMD aligned mremap also
    doesn't hold the ptl lock.  This can result in stale TLB entries as show
    below.
    
    This patch updates the rmap locking requirement in mremap to handle the race condition
    explained below with optimized mremap::
    
    Optmized PMD move
    
        CPU 1                           CPU 2                                   CPU 3
    
        mremap(old_addr, new_addr)      page_shrinker/try_to_unmap_one
    
        mmap_write_lock_killable()
    
                                        addr = old_addr
                                        lock(pte_ptl)
        lock(pmd_ptl)
        pmd = *old_pmd
        pmd_clear(old_pmd)
        flush_tlb_range(old_addr)
    
        *new_pmd = pmd
                                                                                *new_addr = 10; and fills
                                                                                TLB with new addr
                                                                                and old pfn
    
        unlock(pmd_ptl)
                                        ptep_clear_flush()
                                        old pfn is free.
                                                                                Stale TLB entry
    
    Optimized PUD move also suffers from a similar race.  Both the above race
    condition can be fixed if we force mremap path to take rmap lock.
    
    Link: https://lkml.kernel.org/r/20210616045239.370802-7-aneesh.kumar@linux.ibm.com
    Fixes: 2c91bd4a4e2e ("mm: speed up mremap by 20x on large regions")
    Fixes: c49dd3401802 ("mm: speedup mremap on 1GB or larger regions")
    Link: https://lore.kernel.org/linux-mm/CAHk-=wgXVR04eBNtxQfevontWnP6FDm+oj5vauQXP3S-huwbPw@mail.gmail.com
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Christophe Leroy <christophe.leroy@csgroup.eu>
    Cc: Joel Fernandes <joel@joelfernandes.org>
    Cc: Kalesh Singh <kaleshsingh@google.com>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    [patch rewritten for backport since the code was refactored since]
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2613baa3ab2153cc45b175c58700d93f72ef36c4
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Jul 7 18:10:15 2021 -0700

    mm/mremap: hold the rmap lock in write mode when moving page table entries.
    
    commit 97113eb39fa7972722ff490b947d8af023e1f6a2 upstream.
    
    To avoid a race between rmap walk and mremap, mremap does
    take_rmap_locks().  The lock was taken to ensure that rmap walk don't miss
    a page table entry due to PTE moves via move_pagetables().  The kernel
    does further optimization of this lock such that if we are going to find
    the newly added vma after the old vma, the rmap lock is not taken.  This
    is because rmap walk would find the vmas in the same order and if we don't
    find the page table attached to older vma we would find it with the new
    vma which we would iterate later.
    
    As explained in commit eb66ae030829 ("mremap: properly flush TLB before
    releasing the page") mremap is special in that it doesn't take ownership
    of the page.  The optimized version for PUD/PMD aligned mremap also
    doesn't hold the ptl lock.  This can result in stale TLB entries as show
    below.
    
    This patch updates the rmap locking requirement in mremap to handle the race condition
    explained below with optimized mremap::
    
    Optmized PMD move
    
        CPU 1                           CPU 2                                   CPU 3
    
        mremap(old_addr, new_addr)      page_shrinker/try_to_unmap_one
    
        mmap_write_lock_killable()
    
                                        addr = old_addr
                                        lock(pte_ptl)
        lock(pmd_ptl)
        pmd = *old_pmd
        pmd_clear(old_pmd)
        flush_tlb_range(old_addr)
    
        *new_pmd = pmd
                                                                                *new_addr = 10; and fills
                                                                                TLB with new addr
                                                                                and old pfn
    
        unlock(pmd_ptl)
                                        ptep_clear_flush()
                                        old pfn is free.
                                                                                Stale TLB entry
    
    Optimized PUD move also suffers from a similar race.  Both the above race
    condition can be fixed if we force mremap path to take rmap lock.
    
    Link: https://lkml.kernel.org/r/20210616045239.370802-7-aneesh.kumar@linux.ibm.com
    Fixes: 2c91bd4a4e2e ("mm: speed up mremap by 20x on large regions")
    Fixes: c49dd3401802 ("mm: speedup mremap on 1GB or larger regions")
    Link: https://lore.kernel.org/linux-mm/CAHk-=wgXVR04eBNtxQfevontWnP6FDm+oj5vauQXP3S-huwbPw@mail.gmail.com
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Christophe Leroy <christophe.leroy@csgroup.eu>
    Cc: Joel Fernandes <joel@joelfernandes.org>
    Cc: Kalesh Singh <kaleshsingh@google.com>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    [patch rewritten for backport since the code was refactored since]
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 096e8eb9639b342bc35f9b741cf05e26d0106e92
Author: Naohiro Aota <naohiro.aota@wdc.com>
Date:   Sat Jul 9 08:18:40 2022 +0900

    btrfs: replace BTRFS_MAX_EXTENT_SIZE with fs_info->max_extent_size
    
    [ Upstream commit f7b12a62f008a3041f42f2426983e59a6a0a3c59 ]
    
    On zoned filesystem, data write out is limited by max_zone_append_size,
    and a large ordered extent is split according the size of a bio. OTOH,
    the number of extents to be written is calculated using
    BTRFS_MAX_EXTENT_SIZE, and that estimated number is used to reserve the
    metadata bytes to update and/or create the metadata items.
    
    The metadata reservation is done at e.g, btrfs_buffered_write() and then
    released according to the estimation changes. Thus, if the number of extent
    increases massively, the reserved metadata can run out.
    
    The increase of the number of extents easily occurs on zoned filesystem
    if BTRFS_MAX_EXTENT_SIZE > max_zone_append_size. And, it causes the
    following warning on a small RAM environment with disabling metadata
    over-commit (in the following patch).
    
    [75721.498492] ------------[ cut here ]------------
    [75721.505624] BTRFS: block rsv 1 returned -28
    [75721.512230] WARNING: CPU: 24 PID: 2327559 at fs/btrfs/block-rsv.c:537 btrfs_use_block_rsv+0x560/0x760 [btrfs]
    [75721.581854] CPU: 24 PID: 2327559 Comm: kworker/u64:10 Kdump: loaded Tainted: G        W         5.18.0-rc2-BTRFS-ZNS+ #109
    [75721.597200] Hardware name: Supermicro Super Server/H12SSL-NT, BIOS 2.0 02/22/2021
    [75721.607310] Workqueue: btrfs-endio-write btrfs_work_helper [btrfs]
    [75721.616209] RIP: 0010:btrfs_use_block_rsv+0x560/0x760 [btrfs]
    [75721.646649] RSP: 0018:ffffc9000fbdf3e0 EFLAGS: 00010286
    [75721.654126] RAX: 0000000000000000 RBX: 0000000000004000 RCX: 0000000000000000
    [75721.663524] RDX: 0000000000000004 RSI: 0000000000000008 RDI: fffff52001f7be6e
    [75721.672921] RBP: ffffc9000fbdf420 R08: 0000000000000001 R09: ffff889f8d1fc6c7
    [75721.682493] R10: ffffed13f1a3f8d8 R11: 0000000000000001 R12: ffff88980a3c0e28
    [75721.692284] R13: ffff889b66590000 R14: ffff88980a3c0e40 R15: ffff88980a3c0e8a
    [75721.701878] FS:  0000000000000000(0000) GS:ffff889f8d000000(0000) knlGS:0000000000000000
    [75721.712601] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [75721.720726] CR2: 000055d12e05c018 CR3: 0000800193594000 CR4: 0000000000350ee0
    [75721.730499] Call Trace:
    [75721.735166]  <TASK>
    [75721.739886]  btrfs_alloc_tree_block+0x1e1/0x1100 [btrfs]
    [75721.747545]  ? btrfs_alloc_logged_file_extent+0x550/0x550 [btrfs]
    [75721.756145]  ? btrfs_get_32+0xea/0x2d0 [btrfs]
    [75721.762852]  ? btrfs_get_32+0xea/0x2d0 [btrfs]
    [75721.769520]  ? push_leaf_left+0x420/0x620 [btrfs]
    [75721.776431]  ? memcpy+0x4e/0x60
    [75721.781931]  split_leaf+0x433/0x12d0 [btrfs]
    [75721.788392]  ? btrfs_get_token_32+0x580/0x580 [btrfs]
    [75721.795636]  ? push_for_double_split.isra.0+0x420/0x420 [btrfs]
    [75721.803759]  ? leaf_space_used+0x15d/0x1a0 [btrfs]
    [75721.811156]  btrfs_search_slot+0x1bc3/0x2790 [btrfs]
    [75721.818300]  ? lock_downgrade+0x7c0/0x7c0
    [75721.824411]  ? free_extent_buffer.part.0+0x107/0x200 [btrfs]
    [75721.832456]  ? split_leaf+0x12d0/0x12d0 [btrfs]
    [75721.839149]  ? free_extent_buffer.part.0+0x14f/0x200 [btrfs]
    [75721.846945]  ? free_extent_buffer+0x13/0x20 [btrfs]
    [75721.853960]  ? btrfs_release_path+0x4b/0x190 [btrfs]
    [75721.861429]  btrfs_csum_file_blocks+0x85c/0x1500 [btrfs]
    [75721.869313]  ? rcu_read_lock_sched_held+0x16/0x80
    [75721.876085]  ? lock_release+0x552/0xf80
    [75721.881957]  ? btrfs_del_csums+0x8c0/0x8c0 [btrfs]
    [75721.888886]  ? __kasan_check_write+0x14/0x20
    [75721.895152]  ? do_raw_read_unlock+0x44/0x80
    [75721.901323]  ? _raw_write_lock_irq+0x60/0x80
    [75721.907983]  ? btrfs_global_root+0xb9/0xe0 [btrfs]
    [75721.915166]  ? btrfs_csum_root+0x12b/0x180 [btrfs]
    [75721.921918]  ? btrfs_get_global_root+0x820/0x820 [btrfs]
    [75721.929166]  ? _raw_write_unlock+0x23/0x40
    [75721.935116]  ? unpin_extent_cache+0x1e3/0x390 [btrfs]
    [75721.942041]  btrfs_finish_ordered_io.isra.0+0xa0c/0x1dc0 [btrfs]
    [75721.949906]  ? try_to_wake_up+0x30/0x14a0
    [75721.955700]  ? btrfs_unlink_subvol+0xda0/0xda0 [btrfs]
    [75721.962661]  ? rcu_read_lock_sched_held+0x16/0x80
    [75721.969111]  ? lock_acquire+0x41b/0x4c0
    [75721.974982]  finish_ordered_fn+0x15/0x20 [btrfs]
    [75721.981639]  btrfs_work_helper+0x1af/0xa80 [btrfs]
    [75721.988184]  ? _raw_spin_unlock_irq+0x28/0x50
    [75721.994643]  process_one_work+0x815/0x1460
    [75722.000444]  ? pwq_dec_nr_in_flight+0x250/0x250
    [75722.006643]  ? do_raw_spin_trylock+0xbb/0x190
    [75722.013086]  worker_thread+0x59a/0xeb0
    [75722.018511]  kthread+0x2ac/0x360
    [75722.023428]  ? process_one_work+0x1460/0x1460
    [75722.029431]  ? kthread_complete_and_exit+0x30/0x30
    [75722.036044]  ret_from_fork+0x22/0x30
    [75722.041255]  </TASK>
    [75722.045047] irq event stamp: 0
    [75722.049703] hardirqs last  enabled at (0): [<0000000000000000>] 0x0
    [75722.057610] hardirqs last disabled at (0): [<ffffffff8118a94a>] copy_process+0x1c1a/0x66b0
    [75722.067533] softirqs last  enabled at (0): [<ffffffff8118a989>] copy_process+0x1c59/0x66b0
    [75722.077423] softirqs last disabled at (0): [<0000000000000000>] 0x0
    [75722.085335] ---[ end trace 0000000000000000 ]---
    
    To fix the estimation, we need to introduce fs_info->max_extent_size to
    replace BTRFS_MAX_EXTENT_SIZE, which allow setting the different size for
    regular vs zoned filesystem.
    
    Set fs_info->max_extent_size to BTRFS_MAX_EXTENT_SIZE by default. On zoned
    filesystem, it is set to fs_info->max_zone_append_size.
    
    CC: stable@vger.kernel.org # 5.12+
    Fixes: d8e3fb106f39 ("btrfs: zoned: use ZONE_APPEND write for zoned mode")
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Signed-off-by: Naohiro Aota <naohiro.aota@wdc.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 6cb4b96df97082a54634ba02196516919cda228c
Author: Naohiro Aota <naohiro.aota@wdc.com>
Date:   Sat Jul 9 08:18:40 2022 +0900

    btrfs: replace BTRFS_MAX_EXTENT_SIZE with fs_info->max_extent_size
    
    [ Upstream commit f7b12a62f008a3041f42f2426983e59a6a0a3c59 ]
    
    On zoned filesystem, data write out is limited by max_zone_append_size,
    and a large ordered extent is split according the size of a bio. OTOH,
    the number of extents to be written is calculated using
    BTRFS_MAX_EXTENT_SIZE, and that estimated number is used to reserve the
    metadata bytes to update and/or create the metadata items.
    
    The metadata reservation is done at e.g, btrfs_buffered_write() and then
    released according to the estimation changes. Thus, if the number of extent
    increases massively, the reserved metadata can run out.
    
    The increase of the number of extents easily occurs on zoned filesystem
    if BTRFS_MAX_EXTENT_SIZE > max_zone_append_size. And, it causes the
    following warning on a small RAM environment with disabling metadata
    over-commit (in the following patch).
    
    [75721.498492] ------------[ cut here ]------------
    [75721.505624] BTRFS: block rsv 1 returned -28
    [75721.512230] WARNING: CPU: 24 PID: 2327559 at fs/btrfs/block-rsv.c:537 btrfs_use_block_rsv+0x560/0x760 [btrfs]
    [75721.581854] CPU: 24 PID: 2327559 Comm: kworker/u64:10 Kdump: loaded Tainted: G        W         5.18.0-rc2-BTRFS-ZNS+ #109
    [75721.597200] Hardware name: Supermicro Super Server/H12SSL-NT, BIOS 2.0 02/22/2021
    [75721.607310] Workqueue: btrfs-endio-write btrfs_work_helper [btrfs]
    [75721.616209] RIP: 0010:btrfs_use_block_rsv+0x560/0x760 [btrfs]
    [75721.646649] RSP: 0018:ffffc9000fbdf3e0 EFLAGS: 00010286
    [75721.654126] RAX: 0000000000000000 RBX: 0000000000004000 RCX: 0000000000000000
    [75721.663524] RDX: 0000000000000004 RSI: 0000000000000008 RDI: fffff52001f7be6e
    [75721.672921] RBP: ffffc9000fbdf420 R08: 0000000000000001 R09: ffff889f8d1fc6c7
    [75721.682493] R10: ffffed13f1a3f8d8 R11: 0000000000000001 R12: ffff88980a3c0e28
    [75721.692284] R13: ffff889b66590000 R14: ffff88980a3c0e40 R15: ffff88980a3c0e8a
    [75721.701878] FS:  0000000000000000(0000) GS:ffff889f8d000000(0000) knlGS:0000000000000000
    [75721.712601] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [75721.720726] CR2: 000055d12e05c018 CR3: 0000800193594000 CR4: 0000000000350ee0
    [75721.730499] Call Trace:
    [75721.735166]  <TASK>
    [75721.739886]  btrfs_alloc_tree_block+0x1e1/0x1100 [btrfs]
    [75721.747545]  ? btrfs_alloc_logged_file_extent+0x550/0x550 [btrfs]
    [75721.756145]  ? btrfs_get_32+0xea/0x2d0 [btrfs]
    [75721.762852]  ? btrfs_get_32+0xea/0x2d0 [btrfs]
    [75721.769520]  ? push_leaf_left+0x420/0x620 [btrfs]
    [75721.776431]  ? memcpy+0x4e/0x60
    [75721.781931]  split_leaf+0x433/0x12d0 [btrfs]
    [75721.788392]  ? btrfs_get_token_32+0x580/0x580 [btrfs]
    [75721.795636]  ? push_for_double_split.isra.0+0x420/0x420 [btrfs]
    [75721.803759]  ? leaf_space_used+0x15d/0x1a0 [btrfs]
    [75721.811156]  btrfs_search_slot+0x1bc3/0x2790 [btrfs]
    [75721.818300]  ? lock_downgrade+0x7c0/0x7c0
    [75721.824411]  ? free_extent_buffer.part.0+0x107/0x200 [btrfs]
    [75721.832456]  ? split_leaf+0x12d0/0x12d0 [btrfs]
    [75721.839149]  ? free_extent_buffer.part.0+0x14f/0x200 [btrfs]
    [75721.846945]  ? free_extent_buffer+0x13/0x20 [btrfs]
    [75721.853960]  ? btrfs_release_path+0x4b/0x190 [btrfs]
    [75721.861429]  btrfs_csum_file_blocks+0x85c/0x1500 [btrfs]
    [75721.869313]  ? rcu_read_lock_sched_held+0x16/0x80
    [75721.876085]  ? lock_release+0x552/0xf80
    [75721.881957]  ? btrfs_del_csums+0x8c0/0x8c0 [btrfs]
    [75721.888886]  ? __kasan_check_write+0x14/0x20
    [75721.895152]  ? do_raw_read_unlock+0x44/0x80
    [75721.901323]  ? _raw_write_lock_irq+0x60/0x80
    [75721.907983]  ? btrfs_global_root+0xb9/0xe0 [btrfs]
    [75721.915166]  ? btrfs_csum_root+0x12b/0x180 [btrfs]
    [75721.921918]  ? btrfs_get_global_root+0x820/0x820 [btrfs]
    [75721.929166]  ? _raw_write_unlock+0x23/0x40
    [75721.935116]  ? unpin_extent_cache+0x1e3/0x390 [btrfs]
    [75721.942041]  btrfs_finish_ordered_io.isra.0+0xa0c/0x1dc0 [btrfs]
    [75721.949906]  ? try_to_wake_up+0x30/0x14a0
    [75721.955700]  ? btrfs_unlink_subvol+0xda0/0xda0 [btrfs]
    [75721.962661]  ? rcu_read_lock_sched_held+0x16/0x80
    [75721.969111]  ? lock_acquire+0x41b/0x4c0
    [75721.974982]  finish_ordered_fn+0x15/0x20 [btrfs]
    [75721.981639]  btrfs_work_helper+0x1af/0xa80 [btrfs]
    [75721.988184]  ? _raw_spin_unlock_irq+0x28/0x50
    [75721.994643]  process_one_work+0x815/0x1460
    [75722.000444]  ? pwq_dec_nr_in_flight+0x250/0x250
    [75722.006643]  ? do_raw_spin_trylock+0xbb/0x190
    [75722.013086]  worker_thread+0x59a/0xeb0
    [75722.018511]  kthread+0x2ac/0x360
    [75722.023428]  ? process_one_work+0x1460/0x1460
    [75722.029431]  ? kthread_complete_and_exit+0x30/0x30
    [75722.036044]  ret_from_fork+0x22/0x30
    [75722.041255]  </TASK>
    [75722.045047] irq event stamp: 0
    [75722.049703] hardirqs last  enabled at (0): [<0000000000000000>] 0x0
    [75722.057610] hardirqs last disabled at (0): [<ffffffff8118a94a>] copy_process+0x1c1a/0x66b0
    [75722.067533] softirqs last  enabled at (0): [<ffffffff8118a989>] copy_process+0x1c59/0x66b0
    [75722.077423] softirqs last disabled at (0): [<0000000000000000>] 0x0
    [75722.085335] ---[ end trace 0000000000000000 ]---
    
    To fix the estimation, we need to introduce fs_info->max_extent_size to
    replace BTRFS_MAX_EXTENT_SIZE, which allow setting the different size for
    regular vs zoned filesystem.
    
    Set fs_info->max_extent_size to BTRFS_MAX_EXTENT_SIZE by default. On zoned
    filesystem, it is set to fs_info->max_zone_append_size.
    
    CC: stable@vger.kernel.org # 5.12+
    Fixes: d8e3fb106f39 ("btrfs: zoned: use ZONE_APPEND write for zoned mode")
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Signed-off-by: Naohiro Aota <naohiro.aota@wdc.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 3d4b966e54b91e82534d58777bdbbbd67dff165a
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Sun Jul 24 16:51:48 2022 +0200

    Revert "Revert "char/random: silence a lockdep splat with printk()""
    
    In 2019, Sergey fixed a lockdep splat with 15341b1dd409 ("char/random:
    silence a lockdep splat with printk()"), but that got reverted soon
    after from 4.19 because back then it apparently caused various problems.
    But the issue it was fixing is still there, and more generally, many
    patches turning printk() into printk_deferred() have landed since,
    making me suspect it's okay to try this out again.
    
    This should fix the following deadlock found by the kernel test robot:
    
    [   18.287691] WARNING: possible circular locking dependency detected
    [   18.287692] 4.19.248-00165-g3d1f971aa81f #1 Not tainted
    [   18.287693] ------------------------------------------------------
    [   18.287712] stop/202 is trying to acquire lock:
    [   18.287713] (ptrval) (console_owner){..-.}, at: console_unlock (??:?)
    [   18.287717]
    [   18.287718] but task is already holding lock:
    [   18.287718] (ptrval) (&(&port->lock)->rlock){-...}, at: pty_write (pty.c:?)
    [   18.287722]
    [   18.287722] which lock already depends on the new lock.
    [   18.287723]
    [   18.287724]
    [   18.287725] the existing dependency chain (in reverse order) is:
    [   18.287725]
    [   18.287726] -> #2 (&(&port->lock)->rlock){-...}:
    [   18.287729] validate_chain+0x84a/0xe00
    [   18.287729] __lock_acquire (lockdep.c:?)
    [   18.287730] lock_acquire (??:?)
    [   18.287731] _raw_spin_lock_irqsave (??:?)
    [   18.287732] tty_port_tty_get (??:?)
    [   18.287733] tty_port_default_wakeup (tty_port.c:?)
    [   18.287734] tty_port_tty_wakeup (??:?)
    [   18.287734] uart_write_wakeup (??:?)
    [   18.287735] serial8250_tx_chars (??:?)
    [   18.287736] serial8250_handle_irq (??:?)
    [   18.287737] serial8250_default_handle_irq (8250_port.c:?)
    [   18.287738] serial8250_interrupt (8250_core.c:?)
    [   18.287738] __handle_irq_event_percpu (??:?)
    [   18.287739] handle_irq_event_percpu (??:?)
    [   18.287740] handle_irq_event (??:?)
    [   18.287741] handle_edge_irq (??:?)
    [   18.287742] handle_irq (??:?)
    [   18.287742] do_IRQ (??:?)
    [   18.287743] common_interrupt (entry_32.o:?)
    [   18.287744] _raw_spin_unlock_irqrestore (??:?)
    [   18.287745] uart_write (serial_core.c:?)
    [   18.287746] process_output_block (n_tty.c:?)
    [   18.287747] n_tty_write (n_tty.c:?)
    [   18.287747] tty_write (tty_io.c:?)
    [   18.287748] __vfs_write (??:?)
    [   18.287749] vfs_write (??:?)
    [   18.287750] ksys_write (??:?)
    [   18.287750] sys_write (??:?)
    [   18.287751] do_fast_syscall_32 (??:?)
    [   18.287752] entry_SYSENTER_32 (??:?)
    [   18.287752]
    [   18.287753] -> #1 (&port_lock_key){-.-.}:
    [   18.287756]
    [   18.287756] -> #0 (console_owner){..-.}:
    [   18.287759] check_prevs_add (lockdep.c:?)
    [   18.287760] validate_chain+0x84a/0xe00
    [   18.287761] __lock_acquire (lockdep.c:?)
    [   18.287761] lock_acquire (??:?)
    [   18.287762] console_unlock (??:?)
    [   18.287763] vprintk_emit (??:?)
    [   18.287764] vprintk_default (??:?)
    [   18.287764] vprintk_func (??:?)
    [   18.287765] printk (??:?)
    [   18.287766] get_random_u32 (??:?)
    [   18.287767] shuffle_freelist (slub.c:?)
    [   18.287767] allocate_slab (slub.c:?)
    [   18.287768] new_slab (slub.c:?)
    [   18.287769] ___slab_alloc+0x6d0/0xb20
    [   18.287770] __slab_alloc+0xd6/0x2e0
    [   18.287770] __kmalloc (??:?)
    [   18.287771] tty_buffer_alloc (tty_buffer.c:?)
    [   18.287772] __tty_buffer_request_room (tty_buffer.c:?)
    [   18.287773] tty_insert_flip_string_fixed_flag (??:?)
    [   18.287774] pty_write (pty.c:?)
    [   18.287775] process_output_block (n_tty.c:?)
    [   18.287776] n_tty_write (n_tty.c:?)
    [   18.287777] tty_write (tty_io.c:?)
    [   18.287778] __vfs_write (??:?)
    [   18.287779] vfs_write (??:?)
    [   18.287780] ksys_write (??:?)
    [   18.287780] sys_write (??:?)
    [   18.287781] do_fast_syscall_32 (??:?)
    [   18.287782] entry_SYSENTER_32 (??:?)
    [   18.287783]
    [   18.287783] other info that might help us debug this:
    [   18.287784]
    [   18.287785] Chain exists of:
    [   18.287785]   console_owner --> &port_lock_key --> &(&port->lock)->rlock
    [   18.287789]
    [   18.287790]  Possible unsafe locking scenario:
    [   18.287790]
    [   18.287791]        CPU0                    CPU1
    [   18.287792]        ----                    ----
    [   18.287792]   lock(&(&port->lock)->rlock);
    [   18.287794]                                lock(&port_lock_key);
    [   18.287814]                                lock(&(&port->lock)->rlock);
    [   18.287815]   lock(console_owner);
    [   18.287817]
    [   18.287818]  *** DEADLOCK ***
    [   18.287818]
    [   18.287819] 6 locks held by stop/202:
    [   18.287820] #0: (ptrval) (&tty->ldisc_sem){++++}, at: ldsem_down_read (??:?)
    [   18.287823] #1: (ptrval) (&tty->atomic_write_lock){+.+.}, at: tty_write_lock (tty_io.c:?)
    [   18.287826] #2: (ptrval) (&o_tty->termios_rwsem/1){++++}, at: n_tty_write (n_tty.c:?)
    [   18.287830] #3: (ptrval) (&ldata->output_lock){+.+.}, at: process_output_block (n_tty.c:?)
    [   18.287834] #4: (ptrval) (&(&port->lock)->rlock){-...}, at: pty_write (pty.c:?)
    [   18.287838] #5: (ptrval) (console_lock){+.+.}, at: console_trylock_spinning (printk.c:?)
    [   18.287841]
    [   18.287842] stack backtrace:
    [   18.287843] CPU: 0 PID: 202 Comm: stop Not tainted 4.19.248-00165-g3d1f971aa81f #1
    [   18.287843] Call Trace:
    [   18.287844] dump_stack (??:?)
    [   18.287845] print_circular_bug.cold+0x78/0x8b
    [   18.287846] check_prev_add+0x66a/0xd20
    [   18.287847] check_prevs_add (lockdep.c:?)
    [   18.287848] validate_chain+0x84a/0xe00
    [   18.287848] __lock_acquire (lockdep.c:?)
    [   18.287849] lock_acquire (??:?)
    [   18.287850] ? console_unlock (??:?)
    [   18.287851] console_unlock (??:?)
    [   18.287851] ? console_unlock (??:?)
    [   18.287852] ? native_save_fl (??:?)
    [   18.287853] vprintk_emit (??:?)
    [   18.287854] vprintk_default (??:?)
    [   18.287855] vprintk_func (??:?)
    [   18.287855] printk (??:?)
    [   18.287856] get_random_u32 (??:?)
    [   18.287857] ? shuffle_freelist (slub.c:?)
    [   18.287858] shuffle_freelist (slub.c:?)
    [   18.287858] ? page_address (??:?)
    [   18.287859] allocate_slab (slub.c:?)
    [   18.287860] new_slab (slub.c:?)
    [   18.287861] ? pvclock_clocksource_read (??:?)
    [   18.287862] ___slab_alloc+0x6d0/0xb20
    [   18.287862] ? kvm_sched_clock_read (kvmclock.c:?)
    [   18.287863] ? __slab_alloc+0xbc/0x2e0
    [   18.287864] ? native_wbinvd (paravirt.c:?)
    [   18.287865] __slab_alloc+0xd6/0x2e0
    [   18.287865] __kmalloc (??:?)
    [   18.287866] ? __lock_acquire (lockdep.c:?)
    [   18.287867] ? tty_buffer_alloc (tty_buffer.c:?)
    [   18.287868] tty_buffer_alloc (tty_buffer.c:?)
    [   18.287869] __tty_buffer_request_room (tty_buffer.c:?)
    [   18.287869] tty_insert_flip_string_fixed_flag (??:?)
    [   18.287870] pty_write (pty.c:?)
    [   18.287871] process_output_block (n_tty.c:?)
    [   18.287872] n_tty_write (n_tty.c:?)
    [   18.287873] ? print_dl_stats (??:?)
    [   18.287874] ? n_tty_ioctl (n_tty.c:?)
    [   18.287874] tty_write (tty_io.c:?)
    [   18.287875] ? n_tty_ioctl (n_tty.c:?)
    [   18.287876] ? tty_write_unlock (tty_io.c:?)
    [   18.287877] __vfs_write (??:?)
    [   18.287877] vfs_write (??:?)
    [   18.287878] ? __fget_light (file.c:?)
    [   18.287879] ksys_write (??:?)
    
    Cc: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Lech Perczak <l.perczak@camlintechnologies.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Sasha Levin <sashal@kernel.org>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: John Ogness <john.ogness@linutronix.de>
    Reported-by: kernel test robot <oliver.sang@intel.com>
    Link: https://lore.kernel.org/lkml/Ytz+lo4zRQYG3JUR@xsang-OptiPlex-9020
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit df4cb7f30e831cce0bd85bbbc1a88aaa46a5fa98
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Sun Jul 24 16:51:48 2022 +0200

    Revert "Revert "char/random: silence a lockdep splat with printk()""
    
    In 2019, Sergey fixed a lockdep splat with 15341b1dd409 ("char/random:
    silence a lockdep splat with printk()"), but that got reverted soon
    after from 4.19 because back then it apparently caused various problems.
    But the issue it was fixing is still there, and more generally, many
    patches turning printk() into printk_deferred() have landed since,
    making me suspect it's okay to try this out again.
    
    This should fix the following deadlock found by the kernel test robot:
    
    [   18.287691] WARNING: possible circular locking dependency detected
    [   18.287692] 4.19.248-00165-g3d1f971aa81f #1 Not tainted
    [   18.287693] ------------------------------------------------------
    [   18.287712] stop/202 is trying to acquire lock:
    [   18.287713] (ptrval) (console_owner){..-.}, at: console_unlock (??:?)
    [   18.287717]
    [   18.287718] but task is already holding lock:
    [   18.287718] (ptrval) (&(&port->lock)->rlock){-...}, at: pty_write (pty.c:?)
    [   18.287722]
    [   18.287722] which lock already depends on the new lock.
    [   18.287723]
    [   18.287724]
    [   18.287725] the existing dependency chain (in reverse order) is:
    [   18.287725]
    [   18.287726] -> #2 (&(&port->lock)->rlock){-...}:
    [   18.287729] validate_chain+0x84a/0xe00
    [   18.287729] __lock_acquire (lockdep.c:?)
    [   18.287730] lock_acquire (??:?)
    [   18.287731] _raw_spin_lock_irqsave (??:?)
    [   18.287732] tty_port_tty_get (??:?)
    [   18.287733] tty_port_default_wakeup (tty_port.c:?)
    [   18.287734] tty_port_tty_wakeup (??:?)
    [   18.287734] uart_write_wakeup (??:?)
    [   18.287735] serial8250_tx_chars (??:?)
    [   18.287736] serial8250_handle_irq (??:?)
    [   18.287737] serial8250_default_handle_irq (8250_port.c:?)
    [   18.287738] serial8250_interrupt (8250_core.c:?)
    [   18.287738] __handle_irq_event_percpu (??:?)
    [   18.287739] handle_irq_event_percpu (??:?)
    [   18.287740] handle_irq_event (??:?)
    [   18.287741] handle_edge_irq (??:?)
    [   18.287742] handle_irq (??:?)
    [   18.287742] do_IRQ (??:?)
    [   18.287743] common_interrupt (entry_32.o:?)
    [   18.287744] _raw_spin_unlock_irqrestore (??:?)
    [   18.287745] uart_write (serial_core.c:?)
    [   18.287746] process_output_block (n_tty.c:?)
    [   18.287747] n_tty_write (n_tty.c:?)
    [   18.287747] tty_write (tty_io.c:?)
    [   18.287748] __vfs_write (??:?)
    [   18.287749] vfs_write (??:?)
    [   18.287750] ksys_write (??:?)
    [   18.287750] sys_write (??:?)
    [   18.287751] do_fast_syscall_32 (??:?)
    [   18.287752] entry_SYSENTER_32 (??:?)
    [   18.287752]
    [   18.287753] -> #1 (&port_lock_key){-.-.}:
    [   18.287756]
    [   18.287756] -> #0 (console_owner){..-.}:
    [   18.287759] check_prevs_add (lockdep.c:?)
    [   18.287760] validate_chain+0x84a/0xe00
    [   18.287761] __lock_acquire (lockdep.c:?)
    [   18.287761] lock_acquire (??:?)
    [   18.287762] console_unlock (??:?)
    [   18.287763] vprintk_emit (??:?)
    [   18.287764] vprintk_default (??:?)
    [   18.287764] vprintk_func (??:?)
    [   18.287765] printk (??:?)
    [   18.287766] get_random_u32 (??:?)
    [   18.287767] shuffle_freelist (slub.c:?)
    [   18.287767] allocate_slab (slub.c:?)
    [   18.287768] new_slab (slub.c:?)
    [   18.287769] ___slab_alloc+0x6d0/0xb20
    [   18.287770] __slab_alloc+0xd6/0x2e0
    [   18.287770] __kmalloc (??:?)
    [   18.287771] tty_buffer_alloc (tty_buffer.c:?)
    [   18.287772] __tty_buffer_request_room (tty_buffer.c:?)
    [   18.287773] tty_insert_flip_string_fixed_flag (??:?)
    [   18.287774] pty_write (pty.c:?)
    [   18.287775] process_output_block (n_tty.c:?)
    [   18.287776] n_tty_write (n_tty.c:?)
    [   18.287777] tty_write (tty_io.c:?)
    [   18.287778] __vfs_write (??:?)
    [   18.287779] vfs_write (??:?)
    [   18.287780] ksys_write (??:?)
    [   18.287780] sys_write (??:?)
    [   18.287781] do_fast_syscall_32 (??:?)
    [   18.287782] entry_SYSENTER_32 (??:?)
    [   18.287783]
    [   18.287783] other info that might help us debug this:
    [   18.287784]
    [   18.287785] Chain exists of:
    [   18.287785]   console_owner --> &port_lock_key --> &(&port->lock)->rlock
    [   18.287789]
    [   18.287790]  Possible unsafe locking scenario:
    [   18.287790]
    [   18.287791]        CPU0                    CPU1
    [   18.287792]        ----                    ----
    [   18.287792]   lock(&(&port->lock)->rlock);
    [   18.287794]                                lock(&port_lock_key);
    [   18.287814]                                lock(&(&port->lock)->rlock);
    [   18.287815]   lock(console_owner);
    [   18.287817]
    [   18.287818]  *** DEADLOCK ***
    [   18.287818]
    [   18.287819] 6 locks held by stop/202:
    [   18.287820] #0: (ptrval) (&tty->ldisc_sem){++++}, at: ldsem_down_read (??:?)
    [   18.287823] #1: (ptrval) (&tty->atomic_write_lock){+.+.}, at: tty_write_lock (tty_io.c:?)
    [   18.287826] #2: (ptrval) (&o_tty->termios_rwsem/1){++++}, at: n_tty_write (n_tty.c:?)
    [   18.287830] #3: (ptrval) (&ldata->output_lock){+.+.}, at: process_output_block (n_tty.c:?)
    [   18.287834] #4: (ptrval) (&(&port->lock)->rlock){-...}, at: pty_write (pty.c:?)
    [   18.287838] #5: (ptrval) (console_lock){+.+.}, at: console_trylock_spinning (printk.c:?)
    [   18.287841]
    [   18.287842] stack backtrace:
    [   18.287843] CPU: 0 PID: 202 Comm: stop Not tainted 4.19.248-00165-g3d1f971aa81f #1
    [   18.287843] Call Trace:
    [   18.287844] dump_stack (??:?)
    [   18.287845] print_circular_bug.cold+0x78/0x8b
    [   18.287846] check_prev_add+0x66a/0xd20
    [   18.287847] check_prevs_add (lockdep.c:?)
    [   18.287848] validate_chain+0x84a/0xe00
    [   18.287848] __lock_acquire (lockdep.c:?)
    [   18.287849] lock_acquire (??:?)
    [   18.287850] ? console_unlock (??:?)
    [   18.287851] console_unlock (??:?)
    [   18.287851] ? console_unlock (??:?)
    [   18.287852] ? native_save_fl (??:?)
    [   18.287853] vprintk_emit (??:?)
    [   18.287854] vprintk_default (??:?)
    [   18.287855] vprintk_func (??:?)
    [   18.287855] printk (??:?)
    [   18.287856] get_random_u32 (??:?)
    [   18.287857] ? shuffle_freelist (slub.c:?)
    [   18.287858] shuffle_freelist (slub.c:?)
    [   18.287858] ? page_address (??:?)
    [   18.287859] allocate_slab (slub.c:?)
    [   18.287860] new_slab (slub.c:?)
    [   18.287861] ? pvclock_clocksource_read (??:?)
    [   18.287862] ___slab_alloc+0x6d0/0xb20
    [   18.287862] ? kvm_sched_clock_read (kvmclock.c:?)
    [   18.287863] ? __slab_alloc+0xbc/0x2e0
    [   18.287864] ? native_wbinvd (paravirt.c:?)
    [   18.287865] __slab_alloc+0xd6/0x2e0
    [   18.287865] __kmalloc (??:?)
    [   18.287866] ? __lock_acquire (lockdep.c:?)
    [   18.287867] ? tty_buffer_alloc (tty_buffer.c:?)
    [   18.287868] tty_buffer_alloc (tty_buffer.c:?)
    [   18.287869] __tty_buffer_request_room (tty_buffer.c:?)
    [   18.287869] tty_insert_flip_string_fixed_flag (??:?)
    [   18.287870] pty_write (pty.c:?)
    [   18.287871] process_output_block (n_tty.c:?)
    [   18.287872] n_tty_write (n_tty.c:?)
    [   18.287873] ? print_dl_stats (??:?)
    [   18.287874] ? n_tty_ioctl (n_tty.c:?)
    [   18.287874] tty_write (tty_io.c:?)
    [   18.287875] ? n_tty_ioctl (n_tty.c:?)
    [   18.287876] ? tty_write_unlock (tty_io.c:?)
    [   18.287877] __vfs_write (??:?)
    [   18.287877] vfs_write (??:?)
    [   18.287878] ? __fget_light (file.c:?)
    [   18.287879] ksys_write (??:?)
    
    Cc: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Lech Perczak <l.perczak@camlintechnologies.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Sasha Levin <sashal@kernel.org>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: John Ogness <john.ogness@linutronix.de>
    Reported-by: kernel test robot <oliver.sang@intel.com>
    Link: https://lore.kernel.org/lkml/Ytz+lo4zRQYG3JUR@xsang-OptiPlex-9020
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2bbc39ce7809b1e2cd0f96103d4510550a96d02f
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Sun Jul 24 16:51:48 2022 +0200

    Revert "Revert "char/random: silence a lockdep splat with printk()""
    
    In 2019, Sergey fixed a lockdep splat with 15341b1dd409 ("char/random:
    silence a lockdep splat with printk()"), but that got reverted soon
    after from 4.19 because back then it apparently caused various problems.
    But the issue it was fixing is still there, and more generally, many
    patches turning printk() into printk_deferred() have landed since,
    making me suspect it's okay to try this out again.
    
    This should fix the following deadlock found by the kernel test robot:
    
    [   18.287691] WARNING: possible circular locking dependency detected
    [   18.287692] 4.19.248-00165-g3d1f971aa81f #1 Not tainted
    [   18.287693] ------------------------------------------------------
    [   18.287712] stop/202 is trying to acquire lock:
    [   18.287713] (ptrval) (console_owner){..-.}, at: console_unlock (??:?)
    [   18.287717]
    [   18.287718] but task is already holding lock:
    [   18.287718] (ptrval) (&(&port->lock)->rlock){-...}, at: pty_write (pty.c:?)
    [   18.287722]
    [   18.287722] which lock already depends on the new lock.
    [   18.287723]
    [   18.287724]
    [   18.287725] the existing dependency chain (in reverse order) is:
    [   18.287725]
    [   18.287726] -> #2 (&(&port->lock)->rlock){-...}:
    [   18.287729] validate_chain+0x84a/0xe00
    [   18.287729] __lock_acquire (lockdep.c:?)
    [   18.287730] lock_acquire (??:?)
    [   18.287731] _raw_spin_lock_irqsave (??:?)
    [   18.287732] tty_port_tty_get (??:?)
    [   18.287733] tty_port_default_wakeup (tty_port.c:?)
    [   18.287734] tty_port_tty_wakeup (??:?)
    [   18.287734] uart_write_wakeup (??:?)
    [   18.287735] serial8250_tx_chars (??:?)
    [   18.287736] serial8250_handle_irq (??:?)
    [   18.287737] serial8250_default_handle_irq (8250_port.c:?)
    [   18.287738] serial8250_interrupt (8250_core.c:?)
    [   18.287738] __handle_irq_event_percpu (??:?)
    [   18.287739] handle_irq_event_percpu (??:?)
    [   18.287740] handle_irq_event (??:?)
    [   18.287741] handle_edge_irq (??:?)
    [   18.287742] handle_irq (??:?)
    [   18.287742] do_IRQ (??:?)
    [   18.287743] common_interrupt (entry_32.o:?)
    [   18.287744] _raw_spin_unlock_irqrestore (??:?)
    [   18.287745] uart_write (serial_core.c:?)
    [   18.287746] process_output_block (n_tty.c:?)
    [   18.287747] n_tty_write (n_tty.c:?)
    [   18.287747] tty_write (tty_io.c:?)
    [   18.287748] __vfs_write (??:?)
    [   18.287749] vfs_write (??:?)
    [   18.287750] ksys_write (??:?)
    [   18.287750] sys_write (??:?)
    [   18.287751] do_fast_syscall_32 (??:?)
    [   18.287752] entry_SYSENTER_32 (??:?)
    [   18.287752]
    [   18.287753] -> #1 (&port_lock_key){-.-.}:
    [   18.287756]
    [   18.287756] -> #0 (console_owner){..-.}:
    [   18.287759] check_prevs_add (lockdep.c:?)
    [   18.287760] validate_chain+0x84a/0xe00
    [   18.287761] __lock_acquire (lockdep.c:?)
    [   18.287761] lock_acquire (??:?)
    [   18.287762] console_unlock (??:?)
    [   18.287763] vprintk_emit (??:?)
    [   18.287764] vprintk_default (??:?)
    [   18.287764] vprintk_func (??:?)
    [   18.287765] printk (??:?)
    [   18.287766] get_random_u32 (??:?)
    [   18.287767] shuffle_freelist (slub.c:?)
    [   18.287767] allocate_slab (slub.c:?)
    [   18.287768] new_slab (slub.c:?)
    [   18.287769] ___slab_alloc+0x6d0/0xb20
    [   18.287770] __slab_alloc+0xd6/0x2e0
    [   18.287770] __kmalloc (??:?)
    [   18.287771] tty_buffer_alloc (tty_buffer.c:?)
    [   18.287772] __tty_buffer_request_room (tty_buffer.c:?)
    [   18.287773] tty_insert_flip_string_fixed_flag (??:?)
    [   18.287774] pty_write (pty.c:?)
    [   18.287775] process_output_block (n_tty.c:?)
    [   18.287776] n_tty_write (n_tty.c:?)
    [   18.287777] tty_write (tty_io.c:?)
    [   18.287778] __vfs_write (??:?)
    [   18.287779] vfs_write (??:?)
    [   18.287780] ksys_write (??:?)
    [   18.287780] sys_write (??:?)
    [   18.287781] do_fast_syscall_32 (??:?)
    [   18.287782] entry_SYSENTER_32 (??:?)
    [   18.287783]
    [   18.287783] other info that might help us debug this:
    [   18.287784]
    [   18.287785] Chain exists of:
    [   18.287785]   console_owner --> &port_lock_key --> &(&port->lock)->rlock
    [   18.287789]
    [   18.287790]  Possible unsafe locking scenario:
    [   18.287790]
    [   18.287791]        CPU0                    CPU1
    [   18.287792]        ----                    ----
    [   18.287792]   lock(&(&port->lock)->rlock);
    [   18.287794]                                lock(&port_lock_key);
    [   18.287814]                                lock(&(&port->lock)->rlock);
    [   18.287815]   lock(console_owner);
    [   18.287817]
    [   18.287818]  *** DEADLOCK ***
    [   18.287818]
    [   18.287819] 6 locks held by stop/202:
    [   18.287820] #0: (ptrval) (&tty->ldisc_sem){++++}, at: ldsem_down_read (??:?)
    [   18.287823] #1: (ptrval) (&tty->atomic_write_lock){+.+.}, at: tty_write_lock (tty_io.c:?)
    [   18.287826] #2: (ptrval) (&o_tty->termios_rwsem/1){++++}, at: n_tty_write (n_tty.c:?)
    [   18.287830] #3: (ptrval) (&ldata->output_lock){+.+.}, at: process_output_block (n_tty.c:?)
    [   18.287834] #4: (ptrval) (&(&port->lock)->rlock){-...}, at: pty_write (pty.c:?)
    [   18.287838] #5: (ptrval) (console_lock){+.+.}, at: console_trylock_spinning (printk.c:?)
    [   18.287841]
    [   18.287842] stack backtrace:
    [   18.287843] CPU: 0 PID: 202 Comm: stop Not tainted 4.19.248-00165-g3d1f971aa81f #1
    [   18.287843] Call Trace:
    [   18.287844] dump_stack (??:?)
    [   18.287845] print_circular_bug.cold+0x78/0x8b
    [   18.287846] check_prev_add+0x66a/0xd20
    [   18.287847] check_prevs_add (lockdep.c:?)
    [   18.287848] validate_chain+0x84a/0xe00
    [   18.287848] __lock_acquire (lockdep.c:?)
    [   18.287849] lock_acquire (??:?)
    [   18.287850] ? console_unlock (??:?)
    [   18.287851] console_unlock (??:?)
    [   18.287851] ? console_unlock (??:?)
    [   18.287852] ? native_save_fl (??:?)
    [   18.287853] vprintk_emit (??:?)
    [   18.287854] vprintk_default (??:?)
    [   18.287855] vprintk_func (??:?)
    [   18.287855] printk (??:?)
    [   18.287856] get_random_u32 (??:?)
    [   18.287857] ? shuffle_freelist (slub.c:?)
    [   18.287858] shuffle_freelist (slub.c:?)
    [   18.287858] ? page_address (??:?)
    [   18.287859] allocate_slab (slub.c:?)
    [   18.287860] new_slab (slub.c:?)
    [   18.287861] ? pvclock_clocksource_read (??:?)
    [   18.287862] ___slab_alloc+0x6d0/0xb20
    [   18.287862] ? kvm_sched_clock_read (kvmclock.c:?)
    [   18.287863] ? __slab_alloc+0xbc/0x2e0
    [   18.287864] ? native_wbinvd (paravirt.c:?)
    [   18.287865] __slab_alloc+0xd6/0x2e0
    [   18.287865] __kmalloc (??:?)
    [   18.287866] ? __lock_acquire (lockdep.c:?)
    [   18.287867] ? tty_buffer_alloc (tty_buffer.c:?)
    [   18.287868] tty_buffer_alloc (tty_buffer.c:?)
    [   18.287869] __tty_buffer_request_room (tty_buffer.c:?)
    [   18.287869] tty_insert_flip_string_fixed_flag (??:?)
    [   18.287870] pty_write (pty.c:?)
    [   18.287871] process_output_block (n_tty.c:?)
    [   18.287872] n_tty_write (n_tty.c:?)
    [   18.287873] ? print_dl_stats (??:?)
    [   18.287874] ? n_tty_ioctl (n_tty.c:?)
    [   18.287874] tty_write (tty_io.c:?)
    [   18.287875] ? n_tty_ioctl (n_tty.c:?)
    [   18.287876] ? tty_write_unlock (tty_io.c:?)
    [   18.287877] __vfs_write (??:?)
    [   18.287877] vfs_write (??:?)
    [   18.287878] ? __fget_light (file.c:?)
    [   18.287879] ksys_write (??:?)
    
    Cc: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Lech Perczak <l.perczak@camlintechnologies.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Sasha Levin <sashal@kernel.org>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: John Ogness <john.ogness@linutronix.de>
    Reported-by: kernel test robot <oliver.sang@intel.com>
    Link: https://lore.kernel.org/lkml/Ytz+lo4zRQYG3JUR@xsang-OptiPlex-9020
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f7b12a62f008a3041f42f2426983e59a6a0a3c59
Author: Naohiro Aota <naohiro.aota@wdc.com>
Date:   Sat Jul 9 08:18:40 2022 +0900

    btrfs: replace BTRFS_MAX_EXTENT_SIZE with fs_info->max_extent_size
    
    On zoned filesystem, data write out is limited by max_zone_append_size,
    and a large ordered extent is split according the size of a bio. OTOH,
    the number of extents to be written is calculated using
    BTRFS_MAX_EXTENT_SIZE, and that estimated number is used to reserve the
    metadata bytes to update and/or create the metadata items.
    
    The metadata reservation is done at e.g, btrfs_buffered_write() and then
    released according to the estimation changes. Thus, if the number of extent
    increases massively, the reserved metadata can run out.
    
    The increase of the number of extents easily occurs on zoned filesystem
    if BTRFS_MAX_EXTENT_SIZE > max_zone_append_size. And, it causes the
    following warning on a small RAM environment with disabling metadata
    over-commit (in the following patch).
    
    [75721.498492] ------------[ cut here ]------------
    [75721.505624] BTRFS: block rsv 1 returned -28
    [75721.512230] WARNING: CPU: 24 PID: 2327559 at fs/btrfs/block-rsv.c:537 btrfs_use_block_rsv+0x560/0x760 [btrfs]
    [75721.581854] CPU: 24 PID: 2327559 Comm: kworker/u64:10 Kdump: loaded Tainted: G        W         5.18.0-rc2-BTRFS-ZNS+ #109
    [75721.597200] Hardware name: Supermicro Super Server/H12SSL-NT, BIOS 2.0 02/22/2021
    [75721.607310] Workqueue: btrfs-endio-write btrfs_work_helper [btrfs]
    [75721.616209] RIP: 0010:btrfs_use_block_rsv+0x560/0x760 [btrfs]
    [75721.646649] RSP: 0018:ffffc9000fbdf3e0 EFLAGS: 00010286
    [75721.654126] RAX: 0000000000000000 RBX: 0000000000004000 RCX: 0000000000000000
    [75721.663524] RDX: 0000000000000004 RSI: 0000000000000008 RDI: fffff52001f7be6e
    [75721.672921] RBP: ffffc9000fbdf420 R08: 0000000000000001 R09: ffff889f8d1fc6c7
    [75721.682493] R10: ffffed13f1a3f8d8 R11: 0000000000000001 R12: ffff88980a3c0e28
    [75721.692284] R13: ffff889b66590000 R14: ffff88980a3c0e40 R15: ffff88980a3c0e8a
    [75721.701878] FS:  0000000000000000(0000) GS:ffff889f8d000000(0000) knlGS:0000000000000000
    [75721.712601] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [75721.720726] CR2: 000055d12e05c018 CR3: 0000800193594000 CR4: 0000000000350ee0
    [75721.730499] Call Trace:
    [75721.735166]  <TASK>
    [75721.739886]  btrfs_alloc_tree_block+0x1e1/0x1100 [btrfs]
    [75721.747545]  ? btrfs_alloc_logged_file_extent+0x550/0x550 [btrfs]
    [75721.756145]  ? btrfs_get_32+0xea/0x2d0 [btrfs]
    [75721.762852]  ? btrfs_get_32+0xea/0x2d0 [btrfs]
    [75721.769520]  ? push_leaf_left+0x420/0x620 [btrfs]
    [75721.776431]  ? memcpy+0x4e/0x60
    [75721.781931]  split_leaf+0x433/0x12d0 [btrfs]
    [75721.788392]  ? btrfs_get_token_32+0x580/0x580 [btrfs]
    [75721.795636]  ? push_for_double_split.isra.0+0x420/0x420 [btrfs]
    [75721.803759]  ? leaf_space_used+0x15d/0x1a0 [btrfs]
    [75721.811156]  btrfs_search_slot+0x1bc3/0x2790 [btrfs]
    [75721.818300]  ? lock_downgrade+0x7c0/0x7c0
    [75721.824411]  ? free_extent_buffer.part.0+0x107/0x200 [btrfs]
    [75721.832456]  ? split_leaf+0x12d0/0x12d0 [btrfs]
    [75721.839149]  ? free_extent_buffer.part.0+0x14f/0x200 [btrfs]
    [75721.846945]  ? free_extent_buffer+0x13/0x20 [btrfs]
    [75721.853960]  ? btrfs_release_path+0x4b/0x190 [btrfs]
    [75721.861429]  btrfs_csum_file_blocks+0x85c/0x1500 [btrfs]
    [75721.869313]  ? rcu_read_lock_sched_held+0x16/0x80
    [75721.876085]  ? lock_release+0x552/0xf80
    [75721.881957]  ? btrfs_del_csums+0x8c0/0x8c0 [btrfs]
    [75721.888886]  ? __kasan_check_write+0x14/0x20
    [75721.895152]  ? do_raw_read_unlock+0x44/0x80
    [75721.901323]  ? _raw_write_lock_irq+0x60/0x80
    [75721.907983]  ? btrfs_global_root+0xb9/0xe0 [btrfs]
    [75721.915166]  ? btrfs_csum_root+0x12b/0x180 [btrfs]
    [75721.921918]  ? btrfs_get_global_root+0x820/0x820 [btrfs]
    [75721.929166]  ? _raw_write_unlock+0x23/0x40
    [75721.935116]  ? unpin_extent_cache+0x1e3/0x390 [btrfs]
    [75721.942041]  btrfs_finish_ordered_io.isra.0+0xa0c/0x1dc0 [btrfs]
    [75721.949906]  ? try_to_wake_up+0x30/0x14a0
    [75721.955700]  ? btrfs_unlink_subvol+0xda0/0xda0 [btrfs]
    [75721.962661]  ? rcu_read_lock_sched_held+0x16/0x80
    [75721.969111]  ? lock_acquire+0x41b/0x4c0
    [75721.974982]  finish_ordered_fn+0x15/0x20 [btrfs]
    [75721.981639]  btrfs_work_helper+0x1af/0xa80 [btrfs]
    [75721.988184]  ? _raw_spin_unlock_irq+0x28/0x50
    [75721.994643]  process_one_work+0x815/0x1460
    [75722.000444]  ? pwq_dec_nr_in_flight+0x250/0x250
    [75722.006643]  ? do_raw_spin_trylock+0xbb/0x190
    [75722.013086]  worker_thread+0x59a/0xeb0
    [75722.018511]  kthread+0x2ac/0x360
    [75722.023428]  ? process_one_work+0x1460/0x1460
    [75722.029431]  ? kthread_complete_and_exit+0x30/0x30
    [75722.036044]  ret_from_fork+0x22/0x30
    [75722.041255]  </TASK>
    [75722.045047] irq event stamp: 0
    [75722.049703] hardirqs last  enabled at (0): [<0000000000000000>] 0x0
    [75722.057610] hardirqs last disabled at (0): [<ffffffff8118a94a>] copy_process+0x1c1a/0x66b0
    [75722.067533] softirqs last  enabled at (0): [<ffffffff8118a989>] copy_process+0x1c59/0x66b0
    [75722.077423] softirqs last disabled at (0): [<0000000000000000>] 0x0
    [75722.085335] ---[ end trace 0000000000000000 ]---
    
    To fix the estimation, we need to introduce fs_info->max_extent_size to
    replace BTRFS_MAX_EXTENT_SIZE, which allow setting the different size for
    regular vs zoned filesystem.
    
    Set fs_info->max_extent_size to BTRFS_MAX_EXTENT_SIZE by default. On zoned
    filesystem, it is set to fs_info->max_zone_append_size.
    
    CC: stable@vger.kernel.org # 5.12+
    Fixes: d8e3fb106f39 ("btrfs: zoned: use ZONE_APPEND write for zoned mode")
    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Signed-off-by: Naohiro Aota <naohiro.aota@wdc.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 8371666ef44cb5a343307cc741f4b86d5be6dad7
Author: David Howells <dhowells@redhat.com>
Date:   Sat May 21 08:45:28 2022 +0100

    rxrpc: Fix locking issue
    
    [ Upstream commit ad25f5cb39872ca14bcbe00816ae65c22fe04b89 ]
    
    There's a locking issue with the per-netns list of calls in rxrpc.  The
    pieces of code that add and remove a call from the list use write_lock()
    and the calls procfile uses read_lock() to access it.  However, the timer
    callback function may trigger a removal by trying to queue a call for
    processing and finding that it's already queued - at which point it has a
    spare refcount that it has to do something with.  Unfortunately, if it puts
    the call and this reduces the refcount to 0, the call will be removed from
    the list.  Unfortunately, since the _bh variants of the locking functions
    aren't used, this can deadlock.
    
    ================================
    WARNING: inconsistent lock state
    5.18.0-rc3-build4+ #10 Not tainted
    --------------------------------
    inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.
    ksoftirqd/2/25 [HC0[0]:SC1[1]:HE1:SE0] takes:
    ffff888107ac4038 (&rxnet->call_lock){+.?.}-{2:2}, at: rxrpc_put_call+0x103/0x14b
    {SOFTIRQ-ON-W} state was registered at:
    ...
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(&rxnet->call_lock);
      <Interrupt>
        lock(&rxnet->call_lock);
    
     *** DEADLOCK ***
    
    1 lock held by ksoftirqd/2/25:
     #0: ffff8881008ffdb0 ((&call->timer)){+.-.}-{0:0}, at: call_timer_fn+0x5/0x23d
    
    Changes
    =======
    ver #2)
     - Changed to using list_next_rcu() rather than rcu_dereference() directly.
    
    Fixes: 17926a79320a ("[AF_RXRPC]: Provide secure RxRPC sockets for use by userspace and kernel both")
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Marc Dionne <marc.dionne@auristor.com>
    cc: linux-afs@lists.infradead.org
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f617cef465523e453ea3df4e87cb8e02c3efbace
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Nov 26 17:15:29 2021 +0100

    net: Write lock dev_base_lock without disabling bottom halves.
    
    [ Upstream commit fd888e85fe6b661e78044dddfec0be5271afa626 ]
    
    The writer acquires dev_base_lock with disabled bottom halves.
    The reader can acquire dev_base_lock without disabling bottom halves
    because there is no writer in softirq context.
    
    On PREEMPT_RT the softirqs are preemptible and local_bh_disable() acts
    as a lock to ensure that resources, that are protected by disabling
    bottom halves, remain protected.
    This leads to a circular locking dependency if the lock acquired with
    disabled bottom halves (as in write_lock_bh()) and somewhere else with
    enabled bottom halves (as by read_lock() in netstat_show()) followed by
    disabling bottom halves (cxgb_get_stats() -> t4_wr_mbox_meat_timeout()
    -> spin_lock_bh()). This is the reverse locking order.
    
    All read_lock() invocation are from sysfs callback which are not invoked
    from softirq context. Therefore there is no need to disable bottom
    halves while acquiring a write lock.
    
    Acquire the write lock of dev_base_lock without disabling bottom halves.
    
    Reported-by: Pei Zhang <pezhang@redhat.com>
    Reported-by: Luis Claudio R. Goncalves <lgoncalv@redhat.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 79f9a73559cafacabb85e598afa922d1cd38b3e0
Author: David Howells <dhowells@redhat.com>
Date:   Sat May 21 08:45:28 2022 +0100

    rxrpc: Fix locking issue
    
    [ Upstream commit ad25f5cb39872ca14bcbe00816ae65c22fe04b89 ]
    
    There's a locking issue with the per-netns list of calls in rxrpc.  The
    pieces of code that add and remove a call from the list use write_lock()
    and the calls procfile uses read_lock() to access it.  However, the timer
    callback function may trigger a removal by trying to queue a call for
    processing and finding that it's already queued - at which point it has a
    spare refcount that it has to do something with.  Unfortunately, if it puts
    the call and this reduces the refcount to 0, the call will be removed from
    the list.  Unfortunately, since the _bh variants of the locking functions
    aren't used, this can deadlock.
    
    ================================
    WARNING: inconsistent lock state
    5.18.0-rc3-build4+ #10 Not tainted
    --------------------------------
    inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.
    ksoftirqd/2/25 [HC0[0]:SC1[1]:HE1:SE0] takes:
    ffff888107ac4038 (&rxnet->call_lock){+.?.}-{2:2}, at: rxrpc_put_call+0x103/0x14b
    {SOFTIRQ-ON-W} state was registered at:
    ...
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(&rxnet->call_lock);
      <Interrupt>
        lock(&rxnet->call_lock);
    
     *** DEADLOCK ***
    
    1 lock held by ksoftirqd/2/25:
     #0: ffff8881008ffdb0 ((&call->timer)){+.-.}-{0:0}, at: call_timer_fn+0x5/0x23d
    
    Changes
    =======
    ver #2)
     - Changed to using list_next_rcu() rather than rcu_dereference() directly.
    
    Fixes: 17926a79320a ("[AF_RXRPC]: Provide secure RxRPC sockets for use by userspace and kernel both")
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Marc Dionne <marc.dionne@auristor.com>
    cc: linux-afs@lists.infradead.org
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 30f0c3be083c584ad5143113d16ceaa58966cf44
Author: David Howells <dhowells@redhat.com>
Date:   Sat May 21 08:45:28 2022 +0100

    rxrpc: Fix locking issue
    
    [ Upstream commit ad25f5cb39872ca14bcbe00816ae65c22fe04b89 ]
    
    There's a locking issue with the per-netns list of calls in rxrpc.  The
    pieces of code that add and remove a call from the list use write_lock()
    and the calls procfile uses read_lock() to access it.  However, the timer
    callback function may trigger a removal by trying to queue a call for
    processing and finding that it's already queued - at which point it has a
    spare refcount that it has to do something with.  Unfortunately, if it puts
    the call and this reduces the refcount to 0, the call will be removed from
    the list.  Unfortunately, since the _bh variants of the locking functions
    aren't used, this can deadlock.
    
    ================================
    WARNING: inconsistent lock state
    5.18.0-rc3-build4+ #10 Not tainted
    --------------------------------
    inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.
    ksoftirqd/2/25 [HC0[0]:SC1[1]:HE1:SE0] takes:
    ffff888107ac4038 (&rxnet->call_lock){+.?.}-{2:2}, at: rxrpc_put_call+0x103/0x14b
    {SOFTIRQ-ON-W} state was registered at:
    ...
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(&rxnet->call_lock);
      <Interrupt>
        lock(&rxnet->call_lock);
    
     *** DEADLOCK ***
    
    1 lock held by ksoftirqd/2/25:
     #0: ffff8881008ffdb0 ((&call->timer)){+.-.}-{0:0}, at: call_timer_fn+0x5/0x23d
    
    Changes
    =======
    ver #2)
     - Changed to using list_next_rcu() rather than rcu_dereference() directly.
    
    Fixes: 17926a79320a ("[AF_RXRPC]: Provide secure RxRPC sockets for use by userspace and kernel both")
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Marc Dionne <marc.dionne@auristor.com>
    cc: linux-afs@lists.infradead.org
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit ad25f5cb39872ca14bcbe00816ae65c22fe04b89
Author: David Howells <dhowells@redhat.com>
Date:   Sat May 21 08:45:28 2022 +0100

    rxrpc: Fix locking issue
    
    There's a locking issue with the per-netns list of calls in rxrpc.  The
    pieces of code that add and remove a call from the list use write_lock()
    and the calls procfile uses read_lock() to access it.  However, the timer
    callback function may trigger a removal by trying to queue a call for
    processing and finding that it's already queued - at which point it has a
    spare refcount that it has to do something with.  Unfortunately, if it puts
    the call and this reduces the refcount to 0, the call will be removed from
    the list.  Unfortunately, since the _bh variants of the locking functions
    aren't used, this can deadlock.
    
    ================================
    WARNING: inconsistent lock state
    5.18.0-rc3-build4+ #10 Not tainted
    --------------------------------
    inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.
    ksoftirqd/2/25 [HC0[0]:SC1[1]:HE1:SE0] takes:
    ffff888107ac4038 (&rxnet->call_lock){+.?.}-{2:2}, at: rxrpc_put_call+0x103/0x14b
    {SOFTIRQ-ON-W} state was registered at:
    ...
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(&rxnet->call_lock);
      <Interrupt>
        lock(&rxnet->call_lock);
    
     *** DEADLOCK ***
    
    1 lock held by ksoftirqd/2/25:
     #0: ffff8881008ffdb0 ((&call->timer)){+.-.}-{0:0}, at: call_timer_fn+0x5/0x23d
    
    Changes
    =======
    ver #2)
     - Changed to using list_next_rcu() rather than rcu_dereference() directly.
    
    Fixes: 17926a79320a ("[AF_RXRPC]: Provide secure RxRPC sockets for use by userspace and kernel both")
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Marc Dionne <marc.dionne@auristor.com>
    cc: linux-afs@lists.infradead.org
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 325bca1fe0b1bb9f535e69bb9ec48d4a6e0ca3ce
Author: Rolf Eike Beer <eb@emlix.com>
Date:   Thu Apr 28 23:16:11 2022 -0700

    mm/mmap.c: use mmap_assert_write_locked() instead of open coding it
    
    In case the lock is actually not held at this point.
    
    Link: https://lkml.kernel.org/r/5827758.TJ1SttVevJ@mobilepool36.emlix.com
    Signed-off-by: Rolf Eike Beer <eb@emlix.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit 91d47fd5d1d93f1d93f72c93c6d7aab772ce7bf3
Author: Wojciech Drewek <wojciech.drewek@intel.com>
Date:   Fri Apr 8 09:56:10 2022 +0200

    ice: fix crash in switchdev mode
    
    [ Upstream commit d201665147ae788b7cca9fab58a1826f64152034 ]
    
    Below steps end up with crash:
    - modprobe ice
    - devlink dev eswitch set $PF1_PCI mode switchdev
    - echo 64 > /sys/class/net/$PF1/device/sriov_numvfs
    - rmmod ice
    
    Calling ice_eswitch_port_start_xmit while the process of removing
    VFs is in progress ends up with NULL pointer dereference.
    That's because PR netdev is not released but some resources
    are already freed. Fix it by checking if ICE_VF_DIS bit is set.
    
    Call trace:
    [ 1379.595146] BUG: kernel NULL pointer dereference, address: 0000000000000040
    [ 1379.595284] #PF: supervisor read access in kernel mode
    [ 1379.595410] #PF: error_code(0x0000) - not-present page
    [ 1379.595535] PGD 0 P4D 0
    [ 1379.595657] Oops: 0000 [#1] PREEMPT SMP PTI
    [ 1379.595783] CPU: 4 PID: 974 Comm: NetworkManager Kdump: loaded Tainted: G           OE     5.17.0-rc8_mrq_dev-queue+ #12
    [ 1379.595926] Hardware name: Intel Corporation S1200SP/S1200SP, BIOS S1200SP.86B.03.01.0042.013020190050 01/30/2019
    [ 1379.596063] RIP: 0010:ice_eswitch_port_start_xmit+0x46/0xd0 [ice]
    [ 1379.596292] Code: c7 c8 09 00 00 e8 9a c9 fc ff 84 c0 0f 85 82 00 00 00 4c 89 e7 e8 ca 70 fe ff 48 8b 7d 58 48 89 c3 48 85 ff 75 5e 48 8b 53 20 <8b> 42 40 85 c0 74 78 8d 48 01 f0 0f b1 4a 40 75 f2 0f b6 95 84 00
    [ 1379.596456] RSP: 0018:ffffaba0c0d7bad0 EFLAGS: 00010246
    [ 1379.596584] RAX: ffff969c14c71680 RBX: ffff969c14c71680 RCX: 000100107a0f0000
    [ 1379.596715] RDX: 0000000000000000 RSI: ffff969b9d631000 RDI: 0000000000000000
    [ 1379.596846] RBP: ffff969c07b46500 R08: ffff969becfca8ac R09: 0000000000000001
    [ 1379.596977] R10: 0000000000000004 R11: ffffaba0c0d7bbec R12: ffff969b9d631000
    [ 1379.597106] R13: ffffffffc08357a0 R14: ffff969c07b46500 R15: ffff969b9d631000
    [ 1379.597237] FS:  00007f72c0e25c80(0000) GS:ffff969f13500000(0000) knlGS:0000000000000000
    [ 1379.597414] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [ 1379.597562] CR2: 0000000000000040 CR3: 000000012b316006 CR4: 00000000003706e0
    [ 1379.597713] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    [ 1379.597863] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    [ 1379.598015] Call Trace:
    [ 1379.598153]  <TASK>
    [ 1379.598294]  dev_hard_start_xmit+0xd9/0x220
    [ 1379.598444]  sch_direct_xmit+0x8a/0x340
    [ 1379.598592]  __dev_queue_xmit+0xa3c/0xd30
    [ 1379.598739]  ? packet_parse_headers+0xb4/0xf0
    [ 1379.598890]  packet_sendmsg+0xa15/0x1620
    [ 1379.599038]  ? __check_object_size+0x46/0x140
    [ 1379.599186]  sock_sendmsg+0x5e/0x60
    [ 1379.599330]  ____sys_sendmsg+0x22c/0x270
    [ 1379.599474]  ? import_iovec+0x17/0x20
    [ 1379.599622]  ? sendmsg_copy_msghdr+0x59/0x90
    [ 1379.599771]  ___sys_sendmsg+0x81/0xc0
    [ 1379.599917]  ? __pollwait+0xd0/0xd0
    [ 1379.600061]  ? preempt_count_add+0x68/0xa0
    [ 1379.600210]  ? _raw_write_lock_irq+0x1a/0x40
    [ 1379.600369]  ? ep_done_scan+0xc9/0x110
    [ 1379.600494]  ? _raw_spin_unlock_irqrestore+0x25/0x40
    [ 1379.600622]  ? preempt_count_add+0x68/0xa0
    [ 1379.600747]  ? _raw_spin_lock_irq+0x1a/0x40
    [ 1379.600899]  ? __fget_light+0x8f/0x110
    [ 1379.601024]  __sys_sendmsg+0x49/0x80
    [ 1379.601148]  ? release_ds_buffers+0x50/0xe0
    [ 1379.601274]  do_syscall_64+0x3b/0x90
    [ 1379.601399]  entry_SYSCALL_64_after_hwframe+0x44/0xae
    [ 1379.601525] RIP: 0033:0x7f72c1e2e35d
    
    Fixes: f5396b8a663f ("ice: switchdev slow path")
    Signed-off-by: Wojciech Drewek <wojciech.drewek@intel.com>
    Reported-by: Marcin Szycik <marcin.szycik@linux.intel.com>
    Reviewed-by: Michal Swiatkowski <michal.swiatkowski@linux.intel.com>
    Tested-by: Sandeep Penigalapati <sandeep.penigalapati@intel.com>
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d201665147ae788b7cca9fab58a1826f64152034
Author: Wojciech Drewek <wojciech.drewek@intel.com>
Date:   Fri Apr 8 09:56:10 2022 +0200

    ice: fix crash in switchdev mode
    
    Below steps end up with crash:
    - modprobe ice
    - devlink dev eswitch set $PF1_PCI mode switchdev
    - echo 64 > /sys/class/net/$PF1/device/sriov_numvfs
    - rmmod ice
    
    Calling ice_eswitch_port_start_xmit while the process of removing
    VFs is in progress ends up with NULL pointer dereference.
    That's because PR netdev is not released but some resources
    are already freed. Fix it by checking if ICE_VF_DIS bit is set.
    
    Call trace:
    [ 1379.595146] BUG: kernel NULL pointer dereference, address: 0000000000000040
    [ 1379.595284] #PF: supervisor read access in kernel mode
    [ 1379.595410] #PF: error_code(0x0000) - not-present page
    [ 1379.595535] PGD 0 P4D 0
    [ 1379.595657] Oops: 0000 [#1] PREEMPT SMP PTI
    [ 1379.595783] CPU: 4 PID: 974 Comm: NetworkManager Kdump: loaded Tainted: G           OE     5.17.0-rc8_mrq_dev-queue+ #12
    [ 1379.595926] Hardware name: Intel Corporation S1200SP/S1200SP, BIOS S1200SP.86B.03.01.0042.013020190050 01/30/2019
    [ 1379.596063] RIP: 0010:ice_eswitch_port_start_xmit+0x46/0xd0 [ice]
    [ 1379.596292] Code: c7 c8 09 00 00 e8 9a c9 fc ff 84 c0 0f 85 82 00 00 00 4c 89 e7 e8 ca 70 fe ff 48 8b 7d 58 48 89 c3 48 85 ff 75 5e 48 8b 53 20 <8b> 42 40 85 c0 74 78 8d 48 01 f0 0f b1 4a 40 75 f2 0f b6 95 84 00
    [ 1379.596456] RSP: 0018:ffffaba0c0d7bad0 EFLAGS: 00010246
    [ 1379.596584] RAX: ffff969c14c71680 RBX: ffff969c14c71680 RCX: 000100107a0f0000
    [ 1379.596715] RDX: 0000000000000000 RSI: ffff969b9d631000 RDI: 0000000000000000
    [ 1379.596846] RBP: ffff969c07b46500 R08: ffff969becfca8ac R09: 0000000000000001
    [ 1379.596977] R10: 0000000000000004 R11: ffffaba0c0d7bbec R12: ffff969b9d631000
    [ 1379.597106] R13: ffffffffc08357a0 R14: ffff969c07b46500 R15: ffff969b9d631000
    [ 1379.597237] FS:  00007f72c0e25c80(0000) GS:ffff969f13500000(0000) knlGS:0000000000000000
    [ 1379.597414] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [ 1379.597562] CR2: 0000000000000040 CR3: 000000012b316006 CR4: 00000000003706e0
    [ 1379.597713] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    [ 1379.597863] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    [ 1379.598015] Call Trace:
    [ 1379.598153]  <TASK>
    [ 1379.598294]  dev_hard_start_xmit+0xd9/0x220
    [ 1379.598444]  sch_direct_xmit+0x8a/0x340
    [ 1379.598592]  __dev_queue_xmit+0xa3c/0xd30
    [ 1379.598739]  ? packet_parse_headers+0xb4/0xf0
    [ 1379.598890]  packet_sendmsg+0xa15/0x1620
    [ 1379.599038]  ? __check_object_size+0x46/0x140
    [ 1379.599186]  sock_sendmsg+0x5e/0x60
    [ 1379.599330]  ____sys_sendmsg+0x22c/0x270
    [ 1379.599474]  ? import_iovec+0x17/0x20
    [ 1379.599622]  ? sendmsg_copy_msghdr+0x59/0x90
    [ 1379.599771]  ___sys_sendmsg+0x81/0xc0
    [ 1379.599917]  ? __pollwait+0xd0/0xd0
    [ 1379.600061]  ? preempt_count_add+0x68/0xa0
    [ 1379.600210]  ? _raw_write_lock_irq+0x1a/0x40
    [ 1379.600369]  ? ep_done_scan+0xc9/0x110
    [ 1379.600494]  ? _raw_spin_unlock_irqrestore+0x25/0x40
    [ 1379.600622]  ? preempt_count_add+0x68/0xa0
    [ 1379.600747]  ? _raw_spin_lock_irq+0x1a/0x40
    [ 1379.600899]  ? __fget_light+0x8f/0x110
    [ 1379.601024]  __sys_sendmsg+0x49/0x80
    [ 1379.601148]  ? release_ds_buffers+0x50/0xe0
    [ 1379.601274]  do_syscall_64+0x3b/0x90
    [ 1379.601399]  entry_SYSCALL_64_after_hwframe+0x44/0xae
    [ 1379.601525] RIP: 0033:0x7f72c1e2e35d
    
    Fixes: f5396b8a663f ("ice: switchdev slow path")
    Signed-off-by: Wojciech Drewek <wojciech.drewek@intel.com>
    Reported-by: Marcin Szycik <marcin.szycik@linux.intel.com>
    Reviewed-by: Michal Swiatkowski <michal.swiatkowski@linux.intel.com>
    Tested-by: Sandeep Penigalapati <sandeep.penigalapati@intel.com>
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>

commit 512bde6420870f5403e253866d8de7a2267275bb
Author: Ronnie Sahlberg <lsahlber@redhat.com>
Date:   Tue Mar 15 13:44:04 2022 +1000

    cifs: we do not need a spinlock around the tree access during umount
    
    commit 9a14b65d590105d393b63f5320e1594edda7c672 upstream.
    
    Remove the spinlock around the tree traversal as we are calling possibly
    sleeping functions.
    We do not need a spinlock here as there will be no modifications to this
    tree at this point.
    
    This prevents warnings like this to occur in dmesg:
    [  653.774996] BUG: sleeping function called from invalid context at kernel/loc\
    king/mutex.c:280
    [  653.775088] in_atomic(): 1, irqs_disabled(): 0, non_block: 0, pid: 1827, nam\
    e: umount
    [  653.775152] preempt_count: 1, expected: 0
    [  653.775191] CPU: 0 PID: 1827 Comm: umount Tainted: G        W  OE     5.17.0\
    -rc7-00006-g4eb628dd74df #135
    [  653.775195] Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 1.14.0-\
    1.fc33 04/01/2014
    [  653.775197] Call Trace:
    [  653.775199]  <TASK>
    [  653.775202]  dump_stack_lvl+0x34/0x44
    [  653.775209]  __might_resched.cold+0x13f/0x172
    [  653.775213]  mutex_lock+0x75/0xf0
    [  653.775217]  ? __mutex_lock_slowpath+0x10/0x10
    [  653.775220]  ? _raw_write_lock_irq+0xd0/0xd0
    [  653.775224]  ? dput+0x6b/0x360
    [  653.775228]  cifs_kill_sb+0xff/0x1d0 [cifs]
    [  653.775285]  deactivate_locked_super+0x85/0x130
    [  653.775289]  cleanup_mnt+0x32c/0x4d0
    [  653.775292]  ? path_umount+0x228/0x380
    [  653.775296]  task_work_run+0xd8/0x180
    [  653.775301]  exit_to_user_mode_loop+0x152/0x160
    [  653.775306]  exit_to_user_mode_prepare+0x89/0xd0
    [  653.775315]  syscall_exit_to_user_mode+0x12/0x30
    [  653.775322]  do_syscall_64+0x48/0x90
    [  653.775326]  entry_SYSCALL_64_after_hwframe+0x44/0xae
    
    Fixes: 187af6e98b44e5d8f25e1d41a92db138eb54416f ("cifs: fix handlecache and multiuser")
    Reported-by: kernel test robot <oliver.sang@intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Ronnie Sahlberg <lsahlber@redhat.com>
    Signed-off-by: Steve French <stfrench@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4c1d011f2fa4c28a80c42775fa0e373c36f5f1c3
Author: Ronnie Sahlberg <lsahlber@redhat.com>
Date:   Tue Mar 15 13:44:04 2022 +1000

    cifs: we do not need a spinlock around the tree access during umount
    
    commit 9a14b65d590105d393b63f5320e1594edda7c672 upstream.
    
    Remove the spinlock around the tree traversal as we are calling possibly
    sleeping functions.
    We do not need a spinlock here as there will be no modifications to this
    tree at this point.
    
    This prevents warnings like this to occur in dmesg:
    [  653.774996] BUG: sleeping function called from invalid context at kernel/loc\
    king/mutex.c:280
    [  653.775088] in_atomic(): 1, irqs_disabled(): 0, non_block: 0, pid: 1827, nam\
    e: umount
    [  653.775152] preempt_count: 1, expected: 0
    [  653.775191] CPU: 0 PID: 1827 Comm: umount Tainted: G        W  OE     5.17.0\
    -rc7-00006-g4eb628dd74df #135
    [  653.775195] Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 1.14.0-\
    1.fc33 04/01/2014
    [  653.775197] Call Trace:
    [  653.775199]  <TASK>
    [  653.775202]  dump_stack_lvl+0x34/0x44
    [  653.775209]  __might_resched.cold+0x13f/0x172
    [  653.775213]  mutex_lock+0x75/0xf0
    [  653.775217]  ? __mutex_lock_slowpath+0x10/0x10
    [  653.775220]  ? _raw_write_lock_irq+0xd0/0xd0
    [  653.775224]  ? dput+0x6b/0x360
    [  653.775228]  cifs_kill_sb+0xff/0x1d0 [cifs]
    [  653.775285]  deactivate_locked_super+0x85/0x130
    [  653.775289]  cleanup_mnt+0x32c/0x4d0
    [  653.775292]  ? path_umount+0x228/0x380
    [  653.775296]  task_work_run+0xd8/0x180
    [  653.775301]  exit_to_user_mode_loop+0x152/0x160
    [  653.775306]  exit_to_user_mode_prepare+0x89/0xd0
    [  653.775315]  syscall_exit_to_user_mode+0x12/0x30
    [  653.775322]  do_syscall_64+0x48/0x90
    [  653.775326]  entry_SYSCALL_64_after_hwframe+0x44/0xae
    
    Fixes: 187af6e98b44e5d8f25e1d41a92db138eb54416f ("cifs: fix handlecache and multiuser")
    Reported-by: kernel test robot <oliver.sang@intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Ronnie Sahlberg <lsahlber@redhat.com>
    Signed-off-by: Steve French <stfrench@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4d095537cadc8be803dff6a425057ad817511429
Author: Ronnie Sahlberg <lsahlber@redhat.com>
Date:   Tue Mar 15 13:44:04 2022 +1000

    cifs: we do not need a spinlock around the tree access during umount
    
    commit 9a14b65d590105d393b63f5320e1594edda7c672 upstream.
    
    Remove the spinlock around the tree traversal as we are calling possibly
    sleeping functions.
    We do not need a spinlock here as there will be no modifications to this
    tree at this point.
    
    This prevents warnings like this to occur in dmesg:
    [  653.774996] BUG: sleeping function called from invalid context at kernel/loc\
    king/mutex.c:280
    [  653.775088] in_atomic(): 1, irqs_disabled(): 0, non_block: 0, pid: 1827, nam\
    e: umount
    [  653.775152] preempt_count: 1, expected: 0
    [  653.775191] CPU: 0 PID: 1827 Comm: umount Tainted: G        W  OE     5.17.0\
    -rc7-00006-g4eb628dd74df #135
    [  653.775195] Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 1.14.0-\
    1.fc33 04/01/2014
    [  653.775197] Call Trace:
    [  653.775199]  <TASK>
    [  653.775202]  dump_stack_lvl+0x34/0x44
    [  653.775209]  __might_resched.cold+0x13f/0x172
    [  653.775213]  mutex_lock+0x75/0xf0
    [  653.775217]  ? __mutex_lock_slowpath+0x10/0x10
    [  653.775220]  ? _raw_write_lock_irq+0xd0/0xd0
    [  653.775224]  ? dput+0x6b/0x360
    [  653.775228]  cifs_kill_sb+0xff/0x1d0 [cifs]
    [  653.775285]  deactivate_locked_super+0x85/0x130
    [  653.775289]  cleanup_mnt+0x32c/0x4d0
    [  653.775292]  ? path_umount+0x228/0x380
    [  653.775296]  task_work_run+0xd8/0x180
    [  653.775301]  exit_to_user_mode_loop+0x152/0x160
    [  653.775306]  exit_to_user_mode_prepare+0x89/0xd0
    [  653.775315]  syscall_exit_to_user_mode+0x12/0x30
    [  653.775322]  do_syscall_64+0x48/0x90
    [  653.775326]  entry_SYSCALL_64_after_hwframe+0x44/0xae
    
    Fixes: 187af6e98b44e5d8f25e1d41a92db138eb54416f ("cifs: fix handlecache and multiuser")
    Reported-by: kernel test robot <oliver.sang@intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Ronnie Sahlberg <lsahlber@redhat.com>
    Signed-off-by: Steve French <stfrench@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9a14b65d590105d393b63f5320e1594edda7c672
Author: Ronnie Sahlberg <lsahlber@redhat.com>
Date:   Tue Mar 15 13:44:04 2022 +1000

    cifs: we do not need a spinlock around the tree access during umount
    
    Remove the spinlock around the tree traversal as we are calling possibly
    sleeping functions.
    We do not need a spinlock here as there will be no modifications to this
    tree at this point.
    
    This prevents warnings like this to occur in dmesg:
    [  653.774996] BUG: sleeping function called from invalid context at kernel/loc\
    king/mutex.c:280
    [  653.775088] in_atomic(): 1, irqs_disabled(): 0, non_block: 0, pid: 1827, nam\
    e: umount
    [  653.775152] preempt_count: 1, expected: 0
    [  653.775191] CPU: 0 PID: 1827 Comm: umount Tainted: G        W  OE     5.17.0\
    -rc7-00006-g4eb628dd74df #135
    [  653.775195] Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS 1.14.0-\
    1.fc33 04/01/2014
    [  653.775197] Call Trace:
    [  653.775199]  <TASK>
    [  653.775202]  dump_stack_lvl+0x34/0x44
    [  653.775209]  __might_resched.cold+0x13f/0x172
    [  653.775213]  mutex_lock+0x75/0xf0
    [  653.775217]  ? __mutex_lock_slowpath+0x10/0x10
    [  653.775220]  ? _raw_write_lock_irq+0xd0/0xd0
    [  653.775224]  ? dput+0x6b/0x360
    [  653.775228]  cifs_kill_sb+0xff/0x1d0 [cifs]
    [  653.775285]  deactivate_locked_super+0x85/0x130
    [  653.775289]  cleanup_mnt+0x32c/0x4d0
    [  653.775292]  ? path_umount+0x228/0x380
    [  653.775296]  task_work_run+0xd8/0x180
    [  653.775301]  exit_to_user_mode_loop+0x152/0x160
    [  653.775306]  exit_to_user_mode_prepare+0x89/0xd0
    [  653.775315]  syscall_exit_to_user_mode+0x12/0x30
    [  653.775322]  do_syscall_64+0x48/0x90
    [  653.775326]  entry_SYSCALL_64_after_hwframe+0x44/0xae
    
    Fixes: 187af6e98b44e5d8f25e1d41a92db138eb54416f ("cifs: fix handlecache and multiuser")
    Reported-by: kernel test robot <oliver.sang@intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Ronnie Sahlberg <lsahlber@redhat.com>
    Signed-off-by: Steve French <stfrench@microsoft.com>

commit e9c478014b602fda2a99a6370d9eb2e5d7355246
Author: Damien Le Moal <damien.lemoal@opensource.wdc.com>
Date:   Tue Mar 1 20:30:08 2022 +0900

    scsi: scsi_debug: Silence unexpected unlock warnings
    
    The return statement inside the sdeb_read_lock(), sdeb_read_unlock(),
    sdeb_write_lock() and sdeb_write_unlock() confuse sparse, leading to many
    warnings about unexpected unlocks in the resp_xxx() functions.
    
    Modify the lock/unlock functions using the __acquire() and __release()
    inline annotations for the sdebug_no_rwlock == true case to avoid these
    warnings.
    
    Link: https://lore.kernel.org/r/20220301113009.595857-2-damien.lemoal@opensource.wdc.com
    Acked-by: Douglas Gilbert <dgilbert@interlog.com>
    Signed-off-by: Damien Le Moal <damien.lemoal@opensource.wdc.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit 6762a17dfee46a8895eb5a44ccab3db4740a8f59
Author: Qu Wenruo <wqu@suse.com>
Date:   Fri Feb 18 10:13:00 2022 +0800

    btrfs: subpage: fix a wrong check on subpage->writers
    
    commit c992fa1fd52380d0c4ced7b07479e877311ae645 upstream.
    
    [BUG]
    When looping btrfs/074 with 64K page size and 4K sectorsize, there is a
    low chance (1/50~1/100) to crash with the following ASSERT() triggered
    in btrfs_subpage_start_writer():
    
            ret = atomic_add_return(nbits, &subpage->writers);
            ASSERT(ret == nbits); <<< This one <<<
    
    [CAUSE]
    With more debugging output on the parameters of
    btrfs_subpage_start_writer(), it shows a very concerning error:
    
      ret=29 nbits=13 start=393216 len=53248
    
    For @nbits it's correct, but @ret which is the returned value from
    atomic_add_return(), it's not only larger than nbits, but also larger
    than max sectors per page value (for 64K page size and 4K sector size,
    it's 16).
    
    This indicates that some call sites are not properly decreasing the value.
    
    And that's exactly the case, in btrfs_page_unlock_writer(), due to the
    fact that we can have page locked either by lock_page() or
    process_one_page(), we have to check if the subpage has any writer.
    
    If no writers, it's locked by lock_page() and we only need to unlock it.
    
    But unfortunately the check for the writers are completely opposite:
    
            if (atomic_read(&subpage->writers))
                    /* No writers, locked by plain lock_page() */
                    return unlock_page(page);
    
    We directly unlock the page if it has writers, which is the completely
    opposite what we want.
    
    Thankfully the affected call site is only limited to
    extent_write_locked_range(), so it's mostly affecting compressed write.
    
    [FIX]
    Just fix the wrong check condition to fix the bug.
    
    Fixes: e55a0de18572 ("btrfs: rework page locking in __extent_writepage()")
    CC: stable@vger.kernel.org # 5.16
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c992fa1fd52380d0c4ced7b07479e877311ae645
Author: Qu Wenruo <wqu@suse.com>
Date:   Fri Feb 18 10:13:00 2022 +0800

    btrfs: subpage: fix a wrong check on subpage->writers
    
    [BUG]
    When looping btrfs/074 with 64K page size and 4K sectorsize, there is a
    low chance (1/50~1/100) to crash with the following ASSERT() triggered
    in btrfs_subpage_start_writer():
    
            ret = atomic_add_return(nbits, &subpage->writers);
            ASSERT(ret == nbits); <<< This one <<<
    
    [CAUSE]
    With more debugging output on the parameters of
    btrfs_subpage_start_writer(), it shows a very concerning error:
    
      ret=29 nbits=13 start=393216 len=53248
    
    For @nbits it's correct, but @ret which is the returned value from
    atomic_add_return(), it's not only larger than nbits, but also larger
    than max sectors per page value (for 64K page size and 4K sector size,
    it's 16).
    
    This indicates that some call sites are not properly decreasing the value.
    
    And that's exactly the case, in btrfs_page_unlock_writer(), due to the
    fact that we can have page locked either by lock_page() or
    process_one_page(), we have to check if the subpage has any writer.
    
    If no writers, it's locked by lock_page() and we only need to unlock it.
    
    But unfortunately the check for the writers are completely opposite:
    
            if (atomic_read(&subpage->writers))
                    /* No writers, locked by plain lock_page() */
                    return unlock_page(page);
    
    We directly unlock the page if it has writers, which is the completely
    opposite what we want.
    
    Thankfully the affected call site is only limited to
    extent_write_locked_range(), so it's mostly affecting compressed write.
    
    [FIX]
    Just fix the wrong check condition to fix the bug.
    
    Fixes: e55a0de18572 ("btrfs: rework page locking in __extent_writepage()")
    CC: stable@vger.kernel.org # 5.16
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit f7f497cb702462e8505ff3d8d4e7722ad95626a1
Author: Ritesh Harjani <riteshh@linux.ibm.com>
Date:   Wed Feb 16 12:30:35 2022 +0530

    jbd2: kill t_handle_lock transaction spinlock
    
    This patch kills t_handle_lock transaction spinlock completely from
    jbd2.
    
    To explain the reasoning, currently there were three sites at which
    this spinlock was used.
    
    1. jbd2_journal_wait_updates()
       a. Based on careful code review it can be seen that, we don't need this
          lock here. This is since we wait for any currently ongoing updates
          based on a atomic variable t_updates. And we anyway don't take any
          t_handle_lock while in stop_this_handle().
          i.e.
    
            write_lock(&journal->j_state_lock()
            jbd2_journal_wait_updates()                     stop_this_handle()
                    while (atomic_read(txn->t_updates) {            |
                    DEFINE_WAIT(wait);                              |
                    prepare_to_wait();                              |
                    if (atomic_read(txn->t_updates)                 if (atomic_dec_and_test(txn->t_updates))
                            write_unlock(&journal->j_state_lock);
                            schedule();                                     wake_up()
                            write_lock(&journal->j_state_lock);
                    finish_wait();
               }
            txn->t_state = T_COMMIT
            write_unlock(&journal->j_state_lock);
    
       b.  Also note that between atomic_inc(&txn->t_updates) in
           start_this_handle() and jbd2_journal_wait_updates(), the
           synchronization happens via read_lock(journal->j_state_lock) in
           start_this_handle();
    
    2. jbd2_journal_extend()
       a. jbd2_journal_extend() is called with the handle of each process from
          task_struct. So no lock required in updating member fields of handle_t
    
       b. For member fields of h_transaction, all updates happens only via
          atomic APIs (which is also within read_lock()).
          So, no need of this transaction spinlock.
    
    3. update_t_max_wait()
       Based on Jan suggestion, this can be carefully removed using atomic
       cmpxchg API.
       Note that there can be several processes which are waiting for a new
       transaction to be allocated and started. For doing this only one
       process will succeed in taking write_lock() and allocating a new txn.
       After that all of the process will be updating the t_max_wait (max
       transaction wait time). This can be done via below method w/o taking
       any locks using atomic cmpxchg.
       For more details refer [1]
    
               new = get_new_val();
               old = READ_ONCE(ptr->max_val);
               while (old < new)
                    old = cmpxchg(&ptr->max_val, old, new);
    
    [1]: https://lwn.net/Articles/849237/
    
    Suggested-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Ritesh Harjani <riteshh@linux.ibm.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Link: https://lore.kernel.org/r/d89e599658b4a1f3893a48c6feded200073037fc.1644992076.git.riteshh@linux.ibm.com
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

commit 203a35ebb49cdce377416b0690215d3ce090d364
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Feb 1 20:39:42 2022 +0100

    net, neigh: Do not trigger immediate probes on NUD_FAILED from neigh_managed_work
    
    commit 4a81f6da9cb2d1ef911131a6fd8bd15cb61fc772 upstream.
    
    syzkaller was able to trigger a deadlock for NTF_MANAGED entries [0]:
    
      kworker/0:16/14617 is trying to acquire lock:
      ffffffff8d4dd370 (&tbl->lock){++-.}-{2:2}, at: ___neigh_create+0x9e1/0x2990 net/core/neighbour.c:652
      [...]
      but task is already holding lock:
      ffffffff8d4dd370 (&tbl->lock){++-.}-{2:2}, at: neigh_managed_work+0x35/0x250 net/core/neighbour.c:1572
    
    The neighbor entry turned to NUD_FAILED state, where __neigh_event_send()
    triggered an immediate probe as per commit cd28ca0a3dd1 ("neigh: reduce
    arp latency") via neigh_probe() given table lock was held.
    
    One option to fix this situation is to defer the neigh_probe() back to
    the neigh_timer_handler() similarly as pre cd28ca0a3dd1. For the case
    of NTF_MANAGED, this deferral is acceptable given this only happens on
    actual failure state and regular / expected state is NUD_VALID with the
    entry already present.
    
    The fix adds a parameter to __neigh_event_send() in order to communicate
    whether immediate probe is allowed or disallowed. Existing call-sites
    of neigh_event_send() default as-is to immediate probe. However, the
    neigh_managed_work() disables it via use of neigh_event_send_probe().
    
    [0] <TASK>
      __dump_stack lib/dump_stack.c:88 [inline]
      dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
      print_deadlock_bug kernel/locking/lockdep.c:2956 [inline]
      check_deadlock kernel/locking/lockdep.c:2999 [inline]
      validate_chain kernel/locking/lockdep.c:3788 [inline]
      __lock_acquire.cold+0x149/0x3ab kernel/locking/lockdep.c:5027
      lock_acquire kernel/locking/lockdep.c:5639 [inline]
      lock_acquire+0x1ab/0x510 kernel/locking/lockdep.c:5604
      __raw_write_lock_bh include/linux/rwlock_api_smp.h:202 [inline]
      _raw_write_lock_bh+0x2f/0x40 kernel/locking/spinlock.c:334
      ___neigh_create+0x9e1/0x2990 net/core/neighbour.c:652
      ip6_finish_output2+0x1070/0x14f0 net/ipv6/ip6_output.c:123
      __ip6_finish_output net/ipv6/ip6_output.c:191 [inline]
      __ip6_finish_output+0x61e/0xe90 net/ipv6/ip6_output.c:170
      ip6_finish_output+0x32/0x200 net/ipv6/ip6_output.c:201
      NF_HOOK_COND include/linux/netfilter.h:296 [inline]
      ip6_output+0x1e4/0x530 net/ipv6/ip6_output.c:224
      dst_output include/net/dst.h:451 [inline]
      NF_HOOK include/linux/netfilter.h:307 [inline]
      ndisc_send_skb+0xa99/0x17f0 net/ipv6/ndisc.c:508
      ndisc_send_ns+0x3a9/0x840 net/ipv6/ndisc.c:650
      ndisc_solicit+0x2cd/0x4f0 net/ipv6/ndisc.c:742
      neigh_probe+0xc2/0x110 net/core/neighbour.c:1040
      __neigh_event_send+0x37d/0x1570 net/core/neighbour.c:1201
      neigh_event_send include/net/neighbour.h:470 [inline]
      neigh_managed_work+0x162/0x250 net/core/neighbour.c:1574
      process_one_work+0x9ac/0x1650 kernel/workqueue.c:2307
      worker_thread+0x657/0x1110 kernel/workqueue.c:2454
      kthread+0x2e9/0x3a0 kernel/kthread.c:377
      ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
      </TASK>
    
    Fixes: 7482e3841d52 ("net, neigh: Add NTF_MANAGED flag for managed neighbor entries")
    Reported-by: syzbot+5239d0e1778a500d477a@syzkaller.appspotmail.com
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Roopa Prabhu <roopa@nvidia.com>
    Tested-by: syzbot+5239d0e1778a500d477a@syzkaller.appspotmail.com
    Reviewed-by: David Ahern <dsahern@kernel.org>
    Link: https://lore.kernel.org/r/20220201193942.5055-1-daniel@iogearbox.net
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 94fdd7c02a56d0316d20e417a1141b71a8dcee82
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Feb 6 06:33:48 2022 -0800

    net/smc: use GFP_ATOMIC allocation in smc_pnet_add_eth()
    
    My last patch moved the netdev_tracker_alloc() call to a section
    protected by a write_lock().
    
    I should have replaced GFP_KERNEL with GFP_ATOMIC to avoid the infamous:
    
    BUG: sleeping function called from invalid context at include/linux/sched/mm.h:256
    
    Fixes: 28f922213886 ("net/smc: fix ref_tracker issue in smc_pnet_add()")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 4a81f6da9cb2d1ef911131a6fd8bd15cb61fc772
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Feb 1 20:39:42 2022 +0100

    net, neigh: Do not trigger immediate probes on NUD_FAILED from neigh_managed_work
    
    syzkaller was able to trigger a deadlock for NTF_MANAGED entries [0]:
    
      kworker/0:16/14617 is trying to acquire lock:
      ffffffff8d4dd370 (&tbl->lock){++-.}-{2:2}, at: ___neigh_create+0x9e1/0x2990 net/core/neighbour.c:652
      [...]
      but task is already holding lock:
      ffffffff8d4dd370 (&tbl->lock){++-.}-{2:2}, at: neigh_managed_work+0x35/0x250 net/core/neighbour.c:1572
    
    The neighbor entry turned to NUD_FAILED state, where __neigh_event_send()
    triggered an immediate probe as per commit cd28ca0a3dd1 ("neigh: reduce
    arp latency") via neigh_probe() given table lock was held.
    
    One option to fix this situation is to defer the neigh_probe() back to
    the neigh_timer_handler() similarly as pre cd28ca0a3dd1. For the case
    of NTF_MANAGED, this deferral is acceptable given this only happens on
    actual failure state and regular / expected state is NUD_VALID with the
    entry already present.
    
    The fix adds a parameter to __neigh_event_send() in order to communicate
    whether immediate probe is allowed or disallowed. Existing call-sites
    of neigh_event_send() default as-is to immediate probe. However, the
    neigh_managed_work() disables it via use of neigh_event_send_probe().
    
    [0] <TASK>
      __dump_stack lib/dump_stack.c:88 [inline]
      dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
      print_deadlock_bug kernel/locking/lockdep.c:2956 [inline]
      check_deadlock kernel/locking/lockdep.c:2999 [inline]
      validate_chain kernel/locking/lockdep.c:3788 [inline]
      __lock_acquire.cold+0x149/0x3ab kernel/locking/lockdep.c:5027
      lock_acquire kernel/locking/lockdep.c:5639 [inline]
      lock_acquire+0x1ab/0x510 kernel/locking/lockdep.c:5604
      __raw_write_lock_bh include/linux/rwlock_api_smp.h:202 [inline]
      _raw_write_lock_bh+0x2f/0x40 kernel/locking/spinlock.c:334
      ___neigh_create+0x9e1/0x2990 net/core/neighbour.c:652
      ip6_finish_output2+0x1070/0x14f0 net/ipv6/ip6_output.c:123
      __ip6_finish_output net/ipv6/ip6_output.c:191 [inline]
      __ip6_finish_output+0x61e/0xe90 net/ipv6/ip6_output.c:170
      ip6_finish_output+0x32/0x200 net/ipv6/ip6_output.c:201
      NF_HOOK_COND include/linux/netfilter.h:296 [inline]
      ip6_output+0x1e4/0x530 net/ipv6/ip6_output.c:224
      dst_output include/net/dst.h:451 [inline]
      NF_HOOK include/linux/netfilter.h:307 [inline]
      ndisc_send_skb+0xa99/0x17f0 net/ipv6/ndisc.c:508
      ndisc_send_ns+0x3a9/0x840 net/ipv6/ndisc.c:650
      ndisc_solicit+0x2cd/0x4f0 net/ipv6/ndisc.c:742
      neigh_probe+0xc2/0x110 net/core/neighbour.c:1040
      __neigh_event_send+0x37d/0x1570 net/core/neighbour.c:1201
      neigh_event_send include/net/neighbour.h:470 [inline]
      neigh_managed_work+0x162/0x250 net/core/neighbour.c:1574
      process_one_work+0x9ac/0x1650 kernel/workqueue.c:2307
      worker_thread+0x657/0x1110 kernel/workqueue.c:2454
      kthread+0x2e9/0x3a0 kernel/kthread.c:377
      ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
      </TASK>
    
    Fixes: 7482e3841d52 ("net, neigh: Add NTF_MANAGED flag for managed neighbor entries")
    Reported-by: syzbot+5239d0e1778a500d477a@syzkaller.appspotmail.com
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Roopa Prabhu <roopa@nvidia.com>
    Tested-by: syzbot+5239d0e1778a500d477a@syzkaller.appspotmail.com
    Reviewed-by: David Ahern <dsahern@kernel.org>
    Link: https://lore.kernel.org/r/20220201193942.5055-1-daniel@iogearbox.net
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>

commit 1c52283265a462a100ae63ddf58b4e5884acde86
Merge: 8205ae327e39 6e61dde82e8b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 22 11:28:23 2022 +0200

    Merge branch 'akpm' (patches from Andrew)
    
    Merge yet more updates from Andrew Morton:
     "This is the post-linux-next queue. Material which was based on or
      dependent upon material which was in -next.
    
      69 patches.
    
      Subsystems affected by this patch series: mm (migration and zsmalloc),
      sysctl, proc, and lib"
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (69 commits)
      mm: hide the FRONTSWAP Kconfig symbol
      frontswap: remove support for multiple ops
      mm: mark swap_lock and swap_active_head static
      frontswap: simplify frontswap_register_ops
      frontswap: remove frontswap_test
      mm: simplify try_to_unuse
      frontswap: remove the frontswap exports
      frontswap: simplify frontswap_init
      frontswap: remove frontswap_curr_pages
      frontswap: remove frontswap_shrink
      frontswap: remove frontswap_tmem_exclusive_gets
      frontswap: remove frontswap_writethrough
      mm: remove cleancache
      lib/stackdepot: always do filter_irq_stacks() in stack_depot_save()
      lib/stackdepot: allow optional init and stack_table allocation by kvmalloc()
      proc: remove PDE_DATA() completely
      fs: proc: store PDE()->data into inode->i_private
      zsmalloc: replace get_cpu_var with local_lock
      zsmalloc: replace per zpage lock with pool->migrate_lock
      locking/rwlocks: introduce write_lock_nested
      ...

commit 4a57d6bbaecd28c8175dc5da013009e4158018c2
Author: Minchan Kim <minchan@kernel.org>
Date:   Fri Jan 21 22:14:10 2022 -0800

    locking/rwlocks: introduce write_lock_nested
    
    In preparation for converting bit_spin_lock to rwlock in zsmalloc so
    that multiple writers of zspages can run at the same time but those
    zspages are supposed to be different zspage instance.  Thus, it's not
    deadlock.  This patch adds write_lock_nested to support the case for
    LOCKDEP.
    
    [minchan@kernel.org: fix write_lock_nested for RT]
      Link: https://lkml.kernel.org/r/YZfrMTAXV56HFWJY@google.com
    [bigeasy@linutronix.de: fixup write_lock_nested() implementation]
      Link: https://lkml.kernel.org/r/20211123170134.y6xb7pmpgdn4m3bn@linutronix.de
    
    Link: https://lkml.kernel.org/r/20211115185909.3949505-8-minchan@kernel.org
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Tested-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Sergey Senozhatsky <senozhatsky@chromium.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Naresh Kamboju <naresh.kamboju@linaro.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ba535c1caf3ee78aa7719e9e4b07a0dc1d153b9e
Author: Suren Baghdasaryan <surenb@google.com>
Date:   Fri Jan 14 14:06:22 2022 -0800

    mm/oom_kill: allow process_mrelease to run under mmap_lock protection
    
    With exit_mmap holding mmap_write_lock during free_pgtables call,
    process_mrelease does not need to elevate mm->mm_users in order to
    prevent exit_mmap from destrying pagetables while __oom_reap_task_mm is
    walking the VMA tree.  The change prevents process_mrelease from calling
    the last mmput, which can lead to waiting for IO completion in exit_aio.
    
    Link: https://lkml.kernel.org/r/20211209191325.3069345-3-surenb@google.com
    Signed-off-by: Suren Baghdasaryan <surenb@google.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Jason Gunthorpe <jgg@nvidia.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Christian Brauner <christian@brauner.io>
    Cc: Christian Brauner <christian.brauner@ubuntu.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Florian Weimer <fweimer@redhat.com>
    Cc: Jan Engelhardt <jengelh@inai.de>
    Cc: Jann Horn <jannh@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Tim Murray <timmurray@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 64591e8605d6e2fba2ff38e3227645f039b8893f
Author: Suren Baghdasaryan <surenb@google.com>
Date:   Fri Jan 14 14:06:14 2022 -0800

    mm: protect free_pgtables with mmap_lock write lock in exit_mmap
    
    oom-reaper and process_mrelease system call should protect against races
    with exit_mmap which can destroy page tables while they walk the VMA
    tree.  oom-reaper protects from that race by setting MMF_OOM_VICTIM and
    by relying on exit_mmap to set MMF_OOM_SKIP before taking and releasing
    mmap_write_lock.  process_mrelease has to elevate mm->mm_users to
    prevent such race.
    
    Both oom-reaper and process_mrelease hold mmap_read_lock when walking
    the VMA tree.  The locking rules and mechanisms could be simpler if
    exit_mmap takes mmap_write_lock while executing destructive operations
    such as free_pgtables.
    
    Change exit_mmap to hold the mmap_write_lock when calling unlock_range,
    free_pgtables and remove_vma.  Note also that because oom-reaper checks
    VM_LOCKED flag, unlock_range() should not be allowed to race with it.
    
    Before this patch, remove_vma used to be called with no locks held,
    however with fput being executed asynchronously and vm_ops->close not
    being allowed to hold mmap_lock (it is called from __split_vma with
    mmap_sem held for write), changing that should be fine.
    
    In most cases this lock should be uncontended.  Previously, Kirill
    reported ~4% regression caused by a similar change [1].  We reran the
    same test and although the individual results are quite noisy, the
    percentiles show lower regression with 1.6% being the worst case [2].
    The change allows oom-reaper and process_mrelease to execute safely
    under mmap_read_lock without worries that exit_mmap might destroy page
    tables from under them.
    
    [1] https://lore.kernel.org/all/20170725141723.ivukwhddk2voyhuc@node.shutemov.name/
    [2] https://lore.kernel.org/all/CAJuCfpGC9-c9P40x7oy=jy5SphMcd0o0G_6U1-+JAziGKG6dGA@mail.gmail.com/
    
    Link: https://lkml.kernel.org/r/20211209191325.3069345-1-surenb@google.com
    Signed-off-by: Suren Baghdasaryan <surenb@google.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Christian Brauner <christian@brauner.io>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Christian Brauner <christian.brauner@ubuntu.com>
    Cc: Florian Weimer <fweimer@redhat.com>
    Cc: Jan Engelhardt <jengelh@inai.de>
    Cc: Tim Murray <timmurray@google.com>
    Cc: Jason Gunthorpe <jgg@nvidia.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit e5e1c1741b3de3f8d06fe4b700d83709a7da0610
Author: Filipe Manana <fdmanana@suse.com>
Date:   Thu Dec 2 10:30:37 2021 +0000

    btrfs: remove useless condition check before splitting leaf
    
    When inserting a key, we check if the write_lock_level is less than 1,
    and if so we set it to 1, release the path and retry the tree traversal.
    
    However that is unnecessary, because when ins_len is greater than 0, we
    know that write_lock_level can never be less than 1.
    
    The logic to retry is also buggy, because in case ins_len was decremented,
    due to an exact key match and the search is not meant for item extension
    (path->search_for_extension is 0), we retry without incrementing ins_len,
    which would make the next retry decrement it again by the same amount.
    
    So remove the check for write_lock_level being less than 1 and add an
    assertion to assert it's always >= 1.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit dfd5b60b5342b6b505a104e48f08ad9b9bdbbd7b
Author: Andrea Righi <andrea.righi@canonical.com>
Date:   Mon Nov 29 00:08:13 2021 -0800

    Input: elantech - fix stack out of bound access in elantech_change_report_id()
    
    [ Upstream commit 1d72d9f960ccf1052a0630a68c3d358791dbdaaa ]
    
    The array param[] in elantech_change_report_id() must be at least 3
    bytes, because elantech_read_reg_params() is calling ps2_command() with
    PSMOUSE_CMD_GETINFO, that is going to access 3 bytes from param[], but
    it's defined in the stack as an array of 2 bytes, therefore we have a
    potential stack out-of-bounds access here, also confirmed by KASAN:
    
    [    6.512374] BUG: KASAN: stack-out-of-bounds in __ps2_command+0x372/0x7e0
    [    6.512397] Read of size 1 at addr ffff8881024d77c2 by task kworker/2:1/118
    
    [    6.512416] CPU: 2 PID: 118 Comm: kworker/2:1 Not tainted 5.13.0-22-generic #22+arighi20211110
    [    6.512428] Hardware name: LENOVO 20T8000QGE/20T8000QGE, BIOS R1AET32W (1.08 ) 08/14/2020
    [    6.512436] Workqueue: events_long serio_handle_event
    [    6.512453] Call Trace:
    [    6.512462]  show_stack+0x52/0x58
    [    6.512474]  dump_stack+0xa1/0xd3
    [    6.512487]  print_address_description.constprop.0+0x1d/0x140
    [    6.512502]  ? __ps2_command+0x372/0x7e0
    [    6.512516]  __kasan_report.cold+0x7d/0x112
    [    6.512527]  ? _raw_write_lock_irq+0x20/0xd0
    [    6.512539]  ? __ps2_command+0x372/0x7e0
    [    6.512552]  kasan_report+0x3c/0x50
    [    6.512564]  __asan_load1+0x6a/0x70
    [    6.512575]  __ps2_command+0x372/0x7e0
    [    6.512589]  ? ps2_drain+0x240/0x240
    [    6.512601]  ? dev_printk_emit+0xa2/0xd3
    [    6.512612]  ? dev_vprintk_emit+0xc5/0xc5
    [    6.512621]  ? __kasan_check_write+0x14/0x20
    [    6.512634]  ? mutex_lock+0x8f/0xe0
    [    6.512643]  ? __mutex_lock_slowpath+0x20/0x20
    [    6.512655]  ps2_command+0x52/0x90
    [    6.512670]  elantech_ps2_command+0x4f/0xc0 [psmouse]
    [    6.512734]  elantech_change_report_id+0x1e6/0x256 [psmouse]
    [    6.512799]  ? elantech_report_trackpoint.constprop.0.cold+0xd/0xd [psmouse]
    [    6.512863]  ? ps2_command+0x7f/0x90
    [    6.512877]  elantech_query_info.cold+0x6bd/0x9ed [psmouse]
    [    6.512943]  ? elantech_setup_ps2+0x460/0x460 [psmouse]
    [    6.513005]  ? psmouse_reset+0x69/0xb0 [psmouse]
    [    6.513064]  ? psmouse_attr_set_helper+0x2a0/0x2a0 [psmouse]
    [    6.513122]  ? phys_pmd_init+0x30e/0x521
    [    6.513137]  elantech_init+0x8a/0x200 [psmouse]
    [    6.513200]  ? elantech_init_ps2+0xf0/0xf0 [psmouse]
    [    6.513249]  ? elantech_query_info+0x440/0x440 [psmouse]
    [    6.513296]  ? synaptics_send_cmd+0x60/0x60 [psmouse]
    [    6.513342]  ? elantech_query_info+0x440/0x440 [psmouse]
    [    6.513388]  ? psmouse_try_protocol+0x11e/0x170 [psmouse]
    [    6.513432]  psmouse_extensions+0x65d/0x6e0 [psmouse]
    [    6.513476]  ? psmouse_try_protocol+0x170/0x170 [psmouse]
    [    6.513519]  ? mutex_unlock+0x22/0x40
    [    6.513526]  ? ps2_command+0x7f/0x90
    [    6.513536]  ? psmouse_probe+0xa3/0xf0 [psmouse]
    [    6.513580]  psmouse_switch_protocol+0x27d/0x2e0 [psmouse]
    [    6.513624]  psmouse_connect+0x272/0x530 [psmouse]
    [    6.513669]  serio_driver_probe+0x55/0x70
    [    6.513679]  really_probe+0x190/0x720
    [    6.513689]  driver_probe_device+0x160/0x1f0
    [    6.513697]  device_driver_attach+0x119/0x130
    [    6.513705]  ? device_driver_attach+0x130/0x130
    [    6.513713]  __driver_attach+0xe7/0x1a0
    [    6.513720]  ? device_driver_attach+0x130/0x130
    [    6.513728]  bus_for_each_dev+0xfb/0x150
    [    6.513738]  ? subsys_dev_iter_exit+0x10/0x10
    [    6.513748]  ? _raw_write_unlock_bh+0x30/0x30
    [    6.513757]  driver_attach+0x2d/0x40
    [    6.513764]  serio_handle_event+0x199/0x3d0
    [    6.513775]  process_one_work+0x471/0x740
    [    6.513785]  worker_thread+0x2d2/0x790
    [    6.513794]  ? process_one_work+0x740/0x740
    [    6.513802]  kthread+0x1b4/0x1e0
    [    6.513809]  ? set_kthread_struct+0x80/0x80
    [    6.513816]  ret_from_fork+0x22/0x30
    
    [    6.513832] The buggy address belongs to the page:
    [    6.513838] page:00000000bc35e189 refcount:0 mapcount:0 mapping:0000000000000000 index:0x0 pfn:0x1024d7
    [    6.513847] flags: 0x17ffffc0000000(node=0|zone=2|lastcpupid=0x1fffff)
    [    6.513860] raw: 0017ffffc0000000 dead000000000100 dead000000000122 0000000000000000
    [    6.513867] raw: 0000000000000000 0000000000000000 00000000ffffffff 0000000000000000
    [    6.513872] page dumped because: kasan: bad access detected
    
    [    6.513879] addr ffff8881024d77c2 is located in stack of task kworker/2:1/118 at offset 34 in frame:
    [    6.513887]  elantech_change_report_id+0x0/0x256 [psmouse]
    
    [    6.513941] this frame has 1 object:
    [    6.513947]  [32, 34) 'param'
    
    [    6.513956] Memory state around the buggy address:
    [    6.513962]  ffff8881024d7680: f2 f2 f2 f2 f2 00 00 f3 f3 00 00 00 00 00 00 00
    [    6.513969]  ffff8881024d7700: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [    6.513976] >ffff8881024d7780: 00 00 00 00 f1 f1 f1 f1 02 f3 f3 f3 00 00 00 00
    [    6.513982]                                            ^
    [    6.513988]  ffff8881024d7800: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [    6.513995]  ffff8881024d7880: 00 f1 f1 f1 f1 03 f2 03 f2 03 f3 f3 f3 00 00 00
    [    6.514000] ==================================================================
    
    Define param[] in elantech_change_report_id() as an array of 3 bytes to
    prevent the out-of-bounds access in the stack.
    
    Fixes: e4c9062717fe ("Input: elantech - fix protocol errors for some trackpoints in SMBus mode")
    BugLink: https://bugs.launchpad.net/bugs/1945590
    Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
    Reviewed-by: Wolfram Sang <wsa@kernel.org>
    Link: https://lore.kernel.org/r/20211116095559.24395-1-andrea.righi@canonical.com
    Signed-off-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 676c572439e58b7ee6b7ca3f1e5595382921045c
Author: Andrea Righi <andrea.righi@canonical.com>
Date:   Mon Nov 29 00:08:13 2021 -0800

    Input: elantech - fix stack out of bound access in elantech_change_report_id()
    
    [ Upstream commit 1d72d9f960ccf1052a0630a68c3d358791dbdaaa ]
    
    The array param[] in elantech_change_report_id() must be at least 3
    bytes, because elantech_read_reg_params() is calling ps2_command() with
    PSMOUSE_CMD_GETINFO, that is going to access 3 bytes from param[], but
    it's defined in the stack as an array of 2 bytes, therefore we have a
    potential stack out-of-bounds access here, also confirmed by KASAN:
    
    [    6.512374] BUG: KASAN: stack-out-of-bounds in __ps2_command+0x372/0x7e0
    [    6.512397] Read of size 1 at addr ffff8881024d77c2 by task kworker/2:1/118
    
    [    6.512416] CPU: 2 PID: 118 Comm: kworker/2:1 Not tainted 5.13.0-22-generic #22+arighi20211110
    [    6.512428] Hardware name: LENOVO 20T8000QGE/20T8000QGE, BIOS R1AET32W (1.08 ) 08/14/2020
    [    6.512436] Workqueue: events_long serio_handle_event
    [    6.512453] Call Trace:
    [    6.512462]  show_stack+0x52/0x58
    [    6.512474]  dump_stack+0xa1/0xd3
    [    6.512487]  print_address_description.constprop.0+0x1d/0x140
    [    6.512502]  ? __ps2_command+0x372/0x7e0
    [    6.512516]  __kasan_report.cold+0x7d/0x112
    [    6.512527]  ? _raw_write_lock_irq+0x20/0xd0
    [    6.512539]  ? __ps2_command+0x372/0x7e0
    [    6.512552]  kasan_report+0x3c/0x50
    [    6.512564]  __asan_load1+0x6a/0x70
    [    6.512575]  __ps2_command+0x372/0x7e0
    [    6.512589]  ? ps2_drain+0x240/0x240
    [    6.512601]  ? dev_printk_emit+0xa2/0xd3
    [    6.512612]  ? dev_vprintk_emit+0xc5/0xc5
    [    6.512621]  ? __kasan_check_write+0x14/0x20
    [    6.512634]  ? mutex_lock+0x8f/0xe0
    [    6.512643]  ? __mutex_lock_slowpath+0x20/0x20
    [    6.512655]  ps2_command+0x52/0x90
    [    6.512670]  elantech_ps2_command+0x4f/0xc0 [psmouse]
    [    6.512734]  elantech_change_report_id+0x1e6/0x256 [psmouse]
    [    6.512799]  ? elantech_report_trackpoint.constprop.0.cold+0xd/0xd [psmouse]
    [    6.512863]  ? ps2_command+0x7f/0x90
    [    6.512877]  elantech_query_info.cold+0x6bd/0x9ed [psmouse]
    [    6.512943]  ? elantech_setup_ps2+0x460/0x460 [psmouse]
    [    6.513005]  ? psmouse_reset+0x69/0xb0 [psmouse]
    [    6.513064]  ? psmouse_attr_set_helper+0x2a0/0x2a0 [psmouse]
    [    6.513122]  ? phys_pmd_init+0x30e/0x521
    [    6.513137]  elantech_init+0x8a/0x200 [psmouse]
    [    6.513200]  ? elantech_init_ps2+0xf0/0xf0 [psmouse]
    [    6.513249]  ? elantech_query_info+0x440/0x440 [psmouse]
    [    6.513296]  ? synaptics_send_cmd+0x60/0x60 [psmouse]
    [    6.513342]  ? elantech_query_info+0x440/0x440 [psmouse]
    [    6.513388]  ? psmouse_try_protocol+0x11e/0x170 [psmouse]
    [    6.513432]  psmouse_extensions+0x65d/0x6e0 [psmouse]
    [    6.513476]  ? psmouse_try_protocol+0x170/0x170 [psmouse]
    [    6.513519]  ? mutex_unlock+0x22/0x40
    [    6.513526]  ? ps2_command+0x7f/0x90
    [    6.513536]  ? psmouse_probe+0xa3/0xf0 [psmouse]
    [    6.513580]  psmouse_switch_protocol+0x27d/0x2e0 [psmouse]
    [    6.513624]  psmouse_connect+0x272/0x530 [psmouse]
    [    6.513669]  serio_driver_probe+0x55/0x70
    [    6.513679]  really_probe+0x190/0x720
    [    6.513689]  driver_probe_device+0x160/0x1f0
    [    6.513697]  device_driver_attach+0x119/0x130
    [    6.513705]  ? device_driver_attach+0x130/0x130
    [    6.513713]  __driver_attach+0xe7/0x1a0
    [    6.513720]  ? device_driver_attach+0x130/0x130
    [    6.513728]  bus_for_each_dev+0xfb/0x150
    [    6.513738]  ? subsys_dev_iter_exit+0x10/0x10
    [    6.513748]  ? _raw_write_unlock_bh+0x30/0x30
    [    6.513757]  driver_attach+0x2d/0x40
    [    6.513764]  serio_handle_event+0x199/0x3d0
    [    6.513775]  process_one_work+0x471/0x740
    [    6.513785]  worker_thread+0x2d2/0x790
    [    6.513794]  ? process_one_work+0x740/0x740
    [    6.513802]  kthread+0x1b4/0x1e0
    [    6.513809]  ? set_kthread_struct+0x80/0x80
    [    6.513816]  ret_from_fork+0x22/0x30
    
    [    6.513832] The buggy address belongs to the page:
    [    6.513838] page:00000000bc35e189 refcount:0 mapcount:0 mapping:0000000000000000 index:0x0 pfn:0x1024d7
    [    6.513847] flags: 0x17ffffc0000000(node=0|zone=2|lastcpupid=0x1fffff)
    [    6.513860] raw: 0017ffffc0000000 dead000000000100 dead000000000122 0000000000000000
    [    6.513867] raw: 0000000000000000 0000000000000000 00000000ffffffff 0000000000000000
    [    6.513872] page dumped because: kasan: bad access detected
    
    [    6.513879] addr ffff8881024d77c2 is located in stack of task kworker/2:1/118 at offset 34 in frame:
    [    6.513887]  elantech_change_report_id+0x0/0x256 [psmouse]
    
    [    6.513941] this frame has 1 object:
    [    6.513947]  [32, 34) 'param'
    
    [    6.513956] Memory state around the buggy address:
    [    6.513962]  ffff8881024d7680: f2 f2 f2 f2 f2 00 00 f3 f3 00 00 00 00 00 00 00
    [    6.513969]  ffff8881024d7700: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [    6.513976] >ffff8881024d7780: 00 00 00 00 f1 f1 f1 f1 02 f3 f3 f3 00 00 00 00
    [    6.513982]                                            ^
    [    6.513988]  ffff8881024d7800: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [    6.513995]  ffff8881024d7880: 00 f1 f1 f1 f1 03 f2 03 f2 03 f3 f3 f3 00 00 00
    [    6.514000] ==================================================================
    
    Define param[] in elantech_change_report_id() as an array of 3 bytes to
    prevent the out-of-bounds access in the stack.
    
    Fixes: e4c9062717fe ("Input: elantech - fix protocol errors for some trackpoints in SMBus mode")
    BugLink: https://bugs.launchpad.net/bugs/1945590
    Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
    Reviewed-by: Wolfram Sang <wsa@kernel.org>
    Link: https://lore.kernel.org/r/20211116095559.24395-1-andrea.righi@canonical.com
    Signed-off-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit a7f95328c6f0afffdc4555f16e3bbab8bbf0d9be
Author: Andrea Righi <andrea.righi@canonical.com>
Date:   Mon Nov 29 00:08:13 2021 -0800

    Input: elantech - fix stack out of bound access in elantech_change_report_id()
    
    [ Upstream commit 1d72d9f960ccf1052a0630a68c3d358791dbdaaa ]
    
    The array param[] in elantech_change_report_id() must be at least 3
    bytes, because elantech_read_reg_params() is calling ps2_command() with
    PSMOUSE_CMD_GETINFO, that is going to access 3 bytes from param[], but
    it's defined in the stack as an array of 2 bytes, therefore we have a
    potential stack out-of-bounds access here, also confirmed by KASAN:
    
    [    6.512374] BUG: KASAN: stack-out-of-bounds in __ps2_command+0x372/0x7e0
    [    6.512397] Read of size 1 at addr ffff8881024d77c2 by task kworker/2:1/118
    
    [    6.512416] CPU: 2 PID: 118 Comm: kworker/2:1 Not tainted 5.13.0-22-generic #22+arighi20211110
    [    6.512428] Hardware name: LENOVO 20T8000QGE/20T8000QGE, BIOS R1AET32W (1.08 ) 08/14/2020
    [    6.512436] Workqueue: events_long serio_handle_event
    [    6.512453] Call Trace:
    [    6.512462]  show_stack+0x52/0x58
    [    6.512474]  dump_stack+0xa1/0xd3
    [    6.512487]  print_address_description.constprop.0+0x1d/0x140
    [    6.512502]  ? __ps2_command+0x372/0x7e0
    [    6.512516]  __kasan_report.cold+0x7d/0x112
    [    6.512527]  ? _raw_write_lock_irq+0x20/0xd0
    [    6.512539]  ? __ps2_command+0x372/0x7e0
    [    6.512552]  kasan_report+0x3c/0x50
    [    6.512564]  __asan_load1+0x6a/0x70
    [    6.512575]  __ps2_command+0x372/0x7e0
    [    6.512589]  ? ps2_drain+0x240/0x240
    [    6.512601]  ? dev_printk_emit+0xa2/0xd3
    [    6.512612]  ? dev_vprintk_emit+0xc5/0xc5
    [    6.512621]  ? __kasan_check_write+0x14/0x20
    [    6.512634]  ? mutex_lock+0x8f/0xe0
    [    6.512643]  ? __mutex_lock_slowpath+0x20/0x20
    [    6.512655]  ps2_command+0x52/0x90
    [    6.512670]  elantech_ps2_command+0x4f/0xc0 [psmouse]
    [    6.512734]  elantech_change_report_id+0x1e6/0x256 [psmouse]
    [    6.512799]  ? elantech_report_trackpoint.constprop.0.cold+0xd/0xd [psmouse]
    [    6.512863]  ? ps2_command+0x7f/0x90
    [    6.512877]  elantech_query_info.cold+0x6bd/0x9ed [psmouse]
    [    6.512943]  ? elantech_setup_ps2+0x460/0x460 [psmouse]
    [    6.513005]  ? psmouse_reset+0x69/0xb0 [psmouse]
    [    6.513064]  ? psmouse_attr_set_helper+0x2a0/0x2a0 [psmouse]
    [    6.513122]  ? phys_pmd_init+0x30e/0x521
    [    6.513137]  elantech_init+0x8a/0x200 [psmouse]
    [    6.513200]  ? elantech_init_ps2+0xf0/0xf0 [psmouse]
    [    6.513249]  ? elantech_query_info+0x440/0x440 [psmouse]
    [    6.513296]  ? synaptics_send_cmd+0x60/0x60 [psmouse]
    [    6.513342]  ? elantech_query_info+0x440/0x440 [psmouse]
    [    6.513388]  ? psmouse_try_protocol+0x11e/0x170 [psmouse]
    [    6.513432]  psmouse_extensions+0x65d/0x6e0 [psmouse]
    [    6.513476]  ? psmouse_try_protocol+0x170/0x170 [psmouse]
    [    6.513519]  ? mutex_unlock+0x22/0x40
    [    6.513526]  ? ps2_command+0x7f/0x90
    [    6.513536]  ? psmouse_probe+0xa3/0xf0 [psmouse]
    [    6.513580]  psmouse_switch_protocol+0x27d/0x2e0 [psmouse]
    [    6.513624]  psmouse_connect+0x272/0x530 [psmouse]
    [    6.513669]  serio_driver_probe+0x55/0x70
    [    6.513679]  really_probe+0x190/0x720
    [    6.513689]  driver_probe_device+0x160/0x1f0
    [    6.513697]  device_driver_attach+0x119/0x130
    [    6.513705]  ? device_driver_attach+0x130/0x130
    [    6.513713]  __driver_attach+0xe7/0x1a0
    [    6.513720]  ? device_driver_attach+0x130/0x130
    [    6.513728]  bus_for_each_dev+0xfb/0x150
    [    6.513738]  ? subsys_dev_iter_exit+0x10/0x10
    [    6.513748]  ? _raw_write_unlock_bh+0x30/0x30
    [    6.513757]  driver_attach+0x2d/0x40
    [    6.513764]  serio_handle_event+0x199/0x3d0
    [    6.513775]  process_one_work+0x471/0x740
    [    6.513785]  worker_thread+0x2d2/0x790
    [    6.513794]  ? process_one_work+0x740/0x740
    [    6.513802]  kthread+0x1b4/0x1e0
    [    6.513809]  ? set_kthread_struct+0x80/0x80
    [    6.513816]  ret_from_fork+0x22/0x30
    
    [    6.513832] The buggy address belongs to the page:
    [    6.513838] page:00000000bc35e189 refcount:0 mapcount:0 mapping:0000000000000000 index:0x0 pfn:0x1024d7
    [    6.513847] flags: 0x17ffffc0000000(node=0|zone=2|lastcpupid=0x1fffff)
    [    6.513860] raw: 0017ffffc0000000 dead000000000100 dead000000000122 0000000000000000
    [    6.513867] raw: 0000000000000000 0000000000000000 00000000ffffffff 0000000000000000
    [    6.513872] page dumped because: kasan: bad access detected
    
    [    6.513879] addr ffff8881024d77c2 is located in stack of task kworker/2:1/118 at offset 34 in frame:
    [    6.513887]  elantech_change_report_id+0x0/0x256 [psmouse]
    
    [    6.513941] this frame has 1 object:
    [    6.513947]  [32, 34) 'param'
    
    [    6.513956] Memory state around the buggy address:
    [    6.513962]  ffff8881024d7680: f2 f2 f2 f2 f2 00 00 f3 f3 00 00 00 00 00 00 00
    [    6.513969]  ffff8881024d7700: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [    6.513976] >ffff8881024d7780: 00 00 00 00 f1 f1 f1 f1 02 f3 f3 f3 00 00 00 00
    [    6.513982]                                            ^
    [    6.513988]  ffff8881024d7800: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [    6.513995]  ffff8881024d7880: 00 f1 f1 f1 f1 03 f2 03 f2 03 f3 f3 f3 00 00 00
    [    6.514000] ==================================================================
    
    Define param[] in elantech_change_report_id() as an array of 3 bytes to
    prevent the out-of-bounds access in the stack.
    
    Fixes: e4c9062717fe ("Input: elantech - fix protocol errors for some trackpoints in SMBus mode")
    BugLink: https://bugs.launchpad.net/bugs/1945590
    Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
    Reviewed-by: Wolfram Sang <wsa@kernel.org>
    Link: https://lore.kernel.org/r/20211116095559.24395-1-andrea.righi@canonical.com
    Signed-off-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit fd888e85fe6b661e78044dddfec0be5271afa626
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Nov 26 17:15:29 2021 +0100

    net: Write lock dev_base_lock without disabling bottom halves.
    
    The writer acquires dev_base_lock with disabled bottom halves.
    The reader can acquire dev_base_lock without disabling bottom halves
    because there is no writer in softirq context.
    
    On PREEMPT_RT the softirqs are preemptible and local_bh_disable() acts
    as a lock to ensure that resources, that are protected by disabling
    bottom halves, remain protected.
    This leads to a circular locking dependency if the lock acquired with
    disabled bottom halves (as in write_lock_bh()) and somewhere else with
    enabled bottom halves (as by read_lock() in netstat_show()) followed by
    disabling bottom halves (cxgb_get_stats() -> t4_wr_mbox_meat_timeout()
    -> spin_lock_bh()). This is the reverse locking order.
    
    All read_lock() invocation are from sysfs callback which are not invoked
    from softirq context. Therefore there is no need to disable bottom
    halves while acquiring a write lock.
    
    Acquire the write lock of dev_base_lock without disabling bottom halves.
    
    Reported-by: Pei Zhang <pezhang@redhat.com>
    Reported-by: Luis Claudio R. Goncalves <lgoncalv@redhat.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 1d72d9f960ccf1052a0630a68c3d358791dbdaaa
Author: Andrea Righi <andrea.righi@canonical.com>
Date:   Mon Nov 29 00:08:13 2021 -0800

    Input: elantech - fix stack out of bound access in elantech_change_report_id()
    
    The array param[] in elantech_change_report_id() must be at least 3
    bytes, because elantech_read_reg_params() is calling ps2_command() with
    PSMOUSE_CMD_GETINFO, that is going to access 3 bytes from param[], but
    it's defined in the stack as an array of 2 bytes, therefore we have a
    potential stack out-of-bounds access here, also confirmed by KASAN:
    
    [    6.512374] BUG: KASAN: stack-out-of-bounds in __ps2_command+0x372/0x7e0
    [    6.512397] Read of size 1 at addr ffff8881024d77c2 by task kworker/2:1/118
    
    [    6.512416] CPU: 2 PID: 118 Comm: kworker/2:1 Not tainted 5.13.0-22-generic #22+arighi20211110
    [    6.512428] Hardware name: LENOVO 20T8000QGE/20T8000QGE, BIOS R1AET32W (1.08 ) 08/14/2020
    [    6.512436] Workqueue: events_long serio_handle_event
    [    6.512453] Call Trace:
    [    6.512462]  show_stack+0x52/0x58
    [    6.512474]  dump_stack+0xa1/0xd3
    [    6.512487]  print_address_description.constprop.0+0x1d/0x140
    [    6.512502]  ? __ps2_command+0x372/0x7e0
    [    6.512516]  __kasan_report.cold+0x7d/0x112
    [    6.512527]  ? _raw_write_lock_irq+0x20/0xd0
    [    6.512539]  ? __ps2_command+0x372/0x7e0
    [    6.512552]  kasan_report+0x3c/0x50
    [    6.512564]  __asan_load1+0x6a/0x70
    [    6.512575]  __ps2_command+0x372/0x7e0
    [    6.512589]  ? ps2_drain+0x240/0x240
    [    6.512601]  ? dev_printk_emit+0xa2/0xd3
    [    6.512612]  ? dev_vprintk_emit+0xc5/0xc5
    [    6.512621]  ? __kasan_check_write+0x14/0x20
    [    6.512634]  ? mutex_lock+0x8f/0xe0
    [    6.512643]  ? __mutex_lock_slowpath+0x20/0x20
    [    6.512655]  ps2_command+0x52/0x90
    [    6.512670]  elantech_ps2_command+0x4f/0xc0 [psmouse]
    [    6.512734]  elantech_change_report_id+0x1e6/0x256 [psmouse]
    [    6.512799]  ? elantech_report_trackpoint.constprop.0.cold+0xd/0xd [psmouse]
    [    6.512863]  ? ps2_command+0x7f/0x90
    [    6.512877]  elantech_query_info.cold+0x6bd/0x9ed [psmouse]
    [    6.512943]  ? elantech_setup_ps2+0x460/0x460 [psmouse]
    [    6.513005]  ? psmouse_reset+0x69/0xb0 [psmouse]
    [    6.513064]  ? psmouse_attr_set_helper+0x2a0/0x2a0 [psmouse]
    [    6.513122]  ? phys_pmd_init+0x30e/0x521
    [    6.513137]  elantech_init+0x8a/0x200 [psmouse]
    [    6.513200]  ? elantech_init_ps2+0xf0/0xf0 [psmouse]
    [    6.513249]  ? elantech_query_info+0x440/0x440 [psmouse]
    [    6.513296]  ? synaptics_send_cmd+0x60/0x60 [psmouse]
    [    6.513342]  ? elantech_query_info+0x440/0x440 [psmouse]
    [    6.513388]  ? psmouse_try_protocol+0x11e/0x170 [psmouse]
    [    6.513432]  psmouse_extensions+0x65d/0x6e0 [psmouse]
    [    6.513476]  ? psmouse_try_protocol+0x170/0x170 [psmouse]
    [    6.513519]  ? mutex_unlock+0x22/0x40
    [    6.513526]  ? ps2_command+0x7f/0x90
    [    6.513536]  ? psmouse_probe+0xa3/0xf0 [psmouse]
    [    6.513580]  psmouse_switch_protocol+0x27d/0x2e0 [psmouse]
    [    6.513624]  psmouse_connect+0x272/0x530 [psmouse]
    [    6.513669]  serio_driver_probe+0x55/0x70
    [    6.513679]  really_probe+0x190/0x720
    [    6.513689]  driver_probe_device+0x160/0x1f0
    [    6.513697]  device_driver_attach+0x119/0x130
    [    6.513705]  ? device_driver_attach+0x130/0x130
    [    6.513713]  __driver_attach+0xe7/0x1a0
    [    6.513720]  ? device_driver_attach+0x130/0x130
    [    6.513728]  bus_for_each_dev+0xfb/0x150
    [    6.513738]  ? subsys_dev_iter_exit+0x10/0x10
    [    6.513748]  ? _raw_write_unlock_bh+0x30/0x30
    [    6.513757]  driver_attach+0x2d/0x40
    [    6.513764]  serio_handle_event+0x199/0x3d0
    [    6.513775]  process_one_work+0x471/0x740
    [    6.513785]  worker_thread+0x2d2/0x790
    [    6.513794]  ? process_one_work+0x740/0x740
    [    6.513802]  kthread+0x1b4/0x1e0
    [    6.513809]  ? set_kthread_struct+0x80/0x80
    [    6.513816]  ret_from_fork+0x22/0x30
    
    [    6.513832] The buggy address belongs to the page:
    [    6.513838] page:00000000bc35e189 refcount:0 mapcount:0 mapping:0000000000000000 index:0x0 pfn:0x1024d7
    [    6.513847] flags: 0x17ffffc0000000(node=0|zone=2|lastcpupid=0x1fffff)
    [    6.513860] raw: 0017ffffc0000000 dead000000000100 dead000000000122 0000000000000000
    [    6.513867] raw: 0000000000000000 0000000000000000 00000000ffffffff 0000000000000000
    [    6.513872] page dumped because: kasan: bad access detected
    
    [    6.513879] addr ffff8881024d77c2 is located in stack of task kworker/2:1/118 at offset 34 in frame:
    [    6.513887]  elantech_change_report_id+0x0/0x256 [psmouse]
    
    [    6.513941] this frame has 1 object:
    [    6.513947]  [32, 34) 'param'
    
    [    6.513956] Memory state around the buggy address:
    [    6.513962]  ffff8881024d7680: f2 f2 f2 f2 f2 00 00 f3 f3 00 00 00 00 00 00 00
    [    6.513969]  ffff8881024d7700: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [    6.513976] >ffff8881024d7780: 00 00 00 00 f1 f1 f1 f1 02 f3 f3 f3 00 00 00 00
    [    6.513982]                                            ^
    [    6.513988]  ffff8881024d7800: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [    6.513995]  ffff8881024d7880: 00 f1 f1 f1 f1 03 f2 03 f2 03 f3 f3 f3 00 00 00
    [    6.514000] ==================================================================
    
    Define param[] in elantech_change_report_id() as an array of 3 bytes to
    prevent the out-of-bounds access in the stack.
    
    Fixes: e4c9062717fe ("Input: elantech - fix protocol errors for some trackpoints in SMBus mode")
    BugLink: https://bugs.launchpad.net/bugs/1945590
    Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
    Reviewed-by: Wolfram Sang <wsa@kernel.org>
    Link: https://lore.kernel.org/r/20211116095559.24395-1-andrea.righi@canonical.com
    Signed-off-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>

commit 337546e83fc7e50917f44846beee936abb9c9f1f
Author: Suren Baghdasaryan <surenb@google.com>
Date:   Thu Oct 28 14:36:14 2021 -0700

    mm/oom_kill.c: prevent a race between process_mrelease and exit_mmap
    
    Race between process_mrelease and exit_mmap, where free_pgtables is
    called while __oom_reap_task_mm is in progress, leads to kernel crash
    during pte_offset_map_lock call.  oom-reaper avoids this race by setting
    MMF_OOM_VICTIM flag and causing exit_mmap to take and release
    mmap_write_lock, blocking it until oom-reaper releases mmap_read_lock.
    
    Reusing MMF_OOM_VICTIM for process_mrelease would be the simplest way to
    fix this race, however that would be considered a hack.  Fix this race
    by elevating mm->mm_users and preventing exit_mmap from executing until
    process_mrelease is finished.  Patch slightly refactors the code to
    adapt for a possible mmget_not_zero failure.
    
    This fix has considerable negative impact on process_mrelease
    performance and will likely need later optimization.
    
    Link: https://lkml.kernel.org/r/20211022014658.263508-1-surenb@google.com
    Fixes: 884a7e5964e0 ("mm: introduce process_mrelease system call")
    Signed-off-by: Suren Baghdasaryan <surenb@google.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Matthew Wilcox (Oracle) <willy@infradead.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Christian Brauner <christian@brauner.io>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Christian Brauner <christian.brauner@ubuntu.com>
    Cc: Florian Weimer <fweimer@redhat.com>
    Cc: Jan Engelhardt <jengelh@inai.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit e55a0de185726ca43e8a31d363ad73c4f0a6770b
Author: Qu Wenruo <wqu@suse.com>
Date:   Mon Sep 27 15:22:05 2021 +0800

    btrfs: rework page locking in __extent_writepage()
    
    Pages passed to __extent_writepage() are always locked, but they may be
    locked by different functions.
    
    There are two types of locked page for __extent_writepage():
    
    - Page locked by plain lock_page()
      It should not have any subpage::writers count.
      Can be unlocked by unlock_page().
      This is the most common locked page for __extent_writepage() called
      inside extent_write_cache_pages() or extent_write_full_page().
      Rarer cases include the @locked_page from extent_write_locked_range().
    
    - Page locked by lock_delalloc_pages()
      There is only one caller, all pages except @locked_page for
      extent_write_locked_range().
      In this case, we have to call subpage helper to handle the case.
    
    So here we introduce a helper, btrfs_page_unlock_writer(), to allow
    __extent_writepage() to unlock different locked pages.
    
    And since for all other callers of __extent_writepage() their pages are
    ensured to be locked by lock_page(), also add an extra check for
    epd::extent_locked to unlock such pages directly.
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 2b83a0eea5a15d2651953a8cbb1afd0608b6e588
Author: Qu Wenruo <wqu@suse.com>
Date:   Mon Sep 27 15:22:03 2021 +0800

    btrfs: factor uncompressed async extent submission code into a new helper
    
    Introduce a new helper, submit_uncompressed_range(), for async cow cases
    where we fallback to COW.
    
    There are some new updates introduced to the helper:
    
    - Proper locked_page detection
      It's possible that the async_extent range doesn't cover the locked
      page.  In that case we shouldn't unlock the locked page.
    
      In the new helper, we will ensure that we only unlock the locked page
      when:
    
      * The locked page covers part of the async_extent range
      * The locked page is not unlocked by cow_file_range() nor
        extent_write_locked_range()
    
      This also means extra comments are added focusing on the page locking.
    
    - Add extra comment on some rare parameter used.
      We use @unlock_page = 0 for cow_file_range(), where only two call
      sites doing the same thing, including the new helper.
    
      It's definitely worth some comments.
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 66448b9d5b6840c230d81cbf10d6ffaeece2d71b
Author: Qu Wenruo <wqu@suse.com>
Date:   Mon Sep 27 15:22:02 2021 +0800

    btrfs: subpage: make extent_write_locked_range() compatible
    
    There are two sites are not subpage compatible yet for
    extent_write_locked_range():
    
    - How @nr_pages are calculated
      For subpage we can have the following range with 64K page size:
    
      0   32K  64K   96K 128K
      |   |////|/////|   |
    
      In that case, although 96K - 32K == 64K, thus it looks like one page
      is enough, but the range spans two pages, not one.
    
      Fix it by doing proper round_up() and round_down() to calculate
      @nr_pages.
    
      Also add some extra ASSERT()s to ensure the range passed in is already
      aligned.
    
    - How the page end is calculated
      Currently we just use cur + PAGE_SIZE - 1 to calculate the page end.
    
      Which can't handle the above range layout, and will trigger ASSERT()
      in btrfs_writepage_endio_finish_ordered(), as the range is no longer
      covered by the page range.
    
      Fix it by taking page end into consideration.
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 2bd0fc9349b63653216d71671c5ea84d11a4f348
Author: Qu Wenruo <wqu@suse.com>
Date:   Mon Sep 27 15:21:58 2021 +0800

    btrfs: cleanup for extent_write_locked_range()
    
    There are several cleanups for extent_write_locked_range(), most of them
    are pure cleanups, but with some preparation for future subpage support.
    
    - Add a proper comment for which call sites are suitable
      Unlike regular synchronized extent write back, if async COW or zoned
      COW happens, we have all pages in the range still locked.
    
      Thus for those (only) two call sites, we need this function to submit
      page content into bios and submit them.
    
    - Remove @mode parameter
      All the existing two call sites pass WB_SYNC_ALL. No need for @mode
      parameter.
    
    - Better error handling
      Currently if we hit an error during the page iteration loop, we
      overwrite @ret, causing only the last error can be recorded.
    
      Here we add @found_error and @first_error variable to record if we hit
      any error, and the first error we hit.
      So the first error won't get lost.
    
    - Don't reuse @start as the cursor
      We reuse the parameter @start as the cursor to iterate the range, not
      a big problem, but since we're here, introduce a proper @cur as the
      cursor.
    
    - Remove impossible branch
      Since all pages are still locked after the ordered extent is inserted,
      there is no way that pages can get its dirty bit cleared.
      Remove the branch where page is not dirty and replace it with an
      ASSERT().
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 49d0c6424cf13a30768eace116769fe98f8fb69f
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Sep 22 10:36:45 2021 +0100

    btrfs: assert that extent buffers are write locked instead of only locked
    
    We currently use lockdep_assert_held() at btrfs_assert_tree_locked(), and
    that checks that we hold a lock either in read mode or write mode.
    
    However in all contexts we use btrfs_assert_tree_locked(), we actually
    want to check if we are holding a write lock on the extent buffer's rw
    semaphore - it would be a bug if in any of those contexts we were holding
    a read lock instead.
    
    So change btrfs_assert_tree_locked() to use lockdep_assert_held_write()
    instead and, to make it more explicit, rename btrfs_assert_tree_locked()
    to btrfs_assert_tree_write_locked(), so that it's clear we want to check
    we are holding a write lock.
    
    For now there are no contexts where we want to assert that we must have
    a read lock, but in case that is needed in the future, we can add a new
    helper function that just calls out lockdep_assert_held_read().
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 869b44211adc878be7149cc4ae57207f924f7390
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Sep 16 18:15:36 2021 +0000

    kvm: x86: protect masterclock with a seqcount
    
    Protect the reference point for kvmclock with a seqcount, so that
    kvmclock updates for all vCPUs can proceed in parallel.  Xen runstate
    updates will also run in parallel and not bounce the kvmclock cacheline.
    
    Of the variables that were protected by pvclock_gtod_sync_lock,
    nr_vcpus_matched_tsc is different because it is updated outside
    pvclock_update_vm_gtod_copy and read inside it.  Therefore, we
    need to keep it protected by a spinlock.  In fact it must now
    be a raw spinlock, because pvclock_update_vm_gtod_copy, being the
    write-side of a seqcount, is non-preemptible.  Since we already
    have tsc_write_lock which is a raw spinlock, we can just use
    tsc_write_lock as the lock that protects the write-side of the
    seqcount.
    
    Co-developed-by: Oliver Upton <oupton@google.com>
    Message-Id: <20210916181538.968978-6-oupton@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit 880732ae31e890a568a71eb50d5747284b3c4dbe
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Sep 23 19:29:18 2021 +0200

    samples/kfifo: Rename read_lock/write_lock
    
    The variables names read_lock and write_lock can clash with functions used for
    read/writer locks.
    
    Rename read_lock to read_access and write_lock to write_access to avoid a name
    collision.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Link: https://lkml.kernel.org/r/20210806152551.qio7c3ho6pexezup@linutronix.de
    Link: https://lore.kernel.org/r/20210923172918.o22iwgvn3w7ilh44@linutronix.de
    Acked-by: Stefani Seibold <stefani@seibold.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7e3c4fb7fc19bcf20657de3edb718ec1b26c7df3
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Sep 3 10:26:05 2021 -0500

    exec: Check for a pending fatal signal instead of core_state
    
    Prevent exec continuing when a fatal signal is pending by replacing
    mmap_read_lock with mmap_read_lock_killable.  This is always the right
    thing to do as userspace will never observe an exec complete when
    there is a fatal signal pending.
    
    With that change it becomes unnecessary to explicitly test for a core
    dump in progress.  In coredump_wait zap_threads arranges under
    mmap_write_lock for all tasks that use a mm to also have SIGKILL
    pending, which means mmap_read_lock_killable will always return -EINTR
    when old_mm->core_state is present.
    
    Link: https://lkml.kernel.org/r/87fstux27w.fsf@disp2133
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

commit ef1f4804b27a54da34de6984d16f1fe8f2cc7011
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Sep 23 18:54:46 2021 +0200

    locking/rt: Take RCU nesting into account for __might_resched()
    
    The general rule that rcu_read_lock() held sections cannot voluntary sleep
    does apply even on RT kernels. Though the substitution of spin/rw locks on
    RT enabled kernels has to be exempt from that rule. On !RT a spin_lock()
    can obviously nest inside a RCU read side critical section as the lock
    acquisition is not going to block, but on RT this is not longer the case
    due to the 'sleeping' spinlock substitution.
    
    The RT patches contained a cheap hack to ignore the RCU nesting depth in
    might_sleep() checks, which was a pragmatic but incorrect workaround.
    
    Instead of generally ignoring the RCU nesting depth in __might_sleep() and
    __might_resched() checks, pass the rcu_preempt_depth() via the offsets
    argument to __might_resched() from spin/read/write_lock() which makes the
    checks work correctly even in RCU read side critical sections.
    
    The actual blocking on such a substituted lock within a RCU read side
    critical section is already handled correctly in __schedule() by treating
    it as a "preemption" of the RCU read side critical section.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20210923165358.368305497@linutronix.de

commit 3e9cc688e56cc2abb9b6067f57c8397f6c96d42c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Sep 23 18:54:44 2021 +0200

    sched: Make cond_resched_lock() variants RT aware
    
    The __might_resched() checks in the cond_resched_lock() variants use
    PREEMPT_LOCK_OFFSET for preempt count offset checking which takes the
    preemption disable by the spin_lock() which is still held at that point
    into account.
    
    On PREEMPT_RT enabled kernels spin/rw_lock held sections stay preemptible
    which means PREEMPT_LOCK_OFFSET is 0, but that still triggers the
    __might_resched() check because that takes RCU read side nesting into
    account.
    
    On RT enabled kernels spin/read/write_lock() issue rcu_read_lock() to
    resemble the !RT semantics, which means in cond_resched_lock() the might
    resched check will see preempt_count() == 0 and rcu_preempt_depth() == 1.
    
    Introduce PREEMPT_LOCK_SCHED_OFFSET for those might resched checks and map
    them depending on CONFIG_PREEMPT_RT.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20210923165358.305969211@linutronix.de

commit 81121524f1c798c9481bd7900450b72ee7ac2eef
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Thu Sep 9 12:59:19 2021 +0200

    locking/rwbase: Take care of ordering guarantee for fastpath reader
    
    Readers of rwbase can lock and unlock without taking any inner lock, if
    that happens, we need the ordering provided by atomic operations to
    satisfy the ordering semantics of lock/unlock. Without that, considering
    the follow case:
    
            { X = 0 initially }
    
            CPU 0                   CPU 1
            =====                   =====
                                    rt_write_lock();
                                    X = 1
                                    rt_write_unlock():
                                      atomic_add(READER_BIAS - WRITER_BIAS, ->readers);
                                      // ->readers is READER_BIAS.
            rt_read_lock():
              if ((r = atomic_read(->readers)) < 0) // True
                atomic_try_cmpxchg(->readers, r, r + 1); // succeed.
              <acquire the read lock via fast path>
    
            r1 = X; // r1 may be 0, because nothing prevent the reordering
                    // of "X=1" and atomic_add() on CPU 1.
    
    Therefore audit every usage of atomic operations that may happen in a
    fast path, and add necessary barriers.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20210909110203.953991276@infradead.org

commit 616be87eac9fa2ab2dca1069712f7236e50f3bf6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Sep 9 12:59:18 2021 +0200

    locking/rwbase: Extract __rwbase_write_trylock()
    
    The code in rwbase_write_lock() is a little non-obvious vs the
    read+set 'trylock', extract the sequence into a helper function to
    clarify the code.
    
    This also provides a single site to fix fast-path ordering.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/YUCq3L+u44NDieEJ@hirez.programming.kicks-ass.net

commit 95ac706744de78a93a7ec98d603c35fb21de8400
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:12 2021 +0200

    ACPI: processor: Replace deprecated CPU-hotplug functions
    
    The functions cpu_hotplug_begin, cpu_hotplug_done, get_online_cpus() and
    put_online_cpus() have been deprecated during the CPU hotplug rework. They map
    directly to cpus_write_lock(), cpus_write_unlock, cpus_read_lock() and
    cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit aa8866530d6abb9d883dec2f0e8151cdcaf80bc9
Author: Maurizio Lombardi <mlombard@redhat.com>
Date:   Fri Jul 2 10:11:21 2021 +0200

    nvme-tcp: can't set sk_user_data without write_lock
    
    [ Upstream commit 0755d3be2d9bb6ea38598ccd30d6bbaa1a5c3a50 ]
    
    The sk_user_data pointer is supposed to be modified only while
    holding the write_lock "sk_callback_lock", otherwise
    we could race with other threads and crash the kernel.
    
    we can't take the write_lock in nvmet_tcp_state_change()
    because it would cause a deadlock, but the release_work queue
    will set the pointer to NULL later so we can simply remove
    the assignment.
    
    Fixes: b5332a9f3f3d ("nvmet-tcp: fix incorrect locking in state_change sk callback")
    
    Signed-off-by: Maurizio Lombardi <mlombard@redhat.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 89047f0089cd7430e4896fb99ba0f17ade832433
Author: Maurizio Lombardi <mlombard@redhat.com>
Date:   Fri Jul 2 10:11:21 2021 +0200

    nvme-tcp: can't set sk_user_data without write_lock
    
    [ Upstream commit 0755d3be2d9bb6ea38598ccd30d6bbaa1a5c3a50 ]
    
    The sk_user_data pointer is supposed to be modified only while
    holding the write_lock "sk_callback_lock", otherwise
    we could race with other threads and crash the kernel.
    
    we can't take the write_lock in nvmet_tcp_state_change()
    because it would cause a deadlock, but the release_work queue
    will set the pointer to NULL later so we can simply remove
    the assignment.
    
    Fixes: b5332a9f3f3d ("nvmet-tcp: fix incorrect locking in state_change sk callback")
    
    Signed-off-by: Maurizio Lombardi <mlombard@redhat.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 7de053c6811660d9e70cf38753d0a53fbd9728fb
Author: Maurizio Lombardi <mlombard@redhat.com>
Date:   Fri Jul 2 10:11:21 2021 +0200

    nvme-tcp: can't set sk_user_data without write_lock
    
    [ Upstream commit 0755d3be2d9bb6ea38598ccd30d6bbaa1a5c3a50 ]
    
    The sk_user_data pointer is supposed to be modified only while
    holding the write_lock "sk_callback_lock", otherwise
    we could race with other threads and crash the kernel.
    
    we can't take the write_lock in nvmet_tcp_state_change()
    because it would cause a deadlock, but the release_work queue
    will set the pointer to NULL later so we can simply remove
    the assignment.
    
    Fixes: b5332a9f3f3d ("nvmet-tcp: fix incorrect locking in state_change sk callback")
    
    Signed-off-by: Maurizio Lombardi <mlombard@redhat.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit bc6b8e2cd87e894613e331a636ac93168712466e
Author: Maurizio Lombardi <mlombard@redhat.com>
Date:   Fri Jul 2 10:11:21 2021 +0200

    nvme-tcp: can't set sk_user_data without write_lock
    
    [ Upstream commit 0755d3be2d9bb6ea38598ccd30d6bbaa1a5c3a50 ]
    
    The sk_user_data pointer is supposed to be modified only while
    holding the write_lock "sk_callback_lock", otherwise
    we could race with other threads and crash the kernel.
    
    we can't take the write_lock in nvmet_tcp_state_change()
    because it would cause a deadlock, but the release_work queue
    will set the pointer to NULL later so we can simply remove
    the assignment.
    
    Fixes: b5332a9f3f3d ("nvmet-tcp: fix incorrect locking in state_change sk callback")
    
    Signed-off-by: Maurizio Lombardi <mlombard@redhat.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 6126891c6d4f6f4ef50323d2020635ee255a796e
Author: Vasily Averin <vvs@virtuozzo.com>
Date:   Mon Jul 19 13:44:31 2021 +0300

    memcg: enable accounting for IP address and routing-related objects
    
    An netadmin inside container can use 'ip a a' and 'ip r a'
    to assign a large number of ipv4/ipv6 addresses and routing entries
    and force kernel to allocate megabytes of unaccounted memory
    for long-lived per-netdevice related kernel objects:
    'struct in_ifaddr', 'struct inet6_ifaddr', 'struct fib6_node',
    'struct rt6_info', 'struct fib_rules' and ip_fib caches.
    
    These objects can be manually removed, though usually they lives
    in memory till destroy of its net namespace.
    
    It makes sense to account for them to restrict the host's memory
    consumption from inside the memcg-limited container.
    
    One of such objects is the 'struct fib6_node' mostly allocated in
    net/ipv6/route.c::__ip6_ins_rt() inside the lock_bh()/unlock_bh() section:
    
     write_lock_bh(&table->tb6_lock);
     err = fib6_add(&table->tb6_root, rt, info, mxc);
     write_unlock_bh(&table->tb6_lock);
    
    In this case it is not enough to simply add SLAB_ACCOUNT to corresponding
    kmem cache. The proper memory cgroup still cannot be found due to the
    incorrect 'in_interrupt()' check used in memcg_kmem_bypass().
    
    Obsoleted in_interrupt() does not describe real execution context properly.
    >From include/linux/preempt.h:
    
     The following macros are deprecated and should not be used in new code:
     in_interrupt() - We're in NMI,IRQ,SoftIRQ context or have BH disabled
    
    To verify the current execution context new macro should be used instead:
     in_task()      - We're in task context
    
    Signed-off-by: Vasily Averin <vvs@virtuozzo.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 987a852734e668e0829cf65dd182e44a03064800
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Jul 7 18:10:15 2021 -0700

    mm/mremap: hold the rmap lock in write mode when moving page table entries.
    
    commit 97113eb39fa7972722ff490b947d8af023e1f6a2 upstream.
    
    To avoid a race between rmap walk and mremap, mremap does
    take_rmap_locks().  The lock was taken to ensure that rmap walk don't miss
    a page table entry due to PTE moves via move_pagetables().  The kernel
    does further optimization of this lock such that if we are going to find
    the newly added vma after the old vma, the rmap lock is not taken.  This
    is because rmap walk would find the vmas in the same order and if we don't
    find the page table attached to older vma we would find it with the new
    vma which we would iterate later.
    
    As explained in commit eb66ae030829 ("mremap: properly flush TLB before
    releasing the page") mremap is special in that it doesn't take ownership
    of the page.  The optimized version for PUD/PMD aligned mremap also
    doesn't hold the ptl lock.  This can result in stale TLB entries as show
    below.
    
    This patch updates the rmap locking requirement in mremap to handle the race condition
    explained below with optimized mremap::
    
    Optmized PMD move
    
        CPU 1                           CPU 2                                   CPU 3
    
        mremap(old_addr, new_addr)      page_shrinker/try_to_unmap_one
    
        mmap_write_lock_killable()
    
                                        addr = old_addr
                                        lock(pte_ptl)
        lock(pmd_ptl)
        pmd = *old_pmd
        pmd_clear(old_pmd)
        flush_tlb_range(old_addr)
    
        *new_pmd = pmd
                                                                                *new_addr = 10; and fills
                                                                                TLB with new addr
                                                                                and old pfn
    
        unlock(pmd_ptl)
                                        ptep_clear_flush()
                                        old pfn is free.
                                                                                Stale TLB entry
    
    Optimized PUD move also suffers from a similar race.  Both the above race
    condition can be fixed if we force mremap path to take rmap lock.
    
    Link: https://lkml.kernel.org/r/20210616045239.370802-7-aneesh.kumar@linux.ibm.com
    Fixes: 2c91bd4a4e2e ("mm: speed up mremap by 20x on large regions")
    Fixes: c49dd3401802 ("mm: speedup mremap on 1GB or larger regions")
    Link: https://lore.kernel.org/linux-mm/CAHk-=wgXVR04eBNtxQfevontWnP6FDm+oj5vauQXP3S-huwbPw@mail.gmail.com
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Christophe Leroy <christophe.leroy@csgroup.eu>
    Cc: Joel Fernandes <joel@joelfernandes.org>
    Cc: Kalesh Singh <kaleshsingh@google.com>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 864c4d1d25170def283b2bf87726218126634f04
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Jul 7 18:10:15 2021 -0700

    mm/mremap: hold the rmap lock in write mode when moving page table entries.
    
    commit 97113eb39fa7972722ff490b947d8af023e1f6a2 upstream.
    
    To avoid a race between rmap walk and mremap, mremap does
    take_rmap_locks().  The lock was taken to ensure that rmap walk don't miss
    a page table entry due to PTE moves via move_pagetables().  The kernel
    does further optimization of this lock such that if we are going to find
    the newly added vma after the old vma, the rmap lock is not taken.  This
    is because rmap walk would find the vmas in the same order and if we don't
    find the page table attached to older vma we would find it with the new
    vma which we would iterate later.
    
    As explained in commit eb66ae030829 ("mremap: properly flush TLB before
    releasing the page") mremap is special in that it doesn't take ownership
    of the page.  The optimized version for PUD/PMD aligned mremap also
    doesn't hold the ptl lock.  This can result in stale TLB entries as show
    below.
    
    This patch updates the rmap locking requirement in mremap to handle the race condition
    explained below with optimized mremap::
    
    Optmized PMD move
    
        CPU 1                           CPU 2                                   CPU 3
    
        mremap(old_addr, new_addr)      page_shrinker/try_to_unmap_one
    
        mmap_write_lock_killable()
    
                                        addr = old_addr
                                        lock(pte_ptl)
        lock(pmd_ptl)
        pmd = *old_pmd
        pmd_clear(old_pmd)
        flush_tlb_range(old_addr)
    
        *new_pmd = pmd
                                                                                *new_addr = 10; and fills
                                                                                TLB with new addr
                                                                                and old pfn
    
        unlock(pmd_ptl)
                                        ptep_clear_flush()
                                        old pfn is free.
                                                                                Stale TLB entry
    
    Optimized PUD move also suffers from a similar race.  Both the above race
    condition can be fixed if we force mremap path to take rmap lock.
    
    Link: https://lkml.kernel.org/r/20210616045239.370802-7-aneesh.kumar@linux.ibm.com
    Fixes: 2c91bd4a4e2e ("mm: speed up mremap by 20x on large regions")
    Fixes: c49dd3401802 ("mm: speedup mremap on 1GB or larger regions")
    Link: https://lore.kernel.org/linux-mm/CAHk-=wgXVR04eBNtxQfevontWnP6FDm+oj5vauQXP3S-huwbPw@mail.gmail.com
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Christophe Leroy <christophe.leroy@csgroup.eu>
    Cc: Joel Fernandes <joel@joelfernandes.org>
    Cc: Kalesh Singh <kaleshsingh@google.com>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 798c511548b946ae9ec123b0dfe197a5f29e63ec
Author: Jingwen Chen <Jingwen.Chen2@amd.com>
Date:   Thu Jul 1 10:19:17 2021 +0800

    drm/amdgpu: SRIOV flr_work should take write_lock
    
    [Why]
    If flr_work takes read_lock, then other threads who takes
    read_lock can access hardware when host is doing vf flr.
    
    [How]
    flr_work should take write_lock to avoid this case.
    
    Signed-off-by: Jingwen Chen <Jingwen.Chen2@amd.com>
    Reviewed-by: Monk Liu <monk.liu@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

commit a022f7d575bb68c35be0a9ea68860411dec652fe
Merge: 3de62951a5be a731763fc479
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 9 12:05:33 2021 -0700

    Merge tag 'block-5.14-2021-07-08' of git://git.kernel.dk/linux-block
    
    Pull more block updates from Jens Axboe:
     "A combination of changes that ended up depending on both the driver
      and core branch (and/or the IDE removal), and a few late arriving
      fixes. In detail:
    
       - Fix io ticks wrap-around issue (Chunguang)
    
       - nvme-tcp sock locking fix (Maurizio)
    
       - s390-dasd fixes (Kees, Christoph)
    
       - blk_execute_rq polling support (Keith)
    
       - blk-cgroup RCU iteration fix (Yu)
    
       - nbd backend ID addition (Prasanna)
    
       - Partition deletion fix (Yufen)
    
       - Use blk_mq_alloc_disk for mmc, mtip32xx, ubd (Christoph)
    
       - Removal of now dead block request types due to IDE removal
         (Christoph)
    
       - Loop probing and control device cleanups (Christoph)
    
       - Device uevent fix (Christoph)
    
       - Misc cleanups/fixes (Tetsuo, Christoph)"
    
    * tag 'block-5.14-2021-07-08' of git://git.kernel.dk/linux-block: (34 commits)
      blk-cgroup: prevent rcu_sched detected stalls warnings while iterating blkgs
      block: fix the problem of io_ticks becoming smaller
      nvme-tcp: can't set sk_user_data without write_lock
      loop: remove unused variable in loop_set_status()
      block: remove the bdgrab in blk_drop_partitions
      block: grab a device refcount in disk_uevent
      s390/dasd: Avoid field over-reading memcpy()
      dasd: unexport dasd_set_target_state
      block: check disk exist before trying to add partition
      ubd: remove dead code in ubd_setup_common
      nvme: use return value from blk_execute_rq()
      block: return errors from blk_execute_rq()
      nvme: use blk_execute_rq() for passthrough commands
      block: support polling through blk_execute_rq
      block: remove REQ_OP_SCSI_{IN,OUT}
      block: mark blk_mq_init_queue_data static
      loop: rewrite loop_exit using idr_for_each_entry
      loop: split loop_lookup
      loop: don't allow deleting an unspecified loop device
      loop: move loop_ctl_mutex locking into loop_add
      ...

commit b5840166dcb8bb03daa420f6c84eb78d46b713e7
Author: Jingwen Chen <Jingwen.Chen2@amd.com>
Date:   Thu Jul 1 10:19:17 2021 +0800

    drm/amdgpu: SRIOV flr_work should take write_lock
    
    [Why]
    If flr_work takes read_lock, then other threads who takes
    read_lock can access hardware when host is doing vf flr.
    
    [How]
    flr_work should take write_lock to avoid this case.
    
    Signed-off-by: Jingwen Chen <Jingwen.Chen2@amd.com>
    Reviewed-by: Monk Liu <monk.liu@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

commit 97113eb39fa7972722ff490b947d8af023e1f6a2
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Jul 7 18:10:15 2021 -0700

    mm/mremap: hold the rmap lock in write mode when moving page table entries.
    
    To avoid a race between rmap walk and mremap, mremap does
    take_rmap_locks().  The lock was taken to ensure that rmap walk don't miss
    a page table entry due to PTE moves via move_pagetables().  The kernel
    does further optimization of this lock such that if we are going to find
    the newly added vma after the old vma, the rmap lock is not taken.  This
    is because rmap walk would find the vmas in the same order and if we don't
    find the page table attached to older vma we would find it with the new
    vma which we would iterate later.
    
    As explained in commit eb66ae030829 ("mremap: properly flush TLB before
    releasing the page") mremap is special in that it doesn't take ownership
    of the page.  The optimized version for PUD/PMD aligned mremap also
    doesn't hold the ptl lock.  This can result in stale TLB entries as show
    below.
    
    This patch updates the rmap locking requirement in mremap to handle the race condition
    explained below with optimized mremap::
    
    Optmized PMD move
    
        CPU 1                           CPU 2                                   CPU 3
    
        mremap(old_addr, new_addr)      page_shrinker/try_to_unmap_one
    
        mmap_write_lock_killable()
    
                                        addr = old_addr
                                        lock(pte_ptl)
        lock(pmd_ptl)
        pmd = *old_pmd
        pmd_clear(old_pmd)
        flush_tlb_range(old_addr)
    
        *new_pmd = pmd
                                                                                *new_addr = 10; and fills
                                                                                TLB with new addr
                                                                                and old pfn
    
        unlock(pmd_ptl)
                                        ptep_clear_flush()
                                        old pfn is free.
                                                                                Stale TLB entry
    
    Optimized PUD move also suffers from a similar race.  Both the above race
    condition can be fixed if we force mremap path to take rmap lock.
    
    Link: https://lkml.kernel.org/r/20210616045239.370802-7-aneesh.kumar@linux.ibm.com
    Fixes: 2c91bd4a4e2e ("mm: speed up mremap by 20x on large regions")
    Fixes: c49dd3401802 ("mm: speedup mremap on 1GB or larger regions")
    Link: https://lore.kernel.org/linux-mm/CAHk-=wgXVR04eBNtxQfevontWnP6FDm+oj5vauQXP3S-huwbPw@mail.gmail.com
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Christophe Leroy <christophe.leroy@csgroup.eu>
    Cc: Joel Fernandes <joel@joelfernandes.org>
    Cc: Kalesh Singh <kaleshsingh@google.com>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit c6af8db92b9a9bc89e3d25535fdc26cb61f419d0
Merge: 585af8ede703 0755d3be2d9b
Author: Jens Axboe <axboe@kernel.dk>
Date:   Wed Jul 7 06:37:36 2021 -0600

    Merge branch 'nvme-5.14' of git://git.infradead.org/nvme into block-5.14
    
    Pull single NVMe fix from Christoph.
    
    * 'nvme-5.14' of git://git.infradead.org/nvme:
      nvme-tcp: can't set sk_user_data without write_lock

commit 0755d3be2d9bb6ea38598ccd30d6bbaa1a5c3a50
Author: Maurizio Lombardi <mlombard@redhat.com>
Date:   Fri Jul 2 10:11:21 2021 +0200

    nvme-tcp: can't set sk_user_data without write_lock
    
    The sk_user_data pointer is supposed to be modified only while
    holding the write_lock "sk_callback_lock", otherwise
    we could race with other threads and crash the kernel.
    
    we can't take the write_lock in nvmet_tcp_state_change()
    because it would cause a deadlock, but the release_work queue
    will set the pointer to NULL later so we can simply remove
    the assignment.
    
    Fixes: b5332a9f3f3d ("nvmet-tcp: fix incorrect locking in state_change sk callback")
    
    Signed-off-by: Maurizio Lombardi <mlombard@redhat.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

commit 7cc93d888df764a13f196e3d4aef38869f7dd217
Author: Louis Peens <louis.peens@corigine.com>
Date:   Fri Jul 2 11:21:39 2021 +0200

    nfp: flower-ct: remove callback delete deadlock
    
    The current location of the callback delete can lead to a race
    condition where deleting the callback requires a write_lock on
    the nf_table, but at the same time another thread from netfilter
    could have taken a read lock on the table before trying to offload.
    Since the driver is taking a rtnl_lock this can lead into a deadlock
    situation, where the netfilter offload will wait for the cls_flower
    rtnl_lock to be released, but this cannot happen since this is
    waiting for the nf_table read_lock to be released before it can
    delete the callback.
    
    Solve this by completely removing the nf_flow_table_offload_del_cb
    call, as this will now be cleaned up by act_ct itself when cleaning
    up the specific nf_table.
    
    Fixes: 62268e78145f ("nfp: flower-ct: add nft callback stubs")
    Signed-off-by: Louis Peens <louis.peens@corigine.com>
    Signed-off-by: Yinjun Zhang <yinjun.zhang@corigine.com>
    Signed-off-by: Simon Horman <simon.horman@corigine.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit db1d9152c91acf2fef2eb16718a0aafee60dde30
Author: Liam Howlett <liam.howlett@oracle.com>
Date:   Wed Jun 30 18:52:17 2021 -0700

    mm/nommu: unexport do_munmap()
    
    do_munmap() does not take the mmap_write_lock().  vm_munmap() should be
    used instead.
    
    Link: https://lkml.kernel.org/r/20210604194002.648037-1-Liam.Howlett@Oracle.com
    Signed-off-by: Liam R. Howlett <Liam.Howlett@Oracle.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Matthew Wilcox (Oracle) <willy@infradead.org>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 906c538340dde6d891df89fe7dac8eaa724e40da
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Sun Mar 21 00:08:49 2021 -0700

    nvmet-tcp: fix incorrect locking in state_change sk callback
    
    [ Upstream commit b5332a9f3f3d884a1b646ce155e664cc558c1722 ]
    
    We are not changing anything in the TCP connection state so
    we should not take a write_lock but rather a read lock.
    
    This caused a deadlock when running nvmet-tcp and nvme-tcp
    on the same system, where state_change callbacks on the
    host and on the controller side have causal relationship
    and made lockdep report on this with blktests:
    
    ================================
    WARNING: inconsistent lock state
    5.12.0-rc3 #1 Tainted: G          I
    --------------------------------
    inconsistent {IN-SOFTIRQ-W} -> {SOFTIRQ-ON-R} usage.
    nvme/1324 [HC0[0]:SC0[0]:HE1:SE1] takes:
    ffff888363151000 (clock-AF_INET){++-?}-{2:2}, at: nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
    {IN-SOFTIRQ-W} state was registered at:
      __lock_acquire+0x79b/0x18d0
      lock_acquire+0x1ca/0x480
      _raw_write_lock_bh+0x39/0x80
      nvmet_tcp_state_change+0x21/0x170 [nvmet_tcp]
      tcp_fin+0x2a8/0x780
      tcp_data_queue+0xf94/0x1f20
      tcp_rcv_established+0x6ba/0x1f00
      tcp_v4_do_rcv+0x502/0x760
      tcp_v4_rcv+0x257e/0x3430
      ip_protocol_deliver_rcu+0x69/0x6a0
      ip_local_deliver_finish+0x1e2/0x2f0
      ip_local_deliver+0x1a2/0x420
      ip_rcv+0x4fb/0x6b0
      __netif_receive_skb_one_core+0x162/0x1b0
      process_backlog+0x1ff/0x770
      __napi_poll.constprop.0+0xa9/0x5c0
      net_rx_action+0x7b3/0xb30
      __do_softirq+0x1f0/0x940
      do_softirq+0xa1/0xd0
      __local_bh_enable_ip+0xd8/0x100
      ip_finish_output2+0x6b7/0x18a0
      __ip_queue_xmit+0x706/0x1aa0
      __tcp_transmit_skb+0x2068/0x2e20
      tcp_write_xmit+0xc9e/0x2bb0
      __tcp_push_pending_frames+0x92/0x310
      inet_shutdown+0x158/0x300
      __nvme_tcp_stop_queue+0x36/0x270 [nvme_tcp]
      nvme_tcp_stop_queue+0x87/0xb0 [nvme_tcp]
      nvme_tcp_teardown_admin_queue+0x69/0xe0 [nvme_tcp]
      nvme_do_delete_ctrl+0x100/0x10c [nvme_core]
      nvme_sysfs_delete.cold+0x8/0xd [nvme_core]
      kernfs_fop_write_iter+0x2c7/0x460
      new_sync_write+0x36c/0x610
      vfs_write+0x5c0/0x870
      ksys_write+0xf9/0x1d0
      do_syscall_64+0x33/0x40
      entry_SYSCALL_64_after_hwframe+0x44/0xae
    irq event stamp: 10687
    hardirqs last  enabled at (10687): [<ffffffff9ec376bd>] _raw_spin_unlock_irqrestore+0x2d/0x40
    hardirqs last disabled at (10686): [<ffffffff9ec374d8>] _raw_spin_lock_irqsave+0x68/0x90
    softirqs last  enabled at (10684): [<ffffffff9f000608>] __do_softirq+0x608/0x940
    softirqs last disabled at (10649): [<ffffffff9cdedd31>] do_softirq+0xa1/0xd0
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(clock-AF_INET);
      <Interrupt>
        lock(clock-AF_INET);
    
     *** DEADLOCK ***
    
    5 locks held by nvme/1324:
     #0: ffff8884a01fe470 (sb_writers#4){.+.+}-{0:0}, at: ksys_write+0xf9/0x1d0
     #1: ffff8886e435c090 (&of->mutex){+.+.}-{3:3}, at: kernfs_fop_write_iter+0x216/0x460
     #2: ffff888104d90c38 (kn->active#255){++++}-{0:0}, at: kernfs_remove_self+0x22d/0x330
     #3: ffff8884634538d0 (&queue->queue_lock){+.+.}-{3:3}, at: nvme_tcp_stop_queue+0x52/0xb0 [nvme_tcp]
     #4: ffff888363150d30 (sk_lock-AF_INET){+.+.}-{0:0}, at: inet_shutdown+0x59/0x300
    
    stack backtrace:
    CPU: 26 PID: 1324 Comm: nvme Tainted: G          I       5.12.0-rc3 #1
    Hardware name: Dell Inc. PowerEdge R640/06NR82, BIOS 2.10.0 11/12/2020
    Call Trace:
     dump_stack+0x93/0xc2
     mark_lock_irq.cold+0x2c/0xb3
     ? verify_lock_unused+0x390/0x390
     ? stack_trace_consume_entry+0x160/0x160
     ? lock_downgrade+0x100/0x100
     ? save_trace+0x88/0x5e0
     ? _raw_spin_unlock_irqrestore+0x2d/0x40
     mark_lock+0x530/0x1470
     ? mark_lock_irq+0x1d10/0x1d10
     ? enqueue_timer+0x660/0x660
     mark_usage+0x215/0x2a0
     __lock_acquire+0x79b/0x18d0
     ? tcp_schedule_loss_probe.part.0+0x38c/0x520
     lock_acquire+0x1ca/0x480
     ? nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     ? rcu_read_unlock+0x40/0x40
     ? tcp_mtu_probe+0x1ae0/0x1ae0
     ? kmalloc_reserve+0xa0/0xa0
     ? sysfs_file_ops+0x170/0x170
     _raw_read_lock+0x3d/0xa0
     ? nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     ? sysfs_file_ops+0x170/0x170
     inet_shutdown+0x189/0x300
     __nvme_tcp_stop_queue+0x36/0x270 [nvme_tcp]
     nvme_tcp_stop_queue+0x87/0xb0 [nvme_tcp]
     nvme_tcp_teardown_admin_queue+0x69/0xe0 [nvme_tcp]
     nvme_do_delete_ctrl+0x100/0x10c [nvme_core]
     nvme_sysfs_delete.cold+0x8/0xd [nvme_core]
     kernfs_fop_write_iter+0x2c7/0x460
     new_sync_write+0x36c/0x610
     ? new_sync_read+0x600/0x600
     ? lock_acquire+0x1ca/0x480
     ? rcu_read_unlock+0x40/0x40
     ? lock_is_held_type+0x9a/0x110
     vfs_write+0x5c0/0x870
     ksys_write+0xf9/0x1d0
     ? __ia32_sys_read+0xa0/0xa0
     ? lockdep_hardirqs_on_prepare.part.0+0x198/0x340
     ? syscall_enter_from_user_mode+0x27/0x70
     do_syscall_64+0x33/0x40
     entry_SYSCALL_64_after_hwframe+0x44/0xae
    
    Fixes: 872d26a391da ("nvmet-tcp: add NVMe over TCP target driver")
    Reported-by: Yi Zhang <yi.zhang@redhat.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 06beaa1a9f6e501213195e47c30416032fd2bbd5
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Sun Mar 21 00:08:49 2021 -0700

    nvmet-tcp: fix incorrect locking in state_change sk callback
    
    [ Upstream commit b5332a9f3f3d884a1b646ce155e664cc558c1722 ]
    
    We are not changing anything in the TCP connection state so
    we should not take a write_lock but rather a read lock.
    
    This caused a deadlock when running nvmet-tcp and nvme-tcp
    on the same system, where state_change callbacks on the
    host and on the controller side have causal relationship
    and made lockdep report on this with blktests:
    
    ================================
    WARNING: inconsistent lock state
    5.12.0-rc3 #1 Tainted: G          I
    --------------------------------
    inconsistent {IN-SOFTIRQ-W} -> {SOFTIRQ-ON-R} usage.
    nvme/1324 [HC0[0]:SC0[0]:HE1:SE1] takes:
    ffff888363151000 (clock-AF_INET){++-?}-{2:2}, at: nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
    {IN-SOFTIRQ-W} state was registered at:
      __lock_acquire+0x79b/0x18d0
      lock_acquire+0x1ca/0x480
      _raw_write_lock_bh+0x39/0x80
      nvmet_tcp_state_change+0x21/0x170 [nvmet_tcp]
      tcp_fin+0x2a8/0x780
      tcp_data_queue+0xf94/0x1f20
      tcp_rcv_established+0x6ba/0x1f00
      tcp_v4_do_rcv+0x502/0x760
      tcp_v4_rcv+0x257e/0x3430
      ip_protocol_deliver_rcu+0x69/0x6a0
      ip_local_deliver_finish+0x1e2/0x2f0
      ip_local_deliver+0x1a2/0x420
      ip_rcv+0x4fb/0x6b0
      __netif_receive_skb_one_core+0x162/0x1b0
      process_backlog+0x1ff/0x770
      __napi_poll.constprop.0+0xa9/0x5c0
      net_rx_action+0x7b3/0xb30
      __do_softirq+0x1f0/0x940
      do_softirq+0xa1/0xd0
      __local_bh_enable_ip+0xd8/0x100
      ip_finish_output2+0x6b7/0x18a0
      __ip_queue_xmit+0x706/0x1aa0
      __tcp_transmit_skb+0x2068/0x2e20
      tcp_write_xmit+0xc9e/0x2bb0
      __tcp_push_pending_frames+0x92/0x310
      inet_shutdown+0x158/0x300
      __nvme_tcp_stop_queue+0x36/0x270 [nvme_tcp]
      nvme_tcp_stop_queue+0x87/0xb0 [nvme_tcp]
      nvme_tcp_teardown_admin_queue+0x69/0xe0 [nvme_tcp]
      nvme_do_delete_ctrl+0x100/0x10c [nvme_core]
      nvme_sysfs_delete.cold+0x8/0xd [nvme_core]
      kernfs_fop_write_iter+0x2c7/0x460
      new_sync_write+0x36c/0x610
      vfs_write+0x5c0/0x870
      ksys_write+0xf9/0x1d0
      do_syscall_64+0x33/0x40
      entry_SYSCALL_64_after_hwframe+0x44/0xae
    irq event stamp: 10687
    hardirqs last  enabled at (10687): [<ffffffff9ec376bd>] _raw_spin_unlock_irqrestore+0x2d/0x40
    hardirqs last disabled at (10686): [<ffffffff9ec374d8>] _raw_spin_lock_irqsave+0x68/0x90
    softirqs last  enabled at (10684): [<ffffffff9f000608>] __do_softirq+0x608/0x940
    softirqs last disabled at (10649): [<ffffffff9cdedd31>] do_softirq+0xa1/0xd0
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(clock-AF_INET);
      <Interrupt>
        lock(clock-AF_INET);
    
     *** DEADLOCK ***
    
    5 locks held by nvme/1324:
     #0: ffff8884a01fe470 (sb_writers#4){.+.+}-{0:0}, at: ksys_write+0xf9/0x1d0
     #1: ffff8886e435c090 (&of->mutex){+.+.}-{3:3}, at: kernfs_fop_write_iter+0x216/0x460
     #2: ffff888104d90c38 (kn->active#255){++++}-{0:0}, at: kernfs_remove_self+0x22d/0x330
     #3: ffff8884634538d0 (&queue->queue_lock){+.+.}-{3:3}, at: nvme_tcp_stop_queue+0x52/0xb0 [nvme_tcp]
     #4: ffff888363150d30 (sk_lock-AF_INET){+.+.}-{0:0}, at: inet_shutdown+0x59/0x300
    
    stack backtrace:
    CPU: 26 PID: 1324 Comm: nvme Tainted: G          I       5.12.0-rc3 #1
    Hardware name: Dell Inc. PowerEdge R640/06NR82, BIOS 2.10.0 11/12/2020
    Call Trace:
     dump_stack+0x93/0xc2
     mark_lock_irq.cold+0x2c/0xb3
     ? verify_lock_unused+0x390/0x390
     ? stack_trace_consume_entry+0x160/0x160
     ? lock_downgrade+0x100/0x100
     ? save_trace+0x88/0x5e0
     ? _raw_spin_unlock_irqrestore+0x2d/0x40
     mark_lock+0x530/0x1470
     ? mark_lock_irq+0x1d10/0x1d10
     ? enqueue_timer+0x660/0x660
     mark_usage+0x215/0x2a0
     __lock_acquire+0x79b/0x18d0
     ? tcp_schedule_loss_probe.part.0+0x38c/0x520
     lock_acquire+0x1ca/0x480
     ? nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     ? rcu_read_unlock+0x40/0x40
     ? tcp_mtu_probe+0x1ae0/0x1ae0
     ? kmalloc_reserve+0xa0/0xa0
     ? sysfs_file_ops+0x170/0x170
     _raw_read_lock+0x3d/0xa0
     ? nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     ? sysfs_file_ops+0x170/0x170
     inet_shutdown+0x189/0x300
     __nvme_tcp_stop_queue+0x36/0x270 [nvme_tcp]
     nvme_tcp_stop_queue+0x87/0xb0 [nvme_tcp]
     nvme_tcp_teardown_admin_queue+0x69/0xe0 [nvme_tcp]
     nvme_do_delete_ctrl+0x100/0x10c [nvme_core]
     nvme_sysfs_delete.cold+0x8/0xd [nvme_core]
     kernfs_fop_write_iter+0x2c7/0x460
     new_sync_write+0x36c/0x610
     ? new_sync_read+0x600/0x600
     ? lock_acquire+0x1ca/0x480
     ? rcu_read_unlock+0x40/0x40
     ? lock_is_held_type+0x9a/0x110
     vfs_write+0x5c0/0x870
     ksys_write+0xf9/0x1d0
     ? __ia32_sys_read+0xa0/0xa0
     ? lockdep_hardirqs_on_prepare.part.0+0x198/0x340
     ? syscall_enter_from_user_mode+0x27/0x70
     do_syscall_64+0x33/0x40
     entry_SYSCALL_64_after_hwframe+0x44/0xae
    
    Fixes: 872d26a391da ("nvmet-tcp: add NVMe over TCP target driver")
    Reported-by: Yi Zhang <yi.zhang@redhat.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 60ade0d56b06537a28884745059b3801c78e03bc
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Sun Mar 21 00:08:49 2021 -0700

    nvmet-tcp: fix incorrect locking in state_change sk callback
    
    [ Upstream commit b5332a9f3f3d884a1b646ce155e664cc558c1722 ]
    
    We are not changing anything in the TCP connection state so
    we should not take a write_lock but rather a read lock.
    
    This caused a deadlock when running nvmet-tcp and nvme-tcp
    on the same system, where state_change callbacks on the
    host and on the controller side have causal relationship
    and made lockdep report on this with blktests:
    
    ================================
    WARNING: inconsistent lock state
    5.12.0-rc3 #1 Tainted: G          I
    --------------------------------
    inconsistent {IN-SOFTIRQ-W} -> {SOFTIRQ-ON-R} usage.
    nvme/1324 [HC0[0]:SC0[0]:HE1:SE1] takes:
    ffff888363151000 (clock-AF_INET){++-?}-{2:2}, at: nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
    {IN-SOFTIRQ-W} state was registered at:
      __lock_acquire+0x79b/0x18d0
      lock_acquire+0x1ca/0x480
      _raw_write_lock_bh+0x39/0x80
      nvmet_tcp_state_change+0x21/0x170 [nvmet_tcp]
      tcp_fin+0x2a8/0x780
      tcp_data_queue+0xf94/0x1f20
      tcp_rcv_established+0x6ba/0x1f00
      tcp_v4_do_rcv+0x502/0x760
      tcp_v4_rcv+0x257e/0x3430
      ip_protocol_deliver_rcu+0x69/0x6a0
      ip_local_deliver_finish+0x1e2/0x2f0
      ip_local_deliver+0x1a2/0x420
      ip_rcv+0x4fb/0x6b0
      __netif_receive_skb_one_core+0x162/0x1b0
      process_backlog+0x1ff/0x770
      __napi_poll.constprop.0+0xa9/0x5c0
      net_rx_action+0x7b3/0xb30
      __do_softirq+0x1f0/0x940
      do_softirq+0xa1/0xd0
      __local_bh_enable_ip+0xd8/0x100
      ip_finish_output2+0x6b7/0x18a0
      __ip_queue_xmit+0x706/0x1aa0
      __tcp_transmit_skb+0x2068/0x2e20
      tcp_write_xmit+0xc9e/0x2bb0
      __tcp_push_pending_frames+0x92/0x310
      inet_shutdown+0x158/0x300
      __nvme_tcp_stop_queue+0x36/0x270 [nvme_tcp]
      nvme_tcp_stop_queue+0x87/0xb0 [nvme_tcp]
      nvme_tcp_teardown_admin_queue+0x69/0xe0 [nvme_tcp]
      nvme_do_delete_ctrl+0x100/0x10c [nvme_core]
      nvme_sysfs_delete.cold+0x8/0xd [nvme_core]
      kernfs_fop_write_iter+0x2c7/0x460
      new_sync_write+0x36c/0x610
      vfs_write+0x5c0/0x870
      ksys_write+0xf9/0x1d0
      do_syscall_64+0x33/0x40
      entry_SYSCALL_64_after_hwframe+0x44/0xae
    irq event stamp: 10687
    hardirqs last  enabled at (10687): [<ffffffff9ec376bd>] _raw_spin_unlock_irqrestore+0x2d/0x40
    hardirqs last disabled at (10686): [<ffffffff9ec374d8>] _raw_spin_lock_irqsave+0x68/0x90
    softirqs last  enabled at (10684): [<ffffffff9f000608>] __do_softirq+0x608/0x940
    softirqs last disabled at (10649): [<ffffffff9cdedd31>] do_softirq+0xa1/0xd0
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(clock-AF_INET);
      <Interrupt>
        lock(clock-AF_INET);
    
     *** DEADLOCK ***
    
    5 locks held by nvme/1324:
     #0: ffff8884a01fe470 (sb_writers#4){.+.+}-{0:0}, at: ksys_write+0xf9/0x1d0
     #1: ffff8886e435c090 (&of->mutex){+.+.}-{3:3}, at: kernfs_fop_write_iter+0x216/0x460
     #2: ffff888104d90c38 (kn->active#255){++++}-{0:0}, at: kernfs_remove_self+0x22d/0x330
     #3: ffff8884634538d0 (&queue->queue_lock){+.+.}-{3:3}, at: nvme_tcp_stop_queue+0x52/0xb0 [nvme_tcp]
     #4: ffff888363150d30 (sk_lock-AF_INET){+.+.}-{0:0}, at: inet_shutdown+0x59/0x300
    
    stack backtrace:
    CPU: 26 PID: 1324 Comm: nvme Tainted: G          I       5.12.0-rc3 #1
    Hardware name: Dell Inc. PowerEdge R640/06NR82, BIOS 2.10.0 11/12/2020
    Call Trace:
     dump_stack+0x93/0xc2
     mark_lock_irq.cold+0x2c/0xb3
     ? verify_lock_unused+0x390/0x390
     ? stack_trace_consume_entry+0x160/0x160
     ? lock_downgrade+0x100/0x100
     ? save_trace+0x88/0x5e0
     ? _raw_spin_unlock_irqrestore+0x2d/0x40
     mark_lock+0x530/0x1470
     ? mark_lock_irq+0x1d10/0x1d10
     ? enqueue_timer+0x660/0x660
     mark_usage+0x215/0x2a0
     __lock_acquire+0x79b/0x18d0
     ? tcp_schedule_loss_probe.part.0+0x38c/0x520
     lock_acquire+0x1ca/0x480
     ? nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     ? rcu_read_unlock+0x40/0x40
     ? tcp_mtu_probe+0x1ae0/0x1ae0
     ? kmalloc_reserve+0xa0/0xa0
     ? sysfs_file_ops+0x170/0x170
     _raw_read_lock+0x3d/0xa0
     ? nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     ? sysfs_file_ops+0x170/0x170
     inet_shutdown+0x189/0x300
     __nvme_tcp_stop_queue+0x36/0x270 [nvme_tcp]
     nvme_tcp_stop_queue+0x87/0xb0 [nvme_tcp]
     nvme_tcp_teardown_admin_queue+0x69/0xe0 [nvme_tcp]
     nvme_do_delete_ctrl+0x100/0x10c [nvme_core]
     nvme_sysfs_delete.cold+0x8/0xd [nvme_core]
     kernfs_fop_write_iter+0x2c7/0x460
     new_sync_write+0x36c/0x610
     ? new_sync_read+0x600/0x600
     ? lock_acquire+0x1ca/0x480
     ? rcu_read_unlock+0x40/0x40
     ? lock_is_held_type+0x9a/0x110
     vfs_write+0x5c0/0x870
     ksys_write+0xf9/0x1d0
     ? __ia32_sys_read+0xa0/0xa0
     ? lockdep_hardirqs_on_prepare.part.0+0x198/0x340
     ? syscall_enter_from_user_mode+0x27/0x70
     do_syscall_64+0x33/0x40
     entry_SYSCALL_64_after_hwframe+0x44/0xae
    
    Fixes: 872d26a391da ("nvmet-tcp: add NVMe over TCP target driver")
    Reported-by: Yi Zhang <yi.zhang@redhat.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 999d606a820c36ae9b9e9611360c8b3d8d4bb777
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Sun Mar 21 00:08:49 2021 -0700

    nvmet-tcp: fix incorrect locking in state_change sk callback
    
    [ Upstream commit b5332a9f3f3d884a1b646ce155e664cc558c1722 ]
    
    We are not changing anything in the TCP connection state so
    we should not take a write_lock but rather a read lock.
    
    This caused a deadlock when running nvmet-tcp and nvme-tcp
    on the same system, where state_change callbacks on the
    host and on the controller side have causal relationship
    and made lockdep report on this with blktests:
    
    ================================
    WARNING: inconsistent lock state
    5.12.0-rc3 #1 Tainted: G          I
    --------------------------------
    inconsistent {IN-SOFTIRQ-W} -> {SOFTIRQ-ON-R} usage.
    nvme/1324 [HC0[0]:SC0[0]:HE1:SE1] takes:
    ffff888363151000 (clock-AF_INET){++-?}-{2:2}, at: nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
    {IN-SOFTIRQ-W} state was registered at:
      __lock_acquire+0x79b/0x18d0
      lock_acquire+0x1ca/0x480
      _raw_write_lock_bh+0x39/0x80
      nvmet_tcp_state_change+0x21/0x170 [nvmet_tcp]
      tcp_fin+0x2a8/0x780
      tcp_data_queue+0xf94/0x1f20
      tcp_rcv_established+0x6ba/0x1f00
      tcp_v4_do_rcv+0x502/0x760
      tcp_v4_rcv+0x257e/0x3430
      ip_protocol_deliver_rcu+0x69/0x6a0
      ip_local_deliver_finish+0x1e2/0x2f0
      ip_local_deliver+0x1a2/0x420
      ip_rcv+0x4fb/0x6b0
      __netif_receive_skb_one_core+0x162/0x1b0
      process_backlog+0x1ff/0x770
      __napi_poll.constprop.0+0xa9/0x5c0
      net_rx_action+0x7b3/0xb30
      __do_softirq+0x1f0/0x940
      do_softirq+0xa1/0xd0
      __local_bh_enable_ip+0xd8/0x100
      ip_finish_output2+0x6b7/0x18a0
      __ip_queue_xmit+0x706/0x1aa0
      __tcp_transmit_skb+0x2068/0x2e20
      tcp_write_xmit+0xc9e/0x2bb0
      __tcp_push_pending_frames+0x92/0x310
      inet_shutdown+0x158/0x300
      __nvme_tcp_stop_queue+0x36/0x270 [nvme_tcp]
      nvme_tcp_stop_queue+0x87/0xb0 [nvme_tcp]
      nvme_tcp_teardown_admin_queue+0x69/0xe0 [nvme_tcp]
      nvme_do_delete_ctrl+0x100/0x10c [nvme_core]
      nvme_sysfs_delete.cold+0x8/0xd [nvme_core]
      kernfs_fop_write_iter+0x2c7/0x460
      new_sync_write+0x36c/0x610
      vfs_write+0x5c0/0x870
      ksys_write+0xf9/0x1d0
      do_syscall_64+0x33/0x40
      entry_SYSCALL_64_after_hwframe+0x44/0xae
    irq event stamp: 10687
    hardirqs last  enabled at (10687): [<ffffffff9ec376bd>] _raw_spin_unlock_irqrestore+0x2d/0x40
    hardirqs last disabled at (10686): [<ffffffff9ec374d8>] _raw_spin_lock_irqsave+0x68/0x90
    softirqs last  enabled at (10684): [<ffffffff9f000608>] __do_softirq+0x608/0x940
    softirqs last disabled at (10649): [<ffffffff9cdedd31>] do_softirq+0xa1/0xd0
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(clock-AF_INET);
      <Interrupt>
        lock(clock-AF_INET);
    
     *** DEADLOCK ***
    
    5 locks held by nvme/1324:
     #0: ffff8884a01fe470 (sb_writers#4){.+.+}-{0:0}, at: ksys_write+0xf9/0x1d0
     #1: ffff8886e435c090 (&of->mutex){+.+.}-{3:3}, at: kernfs_fop_write_iter+0x216/0x460
     #2: ffff888104d90c38 (kn->active#255){++++}-{0:0}, at: kernfs_remove_self+0x22d/0x330
     #3: ffff8884634538d0 (&queue->queue_lock){+.+.}-{3:3}, at: nvme_tcp_stop_queue+0x52/0xb0 [nvme_tcp]
     #4: ffff888363150d30 (sk_lock-AF_INET){+.+.}-{0:0}, at: inet_shutdown+0x59/0x300
    
    stack backtrace:
    CPU: 26 PID: 1324 Comm: nvme Tainted: G          I       5.12.0-rc3 #1
    Hardware name: Dell Inc. PowerEdge R640/06NR82, BIOS 2.10.0 11/12/2020
    Call Trace:
     dump_stack+0x93/0xc2
     mark_lock_irq.cold+0x2c/0xb3
     ? verify_lock_unused+0x390/0x390
     ? stack_trace_consume_entry+0x160/0x160
     ? lock_downgrade+0x100/0x100
     ? save_trace+0x88/0x5e0
     ? _raw_spin_unlock_irqrestore+0x2d/0x40
     mark_lock+0x530/0x1470
     ? mark_lock_irq+0x1d10/0x1d10
     ? enqueue_timer+0x660/0x660
     mark_usage+0x215/0x2a0
     __lock_acquire+0x79b/0x18d0
     ? tcp_schedule_loss_probe.part.0+0x38c/0x520
     lock_acquire+0x1ca/0x480
     ? nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     ? rcu_read_unlock+0x40/0x40
     ? tcp_mtu_probe+0x1ae0/0x1ae0
     ? kmalloc_reserve+0xa0/0xa0
     ? sysfs_file_ops+0x170/0x170
     _raw_read_lock+0x3d/0xa0
     ? nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     ? sysfs_file_ops+0x170/0x170
     inet_shutdown+0x189/0x300
     __nvme_tcp_stop_queue+0x36/0x270 [nvme_tcp]
     nvme_tcp_stop_queue+0x87/0xb0 [nvme_tcp]
     nvme_tcp_teardown_admin_queue+0x69/0xe0 [nvme_tcp]
     nvme_do_delete_ctrl+0x100/0x10c [nvme_core]
     nvme_sysfs_delete.cold+0x8/0xd [nvme_core]
     kernfs_fop_write_iter+0x2c7/0x460
     new_sync_write+0x36c/0x610
     ? new_sync_read+0x600/0x600
     ? lock_acquire+0x1ca/0x480
     ? rcu_read_unlock+0x40/0x40
     ? lock_is_held_type+0x9a/0x110
     vfs_write+0x5c0/0x870
     ksys_write+0xf9/0x1d0
     ? __ia32_sys_read+0xa0/0xa0
     ? lockdep_hardirqs_on_prepare.part.0+0x198/0x340
     ? syscall_enter_from_user_mode+0x27/0x70
     do_syscall_64+0x33/0x40
     entry_SYSCALL_64_after_hwframe+0x44/0xae
    
    Fixes: 872d26a391da ("nvmet-tcp: add NVMe over TCP target driver")
    Reported-by: Yi Zhang <yi.zhang@redhat.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 732a27a0891cb5db1a0f9c33a018ea6eca9a4023
Merge: 85bbba1c0778 51cf94d16860
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 9 13:07:03 2021 -0700

    Merge tag 'locking-urgent-2021-05-09' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fixes from Thomas Gleixner:
     "A set of locking related fixes and updates:
    
       - Two fixes for the futex syscall related to the timeout handling.
    
         FUTEX_LOCK_PI does not support the FUTEX_CLOCK_REALTIME bit and
         because it's not set the time namespace adjustment for clock
         MONOTONIC is applied wrongly.
    
         FUTEX_WAIT cannot support the FUTEX_CLOCK_REALTIME bit because its
         always a relative timeout.
    
       - Cleanups in the futex syscall entry points which became obvious
         when the two timeout handling bugs were fixed.
    
       - Cleanup of queued_write_lock_slowpath() as suggested by Linus
    
       - Fixup of the smp_call_function_single_async() prototype"
    
    * tag 'locking-urgent-2021-05-09' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      futex: Make syscall entry points less convoluted
      futex: Get rid of the val2 conditional dance
      futex: Do not apply time namespace adjustment on FUTEX_LOCK_PI
      Revert 337f13046ff0 ("futex: Allow FUTEX_CLOCK_REALTIME with FUTEX_WAIT op")
      locking/qrwlock: Cleanup queued_write_lock_slowpath()
      smp: Fix smp_call_function_single_async prototype

commit 28ce0e70ecc30cc7d558a0304e6b816d70848f9a
Author: Waiman Long <longman@redhat.com>
Date:   Mon Apr 26 14:50:17 2021 -0400

    locking/qrwlock: Cleanup queued_write_lock_slowpath()
    
    Make the code more readable by replacing the atomic_cmpxchg_acquire()
    by an equivalent atomic_try_cmpxchg_acquire() and change atomic_add()
    to atomic_or().
    
    For architectures that use qrwlock, I do not find one that has an
    atomic_add() defined but not an atomic_or().  I guess it should be fine
    by changing atomic_add() to atomic_or().
    
    Note that the previous use of atomic_add() isn't wrong as only one
    writer that is the wait_lock owner can set the waiting flag and the
    flag will be cleared later on when acquiring the write lock.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lkml.kernel.org/r/20210426185017.19815-1-longman@redhat.com

commit d558fcdb17139728347bccc60a16af3e639649d2
Author: Ali Saidi <alisaidi@amazon.com>
Date:   Thu Apr 15 17:27:11 2021 +0000

    locking/qrwlock: Fix ordering in queued_write_lock_slowpath()
    
    [ Upstream commit 84a24bf8c52e66b7ac89ada5e3cfbe72d65c1896 ]
    
    While this code is executed with the wait_lock held, a reader can
    acquire the lock without holding wait_lock.  The writer side loops
    checking the value with the atomic_cond_read_acquire(), but only truly
    acquires the lock when the compare-and-exchange is completed
    successfully which isn’t ordered. This exposes the window between the
    acquire and the cmpxchg to an A-B-A problem which allows reads
    following the lock acquisition to observe values speculatively before
    the write lock is truly acquired.
    
    We've seen a problem in epoll where the reader does a xchg while
    holding the read lock, but the writer can see a value change out from
    under it.
    
      Writer                                | Reader
      --------------------------------------------------------------------------------
      ep_scan_ready_list()                  |
      |- write_lock_irq()                   |
          |- queued_write_lock_slowpath()   |
            |- atomic_cond_read_acquire()   |
                                            | read_lock_irqsave(&ep->lock, flags);
         --> (observes value before unlock) |  chain_epi_lockless()
         |                                  |    epi->next = xchg(&ep->ovflist, epi);
         |                                  | read_unlock_irqrestore(&ep->lock, flags);
         |                                  |
         |     atomic_cmpxchg_relaxed()     |
         |-- READ_ONCE(ep->ovflist);        |
    
    A core can order the read of the ovflist ahead of the
    atomic_cmpxchg_relaxed(). Switching the cmpxchg to use acquire
    semantics addresses this issue at which point the atomic_cond_read can
    be switched to use relaxed semantics.
    
    Fixes: b519b56e378ee ("locking/qrwlock: Use atomic_cond_read_acquire() when spinning in qrwlock")
    Signed-off-by: Ali Saidi <alisaidi@amazon.com>
    [peterz: use try_cmpxchg()]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Tested-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 82fa9ced35d88581cffa4a1c856fc41fca96d80a
Author: Ali Saidi <alisaidi@amazon.com>
Date:   Thu Apr 15 17:27:11 2021 +0000

    locking/qrwlock: Fix ordering in queued_write_lock_slowpath()
    
    [ Upstream commit 84a24bf8c52e66b7ac89ada5e3cfbe72d65c1896 ]
    
    While this code is executed with the wait_lock held, a reader can
    acquire the lock without holding wait_lock.  The writer side loops
    checking the value with the atomic_cond_read_acquire(), but only truly
    acquires the lock when the compare-and-exchange is completed
    successfully which isn’t ordered. This exposes the window between the
    acquire and the cmpxchg to an A-B-A problem which allows reads
    following the lock acquisition to observe values speculatively before
    the write lock is truly acquired.
    
    We've seen a problem in epoll where the reader does a xchg while
    holding the read lock, but the writer can see a value change out from
    under it.
    
      Writer                                | Reader
      --------------------------------------------------------------------------------
      ep_scan_ready_list()                  |
      |- write_lock_irq()                   |
          |- queued_write_lock_slowpath()   |
            |- atomic_cond_read_acquire()   |
                                            | read_lock_irqsave(&ep->lock, flags);
         --> (observes value before unlock) |  chain_epi_lockless()
         |                                  |    epi->next = xchg(&ep->ovflist, epi);
         |                                  | read_unlock_irqrestore(&ep->lock, flags);
         |                                  |
         |     atomic_cmpxchg_relaxed()     |
         |-- READ_ONCE(ep->ovflist);        |
    
    A core can order the read of the ovflist ahead of the
    atomic_cmpxchg_relaxed(). Switching the cmpxchg to use acquire
    semantics addresses this issue at which point the atomic_cond_read can
    be switched to use relaxed semantics.
    
    Fixes: b519b56e378ee ("locking/qrwlock: Use atomic_cond_read_acquire() when spinning in qrwlock")
    Signed-off-by: Ali Saidi <alisaidi@amazon.com>
    [peterz: use try_cmpxchg()]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Tested-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 82808cc026811fbc3ecf0c0b267a12a339eead56
Author: Ali Saidi <alisaidi@amazon.com>
Date:   Thu Apr 15 17:27:11 2021 +0000

    locking/qrwlock: Fix ordering in queued_write_lock_slowpath()
    
    [ Upstream commit 84a24bf8c52e66b7ac89ada5e3cfbe72d65c1896 ]
    
    While this code is executed with the wait_lock held, a reader can
    acquire the lock without holding wait_lock.  The writer side loops
    checking the value with the atomic_cond_read_acquire(), but only truly
    acquires the lock when the compare-and-exchange is completed
    successfully which isn’t ordered. This exposes the window between the
    acquire and the cmpxchg to an A-B-A problem which allows reads
    following the lock acquisition to observe values speculatively before
    the write lock is truly acquired.
    
    We've seen a problem in epoll where the reader does a xchg while
    holding the read lock, but the writer can see a value change out from
    under it.
    
      Writer                                | Reader
      --------------------------------------------------------------------------------
      ep_scan_ready_list()                  |
      |- write_lock_irq()                   |
          |- queued_write_lock_slowpath()   |
            |- atomic_cond_read_acquire()   |
                                            | read_lock_irqsave(&ep->lock, flags);
         --> (observes value before unlock) |  chain_epi_lockless()
         |                                  |    epi->next = xchg(&ep->ovflist, epi);
         |                                  | read_unlock_irqrestore(&ep->lock, flags);
         |                                  |
         |     atomic_cmpxchg_relaxed()     |
         |-- READ_ONCE(ep->ovflist);        |
    
    A core can order the read of the ovflist ahead of the
    atomic_cmpxchg_relaxed(). Switching the cmpxchg to use acquire
    semantics addresses this issue at which point the atomic_cond_read can
    be switched to use relaxed semantics.
    
    Fixes: b519b56e378ee ("locking/qrwlock: Use atomic_cond_read_acquire() when spinning in qrwlock")
    Signed-off-by: Ali Saidi <alisaidi@amazon.com>
    [peterz: use try_cmpxchg()]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Tested-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 5902f9453a313be8fe78cbd7e7ca9dba9319fc6e
Author: Ali Saidi <alisaidi@amazon.com>
Date:   Thu Apr 15 17:27:11 2021 +0000

    locking/qrwlock: Fix ordering in queued_write_lock_slowpath()
    
    [ Upstream commit 84a24bf8c52e66b7ac89ada5e3cfbe72d65c1896 ]
    
    While this code is executed with the wait_lock held, a reader can
    acquire the lock without holding wait_lock.  The writer side loops
    checking the value with the atomic_cond_read_acquire(), but only truly
    acquires the lock when the compare-and-exchange is completed
    successfully which isn’t ordered. This exposes the window between the
    acquire and the cmpxchg to an A-B-A problem which allows reads
    following the lock acquisition to observe values speculatively before
    the write lock is truly acquired.
    
    We've seen a problem in epoll where the reader does a xchg while
    holding the read lock, but the writer can see a value change out from
    under it.
    
      Writer                                | Reader
      --------------------------------------------------------------------------------
      ep_scan_ready_list()                  |
      |- write_lock_irq()                   |
          |- queued_write_lock_slowpath()   |
            |- atomic_cond_read_acquire()   |
                                            | read_lock_irqsave(&ep->lock, flags);
         --> (observes value before unlock) |  chain_epi_lockless()
         |                                  |    epi->next = xchg(&ep->ovflist, epi);
         |                                  | read_unlock_irqrestore(&ep->lock, flags);
         |                                  |
         |     atomic_cmpxchg_relaxed()     |
         |-- READ_ONCE(ep->ovflist);        |
    
    A core can order the read of the ovflist ahead of the
    atomic_cmpxchg_relaxed(). Switching the cmpxchg to use acquire
    semantics addresses this issue at which point the atomic_cond_read can
    be switched to use relaxed semantics.
    
    Fixes: b519b56e378ee ("locking/qrwlock: Use atomic_cond_read_acquire() when spinning in qrwlock")
    Signed-off-by: Ali Saidi <alisaidi@amazon.com>
    [peterz: use try_cmpxchg()]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Tested-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 0146da0d4cecad571f69f02fe35d75d6dba9723c
Merge: 682b26bd80f9 84a24bf8c52e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 25 09:10:10 2021 -0700

    Merge tag 'locking_urgent_for_v5.12' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fix from Borislav Petkov:
     "Fix ordering in the queued writer lock's slowpath"
    
    * tag 'locking_urgent_for_v5.12' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/qrwlock: Fix ordering in queued_write_lock_slowpath()

commit 4683cfecadeb383b03019ad11aeea1efac38c1ba
Author: Philip Yang <Philip.Yang@amd.com>
Date:   Wed Apr 8 11:09:45 2020 -0400

    drm/amdkfd: deregister svm range
    
    When application explicitly call unmap or unmap from mmput when
    application exit, driver will receive MMU_NOTIFY_UNMAP event to remove
    svm range from process svms object tree and list first, unmap from GPUs
    (in the following patch).
    
    Split the svm ranges to handle partial unmapping of svm ranges. To
    avoid deadlocks, updating MMU notifiers, range lists and interval trees
    is done in a deferred worker. New child ranges are attached to their
    parent range's child_list until the worker can update the
    svm_range_list. svm_range_set_attr flushes deferred work and takes the
    mmap_write_lock to guarantee that it has an up-to-date svm_range_list.
    
    Signed-off-by: Philip Yang <Philip.Yang@amd.com>
    Signed-off-by: Alex Sierra <alex.sierra@amd.com>
    Reviewed-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

commit 87d5742b73f24ca389cd832fa088170ca5d3d093
Author: Jiapeng Chong <jiapeng.chong@linux.alibaba.com>
Date:   Wed Apr 14 14:32:56 2021 +0800

    dm clone metadata: remove unused function
    
    Fix the following clang warning:
    
    drivers/md/dm-clone-metadata.c:279:19: warning: unused function
    'superblock_write_lock' [-Wunused-function].
    
    Reported-by: Abaci Robot <abaci@linux.alibaba.com>
    Signed-off-by: Jiapeng Chong <jiapeng.chong@linux.alibaba.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

commit 84a24bf8c52e66b7ac89ada5e3cfbe72d65c1896
Author: Ali Saidi <alisaidi@amazon.com>
Date:   Thu Apr 15 17:27:11 2021 +0000

    locking/qrwlock: Fix ordering in queued_write_lock_slowpath()
    
    While this code is executed with the wait_lock held, a reader can
    acquire the lock without holding wait_lock.  The writer side loops
    checking the value with the atomic_cond_read_acquire(), but only truly
    acquires the lock when the compare-and-exchange is completed
    successfully which isn’t ordered. This exposes the window between the
    acquire and the cmpxchg to an A-B-A problem which allows reads
    following the lock acquisition to observe values speculatively before
    the write lock is truly acquired.
    
    We've seen a problem in epoll where the reader does a xchg while
    holding the read lock, but the writer can see a value change out from
    under it.
    
      Writer                                | Reader
      --------------------------------------------------------------------------------
      ep_scan_ready_list()                  |
      |- write_lock_irq()                   |
          |- queued_write_lock_slowpath()   |
            |- atomic_cond_read_acquire()   |
                                            | read_lock_irqsave(&ep->lock, flags);
         --> (observes value before unlock) |  chain_epi_lockless()
         |                                  |    epi->next = xchg(&ep->ovflist, epi);
         |                                  | read_unlock_irqrestore(&ep->lock, flags);
         |                                  |
         |     atomic_cmpxchg_relaxed()     |
         |-- READ_ONCE(ep->ovflist);        |
    
    A core can order the read of the ovflist ahead of the
    atomic_cmpxchg_relaxed(). Switching the cmpxchg to use acquire
    semantics addresses this issue at which point the atomic_cond_read can
    be switched to use relaxed semantics.
    
    Fixes: b519b56e378ee ("locking/qrwlock: Use atomic_cond_read_acquire() when spinning in qrwlock")
    Signed-off-by: Ali Saidi <alisaidi@amazon.com>
    [peterz: use try_cmpxchg()]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Tested-by: Steve Capper <steve.capper@arm.com>

commit b5332a9f3f3d884a1b646ce155e664cc558c1722
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Sun Mar 21 00:08:49 2021 -0700

    nvmet-tcp: fix incorrect locking in state_change sk callback
    
    We are not changing anything in the TCP connection state so
    we should not take a write_lock but rather a read lock.
    
    This caused a deadlock when running nvmet-tcp and nvme-tcp
    on the same system, where state_change callbacks on the
    host and on the controller side have causal relationship
    and made lockdep report on this with blktests:
    
    ================================
    WARNING: inconsistent lock state
    5.12.0-rc3 #1 Tainted: G          I
    --------------------------------
    inconsistent {IN-SOFTIRQ-W} -> {SOFTIRQ-ON-R} usage.
    nvme/1324 [HC0[0]:SC0[0]:HE1:SE1] takes:
    ffff888363151000 (clock-AF_INET){++-?}-{2:2}, at: nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
    {IN-SOFTIRQ-W} state was registered at:
      __lock_acquire+0x79b/0x18d0
      lock_acquire+0x1ca/0x480
      _raw_write_lock_bh+0x39/0x80
      nvmet_tcp_state_change+0x21/0x170 [nvmet_tcp]
      tcp_fin+0x2a8/0x780
      tcp_data_queue+0xf94/0x1f20
      tcp_rcv_established+0x6ba/0x1f00
      tcp_v4_do_rcv+0x502/0x760
      tcp_v4_rcv+0x257e/0x3430
      ip_protocol_deliver_rcu+0x69/0x6a0
      ip_local_deliver_finish+0x1e2/0x2f0
      ip_local_deliver+0x1a2/0x420
      ip_rcv+0x4fb/0x6b0
      __netif_receive_skb_one_core+0x162/0x1b0
      process_backlog+0x1ff/0x770
      __napi_poll.constprop.0+0xa9/0x5c0
      net_rx_action+0x7b3/0xb30
      __do_softirq+0x1f0/0x940
      do_softirq+0xa1/0xd0
      __local_bh_enable_ip+0xd8/0x100
      ip_finish_output2+0x6b7/0x18a0
      __ip_queue_xmit+0x706/0x1aa0
      __tcp_transmit_skb+0x2068/0x2e20
      tcp_write_xmit+0xc9e/0x2bb0
      __tcp_push_pending_frames+0x92/0x310
      inet_shutdown+0x158/0x300
      __nvme_tcp_stop_queue+0x36/0x270 [nvme_tcp]
      nvme_tcp_stop_queue+0x87/0xb0 [nvme_tcp]
      nvme_tcp_teardown_admin_queue+0x69/0xe0 [nvme_tcp]
      nvme_do_delete_ctrl+0x100/0x10c [nvme_core]
      nvme_sysfs_delete.cold+0x8/0xd [nvme_core]
      kernfs_fop_write_iter+0x2c7/0x460
      new_sync_write+0x36c/0x610
      vfs_write+0x5c0/0x870
      ksys_write+0xf9/0x1d0
      do_syscall_64+0x33/0x40
      entry_SYSCALL_64_after_hwframe+0x44/0xae
    irq event stamp: 10687
    hardirqs last  enabled at (10687): [<ffffffff9ec376bd>] _raw_spin_unlock_irqrestore+0x2d/0x40
    hardirqs last disabled at (10686): [<ffffffff9ec374d8>] _raw_spin_lock_irqsave+0x68/0x90
    softirqs last  enabled at (10684): [<ffffffff9f000608>] __do_softirq+0x608/0x940
    softirqs last disabled at (10649): [<ffffffff9cdedd31>] do_softirq+0xa1/0xd0
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(clock-AF_INET);
      <Interrupt>
        lock(clock-AF_INET);
    
     *** DEADLOCK ***
    
    5 locks held by nvme/1324:
     #0: ffff8884a01fe470 (sb_writers#4){.+.+}-{0:0}, at: ksys_write+0xf9/0x1d0
     #1: ffff8886e435c090 (&of->mutex){+.+.}-{3:3}, at: kernfs_fop_write_iter+0x216/0x460
     #2: ffff888104d90c38 (kn->active#255){++++}-{0:0}, at: kernfs_remove_self+0x22d/0x330
     #3: ffff8884634538d0 (&queue->queue_lock){+.+.}-{3:3}, at: nvme_tcp_stop_queue+0x52/0xb0 [nvme_tcp]
     #4: ffff888363150d30 (sk_lock-AF_INET){+.+.}-{0:0}, at: inet_shutdown+0x59/0x300
    
    stack backtrace:
    CPU: 26 PID: 1324 Comm: nvme Tainted: G          I       5.12.0-rc3 #1
    Hardware name: Dell Inc. PowerEdge R640/06NR82, BIOS 2.10.0 11/12/2020
    Call Trace:
     dump_stack+0x93/0xc2
     mark_lock_irq.cold+0x2c/0xb3
     ? verify_lock_unused+0x390/0x390
     ? stack_trace_consume_entry+0x160/0x160
     ? lock_downgrade+0x100/0x100
     ? save_trace+0x88/0x5e0
     ? _raw_spin_unlock_irqrestore+0x2d/0x40
     mark_lock+0x530/0x1470
     ? mark_lock_irq+0x1d10/0x1d10
     ? enqueue_timer+0x660/0x660
     mark_usage+0x215/0x2a0
     __lock_acquire+0x79b/0x18d0
     ? tcp_schedule_loss_probe.part.0+0x38c/0x520
     lock_acquire+0x1ca/0x480
     ? nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     ? rcu_read_unlock+0x40/0x40
     ? tcp_mtu_probe+0x1ae0/0x1ae0
     ? kmalloc_reserve+0xa0/0xa0
     ? sysfs_file_ops+0x170/0x170
     _raw_read_lock+0x3d/0xa0
     ? nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     nvme_tcp_state_change+0x21/0x150 [nvme_tcp]
     ? sysfs_file_ops+0x170/0x170
     inet_shutdown+0x189/0x300
     __nvme_tcp_stop_queue+0x36/0x270 [nvme_tcp]
     nvme_tcp_stop_queue+0x87/0xb0 [nvme_tcp]
     nvme_tcp_teardown_admin_queue+0x69/0xe0 [nvme_tcp]
     nvme_do_delete_ctrl+0x100/0x10c [nvme_core]
     nvme_sysfs_delete.cold+0x8/0xd [nvme_core]
     kernfs_fop_write_iter+0x2c7/0x460
     new_sync_write+0x36c/0x610
     ? new_sync_read+0x600/0x600
     ? lock_acquire+0x1ca/0x480
     ? rcu_read_unlock+0x40/0x40
     ? lock_is_held_type+0x9a/0x110
     vfs_write+0x5c0/0x870
     ksys_write+0xf9/0x1d0
     ? __ia32_sys_read+0xa0/0xa0
     ? lockdep_hardirqs_on_prepare.part.0+0x198/0x340
     ? syscall_enter_from_user_mode+0x27/0x70
     do_syscall_64+0x33/0x40
     entry_SYSCALL_64_after_hwframe+0x44/0xae
    
    Fixes: 872d26a391da ("nvmet-tcp: add NVMe over TCP target driver")
    Reported-by: Yi Zhang <yi.zhang@redhat.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

commit 97bc84bbd4de3c060b68aea4d546c7f21c4d6814
Author: Hoang Huu Le <hoang.h.le@dektech.com.au>
Date:   Thu Mar 11 10:33:23 2021 +0700

    tipc: clean up warnings detected by sparse
    
    This patch fixes the following warning from sparse:
    
    net/tipc/monitor.c:263:35: warning: incorrect type in assignment (different base types)
    net/tipc/monitor.c:263:35:    expected unsigned int
    net/tipc/monitor.c:263:35:    got restricted __be32 [usertype]
    [...]
    net/tipc/node.c:374:13: warning: context imbalance in 'tipc_node_read_lock' - wrong count at exit
    net/tipc/node.c:379:13: warning: context imbalance in 'tipc_node_read_unlock' - unexpected unlock
    net/tipc/node.c:384:13: warning: context imbalance in 'tipc_node_write_lock' - wrong count at exit
    net/tipc/node.c:389:13: warning: context imbalance in 'tipc_node_write_unlock_fast' - unexpected unlock
    net/tipc/node.c:404:17: warning: context imbalance in 'tipc_node_write_unlock' - unexpected unlock
    [...]
    net/tipc/crypto.c:1201:9: warning: incorrect type in initializer (different address spaces)
    net/tipc/crypto.c:1201:9:    expected struct tipc_aead [noderef] __rcu *__tmp
    net/tipc/crypto.c:1201:9:    got struct tipc_aead *
    [...]
    
    Acked-by: Jon Maloy <jmaloy@redhat.com>
    Signed-off-by: Hoang Huu Le <hoang.h.le@dektech.com.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 736924801c1d0e784af2a7336b4dd92fc6d2bd82
Author: Andrea Righi <andrea.righi@canonical.com>
Date:   Wed Nov 25 16:18:22 2020 +0100

    leds: trigger: fix potential deadlock with libata
    
    commit 27af8e2c90fba242460b01fa020e6e19ed68c495 upstream.
    
    We have the following potential deadlock condition:
    
     ========================================================
     WARNING: possible irq lock inversion dependency detected
     5.10.0-rc2+ #25 Not tainted
     --------------------------------------------------------
     swapper/3/0 just changed the state of lock:
     ffff8880063bd618 (&host->lock){-...}-{2:2}, at: ata_bmdma_interrupt+0x27/0x200
     but this lock took another, HARDIRQ-READ-unsafe lock in the past:
      (&trig->leddev_list_lock){.+.?}-{2:2}
    
     and interrupts could create inverse lock ordering between them.
    
     other info that might help us debug this:
      Possible interrupt unsafe locking scenario:
    
            CPU0                    CPU1
            ----                    ----
       lock(&trig->leddev_list_lock);
                                    local_irq_disable();
                                    lock(&host->lock);
                                    lock(&trig->leddev_list_lock);
       <Interrupt>
         lock(&host->lock);
    
      *** DEADLOCK ***
    
     no locks held by swapper/3/0.
    
     the shortest dependencies between 2nd lock and 1st lock:
      -> (&trig->leddev_list_lock){.+.?}-{2:2} ops: 46 {
         HARDIRQ-ON-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           rfkill_global_led_trigger_worker+0x94/0xb0
                           process_one_work+0x240/0x560
                           worker_thread+0x58/0x3d0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         IN-SOFTIRQ-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           kbd_bh+0x9e/0xc0
                           tasklet_action_common.constprop.0+0xe9/0x100
                           tasklet_action+0x22/0x30
                           __do_softirq+0xcc/0x46d
                           run_ksoftirqd+0x3f/0x70
                           smpboot_thread_fn+0x116/0x1f0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         SOFTIRQ-ON-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           rfkill_global_led_trigger_worker+0x94/0xb0
                           process_one_work+0x240/0x560
                           worker_thread+0x58/0x3d0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         INITIAL READ USE at:
                               lock_acquire+0x15f/0x420
                               _raw_read_lock+0x42/0x90
                               led_trigger_event+0x2b/0x70
                               rfkill_global_led_trigger_worker+0x94/0xb0
                               process_one_work+0x240/0x560
                               worker_thread+0x58/0x3d0
                               kthread+0x151/0x170
                               ret_from_fork+0x1f/0x30
       }
       ... key      at: [<ffffffff83da4c00>] __key.0+0x0/0x10
       ... acquired at:
        _raw_read_lock+0x42/0x90
        led_trigger_blink_oneshot+0x3b/0x90
        ledtrig_disk_activity+0x3c/0xa0
        ata_qc_complete+0x26/0x450
        ata_do_link_abort+0xa3/0xe0
        ata_port_freeze+0x2e/0x40
        ata_hsm_qc_complete+0x94/0xa0
        ata_sff_hsm_move+0x177/0x7a0
        ata_sff_pio_task+0xc7/0x1b0
        process_one_work+0x240/0x560
        worker_thread+0x58/0x3d0
        kthread+0x151/0x170
        ret_from_fork+0x1f/0x30
    
     -> (&host->lock){-...}-{2:2} ops: 69 {
        IN-HARDIRQ-W at:
                         lock_acquire+0x15f/0x420
                         _raw_spin_lock_irqsave+0x52/0xa0
                         ata_bmdma_interrupt+0x27/0x200
                         __handle_irq_event_percpu+0xd5/0x2b0
                         handle_irq_event+0x57/0xb0
                         handle_edge_irq+0x8c/0x230
                         asm_call_irq_on_stack+0xf/0x20
                         common_interrupt+0x100/0x1c0
                         asm_common_interrupt+0x1e/0x40
                         native_safe_halt+0xe/0x10
                         arch_cpu_idle+0x15/0x20
                         default_idle_call+0x59/0x1c0
                         do_idle+0x22c/0x2c0
                         cpu_startup_entry+0x20/0x30
                         start_secondary+0x11d/0x150
                         secondary_startup_64_no_verify+0xa6/0xab
        INITIAL USE at:
                        lock_acquire+0x15f/0x420
                        _raw_spin_lock_irqsave+0x52/0xa0
                        ata_dev_init+0x54/0xe0
                        ata_link_init+0x8b/0xd0
                        ata_port_alloc+0x1f1/0x210
                        ata_host_alloc+0xf1/0x130
                        ata_host_alloc_pinfo+0x14/0xb0
                        ata_pci_sff_prepare_host+0x41/0xa0
                        ata_pci_bmdma_prepare_host+0x14/0x30
                        piix_init_one+0x21f/0x600
                        local_pci_probe+0x48/0x80
                        pci_device_probe+0x105/0x1c0
                        really_probe+0x221/0x490
                        driver_probe_device+0xe9/0x160
                        device_driver_attach+0xb2/0xc0
                        __driver_attach+0x91/0x150
                        bus_for_each_dev+0x81/0xc0
                        driver_attach+0x1e/0x20
                        bus_add_driver+0x138/0x1f0
                        driver_register+0x91/0xf0
                        __pci_register_driver+0x73/0x80
                        piix_init+0x1e/0x2e
                        do_one_initcall+0x5f/0x2d0
                        kernel_init_freeable+0x26f/0x2cf
                        kernel_init+0xe/0x113
                        ret_from_fork+0x1f/0x30
      }
      ... key      at: [<ffffffff83d9fdc0>] __key.6+0x0/0x10
      ... acquired at:
        __lock_acquire+0x9da/0x2370
        lock_acquire+0x15f/0x420
        _raw_spin_lock_irqsave+0x52/0xa0
        ata_bmdma_interrupt+0x27/0x200
        __handle_irq_event_percpu+0xd5/0x2b0
        handle_irq_event+0x57/0xb0
        handle_edge_irq+0x8c/0x230
        asm_call_irq_on_stack+0xf/0x20
        common_interrupt+0x100/0x1c0
        asm_common_interrupt+0x1e/0x40
        native_safe_halt+0xe/0x10
        arch_cpu_idle+0x15/0x20
        default_idle_call+0x59/0x1c0
        do_idle+0x22c/0x2c0
        cpu_startup_entry+0x20/0x30
        start_secondary+0x11d/0x150
        secondary_startup_64_no_verify+0xa6/0xab
    
    This lockdep splat is reported after:
    commit e918188611f0 ("locking: More accurate annotations for read_lock()")
    
    To clarify:
     - read-locks are recursive only in interrupt context (when
       in_interrupt() returns true)
     - after acquiring host->lock in CPU1, another cpu (i.e. CPU2) may call
       write_lock(&trig->leddev_list_lock) that would be blocked by CPU0
       that holds trig->leddev_list_lock in read-mode
     - when CPU1 (ata_ac_complete()) tries to read-lock
       trig->leddev_list_lock, it would be blocked by the write-lock waiter
       on CPU2 (because we are not in interrupt context, so the read-lock is
       not recursive)
     - at this point if an interrupt happens on CPU0 and
       ata_bmdma_interrupt() is executed it will try to acquire host->lock,
       that is held by CPU1, that is currently blocked by CPU2, so:
    
       * CPU0 blocked by CPU1
       * CPU1 blocked by CPU2
       * CPU2 blocked by CPU0
    
         *** DEADLOCK ***
    
    The deadlock scenario is better represented by the following schema
    (thanks to Boqun Feng <boqun.feng@gmail.com> for the schema and the
    detailed explanation of the deadlock condition):
    
     CPU 0:                          CPU 1:                        CPU 2:
     -----                           -----                         -----
     led_trigger_event():
       read_lock(&trig->leddev_list_lock);
                                    <workqueue>
                                    ata_hsm_qc_complete():
                                      spin_lock_irqsave(&host->lock);
                                                                    write_lock(&trig->leddev_list_lock);
                                      ata_port_freeze():
                                        ata_do_link_abort():
                                          ata_qc_complete():
                                            ledtrig_disk_activity():
                                              led_trigger_blink_oneshot():
                                                read_lock(&trig->leddev_list_lock);
                                                // ^ not in in_interrupt() context, so could get blocked by CPU 2
     <interrupt>
       ata_bmdma_interrupt():
         spin_lock_irqsave(&host->lock);
    
    Fix by using read_lock_irqsave/irqrestore() in led_trigger_event(), so
    that no interrupt can happen in between, preventing the deadlock
    condition.
    
    Apply the same change to led_trigger_blink_setup() as well, since the
    same deadlock scenario can also happen in power_supply_update_bat_leds()
    -> led_trigger_blink() -> led_trigger_blink_setup() (workqueue context),
    and potentially prevent other similar usages.
    
    Link: https://lore.kernel.org/lkml/20201101092614.GB3989@xps-13-7390/
    Fixes: eb25cb9956cc ("leds: convert IDE trigger to common disk trigger")
    Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
    Signed-off-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6aceac245059768d2d60355b89726c933a2d5456
Author: Andrea Righi <andrea.righi@canonical.com>
Date:   Wed Nov 25 16:18:22 2020 +0100

    leds: trigger: fix potential deadlock with libata
    
    commit 27af8e2c90fba242460b01fa020e6e19ed68c495 upstream.
    
    We have the following potential deadlock condition:
    
     ========================================================
     WARNING: possible irq lock inversion dependency detected
     5.10.0-rc2+ #25 Not tainted
     --------------------------------------------------------
     swapper/3/0 just changed the state of lock:
     ffff8880063bd618 (&host->lock){-...}-{2:2}, at: ata_bmdma_interrupt+0x27/0x200
     but this lock took another, HARDIRQ-READ-unsafe lock in the past:
      (&trig->leddev_list_lock){.+.?}-{2:2}
    
     and interrupts could create inverse lock ordering between them.
    
     other info that might help us debug this:
      Possible interrupt unsafe locking scenario:
    
            CPU0                    CPU1
            ----                    ----
       lock(&trig->leddev_list_lock);
                                    local_irq_disable();
                                    lock(&host->lock);
                                    lock(&trig->leddev_list_lock);
       <Interrupt>
         lock(&host->lock);
    
      *** DEADLOCK ***
    
     no locks held by swapper/3/0.
    
     the shortest dependencies between 2nd lock and 1st lock:
      -> (&trig->leddev_list_lock){.+.?}-{2:2} ops: 46 {
         HARDIRQ-ON-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           rfkill_global_led_trigger_worker+0x94/0xb0
                           process_one_work+0x240/0x560
                           worker_thread+0x58/0x3d0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         IN-SOFTIRQ-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           kbd_bh+0x9e/0xc0
                           tasklet_action_common.constprop.0+0xe9/0x100
                           tasklet_action+0x22/0x30
                           __do_softirq+0xcc/0x46d
                           run_ksoftirqd+0x3f/0x70
                           smpboot_thread_fn+0x116/0x1f0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         SOFTIRQ-ON-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           rfkill_global_led_trigger_worker+0x94/0xb0
                           process_one_work+0x240/0x560
                           worker_thread+0x58/0x3d0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         INITIAL READ USE at:
                               lock_acquire+0x15f/0x420
                               _raw_read_lock+0x42/0x90
                               led_trigger_event+0x2b/0x70
                               rfkill_global_led_trigger_worker+0x94/0xb0
                               process_one_work+0x240/0x560
                               worker_thread+0x58/0x3d0
                               kthread+0x151/0x170
                               ret_from_fork+0x1f/0x30
       }
       ... key      at: [<ffffffff83da4c00>] __key.0+0x0/0x10
       ... acquired at:
        _raw_read_lock+0x42/0x90
        led_trigger_blink_oneshot+0x3b/0x90
        ledtrig_disk_activity+0x3c/0xa0
        ata_qc_complete+0x26/0x450
        ata_do_link_abort+0xa3/0xe0
        ata_port_freeze+0x2e/0x40
        ata_hsm_qc_complete+0x94/0xa0
        ata_sff_hsm_move+0x177/0x7a0
        ata_sff_pio_task+0xc7/0x1b0
        process_one_work+0x240/0x560
        worker_thread+0x58/0x3d0
        kthread+0x151/0x170
        ret_from_fork+0x1f/0x30
    
     -> (&host->lock){-...}-{2:2} ops: 69 {
        IN-HARDIRQ-W at:
                         lock_acquire+0x15f/0x420
                         _raw_spin_lock_irqsave+0x52/0xa0
                         ata_bmdma_interrupt+0x27/0x200
                         __handle_irq_event_percpu+0xd5/0x2b0
                         handle_irq_event+0x57/0xb0
                         handle_edge_irq+0x8c/0x230
                         asm_call_irq_on_stack+0xf/0x20
                         common_interrupt+0x100/0x1c0
                         asm_common_interrupt+0x1e/0x40
                         native_safe_halt+0xe/0x10
                         arch_cpu_idle+0x15/0x20
                         default_idle_call+0x59/0x1c0
                         do_idle+0x22c/0x2c0
                         cpu_startup_entry+0x20/0x30
                         start_secondary+0x11d/0x150
                         secondary_startup_64_no_verify+0xa6/0xab
        INITIAL USE at:
                        lock_acquire+0x15f/0x420
                        _raw_spin_lock_irqsave+0x52/0xa0
                        ata_dev_init+0x54/0xe0
                        ata_link_init+0x8b/0xd0
                        ata_port_alloc+0x1f1/0x210
                        ata_host_alloc+0xf1/0x130
                        ata_host_alloc_pinfo+0x14/0xb0
                        ata_pci_sff_prepare_host+0x41/0xa0
                        ata_pci_bmdma_prepare_host+0x14/0x30
                        piix_init_one+0x21f/0x600
                        local_pci_probe+0x48/0x80
                        pci_device_probe+0x105/0x1c0
                        really_probe+0x221/0x490
                        driver_probe_device+0xe9/0x160
                        device_driver_attach+0xb2/0xc0
                        __driver_attach+0x91/0x150
                        bus_for_each_dev+0x81/0xc0
                        driver_attach+0x1e/0x20
                        bus_add_driver+0x138/0x1f0
                        driver_register+0x91/0xf0
                        __pci_register_driver+0x73/0x80
                        piix_init+0x1e/0x2e
                        do_one_initcall+0x5f/0x2d0
                        kernel_init_freeable+0x26f/0x2cf
                        kernel_init+0xe/0x113
                        ret_from_fork+0x1f/0x30
      }
      ... key      at: [<ffffffff83d9fdc0>] __key.6+0x0/0x10
      ... acquired at:
        __lock_acquire+0x9da/0x2370
        lock_acquire+0x15f/0x420
        _raw_spin_lock_irqsave+0x52/0xa0
        ata_bmdma_interrupt+0x27/0x200
        __handle_irq_event_percpu+0xd5/0x2b0
        handle_irq_event+0x57/0xb0
        handle_edge_irq+0x8c/0x230
        asm_call_irq_on_stack+0xf/0x20
        common_interrupt+0x100/0x1c0
        asm_common_interrupt+0x1e/0x40
        native_safe_halt+0xe/0x10
        arch_cpu_idle+0x15/0x20
        default_idle_call+0x59/0x1c0
        do_idle+0x22c/0x2c0
        cpu_startup_entry+0x20/0x30
        start_secondary+0x11d/0x150
        secondary_startup_64_no_verify+0xa6/0xab
    
    This lockdep splat is reported after:
    commit e918188611f0 ("locking: More accurate annotations for read_lock()")
    
    To clarify:
     - read-locks are recursive only in interrupt context (when
       in_interrupt() returns true)
     - after acquiring host->lock in CPU1, another cpu (i.e. CPU2) may call
       write_lock(&trig->leddev_list_lock) that would be blocked by CPU0
       that holds trig->leddev_list_lock in read-mode
     - when CPU1 (ata_ac_complete()) tries to read-lock
       trig->leddev_list_lock, it would be blocked by the write-lock waiter
       on CPU2 (because we are not in interrupt context, so the read-lock is
       not recursive)
     - at this point if an interrupt happens on CPU0 and
       ata_bmdma_interrupt() is executed it will try to acquire host->lock,
       that is held by CPU1, that is currently blocked by CPU2, so:
    
       * CPU0 blocked by CPU1
       * CPU1 blocked by CPU2
       * CPU2 blocked by CPU0
    
         *** DEADLOCK ***
    
    The deadlock scenario is better represented by the following schema
    (thanks to Boqun Feng <boqun.feng@gmail.com> for the schema and the
    detailed explanation of the deadlock condition):
    
     CPU 0:                          CPU 1:                        CPU 2:
     -----                           -----                         -----
     led_trigger_event():
       read_lock(&trig->leddev_list_lock);
                                    <workqueue>
                                    ata_hsm_qc_complete():
                                      spin_lock_irqsave(&host->lock);
                                                                    write_lock(&trig->leddev_list_lock);
                                      ata_port_freeze():
                                        ata_do_link_abort():
                                          ata_qc_complete():
                                            ledtrig_disk_activity():
                                              led_trigger_blink_oneshot():
                                                read_lock(&trig->leddev_list_lock);
                                                // ^ not in in_interrupt() context, so could get blocked by CPU 2
     <interrupt>
       ata_bmdma_interrupt():
         spin_lock_irqsave(&host->lock);
    
    Fix by using read_lock_irqsave/irqrestore() in led_trigger_event(), so
    that no interrupt can happen in between, preventing the deadlock
    condition.
    
    Apply the same change to led_trigger_blink_setup() as well, since the
    same deadlock scenario can also happen in power_supply_update_bat_leds()
    -> led_trigger_blink() -> led_trigger_blink_setup() (workqueue context),
    and potentially prevent other similar usages.
    
    Link: https://lore.kernel.org/lkml/20201101092614.GB3989@xps-13-7390/
    Fixes: eb25cb9956cc ("leds: convert IDE trigger to common disk trigger")
    Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
    Signed-off-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 62b47c35c2737a4eb9ef9bdf356e8764d8579ba5
Author: Andrea Righi <andrea.righi@canonical.com>
Date:   Wed Nov 25 16:18:22 2020 +0100

    leds: trigger: fix potential deadlock with libata
    
    commit 27af8e2c90fba242460b01fa020e6e19ed68c495 upstream.
    
    We have the following potential deadlock condition:
    
     ========================================================
     WARNING: possible irq lock inversion dependency detected
     5.10.0-rc2+ #25 Not tainted
     --------------------------------------------------------
     swapper/3/0 just changed the state of lock:
     ffff8880063bd618 (&host->lock){-...}-{2:2}, at: ata_bmdma_interrupt+0x27/0x200
     but this lock took another, HARDIRQ-READ-unsafe lock in the past:
      (&trig->leddev_list_lock){.+.?}-{2:2}
    
     and interrupts could create inverse lock ordering between them.
    
     other info that might help us debug this:
      Possible interrupt unsafe locking scenario:
    
            CPU0                    CPU1
            ----                    ----
       lock(&trig->leddev_list_lock);
                                    local_irq_disable();
                                    lock(&host->lock);
                                    lock(&trig->leddev_list_lock);
       <Interrupt>
         lock(&host->lock);
    
      *** DEADLOCK ***
    
     no locks held by swapper/3/0.
    
     the shortest dependencies between 2nd lock and 1st lock:
      -> (&trig->leddev_list_lock){.+.?}-{2:2} ops: 46 {
         HARDIRQ-ON-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           rfkill_global_led_trigger_worker+0x94/0xb0
                           process_one_work+0x240/0x560
                           worker_thread+0x58/0x3d0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         IN-SOFTIRQ-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           kbd_bh+0x9e/0xc0
                           tasklet_action_common.constprop.0+0xe9/0x100
                           tasklet_action+0x22/0x30
                           __do_softirq+0xcc/0x46d
                           run_ksoftirqd+0x3f/0x70
                           smpboot_thread_fn+0x116/0x1f0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         SOFTIRQ-ON-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           rfkill_global_led_trigger_worker+0x94/0xb0
                           process_one_work+0x240/0x560
                           worker_thread+0x58/0x3d0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         INITIAL READ USE at:
                               lock_acquire+0x15f/0x420
                               _raw_read_lock+0x42/0x90
                               led_trigger_event+0x2b/0x70
                               rfkill_global_led_trigger_worker+0x94/0xb0
                               process_one_work+0x240/0x560
                               worker_thread+0x58/0x3d0
                               kthread+0x151/0x170
                               ret_from_fork+0x1f/0x30
       }
       ... key      at: [<ffffffff83da4c00>] __key.0+0x0/0x10
       ... acquired at:
        _raw_read_lock+0x42/0x90
        led_trigger_blink_oneshot+0x3b/0x90
        ledtrig_disk_activity+0x3c/0xa0
        ata_qc_complete+0x26/0x450
        ata_do_link_abort+0xa3/0xe0
        ata_port_freeze+0x2e/0x40
        ata_hsm_qc_complete+0x94/0xa0
        ata_sff_hsm_move+0x177/0x7a0
        ata_sff_pio_task+0xc7/0x1b0
        process_one_work+0x240/0x560
        worker_thread+0x58/0x3d0
        kthread+0x151/0x170
        ret_from_fork+0x1f/0x30
    
     -> (&host->lock){-...}-{2:2} ops: 69 {
        IN-HARDIRQ-W at:
                         lock_acquire+0x15f/0x420
                         _raw_spin_lock_irqsave+0x52/0xa0
                         ata_bmdma_interrupt+0x27/0x200
                         __handle_irq_event_percpu+0xd5/0x2b0
                         handle_irq_event+0x57/0xb0
                         handle_edge_irq+0x8c/0x230
                         asm_call_irq_on_stack+0xf/0x20
                         common_interrupt+0x100/0x1c0
                         asm_common_interrupt+0x1e/0x40
                         native_safe_halt+0xe/0x10
                         arch_cpu_idle+0x15/0x20
                         default_idle_call+0x59/0x1c0
                         do_idle+0x22c/0x2c0
                         cpu_startup_entry+0x20/0x30
                         start_secondary+0x11d/0x150
                         secondary_startup_64_no_verify+0xa6/0xab
        INITIAL USE at:
                        lock_acquire+0x15f/0x420
                        _raw_spin_lock_irqsave+0x52/0xa0
                        ata_dev_init+0x54/0xe0
                        ata_link_init+0x8b/0xd0
                        ata_port_alloc+0x1f1/0x210
                        ata_host_alloc+0xf1/0x130
                        ata_host_alloc_pinfo+0x14/0xb0
                        ata_pci_sff_prepare_host+0x41/0xa0
                        ata_pci_bmdma_prepare_host+0x14/0x30
                        piix_init_one+0x21f/0x600
                        local_pci_probe+0x48/0x80
                        pci_device_probe+0x105/0x1c0
                        really_probe+0x221/0x490
                        driver_probe_device+0xe9/0x160
                        device_driver_attach+0xb2/0xc0
                        __driver_attach+0x91/0x150
                        bus_for_each_dev+0x81/0xc0
                        driver_attach+0x1e/0x20
                        bus_add_driver+0x138/0x1f0
                        driver_register+0x91/0xf0
                        __pci_register_driver+0x73/0x80
                        piix_init+0x1e/0x2e
                        do_one_initcall+0x5f/0x2d0
                        kernel_init_freeable+0x26f/0x2cf
                        kernel_init+0xe/0x113
                        ret_from_fork+0x1f/0x30
      }
      ... key      at: [<ffffffff83d9fdc0>] __key.6+0x0/0x10
      ... acquired at:
        __lock_acquire+0x9da/0x2370
        lock_acquire+0x15f/0x420
        _raw_spin_lock_irqsave+0x52/0xa0
        ata_bmdma_interrupt+0x27/0x200
        __handle_irq_event_percpu+0xd5/0x2b0
        handle_irq_event+0x57/0xb0
        handle_edge_irq+0x8c/0x230
        asm_call_irq_on_stack+0xf/0x20
        common_interrupt+0x100/0x1c0
        asm_common_interrupt+0x1e/0x40
        native_safe_halt+0xe/0x10
        arch_cpu_idle+0x15/0x20
        default_idle_call+0x59/0x1c0
        do_idle+0x22c/0x2c0
        cpu_startup_entry+0x20/0x30
        start_secondary+0x11d/0x150
        secondary_startup_64_no_verify+0xa6/0xab
    
    This lockdep splat is reported after:
    commit e918188611f0 ("locking: More accurate annotations for read_lock()")
    
    To clarify:
     - read-locks are recursive only in interrupt context (when
       in_interrupt() returns true)
     - after acquiring host->lock in CPU1, another cpu (i.e. CPU2) may call
       write_lock(&trig->leddev_list_lock) that would be blocked by CPU0
       that holds trig->leddev_list_lock in read-mode
     - when CPU1 (ata_ac_complete()) tries to read-lock
       trig->leddev_list_lock, it would be blocked by the write-lock waiter
       on CPU2 (because we are not in interrupt context, so the read-lock is
       not recursive)
     - at this point if an interrupt happens on CPU0 and
       ata_bmdma_interrupt() is executed it will try to acquire host->lock,
       that is held by CPU1, that is currently blocked by CPU2, so:
    
       * CPU0 blocked by CPU1
       * CPU1 blocked by CPU2
       * CPU2 blocked by CPU0
    
         *** DEADLOCK ***
    
    The deadlock scenario is better represented by the following schema
    (thanks to Boqun Feng <boqun.feng@gmail.com> for the schema and the
    detailed explanation of the deadlock condition):
    
     CPU 0:                          CPU 1:                        CPU 2:
     -----                           -----                         -----
     led_trigger_event():
       read_lock(&trig->leddev_list_lock);
                                    <workqueue>
                                    ata_hsm_qc_complete():
                                      spin_lock_irqsave(&host->lock);
                                                                    write_lock(&trig->leddev_list_lock);
                                      ata_port_freeze():
                                        ata_do_link_abort():
                                          ata_qc_complete():
                                            ledtrig_disk_activity():
                                              led_trigger_blink_oneshot():
                                                read_lock(&trig->leddev_list_lock);
                                                // ^ not in in_interrupt() context, so could get blocked by CPU 2
     <interrupt>
       ata_bmdma_interrupt():
         spin_lock_irqsave(&host->lock);
    
    Fix by using read_lock_irqsave/irqrestore() in led_trigger_event(), so
    that no interrupt can happen in between, preventing the deadlock
    condition.
    
    Apply the same change to led_trigger_blink_setup() as well, since the
    same deadlock scenario can also happen in power_supply_update_bat_leds()
    -> led_trigger_blink() -> led_trigger_blink_setup() (workqueue context),
    and potentially prevent other similar usages.
    
    Link: https://lore.kernel.org/lkml/20201101092614.GB3989@xps-13-7390/
    Fixes: eb25cb9956cc ("leds: convert IDE trigger to common disk trigger")
    Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
    Signed-off-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 659788d29cc31ccf9a837945878e0f4652d7e9b8
Author: Andrea Righi <andrea.righi@canonical.com>
Date:   Wed Nov 25 16:18:22 2020 +0100

    leds: trigger: fix potential deadlock with libata
    
    commit 27af8e2c90fba242460b01fa020e6e19ed68c495 upstream.
    
    We have the following potential deadlock condition:
    
     ========================================================
     WARNING: possible irq lock inversion dependency detected
     5.10.0-rc2+ #25 Not tainted
     --------------------------------------------------------
     swapper/3/0 just changed the state of lock:
     ffff8880063bd618 (&host->lock){-...}-{2:2}, at: ata_bmdma_interrupt+0x27/0x200
     but this lock took another, HARDIRQ-READ-unsafe lock in the past:
      (&trig->leddev_list_lock){.+.?}-{2:2}
    
     and interrupts could create inverse lock ordering between them.
    
     other info that might help us debug this:
      Possible interrupt unsafe locking scenario:
    
            CPU0                    CPU1
            ----                    ----
       lock(&trig->leddev_list_lock);
                                    local_irq_disable();
                                    lock(&host->lock);
                                    lock(&trig->leddev_list_lock);
       <Interrupt>
         lock(&host->lock);
    
      *** DEADLOCK ***
    
     no locks held by swapper/3/0.
    
     the shortest dependencies between 2nd lock and 1st lock:
      -> (&trig->leddev_list_lock){.+.?}-{2:2} ops: 46 {
         HARDIRQ-ON-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           rfkill_global_led_trigger_worker+0x94/0xb0
                           process_one_work+0x240/0x560
                           worker_thread+0x58/0x3d0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         IN-SOFTIRQ-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           kbd_bh+0x9e/0xc0
                           tasklet_action_common.constprop.0+0xe9/0x100
                           tasklet_action+0x22/0x30
                           __do_softirq+0xcc/0x46d
                           run_ksoftirqd+0x3f/0x70
                           smpboot_thread_fn+0x116/0x1f0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         SOFTIRQ-ON-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           rfkill_global_led_trigger_worker+0x94/0xb0
                           process_one_work+0x240/0x560
                           worker_thread+0x58/0x3d0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         INITIAL READ USE at:
                               lock_acquire+0x15f/0x420
                               _raw_read_lock+0x42/0x90
                               led_trigger_event+0x2b/0x70
                               rfkill_global_led_trigger_worker+0x94/0xb0
                               process_one_work+0x240/0x560
                               worker_thread+0x58/0x3d0
                               kthread+0x151/0x170
                               ret_from_fork+0x1f/0x30
       }
       ... key      at: [<ffffffff83da4c00>] __key.0+0x0/0x10
       ... acquired at:
        _raw_read_lock+0x42/0x90
        led_trigger_blink_oneshot+0x3b/0x90
        ledtrig_disk_activity+0x3c/0xa0
        ata_qc_complete+0x26/0x450
        ata_do_link_abort+0xa3/0xe0
        ata_port_freeze+0x2e/0x40
        ata_hsm_qc_complete+0x94/0xa0
        ata_sff_hsm_move+0x177/0x7a0
        ata_sff_pio_task+0xc7/0x1b0
        process_one_work+0x240/0x560
        worker_thread+0x58/0x3d0
        kthread+0x151/0x170
        ret_from_fork+0x1f/0x30
    
     -> (&host->lock){-...}-{2:2} ops: 69 {
        IN-HARDIRQ-W at:
                         lock_acquire+0x15f/0x420
                         _raw_spin_lock_irqsave+0x52/0xa0
                         ata_bmdma_interrupt+0x27/0x200
                         __handle_irq_event_percpu+0xd5/0x2b0
                         handle_irq_event+0x57/0xb0
                         handle_edge_irq+0x8c/0x230
                         asm_call_irq_on_stack+0xf/0x20
                         common_interrupt+0x100/0x1c0
                         asm_common_interrupt+0x1e/0x40
                         native_safe_halt+0xe/0x10
                         arch_cpu_idle+0x15/0x20
                         default_idle_call+0x59/0x1c0
                         do_idle+0x22c/0x2c0
                         cpu_startup_entry+0x20/0x30
                         start_secondary+0x11d/0x150
                         secondary_startup_64_no_verify+0xa6/0xab
        INITIAL USE at:
                        lock_acquire+0x15f/0x420
                        _raw_spin_lock_irqsave+0x52/0xa0
                        ata_dev_init+0x54/0xe0
                        ata_link_init+0x8b/0xd0
                        ata_port_alloc+0x1f1/0x210
                        ata_host_alloc+0xf1/0x130
                        ata_host_alloc_pinfo+0x14/0xb0
                        ata_pci_sff_prepare_host+0x41/0xa0
                        ata_pci_bmdma_prepare_host+0x14/0x30
                        piix_init_one+0x21f/0x600
                        local_pci_probe+0x48/0x80
                        pci_device_probe+0x105/0x1c0
                        really_probe+0x221/0x490
                        driver_probe_device+0xe9/0x160
                        device_driver_attach+0xb2/0xc0
                        __driver_attach+0x91/0x150
                        bus_for_each_dev+0x81/0xc0
                        driver_attach+0x1e/0x20
                        bus_add_driver+0x138/0x1f0
                        driver_register+0x91/0xf0
                        __pci_register_driver+0x73/0x80
                        piix_init+0x1e/0x2e
                        do_one_initcall+0x5f/0x2d0
                        kernel_init_freeable+0x26f/0x2cf
                        kernel_init+0xe/0x113
                        ret_from_fork+0x1f/0x30
      }
      ... key      at: [<ffffffff83d9fdc0>] __key.6+0x0/0x10
      ... acquired at:
        __lock_acquire+0x9da/0x2370
        lock_acquire+0x15f/0x420
        _raw_spin_lock_irqsave+0x52/0xa0
        ata_bmdma_interrupt+0x27/0x200
        __handle_irq_event_percpu+0xd5/0x2b0
        handle_irq_event+0x57/0xb0
        handle_edge_irq+0x8c/0x230
        asm_call_irq_on_stack+0xf/0x20
        common_interrupt+0x100/0x1c0
        asm_common_interrupt+0x1e/0x40
        native_safe_halt+0xe/0x10
        arch_cpu_idle+0x15/0x20
        default_idle_call+0x59/0x1c0
        do_idle+0x22c/0x2c0
        cpu_startup_entry+0x20/0x30
        start_secondary+0x11d/0x150
        secondary_startup_64_no_verify+0xa6/0xab
    
    This lockdep splat is reported after:
    commit e918188611f0 ("locking: More accurate annotations for read_lock()")
    
    To clarify:
     - read-locks are recursive only in interrupt context (when
       in_interrupt() returns true)
     - after acquiring host->lock in CPU1, another cpu (i.e. CPU2) may call
       write_lock(&trig->leddev_list_lock) that would be blocked by CPU0
       that holds trig->leddev_list_lock in read-mode
     - when CPU1 (ata_ac_complete()) tries to read-lock
       trig->leddev_list_lock, it would be blocked by the write-lock waiter
       on CPU2 (because we are not in interrupt context, so the read-lock is
       not recursive)
     - at this point if an interrupt happens on CPU0 and
       ata_bmdma_interrupt() is executed it will try to acquire host->lock,
       that is held by CPU1, that is currently blocked by CPU2, so:
    
       * CPU0 blocked by CPU1
       * CPU1 blocked by CPU2
       * CPU2 blocked by CPU0
    
         *** DEADLOCK ***
    
    The deadlock scenario is better represented by the following schema
    (thanks to Boqun Feng <boqun.feng@gmail.com> for the schema and the
    detailed explanation of the deadlock condition):
    
     CPU 0:                          CPU 1:                        CPU 2:
     -----                           -----                         -----
     led_trigger_event():
       read_lock(&trig->leddev_list_lock);
                                    <workqueue>
                                    ata_hsm_qc_complete():
                                      spin_lock_irqsave(&host->lock);
                                                                    write_lock(&trig->leddev_list_lock);
                                      ata_port_freeze():
                                        ata_do_link_abort():
                                          ata_qc_complete():
                                            ledtrig_disk_activity():
                                              led_trigger_blink_oneshot():
                                                read_lock(&trig->leddev_list_lock);
                                                // ^ not in in_interrupt() context, so could get blocked by CPU 2
     <interrupt>
       ata_bmdma_interrupt():
         spin_lock_irqsave(&host->lock);
    
    Fix by using read_lock_irqsave/irqrestore() in led_trigger_event(), so
    that no interrupt can happen in between, preventing the deadlock
    condition.
    
    Apply the same change to led_trigger_blink_setup() as well, since the
    same deadlock scenario can also happen in power_supply_update_bat_leds()
    -> led_trigger_blink() -> led_trigger_blink_setup() (workqueue context),
    and potentially prevent other similar usages.
    
    Link: https://lore.kernel.org/lkml/20201101092614.GB3989@xps-13-7390/
    Fixes: eb25cb9956cc ("leds: convert IDE trigger to common disk trigger")
    Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
    Signed-off-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3aeace1ce92722ef40a3f8121eb39d9e40a5fd8f
Author: Andrea Righi <andrea.righi@canonical.com>
Date:   Wed Nov 25 16:18:22 2020 +0100

    leds: trigger: fix potential deadlock with libata
    
    commit 27af8e2c90fba242460b01fa020e6e19ed68c495 upstream.
    
    We have the following potential deadlock condition:
    
     ========================================================
     WARNING: possible irq lock inversion dependency detected
     5.10.0-rc2+ #25 Not tainted
     --------------------------------------------------------
     swapper/3/0 just changed the state of lock:
     ffff8880063bd618 (&host->lock){-...}-{2:2}, at: ata_bmdma_interrupt+0x27/0x200
     but this lock took another, HARDIRQ-READ-unsafe lock in the past:
      (&trig->leddev_list_lock){.+.?}-{2:2}
    
     and interrupts could create inverse lock ordering between them.
    
     other info that might help us debug this:
      Possible interrupt unsafe locking scenario:
    
            CPU0                    CPU1
            ----                    ----
       lock(&trig->leddev_list_lock);
                                    local_irq_disable();
                                    lock(&host->lock);
                                    lock(&trig->leddev_list_lock);
       <Interrupt>
         lock(&host->lock);
    
      *** DEADLOCK ***
    
     no locks held by swapper/3/0.
    
     the shortest dependencies between 2nd lock and 1st lock:
      -> (&trig->leddev_list_lock){.+.?}-{2:2} ops: 46 {
         HARDIRQ-ON-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           rfkill_global_led_trigger_worker+0x94/0xb0
                           process_one_work+0x240/0x560
                           worker_thread+0x58/0x3d0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         IN-SOFTIRQ-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           kbd_bh+0x9e/0xc0
                           tasklet_action_common.constprop.0+0xe9/0x100
                           tasklet_action+0x22/0x30
                           __do_softirq+0xcc/0x46d
                           run_ksoftirqd+0x3f/0x70
                           smpboot_thread_fn+0x116/0x1f0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         SOFTIRQ-ON-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           rfkill_global_led_trigger_worker+0x94/0xb0
                           process_one_work+0x240/0x560
                           worker_thread+0x58/0x3d0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         INITIAL READ USE at:
                               lock_acquire+0x15f/0x420
                               _raw_read_lock+0x42/0x90
                               led_trigger_event+0x2b/0x70
                               rfkill_global_led_trigger_worker+0x94/0xb0
                               process_one_work+0x240/0x560
                               worker_thread+0x58/0x3d0
                               kthread+0x151/0x170
                               ret_from_fork+0x1f/0x30
       }
       ... key      at: [<ffffffff83da4c00>] __key.0+0x0/0x10
       ... acquired at:
        _raw_read_lock+0x42/0x90
        led_trigger_blink_oneshot+0x3b/0x90
        ledtrig_disk_activity+0x3c/0xa0
        ata_qc_complete+0x26/0x450
        ata_do_link_abort+0xa3/0xe0
        ata_port_freeze+0x2e/0x40
        ata_hsm_qc_complete+0x94/0xa0
        ata_sff_hsm_move+0x177/0x7a0
        ata_sff_pio_task+0xc7/0x1b0
        process_one_work+0x240/0x560
        worker_thread+0x58/0x3d0
        kthread+0x151/0x170
        ret_from_fork+0x1f/0x30
    
     -> (&host->lock){-...}-{2:2} ops: 69 {
        IN-HARDIRQ-W at:
                         lock_acquire+0x15f/0x420
                         _raw_spin_lock_irqsave+0x52/0xa0
                         ata_bmdma_interrupt+0x27/0x200
                         __handle_irq_event_percpu+0xd5/0x2b0
                         handle_irq_event+0x57/0xb0
                         handle_edge_irq+0x8c/0x230
                         asm_call_irq_on_stack+0xf/0x20
                         common_interrupt+0x100/0x1c0
                         asm_common_interrupt+0x1e/0x40
                         native_safe_halt+0xe/0x10
                         arch_cpu_idle+0x15/0x20
                         default_idle_call+0x59/0x1c0
                         do_idle+0x22c/0x2c0
                         cpu_startup_entry+0x20/0x30
                         start_secondary+0x11d/0x150
                         secondary_startup_64_no_verify+0xa6/0xab
        INITIAL USE at:
                        lock_acquire+0x15f/0x420
                        _raw_spin_lock_irqsave+0x52/0xa0
                        ata_dev_init+0x54/0xe0
                        ata_link_init+0x8b/0xd0
                        ata_port_alloc+0x1f1/0x210
                        ata_host_alloc+0xf1/0x130
                        ata_host_alloc_pinfo+0x14/0xb0
                        ata_pci_sff_prepare_host+0x41/0xa0
                        ata_pci_bmdma_prepare_host+0x14/0x30
                        piix_init_one+0x21f/0x600
                        local_pci_probe+0x48/0x80
                        pci_device_probe+0x105/0x1c0
                        really_probe+0x221/0x490
                        driver_probe_device+0xe9/0x160
                        device_driver_attach+0xb2/0xc0
                        __driver_attach+0x91/0x150
                        bus_for_each_dev+0x81/0xc0
                        driver_attach+0x1e/0x20
                        bus_add_driver+0x138/0x1f0
                        driver_register+0x91/0xf0
                        __pci_register_driver+0x73/0x80
                        piix_init+0x1e/0x2e
                        do_one_initcall+0x5f/0x2d0
                        kernel_init_freeable+0x26f/0x2cf
                        kernel_init+0xe/0x113
                        ret_from_fork+0x1f/0x30
      }
      ... key      at: [<ffffffff83d9fdc0>] __key.6+0x0/0x10
      ... acquired at:
        __lock_acquire+0x9da/0x2370
        lock_acquire+0x15f/0x420
        _raw_spin_lock_irqsave+0x52/0xa0
        ata_bmdma_interrupt+0x27/0x200
        __handle_irq_event_percpu+0xd5/0x2b0
        handle_irq_event+0x57/0xb0
        handle_edge_irq+0x8c/0x230
        asm_call_irq_on_stack+0xf/0x20
        common_interrupt+0x100/0x1c0
        asm_common_interrupt+0x1e/0x40
        native_safe_halt+0xe/0x10
        arch_cpu_idle+0x15/0x20
        default_idle_call+0x59/0x1c0
        do_idle+0x22c/0x2c0
        cpu_startup_entry+0x20/0x30
        start_secondary+0x11d/0x150
        secondary_startup_64_no_verify+0xa6/0xab
    
    This lockdep splat is reported after:
    commit e918188611f0 ("locking: More accurate annotations for read_lock()")
    
    To clarify:
     - read-locks are recursive only in interrupt context (when
       in_interrupt() returns true)
     - after acquiring host->lock in CPU1, another cpu (i.e. CPU2) may call
       write_lock(&trig->leddev_list_lock) that would be blocked by CPU0
       that holds trig->leddev_list_lock in read-mode
     - when CPU1 (ata_ac_complete()) tries to read-lock
       trig->leddev_list_lock, it would be blocked by the write-lock waiter
       on CPU2 (because we are not in interrupt context, so the read-lock is
       not recursive)
     - at this point if an interrupt happens on CPU0 and
       ata_bmdma_interrupt() is executed it will try to acquire host->lock,
       that is held by CPU1, that is currently blocked by CPU2, so:
    
       * CPU0 blocked by CPU1
       * CPU1 blocked by CPU2
       * CPU2 blocked by CPU0
    
         *** DEADLOCK ***
    
    The deadlock scenario is better represented by the following schema
    (thanks to Boqun Feng <boqun.feng@gmail.com> for the schema and the
    detailed explanation of the deadlock condition):
    
     CPU 0:                          CPU 1:                        CPU 2:
     -----                           -----                         -----
     led_trigger_event():
       read_lock(&trig->leddev_list_lock);
                                    <workqueue>
                                    ata_hsm_qc_complete():
                                      spin_lock_irqsave(&host->lock);
                                                                    write_lock(&trig->leddev_list_lock);
                                      ata_port_freeze():
                                        ata_do_link_abort():
                                          ata_qc_complete():
                                            ledtrig_disk_activity():
                                              led_trigger_blink_oneshot():
                                                read_lock(&trig->leddev_list_lock);
                                                // ^ not in in_interrupt() context, so could get blocked by CPU 2
     <interrupt>
       ata_bmdma_interrupt():
         spin_lock_irqsave(&host->lock);
    
    Fix by using read_lock_irqsave/irqrestore() in led_trigger_event(), so
    that no interrupt can happen in between, preventing the deadlock
    condition.
    
    Apply the same change to led_trigger_blink_setup() as well, since the
    same deadlock scenario can also happen in power_supply_update_bat_leds()
    -> led_trigger_blink() -> led_trigger_blink_setup() (workqueue context),
    and potentially prevent other similar usages.
    
    Link: https://lore.kernel.org/lkml/20201101092614.GB3989@xps-13-7390/
    Fixes: eb25cb9956cc ("leds: convert IDE trigger to common disk trigger")
    Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
    Signed-off-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 27af8e2c90fba242460b01fa020e6e19ed68c495
Author: Andrea Righi <andrea.righi@canonical.com>
Date:   Wed Nov 25 16:18:22 2020 +0100

    leds: trigger: fix potential deadlock with libata
    
    We have the following potential deadlock condition:
    
     ========================================================
     WARNING: possible irq lock inversion dependency detected
     5.10.0-rc2+ #25 Not tainted
     --------------------------------------------------------
     swapper/3/0 just changed the state of lock:
     ffff8880063bd618 (&host->lock){-...}-{2:2}, at: ata_bmdma_interrupt+0x27/0x200
     but this lock took another, HARDIRQ-READ-unsafe lock in the past:
      (&trig->leddev_list_lock){.+.?}-{2:2}
    
     and interrupts could create inverse lock ordering between them.
    
     other info that might help us debug this:
      Possible interrupt unsafe locking scenario:
    
            CPU0                    CPU1
            ----                    ----
       lock(&trig->leddev_list_lock);
                                    local_irq_disable();
                                    lock(&host->lock);
                                    lock(&trig->leddev_list_lock);
       <Interrupt>
         lock(&host->lock);
    
      *** DEADLOCK ***
    
     no locks held by swapper/3/0.
    
     the shortest dependencies between 2nd lock and 1st lock:
      -> (&trig->leddev_list_lock){.+.?}-{2:2} ops: 46 {
         HARDIRQ-ON-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           rfkill_global_led_trigger_worker+0x94/0xb0
                           process_one_work+0x240/0x560
                           worker_thread+0x58/0x3d0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         IN-SOFTIRQ-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           kbd_bh+0x9e/0xc0
                           tasklet_action_common.constprop.0+0xe9/0x100
                           tasklet_action+0x22/0x30
                           __do_softirq+0xcc/0x46d
                           run_ksoftirqd+0x3f/0x70
                           smpboot_thread_fn+0x116/0x1f0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         SOFTIRQ-ON-R at:
                           lock_acquire+0x15f/0x420
                           _raw_read_lock+0x42/0x90
                           led_trigger_event+0x2b/0x70
                           rfkill_global_led_trigger_worker+0x94/0xb0
                           process_one_work+0x240/0x560
                           worker_thread+0x58/0x3d0
                           kthread+0x151/0x170
                           ret_from_fork+0x1f/0x30
         INITIAL READ USE at:
                               lock_acquire+0x15f/0x420
                               _raw_read_lock+0x42/0x90
                               led_trigger_event+0x2b/0x70
                               rfkill_global_led_trigger_worker+0x94/0xb0
                               process_one_work+0x240/0x560
                               worker_thread+0x58/0x3d0
                               kthread+0x151/0x170
                               ret_from_fork+0x1f/0x30
       }
       ... key      at: [<ffffffff83da4c00>] __key.0+0x0/0x10
       ... acquired at:
        _raw_read_lock+0x42/0x90
        led_trigger_blink_oneshot+0x3b/0x90
        ledtrig_disk_activity+0x3c/0xa0
        ata_qc_complete+0x26/0x450
        ata_do_link_abort+0xa3/0xe0
        ata_port_freeze+0x2e/0x40
        ata_hsm_qc_complete+0x94/0xa0
        ata_sff_hsm_move+0x177/0x7a0
        ata_sff_pio_task+0xc7/0x1b0
        process_one_work+0x240/0x560
        worker_thread+0x58/0x3d0
        kthread+0x151/0x170
        ret_from_fork+0x1f/0x30
    
     -> (&host->lock){-...}-{2:2} ops: 69 {
        IN-HARDIRQ-W at:
                         lock_acquire+0x15f/0x420
                         _raw_spin_lock_irqsave+0x52/0xa0
                         ata_bmdma_interrupt+0x27/0x200
                         __handle_irq_event_percpu+0xd5/0x2b0
                         handle_irq_event+0x57/0xb0
                         handle_edge_irq+0x8c/0x230
                         asm_call_irq_on_stack+0xf/0x20
                         common_interrupt+0x100/0x1c0
                         asm_common_interrupt+0x1e/0x40
                         native_safe_halt+0xe/0x10
                         arch_cpu_idle+0x15/0x20
                         default_idle_call+0x59/0x1c0
                         do_idle+0x22c/0x2c0
                         cpu_startup_entry+0x20/0x30
                         start_secondary+0x11d/0x150
                         secondary_startup_64_no_verify+0xa6/0xab
        INITIAL USE at:
                        lock_acquire+0x15f/0x420
                        _raw_spin_lock_irqsave+0x52/0xa0
                        ata_dev_init+0x54/0xe0
                        ata_link_init+0x8b/0xd0
                        ata_port_alloc+0x1f1/0x210
                        ata_host_alloc+0xf1/0x130
                        ata_host_alloc_pinfo+0x14/0xb0
                        ata_pci_sff_prepare_host+0x41/0xa0
                        ata_pci_bmdma_prepare_host+0x14/0x30
                        piix_init_one+0x21f/0x600
                        local_pci_probe+0x48/0x80
                        pci_device_probe+0x105/0x1c0
                        really_probe+0x221/0x490
                        driver_probe_device+0xe9/0x160
                        device_driver_attach+0xb2/0xc0
                        __driver_attach+0x91/0x150
                        bus_for_each_dev+0x81/0xc0
                        driver_attach+0x1e/0x20
                        bus_add_driver+0x138/0x1f0
                        driver_register+0x91/0xf0
                        __pci_register_driver+0x73/0x80
                        piix_init+0x1e/0x2e
                        do_one_initcall+0x5f/0x2d0
                        kernel_init_freeable+0x26f/0x2cf
                        kernel_init+0xe/0x113
                        ret_from_fork+0x1f/0x30
      }
      ... key      at: [<ffffffff83d9fdc0>] __key.6+0x0/0x10
      ... acquired at:
        __lock_acquire+0x9da/0x2370
        lock_acquire+0x15f/0x420
        _raw_spin_lock_irqsave+0x52/0xa0
        ata_bmdma_interrupt+0x27/0x200
        __handle_irq_event_percpu+0xd5/0x2b0
        handle_irq_event+0x57/0xb0
        handle_edge_irq+0x8c/0x230
        asm_call_irq_on_stack+0xf/0x20
        common_interrupt+0x100/0x1c0
        asm_common_interrupt+0x1e/0x40
        native_safe_halt+0xe/0x10
        arch_cpu_idle+0x15/0x20
        default_idle_call+0x59/0x1c0
        do_idle+0x22c/0x2c0
        cpu_startup_entry+0x20/0x30
        start_secondary+0x11d/0x150
        secondary_startup_64_no_verify+0xa6/0xab
    
    This lockdep splat is reported after:
    commit e918188611f0 ("locking: More accurate annotations for read_lock()")
    
    To clarify:
     - read-locks are recursive only in interrupt context (when
       in_interrupt() returns true)
     - after acquiring host->lock in CPU1, another cpu (i.e. CPU2) may call
       write_lock(&trig->leddev_list_lock) that would be blocked by CPU0
       that holds trig->leddev_list_lock in read-mode
     - when CPU1 (ata_ac_complete()) tries to read-lock
       trig->leddev_list_lock, it would be blocked by the write-lock waiter
       on CPU2 (because we are not in interrupt context, so the read-lock is
       not recursive)
     - at this point if an interrupt happens on CPU0 and
       ata_bmdma_interrupt() is executed it will try to acquire host->lock,
       that is held by CPU1, that is currently blocked by CPU2, so:
    
       * CPU0 blocked by CPU1
       * CPU1 blocked by CPU2
       * CPU2 blocked by CPU0
    
         *** DEADLOCK ***
    
    The deadlock scenario is better represented by the following schema
    (thanks to Boqun Feng <boqun.feng@gmail.com> for the schema and the
    detailed explanation of the deadlock condition):
    
     CPU 0:                          CPU 1:                        CPU 2:
     -----                           -----                         -----
     led_trigger_event():
       read_lock(&trig->leddev_list_lock);
                                    <workqueue>
                                    ata_hsm_qc_complete():
                                      spin_lock_irqsave(&host->lock);
                                                                    write_lock(&trig->leddev_list_lock);
                                      ata_port_freeze():
                                        ata_do_link_abort():
                                          ata_qc_complete():
                                            ledtrig_disk_activity():
                                              led_trigger_blink_oneshot():
                                                read_lock(&trig->leddev_list_lock);
                                                // ^ not in in_interrupt() context, so could get blocked by CPU 2
     <interrupt>
       ata_bmdma_interrupt():
         spin_lock_irqsave(&host->lock);
    
    Fix by using read_lock_irqsave/irqrestore() in led_trigger_event(), so
    that no interrupt can happen in between, preventing the deadlock
    condition.
    
    Apply the same change to led_trigger_blink_setup() as well, since the
    same deadlock scenario can also happen in power_supply_update_bat_leds()
    -> led_trigger_blink() -> led_trigger_blink_setup() (workqueue context),
    and potentially prevent other similar usages.
    
    Link: https://lore.kernel.org/lkml/20201101092614.GB3989@xps-13-7390/
    Fixes: eb25cb9956cc ("leds: convert IDE trigger to common disk trigger")
    Signed-off-by: Andrea Righi <andrea.righi@canonical.com>
    Signed-off-by: Pavel Machek <pavel@ucw.cz>

commit 908030501772553dc8553792d6c97a24000ab04a
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Thu Nov 5 14:23:51 2020 +0800

    fcntl: Fix potential deadlock in send_sig{io, urg}()
    
    commit 8d1ddb5e79374fb277985a6b3faa2ed8631c5b4c upstream.
    
    Syzbot reports a potential deadlock found by the newly added recursive
    read deadlock detection in lockdep:
    
    [...] ========================================================
    [...] WARNING: possible irq lock inversion dependency detected
    [...] 5.9.0-rc2-syzkaller #0 Not tainted
    [...] --------------------------------------------------------
    [...] syz-executor.1/10214 just changed the state of lock:
    [...] ffff88811f506338 (&f->f_owner.lock){.+..}-{2:2}, at: send_sigurg+0x1d/0x200
    [...] but this lock was taken by another, HARDIRQ-safe lock in the past:
    [...]  (&dev->event_lock){-...}-{2:2}
    [...]
    [...]
    [...] and interrupts could create inverse lock ordering between them.
    [...]
    [...]
    [...] other info that might help us debug this:
    [...] Chain exists of:
    [...]   &dev->event_lock --> &new->fa_lock --> &f->f_owner.lock
    [...]
    [...]  Possible interrupt unsafe locking scenario:
    [...]
    [...]        CPU0                    CPU1
    [...]        ----                    ----
    [...]   lock(&f->f_owner.lock);
    [...]                                local_irq_disable();
    [...]                                lock(&dev->event_lock);
    [...]                                lock(&new->fa_lock);
    [...]   <Interrupt>
    [...]     lock(&dev->event_lock);
    [...]
    [...]  *** DEADLOCK ***
    
    The corresponding deadlock case is as followed:
    
            CPU 0           CPU 1           CPU 2
            read_lock(&fown->lock);
                            spin_lock_irqsave(&dev->event_lock, ...)
                                            write_lock_irq(&filp->f_owner.lock); // wait for the lock
                            read_lock(&fown-lock); // have to wait until the writer release
                                                   // due to the fairness
            <interrupted>
            spin_lock_irqsave(&dev->event_lock); // wait for the lock
    
    The lock dependency on CPU 1 happens if there exists a call sequence:
    
            input_inject_event():
              spin_lock_irqsave(&dev->event_lock,...);
              input_handle_event():
                input_pass_values():
                  input_to_handler():
                    handler->event(): // evdev_event()
                      evdev_pass_values():
                        spin_lock(&client->buffer_lock);
                        __pass_event():
                          kill_fasync():
                            kill_fasync_rcu():
                              read_lock(&fa->fa_lock);
                              send_sigio():
                                read_lock(&fown->lock);
    
    To fix this, make the reader in send_sigurg() and send_sigio() use
    read_lock_irqsave() and read_lock_irqrestore().
    
    Reported-by: syzbot+22e87cdf94021b984aa6@syzkaller.appspotmail.com
    Reported-by: syzbot+c5e32344981ad9f33750@syzkaller.appspotmail.com
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 642c2d74c365ae8f642e50e3c141a8b8b35cc7dd
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Thu Nov 5 14:23:51 2020 +0800

    fcntl: Fix potential deadlock in send_sig{io, urg}()
    
    commit 8d1ddb5e79374fb277985a6b3faa2ed8631c5b4c upstream.
    
    Syzbot reports a potential deadlock found by the newly added recursive
    read deadlock detection in lockdep:
    
    [...] ========================================================
    [...] WARNING: possible irq lock inversion dependency detected
    [...] 5.9.0-rc2-syzkaller #0 Not tainted
    [...] --------------------------------------------------------
    [...] syz-executor.1/10214 just changed the state of lock:
    [...] ffff88811f506338 (&f->f_owner.lock){.+..}-{2:2}, at: send_sigurg+0x1d/0x200
    [...] but this lock was taken by another, HARDIRQ-safe lock in the past:
    [...]  (&dev->event_lock){-...}-{2:2}
    [...]
    [...]
    [...] and interrupts could create inverse lock ordering between them.
    [...]
    [...]
    [...] other info that might help us debug this:
    [...] Chain exists of:
    [...]   &dev->event_lock --> &new->fa_lock --> &f->f_owner.lock
    [...]
    [...]  Possible interrupt unsafe locking scenario:
    [...]
    [...]        CPU0                    CPU1
    [...]        ----                    ----
    [...]   lock(&f->f_owner.lock);
    [...]                                local_irq_disable();
    [...]                                lock(&dev->event_lock);
    [...]                                lock(&new->fa_lock);
    [...]   <Interrupt>
    [...]     lock(&dev->event_lock);
    [...]
    [...]  *** DEADLOCK ***
    
    The corresponding deadlock case is as followed:
    
            CPU 0           CPU 1           CPU 2
            read_lock(&fown->lock);
                            spin_lock_irqsave(&dev->event_lock, ...)
                                            write_lock_irq(&filp->f_owner.lock); // wait for the lock
                            read_lock(&fown-lock); // have to wait until the writer release
                                                   // due to the fairness
            <interrupted>
            spin_lock_irqsave(&dev->event_lock); // wait for the lock
    
    The lock dependency on CPU 1 happens if there exists a call sequence:
    
            input_inject_event():
              spin_lock_irqsave(&dev->event_lock,...);
              input_handle_event():
                input_pass_values():
                  input_to_handler():
                    handler->event(): // evdev_event()
                      evdev_pass_values():
                        spin_lock(&client->buffer_lock);
                        __pass_event():
                          kill_fasync():
                            kill_fasync_rcu():
                              read_lock(&fa->fa_lock);
                              send_sigio():
                                read_lock(&fown->lock);
    
    To fix this, make the reader in send_sigurg() and send_sigio() use
    read_lock_irqsave() and read_lock_irqrestore().
    
    Reported-by: syzbot+22e87cdf94021b984aa6@syzkaller.appspotmail.com
    Reported-by: syzbot+c5e32344981ad9f33750@syzkaller.appspotmail.com
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8e63266b0d42a2dc233cfc468636889b5b3ba1cf
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Thu Nov 5 14:23:51 2020 +0800

    fcntl: Fix potential deadlock in send_sig{io, urg}()
    
    commit 8d1ddb5e79374fb277985a6b3faa2ed8631c5b4c upstream.
    
    Syzbot reports a potential deadlock found by the newly added recursive
    read deadlock detection in lockdep:
    
    [...] ========================================================
    [...] WARNING: possible irq lock inversion dependency detected
    [...] 5.9.0-rc2-syzkaller #0 Not tainted
    [...] --------------------------------------------------------
    [...] syz-executor.1/10214 just changed the state of lock:
    [...] ffff88811f506338 (&f->f_owner.lock){.+..}-{2:2}, at: send_sigurg+0x1d/0x200
    [...] but this lock was taken by another, HARDIRQ-safe lock in the past:
    [...]  (&dev->event_lock){-...}-{2:2}
    [...]
    [...]
    [...] and interrupts could create inverse lock ordering between them.
    [...]
    [...]
    [...] other info that might help us debug this:
    [...] Chain exists of:
    [...]   &dev->event_lock --> &new->fa_lock --> &f->f_owner.lock
    [...]
    [...]  Possible interrupt unsafe locking scenario:
    [...]
    [...]        CPU0                    CPU1
    [...]        ----                    ----
    [...]   lock(&f->f_owner.lock);
    [...]                                local_irq_disable();
    [...]                                lock(&dev->event_lock);
    [...]                                lock(&new->fa_lock);
    [...]   <Interrupt>
    [...]     lock(&dev->event_lock);
    [...]
    [...]  *** DEADLOCK ***
    
    The corresponding deadlock case is as followed:
    
            CPU 0           CPU 1           CPU 2
            read_lock(&fown->lock);
                            spin_lock_irqsave(&dev->event_lock, ...)
                                            write_lock_irq(&filp->f_owner.lock); // wait for the lock
                            read_lock(&fown-lock); // have to wait until the writer release
                                                   // due to the fairness
            <interrupted>
            spin_lock_irqsave(&dev->event_lock); // wait for the lock
    
    The lock dependency on CPU 1 happens if there exists a call sequence:
    
            input_inject_event():
              spin_lock_irqsave(&dev->event_lock,...);
              input_handle_event():
                input_pass_values():
                  input_to_handler():
                    handler->event(): // evdev_event()
                      evdev_pass_values():
                        spin_lock(&client->buffer_lock);
                        __pass_event():
                          kill_fasync():
                            kill_fasync_rcu():
                              read_lock(&fa->fa_lock);
                              send_sigio():
                                read_lock(&fown->lock);
    
    To fix this, make the reader in send_sigurg() and send_sigio() use
    read_lock_irqsave() and read_lock_irqrestore().
    
    Reported-by: syzbot+22e87cdf94021b984aa6@syzkaller.appspotmail.com
    Reported-by: syzbot+c5e32344981ad9f33750@syzkaller.appspotmail.com
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8d1ddb5e79374fb277985a6b3faa2ed8631c5b4c
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Thu Nov 5 14:23:51 2020 +0800

    fcntl: Fix potential deadlock in send_sig{io, urg}()
    
    Syzbot reports a potential deadlock found by the newly added recursive
    read deadlock detection in lockdep:
    
    [...] ========================================================
    [...] WARNING: possible irq lock inversion dependency detected
    [...] 5.9.0-rc2-syzkaller #0 Not tainted
    [...] --------------------------------------------------------
    [...] syz-executor.1/10214 just changed the state of lock:
    [...] ffff88811f506338 (&f->f_owner.lock){.+..}-{2:2}, at: send_sigurg+0x1d/0x200
    [...] but this lock was taken by another, HARDIRQ-safe lock in the past:
    [...]  (&dev->event_lock){-...}-{2:2}
    [...]
    [...]
    [...] and interrupts could create inverse lock ordering between them.
    [...]
    [...]
    [...] other info that might help us debug this:
    [...] Chain exists of:
    [...]   &dev->event_lock --> &new->fa_lock --> &f->f_owner.lock
    [...]
    [...]  Possible interrupt unsafe locking scenario:
    [...]
    [...]        CPU0                    CPU1
    [...]        ----                    ----
    [...]   lock(&f->f_owner.lock);
    [...]                                local_irq_disable();
    [...]                                lock(&dev->event_lock);
    [...]                                lock(&new->fa_lock);
    [...]   <Interrupt>
    [...]     lock(&dev->event_lock);
    [...]
    [...]  *** DEADLOCK ***
    
    The corresponding deadlock case is as followed:
    
            CPU 0           CPU 1           CPU 2
            read_lock(&fown->lock);
                            spin_lock_irqsave(&dev->event_lock, ...)
                                            write_lock_irq(&filp->f_owner.lock); // wait for the lock
                            read_lock(&fown-lock); // have to wait until the writer release
                                                   // due to the fairness
            <interrupted>
            spin_lock_irqsave(&dev->event_lock); // wait for the lock
    
    The lock dependency on CPU 1 happens if there exists a call sequence:
    
            input_inject_event():
              spin_lock_irqsave(&dev->event_lock,...);
              input_handle_event():
                input_pass_values():
                  input_to_handler():
                    handler->event(): // evdev_event()
                      evdev_pass_values():
                        spin_lock(&client->buffer_lock);
                        __pass_event():
                          kill_fasync():
                            kill_fasync_rcu():
                              read_lock(&fa->fa_lock);
                              send_sigio():
                                read_lock(&fown->lock);
    
    To fix this, make the reader in send_sigurg() and send_sigio() use
    read_lock_irqsave() and read_lock_irqrestore().
    
    Reported-by: syzbot+22e87cdf94021b984aa6@syzkaller.appspotmail.com
    Reported-by: syzbot+c5e32344981ad9f33750@syzkaller.appspotmail.com
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Jeff Layton <jlayton@kernel.org>

commit 6971e07b6b0cf7dd74fb98d97011dbe4546dd2cf
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Tue Oct 13 12:08:45 2020 +0100

    drm/i915/gt: Cleanup kasan warning for on-stack (unsigned long) casting
    
    Kasan is gving a warning for passing a u32 parameter into find_first_bit
    (casting to a unsigned long *, with appropriate length restrictions):
    
    [   44.678262] BUG: KASAN: stack-out-of-bounds in find_first_bit+0x2e/0x50
    [   44.678295] Read of size 8 at addr ffff888233f4fc30 by task core_hotunplug/474
    [   44.678326]
    [   44.678358] CPU: 0 PID: 474 Comm: core_hotunplug Not tainted 5.9.0+ #608
    [   44.678465] Hardware name: BESSTAR (HK) LIMITED GN41/Default string, BIOS BLT-BI-MINIPC-F4G-EX3R110-GA65A-101-D 10/12/2018
    [   44.678500] Call Trace:
    [   44.678534]  dump_stack+0x84/0xba
    [   44.678569]  print_address_description.constprop.0+0x21/0x220
    [   44.678605]  ? kmsg_dump_rewind_nolock+0x5f/0x5f
    [   44.678638]  ? _raw_spin_lock_irqsave+0x6d/0xb0
    [   44.678669]  ? _raw_write_lock_irqsave+0xb0/0xb0
    [   44.678702]  ? set_task_cpu+0x1e0/0x1e0
    [   44.678733]  ? find_first_bit+0x2e/0x50
    [   44.678763]  kasan_report.cold+0x20/0x42
    [   44.678794]  ? find_first_bit+0x2e/0x50
    [   44.678825]  __asan_load8+0x69/0x90
    [   44.678856]  find_first_bit+0x2e/0x50
    [   44.679027]  __caps_show.isra.0+0x9e/0x1f0 [i915]
    
    Since we are only using the shorter type for our own convenience,
    accommodate kasan and use unsigned long.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20201013110845.16127-1-chris@chris-wilson.co.uk

commit eac48eb6ce10c1dc6fd3366608f4d3ca2430166c
Author: Petr Mladek <mladek.petr@gmail.com>
Date:   Wed Oct 14 19:50:51 2020 +0200

    printk: ringbuffer: Wrong data pointer when appending small string
    
    data_realloc() returns wrong data pointer when the block is wrapped and
    the size is not increased. It might happen when pr_cont() wants to
    add only few characters and there is already a space for them because
    of alignment.
    
    It might cause writing outsite the buffer. It has been detected by LTP
    tests with KASAN enabled:
    
    [  221.921944] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=c,mems_allowed=0,oom_memcg=/0,task_memcg=in
    [  221.922108] ==================================================================
    [  221.922111] BUG: KASAN: global-out-of-bounds in vprintk_store+0x362/0x3d0
    [  221.922112] Write of size 2 at addr ffffffffba51dbcd by task
    memcg_test_1/11282
    [  221.922113]
    [  221.922114] CPU: 1 PID: 11282 Comm: memcg_test_1 Not tainted
    5.9.0-next-20201013 #1
    [  221.922116] Hardware name: Supermicro SYS-5019S-ML/X11SSH-F, BIOS
    2.0b 07/27/2017
    [  221.922116] Call Trace:
    [  221.922117]  dump_stack+0xa4/0xd9
    [  221.922118]  print_address_description.constprop.0+0x21/0x210
    [  221.922119]  ? _raw_write_lock_bh+0xe0/0xe0
    [  221.922120]  ? vprintk_store+0x362/0x3d0
    [  221.922121]  kasan_report.cold+0x37/0x7c
    [  221.922122]  ? vprintk_store+0x362/0x3d0
    [  221.922123]  check_memory_region+0x18c/0x1f0
    [  221.922124]  memcpy+0x3c/0x60
    [  221.922125]  vprintk_store+0x362/0x3d0
    [  221.922125]  ? __ia32_sys_syslog+0x50/0x50
    [  221.922126]  ? _raw_spin_lock_irqsave+0x9b/0x100
    [  221.922127]  ? _raw_spin_lock_irq+0xf0/0xf0
    [  221.922128]  ? __kasan_check_write+0x14/0x20
    [  221.922129]  vprintk_emit+0x8d/0x1f0
    [  221.922130]  vprintk_default+0x1d/0x20
    [  221.922131]  vprintk_func+0x5a/0x100
    [  221.922132]  printk+0xb2/0xe3
    [  221.922133]  ? swsusp_write.cold+0x189/0x189
    [  221.922134]  ? kernfs_vfs_xattr_set+0x60/0x60
    [  221.922134]  ? _raw_write_lock_bh+0xe0/0xe0
    [  221.922135]  ? trace_hardirqs_on+0x38/0x100
    [  221.922136]  pr_cont_kernfs_path.cold+0x49/0x4b
    [  221.922137]  mem_cgroup_print_oom_context.cold+0x74/0xc3
    [  221.922138]  dump_header+0x340/0x3bf
    [  221.922139]  oom_kill_process.cold+0xb/0x10
    [  221.922140]  out_of_memory+0x1e9/0x860
    [  221.922141]  ? oom_killer_disable+0x210/0x210
    [  221.922142]  mem_cgroup_out_of_memory+0x198/0x1c0
    [  221.922143]  ? mem_cgroup_count_precharge_pte_range+0x250/0x250
    [  221.922144]  try_charge+0xa9b/0xc50
    [  221.922145]  ? arch_stack_walk+0x9e/0xf0
    [  221.922146]  ? memory_high_write+0x230/0x230
    [  221.922146]  ? avc_has_extended_perms+0x830/0x830
    [  221.922147]  ? stack_trace_save+0x94/0xc0
    [  221.922148]  ? stack_trace_consume_entry+0x90/0x90
    [  221.922149]  __memcg_kmem_charge+0x73/0x120
    [  221.922150]  ? cred_has_capability+0x10f/0x200
    [  221.922151]  ? mem_cgroup_can_attach+0x260/0x260
    [  221.922152]  ? selinux_sb_eat_lsm_opts+0x2f0/0x2f0
    [  221.922153]  ? obj_cgroup_charge+0x16b/0x220
    [  221.922154]  ? kmem_cache_alloc+0x78/0x4c0
    [  221.922155]  obj_cgroup_charge+0x122/0x220
    [  221.922156]  ? vm_area_alloc+0x20/0x90
    [  221.922156]  kmem_cache_alloc+0x78/0x4c0
    [  221.922157]  vm_area_alloc+0x20/0x90
    [  221.922158]  mmap_region+0x3ed/0x9a0
    [  221.922159]  ? cap_mmap_addr+0x1d/0x80
    [  221.922160]  do_mmap+0x3ee/0x720
    [  221.922161]  vm_mmap_pgoff+0x16a/0x1c0
    [  221.922162]  ? randomize_stack_top+0x90/0x90
    [  221.922163]  ? copy_page_range+0x1980/0x1980
    [  221.922163]  ksys_mmap_pgoff+0xab/0x350
    [  221.922164]  ? find_mergeable_anon_vma+0x110/0x110
    [  221.922165]  ? __audit_syscall_entry+0x1a6/0x1e0
    [  221.922166]  __x64_sys_mmap+0x8d/0xb0
    [  221.922167]  do_syscall_64+0x38/0x50
    [  221.922168]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    [  221.922169] RIP: 0033:0x7fe8f5e75103
    [  221.922172] Code: 54 41 89 d4 55 48 89 fd 53 4c 89 cb 48 85 ff 74
    56 49 89 d9 45 89 f8 45 89 f2 44 89 e2 4c 89 ee 48 89 ef b8 09 00 00
    00 0f 05 <48> 3d 00 f0 ff ff 77 7d 5b 5d 41 5c 41 5d 41 5e 41 5f c3 66
    2e 0f
    [  221.922173] RSP: 002b:00007ffd38c90198 EFLAGS: 00000246 ORIG_RAX:
    0000000000000009
    [  221.922175] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007fe8f5e75103
    [  221.922176] RDX: 0000000000000003 RSI: 0000000000001000 RDI: 0000000000000000
    [  221.922178] RBP: 0000000000000000 R08: 0000000000000000 R09: 0000000000000000
    [  221.922179] R10: 0000000000002022 R11: 0000000000000246 R12: 0000000000000003
    [  221.922180] R13: 0000000000001000 R14: 0000000000002022 R15: 0000000000000000
    [  221.922181]
    [  213O[  221.922182] The buggy address belongs to the variable:
    [  221.922183]  clear_seq+0x2d/0x40
    [  221.922183]
    [  221.922184] Memory state around the buggy address:
    [  221.922185]  ffffffffba51da80: 00 00 00 00 00 00 00 00 00 00 00 00
    00 00 00 00
    [  221.922187]  ffffffffba51db00: 00 00 00 00 00 00 00 00 00 00 00 00
    00 00 00 00
    [  221.922188] >ffffffffba51db80: f9 f9 f9 f9 00 f9 f9 f9 f9 f9 f9 f9
    00 f9 f9 f9
    [  221.922189]                                               ^
    [  221.922190]  ffffffffba51dc00: f9 f9 f9 f9 00 f9 f9 f9 f9 f9 f9 f9
    00 f9 f9 f9
    [  221.922191]  ffffffffba51dc80: f9 f9 f9 f9 01 f9 f9 f9 f9 f9 f9 f9
    00 f9 f9 f9
    [  221.922193] ==================================================================
    [  221.922194] Disabling lock debugging due to kernel taint
    [  221.922196] ,task=memcg_test_1,pid=11280,uid=0
    [  221.922205] Memory cgroup out of memory: Killed process 11280
    
    Link: https://lore.kernel.org/r/CA+G9fYt46oC7-BKryNDaaXPJ9GztvS2cs_7GjYRjanRi4+ryCQ@mail.gmail.com
    Fixes: 4cfc7258f876a7feba673ac ("printk: ringbuffer: add finalization/extension support")
    Reported-by: Naresh Kamboju <naresh.kamboju@linaro.org>
    Reviewed-by: John Ogness <john.ogness@linutronix.de>
    Acked-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Link: https://lore.kernel.org/r/20201014175051.GC13775@alley

commit ed016af52ee3035b4799ebd7d53f9ae59d5782c4
Merge: edaa5ddf3833 2116d708b058
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 12 13:06:20 2020 -0700

    Merge tag 'locking-core-2020-10-12' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "These are the locking updates for v5.10:
    
       - Add deadlock detection for recursive read-locks.
    
         The rationale is outlined in commit 224ec489d3cd ("lockdep/
         Documention: Recursive read lock detection reasoning")
    
         The main deadlock pattern we want to detect is:
    
               TASK A:                 TASK B:
    
               read_lock(X);
                                       write_lock(X);
               read_lock_2(X);
    
       - Add "latch sequence counters" (seqcount_latch_t):
    
         A sequence counter variant where the counter even/odd value is used
         to switch between two copies of protected data. This allows the
         read path, typically NMIs, to safely interrupt the write side
         critical section.
    
         We utilize this new variant for sched-clock, and to make x86 TSC
         handling safer.
    
       - Other seqlock cleanups, fixes and enhancements
    
       - KCSAN updates
    
       - LKMM updates
    
       - Misc updates, cleanups and fixes"
    
    * tag 'locking-core-2020-10-12' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (67 commits)
      lockdep: Revert "lockdep: Use raw_cpu_*() for per-cpu variables"
      lockdep: Fix lockdep recursion
      lockdep: Fix usage_traceoverflow
      locking/atomics: Check atomic-arch-fallback.h too
      locking/seqlock: Tweak DEFINE_SEQLOCK() kernel doc
      lockdep: Optimize the memory usage of circular queue
      seqlock: Unbreak lockdep
      seqlock: PREEMPT_RT: Do not starve seqlock_t writers
      seqlock: seqcount_LOCKNAME_t: Introduce PREEMPT_RT support
      seqlock: seqcount_t: Implement all read APIs as statement expressions
      seqlock: Use unique prefix for seqcount_t property accessors
      seqlock: seqcount_LOCKNAME_t: Standardize naming convention
      seqlock: seqcount latch APIs: Only allow seqcount_latch_t
      rbtree_latch: Use seqcount_latch_t
      x86/tsc: Use seqcount_latch_t
      timekeeping: Use seqcount_latch_t
      time/sched_clock: Use seqcount_latch_t
      seqlock: Introduce seqcount_latch_t
      mm/swap: Do not abuse the seqcount_t latching API
      time/sched_clock: Use raw_read_seqcount_latch() during suspend
      ...

commit 6564d0ad67efb2d977e130e7448505ee538af016
Author: Takashi Iwai <tiwai@suse.de>
Date:   Tue Sep 22 10:49:53 2020 +0200

    ALSA: ctl: Workaround for lockdep warning wrt card->ctl_files_rwlock
    
    The recent change in lockdep for read lock caused the deadlock
    warnings in ALSA control code which uses the read_lock() for
    notification and else while write_lock_irqsave() is used for adding
    and removing the list entry.  Although a deadlock would practically
    never hit in a real usage (the addition and the deletion can't happen
    with the notification), it's better to fix the read_lock() usage in a
    semantically correct way.
    
    This patch replaces the read_lock() calls with read_lock_irqsave()
    version for avoiding a reported deadlock.  The notification code path
    takes the irq disablement in anyway, and other code paths are very
    short execution, hence there shouldn't be any big performance hit by
    this change.
    
    Fixes: e918188611f0 ("locking: More accurate annotations for read_lock()")
    Reported-by: syzbot+561a74f84100162990b2@syzkaller.appspotmail.com
    Link: https://lore.kernel.org/r/20200922084953.29018-1-tiwai@suse.de
    Signed-off-by: Takashi Iwai <tiwai@suse.de>

commit ad56450db86413ff911eb527b5a49e04a4345e61
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Fri Aug 7 15:42:37 2020 +0800

    locking/selftest: Add test cases for queued_read_lock()
    
    Add two self test cases for the following case:
    
            P0:                     P1:                     P2:
    
                                    <in irq handler>
            spin_lock_irq(&slock)   read_lock(&rwlock)
                                                            write_lock_irq(&rwlock)
            read_lock(&rwlock)      spin_lock(&slock)
    
    , which is a deadlock, as the read_lock() on P0 cannot get the lock
    because of the fairness.
    
            P0:                     P1:                     P2:
    
            <in irq handler>
            spin_lock(&slock)       read_lock(&rwlock)
                                                            write_lock(&rwlock)
            read_lock(&rwlock)      spin_lock_irq(&slock)
    
    , which is not a deadlock, as the read_lock() on P0 can get the lock
    because it could use the unfair fastpass.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200807074238.1632519-19-boqun.feng@gmail.com

commit f611e8cf98ec908b9c2c0da6064a660fc6022487
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Fri Aug 7 15:42:33 2020 +0800

    lockdep: Take read/write status in consideration when generate chainkey
    
    Currently, the chainkey of a lock chain is a hash sum of the class_idx
    of all the held locks, the read/write status are not taken in to
    consideration while generating the chainkey. This could result into a
    problem, if we have:
    
            P1()
            {
                    read_lock(B);
                    lock(A);
            }
    
            P2()
            {
                    lock(A);
                    read_lock(B);
            }
    
            P3()
            {
                    lock(A);
                    write_lock(B);
            }
    
    , and P1(), P2(), P3() run one by one. And when running P2(), lockdep
    detects such a lock chain A -> B is not a deadlock, then it's added in
    the chain cache, and then when running P3(), even if it's a deadlock, we
    could miss it because of the hit of chain cache. This could be confirmed
    by self testcase "chain cached mixed R-L/L-W ".
    
    To resolve this, we use concept "hlock_id" to generate the chainkey, the
    hlock_id is a tuple (hlock->class_idx, hlock->read), which fits in a u16
    type. With this, the chainkeys are different is the lock sequences have
    the same locks but different read/write status.
    
    Besides, since we use "hlock_id" to generate chainkeys, the chain_hlocks
    array now store the "hlock_id"s rather than lock_class indexes.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200807074238.1632519-15-boqun.feng@gmail.com

commit f08e3888574d490b31481eef6d84c61bedba7a47
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Fri Aug 7 15:42:30 2020 +0800

    lockdep: Fix recursive read lock related safe->unsafe detection
    
    Currently, in safe->unsafe detection, lockdep misses the fact that a
    LOCK_ENABLED_IRQ_*_READ usage and a LOCK_USED_IN_IRQ_*_READ usage may
    cause deadlock too, for example:
    
            P1                          P2
            <irq disabled>
            write_lock(l1);             <irq enabled>
                                        read_lock(l2);
            write_lock(l2);
                                        <in irq>
                                        read_lock(l1);
    
    Actually, all of the following cases may cause deadlocks:
    
            LOCK_USED_IN_IRQ_* -> LOCK_ENABLED_IRQ_*
            LOCK_USED_IN_IRQ_*_READ -> LOCK_ENABLED_IRQ_*
            LOCK_USED_IN_IRQ_* -> LOCK_ENABLED_IRQ_*_READ
            LOCK_USED_IN_IRQ_*_READ -> LOCK_ENABLED_IRQ_*_READ
    
    To fix this, we need to 1) change the calculation of exclusive_mask() so
    that READ bits are not dropped and 2) always call usage() in
    mark_lock_irq() to check usage deadlocks, even when the new usage of the
    lock is READ.
    
    Besides, adjust usage_match() and usage_acculumate() to recursive read
    lock changes.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200807074238.1632519-12-boqun.feng@gmail.com

commit 6971c0f345620aae5e6172207a57b7524603a34e
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Fri Aug 7 15:42:26 2020 +0800

    lockdep: Extend __bfs() to work with multiple types of dependencies
    
    Now we have four types of dependencies in the dependency graph, and not
    all the pathes carry real dependencies (the dependencies that may cause
    a deadlock), for example:
    
            Given lock A and B, if we have:
    
            CPU1                    CPU2
            =============           ==============
            write_lock(A);          read_lock(B);
            read_lock(B);           write_lock(A);
    
            (assuming read_lock(B) is a recursive reader)
    
            then we have dependencies A -(ER)-> B, and B -(SN)-> A, and a
            dependency path A -(ER)-> B -(SN)-> A.
    
            In lockdep w/o recursive locks, a dependency path from A to A
            means a deadlock. However, the above case is obviously not a
            deadlock, because no one holds B exclusively, therefore no one
            waits for the other to release B, so who get A first in CPU1 and
            CPU2 will run non-blockingly.
    
            As a result, dependency path A -(ER)-> B -(SN)-> A is not a
            real/strong dependency that could cause a deadlock.
    
    From the observation above, we know that for a dependency path to be
    real/strong, no two adjacent dependencies can be as -(*R)-> -(S*)->.
    
    Now our mission is to make __bfs() traverse only the strong dependency
    paths, which is simple: we record whether we only have -(*R)-> for the
    previous lock_list of the path in lock_list::only_xr, and when we pick a
    dependency in the traverse, we 1) filter out -(S*)-> dependency if the
    previous lock_list only has -(*R)-> dependency (i.e. ->only_xr is true)
    and 2) set the next lock_list::only_xr to true if we only have -(*R)->
    left after we filter out dependencies based on 1), otherwise, set it to
    false.
    
    With this extension for __bfs(), we now need to initialize the root of
    __bfs() properly (with a correct ->only_xr), to do so, we introduce some
    helper functions, which also cleans up a little bit for the __bfs() root
    initialization code.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200807074238.1632519-8-boqun.feng@gmail.com

commit 3454a36d6a39186de508dd43df590a6363364176
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Fri Aug 7 15:42:25 2020 +0800

    lockdep: Introduce lock_list::dep
    
    To add recursive read locks into the dependency graph, we need to store
    the types of dependencies for the BFS later. There are four types of
    dependencies:
    
    *       Exclusive -> Non-recursive dependencies: EN
            e.g. write_lock(prev) held and try to acquire write_lock(next)
            or non-recursive read_lock(next), which can be represented as
            "prev -(EN)-> next"
    
    *       Shared -> Non-recursive dependencies: SN
            e.g. read_lock(prev) held and try to acquire write_lock(next) or
            non-recursive read_lock(next), which can be represented as
            "prev -(SN)-> next"
    
    *       Exclusive -> Recursive dependencies: ER
            e.g. write_lock(prev) held and try to acquire recursive
            read_lock(next), which can be represented as "prev -(ER)-> next"
    
    *       Shared -> Recursive dependencies: SR
            e.g. read_lock(prev) held and try to acquire recursive
            read_lock(next), which can be represented as "prev -(SR)-> next"
    
    So we use 4 bits for the presence of each type in lock_list::dep. Helper
    functions and macros are also introduced to convert a pair of locks into
    lock_list::dep bit and maintain the addition of different types of
    dependencies.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200807074238.1632519-7-boqun.feng@gmail.com

commit aa80bd09914add1a37016c4cf9abe79899242249
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Jul 22 12:28:37 2020 +0100

    btrfs: fix race between page release and a fast fsync
    
    commit 3d6448e631591756da36efb3ea6355ff6f383c3a upstream.
    
    When releasing an extent map, done through the page release callback, we
    can race with an ongoing fast fsync and cause the fsync to miss a new
    extent and not log it. The steps for this to happen are the following:
    
    1) A page is dirtied for some inode I;
    
    2) Writeback for that page is triggered by a path other than fsync, for
       example by the system due to memory pressure;
    
    3) When the ordered extent for the extent (a single 4K page) finishes,
       we unpin the corresponding extent map and set its generation to N,
       the current transaction's generation;
    
    4) The btrfs_releasepage() callback is invoked by the system due to
       memory pressure for that no longer dirty page of inode I;
    
    5) At the same time, some task calls fsync on inode I, joins transaction
       N, and at btrfs_log_inode() it sees that the inode does not have the
       full sync flag set, so we proceed with a fast fsync. But before we get
       into btrfs_log_changed_extents() and lock the inode's extent map tree:
    
    6) Through btrfs_releasepage() we end up at try_release_extent_mapping()
       and we remove the extent map for the new 4Kb extent, because it is
       neither pinned anymore nor locked. By calling remove_extent_mapping(),
       we remove the extent map from the list of modified extents, since the
       extent map does not have the logging flag set. We unlock the inode's
       extent map tree;
    
    7) The task doing the fast fsync now enters btrfs_log_changed_extents(),
       locks the inode's extent map tree and iterates its list of modified
       extents, which no longer has the 4Kb extent in it, so it does not log
       the extent;
    
    8) The fsync finishes;
    
    9) Before transaction N is committed, a power failure happens. After
       replaying the log, the 4K extent of inode I will be missing, since
       it was not logged due to the race with try_release_extent_mapping().
    
    So fix this by teaching try_release_extent_mapping() to not remove an
    extent map if it's still in the list of modified extents.
    
    Fixes: ff44c6e36dc9dc ("Btrfs: do not hold the write_lock on the extent tree while logging")
    CC: stable@vger.kernel.org # 5.4+
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3505fbfa588ddd9ebe848e548284b46e5612d742
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Jul 22 12:28:37 2020 +0100

    btrfs: fix race between page release and a fast fsync
    
    commit 3d6448e631591756da36efb3ea6355ff6f383c3a upstream.
    
    When releasing an extent map, done through the page release callback, we
    can race with an ongoing fast fsync and cause the fsync to miss a new
    extent and not log it. The steps for this to happen are the following:
    
    1) A page is dirtied for some inode I;
    
    2) Writeback for that page is triggered by a path other than fsync, for
       example by the system due to memory pressure;
    
    3) When the ordered extent for the extent (a single 4K page) finishes,
       we unpin the corresponding extent map and set its generation to N,
       the current transaction's generation;
    
    4) The btrfs_releasepage() callback is invoked by the system due to
       memory pressure for that no longer dirty page of inode I;
    
    5) At the same time, some task calls fsync on inode I, joins transaction
       N, and at btrfs_log_inode() it sees that the inode does not have the
       full sync flag set, so we proceed with a fast fsync. But before we get
       into btrfs_log_changed_extents() and lock the inode's extent map tree:
    
    6) Through btrfs_releasepage() we end up at try_release_extent_mapping()
       and we remove the extent map for the new 4Kb extent, because it is
       neither pinned anymore nor locked. By calling remove_extent_mapping(),
       we remove the extent map from the list of modified extents, since the
       extent map does not have the logging flag set. We unlock the inode's
       extent map tree;
    
    7) The task doing the fast fsync now enters btrfs_log_changed_extents(),
       locks the inode's extent map tree and iterates its list of modified
       extents, which no longer has the 4Kb extent in it, so it does not log
       the extent;
    
    8) The fsync finishes;
    
    9) Before transaction N is committed, a power failure happens. After
       replaying the log, the 4K extent of inode I will be missing, since
       it was not logged due to the race with try_release_extent_mapping().
    
    So fix this by teaching try_release_extent_mapping() to not remove an
    extent map if it's still in the list of modified extents.
    
    Fixes: ff44c6e36dc9dc ("Btrfs: do not hold the write_lock on the extent tree while logging")
    CC: stable@vger.kernel.org # 5.4+
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 242747612209637f6e8499da852d0ace929407e3
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Jul 22 12:28:37 2020 +0100

    btrfs: fix race between page release and a fast fsync
    
    commit 3d6448e631591756da36efb3ea6355ff6f383c3a upstream.
    
    When releasing an extent map, done through the page release callback, we
    can race with an ongoing fast fsync and cause the fsync to miss a new
    extent and not log it. The steps for this to happen are the following:
    
    1) A page is dirtied for some inode I;
    
    2) Writeback for that page is triggered by a path other than fsync, for
       example by the system due to memory pressure;
    
    3) When the ordered extent for the extent (a single 4K page) finishes,
       we unpin the corresponding extent map and set its generation to N,
       the current transaction's generation;
    
    4) The btrfs_releasepage() callback is invoked by the system due to
       memory pressure for that no longer dirty page of inode I;
    
    5) At the same time, some task calls fsync on inode I, joins transaction
       N, and at btrfs_log_inode() it sees that the inode does not have the
       full sync flag set, so we proceed with a fast fsync. But before we get
       into btrfs_log_changed_extents() and lock the inode's extent map tree:
    
    6) Through btrfs_releasepage() we end up at try_release_extent_mapping()
       and we remove the extent map for the new 4Kb extent, because it is
       neither pinned anymore nor locked. By calling remove_extent_mapping(),
       we remove the extent map from the list of modified extents, since the
       extent map does not have the logging flag set. We unlock the inode's
       extent map tree;
    
    7) The task doing the fast fsync now enters btrfs_log_changed_extents(),
       locks the inode's extent map tree and iterates its list of modified
       extents, which no longer has the 4Kb extent in it, so it does not log
       the extent;
    
    8) The fsync finishes;
    
    9) Before transaction N is committed, a power failure happens. After
       replaying the log, the 4K extent of inode I will be missing, since
       it was not logged due to the race with try_release_extent_mapping().
    
    So fix this by teaching try_release_extent_mapping() to not remove an
    extent map if it's still in the list of modified extents.
    
    Fixes: ff44c6e36dc9dc ("Btrfs: do not hold the write_lock on the extent tree while logging")
    CC: stable@vger.kernel.org # 5.4+
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 22ca8cb2a5b9b34034ae19d0e4c1c969d5576051
Author: Johannes Thumshirn <johannes.thumshirn@wdc.com>
Date:   Thu Jul 30 20:25:17 2020 +0900

    block: don't do revalidate zones on invalid devices
    
    [ Upstream commit 1a1206dc4cf02cee4b5cbce583ee4c22368b4c28 ]
    
    When we loose a device for whatever reason while (re)scanning zones, we
    trip over a NULL pointer in blk_revalidate_zone_cb, like in the following
    log:
    
    sd 0:0:0:0: [sda] 3418095616 4096-byte logical blocks: (14.0 TB/12.7 TiB)
    sd 0:0:0:0: [sda] 52156 zones of 65536 logical blocks
    sd 0:0:0:0: [sda] Write Protect is off
    sd 0:0:0:0: [sda] Mode Sense: 37 00 00 08
    sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA
    sd 0:0:0:0: [sda] REPORT ZONES start lba 1065287680 failed
    sd 0:0:0:0: [sda] REPORT ZONES: Result: hostbyte=0x00 driverbyte=0x08
    sd 0:0:0:0: [sda] Sense Key : 0xb [current]
    sd 0:0:0:0: [sda] ASC=0x0 ASCQ=0x6
    sda: failed to revalidate zones
    sd 0:0:0:0: [sda] 0 4096-byte logical blocks: (0 B/0 B)
    sda: detected capacity change from 14000519643136 to 0
    ==================================================================
    BUG: KASAN: null-ptr-deref in blk_revalidate_zone_cb+0x1b7/0x550
    Write of size 8 at addr 0000000000000010 by task kworker/u4:1/58
    
    CPU: 1 PID: 58 Comm: kworker/u4:1 Not tainted 5.8.0-rc1 #692
    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.13.0-0-gf21b5a4-rebuilt.opensuse.org 04/01/2014
    Workqueue: events_unbound async_run_entry_fn
    Call Trace:
     dump_stack+0x7d/0xb0
     ? blk_revalidate_zone_cb+0x1b7/0x550
     kasan_report.cold+0x5/0x37
     ? blk_revalidate_zone_cb+0x1b7/0x550
     check_memory_region+0x145/0x1a0
     blk_revalidate_zone_cb+0x1b7/0x550
     sd_zbc_parse_report+0x1f1/0x370
     ? blk_req_zone_write_trylock+0x200/0x200
     ? sectors_to_logical+0x60/0x60
     ? blk_req_zone_write_trylock+0x200/0x200
     ? blk_req_zone_write_trylock+0x200/0x200
     sd_zbc_report_zones+0x3c4/0x5e0
     ? sd_dif_config_host+0x500/0x500
     blk_revalidate_disk_zones+0x231/0x44d
     ? _raw_write_lock_irqsave+0xb0/0xb0
     ? blk_queue_free_zone_bitmaps+0xd0/0xd0
     sd_zbc_read_zones+0x8cf/0x11a0
     sd_revalidate_disk+0x305c/0x64e0
     ? __device_add_disk+0x776/0xf20
     ? read_capacity_16.part.0+0x1080/0x1080
     ? blk_alloc_devt+0x250/0x250
     ? create_object.isra.0+0x595/0xa20
     ? kasan_unpoison_shadow+0x33/0x40
     sd_probe+0x8dc/0xcd2
     really_probe+0x20e/0xaf0
     __driver_attach_async_helper+0x249/0x2d0
     async_run_entry_fn+0xbe/0x560
     process_one_work+0x764/0x1290
     ? _raw_read_unlock_irqrestore+0x30/0x30
     worker_thread+0x598/0x12f0
     ? __kthread_parkme+0xc6/0x1b0
     ? schedule+0xed/0x2c0
     ? process_one_work+0x1290/0x1290
     kthread+0x36b/0x440
     ? kthread_create_worker_on_cpu+0xa0/0xa0
     ret_from_fork+0x22/0x30
    ==================================================================
    
    When the device is already gone we end up with the following scenario:
    The device's capacity is 0 and thus the number of zones will be 0 as well. When
    allocating the bitmap for the conventional zones, we then trip over a NULL
    pointer.
    
    So if we encounter a zoned block device with a 0 capacity, don't dare to
    revalidate the zones sizes.
    
    Fixes: 6c6b35491422 ("block: set the zone size in blk_revalidate_disk_zones atomically")
    Signed-off-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 8904c89418a5a691b16be9d862e8bda5f0332803
Author: Johannes Thumshirn <johannes.thumshirn@wdc.com>
Date:   Thu Jul 30 20:25:17 2020 +0900

    block: don't do revalidate zones on invalid devices
    
    [ Upstream commit 1a1206dc4cf02cee4b5cbce583ee4c22368b4c28 ]
    
    When we loose a device for whatever reason while (re)scanning zones, we
    trip over a NULL pointer in blk_revalidate_zone_cb, like in the following
    log:
    
    sd 0:0:0:0: [sda] 3418095616 4096-byte logical blocks: (14.0 TB/12.7 TiB)
    sd 0:0:0:0: [sda] 52156 zones of 65536 logical blocks
    sd 0:0:0:0: [sda] Write Protect is off
    sd 0:0:0:0: [sda] Mode Sense: 37 00 00 08
    sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA
    sd 0:0:0:0: [sda] REPORT ZONES start lba 1065287680 failed
    sd 0:0:0:0: [sda] REPORT ZONES: Result: hostbyte=0x00 driverbyte=0x08
    sd 0:0:0:0: [sda] Sense Key : 0xb [current]
    sd 0:0:0:0: [sda] ASC=0x0 ASCQ=0x6
    sda: failed to revalidate zones
    sd 0:0:0:0: [sda] 0 4096-byte logical blocks: (0 B/0 B)
    sda: detected capacity change from 14000519643136 to 0
    ==================================================================
    BUG: KASAN: null-ptr-deref in blk_revalidate_zone_cb+0x1b7/0x550
    Write of size 8 at addr 0000000000000010 by task kworker/u4:1/58
    
    CPU: 1 PID: 58 Comm: kworker/u4:1 Not tainted 5.8.0-rc1 #692
    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.13.0-0-gf21b5a4-rebuilt.opensuse.org 04/01/2014
    Workqueue: events_unbound async_run_entry_fn
    Call Trace:
     dump_stack+0x7d/0xb0
     ? blk_revalidate_zone_cb+0x1b7/0x550
     kasan_report.cold+0x5/0x37
     ? blk_revalidate_zone_cb+0x1b7/0x550
     check_memory_region+0x145/0x1a0
     blk_revalidate_zone_cb+0x1b7/0x550
     sd_zbc_parse_report+0x1f1/0x370
     ? blk_req_zone_write_trylock+0x200/0x200
     ? sectors_to_logical+0x60/0x60
     ? blk_req_zone_write_trylock+0x200/0x200
     ? blk_req_zone_write_trylock+0x200/0x200
     sd_zbc_report_zones+0x3c4/0x5e0
     ? sd_dif_config_host+0x500/0x500
     blk_revalidate_disk_zones+0x231/0x44d
     ? _raw_write_lock_irqsave+0xb0/0xb0
     ? blk_queue_free_zone_bitmaps+0xd0/0xd0
     sd_zbc_read_zones+0x8cf/0x11a0
     sd_revalidate_disk+0x305c/0x64e0
     ? __device_add_disk+0x776/0xf20
     ? read_capacity_16.part.0+0x1080/0x1080
     ? blk_alloc_devt+0x250/0x250
     ? create_object.isra.0+0x595/0xa20
     ? kasan_unpoison_shadow+0x33/0x40
     sd_probe+0x8dc/0xcd2
     really_probe+0x20e/0xaf0
     __driver_attach_async_helper+0x249/0x2d0
     async_run_entry_fn+0xbe/0x560
     process_one_work+0x764/0x1290
     ? _raw_read_unlock_irqrestore+0x30/0x30
     worker_thread+0x598/0x12f0
     ? __kthread_parkme+0xc6/0x1b0
     ? schedule+0xed/0x2c0
     ? process_one_work+0x1290/0x1290
     kthread+0x36b/0x440
     ? kthread_create_worker_on_cpu+0xa0/0xa0
     ret_from_fork+0x22/0x30
    ==================================================================
    
    When the device is already gone we end up with the following scenario:
    The device's capacity is 0 and thus the number of zones will be 0 as well. When
    allocating the bitmap for the conventional zones, we then trip over a NULL
    pointer.
    
    So if we encounter a zoned block device with a 0 capacity, don't dare to
    revalidate the zones sizes.
    
    Fixes: 6c6b35491422 ("block: set the zone size in blk_revalidate_disk_zones atomically")
    Signed-off-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 9cbbc451098ec1e9942886023203b2247dec94bd
Author: Edward Cree <ecree@solarflare.com>
Date:   Tue Aug 18 13:43:57 2020 +0100

    sfc: take correct lock in ef100_reset()
    
    When downing and upping the ef100 filter table, we need to take a write
     lock on efx->filter_sem, not just a read lock, because we may kfree()
     the table pointers.
    Without this, resets cause a WARN_ON from efx_rwsem_assert_write_locked().
    
    Fixes: a9dc3d5612ce ("sfc_ef100: RX filter table management and related gubbins")
    Signed-off-by: Edward Cree <ecree@solarflare.com>
    Reviewed-by: Jesse Brandeburg <jesse.brandeburg@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e348e65a081d761caf72c2753c4e3d67bfb1ef80
Author: Yubo Feng <fengyubo3@huawei.com>
Date:   Tue Aug 11 18:35:56 2020 -0700

    fatfs: switch write_lock to read_lock in fat_ioctl_get_attributes
    
    There is no need to hold write_lock in fat_ioctl_get_attributes.
    write_lock may make an impact on concurrency of fat_ioctl_get_attributes.
    
    Signed-off-by: Yubo Feng <fengyubo3@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
    Link: http://lkml.kernel.org/r/1593308053-12702-1-git-send-email-fengyubo3@huawei.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 0a72761b27fe3b10e3f336bf2f2aa22635504cdd
Merge: 3950e975431b 55d9ad97e417
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 4 14:40:07 2020 -0700

    Merge tag 'threads-v5.9' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux
    
    Pull thread updates from Christian Brauner:
     "This contains the changes to add the missing support for attaching to
      time namespaces via pidfds.
    
      Last cycle setns() was changed to support attaching to multiple
      namespaces atomically. This requires all namespaces to have a point of
      no return where they can't fail anymore.
    
      Specifically, <namespace-type>_install() is allowed to perform
      permission checks and install the namespace into the new struct nsset
      that it has been given but it is not allowed to make visible changes
      to the affected task. Once <namespace-type>_install() returns,
      anything that the given namespace type additionally requires to be
      setup needs to ideally be done in a function that can't fail or if it
      fails the failure must be non-fatal.
    
      For time namespaces the relevant functions that fell into this
      category were timens_set_vvar_page() and vdso_join_timens(). The
      latter could still fail although it didn't need to. This function is
      only implemented for vdso_join_timens() in current mainline. As
      discussed on-list (cf. [1]), in order to make setns() support time
      namespaces when attaching to multiple namespaces at once properly we
      changed vdso_join_timens() to always succeed. So vdso_join_timens()
      replaces the mmap_write_lock_killable() with mmap_read_lock().
    
      Please note that arm is about to grow vdso support for time namespaces
      (possibly this merge window). We've synced on this change and arm64
      also uses mmap_read_lock(), i.e. makes vdso_join_timens() a function
      that can't fail. Once the changes here and the arm64 changes have
      landed, vdso_join_timens() should be turned into a void function so
      it's obvious to callers and implementers on other architectures that
      the expectation is that it can't fail.
    
      We didn't do this right away because it would've introduced
      unnecessary merge conflicts between the two trees for no major gain.
    
      As always, tests included"
    
    [1]: https://lore.kernel.org/lkml/20200611110221.pgd3r5qkjrjmfqa2@wittgenstein
    
    * tag 'threads-v5.9' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:
      tests: add CLONE_NEWTIME setns tests
      nsproxy: support CLONE_NEWTIME with setns()
      timens: add timens_commit() helper
      timens: make vdso_join_timens() always succeed

commit 1a1206dc4cf02cee4b5cbce583ee4c22368b4c28
Author: Johannes Thumshirn <johannes.thumshirn@wdc.com>
Date:   Thu Jul 30 20:25:17 2020 +0900

    block: don't do revalidate zones on invalid devices
    
    When we loose a device for whatever reason while (re)scanning zones, we
    trip over a NULL pointer in blk_revalidate_zone_cb, like in the following
    log:
    
    sd 0:0:0:0: [sda] 3418095616 4096-byte logical blocks: (14.0 TB/12.7 TiB)
    sd 0:0:0:0: [sda] 52156 zones of 65536 logical blocks
    sd 0:0:0:0: [sda] Write Protect is off
    sd 0:0:0:0: [sda] Mode Sense: 37 00 00 08
    sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA
    sd 0:0:0:0: [sda] REPORT ZONES start lba 1065287680 failed
    sd 0:0:0:0: [sda] REPORT ZONES: Result: hostbyte=0x00 driverbyte=0x08
    sd 0:0:0:0: [sda] Sense Key : 0xb [current]
    sd 0:0:0:0: [sda] ASC=0x0 ASCQ=0x6
    sda: failed to revalidate zones
    sd 0:0:0:0: [sda] 0 4096-byte logical blocks: (0 B/0 B)
    sda: detected capacity change from 14000519643136 to 0
    ==================================================================
    BUG: KASAN: null-ptr-deref in blk_revalidate_zone_cb+0x1b7/0x550
    Write of size 8 at addr 0000000000000010 by task kworker/u4:1/58
    
    CPU: 1 PID: 58 Comm: kworker/u4:1 Not tainted 5.8.0-rc1 #692
    Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.13.0-0-gf21b5a4-rebuilt.opensuse.org 04/01/2014
    Workqueue: events_unbound async_run_entry_fn
    Call Trace:
     dump_stack+0x7d/0xb0
     ? blk_revalidate_zone_cb+0x1b7/0x550
     kasan_report.cold+0x5/0x37
     ? blk_revalidate_zone_cb+0x1b7/0x550
     check_memory_region+0x145/0x1a0
     blk_revalidate_zone_cb+0x1b7/0x550
     sd_zbc_parse_report+0x1f1/0x370
     ? blk_req_zone_write_trylock+0x200/0x200
     ? sectors_to_logical+0x60/0x60
     ? blk_req_zone_write_trylock+0x200/0x200
     ? blk_req_zone_write_trylock+0x200/0x200
     sd_zbc_report_zones+0x3c4/0x5e0
     ? sd_dif_config_host+0x500/0x500
     blk_revalidate_disk_zones+0x231/0x44d
     ? _raw_write_lock_irqsave+0xb0/0xb0
     ? blk_queue_free_zone_bitmaps+0xd0/0xd0
     sd_zbc_read_zones+0x8cf/0x11a0
     sd_revalidate_disk+0x305c/0x64e0
     ? __device_add_disk+0x776/0xf20
     ? read_capacity_16.part.0+0x1080/0x1080
     ? blk_alloc_devt+0x250/0x250
     ? create_object.isra.0+0x595/0xa20
     ? kasan_unpoison_shadow+0x33/0x40
     sd_probe+0x8dc/0xcd2
     really_probe+0x20e/0xaf0
     __driver_attach_async_helper+0x249/0x2d0
     async_run_entry_fn+0xbe/0x560
     process_one_work+0x764/0x1290
     ? _raw_read_unlock_irqrestore+0x30/0x30
     worker_thread+0x598/0x12f0
     ? __kthread_parkme+0xc6/0x1b0
     ? schedule+0xed/0x2c0
     ? process_one_work+0x1290/0x1290
     kthread+0x36b/0x440
     ? kthread_create_worker_on_cpu+0xa0/0xa0
     ret_from_fork+0x22/0x30
    ==================================================================
    
    When the device is already gone we end up with the following scenario:
    The device's capacity is 0 and thus the number of zones will be 0 as well. When
    allocating the bitmap for the conventional zones, we then trip over a NULL
    pointer.
    
    So if we encounter a zoned block device with a 0 capacity, don't dare to
    revalidate the zones sizes.
    
    Fixes: 6c6b35491422 ("block: set the zone size in blk_revalidate_disk_zones atomically")
    Signed-off-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Reviewed-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 3d6448e631591756da36efb3ea6355ff6f383c3a
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Jul 22 12:28:37 2020 +0100

    btrfs: fix race between page release and a fast fsync
    
    When releasing an extent map, done through the page release callback, we
    can race with an ongoing fast fsync and cause the fsync to miss a new
    extent and not log it. The steps for this to happen are the following:
    
    1) A page is dirtied for some inode I;
    
    2) Writeback for that page is triggered by a path other than fsync, for
       example by the system due to memory pressure;
    
    3) When the ordered extent for the extent (a single 4K page) finishes,
       we unpin the corresponding extent map and set its generation to N,
       the current transaction's generation;
    
    4) The btrfs_releasepage() callback is invoked by the system due to
       memory pressure for that no longer dirty page of inode I;
    
    5) At the same time, some task calls fsync on inode I, joins transaction
       N, and at btrfs_log_inode() it sees that the inode does not have the
       full sync flag set, so we proceed with a fast fsync. But before we get
       into btrfs_log_changed_extents() and lock the inode's extent map tree:
    
    6) Through btrfs_releasepage() we end up at try_release_extent_mapping()
       and we remove the extent map for the new 4Kb extent, because it is
       neither pinned anymore nor locked. By calling remove_extent_mapping(),
       we remove the extent map from the list of modified extents, since the
       extent map does not have the logging flag set. We unlock the inode's
       extent map tree;
    
    7) The task doing the fast fsync now enters btrfs_log_changed_extents(),
       locks the inode's extent map tree and iterates its list of modified
       extents, which no longer has the 4Kb extent in it, so it does not log
       the extent;
    
    8) The fsync finishes;
    
    9) Before transaction N is committed, a power failure happens. After
       replaying the log, the 4K extent of inode I will be missing, since
       it was not logged due to the race with try_release_extent_mapping().
    
    So fix this by teaching try_release_extent_mapping() to not remove an
    extent map if it's still in the list of modified extents.
    
    Fixes: ff44c6e36dc9dc ("Btrfs: do not hold the write_lock on the extent tree while logging")
    CC: stable@vger.kernel.org # 5.4+
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 2d15663304f563debeef6a4d0ac1a8295d5f261b
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Wed Jul 1 13:17:40 2020 +0200

    netfilter: conntrack: refetch conntrack after nf_conntrack_update()
    
    [ Upstream commit d005fbb855d3b5660d62ee5a6bd2d99c13ff8cf3 ]
    
    __nf_conntrack_update() might refresh the conntrack object that is
    attached to the skbuff. Otherwise, this triggers UAF.
    
    [  633.200434] ==================================================================
    [  633.200472] BUG: KASAN: use-after-free in nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200478] Read of size 1 at addr ffff888370804c00 by task nfqnl_test/6769
    
    [  633.200487] CPU: 1 PID: 6769 Comm: nfqnl_test Not tainted 5.8.0-rc2+ #388
    [  633.200490] Hardware name: LENOVO 23259H1/23259H1, BIOS G2ET32WW (1.12 ) 05/30/2012
    [  633.200491] Call Trace:
    [  633.200499]  dump_stack+0x7c/0xb0
    [  633.200526]  ? nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200532]  print_address_description.constprop.6+0x1a/0x200
    [  633.200539]  ? _raw_write_lock_irqsave+0xc0/0xc0
    [  633.200568]  ? nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200594]  ? nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200598]  kasan_report.cold.9+0x1f/0x42
    [  633.200604]  ? call_rcu+0x2c0/0x390
    [  633.200633]  ? nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200659]  nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200687]  ? nf_conntrack_find_get+0x30/0x30 [nf_conntrack]
    
    Closes: https://bugzilla.netfilter.org/show_bug.cgi?id=1436
    Fixes: ee04805ff54a ("netfilter: conntrack: make conntrack userspace helpers work again")
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2000bb546525890584a3cf0ce4787270250b582c
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Thu Jun 25 16:12:59 2020 -0700

    bpf, sockmap: RCU splat with redirect and strparser error or TLS
    
    [ Upstream commit 93dd5f185916b05e931cffae636596f21f98546e ]
    
    There are two paths to generate the below RCU splat the first and
    most obvious is the result of the BPF verdict program issuing a
    redirect on a TLS socket (This is the splat shown below). Unlike
    the non-TLS case the caller of the *strp_read() hooks does not
    wrap the call in a rcu_read_lock/unlock. Then if the BPF program
    issues a redirect action we hit the RCU splat.
    
    However, in the non-TLS socket case the splat appears to be
    relatively rare, because the skmsg caller into the strp_data_ready()
    is wrapped in a rcu_read_lock/unlock. Shown here,
    
     static void sk_psock_strp_data_ready(struct sock *sk)
     {
            struct sk_psock *psock;
    
            rcu_read_lock();
            psock = sk_psock(sk);
            if (likely(psock)) {
                    if (tls_sw_has_ctx_rx(sk)) {
                            psock->parser.saved_data_ready(sk);
                    } else {
                            write_lock_bh(&sk->sk_callback_lock);
                            strp_data_ready(&psock->parser.strp);
                            write_unlock_bh(&sk->sk_callback_lock);
                    }
            }
            rcu_read_unlock();
     }
    
    If the above was the only way to run the verdict program we
    would be safe. But, there is a case where the strparser may throw an
    ENOMEM error while parsing the skb. This is a result of a failed
    skb_clone, or alloc_skb_for_msg while building a new merged skb when
    the msg length needed spans multiple skbs. This will in turn put the
    skb on the strp_wrk workqueue in the strparser code. The skb will
    later be dequeued and verdict programs run, but now from a
    different context without the rcu_read_lock()/unlock() critical
    section in sk_psock_strp_data_ready() shown above. In practice
    I have not seen this yet, because as far as I know most users of the
    verdict programs are also only working on single skbs. In this case no
    merge happens which could trigger the above ENOMEM errors. In addition
    the system would need to be under memory pressure. For example, we
    can't hit the above case in selftests because we missed having tests
    to merge skbs. (Added in later patch)
    
    To fix the below splat extend the rcu_read_lock/unnlock block to
    include the call to sk_psock_tls_verdict_apply(). This will fix both
    TLS redirect case and non-TLS redirect+error case. Also remove
    psock from the sk_psock_tls_verdict_apply() function signature its
    not used there.
    
    [ 1095.937597] WARNING: suspicious RCU usage
    [ 1095.940964] 5.7.0-rc7-02911-g463bac5f1ca79 #1 Tainted: G        W
    [ 1095.944363] -----------------------------
    [ 1095.947384] include/linux/skmsg.h:284 suspicious rcu_dereference_check() usage!
    [ 1095.950866]
    [ 1095.950866] other info that might help us debug this:
    [ 1095.950866]
    [ 1095.957146]
    [ 1095.957146] rcu_scheduler_active = 2, debug_locks = 1
    [ 1095.961482] 1 lock held by test_sockmap/15970:
    [ 1095.964501]  #0: ffff9ea6b25de660 (sk_lock-AF_INET){+.+.}-{0:0}, at: tls_sw_recvmsg+0x13a/0x840 [tls]
    [ 1095.968568]
    [ 1095.968568] stack backtrace:
    [ 1095.975001] CPU: 1 PID: 15970 Comm: test_sockmap Tainted: G        W         5.7.0-rc7-02911-g463bac5f1ca79 #1
    [ 1095.977883] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-1 04/01/2014
    [ 1095.980519] Call Trace:
    [ 1095.982191]  dump_stack+0x8f/0xd0
    [ 1095.984040]  sk_psock_skb_redirect+0xa6/0xf0
    [ 1095.986073]  sk_psock_tls_strp_read+0x1d8/0x250
    [ 1095.988095]  tls_sw_recvmsg+0x714/0x840 [tls]
    
    v2: Improve commit message to identify non-TLS redirect plus error case
        condition as well as more common TLS case. In the process I decided
        doing the rcu_read_unlock followed by the lock/unlock inside branches
        was unnecessarily complex. We can just extend the current rcu block
        and get the same effeective without the shuffling and branching.
        Thanks Martin!
    
    Fixes: e91de6afa81c1 ("bpf: Fix running sk_skb program types with ktls")
    Reported-by: Jakub Sitnicki <jakub@cloudflare.com>
    Reported-by: kernel test robot <rong.a.chen@intel.com>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Jakub Sitnicki <jakub@cloudflare.com>
    Link: https://lore.kernel.org/bpf/159312677907.18340.11064813152758406626.stgit@john-XPS-13-9370
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit c7e08f82172d4cf59a019fcad0f1dfbd59aeaa62
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Wed Jul 1 13:17:40 2020 +0200

    netfilter: conntrack: refetch conntrack after nf_conntrack_update()
    
    [ Upstream commit d005fbb855d3b5660d62ee5a6bd2d99c13ff8cf3 ]
    
    __nf_conntrack_update() might refresh the conntrack object that is
    attached to the skbuff. Otherwise, this triggers UAF.
    
    [  633.200434] ==================================================================
    [  633.200472] BUG: KASAN: use-after-free in nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200478] Read of size 1 at addr ffff888370804c00 by task nfqnl_test/6769
    
    [  633.200487] CPU: 1 PID: 6769 Comm: nfqnl_test Not tainted 5.8.0-rc2+ #388
    [  633.200490] Hardware name: LENOVO 23259H1/23259H1, BIOS G2ET32WW (1.12 ) 05/30/2012
    [  633.200491] Call Trace:
    [  633.200499]  dump_stack+0x7c/0xb0
    [  633.200526]  ? nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200532]  print_address_description.constprop.6+0x1a/0x200
    [  633.200539]  ? _raw_write_lock_irqsave+0xc0/0xc0
    [  633.200568]  ? nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200594]  ? nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200598]  kasan_report.cold.9+0x1f/0x42
    [  633.200604]  ? call_rcu+0x2c0/0x390
    [  633.200633]  ? nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200659]  nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200687]  ? nf_conntrack_find_get+0x30/0x30 [nf_conntrack]
    
    Closes: https://bugzilla.netfilter.org/show_bug.cgi?id=1436
    Fixes: ee04805ff54a ("netfilter: conntrack: make conntrack userspace helpers work again")
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 7f1c6b6194807a926deac5363d532c662489a09f
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Thu Jun 25 16:12:59 2020 -0700

    bpf, sockmap: RCU splat with redirect and strparser error or TLS
    
    [ Upstream commit 93dd5f185916b05e931cffae636596f21f98546e ]
    
    There are two paths to generate the below RCU splat the first and
    most obvious is the result of the BPF verdict program issuing a
    redirect on a TLS socket (This is the splat shown below). Unlike
    the non-TLS case the caller of the *strp_read() hooks does not
    wrap the call in a rcu_read_lock/unlock. Then if the BPF program
    issues a redirect action we hit the RCU splat.
    
    However, in the non-TLS socket case the splat appears to be
    relatively rare, because the skmsg caller into the strp_data_ready()
    is wrapped in a rcu_read_lock/unlock. Shown here,
    
     static void sk_psock_strp_data_ready(struct sock *sk)
     {
            struct sk_psock *psock;
    
            rcu_read_lock();
            psock = sk_psock(sk);
            if (likely(psock)) {
                    if (tls_sw_has_ctx_rx(sk)) {
                            psock->parser.saved_data_ready(sk);
                    } else {
                            write_lock_bh(&sk->sk_callback_lock);
                            strp_data_ready(&psock->parser.strp);
                            write_unlock_bh(&sk->sk_callback_lock);
                    }
            }
            rcu_read_unlock();
     }
    
    If the above was the only way to run the verdict program we
    would be safe. But, there is a case where the strparser may throw an
    ENOMEM error while parsing the skb. This is a result of a failed
    skb_clone, or alloc_skb_for_msg while building a new merged skb when
    the msg length needed spans multiple skbs. This will in turn put the
    skb on the strp_wrk workqueue in the strparser code. The skb will
    later be dequeued and verdict programs run, but now from a
    different context without the rcu_read_lock()/unlock() critical
    section in sk_psock_strp_data_ready() shown above. In practice
    I have not seen this yet, because as far as I know most users of the
    verdict programs are also only working on single skbs. In this case no
    merge happens which could trigger the above ENOMEM errors. In addition
    the system would need to be under memory pressure. For example, we
    can't hit the above case in selftests because we missed having tests
    to merge skbs. (Added in later patch)
    
    To fix the below splat extend the rcu_read_lock/unnlock block to
    include the call to sk_psock_tls_verdict_apply(). This will fix both
    TLS redirect case and non-TLS redirect+error case. Also remove
    psock from the sk_psock_tls_verdict_apply() function signature its
    not used there.
    
    [ 1095.937597] WARNING: suspicious RCU usage
    [ 1095.940964] 5.7.0-rc7-02911-g463bac5f1ca79 #1 Tainted: G        W
    [ 1095.944363] -----------------------------
    [ 1095.947384] include/linux/skmsg.h:284 suspicious rcu_dereference_check() usage!
    [ 1095.950866]
    [ 1095.950866] other info that might help us debug this:
    [ 1095.950866]
    [ 1095.957146]
    [ 1095.957146] rcu_scheduler_active = 2, debug_locks = 1
    [ 1095.961482] 1 lock held by test_sockmap/15970:
    [ 1095.964501]  #0: ffff9ea6b25de660 (sk_lock-AF_INET){+.+.}-{0:0}, at: tls_sw_recvmsg+0x13a/0x840 [tls]
    [ 1095.968568]
    [ 1095.968568] stack backtrace:
    [ 1095.975001] CPU: 1 PID: 15970 Comm: test_sockmap Tainted: G        W         5.7.0-rc7-02911-g463bac5f1ca79 #1
    [ 1095.977883] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-1 04/01/2014
    [ 1095.980519] Call Trace:
    [ 1095.982191]  dump_stack+0x8f/0xd0
    [ 1095.984040]  sk_psock_skb_redirect+0xa6/0xf0
    [ 1095.986073]  sk_psock_tls_strp_read+0x1d8/0x250
    [ 1095.988095]  tls_sw_recvmsg+0x714/0x840 [tls]
    
    v2: Improve commit message to identify non-TLS redirect plus error case
        condition as well as more common TLS case. In the process I decided
        doing the rcu_read_unlock followed by the lock/unlock inside branches
        was unnecessarily complex. We can just extend the current rcu block
        and get the same effeective without the shuffling and branching.
        Thanks Martin!
    
    Fixes: e91de6afa81c1 ("bpf: Fix running sk_skb program types with ktls")
    Reported-by: Jakub Sitnicki <jakub@cloudflare.com>
    Reported-by: kernel test robot <rong.a.chen@intel.com>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Jakub Sitnicki <jakub@cloudflare.com>
    Link: https://lore.kernel.org/bpf/159312677907.18340.11064813152758406626.stgit@john-XPS-13-9370
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 42815808f1791eb53c3a05fb78c0a8642ecf8467
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Mon Jul 6 17:49:09 2020 +0200

    timens: make vdso_join_timens() always succeed
    
    As discussed on-list (cf. [1]), in order to make setns() support time
    namespaces when attaching to multiple namespaces at once properly we
    need to tweak vdso_join_timens() to always succeed. So switch
    vdso_join_timens() to using a read lock and replacing
    mmap_write_lock_killable() to mmap_read_lock() as we discussed.
    
    Last cycle setns() was changed to support attaching to multiple namespaces
    atomically. This requires all namespaces to have a point of no return where
    they can't fail anymore. Specifically, <namespace-type>_install() is
    allowed to perform permission checks and install the namespace into the new
    struct nsset that it has been given but it is not allowed to make visible
    changes to the affected task. Once <namespace-type>_install() returns
    anything that the given namespace type requires to be setup in addition
    needs to ideally be done in a function that can't fail or if it fails the
    failure is not fatal. For time namespaces the relevant functions that fall
    into this category are timens_set_vvar_page() and vdso_join_timens().
    Currently the latter can fail but doesn't need to. With this we can go on
    to implement a timens_commit() helper in a follow up patch to be used by
    setns().
    
    [1]: https://lore.kernel.org/lkml/20200611110221.pgd3r5qkjrjmfqa2@wittgenstein
    
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Reviewed-by: Andrei Vagin <avagin@gmail.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Dmitry Safonov <dima@arista.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Link: https://lore.kernel.org/r/20200706154912.3248030-2-christian.brauner@ubuntu.com

commit d005fbb855d3b5660d62ee5a6bd2d99c13ff8cf3
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Wed Jul 1 13:17:40 2020 +0200

    netfilter: conntrack: refetch conntrack after nf_conntrack_update()
    
    __nf_conntrack_update() might refresh the conntrack object that is
    attached to the skbuff. Otherwise, this triggers UAF.
    
    [  633.200434] ==================================================================
    [  633.200472] BUG: KASAN: use-after-free in nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200478] Read of size 1 at addr ffff888370804c00 by task nfqnl_test/6769
    
    [  633.200487] CPU: 1 PID: 6769 Comm: nfqnl_test Not tainted 5.8.0-rc2+ #388
    [  633.200490] Hardware name: LENOVO 23259H1/23259H1, BIOS G2ET32WW (1.12 ) 05/30/2012
    [  633.200491] Call Trace:
    [  633.200499]  dump_stack+0x7c/0xb0
    [  633.200526]  ? nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200532]  print_address_description.constprop.6+0x1a/0x200
    [  633.200539]  ? _raw_write_lock_irqsave+0xc0/0xc0
    [  633.200568]  ? nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200594]  ? nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200598]  kasan_report.cold.9+0x1f/0x42
    [  633.200604]  ? call_rcu+0x2c0/0x390
    [  633.200633]  ? nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200659]  nf_conntrack_update+0x34e/0x770 [nf_conntrack]
    [  633.200687]  ? nf_conntrack_find_get+0x30/0x30 [nf_conntrack]
    
    Closes: https://bugzilla.netfilter.org/show_bug.cgi?id=1436
    Fixes: ee04805ff54a ("netfilter: conntrack: make conntrack userspace helpers work again")
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

commit 51a544f05b056d805cbd5a3bd5693fd1530b1ef6
Author: Michal Kalderon <michal.kalderon@marvell.com>
Date:   Tue Jun 16 12:34:08 2020 +0300

    RDMA/qedr: Fix KASAN: use-after-free in ucma_event_handler+0x532
    
    [ Upstream commit 0dfbd5ecf28cbcb81674c49d34ee97366db1be44 ]
    
    Private data passed to iwarp_cm_handler is copied for connection request /
    response, but ignored otherwise.  If junk is passed, it is stored in the
    event and used later in the event processing.
    
    The driver passes an old junk pointer during connection close which leads
    to a use-after-free on event processing.  Set private data to NULL for
    events that don 't have private data.
    
      BUG: KASAN: use-after-free in ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: Read of size 4 at addr ffff8886caa71200 by task kworker/u128:1/5250
      kernel:
      kernel: Workqueue: iw_cm_wq cm_work_handler [iw_cm]
      kernel: Call Trace:
      kernel: dump_stack+0x8c/0xc0
      kernel: print_address_description.constprop.0+0x1b/0x210
      kernel: ? ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: ? ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: __kasan_report.cold+0x1a/0x33
      kernel: ? ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: kasan_report+0xe/0x20
      kernel: check_memory_region+0x130/0x1a0
      kernel: memcpy+0x20/0x50
      kernel: ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: ? __rpc_execute+0x608/0x620 [sunrpc]
      kernel: cma_iw_handler+0x212/0x330 [rdma_cm]
      kernel: ? iw_conn_req_handler+0x6e0/0x6e0 [rdma_cm]
      kernel: ? enqueue_timer+0x86/0x140
      kernel: ? _raw_write_lock_irq+0xd0/0xd0
      kernel: cm_work_handler+0xd3d/0x1070 [iw_cm]
    
    Fixes: e411e0587e0d ("RDMA/qedr: Add iWARP connection management functions")
    Link: https://lore.kernel.org/r/20200616093408.17827-1-michal.kalderon@marvell.com
    Signed-off-by: Ariel Elior <ariel.elior@marvell.com>
    Signed-off-by: Michal Kalderon <michal.kalderon@marvell.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit be676835b2250861d525b8e2a8e2bed8c1a9027d
Author: Zhiqiang Liu <liuzhiqiang26@huawei.com>
Date:   Mon Jun 15 00:53:30 2020 +0800

    bcache: fix potential deadlock problem in btree_gc_coalesce
    
    [ Upstream commit be23e837333a914df3f24bf0b32e87b0331ab8d1 ]
    
    coccicheck reports:
      drivers/md//bcache/btree.c:1538:1-7: preceding lock on line 1417
    
    In btree_gc_coalesce func, if the coalescing process fails, we will goto
    to out_nocoalesce tag directly without releasing new_nodes[i]->write_lock.
    Then, it will cause a deadlock when trying to acquire new_nodes[i]->
    write_lock for freeing new_nodes[i] before return.
    
    btree_gc_coalesce func details as follows:
            if alloc new_nodes[i] fails:
                    goto out_nocoalesce;
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock)
            // main coalescing process
            for (i = nodes - 1; i > 0; --i)
                    [snipped]
                    if coalescing process fails:
                            // Here, directly goto out_nocoalesce
                             // tag will cause a deadlock
                            goto out_nocoalesce;
                    [snipped]
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock)
            // coalesing succ, return
            return;
    out_nocoalesce:
            btree_node_free(new_nodes[i])   // free new_nodes[i]
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock);
            // set flag for reuse
            clear_bit(BTREE_NODE_dirty, &ew_nodes[i]->flags);
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock);
    
    To fix the problem, we add a new tag 'out_unlock_nocoalesce' for
    releasing new_nodes[i]->write_lock before out_nocoalesce tag. If
    coalescing process fails, we will go to out_unlock_nocoalesce tag
    for releasing new_nodes[i]->write_lock before free new_nodes[i] in
    out_nocoalesce tag.
    
    (Coly Li helps to clean up commit log format.)
    
    Fixes: 2a285686c109816 ("bcache: btree locking rework")
    Signed-off-by: Zhiqiang Liu <liuzhiqiang26@huawei.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f0078dc6750fe46c7955de081b05cbf7d00d9093
Author: Michal Kalderon <michal.kalderon@marvell.com>
Date:   Tue Jun 16 12:34:08 2020 +0300

    RDMA/qedr: Fix KASAN: use-after-free in ucma_event_handler+0x532
    
    [ Upstream commit 0dfbd5ecf28cbcb81674c49d34ee97366db1be44 ]
    
    Private data passed to iwarp_cm_handler is copied for connection request /
    response, but ignored otherwise.  If junk is passed, it is stored in the
    event and used later in the event processing.
    
    The driver passes an old junk pointer during connection close which leads
    to a use-after-free on event processing.  Set private data to NULL for
    events that don 't have private data.
    
      BUG: KASAN: use-after-free in ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: Read of size 4 at addr ffff8886caa71200 by task kworker/u128:1/5250
      kernel:
      kernel: Workqueue: iw_cm_wq cm_work_handler [iw_cm]
      kernel: Call Trace:
      kernel: dump_stack+0x8c/0xc0
      kernel: print_address_description.constprop.0+0x1b/0x210
      kernel: ? ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: ? ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: __kasan_report.cold+0x1a/0x33
      kernel: ? ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: kasan_report+0xe/0x20
      kernel: check_memory_region+0x130/0x1a0
      kernel: memcpy+0x20/0x50
      kernel: ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: ? __rpc_execute+0x608/0x620 [sunrpc]
      kernel: cma_iw_handler+0x212/0x330 [rdma_cm]
      kernel: ? iw_conn_req_handler+0x6e0/0x6e0 [rdma_cm]
      kernel: ? enqueue_timer+0x86/0x140
      kernel: ? _raw_write_lock_irq+0xd0/0xd0
      kernel: cm_work_handler+0xd3d/0x1070 [iw_cm]
    
    Fixes: e411e0587e0d ("RDMA/qedr: Add iWARP connection management functions")
    Link: https://lore.kernel.org/r/20200616093408.17827-1-michal.kalderon@marvell.com
    Signed-off-by: Ariel Elior <ariel.elior@marvell.com>
    Signed-off-by: Michal Kalderon <michal.kalderon@marvell.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit c6e0bca33fa8a01e038d3de648dd848d2d286002
Author: Michal Kalderon <michal.kalderon@marvell.com>
Date:   Tue Jun 16 12:34:08 2020 +0300

    RDMA/qedr: Fix KASAN: use-after-free in ucma_event_handler+0x532
    
    [ Upstream commit 0dfbd5ecf28cbcb81674c49d34ee97366db1be44 ]
    
    Private data passed to iwarp_cm_handler is copied for connection request /
    response, but ignored otherwise.  If junk is passed, it is stored in the
    event and used later in the event processing.
    
    The driver passes an old junk pointer during connection close which leads
    to a use-after-free on event processing.  Set private data to NULL for
    events that don 't have private data.
    
      BUG: KASAN: use-after-free in ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: Read of size 4 at addr ffff8886caa71200 by task kworker/u128:1/5250
      kernel:
      kernel: Workqueue: iw_cm_wq cm_work_handler [iw_cm]
      kernel: Call Trace:
      kernel: dump_stack+0x8c/0xc0
      kernel: print_address_description.constprop.0+0x1b/0x210
      kernel: ? ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: ? ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: __kasan_report.cold+0x1a/0x33
      kernel: ? ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: kasan_report+0xe/0x20
      kernel: check_memory_region+0x130/0x1a0
      kernel: memcpy+0x20/0x50
      kernel: ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: ? __rpc_execute+0x608/0x620 [sunrpc]
      kernel: cma_iw_handler+0x212/0x330 [rdma_cm]
      kernel: ? iw_conn_req_handler+0x6e0/0x6e0 [rdma_cm]
      kernel: ? enqueue_timer+0x86/0x140
      kernel: ? _raw_write_lock_irq+0xd0/0xd0
      kernel: cm_work_handler+0xd3d/0x1070 [iw_cm]
    
    Fixes: e411e0587e0d ("RDMA/qedr: Add iWARP connection management functions")
    Link: https://lore.kernel.org/r/20200616093408.17827-1-michal.kalderon@marvell.com
    Signed-off-by: Ariel Elior <ariel.elior@marvell.com>
    Signed-off-by: Michal Kalderon <michal.kalderon@marvell.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 9517bec2c1ff8f223f8f2d28c743731e8f216bbe
Author: Zhiqiang Liu <liuzhiqiang26@huawei.com>
Date:   Mon Jun 15 00:53:30 2020 +0800

    bcache: fix potential deadlock problem in btree_gc_coalesce
    
    [ Upstream commit be23e837333a914df3f24bf0b32e87b0331ab8d1 ]
    
    coccicheck reports:
      drivers/md//bcache/btree.c:1538:1-7: preceding lock on line 1417
    
    In btree_gc_coalesce func, if the coalescing process fails, we will goto
    to out_nocoalesce tag directly without releasing new_nodes[i]->write_lock.
    Then, it will cause a deadlock when trying to acquire new_nodes[i]->
    write_lock for freeing new_nodes[i] before return.
    
    btree_gc_coalesce func details as follows:
            if alloc new_nodes[i] fails:
                    goto out_nocoalesce;
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock)
            // main coalescing process
            for (i = nodes - 1; i > 0; --i)
                    [snipped]
                    if coalescing process fails:
                            // Here, directly goto out_nocoalesce
                             // tag will cause a deadlock
                            goto out_nocoalesce;
                    [snipped]
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock)
            // coalesing succ, return
            return;
    out_nocoalesce:
            btree_node_free(new_nodes[i])   // free new_nodes[i]
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock);
            // set flag for reuse
            clear_bit(BTREE_NODE_dirty, &ew_nodes[i]->flags);
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock);
    
    To fix the problem, we add a new tag 'out_unlock_nocoalesce' for
    releasing new_nodes[i]->write_lock before out_nocoalesce tag. If
    coalescing process fails, we will go to out_unlock_nocoalesce tag
    for releasing new_nodes[i]->write_lock before free new_nodes[i] in
    out_nocoalesce tag.
    
    (Coly Li helps to clean up commit log format.)
    
    Fixes: 2a285686c109816 ("bcache: btree locking rework")
    Signed-off-by: Zhiqiang Liu <liuzhiqiang26@huawei.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 93dd5f185916b05e931cffae636596f21f98546e
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Thu Jun 25 16:12:59 2020 -0700

    bpf, sockmap: RCU splat with redirect and strparser error or TLS
    
    There are two paths to generate the below RCU splat the first and
    most obvious is the result of the BPF verdict program issuing a
    redirect on a TLS socket (This is the splat shown below). Unlike
    the non-TLS case the caller of the *strp_read() hooks does not
    wrap the call in a rcu_read_lock/unlock. Then if the BPF program
    issues a redirect action we hit the RCU splat.
    
    However, in the non-TLS socket case the splat appears to be
    relatively rare, because the skmsg caller into the strp_data_ready()
    is wrapped in a rcu_read_lock/unlock. Shown here,
    
     static void sk_psock_strp_data_ready(struct sock *sk)
     {
            struct sk_psock *psock;
    
            rcu_read_lock();
            psock = sk_psock(sk);
            if (likely(psock)) {
                    if (tls_sw_has_ctx_rx(sk)) {
                            psock->parser.saved_data_ready(sk);
                    } else {
                            write_lock_bh(&sk->sk_callback_lock);
                            strp_data_ready(&psock->parser.strp);
                            write_unlock_bh(&sk->sk_callback_lock);
                    }
            }
            rcu_read_unlock();
     }
    
    If the above was the only way to run the verdict program we
    would be safe. But, there is a case where the strparser may throw an
    ENOMEM error while parsing the skb. This is a result of a failed
    skb_clone, or alloc_skb_for_msg while building a new merged skb when
    the msg length needed spans multiple skbs. This will in turn put the
    skb on the strp_wrk workqueue in the strparser code. The skb will
    later be dequeued and verdict programs run, but now from a
    different context without the rcu_read_lock()/unlock() critical
    section in sk_psock_strp_data_ready() shown above. In practice
    I have not seen this yet, because as far as I know most users of the
    verdict programs are also only working on single skbs. In this case no
    merge happens which could trigger the above ENOMEM errors. In addition
    the system would need to be under memory pressure. For example, we
    can't hit the above case in selftests because we missed having tests
    to merge skbs. (Added in later patch)
    
    To fix the below splat extend the rcu_read_lock/unnlock block to
    include the call to sk_psock_tls_verdict_apply(). This will fix both
    TLS redirect case and non-TLS redirect+error case. Also remove
    psock from the sk_psock_tls_verdict_apply() function signature its
    not used there.
    
    [ 1095.937597] WARNING: suspicious RCU usage
    [ 1095.940964] 5.7.0-rc7-02911-g463bac5f1ca79 #1 Tainted: G        W
    [ 1095.944363] -----------------------------
    [ 1095.947384] include/linux/skmsg.h:284 suspicious rcu_dereference_check() usage!
    [ 1095.950866]
    [ 1095.950866] other info that might help us debug this:
    [ 1095.950866]
    [ 1095.957146]
    [ 1095.957146] rcu_scheduler_active = 2, debug_locks = 1
    [ 1095.961482] 1 lock held by test_sockmap/15970:
    [ 1095.964501]  #0: ffff9ea6b25de660 (sk_lock-AF_INET){+.+.}-{0:0}, at: tls_sw_recvmsg+0x13a/0x840 [tls]
    [ 1095.968568]
    [ 1095.968568] stack backtrace:
    [ 1095.975001] CPU: 1 PID: 15970 Comm: test_sockmap Tainted: G        W         5.7.0-rc7-02911-g463bac5f1ca79 #1
    [ 1095.977883] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-1 04/01/2014
    [ 1095.980519] Call Trace:
    [ 1095.982191]  dump_stack+0x8f/0xd0
    [ 1095.984040]  sk_psock_skb_redirect+0xa6/0xf0
    [ 1095.986073]  sk_psock_tls_strp_read+0x1d8/0x250
    [ 1095.988095]  tls_sw_recvmsg+0x714/0x840 [tls]
    
    v2: Improve commit message to identify non-TLS redirect plus error case
        condition as well as more common TLS case. In the process I decided
        doing the rcu_read_unlock followed by the lock/unlock inside branches
        was unnecessarily complex. We can just extend the current rcu block
        and get the same effeective without the shuffling and branching.
        Thanks Martin!
    
    Fixes: e91de6afa81c1 ("bpf: Fix running sk_skb program types with ktls")
    Reported-by: Jakub Sitnicki <jakub@cloudflare.com>
    Reported-by: kernel test robot <rong.a.chen@intel.com>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Jakub Sitnicki <jakub@cloudflare.com>
    Link: https://lore.kernel.org/bpf/159312677907.18340.11064813152758406626.stgit@john-XPS-13-9370

commit 313a5257b84c26b7f080c5d294aabe7d38ca439c
Author: Stafford Horne <shorne@gmail.com>
Date:   Thu Jun 25 20:29:17 2020 -0700

    openrisc: fix boot oops when DEBUG_VM is enabled
    
    Since v5.8-rc1 OpenRISC Linux fails to boot when DEBUG_VM is enabled.
    This has been bisected to commit 42fc541404f2 ("mmap locking API: add
    mmap_assert_locked() and mmap_assert_write_locked()").
    
    The added locking checks exposed the issue that OpenRISC was not taking
    this mmap lock when during page walks for DMA operations.  This patch
    locks and unlocks the mmap lock for page walking.
    
    Link: http://lkml.kernel.org/r/20200617090247.1680188-1-shorne@gmail.com
    Fixes: 42fc541404f2 ("mmap locking API: add mmap_assert_locked() and mmap_assert_write_locked()"
    Signed-off-by: Stafford Horne <shorne@gmail.com>
    Reviewed-by: Michel Lespinasse <walken@google.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Stefan Kristiansson <stefan.kristiansson@saunalahti.fi>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Steven Price <steven.price@arm.com>
    Cc: Thomas Hellstrom <thellstrom@vmware.com>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Daniel Jordan <daniel.m.jordan@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 80d90125a3e1c98aafb6b13e62bd680fe9a4cd5a
Author: Zhiqiang Liu <liuzhiqiang26@huawei.com>
Date:   Mon Jun 15 00:53:30 2020 +0800

    bcache: fix potential deadlock problem in btree_gc_coalesce
    
    [ Upstream commit be23e837333a914df3f24bf0b32e87b0331ab8d1 ]
    
    coccicheck reports:
      drivers/md//bcache/btree.c:1538:1-7: preceding lock on line 1417
    
    In btree_gc_coalesce func, if the coalescing process fails, we will goto
    to out_nocoalesce tag directly without releasing new_nodes[i]->write_lock.
    Then, it will cause a deadlock when trying to acquire new_nodes[i]->
    write_lock for freeing new_nodes[i] before return.
    
    btree_gc_coalesce func details as follows:
            if alloc new_nodes[i] fails:
                    goto out_nocoalesce;
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock)
            // main coalescing process
            for (i = nodes - 1; i > 0; --i)
                    [snipped]
                    if coalescing process fails:
                            // Here, directly goto out_nocoalesce
                             // tag will cause a deadlock
                            goto out_nocoalesce;
                    [snipped]
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock)
            // coalesing succ, return
            return;
    out_nocoalesce:
            btree_node_free(new_nodes[i])   // free new_nodes[i]
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock);
            // set flag for reuse
            clear_bit(BTREE_NODE_dirty, &ew_nodes[i]->flags);
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock);
    
    To fix the problem, we add a new tag 'out_unlock_nocoalesce' for
    releasing new_nodes[i]->write_lock before out_nocoalesce tag. If
    coalescing process fails, we will go to out_unlock_nocoalesce tag
    for releasing new_nodes[i]->write_lock before free new_nodes[i] in
    out_nocoalesce tag.
    
    (Coly Li helps to clean up commit log format.)
    
    Fixes: 2a285686c109816 ("bcache: btree locking rework")
    Signed-off-by: Zhiqiang Liu <liuzhiqiang26@huawei.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2ee8f6e72269eb06649ebab3da6bbf59aed3eabe
Author: Zhiqiang Liu <liuzhiqiang26@huawei.com>
Date:   Mon Jun 15 00:53:30 2020 +0800

    bcache: fix potential deadlock problem in btree_gc_coalesce
    
    [ Upstream commit be23e837333a914df3f24bf0b32e87b0331ab8d1 ]
    
    coccicheck reports:
      drivers/md//bcache/btree.c:1538:1-7: preceding lock on line 1417
    
    In btree_gc_coalesce func, if the coalescing process fails, we will goto
    to out_nocoalesce tag directly without releasing new_nodes[i]->write_lock.
    Then, it will cause a deadlock when trying to acquire new_nodes[i]->
    write_lock for freeing new_nodes[i] before return.
    
    btree_gc_coalesce func details as follows:
            if alloc new_nodes[i] fails:
                    goto out_nocoalesce;
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock)
            // main coalescing process
            for (i = nodes - 1; i > 0; --i)
                    [snipped]
                    if coalescing process fails:
                            // Here, directly goto out_nocoalesce
                             // tag will cause a deadlock
                            goto out_nocoalesce;
                    [snipped]
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock)
            // coalesing succ, return
            return;
    out_nocoalesce:
            btree_node_free(new_nodes[i])   // free new_nodes[i]
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock);
            // set flag for reuse
            clear_bit(BTREE_NODE_dirty, &ew_nodes[i]->flags);
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock);
    
    To fix the problem, we add a new tag 'out_unlock_nocoalesce' for
    releasing new_nodes[i]->write_lock before out_nocoalesce tag. If
    coalescing process fails, we will go to out_unlock_nocoalesce tag
    for releasing new_nodes[i]->write_lock before free new_nodes[i] in
    out_nocoalesce tag.
    
    (Coly Li helps to clean up commit log format.)
    
    Fixes: 2a285686c109816 ("bcache: btree locking rework")
    Signed-off-by: Zhiqiang Liu <liuzhiqiang26@huawei.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f651e94899ed08b1766bda30f410d33fdd3970ff
Author: Zhiqiang Liu <liuzhiqiang26@huawei.com>
Date:   Mon Jun 15 00:53:30 2020 +0800

    bcache: fix potential deadlock problem in btree_gc_coalesce
    
    [ Upstream commit be23e837333a914df3f24bf0b32e87b0331ab8d1 ]
    
    coccicheck reports:
      drivers/md//bcache/btree.c:1538:1-7: preceding lock on line 1417
    
    In btree_gc_coalesce func, if the coalescing process fails, we will goto
    to out_nocoalesce tag directly without releasing new_nodes[i]->write_lock.
    Then, it will cause a deadlock when trying to acquire new_nodes[i]->
    write_lock for freeing new_nodes[i] before return.
    
    btree_gc_coalesce func details as follows:
            if alloc new_nodes[i] fails:
                    goto out_nocoalesce;
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock)
            // main coalescing process
            for (i = nodes - 1; i > 0; --i)
                    [snipped]
                    if coalescing process fails:
                            // Here, directly goto out_nocoalesce
                             // tag will cause a deadlock
                            goto out_nocoalesce;
                    [snipped]
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock)
            // coalesing succ, return
            return;
    out_nocoalesce:
            btree_node_free(new_nodes[i])   // free new_nodes[i]
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock);
            // set flag for reuse
            clear_bit(BTREE_NODE_dirty, &ew_nodes[i]->flags);
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock);
    
    To fix the problem, we add a new tag 'out_unlock_nocoalesce' for
    releasing new_nodes[i]->write_lock before out_nocoalesce tag. If
    coalescing process fails, we will go to out_unlock_nocoalesce tag
    for releasing new_nodes[i]->write_lock before free new_nodes[i] in
    out_nocoalesce tag.
    
    (Coly Li helps to clean up commit log format.)
    
    Fixes: 2a285686c109816 ("bcache: btree locking rework")
    Signed-off-by: Zhiqiang Liu <liuzhiqiang26@huawei.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 4813dd656732207ad9df7738652bbbbde4c7c928
Author: Zhiqiang Liu <liuzhiqiang26@huawei.com>
Date:   Mon Jun 15 00:53:30 2020 +0800

    bcache: fix potential deadlock problem in btree_gc_coalesce
    
    [ Upstream commit be23e837333a914df3f24bf0b32e87b0331ab8d1 ]
    
    coccicheck reports:
      drivers/md//bcache/btree.c:1538:1-7: preceding lock on line 1417
    
    In btree_gc_coalesce func, if the coalescing process fails, we will goto
    to out_nocoalesce tag directly without releasing new_nodes[i]->write_lock.
    Then, it will cause a deadlock when trying to acquire new_nodes[i]->
    write_lock for freeing new_nodes[i] before return.
    
    btree_gc_coalesce func details as follows:
            if alloc new_nodes[i] fails:
                    goto out_nocoalesce;
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock)
            // main coalescing process
            for (i = nodes - 1; i > 0; --i)
                    [snipped]
                    if coalescing process fails:
                            // Here, directly goto out_nocoalesce
                             // tag will cause a deadlock
                            goto out_nocoalesce;
                    [snipped]
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock)
            // coalesing succ, return
            return;
    out_nocoalesce:
            btree_node_free(new_nodes[i])   // free new_nodes[i]
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock);
            // set flag for reuse
            clear_bit(BTREE_NODE_dirty, &ew_nodes[i]->flags);
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock);
    
    To fix the problem, we add a new tag 'out_unlock_nocoalesce' for
    releasing new_nodes[i]->write_lock before out_nocoalesce tag. If
    coalescing process fails, we will go to out_unlock_nocoalesce tag
    for releasing new_nodes[i]->write_lock before free new_nodes[i] in
    out_nocoalesce tag.
    
    (Coly Li helps to clean up commit log format.)
    
    Fixes: 2a285686c109816 ("bcache: btree locking rework")
    Signed-off-by: Zhiqiang Liu <liuzhiqiang26@huawei.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 0dfbd5ecf28cbcb81674c49d34ee97366db1be44
Author: Michal Kalderon <michal.kalderon@marvell.com>
Date:   Tue Jun 16 12:34:08 2020 +0300

    RDMA/qedr: Fix KASAN: use-after-free in ucma_event_handler+0x532
    
    Private data passed to iwarp_cm_handler is copied for connection request /
    response, but ignored otherwise.  If junk is passed, it is stored in the
    event and used later in the event processing.
    
    The driver passes an old junk pointer during connection close which leads
    to a use-after-free on event processing.  Set private data to NULL for
    events that don 't have private data.
    
      BUG: KASAN: use-after-free in ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: Read of size 4 at addr ffff8886caa71200 by task kworker/u128:1/5250
      kernel:
      kernel: Workqueue: iw_cm_wq cm_work_handler [iw_cm]
      kernel: Call Trace:
      kernel: dump_stack+0x8c/0xc0
      kernel: print_address_description.constprop.0+0x1b/0x210
      kernel: ? ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: ? ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: __kasan_report.cold+0x1a/0x33
      kernel: ? ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: kasan_report+0xe/0x20
      kernel: check_memory_region+0x130/0x1a0
      kernel: memcpy+0x20/0x50
      kernel: ucma_event_handler+0x532/0x560 [rdma_ucm]
      kernel: ? __rpc_execute+0x608/0x620 [sunrpc]
      kernel: cma_iw_handler+0x212/0x330 [rdma_cm]
      kernel: ? iw_conn_req_handler+0x6e0/0x6e0 [rdma_cm]
      kernel: ? enqueue_timer+0x86/0x140
      kernel: ? _raw_write_lock_irq+0xd0/0xd0
      kernel: cm_work_handler+0xd3d/0x1070 [iw_cm]
    
    Fixes: e411e0587e0d ("RDMA/qedr: Add iWARP connection management functions")
    Link: https://lore.kernel.org/r/20200616093408.17827-1-michal.kalderon@marvell.com
    Signed-off-by: Ariel Elior <ariel.elior@marvell.com>
    Signed-off-by: Michal Kalderon <michal.kalderon@marvell.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit be23e837333a914df3f24bf0b32e87b0331ab8d1
Author: Zhiqiang Liu <liuzhiqiang26@huawei.com>
Date:   Mon Jun 15 00:53:30 2020 +0800

    bcache: fix potential deadlock problem in btree_gc_coalesce
    
    coccicheck reports:
      drivers/md//bcache/btree.c:1538:1-7: preceding lock on line 1417
    
    In btree_gc_coalesce func, if the coalescing process fails, we will goto
    to out_nocoalesce tag directly without releasing new_nodes[i]->write_lock.
    Then, it will cause a deadlock when trying to acquire new_nodes[i]->
    write_lock for freeing new_nodes[i] before return.
    
    btree_gc_coalesce func details as follows:
            if alloc new_nodes[i] fails:
                    goto out_nocoalesce;
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock)
            // main coalescing process
            for (i = nodes - 1; i > 0; --i)
                    [snipped]
                    if coalescing process fails:
                            // Here, directly goto out_nocoalesce
                             // tag will cause a deadlock
                            goto out_nocoalesce;
                    [snipped]
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock)
            // coalesing succ, return
            return;
    out_nocoalesce:
            btree_node_free(new_nodes[i])   // free new_nodes[i]
            // obtain new_nodes[i]->write_lock
            mutex_lock(&new_nodes[i]->write_lock);
            // set flag for reuse
            clear_bit(BTREE_NODE_dirty, &ew_nodes[i]->flags);
            // release new_nodes[i]->write_lock
            mutex_unlock(&new_nodes[i]->write_lock);
    
    To fix the problem, we add a new tag 'out_unlock_nocoalesce' for
    releasing new_nodes[i]->write_lock before out_nocoalesce tag. If
    coalescing process fails, we will go to out_unlock_nocoalesce tag
    for releasing new_nodes[i]->write_lock before free new_nodes[i] in
    out_nocoalesce tag.
    
    (Coly Li helps to clean up commit log format.)
    
    Fixes: 2a285686c109816 ("bcache: btree locking rework")
    Signed-off-by: Zhiqiang Liu <liuzhiqiang26@huawei.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 42fc541404f249778e752ab39c8bc25fcb2dbe1e
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:44 2020 -0700

    mmap locking API: add mmap_assert_locked() and mmap_assert_write_locked()
    
    Add new APIs to assert that mmap_sem is held.
    
    Using this instead of rwsem_is_locked and lockdep_assert_held[_write]
    makes the assertions more tolerant of future changes to the lock type.
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Laurent Dufour <ldufour@linux.ibm.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-10-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 5b384f933590a086ca9a0abdc2e55e41107ac440
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Wed Apr 29 10:24:52 2020 -0500

    x86/module: Use text_mutex in apply_relocate_add()
    
    Now that the livepatch code no longer needs the text_mutex for changing
    module permissions, move its usage down to apply_relocate_add().
    
    Note the s390 version of apply_relocate_add() doesn't need to use the
    text_mutex because it already uses s390_kernel_write_lock, which
    accomplishes the same task.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Joe Lawrence <joe.lawrence@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

commit 3c32e1a8b7ea89ca1831d883cd6b9df447f42be7
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Dec 12 10:32:13 2019 -0800

    6pack,mkiss: fix possible deadlock
    
    commit 5c9934b6767b16ba60be22ec3cbd4379ad64170d upstream.
    
    We got another syzbot report [1] that tells us we must use
    write_lock_irq()/write_unlock_irq() to avoid possible deadlock.
    
    [1]
    
    WARNING: inconsistent lock state
    5.5.0-rc1-syzkaller #0 Not tainted
    --------------------------------
    inconsistent {HARDIRQ-ON-W} -> {IN-HARDIRQ-R} usage.
    syz-executor826/9605 [HC1[1]:SC0[0]:HE0:SE1] takes:
    ffffffff8a128718 (disc_data_lock){+-..}, at: sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
    {HARDIRQ-ON-W} state was registered at:
      lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
      __raw_write_lock_bh include/linux/rwlock_api_smp.h:203 [inline]
      _raw_write_lock_bh+0x33/0x50 kernel/locking/spinlock.c:319
      sixpack_close+0x1d/0x250 drivers/net/hamradio/6pack.c:657
      tty_ldisc_close.isra.0+0x119/0x1a0 drivers/tty/tty_ldisc.c:489
      tty_set_ldisc+0x230/0x6b0 drivers/tty/tty_ldisc.c:585
      tiocsetd drivers/tty/tty_io.c:2337 [inline]
      tty_ioctl+0xe8d/0x14f0 drivers/tty/tty_io.c:2597
      vfs_ioctl fs/ioctl.c:47 [inline]
      file_ioctl fs/ioctl.c:545 [inline]
      do_vfs_ioctl+0x977/0x14e0 fs/ioctl.c:732
      ksys_ioctl+0xab/0xd0 fs/ioctl.c:749
      __do_sys_ioctl fs/ioctl.c:756 [inline]
      __se_sys_ioctl fs/ioctl.c:754 [inline]
      __x64_sys_ioctl+0x73/0xb0 fs/ioctl.c:754
      do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
      entry_SYSCALL_64_after_hwframe+0x49/0xbe
    irq event stamp: 3946
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] __raw_spin_unlock_irq include/linux/spinlock_api_smp.h:168 [inline]
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] _raw_spin_unlock_irq+0x23/0x80 kernel/locking/spinlock.c:199
    hardirqs last disabled at (3946): [<ffffffff8100675f>] trace_hardirqs_off_thunk+0x1a/0x1c arch/x86/entry/thunk_64.S:42
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] spin_unlock_bh include/linux/spinlock.h:383 [inline]
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] clusterip_netdev_event+0x46f/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:222
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] spin_lock_bh include/linux/spinlock.h:343 [inline]
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] clusterip_netdev_event+0x1bb/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:196
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(disc_data_lock);
      <Interrupt>
        lock(disc_data_lock);
    
     *** DEADLOCK ***
    
    5 locks held by syz-executor826/9605:
     #0: ffff8880a905e198 (&tty->legacy_mutex){+.+.}, at: tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     #1: ffffffff899a56c0 (rcu_read_lock){....}, at: mutex_spin_on_owner+0x0/0x330 kernel/locking/mutex.c:413
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: spin_lock include/linux/spinlock.h:338 [inline]
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: serial8250_interrupt+0x2d/0x1a0 drivers/tty/serial/8250/8250_core.c:116
     #3: ffffffff8c104048 (&port_lock_key){-.-.}, at: serial8250_handle_irq.part.0+0x24/0x330 drivers/tty/serial/8250/8250_port.c:1823
     #4: ffff8880a905e090 (&tty->ldisc_sem){++++}, at: tty_ldisc_ref+0x22/0x90 drivers/tty/tty_ldisc.c:288
    
    stack backtrace:
    CPU: 1 PID: 9605 Comm: syz-executor826 Not tainted 5.5.0-rc1-syzkaller #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x197/0x210 lib/dump_stack.c:118
     print_usage_bug.cold+0x327/0x378 kernel/locking/lockdep.c:3101
     valid_state kernel/locking/lockdep.c:3112 [inline]
     mark_lock_irq kernel/locking/lockdep.c:3309 [inline]
     mark_lock+0xbb4/0x1220 kernel/locking/lockdep.c:3666
     mark_usage kernel/locking/lockdep.c:3554 [inline]
     __lock_acquire+0x1e55/0x4a00 kernel/locking/lockdep.c:3909
     lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
     __raw_read_lock include/linux/rwlock_api_smp.h:149 [inline]
     _raw_read_lock+0x32/0x50 kernel/locking/spinlock.c:223
     sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
     sixpack_write_wakeup+0x25/0x340 drivers/net/hamradio/6pack.c:402
     tty_wakeup+0xe9/0x120 drivers/tty/tty_io.c:536
     tty_port_default_wakeup+0x2b/0x40 drivers/tty/tty_port.c:50
     tty_port_tty_wakeup+0x57/0x70 drivers/tty/tty_port.c:387
     uart_write_wakeup+0x46/0x70 drivers/tty/serial/serial_core.c:104
     serial8250_tx_chars+0x495/0xaf0 drivers/tty/serial/8250/8250_port.c:1761
     serial8250_handle_irq.part.0+0x2a2/0x330 drivers/tty/serial/8250/8250_port.c:1834
     serial8250_handle_irq drivers/tty/serial/8250/8250_port.c:1820 [inline]
     serial8250_default_handle_irq+0xc0/0x150 drivers/tty/serial/8250/8250_port.c:1850
     serial8250_interrupt+0xf1/0x1a0 drivers/tty/serial/8250/8250_core.c:126
     __handle_irq_event_percpu+0x15d/0x970 kernel/irq/handle.c:149
     handle_irq_event_percpu+0x74/0x160 kernel/irq/handle.c:189
     handle_irq_event+0xa7/0x134 kernel/irq/handle.c:206
     handle_edge_irq+0x25e/0x8d0 kernel/irq/chip.c:830
     generic_handle_irq_desc include/linux/irqdesc.h:156 [inline]
     do_IRQ+0xde/0x280 arch/x86/kernel/irq.c:250
     common_interrupt+0xf/0xf arch/x86/entry/entry_64.S:607
     </IRQ>
    RIP: 0010:cpu_relax arch/x86/include/asm/processor.h:685 [inline]
    RIP: 0010:mutex_spin_on_owner+0x247/0x330 kernel/locking/mutex.c:579
    Code: c3 be 08 00 00 00 4c 89 e7 e8 e5 06 59 00 4c 89 e0 48 c1 e8 03 42 80 3c 38 00 0f 85 e1 00 00 00 49 8b 04 24 a8 01 75 96 f3 90 <e9> 2f fe ff ff 0f 0b e8 0d 19 09 00 84 c0 0f 85 ff fd ff ff 48 c7
    RSP: 0018:ffffc90001eafa20 EFLAGS: 00000246 ORIG_RAX: ffffffffffffffd7
    RAX: 0000000000000000 RBX: ffff88809fd9e0c0 RCX: 1ffffffff13266dd
    RDX: 0000000000000000 RSI: 0000000000000008 RDI: 0000000000000000
    RBP: ffffc90001eafa60 R08: 1ffff11013d22898 R09: ffffed1013d22899
    R10: ffffed1013d22898 R11: ffff88809e9144c7 R12: ffff8880a905e138
    R13: ffff88809e9144c0 R14: 0000000000000000 R15: dffffc0000000000
     mutex_optimistic_spin kernel/locking/mutex.c:673 [inline]
     __mutex_lock_common kernel/locking/mutex.c:962 [inline]
     __mutex_lock+0x32b/0x13c0 kernel/locking/mutex.c:1106
     mutex_lock_nested+0x16/0x20 kernel/locking/mutex.c:1121
     tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     tty_release+0xb5/0xe90 drivers/tty/tty_io.c:1665
     __fput+0x2ff/0x890 fs/file_table.c:280
     ____fput+0x16/0x20 fs/file_table.c:313
     task_work_run+0x145/0x1c0 kernel/task_work.c:113
     exit_task_work include/linux/task_work.h:22 [inline]
     do_exit+0x8e7/0x2ef0 kernel/exit.c:797
     do_group_exit+0x135/0x360 kernel/exit.c:895
     __do_sys_exit_group kernel/exit.c:906 [inline]
     __se_sys_exit_group kernel/exit.c:904 [inline]
     __x64_sys_exit_group+0x44/0x50 kernel/exit.c:904
     do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x43fef8
    Code: Bad RIP value.
    RSP: 002b:00007ffdb07d2338 EFLAGS: 00000246 ORIG_RAX: 00000000000000e7
    RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 000000000043fef8
    RDX: 0000000000000000 RSI: 000000000000003c RDI: 0000000000000000
    RBP: 00000000004bf730 R08: 00000000000000e7 R09: ffffffffffffffd0
    R10: 00000000004002c8 R11: 0000000000000246 R12: 0000000000000001
    R13: 00000000006d1180 R14: 0000000000000000 R15: 0000000000000000
    
    Fixes: 6e4e2f811bad ("6pack,mkiss: fix lock inconsistency")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 62bcfd5e5316f81e6c3f273443df4d89ee6d3e78
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Dec 11 09:01:40 2019 +0000

    Btrfs: fix infinite loop during nocow writeback due to race
    
    commit de7999afedff02c6631feab3ea726a0e8f8c3d40 upstream.
    
    When starting writeback for a range that covers part of a preallocated
    extent, due to a race with writeback for another range that also covers
    another part of the same preallocated extent, we can end up in an infinite
    loop.
    
    Consider the following example where for inode 280 we have two dirty
    ranges:
    
      range A, from 294912 to 303103, 8192 bytes
      range B, from 348160 to 438271, 90112 bytes
    
    and we have the following file extent item layout for our inode:
    
      leaf 38895616 gen 24544 total ptrs 29 free space 13820 owner 5
          (...)
          item 27 key (280 108 200704) itemoff 14598 itemsize 53
              extent data disk bytenr 0 nr 0 type 1 (regular)
              extent data offset 0 nr 94208 ram 94208
          item 28 key (280 108 294912) itemoff 14545 itemsize 53
              extent data disk bytenr 10433052672 nr 81920 type 2 (prealloc)
              extent data offset 0 nr 81920 ram 81920
    
    Then the following happens:
    
    1) Writeback starts for range B (from 348160 to 438271), execution of
       run_delalloc_nocow() starts;
    
    2) The first iteration of run_delalloc_nocow()'s whil loop leaves us at
       the extent item at slot 28, pointing to the prealloc extent item
       covering the range from 294912 to 376831. This extent covers part of
       our range;
    
    3) An ordered extent is created against that extent, covering the file
       range from 348160 to 376831 (28672 bytes);
    
    4) We adjust 'cur_offset' to 376832 and move on to the next iteration of
       the while loop;
    
    5) The call to btrfs_lookup_file_extent() leaves us at the same leaf,
       pointing to slot 29, 1 slot after the last item (the extent item
       we processed in the previous iteration);
    
    6) Because we are a slot beyond the last item, we call btrfs_next_leaf(),
       which releases the search path before doing a another search for the
       last key of the leaf (280 108 294912);
    
    7) Right after btrfs_next_leaf() released the path, and before it did
       another search for the last key of the leaf, writeback for the range
       A (from 294912 to 303103) completes (it was previously started at
       some point);
    
    8) Upon completion of the ordered extent for range A, the prealloc extent
       we previously found got split into two extent items, one covering the
       range from 294912 to 303103 (8192 bytes), with a type of regular extent
       (and no longer prealloc) and another covering the range from 303104 to
       376831 (73728 bytes), with a type of prealloc and an offset of 8192
       bytes. So our leaf now has the following layout:
    
         leaf 38895616 gen 24544 total ptrs 31 free space 13664 owner 5
             (...)
             item 27 key (280 108 200704) itemoff 14598 itemsize 53
                 extent data disk bytenr 0 nr 0 type 1
                 extent data offset 0 nr 8192 ram 94208
             item 28 key (280 108 208896) itemoff 14545 itemsize 53
                 extent data disk bytenr 10433142784 nr 86016 type 1
                 extent data offset 0 nr 86016 ram 86016
             item 29 key (280 108 294912) itemoff 14492 itemsize 53
                 extent data disk bytenr 10433052672 nr 81920 type 1
                 extent data offset 0 nr 8192 ram 81920
             item 30 key (280 108 303104) itemoff 14439 itemsize 53
                 extent data disk bytenr 10433052672 nr 81920 type 2
                 extent data offset 8192 nr 73728 ram 81920
    
    9) After btrfs_next_leaf() returns, we have our path pointing to that same
       leaf and at slot 30, since it has a key we didn't have before and it's
       the first key greater then the key that was previously the last key of
       the leaf (key (280 108 294912));
    
    10) The extent item at slot 30 covers the range from 303104 to 376831
        which is in our target range, so we process it, despite having already
        created an ordered extent against this extent for the file range from
        348160 to 376831. This is because we skip to the next extent item only
        if its end is less than or equals to the start of our delalloc range,
        and not less than or equals to the current offset ('cur_offset');
    
    11) As a result we compute 'num_bytes' as:
    
        num_bytes = min(end + 1, extent_end) - cur_offset;
                  = min(438271 + 1, 376832) - 376832 = 0
    
    12) We then call create_io_em() for a 0 bytes range starting at offset
        376832;
    
    13) Then create_io_em() enters an infinite loop because its calls to
        btrfs_drop_extent_cache() do nothing due to the 0 length range
        passed to it. So no existing extent maps that cover the offset
        376832 get removed, and therefore calls to add_extent_mapping()
        return -EEXIST, resulting in an infinite loop. This loop from
        create_io_em() is the following:
    
        do {
            btrfs_drop_extent_cache(BTRFS_I(inode), em->start,
                                    em->start + em->len - 1, 0);
            write_lock(&em_tree->lock);
            ret = add_extent_mapping(em_tree, em, 1);
            write_unlock(&em_tree->lock);
            /*
             * The caller has taken lock_extent(), who could race with us
             * to add em?
             */
        } while (ret == -EEXIST);
    
    Also, each call to btrfs_drop_extent_cache() triggers a warning because
    the start offset passed to it (376832) is smaller then the end offset
    (376832 - 1) passed to it by -1, due to the 0 length:
    
      [258532.052621] ------------[ cut here ]------------
      [258532.052643] WARNING: CPU: 0 PID: 9987 at fs/btrfs/file.c:602 btrfs_drop_extent_cache+0x3f4/0x590 [btrfs]
      (...)
      [258532.052672] CPU: 0 PID: 9987 Comm: fsx Tainted: G        W         5.4.0-rc7-btrfs-next-64 #1
      [258532.052673] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.0-0-ga698c8995f-prebuilt.qemu.org 04/01/2014
      [258532.052691] RIP: 0010:btrfs_drop_extent_cache+0x3f4/0x590 [btrfs]
      (...)
      [258532.052695] RSP: 0018:ffffb4be0153f860 EFLAGS: 00010287
      [258532.052700] RAX: ffff975b445ee360 RBX: ffff975b44eb3e08 RCX: 0000000000000000
      [258532.052700] RDX: 0000000000038fff RSI: 0000000000039000 RDI: ffff975b445ee308
      [258532.052700] RBP: 0000000000038fff R08: 0000000000000000 R09: 0000000000000001
      [258532.052701] R10: ffff975b513c5c10 R11: 00000000e3c0cfa9 R12: 0000000000039000
      [258532.052703] R13: ffff975b445ee360 R14: 00000000ffffffef R15: ffff975b445ee308
      [258532.052705] FS:  00007f86a821de80(0000) GS:ffff975b76a00000(0000) knlGS:0000000000000000
      [258532.052707] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      [258532.052708] CR2: 00007fdacf0f3ab4 CR3: 00000001f9d26002 CR4: 00000000003606f0
      [258532.052712] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      [258532.052717] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      [258532.052717] Call Trace:
      [258532.052718]  ? preempt_schedule_common+0x32/0x70
      [258532.052722]  ? ___preempt_schedule+0x16/0x20
      [258532.052741]  create_io_em+0xff/0x180 [btrfs]
      [258532.052767]  run_delalloc_nocow+0x942/0xb10 [btrfs]
      [258532.052791]  btrfs_run_delalloc_range+0x30b/0x520 [btrfs]
      [258532.052812]  ? find_lock_delalloc_range+0x221/0x250 [btrfs]
      [258532.052834]  writepage_delalloc+0xe4/0x140 [btrfs]
      [258532.052855]  __extent_writepage+0x110/0x4e0 [btrfs]
      [258532.052876]  extent_write_cache_pages+0x21c/0x480 [btrfs]
      [258532.052906]  extent_writepages+0x52/0xb0 [btrfs]
      [258532.052911]  do_writepages+0x23/0x80
      [258532.052915]  __filemap_fdatawrite_range+0xd2/0x110
      [258532.052938]  btrfs_fdatawrite_range+0x1b/0x50 [btrfs]
      [258532.052954]  start_ordered_ops+0x57/0xa0 [btrfs]
      [258532.052973]  ? btrfs_sync_file+0x225/0x490 [btrfs]
      [258532.052988]  btrfs_sync_file+0x225/0x490 [btrfs]
      [258532.052997]  __x64_sys_msync+0x199/0x200
      [258532.053004]  do_syscall_64+0x5c/0x250
      [258532.053007]  entry_SYSCALL_64_after_hwframe+0x49/0xbe
      [258532.053010] RIP: 0033:0x7f86a7dfd760
      (...)
      [258532.053014] RSP: 002b:00007ffd99af0368 EFLAGS: 00000246 ORIG_RAX: 000000000000001a
      [258532.053016] RAX: ffffffffffffffda RBX: 0000000000000ec9 RCX: 00007f86a7dfd760
      [258532.053017] RDX: 0000000000000004 RSI: 000000000000836c RDI: 00007f86a8221000
      [258532.053019] RBP: 0000000000021ec9 R08: 0000000000000003 R09: 00007f86a812037c
      [258532.053020] R10: 0000000000000001 R11: 0000000000000246 R12: 00000000000074a3
      [258532.053021] R13: 00007f86a8221000 R14: 000000000000836c R15: 0000000000000001
      [258532.053032] irq event stamp: 1653450494
      [258532.053035] hardirqs last  enabled at (1653450493): [<ffffffff9dec69f9>] _raw_spin_unlock_irq+0x29/0x50
      [258532.053037] hardirqs last disabled at (1653450494): [<ffffffff9d4048ea>] trace_hardirqs_off_thunk+0x1a/0x20
      [258532.053039] softirqs last  enabled at (1653449852): [<ffffffff9e200466>] __do_softirq+0x466/0x6bd
      [258532.053042] softirqs last disabled at (1653449845): [<ffffffff9d4c8a0c>] irq_exit+0xec/0x120
      [258532.053043] ---[ end trace 8476fce13d9ce20a ]---
    
    Which results in flooding dmesg/syslog since btrfs_drop_extent_cache()
    uses WARN_ON() and not WARN_ON_ONCE().
    
    So fix this issue by changing run_delalloc_nocow()'s loop to move to the
    next extent item when the current extent item ends at at offset less than
    or equals to the current offset instead of the start offset.
    
    Fixes: 80ff385665b7fc ("Btrfs: update nodatacow code v2")
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    [bwh: Backported to 3.16: adjust context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit cce0d1bd3fbae5595b16a7b9e4b07b6e7c525e50
Author: Taras Chornyi <taras.chornyi@plvision.eu>
Date:   Thu Apr 9 20:25:24 2020 +0300

    net: ipv4: devinet: Fix crash when add/del multicast IP with autojoin
    
    [ Upstream commit 690cc86321eb9bcee371710252742fb16fe96824 ]
    
    When CONFIG_IP_MULTICAST is not set and multicast ip is added to the device
    with autojoin flag or when multicast ip is deleted kernel will crash.
    
    steps to reproduce:
    
    ip addr add 224.0.0.0/32 dev eth0
    ip addr del 224.0.0.0/32 dev eth0
    
    or
    
    ip addr add 224.0.0.0/32 dev eth0 autojoin
    
    Unable to handle kernel NULL pointer dereference at virtual address 0000000000000088
     pc : _raw_write_lock_irqsave+0x1e0/0x2ac
     lr : lock_sock_nested+0x1c/0x60
     Call trace:
      _raw_write_lock_irqsave+0x1e0/0x2ac
      lock_sock_nested+0x1c/0x60
      ip_mc_config.isra.28+0x50/0xe0
      inet_rtm_deladdr+0x1a8/0x1f0
      rtnetlink_rcv_msg+0x120/0x350
      netlink_rcv_skb+0x58/0x120
      rtnetlink_rcv+0x14/0x20
      netlink_unicast+0x1b8/0x270
      netlink_sendmsg+0x1a0/0x3b0
      ____sys_sendmsg+0x248/0x290
      ___sys_sendmsg+0x80/0xc0
      __sys_sendmsg+0x68/0xc0
      __arm64_sys_sendmsg+0x20/0x30
      el0_svc_common.constprop.2+0x88/0x150
      do_el0_svc+0x20/0x80
     el0_sync_handler+0x118/0x190
      el0_sync+0x140/0x180
    
    Fixes: 93a714d6b53d ("multicast: Extend ip address command to enable multicast group join/leave on")
    Signed-off-by: Taras Chornyi <taras.chornyi@plvision.eu>
    Signed-off-by: Vadym Kochan <vadym.kochan@plvision.eu>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1cd63ccd59f90dada407d19a3214404a63cc3140
Author: Taras Chornyi <taras.chornyi@plvision.eu>
Date:   Thu Apr 9 20:25:24 2020 +0300

    net: ipv4: devinet: Fix crash when add/del multicast IP with autojoin
    
    [ Upstream commit 690cc86321eb9bcee371710252742fb16fe96824 ]
    
    When CONFIG_IP_MULTICAST is not set and multicast ip is added to the device
    with autojoin flag or when multicast ip is deleted kernel will crash.
    
    steps to reproduce:
    
    ip addr add 224.0.0.0/32 dev eth0
    ip addr del 224.0.0.0/32 dev eth0
    
    or
    
    ip addr add 224.0.0.0/32 dev eth0 autojoin
    
    Unable to handle kernel NULL pointer dereference at virtual address 0000000000000088
     pc : _raw_write_lock_irqsave+0x1e0/0x2ac
     lr : lock_sock_nested+0x1c/0x60
     Call trace:
      _raw_write_lock_irqsave+0x1e0/0x2ac
      lock_sock_nested+0x1c/0x60
      ip_mc_config.isra.28+0x50/0xe0
      inet_rtm_deladdr+0x1a8/0x1f0
      rtnetlink_rcv_msg+0x120/0x350
      netlink_rcv_skb+0x58/0x120
      rtnetlink_rcv+0x14/0x20
      netlink_unicast+0x1b8/0x270
      netlink_sendmsg+0x1a0/0x3b0
      ____sys_sendmsg+0x248/0x290
      ___sys_sendmsg+0x80/0xc0
      __sys_sendmsg+0x68/0xc0
      __arm64_sys_sendmsg+0x20/0x30
      el0_svc_common.constprop.2+0x88/0x150
      do_el0_svc+0x20/0x80
     el0_sync_handler+0x118/0x190
      el0_sync+0x140/0x180
    
    Fixes: 93a714d6b53d ("multicast: Extend ip address command to enable multicast group join/leave on")
    Signed-off-by: Taras Chornyi <taras.chornyi@plvision.eu>
    Signed-off-by: Vadym Kochan <vadym.kochan@plvision.eu>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 773b99a0872dcc3f5d57d6e9c60547a14dfaeaf5
Author: Taras Chornyi <taras.chornyi@plvision.eu>
Date:   Thu Apr 9 20:25:24 2020 +0300

    net: ipv4: devinet: Fix crash when add/del multicast IP with autojoin
    
    [ Upstream commit 690cc86321eb9bcee371710252742fb16fe96824 ]
    
    When CONFIG_IP_MULTICAST is not set and multicast ip is added to the device
    with autojoin flag or when multicast ip is deleted kernel will crash.
    
    steps to reproduce:
    
    ip addr add 224.0.0.0/32 dev eth0
    ip addr del 224.0.0.0/32 dev eth0
    
    or
    
    ip addr add 224.0.0.0/32 dev eth0 autojoin
    
    Unable to handle kernel NULL pointer dereference at virtual address 0000000000000088
     pc : _raw_write_lock_irqsave+0x1e0/0x2ac
     lr : lock_sock_nested+0x1c/0x60
     Call trace:
      _raw_write_lock_irqsave+0x1e0/0x2ac
      lock_sock_nested+0x1c/0x60
      ip_mc_config.isra.28+0x50/0xe0
      inet_rtm_deladdr+0x1a8/0x1f0
      rtnetlink_rcv_msg+0x120/0x350
      netlink_rcv_skb+0x58/0x120
      rtnetlink_rcv+0x14/0x20
      netlink_unicast+0x1b8/0x270
      netlink_sendmsg+0x1a0/0x3b0
      ____sys_sendmsg+0x248/0x290
      ___sys_sendmsg+0x80/0xc0
      __sys_sendmsg+0x68/0xc0
      __arm64_sys_sendmsg+0x20/0x30
      el0_svc_common.constprop.2+0x88/0x150
      do_el0_svc+0x20/0x80
     el0_sync_handler+0x118/0x190
      el0_sync+0x140/0x180
    
    Fixes: 93a714d6b53d ("multicast: Extend ip address command to enable multicast group join/leave on")
    Signed-off-by: Taras Chornyi <taras.chornyi@plvision.eu>
    Signed-off-by: Vadym Kochan <vadym.kochan@plvision.eu>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d1c66b1464bbc963a0c4a989c9fbde320f73a67b
Author: Taras Chornyi <taras.chornyi@plvision.eu>
Date:   Thu Apr 9 20:25:24 2020 +0300

    net: ipv4: devinet: Fix crash when add/del multicast IP with autojoin
    
    [ Upstream commit 690cc86321eb9bcee371710252742fb16fe96824 ]
    
    When CONFIG_IP_MULTICAST is not set and multicast ip is added to the device
    with autojoin flag or when multicast ip is deleted kernel will crash.
    
    steps to reproduce:
    
    ip addr add 224.0.0.0/32 dev eth0
    ip addr del 224.0.0.0/32 dev eth0
    
    or
    
    ip addr add 224.0.0.0/32 dev eth0 autojoin
    
    Unable to handle kernel NULL pointer dereference at virtual address 0000000000000088
     pc : _raw_write_lock_irqsave+0x1e0/0x2ac
     lr : lock_sock_nested+0x1c/0x60
     Call trace:
      _raw_write_lock_irqsave+0x1e0/0x2ac
      lock_sock_nested+0x1c/0x60
      ip_mc_config.isra.28+0x50/0xe0
      inet_rtm_deladdr+0x1a8/0x1f0
      rtnetlink_rcv_msg+0x120/0x350
      netlink_rcv_skb+0x58/0x120
      rtnetlink_rcv+0x14/0x20
      netlink_unicast+0x1b8/0x270
      netlink_sendmsg+0x1a0/0x3b0
      ____sys_sendmsg+0x248/0x290
      ___sys_sendmsg+0x80/0xc0
      __sys_sendmsg+0x68/0xc0
      __arm64_sys_sendmsg+0x20/0x30
      el0_svc_common.constprop.2+0x88/0x150
      do_el0_svc+0x20/0x80
     el0_sync_handler+0x118/0x190
      el0_sync+0x140/0x180
    
    Fixes: 93a714d6b53d ("multicast: Extend ip address command to enable multicast group join/leave on")
    Signed-off-by: Taras Chornyi <taras.chornyi@plvision.eu>
    Signed-off-by: Vadym Kochan <vadym.kochan@plvision.eu>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c8ac9a7c523723b5f20c75fd90997c892fcd036e
Author: Taras Chornyi <taras.chornyi@plvision.eu>
Date:   Thu Apr 9 20:25:24 2020 +0300

    net: ipv4: devinet: Fix crash when add/del multicast IP with autojoin
    
    [ Upstream commit 690cc86321eb9bcee371710252742fb16fe96824 ]
    
    When CONFIG_IP_MULTICAST is not set and multicast ip is added to the device
    with autojoin flag or when multicast ip is deleted kernel will crash.
    
    steps to reproduce:
    
    ip addr add 224.0.0.0/32 dev eth0
    ip addr del 224.0.0.0/32 dev eth0
    
    or
    
    ip addr add 224.0.0.0/32 dev eth0 autojoin
    
    Unable to handle kernel NULL pointer dereference at virtual address 0000000000000088
     pc : _raw_write_lock_irqsave+0x1e0/0x2ac
     lr : lock_sock_nested+0x1c/0x60
     Call trace:
      _raw_write_lock_irqsave+0x1e0/0x2ac
      lock_sock_nested+0x1c/0x60
      ip_mc_config.isra.28+0x50/0xe0
      inet_rtm_deladdr+0x1a8/0x1f0
      rtnetlink_rcv_msg+0x120/0x350
      netlink_rcv_skb+0x58/0x120
      rtnetlink_rcv+0x14/0x20
      netlink_unicast+0x1b8/0x270
      netlink_sendmsg+0x1a0/0x3b0
      ____sys_sendmsg+0x248/0x290
      ___sys_sendmsg+0x80/0xc0
      __sys_sendmsg+0x68/0xc0
      __arm64_sys_sendmsg+0x20/0x30
      el0_svc_common.constprop.2+0x88/0x150
      do_el0_svc+0x20/0x80
     el0_sync_handler+0x118/0x190
      el0_sync+0x140/0x180
    
    Fixes: 93a714d6b53d ("multicast: Extend ip address command to enable multicast group join/leave on")
    Signed-off-by: Taras Chornyi <taras.chornyi@plvision.eu>
    Signed-off-by: Vadym Kochan <vadym.kochan@plvision.eu>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 22e56cb2f9519dec5ceae24b216b3708baa96a51
Author: Taras Chornyi <taras.chornyi@plvision.eu>
Date:   Thu Apr 9 20:25:24 2020 +0300

    net: ipv4: devinet: Fix crash when add/del multicast IP with autojoin
    
    [ Upstream commit 690cc86321eb9bcee371710252742fb16fe96824 ]
    
    When CONFIG_IP_MULTICAST is not set and multicast ip is added to the device
    with autojoin flag or when multicast ip is deleted kernel will crash.
    
    steps to reproduce:
    
    ip addr add 224.0.0.0/32 dev eth0
    ip addr del 224.0.0.0/32 dev eth0
    
    or
    
    ip addr add 224.0.0.0/32 dev eth0 autojoin
    
    Unable to handle kernel NULL pointer dereference at virtual address 0000000000000088
     pc : _raw_write_lock_irqsave+0x1e0/0x2ac
     lr : lock_sock_nested+0x1c/0x60
     Call trace:
      _raw_write_lock_irqsave+0x1e0/0x2ac
      lock_sock_nested+0x1c/0x60
      ip_mc_config.isra.28+0x50/0xe0
      inet_rtm_deladdr+0x1a8/0x1f0
      rtnetlink_rcv_msg+0x120/0x350
      netlink_rcv_skb+0x58/0x120
      rtnetlink_rcv+0x14/0x20
      netlink_unicast+0x1b8/0x270
      netlink_sendmsg+0x1a0/0x3b0
      ____sys_sendmsg+0x248/0x290
      ___sys_sendmsg+0x80/0xc0
      __sys_sendmsg+0x68/0xc0
      __arm64_sys_sendmsg+0x20/0x30
      el0_svc_common.constprop.2+0x88/0x150
      do_el0_svc+0x20/0x80
     el0_sync_handler+0x118/0x190
      el0_sync+0x140/0x180
    
    Fixes: 93a714d6b53d ("multicast: Extend ip address command to enable multicast group join/leave on")
    Signed-off-by: Taras Chornyi <taras.chornyi@plvision.eu>
    Signed-off-by: Vadym Kochan <vadym.kochan@plvision.eu>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 80dd8146df680b8982b659341b8ecd3361f032ca
Author: Taras Chornyi <taras.chornyi@plvision.eu>
Date:   Thu Apr 9 20:25:24 2020 +0300

    net: ipv4: devinet: Fix crash when add/del multicast IP with autojoin
    
    [ Upstream commit 690cc86321eb9bcee371710252742fb16fe96824 ]
    
    When CONFIG_IP_MULTICAST is not set and multicast ip is added to the device
    with autojoin flag or when multicast ip is deleted kernel will crash.
    
    steps to reproduce:
    
    ip addr add 224.0.0.0/32 dev eth0
    ip addr del 224.0.0.0/32 dev eth0
    
    or
    
    ip addr add 224.0.0.0/32 dev eth0 autojoin
    
    Unable to handle kernel NULL pointer dereference at virtual address 0000000000000088
     pc : _raw_write_lock_irqsave+0x1e0/0x2ac
     lr : lock_sock_nested+0x1c/0x60
     Call trace:
      _raw_write_lock_irqsave+0x1e0/0x2ac
      lock_sock_nested+0x1c/0x60
      ip_mc_config.isra.28+0x50/0xe0
      inet_rtm_deladdr+0x1a8/0x1f0
      rtnetlink_rcv_msg+0x120/0x350
      netlink_rcv_skb+0x58/0x120
      rtnetlink_rcv+0x14/0x20
      netlink_unicast+0x1b8/0x270
      netlink_sendmsg+0x1a0/0x3b0
      ____sys_sendmsg+0x248/0x290
      ___sys_sendmsg+0x80/0xc0
      __sys_sendmsg+0x68/0xc0
      __arm64_sys_sendmsg+0x20/0x30
      el0_svc_common.constprop.2+0x88/0x150
      do_el0_svc+0x20/0x80
     el0_sync_handler+0x118/0x190
      el0_sync+0x140/0x180
    
    Fixes: 93a714d6b53d ("multicast: Extend ip address command to enable multicast group join/leave on")
    Signed-off-by: Taras Chornyi <taras.chornyi@plvision.eu>
    Signed-off-by: Vadym Kochan <vadym.kochan@plvision.eu>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 690cc86321eb9bcee371710252742fb16fe96824
Author: Taras Chornyi <taras.chornyi@plvision.eu>
Date:   Thu Apr 9 20:25:24 2020 +0300

    net: ipv4: devinet: Fix crash when add/del multicast IP with autojoin
    
    When CONFIG_IP_MULTICAST is not set and multicast ip is added to the device
    with autojoin flag or when multicast ip is deleted kernel will crash.
    
    steps to reproduce:
    
    ip addr add 224.0.0.0/32 dev eth0
    ip addr del 224.0.0.0/32 dev eth0
    
    or
    
    ip addr add 224.0.0.0/32 dev eth0 autojoin
    
    Unable to handle kernel NULL pointer dereference at virtual address 0000000000000088
     pc : _raw_write_lock_irqsave+0x1e0/0x2ac
     lr : lock_sock_nested+0x1c/0x60
     Call trace:
      _raw_write_lock_irqsave+0x1e0/0x2ac
      lock_sock_nested+0x1c/0x60
      ip_mc_config.isra.28+0x50/0xe0
      inet_rtm_deladdr+0x1a8/0x1f0
      rtnetlink_rcv_msg+0x120/0x350
      netlink_rcv_skb+0x58/0x120
      rtnetlink_rcv+0x14/0x20
      netlink_unicast+0x1b8/0x270
      netlink_sendmsg+0x1a0/0x3b0
      ____sys_sendmsg+0x248/0x290
      ___sys_sendmsg+0x80/0xc0
      __sys_sendmsg+0x68/0xc0
      __arm64_sys_sendmsg+0x20/0x30
      el0_svc_common.constprop.2+0x88/0x150
      do_el0_svc+0x20/0x80
     el0_sync_handler+0x118/0x190
      el0_sync+0x140/0x180
    
    Fixes: 93a714d6b53d ("multicast: Extend ip address command to enable multicast group join/leave on")
    Signed-off-by: Taras Chornyi <taras.chornyi@plvision.eu>
    Signed-off-by: Vadym Kochan <vadym.kochan@plvision.eu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 0ac1dd7bb8f1b40f1bf494f6a27235a7a3b36350
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Mon Mar 23 15:32:39 2020 +0800

    xfrm: policy: Fix doulbe free in xfrm_policy_timer
    
    commit 4c59406ed00379c8663f8663d82b2537467ce9d7 upstream.
    
    After xfrm_add_policy add a policy, its ref is 2, then
    
                                 xfrm_policy_timer
                                   read_lock
                                   xp->walk.dead is 0
                                   ....
                                   mod_timer()
    xfrm_policy_kill
      policy->walk.dead = 1
      ....
      del_timer(&policy->timer)
        xfrm_pol_put //ref is 1
      xfrm_pol_put  //ref is 0
        xfrm_policy_destroy
          call_rcu
                                     xfrm_pol_hold //ref is 1
                                   read_unlock
                                   xfrm_pol_put //ref is 0
                                     xfrm_policy_destroy
                                      call_rcu
    
    xfrm_policy_destroy is called twice, which may leads to
    double free.
    
    Call Trace:
    RIP: 0010:refcount_warn_saturate+0x161/0x210
    ...
     xfrm_policy_timer+0x522/0x600
     call_timer_fn+0x1b3/0x5e0
     ? __xfrm_decode_session+0x2990/0x2990
     ? msleep+0xb0/0xb0
     ? _raw_spin_unlock_irq+0x24/0x40
     ? __xfrm_decode_session+0x2990/0x2990
     ? __xfrm_decode_session+0x2990/0x2990
     run_timer_softirq+0x5c5/0x10e0
    
    Fix this by use write_lock_bh in xfrm_policy_kill.
    
    Fixes: ea2dea9dacc2 ("xfrm: remove policy lock when accessing policy->walk.dead")
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Acked-by: Timo Teräs <timo.teras@iki.fi>
    Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Steffen Klassert <steffen.klassert@secunet.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 86e98ce7de083649e330d518e98a80b9e39b5d43
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Mon Mar 23 15:32:39 2020 +0800

    xfrm: policy: Fix doulbe free in xfrm_policy_timer
    
    commit 4c59406ed00379c8663f8663d82b2537467ce9d7 upstream.
    
    After xfrm_add_policy add a policy, its ref is 2, then
    
                                 xfrm_policy_timer
                                   read_lock
                                   xp->walk.dead is 0
                                   ....
                                   mod_timer()
    xfrm_policy_kill
      policy->walk.dead = 1
      ....
      del_timer(&policy->timer)
        xfrm_pol_put //ref is 1
      xfrm_pol_put  //ref is 0
        xfrm_policy_destroy
          call_rcu
                                     xfrm_pol_hold //ref is 1
                                   read_unlock
                                   xfrm_pol_put //ref is 0
                                     xfrm_policy_destroy
                                      call_rcu
    
    xfrm_policy_destroy is called twice, which may leads to
    double free.
    
    Call Trace:
    RIP: 0010:refcount_warn_saturate+0x161/0x210
    ...
     xfrm_policy_timer+0x522/0x600
     call_timer_fn+0x1b3/0x5e0
     ? __xfrm_decode_session+0x2990/0x2990
     ? msleep+0xb0/0xb0
     ? _raw_spin_unlock_irq+0x24/0x40
     ? __xfrm_decode_session+0x2990/0x2990
     ? __xfrm_decode_session+0x2990/0x2990
     run_timer_softirq+0x5c5/0x10e0
    
    Fix this by use write_lock_bh in xfrm_policy_kill.
    
    Fixes: ea2dea9dacc2 ("xfrm: remove policy lock when accessing policy->walk.dead")
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Acked-by: Timo Teräs <timo.teras@iki.fi>
    Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Steffen Klassert <steffen.klassert@secunet.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit dc0ea9b710102ef628a26663d892031a2c381549
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Mon Mar 23 15:32:39 2020 +0800

    xfrm: policy: Fix doulbe free in xfrm_policy_timer
    
    commit 4c59406ed00379c8663f8663d82b2537467ce9d7 upstream.
    
    After xfrm_add_policy add a policy, its ref is 2, then
    
                                 xfrm_policy_timer
                                   read_lock
                                   xp->walk.dead is 0
                                   ....
                                   mod_timer()
    xfrm_policy_kill
      policy->walk.dead = 1
      ....
      del_timer(&policy->timer)
        xfrm_pol_put //ref is 1
      xfrm_pol_put  //ref is 0
        xfrm_policy_destroy
          call_rcu
                                     xfrm_pol_hold //ref is 1
                                   read_unlock
                                   xfrm_pol_put //ref is 0
                                     xfrm_policy_destroy
                                      call_rcu
    
    xfrm_policy_destroy is called twice, which may leads to
    double free.
    
    Call Trace:
    RIP: 0010:refcount_warn_saturate+0x161/0x210
    ...
     xfrm_policy_timer+0x522/0x600
     call_timer_fn+0x1b3/0x5e0
     ? __xfrm_decode_session+0x2990/0x2990
     ? msleep+0xb0/0xb0
     ? _raw_spin_unlock_irq+0x24/0x40
     ? __xfrm_decode_session+0x2990/0x2990
     ? __xfrm_decode_session+0x2990/0x2990
     run_timer_softirq+0x5c5/0x10e0
    
    Fix this by use write_lock_bh in xfrm_policy_kill.
    
    Fixes: ea2dea9dacc2 ("xfrm: remove policy lock when accessing policy->walk.dead")
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Acked-by: Timo Teräs <timo.teras@iki.fi>
    Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Steffen Klassert <steffen.klassert@secunet.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7ad217a824f7fab1e8534a6dfa82899ae1900bcb
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Mon Mar 23 15:32:39 2020 +0800

    xfrm: policy: Fix doulbe free in xfrm_policy_timer
    
    commit 4c59406ed00379c8663f8663d82b2537467ce9d7 upstream.
    
    After xfrm_add_policy add a policy, its ref is 2, then
    
                                 xfrm_policy_timer
                                   read_lock
                                   xp->walk.dead is 0
                                   ....
                                   mod_timer()
    xfrm_policy_kill
      policy->walk.dead = 1
      ....
      del_timer(&policy->timer)
        xfrm_pol_put //ref is 1
      xfrm_pol_put  //ref is 0
        xfrm_policy_destroy
          call_rcu
                                     xfrm_pol_hold //ref is 1
                                   read_unlock
                                   xfrm_pol_put //ref is 0
                                     xfrm_policy_destroy
                                      call_rcu
    
    xfrm_policy_destroy is called twice, which may leads to
    double free.
    
    Call Trace:
    RIP: 0010:refcount_warn_saturate+0x161/0x210
    ...
     xfrm_policy_timer+0x522/0x600
     call_timer_fn+0x1b3/0x5e0
     ? __xfrm_decode_session+0x2990/0x2990
     ? msleep+0xb0/0xb0
     ? _raw_spin_unlock_irq+0x24/0x40
     ? __xfrm_decode_session+0x2990/0x2990
     ? __xfrm_decode_session+0x2990/0x2990
     run_timer_softirq+0x5c5/0x10e0
    
    Fix this by use write_lock_bh in xfrm_policy_kill.
    
    Fixes: ea2dea9dacc2 ("xfrm: remove policy lock when accessing policy->walk.dead")
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Acked-by: Timo Teräs <timo.teras@iki.fi>
    Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Steffen Klassert <steffen.klassert@secunet.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 21af83e17ffae4955bbd8154a1e975826b8188a1
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Mon Mar 23 15:32:39 2020 +0800

    xfrm: policy: Fix doulbe free in xfrm_policy_timer
    
    commit 4c59406ed00379c8663f8663d82b2537467ce9d7 upstream.
    
    After xfrm_add_policy add a policy, its ref is 2, then
    
                                 xfrm_policy_timer
                                   read_lock
                                   xp->walk.dead is 0
                                   ....
                                   mod_timer()
    xfrm_policy_kill
      policy->walk.dead = 1
      ....
      del_timer(&policy->timer)
        xfrm_pol_put //ref is 1
      xfrm_pol_put  //ref is 0
        xfrm_policy_destroy
          call_rcu
                                     xfrm_pol_hold //ref is 1
                                   read_unlock
                                   xfrm_pol_put //ref is 0
                                     xfrm_policy_destroy
                                      call_rcu
    
    xfrm_policy_destroy is called twice, which may leads to
    double free.
    
    Call Trace:
    RIP: 0010:refcount_warn_saturate+0x161/0x210
    ...
     xfrm_policy_timer+0x522/0x600
     call_timer_fn+0x1b3/0x5e0
     ? __xfrm_decode_session+0x2990/0x2990
     ? msleep+0xb0/0xb0
     ? _raw_spin_unlock_irq+0x24/0x40
     ? __xfrm_decode_session+0x2990/0x2990
     ? __xfrm_decode_session+0x2990/0x2990
     run_timer_softirq+0x5c5/0x10e0
    
    Fix this by use write_lock_bh in xfrm_policy_kill.
    
    Fixes: ea2dea9dacc2 ("xfrm: remove policy lock when accessing policy->walk.dead")
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Acked-by: Timo Teräs <timo.teras@iki.fi>
    Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Steffen Klassert <steffen.klassert@secunet.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a6ca6e66a8b2f8741610d99c95c9bf5748e08372
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Mon Mar 23 15:32:39 2020 +0800

    xfrm: policy: Fix doulbe free in xfrm_policy_timer
    
    commit 4c59406ed00379c8663f8663d82b2537467ce9d7 upstream.
    
    After xfrm_add_policy add a policy, its ref is 2, then
    
                                 xfrm_policy_timer
                                   read_lock
                                   xp->walk.dead is 0
                                   ....
                                   mod_timer()
    xfrm_policy_kill
      policy->walk.dead = 1
      ....
      del_timer(&policy->timer)
        xfrm_pol_put //ref is 1
      xfrm_pol_put  //ref is 0
        xfrm_policy_destroy
          call_rcu
                                     xfrm_pol_hold //ref is 1
                                   read_unlock
                                   xfrm_pol_put //ref is 0
                                     xfrm_policy_destroy
                                      call_rcu
    
    xfrm_policy_destroy is called twice, which may leads to
    double free.
    
    Call Trace:
    RIP: 0010:refcount_warn_saturate+0x161/0x210
    ...
     xfrm_policy_timer+0x522/0x600
     call_timer_fn+0x1b3/0x5e0
     ? __xfrm_decode_session+0x2990/0x2990
     ? msleep+0xb0/0xb0
     ? _raw_spin_unlock_irq+0x24/0x40
     ? __xfrm_decode_session+0x2990/0x2990
     ? __xfrm_decode_session+0x2990/0x2990
     run_timer_softirq+0x5c5/0x10e0
    
    Fix this by use write_lock_bh in xfrm_policy_kill.
    
    Fixes: ea2dea9dacc2 ("xfrm: remove policy lock when accessing policy->walk.dead")
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Acked-by: Timo Teräs <timo.teras@iki.fi>
    Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Steffen Klassert <steffen.klassert@secunet.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4c59406ed00379c8663f8663d82b2537467ce9d7
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Mon Mar 23 15:32:39 2020 +0800

    xfrm: policy: Fix doulbe free in xfrm_policy_timer
    
    After xfrm_add_policy add a policy, its ref is 2, then
    
                                 xfrm_policy_timer
                                   read_lock
                                   xp->walk.dead is 0
                                   ....
                                   mod_timer()
    xfrm_policy_kill
      policy->walk.dead = 1
      ....
      del_timer(&policy->timer)
        xfrm_pol_put //ref is 1
      xfrm_pol_put  //ref is 0
        xfrm_policy_destroy
          call_rcu
                                     xfrm_pol_hold //ref is 1
                                   read_unlock
                                   xfrm_pol_put //ref is 0
                                     xfrm_policy_destroy
                                      call_rcu
    
    xfrm_policy_destroy is called twice, which may leads to
    double free.
    
    Call Trace:
    RIP: 0010:refcount_warn_saturate+0x161/0x210
    ...
     xfrm_policy_timer+0x522/0x600
     call_timer_fn+0x1b3/0x5e0
     ? __xfrm_decode_session+0x2990/0x2990
     ? msleep+0xb0/0xb0
     ? _raw_spin_unlock_irq+0x24/0x40
     ? __xfrm_decode_session+0x2990/0x2990
     ? __xfrm_decode_session+0x2990/0x2990
     run_timer_softirq+0x5c5/0x10e0
    
    Fix this by use write_lock_bh in xfrm_policy_kill.
    
    Fixes: ea2dea9dacc2 ("xfrm: remove policy lock when accessing policy->walk.dead")
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Acked-by: Timo Teräs <timo.teras@iki.fi>
    Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Steffen Klassert <steffen.klassert@secunet.com>

commit 68464d79015a1bbb41872a9119a07b2cb151a4b9
Author: Jules Irenge <jbi.octave@gmail.com>
Date:   Fri Feb 14 20:47:29 2020 +0000

    driver core: Add missing annotation for device_links_write_lock()
    
    Sparse reports a warning at device_links_write_lock()
    
    warning: context imbalance in evice_links_write_lock()
             - wrong count at exit
    
    The root cause is the missing annotation at device_links_write_lock()
    Add the missing __acquires(&device_links_srcu) annotation
    
    Signed-off-by: Jules Irenge <jbi.octave@gmail.com>
    Link: https://lore.kernel.org/r/20200214204741.94112-19-jbi.octave@gmail.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 350014475f6f977ad8f7c75f442ed80fe18495a3
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Jan 17 09:02:20 2020 -0500

    btrfs: don't set path->leave_spinning for truncate
    
    commit 52e29e331070cd7d52a64cbf1b0958212a340e28 upstream.
    
    The only time we actually leave the path spinning is if we're truncating
    a small amount and don't actually free an extent, which is not a common
    occurrence.  We have to set the path blocking in order to add the
    delayed ref anyway, so the first extent we find we set the path to
    blocking and stay blocking for the duration of the operation.  With the
    upcoming file extent map stuff there will be another case that we have
    to have the path blocking, so just swap to blocking always.
    
    Note: this patch also fixes a warning after 28553fa992cb ("Btrfs: fix
    race between shrinking truncate and fiemap") got merged that inserts
    extent locks around truncation so the path must not leave spinning locks
    after btrfs_search_slot.
    
      [70.794783] BUG: sleeping function called from invalid context at mm/slab.h:565
      [70.794834] in_atomic(): 1, irqs_disabled(): 0, non_block: 0, pid: 1141, name: rsync
      [70.794863] 5 locks held by rsync/1141:
      [70.794876]  #0: ffff888417b9c408 (sb_writers#17){.+.+}, at: mnt_want_write+0x20/0x50
      [70.795030]  #1: ffff888428de28e8 (&type->i_mutex_dir_key#13/1){+.+.}, at: lock_rename+0xf1/0x100
      [70.795051]  #2: ffff888417b9c608 (sb_internal#2){.+.+}, at: start_transaction+0x394/0x560
      [70.795124]  #3: ffff888403081768 (btrfs-fs-01){++++}, at: btrfs_try_tree_write_lock+0x2f/0x160
      [70.795203]  #4: ffff888403086568 (btrfs-fs-00){++++}, at: btrfs_try_tree_write_lock+0x2f/0x160
      [70.795222] CPU: 5 PID: 1141 Comm: rsync Not tainted 5.6.0-rc2-backup+ #2
      [70.795362] Call Trace:
      [70.795374]  dump_stack+0x71/0xa0
      [70.795445]  ___might_sleep.part.96.cold.106+0xa6/0xb6
      [70.795459]  kmem_cache_alloc+0x1d3/0x290
      [70.795471]  alloc_extent_state+0x22/0x1c0
      [70.795544]  __clear_extent_bit+0x3ba/0x580
      [70.795557]  ? _raw_spin_unlock_irq+0x24/0x30
      [70.795569]  btrfs_truncate_inode_items+0x339/0xe50
      [70.795647]  btrfs_evict_inode+0x269/0x540
      [70.795659]  ? dput.part.38+0x29/0x460
      [70.795671]  evict+0xcd/0x190
      [70.795682]  __dentry_kill+0xd6/0x180
      [70.795754]  dput.part.38+0x2ad/0x460
      [70.795765]  do_renameat2+0x3cb/0x540
      [70.795777]  __x64_sys_rename+0x1c/0x20
    
    Reported-by: Dave Jones <davej@codemonkey.org.uk>
    Fixes: 28553fa992cb ("Btrfs: fix race between shrinking truncate and fiemap")
    CC: stable@vger.kernel.org # 4.4+
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ add note ]
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 73e1f266327391303aa3da67ee28984e104c006f
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Jan 17 09:02:20 2020 -0500

    btrfs: don't set path->leave_spinning for truncate
    
    commit 52e29e331070cd7d52a64cbf1b0958212a340e28 upstream.
    
    The only time we actually leave the path spinning is if we're truncating
    a small amount and don't actually free an extent, which is not a common
    occurrence.  We have to set the path blocking in order to add the
    delayed ref anyway, so the first extent we find we set the path to
    blocking and stay blocking for the duration of the operation.  With the
    upcoming file extent map stuff there will be another case that we have
    to have the path blocking, so just swap to blocking always.
    
    Note: this patch also fixes a warning after 28553fa992cb ("Btrfs: fix
    race between shrinking truncate and fiemap") got merged that inserts
    extent locks around truncation so the path must not leave spinning locks
    after btrfs_search_slot.
    
      [70.794783] BUG: sleeping function called from invalid context at mm/slab.h:565
      [70.794834] in_atomic(): 1, irqs_disabled(): 0, non_block: 0, pid: 1141, name: rsync
      [70.794863] 5 locks held by rsync/1141:
      [70.794876]  #0: ffff888417b9c408 (sb_writers#17){.+.+}, at: mnt_want_write+0x20/0x50
      [70.795030]  #1: ffff888428de28e8 (&type->i_mutex_dir_key#13/1){+.+.}, at: lock_rename+0xf1/0x100
      [70.795051]  #2: ffff888417b9c608 (sb_internal#2){.+.+}, at: start_transaction+0x394/0x560
      [70.795124]  #3: ffff888403081768 (btrfs-fs-01){++++}, at: btrfs_try_tree_write_lock+0x2f/0x160
      [70.795203]  #4: ffff888403086568 (btrfs-fs-00){++++}, at: btrfs_try_tree_write_lock+0x2f/0x160
      [70.795222] CPU: 5 PID: 1141 Comm: rsync Not tainted 5.6.0-rc2-backup+ #2
      [70.795362] Call Trace:
      [70.795374]  dump_stack+0x71/0xa0
      [70.795445]  ___might_sleep.part.96.cold.106+0xa6/0xb6
      [70.795459]  kmem_cache_alloc+0x1d3/0x290
      [70.795471]  alloc_extent_state+0x22/0x1c0
      [70.795544]  __clear_extent_bit+0x3ba/0x580
      [70.795557]  ? _raw_spin_unlock_irq+0x24/0x30
      [70.795569]  btrfs_truncate_inode_items+0x339/0xe50
      [70.795647]  btrfs_evict_inode+0x269/0x540
      [70.795659]  ? dput.part.38+0x29/0x460
      [70.795671]  evict+0xcd/0x190
      [70.795682]  __dentry_kill+0xd6/0x180
      [70.795754]  dput.part.38+0x2ad/0x460
      [70.795765]  do_renameat2+0x3cb/0x540
      [70.795777]  __x64_sys_rename+0x1c/0x20
    
    Reported-by: Dave Jones <davej@codemonkey.org.uk>
    Fixes: 28553fa992cb ("Btrfs: fix race between shrinking truncate and fiemap")
    CC: stable@vger.kernel.org # 4.4+
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ add note ]
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 46991b1804675f043a6b91567b6a23399a984c3f
Author: Coly Li <colyli@suse.de>
Date:   Sat Feb 1 22:42:34 2020 +0800

    bcache: fix incorrect data type usage in btree_flush_write()
    
    [ Upstream commit d1c3cc34f5a78b38d2b809b289d912c3560545df ]
    
    Dan Carpenter points out that from commit 2aa8c529387c ("bcache: avoid
    unnecessary btree nodes flushing in btree_flush_write()"), there is a
    incorrect data type usage which leads to the following static checker
    warning:
            drivers/md/bcache/journal.c:444 btree_flush_write()
            warn: 'ref_nr' unsigned <= 0
    
    drivers/md/bcache/journal.c
       422  static void btree_flush_write(struct cache_set *c)
       423  {
       424          struct btree *b, *t, *btree_nodes[BTREE_FLUSH_NR];
       425          unsigned int i, nr, ref_nr;
                                        ^^^^^^
    
       426          atomic_t *fifo_front_p, *now_fifo_front_p;
       427          size_t mask;
       428
       429          if (c->journal.btree_flushing)
       430                  return;
       431
       432          spin_lock(&c->journal.flush_write_lock);
       433          if (c->journal.btree_flushing) {
       434                  spin_unlock(&c->journal.flush_write_lock);
       435                  return;
       436          }
       437          c->journal.btree_flushing = true;
       438          spin_unlock(&c->journal.flush_write_lock);
       439
       440          /* get the oldest journal entry and check its refcount */
       441          spin_lock(&c->journal.lock);
       442          fifo_front_p = &fifo_front(&c->journal.pin);
       443          ref_nr = atomic_read(fifo_front_p);
       444          if (ref_nr <= 0) {
                        ^^^^^^^^^^^
    Unsigned can't be less than zero.
    
       445                  /*
       446                   * do nothing if no btree node references
       447                   * the oldest journal entry
       448                   */
       449                  spin_unlock(&c->journal.lock);
       450                  goto out;
       451          }
       452          spin_unlock(&c->journal.lock);
    
    As the warning information indicates, local varaible ref_nr in unsigned
    int type is wrong, which does not matche atomic_read() and the "<= 0"
    checking.
    
    This patch fixes the above error by defining local variable ref_nr as
    int type.
    
    Fixes: 2aa8c529387c ("bcache: avoid unnecessary btree nodes flushing in btree_flush_write()")
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 793137b0511ca6b92e2350da72a73cb6caba58fa
Author: Coly Li <colyli@suse.de>
Date:   Sat Feb 1 22:42:34 2020 +0800

    bcache: fix incorrect data type usage in btree_flush_write()
    
    [ Upstream commit d1c3cc34f5a78b38d2b809b289d912c3560545df ]
    
    Dan Carpenter points out that from commit 2aa8c529387c ("bcache: avoid
    unnecessary btree nodes flushing in btree_flush_write()"), there is a
    incorrect data type usage which leads to the following static checker
    warning:
            drivers/md/bcache/journal.c:444 btree_flush_write()
            warn: 'ref_nr' unsigned <= 0
    
    drivers/md/bcache/journal.c
       422  static void btree_flush_write(struct cache_set *c)
       423  {
       424          struct btree *b, *t, *btree_nodes[BTREE_FLUSH_NR];
       425          unsigned int i, nr, ref_nr;
                                        ^^^^^^
    
       426          atomic_t *fifo_front_p, *now_fifo_front_p;
       427          size_t mask;
       428
       429          if (c->journal.btree_flushing)
       430                  return;
       431
       432          spin_lock(&c->journal.flush_write_lock);
       433          if (c->journal.btree_flushing) {
       434                  spin_unlock(&c->journal.flush_write_lock);
       435                  return;
       436          }
       437          c->journal.btree_flushing = true;
       438          spin_unlock(&c->journal.flush_write_lock);
       439
       440          /* get the oldest journal entry and check its refcount */
       441          spin_lock(&c->journal.lock);
       442          fifo_front_p = &fifo_front(&c->journal.pin);
       443          ref_nr = atomic_read(fifo_front_p);
       444          if (ref_nr <= 0) {
                        ^^^^^^^^^^^
    Unsigned can't be less than zero.
    
       445                  /*
       446                   * do nothing if no btree node references
       447                   * the oldest journal entry
       448                   */
       449                  spin_unlock(&c->journal.lock);
       450                  goto out;
       451          }
       452          spin_unlock(&c->journal.lock);
    
    As the warning information indicates, local varaible ref_nr in unsigned
    int type is wrong, which does not matche atomic_read() and the "<= 0"
    checking.
    
    This patch fixes the above error by defining local variable ref_nr as
    int type.
    
    Fixes: 2aa8c529387c ("bcache: avoid unnecessary btree nodes flushing in btree_flush_write()")
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 52e29e331070cd7d52a64cbf1b0958212a340e28
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Fri Jan 17 09:02:20 2020 -0500

    btrfs: don't set path->leave_spinning for truncate
    
    The only time we actually leave the path spinning is if we're truncating
    a small amount and don't actually free an extent, which is not a common
    occurrence.  We have to set the path blocking in order to add the
    delayed ref anyway, so the first extent we find we set the path to
    blocking and stay blocking for the duration of the operation.  With the
    upcoming file extent map stuff there will be another case that we have
    to have the path blocking, so just swap to blocking always.
    
    Note: this patch also fixes a warning after 28553fa992cb ("Btrfs: fix
    race between shrinking truncate and fiemap") got merged that inserts
    extent locks around truncation so the path must not leave spinning locks
    after btrfs_search_slot.
    
      [70.794783] BUG: sleeping function called from invalid context at mm/slab.h:565
      [70.794834] in_atomic(): 1, irqs_disabled(): 0, non_block: 0, pid: 1141, name: rsync
      [70.794863] 5 locks held by rsync/1141:
      [70.794876]  #0: ffff888417b9c408 (sb_writers#17){.+.+}, at: mnt_want_write+0x20/0x50
      [70.795030]  #1: ffff888428de28e8 (&type->i_mutex_dir_key#13/1){+.+.}, at: lock_rename+0xf1/0x100
      [70.795051]  #2: ffff888417b9c608 (sb_internal#2){.+.+}, at: start_transaction+0x394/0x560
      [70.795124]  #3: ffff888403081768 (btrfs-fs-01){++++}, at: btrfs_try_tree_write_lock+0x2f/0x160
      [70.795203]  #4: ffff888403086568 (btrfs-fs-00){++++}, at: btrfs_try_tree_write_lock+0x2f/0x160
      [70.795222] CPU: 5 PID: 1141 Comm: rsync Not tainted 5.6.0-rc2-backup+ #2
      [70.795362] Call Trace:
      [70.795374]  dump_stack+0x71/0xa0
      [70.795445]  ___might_sleep.part.96.cold.106+0xa6/0xb6
      [70.795459]  kmem_cache_alloc+0x1d3/0x290
      [70.795471]  alloc_extent_state+0x22/0x1c0
      [70.795544]  __clear_extent_bit+0x3ba/0x580
      [70.795557]  ? _raw_spin_unlock_irq+0x24/0x30
      [70.795569]  btrfs_truncate_inode_items+0x339/0xe50
      [70.795647]  btrfs_evict_inode+0x269/0x540
      [70.795659]  ? dput.part.38+0x29/0x460
      [70.795671]  evict+0xcd/0x190
      [70.795682]  __dentry_kill+0xd6/0x180
      [70.795754]  dput.part.38+0x2ad/0x460
      [70.795765]  do_renameat2+0x3cb/0x540
      [70.795777]  __x64_sys_rename+0x1c/0x20
    
    Reported-by: Dave Jones <davej@codemonkey.org.uk>
    Fixes: 28553fa992cb ("Btrfs: fix race between shrinking truncate and fiemap")
    CC: stable@vger.kernel.org # 4.4+
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ add note ]
    Signed-off-by: David Sterba <dsterba@suse.com>

commit b43de9e5b65f99cf92bc88665b72bb1fd7bd78ae
Author: Coly Li <colyli@suse.de>
Date:   Fri Jan 24 01:01:37 2020 +0800

    bcache: avoid unnecessary btree nodes flushing in btree_flush_write()
    
    commit 2aa8c529387c25606fdc1484154b92f8bfbc5746 upstream.
    
    the commit 91be66e1318f ("bcache: performance improvement for
    btree_flush_write()") was an effort to flushing btree node with oldest
    btree node faster in following methods,
    - Only iterate dirty btree nodes in c->btree_cache, avoid scanning a lot
      of clean btree nodes.
    - Take c->btree_cache as a LRU-like list, aggressively flushing all
      dirty nodes from tail of c->btree_cache util the btree node with
      oldest journal entry is flushed. This is to reduce the time of holding
      c->bucket_lock.
    
    Guoju Fang and Shuang Li reported that they observe unexptected extra
    write I/Os on cache device after applying the above patch. Guoju Fang
    provideed more detailed diagnose information that the aggressive
    btree nodes flushing may cause 10x more btree nodes to flush in his
    workload. He points out when system memory is large enough to hold all
    btree nodes in memory, c->btree_cache is not a LRU-like list any more.
    Then the btree node with oldest journal entry is very probably not-
    close to the tail of c->btree_cache list. In such situation much more
    dirty btree nodes will be aggressively flushed before the target node
    is flushed. When slow SATA SSD is used as cache device, such over-
    aggressive flushing behavior will cause performance regression.
    
    After spending a lot of time on debug and diagnose, I find the real
    condition is more complicated, aggressive flushing dirty btree nodes
    from tail of c->btree_cache list is not a good solution.
    - When all btree nodes are cached in memory, c->btree_cache is not
      a LRU-like list, the btree nodes with oldest journal entry won't
      be close to the tail of the list.
    - There can be hundreds dirty btree nodes reference the oldest journal
      entry, before flushing all the nodes the oldest journal entry cannot
      be reclaimed.
    When the above two conditions mixed together, a simply flushing from
    tail of c->btree_cache list is really NOT a good idea.
    
    Fortunately there is still chance to make btree_flush_write() work
    better. Here is how this patch avoids unnecessary btree nodes flushing,
    - Only acquire c->journal.lock when getting oldest journal entry of
      fifo c->journal.pin. In rested locations check the journal entries
      locklessly, so their values can be changed on other cores
      in parallel.
    - In loop list_for_each_entry_safe_reverse(), checking latest front
      point of fifo c->journal.pin. If it is different from the original
      point which we get with locking c->journal.lock, it means the oldest
      journal entry is reclaim on other cores. At this moment, all selected
      dirty nodes recorded in array btree_nodes[] are all flushed and clean
      on other CPU cores, it is unncessary to iterate c->btree_cache any
      longer. Just quit the list_for_each_entry_safe_reverse() loop and
      the following for-loop will skip all the selected clean nodes.
    - Find a proper time to quit the list_for_each_entry_safe_reverse()
      loop. Check the refcount value of orignial fifo front point, if the
      value is larger than selected node number of btree_nodes[], it means
      more matching btree nodes should be scanned. Otherwise it means no
      more matching btee nodes in rest of c->btree_cache list, the loop
      can be quit. If the original oldest journal entry is reclaimed and
      fifo front point is updated, the refcount of original fifo front point
      will be 0, then the loop will be quit too.
    - Not hold c->bucket_lock too long time. c->bucket_lock is also required
      for space allocation for cached data, hold it for too long time will
      block regular I/O requests. When iterating list c->btree_cache, even
      there are a lot of maching btree nodes, in order to not holding
      c->bucket_lock for too long time, only BTREE_FLUSH_NR nodes are
      selected and to flush in following for-loop.
    With this patch, only btree nodes referencing oldest journal entry
    are flushed to cache device, no aggressive flushing for  unnecessary
    btree node any more. And in order to avoid blocking regluar I/O
    requests, each time when btree_flush_write() called, at most only
    BTREE_FLUSH_NR btree nodes are selected to flush, even there are more
    maching btree nodes in list c->btree_cache.
    
    At last, one more thing to explain: Why it is safe to read front point
    of c->journal.pin without holding c->journal.lock inside the
    list_for_each_entry_safe_reverse() loop ?
    
    Here is my answer: When reading the front point of fifo c->journal.pin,
    we don't need to know the exact value of front point, we just want to
    check whether the value is different from the original front point
    (which is accurate value because we get it while c->jouranl.lock is
    held). For such purpose, it works as expected without holding
    c->journal.lock. Even the front point is changed on other CPU core and
    not updated to local core, and current iterating btree node has
    identical journal entry local as original fetched fifo front point, it
    is still safe. Because after holding mutex b->write_lock (with memory
    barrier) this btree node can be found as clean and skipped, the loop
    will quite latter when iterate on next node of list c->btree_cache.
    
    Fixes: 91be66e1318f ("bcache: performance improvement for btree_flush_write()")
    Reported-by: Guoju Fang <fangguoju@gmail.com>
    Reported-by: Shuang Li <psymon@bonuscloud.io>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 837c36e0451f14225e684bb4942a9f4b4eed3cb7
Author: Coly Li <colyli@suse.de>
Date:   Fri Jan 24 01:01:37 2020 +0800

    bcache: avoid unnecessary btree nodes flushing in btree_flush_write()
    
    commit 2aa8c529387c25606fdc1484154b92f8bfbc5746 upstream.
    
    the commit 91be66e1318f ("bcache: performance improvement for
    btree_flush_write()") was an effort to flushing btree node with oldest
    btree node faster in following methods,
    - Only iterate dirty btree nodes in c->btree_cache, avoid scanning a lot
      of clean btree nodes.
    - Take c->btree_cache as a LRU-like list, aggressively flushing all
      dirty nodes from tail of c->btree_cache util the btree node with
      oldest journal entry is flushed. This is to reduce the time of holding
      c->bucket_lock.
    
    Guoju Fang and Shuang Li reported that they observe unexptected extra
    write I/Os on cache device after applying the above patch. Guoju Fang
    provideed more detailed diagnose information that the aggressive
    btree nodes flushing may cause 10x more btree nodes to flush in his
    workload. He points out when system memory is large enough to hold all
    btree nodes in memory, c->btree_cache is not a LRU-like list any more.
    Then the btree node with oldest journal entry is very probably not-
    close to the tail of c->btree_cache list. In such situation much more
    dirty btree nodes will be aggressively flushed before the target node
    is flushed. When slow SATA SSD is used as cache device, such over-
    aggressive flushing behavior will cause performance regression.
    
    After spending a lot of time on debug and diagnose, I find the real
    condition is more complicated, aggressive flushing dirty btree nodes
    from tail of c->btree_cache list is not a good solution.
    - When all btree nodes are cached in memory, c->btree_cache is not
      a LRU-like list, the btree nodes with oldest journal entry won't
      be close to the tail of the list.
    - There can be hundreds dirty btree nodes reference the oldest journal
      entry, before flushing all the nodes the oldest journal entry cannot
      be reclaimed.
    When the above two conditions mixed together, a simply flushing from
    tail of c->btree_cache list is really NOT a good idea.
    
    Fortunately there is still chance to make btree_flush_write() work
    better. Here is how this patch avoids unnecessary btree nodes flushing,
    - Only acquire c->journal.lock when getting oldest journal entry of
      fifo c->journal.pin. In rested locations check the journal entries
      locklessly, so their values can be changed on other cores
      in parallel.
    - In loop list_for_each_entry_safe_reverse(), checking latest front
      point of fifo c->journal.pin. If it is different from the original
      point which we get with locking c->journal.lock, it means the oldest
      journal entry is reclaim on other cores. At this moment, all selected
      dirty nodes recorded in array btree_nodes[] are all flushed and clean
      on other CPU cores, it is unncessary to iterate c->btree_cache any
      longer. Just quit the list_for_each_entry_safe_reverse() loop and
      the following for-loop will skip all the selected clean nodes.
    - Find a proper time to quit the list_for_each_entry_safe_reverse()
      loop. Check the refcount value of orignial fifo front point, if the
      value is larger than selected node number of btree_nodes[], it means
      more matching btree nodes should be scanned. Otherwise it means no
      more matching btee nodes in rest of c->btree_cache list, the loop
      can be quit. If the original oldest journal entry is reclaimed and
      fifo front point is updated, the refcount of original fifo front point
      will be 0, then the loop will be quit too.
    - Not hold c->bucket_lock too long time. c->bucket_lock is also required
      for space allocation for cached data, hold it for too long time will
      block regular I/O requests. When iterating list c->btree_cache, even
      there are a lot of maching btree nodes, in order to not holding
      c->bucket_lock for too long time, only BTREE_FLUSH_NR nodes are
      selected and to flush in following for-loop.
    With this patch, only btree nodes referencing oldest journal entry
    are flushed to cache device, no aggressive flushing for  unnecessary
    btree node any more. And in order to avoid blocking regluar I/O
    requests, each time when btree_flush_write() called, at most only
    BTREE_FLUSH_NR btree nodes are selected to flush, even there are more
    maching btree nodes in list c->btree_cache.
    
    At last, one more thing to explain: Why it is safe to read front point
    of c->journal.pin without holding c->journal.lock inside the
    list_for_each_entry_safe_reverse() loop ?
    
    Here is my answer: When reading the front point of fifo c->journal.pin,
    we don't need to know the exact value of front point, we just want to
    check whether the value is different from the original front point
    (which is accurate value because we get it while c->jouranl.lock is
    held). For such purpose, it works as expected without holding
    c->journal.lock. Even the front point is changed on other CPU core and
    not updated to local core, and current iterating btree node has
    identical journal entry local as original fetched fifo front point, it
    is still safe. Because after holding mutex b->write_lock (with memory
    barrier) this btree node can be found as clean and skipped, the loop
    will quite latter when iterate on next node of list c->btree_cache.
    
    Fixes: 91be66e1318f ("bcache: performance improvement for btree_flush_write()")
    Reported-by: Guoju Fang <fangguoju@gmail.com>
    Reported-by: Shuang Li <psymon@bonuscloud.io>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1ebada600cf56ab58f533279392373a0b22e53b6
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Mon Jan 13 11:18:51 2020 -0500

    dm thin metadata: use pool locking at end of dm_pool_metadata_close
    
    commit 44d8ebf436399a40fcd10dd31b29d37823d62fcc upstream.
    
    Ensure that the pool is locked during calls to __commit_transaction and
    __destroy_persistent_data_objects.  Just being consistent with locking,
    but reality is dm_pool_metadata_close is called once pool is being
    destroyed so access to pool shouldn't be contended.
    
    Also, use pmd_write_lock_in_core rather than __pmd_write_lock in
    dm_pool_commit_metadata and rename __pmd_write_lock to
    pmd_write_lock_in_core -- there was no need for the alias.
    
    In addition, verify that the pool is locked in __commit_transaction().
    
    Fixes: 873f258becca ("dm thin metadata: do not write metadata if no changes occurred")
    Cc: stable@vger.kernel.org
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1426201af047f233d98958e1862a13f66e9ce09f
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Mon Jan 13 11:18:51 2020 -0500

    dm thin metadata: use pool locking at end of dm_pool_metadata_close
    
    commit 44d8ebf436399a40fcd10dd31b29d37823d62fcc upstream.
    
    Ensure that the pool is locked during calls to __commit_transaction and
    __destroy_persistent_data_objects.  Just being consistent with locking,
    but reality is dm_pool_metadata_close is called once pool is being
    destroyed so access to pool shouldn't be contended.
    
    Also, use pmd_write_lock_in_core rather than __pmd_write_lock in
    dm_pool_commit_metadata and rename __pmd_write_lock to
    pmd_write_lock_in_core -- there was no need for the alias.
    
    In addition, verify that the pool is locked in __commit_transaction().
    
    Fixes: 873f258becca ("dm thin metadata: do not write metadata if no changes occurred")
    Cc: stable@vger.kernel.org
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d1c3cc34f5a78b38d2b809b289d912c3560545df
Author: Coly Li <colyli@suse.de>
Date:   Sat Feb 1 22:42:34 2020 +0800

    bcache: fix incorrect data type usage in btree_flush_write()
    
    Dan Carpenter points out that from commit 2aa8c529387c ("bcache: avoid
    unnecessary btree nodes flushing in btree_flush_write()"), there is a
    incorrect data type usage which leads to the following static checker
    warning:
            drivers/md/bcache/journal.c:444 btree_flush_write()
            warn: 'ref_nr' unsigned <= 0
    
    drivers/md/bcache/journal.c
       422  static void btree_flush_write(struct cache_set *c)
       423  {
       424          struct btree *b, *t, *btree_nodes[BTREE_FLUSH_NR];
       425          unsigned int i, nr, ref_nr;
                                        ^^^^^^
    
       426          atomic_t *fifo_front_p, *now_fifo_front_p;
       427          size_t mask;
       428
       429          if (c->journal.btree_flushing)
       430                  return;
       431
       432          spin_lock(&c->journal.flush_write_lock);
       433          if (c->journal.btree_flushing) {
       434                  spin_unlock(&c->journal.flush_write_lock);
       435                  return;
       436          }
       437          c->journal.btree_flushing = true;
       438          spin_unlock(&c->journal.flush_write_lock);
       439
       440          /* get the oldest journal entry and check its refcount */
       441          spin_lock(&c->journal.lock);
       442          fifo_front_p = &fifo_front(&c->journal.pin);
       443          ref_nr = atomic_read(fifo_front_p);
       444          if (ref_nr <= 0) {
                        ^^^^^^^^^^^
    Unsigned can't be less than zero.
    
       445                  /*
       446                   * do nothing if no btree node references
       447                   * the oldest journal entry
       448                   */
       449                  spin_unlock(&c->journal.lock);
       450                  goto out;
       451          }
       452          spin_unlock(&c->journal.lock);
    
    As the warning information indicates, local varaible ref_nr in unsigned
    int type is wrong, which does not matche atomic_read() and the "<= 0"
    checking.
    
    This patch fixes the above error by defining local variable ref_nr as
    int type.
    
    Fixes: 2aa8c529387c ("bcache: avoid unnecessary btree nodes flushing in btree_flush_write()")
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 301c669961142183a7b157cc79bbba3d963b8a29
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Feb 1 01:47:53 2019 +0100

    driver core: Do not resume suppliers under device_links_write_lock()
    
    [ Upstream commit 5db25c9eb893df8f6b93c1d97b8006d768e1b6f5 ]
    
    It is incorrect to call pm_runtime_get_sync() under
    device_links_write_lock(), because it may end up trying to take
    device_links_read_lock() while resuming the target device and that
    will deadlock in the non-SRCU case, so avoid that by resuming the
    supplier device in device_link_add() before calling
    device_links_write_lock().
    
    Fixes: 21d5c57b3726 ("PM / runtime: Use device links")
    Fixes: baa8809f6097 ("PM / runtime: Optimize the use of device links")
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d7ee5bfb5541b2d8b652f1026c12a5a631d14b8e
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Feb 1 01:47:53 2019 +0100

    driver core: Do not resume suppliers under device_links_write_lock()
    
    [ Upstream commit 5db25c9eb893df8f6b93c1d97b8006d768e1b6f5 ]
    
    It is incorrect to call pm_runtime_get_sync() under
    device_links_write_lock(), because it may end up trying to take
    device_links_read_lock() while resuming the target device and that
    will deadlock in the non-SRCU case, so avoid that by resuming the
    supplier device in device_link_add() before calling
    device_links_write_lock().
    
    Fixes: 21d5c57b3726 ("PM / runtime: Use device links")
    Fixes: baa8809f6097 ("PM / runtime: Optimize the use of device links")
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2aa8c529387c25606fdc1484154b92f8bfbc5746
Author: Coly Li <colyli@suse.de>
Date:   Fri Jan 24 01:01:37 2020 +0800

    bcache: avoid unnecessary btree nodes flushing in btree_flush_write()
    
    the commit 91be66e1318f ("bcache: performance improvement for
    btree_flush_write()") was an effort to flushing btree node with oldest
    btree node faster in following methods,
    - Only iterate dirty btree nodes in c->btree_cache, avoid scanning a lot
      of clean btree nodes.
    - Take c->btree_cache as a LRU-like list, aggressively flushing all
      dirty nodes from tail of c->btree_cache util the btree node with
      oldest journal entry is flushed. This is to reduce the time of holding
      c->bucket_lock.
    
    Guoju Fang and Shuang Li reported that they observe unexptected extra
    write I/Os on cache device after applying the above patch. Guoju Fang
    provideed more detailed diagnose information that the aggressive
    btree nodes flushing may cause 10x more btree nodes to flush in his
    workload. He points out when system memory is large enough to hold all
    btree nodes in memory, c->btree_cache is not a LRU-like list any more.
    Then the btree node with oldest journal entry is very probably not-
    close to the tail of c->btree_cache list. In such situation much more
    dirty btree nodes will be aggressively flushed before the target node
    is flushed. When slow SATA SSD is used as cache device, such over-
    aggressive flushing behavior will cause performance regression.
    
    After spending a lot of time on debug and diagnose, I find the real
    condition is more complicated, aggressive flushing dirty btree nodes
    from tail of c->btree_cache list is not a good solution.
    - When all btree nodes are cached in memory, c->btree_cache is not
      a LRU-like list, the btree nodes with oldest journal entry won't
      be close to the tail of the list.
    - There can be hundreds dirty btree nodes reference the oldest journal
      entry, before flushing all the nodes the oldest journal entry cannot
      be reclaimed.
    When the above two conditions mixed together, a simply flushing from
    tail of c->btree_cache list is really NOT a good idea.
    
    Fortunately there is still chance to make btree_flush_write() work
    better. Here is how this patch avoids unnecessary btree nodes flushing,
    - Only acquire c->journal.lock when getting oldest journal entry of
      fifo c->journal.pin. In rested locations check the journal entries
      locklessly, so their values can be changed on other cores
      in parallel.
    - In loop list_for_each_entry_safe_reverse(), checking latest front
      point of fifo c->journal.pin. If it is different from the original
      point which we get with locking c->journal.lock, it means the oldest
      journal entry is reclaim on other cores. At this moment, all selected
      dirty nodes recorded in array btree_nodes[] are all flushed and clean
      on other CPU cores, it is unncessary to iterate c->btree_cache any
      longer. Just quit the list_for_each_entry_safe_reverse() loop and
      the following for-loop will skip all the selected clean nodes.
    - Find a proper time to quit the list_for_each_entry_safe_reverse()
      loop. Check the refcount value of orignial fifo front point, if the
      value is larger than selected node number of btree_nodes[], it means
      more matching btree nodes should be scanned. Otherwise it means no
      more matching btee nodes in rest of c->btree_cache list, the loop
      can be quit. If the original oldest journal entry is reclaimed and
      fifo front point is updated, the refcount of original fifo front point
      will be 0, then the loop will be quit too.
    - Not hold c->bucket_lock too long time. c->bucket_lock is also required
      for space allocation for cached data, hold it for too long time will
      block regular I/O requests. When iterating list c->btree_cache, even
      there are a lot of maching btree nodes, in order to not holding
      c->bucket_lock for too long time, only BTREE_FLUSH_NR nodes are
      selected and to flush in following for-loop.
    With this patch, only btree nodes referencing oldest journal entry
    are flushed to cache device, no aggressive flushing for  unnecessary
    btree node any more. And in order to avoid blocking regluar I/O
    requests, each time when btree_flush_write() called, at most only
    BTREE_FLUSH_NR btree nodes are selected to flush, even there are more
    maching btree nodes in list c->btree_cache.
    
    At last, one more thing to explain: Why it is safe to read front point
    of c->journal.pin without holding c->journal.lock inside the
    list_for_each_entry_safe_reverse() loop ?
    
    Here is my answer: When reading the front point of fifo c->journal.pin,
    we don't need to know the exact value of front point, we just want to
    check whether the value is different from the original front point
    (which is accurate value because we get it while c->jouranl.lock is
    held). For such purpose, it works as expected without holding
    c->journal.lock. Even the front point is changed on other CPU core and
    not updated to local core, and current iterating btree node has
    identical journal entry local as original fetched fifo front point, it
    is still safe. Because after holding mutex b->write_lock (with memory
    barrier) this btree node can be found as clean and skipped, the loop
    will quite latter when iterate on next node of list c->btree_cache.
    
    Fixes: 91be66e1318f ("bcache: performance improvement for btree_flush_write()")
    Reported-by: Guoju Fang <fangguoju@gmail.com>
    Reported-by: Shuang Li <psymon@bonuscloud.io>
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 44d8ebf436399a40fcd10dd31b29d37823d62fcc
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Mon Jan 13 11:18:51 2020 -0500

    dm thin metadata: use pool locking at end of dm_pool_metadata_close
    
    Ensure that the pool is locked during calls to __commit_transaction and
    __destroy_persistent_data_objects.  Just being consistent with locking,
    but reality is dm_pool_metadata_close is called once pool is being
    destroyed so access to pool shouldn't be contended.
    
    Also, use pmd_write_lock_in_core rather than __pmd_write_lock in
    dm_pool_commit_metadata and rename __pmd_write_lock to
    pmd_write_lock_in_core -- there was no need for the alias.
    
    In addition, verify that the pool is locked in __commit_transaction().
    
    Fixes: 873f258becca ("dm thin metadata: do not write metadata if no changes occurred")
    Cc: stable@vger.kernel.org
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

commit 4e1269e147980f9f765c4d049058b2e8b000d196
Author: Chris Mason <clm@fb.com>
Date:   Wed Jul 10 12:28:16 2019 -0700

    Btrfs: only associate the locked page with one async_chunk struct
    
    [ Upstream commit 1d53c9e6723022b12e4a5ed4b141f67c834b7f6f ]
    
    The btrfs writepages function collects a large range of pages flagged
    for delayed allocation, and then sends them down through the COW code
    for processing.  When compression is on, we allocate one async_chunk
    structure for every 512K, and then run those pages through the
    compression code for IO submission.
    
    writepages starts all of this off with a single page, locked by the
    original call to extent_write_cache_pages(), and it's important to keep
    track of this page because it has already been through
    clear_page_dirty_for_io().
    
    The btrfs async_chunk struct has a pointer to the locked_page, and when
    we're redirtying the page because compression had to fallback to
    uncompressed IO, we use page->index to decide if a given async_chunk
    struct really owns that page.
    
    But, this is racey.  If a given delalloc range is broken up into two
    async_chunks (chunkA and chunkB), we can end up with something like
    this:
    
     compress_file_range(chunkA)
     submit_compress_extents(chunkA)
     submit compressed bios(chunkA)
     put_page(locked_page)
    
                                     compress_file_range(chunkB)
                                     ...
    
    Or:
    
     async_cow_submit
      submit_compressed_extents <--- falls back to buffered writeout
       cow_file_range
        extent_clear_unlock_delalloc
         __process_pages_contig
           put_page(locked_pages)
    
                                                async_cow_submit
    
    The end result is that chunkA is completed and cleaned up before chunkB
    even starts processing.  This means we can free locked_page() and reuse
    it elsewhere.  If we get really lucky, it'll have the same page->index
    in its new home as it did before.
    
    While we're processing chunkB, we might decide we need to fall back to
    uncompressed IO, and so compress_file_range() will call
    __set_page_dirty_nobufers() on chunkB->locked_page.
    
    Without cgroups in use, this creates as a phantom dirty page, which
    isn't great but isn't the end of the world. What can happen, it can go
    through the fixup worker and the whole COW machinery again:
    
    in submit_compressed_extents():
      while (async extents) {
      ...
        cow_file_range
        if (!page_started ...)
          extent_write_locked_range
        else if (...)
          unlock_page
        continue;
    
    This hasn't been observed in practice but is still possible.
    
    With cgroups in use, we might crash in the accounting code because
    page->mapping->i_wb isn't set.
    
      BUG: unable to handle kernel NULL pointer dereference at 00000000000000d0
      IP: percpu_counter_add_batch+0x11/0x70
      PGD 66534e067 P4D 66534e067 PUD 66534f067 PMD 0
      Oops: 0000 [#1] SMP DEBUG_PAGEALLOC
      CPU: 16 PID: 2172 Comm: rm Not tainted
      RIP: 0010:percpu_counter_add_batch+0x11/0x70
      RSP: 0018:ffffc9000a97bbe0 EFLAGS: 00010286
      RAX: 0000000000000005 RBX: 0000000000000090 RCX: 0000000000026115
      RDX: 0000000000000030 RSI: ffffffffffffffff RDI: 0000000000000090
      RBP: 0000000000000000 R08: fffffffffffffff5 R09: 0000000000000000
      R10: 00000000000260c0 R11: ffff881037fc26c0 R12: ffffffffffffffff
      R13: ffff880fe4111548 R14: ffffc9000a97bc90 R15: 0000000000000001
      FS:  00007f5503ced480(0000) GS:ffff880ff7200000(0000) knlGS:0000000000000000
      CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      CR2: 00000000000000d0 CR3: 00000001e0459005 CR4: 0000000000360ee0
      DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      Call Trace:
       account_page_cleaned+0x15b/0x1f0
       __cancel_dirty_page+0x146/0x200
       truncate_cleanup_page+0x92/0xb0
       truncate_inode_pages_range+0x202/0x7d0
       btrfs_evict_inode+0x92/0x5a0
       evict+0xc1/0x190
       do_unlinkat+0x176/0x280
       do_syscall_64+0x63/0x1a0
       entry_SYSCALL_64_after_hwframe+0x42/0xb7
    
    The fix here is to make asyc_chunk->locked_page NULL everywhere but the
    one async_chunk struct that's allowed to do things to the locked page.
    
    Link: https://lore.kernel.org/linux-btrfs/c2419d01-5c84-3fb4-189e-4db519d08796@suse.com/
    Fixes: 771ed689d2cd ("Btrfs: Optimize compressed writeback and reads")
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Chris Mason <clm@fb.com>
    [ update changelog from mail thread discussion ]
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 842c4cd688268859f077f552fd60f0521abf52dc
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Dec 11 09:01:40 2019 +0000

    Btrfs: fix infinite loop during nocow writeback due to race
    
    commit de7999afedff02c6631feab3ea726a0e8f8c3d40 upstream.
    
    When starting writeback for a range that covers part of a preallocated
    extent, due to a race with writeback for another range that also covers
    another part of the same preallocated extent, we can end up in an infinite
    loop.
    
    Consider the following example where for inode 280 we have two dirty
    ranges:
    
      range A, from 294912 to 303103, 8192 bytes
      range B, from 348160 to 438271, 90112 bytes
    
    and we have the following file extent item layout for our inode:
    
      leaf 38895616 gen 24544 total ptrs 29 free space 13820 owner 5
          (...)
          item 27 key (280 108 200704) itemoff 14598 itemsize 53
              extent data disk bytenr 0 nr 0 type 1 (regular)
              extent data offset 0 nr 94208 ram 94208
          item 28 key (280 108 294912) itemoff 14545 itemsize 53
              extent data disk bytenr 10433052672 nr 81920 type 2 (prealloc)
              extent data offset 0 nr 81920 ram 81920
    
    Then the following happens:
    
    1) Writeback starts for range B (from 348160 to 438271), execution of
       run_delalloc_nocow() starts;
    
    2) The first iteration of run_delalloc_nocow()'s whil loop leaves us at
       the extent item at slot 28, pointing to the prealloc extent item
       covering the range from 294912 to 376831. This extent covers part of
       our range;
    
    3) An ordered extent is created against that extent, covering the file
       range from 348160 to 376831 (28672 bytes);
    
    4) We adjust 'cur_offset' to 376832 and move on to the next iteration of
       the while loop;
    
    5) The call to btrfs_lookup_file_extent() leaves us at the same leaf,
       pointing to slot 29, 1 slot after the last item (the extent item
       we processed in the previous iteration);
    
    6) Because we are a slot beyond the last item, we call btrfs_next_leaf(),
       which releases the search path before doing a another search for the
       last key of the leaf (280 108 294912);
    
    7) Right after btrfs_next_leaf() released the path, and before it did
       another search for the last key of the leaf, writeback for the range
       A (from 294912 to 303103) completes (it was previously started at
       some point);
    
    8) Upon completion of the ordered extent for range A, the prealloc extent
       we previously found got split into two extent items, one covering the
       range from 294912 to 303103 (8192 bytes), with a type of regular extent
       (and no longer prealloc) and another covering the range from 303104 to
       376831 (73728 bytes), with a type of prealloc and an offset of 8192
       bytes. So our leaf now has the following layout:
    
         leaf 38895616 gen 24544 total ptrs 31 free space 13664 owner 5
             (...)
             item 27 key (280 108 200704) itemoff 14598 itemsize 53
                 extent data disk bytenr 0 nr 0 type 1
                 extent data offset 0 nr 8192 ram 94208
             item 28 key (280 108 208896) itemoff 14545 itemsize 53
                 extent data disk bytenr 10433142784 nr 86016 type 1
                 extent data offset 0 nr 86016 ram 86016
             item 29 key (280 108 294912) itemoff 14492 itemsize 53
                 extent data disk bytenr 10433052672 nr 81920 type 1
                 extent data offset 0 nr 8192 ram 81920
             item 30 key (280 108 303104) itemoff 14439 itemsize 53
                 extent data disk bytenr 10433052672 nr 81920 type 2
                 extent data offset 8192 nr 73728 ram 81920
    
    9) After btrfs_next_leaf() returns, we have our path pointing to that same
       leaf and at slot 30, since it has a key we didn't have before and it's
       the first key greater then the key that was previously the last key of
       the leaf (key (280 108 294912));
    
    10) The extent item at slot 30 covers the range from 303104 to 376831
        which is in our target range, so we process it, despite having already
        created an ordered extent against this extent for the file range from
        348160 to 376831. This is because we skip to the next extent item only
        if its end is less than or equals to the start of our delalloc range,
        and not less than or equals to the current offset ('cur_offset');
    
    11) As a result we compute 'num_bytes' as:
    
        num_bytes = min(end + 1, extent_end) - cur_offset;
                  = min(438271 + 1, 376832) - 376832 = 0
    
    12) We then call create_io_em() for a 0 bytes range starting at offset
        376832;
    
    13) Then create_io_em() enters an infinite loop because its calls to
        btrfs_drop_extent_cache() do nothing due to the 0 length range
        passed to it. So no existing extent maps that cover the offset
        376832 get removed, and therefore calls to add_extent_mapping()
        return -EEXIST, resulting in an infinite loop. This loop from
        create_io_em() is the following:
    
        do {
            btrfs_drop_extent_cache(BTRFS_I(inode), em->start,
                                    em->start + em->len - 1, 0);
            write_lock(&em_tree->lock);
            ret = add_extent_mapping(em_tree, em, 1);
            write_unlock(&em_tree->lock);
            /*
             * The caller has taken lock_extent(), who could race with us
             * to add em?
             */
        } while (ret == -EEXIST);
    
    Also, each call to btrfs_drop_extent_cache() triggers a warning because
    the start offset passed to it (376832) is smaller then the end offset
    (376832 - 1) passed to it by -1, due to the 0 length:
    
      [258532.052621] ------------[ cut here ]------------
      [258532.052643] WARNING: CPU: 0 PID: 9987 at fs/btrfs/file.c:602 btrfs_drop_extent_cache+0x3f4/0x590 [btrfs]
      (...)
      [258532.052672] CPU: 0 PID: 9987 Comm: fsx Tainted: G        W         5.4.0-rc7-btrfs-next-64 #1
      [258532.052673] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.0-0-ga698c8995f-prebuilt.qemu.org 04/01/2014
      [258532.052691] RIP: 0010:btrfs_drop_extent_cache+0x3f4/0x590 [btrfs]
      (...)
      [258532.052695] RSP: 0018:ffffb4be0153f860 EFLAGS: 00010287
      [258532.052700] RAX: ffff975b445ee360 RBX: ffff975b44eb3e08 RCX: 0000000000000000
      [258532.052700] RDX: 0000000000038fff RSI: 0000000000039000 RDI: ffff975b445ee308
      [258532.052700] RBP: 0000000000038fff R08: 0000000000000000 R09: 0000000000000001
      [258532.052701] R10: ffff975b513c5c10 R11: 00000000e3c0cfa9 R12: 0000000000039000
      [258532.052703] R13: ffff975b445ee360 R14: 00000000ffffffef R15: ffff975b445ee308
      [258532.052705] FS:  00007f86a821de80(0000) GS:ffff975b76a00000(0000) knlGS:0000000000000000
      [258532.052707] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      [258532.052708] CR2: 00007fdacf0f3ab4 CR3: 00000001f9d26002 CR4: 00000000003606f0
      [258532.052712] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      [258532.052717] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      [258532.052717] Call Trace:
      [258532.052718]  ? preempt_schedule_common+0x32/0x70
      [258532.052722]  ? ___preempt_schedule+0x16/0x20
      [258532.052741]  create_io_em+0xff/0x180 [btrfs]
      [258532.052767]  run_delalloc_nocow+0x942/0xb10 [btrfs]
      [258532.052791]  btrfs_run_delalloc_range+0x30b/0x520 [btrfs]
      [258532.052812]  ? find_lock_delalloc_range+0x221/0x250 [btrfs]
      [258532.052834]  writepage_delalloc+0xe4/0x140 [btrfs]
      [258532.052855]  __extent_writepage+0x110/0x4e0 [btrfs]
      [258532.052876]  extent_write_cache_pages+0x21c/0x480 [btrfs]
      [258532.052906]  extent_writepages+0x52/0xb0 [btrfs]
      [258532.052911]  do_writepages+0x23/0x80
      [258532.052915]  __filemap_fdatawrite_range+0xd2/0x110
      [258532.052938]  btrfs_fdatawrite_range+0x1b/0x50 [btrfs]
      [258532.052954]  start_ordered_ops+0x57/0xa0 [btrfs]
      [258532.052973]  ? btrfs_sync_file+0x225/0x490 [btrfs]
      [258532.052988]  btrfs_sync_file+0x225/0x490 [btrfs]
      [258532.052997]  __x64_sys_msync+0x199/0x200
      [258532.053004]  do_syscall_64+0x5c/0x250
      [258532.053007]  entry_SYSCALL_64_after_hwframe+0x49/0xbe
      [258532.053010] RIP: 0033:0x7f86a7dfd760
      (...)
      [258532.053014] RSP: 002b:00007ffd99af0368 EFLAGS: 00000246 ORIG_RAX: 000000000000001a
      [258532.053016] RAX: ffffffffffffffda RBX: 0000000000000ec9 RCX: 00007f86a7dfd760
      [258532.053017] RDX: 0000000000000004 RSI: 000000000000836c RDI: 00007f86a8221000
      [258532.053019] RBP: 0000000000021ec9 R08: 0000000000000003 R09: 00007f86a812037c
      [258532.053020] R10: 0000000000000001 R11: 0000000000000246 R12: 00000000000074a3
      [258532.053021] R13: 00007f86a8221000 R14: 000000000000836c R15: 0000000000000001
      [258532.053032] irq event stamp: 1653450494
      [258532.053035] hardirqs last  enabled at (1653450493): [<ffffffff9dec69f9>] _raw_spin_unlock_irq+0x29/0x50
      [258532.053037] hardirqs last disabled at (1653450494): [<ffffffff9d4048ea>] trace_hardirqs_off_thunk+0x1a/0x20
      [258532.053039] softirqs last  enabled at (1653449852): [<ffffffff9e200466>] __do_softirq+0x466/0x6bd
      [258532.053042] softirqs last disabled at (1653449845): [<ffffffff9d4c8a0c>] irq_exit+0xec/0x120
      [258532.053043] ---[ end trace 8476fce13d9ce20a ]---
    
    Which results in flooding dmesg/syslog since btrfs_drop_extent_cache()
    uses WARN_ON() and not WARN_ON_ONCE().
    
    So fix this issue by changing run_delalloc_nocow()'s loop to move to the
    next extent item when the current extent item ends at at offset less than
    or equals to the current offset instead of the start offset.
    
    Fixes: 80ff385665b7fc ("Btrfs: update nodatacow code v2")
    CC: stable@vger.kernel.org # 4.4+
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f0a1380de70a88f63ebfc2e3756d7f1543d2d68f
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Dec 12 10:32:13 2019 -0800

    6pack,mkiss: fix possible deadlock
    
    commit 5c9934b6767b16ba60be22ec3cbd4379ad64170d upstream.
    
    We got another syzbot report [1] that tells us we must use
    write_lock_irq()/write_unlock_irq() to avoid possible deadlock.
    
    [1]
    
    WARNING: inconsistent lock state
    5.5.0-rc1-syzkaller #0 Not tainted
    --------------------------------
    inconsistent {HARDIRQ-ON-W} -> {IN-HARDIRQ-R} usage.
    syz-executor826/9605 [HC1[1]:SC0[0]:HE0:SE1] takes:
    ffffffff8a128718 (disc_data_lock){+-..}, at: sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
    {HARDIRQ-ON-W} state was registered at:
      lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
      __raw_write_lock_bh include/linux/rwlock_api_smp.h:203 [inline]
      _raw_write_lock_bh+0x33/0x50 kernel/locking/spinlock.c:319
      sixpack_close+0x1d/0x250 drivers/net/hamradio/6pack.c:657
      tty_ldisc_close.isra.0+0x119/0x1a0 drivers/tty/tty_ldisc.c:489
      tty_set_ldisc+0x230/0x6b0 drivers/tty/tty_ldisc.c:585
      tiocsetd drivers/tty/tty_io.c:2337 [inline]
      tty_ioctl+0xe8d/0x14f0 drivers/tty/tty_io.c:2597
      vfs_ioctl fs/ioctl.c:47 [inline]
      file_ioctl fs/ioctl.c:545 [inline]
      do_vfs_ioctl+0x977/0x14e0 fs/ioctl.c:732
      ksys_ioctl+0xab/0xd0 fs/ioctl.c:749
      __do_sys_ioctl fs/ioctl.c:756 [inline]
      __se_sys_ioctl fs/ioctl.c:754 [inline]
      __x64_sys_ioctl+0x73/0xb0 fs/ioctl.c:754
      do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
      entry_SYSCALL_64_after_hwframe+0x49/0xbe
    irq event stamp: 3946
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] __raw_spin_unlock_irq include/linux/spinlock_api_smp.h:168 [inline]
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] _raw_spin_unlock_irq+0x23/0x80 kernel/locking/spinlock.c:199
    hardirqs last disabled at (3946): [<ffffffff8100675f>] trace_hardirqs_off_thunk+0x1a/0x1c arch/x86/entry/thunk_64.S:42
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] spin_unlock_bh include/linux/spinlock.h:383 [inline]
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] clusterip_netdev_event+0x46f/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:222
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] spin_lock_bh include/linux/spinlock.h:343 [inline]
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] clusterip_netdev_event+0x1bb/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:196
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(disc_data_lock);
      <Interrupt>
        lock(disc_data_lock);
    
     *** DEADLOCK ***
    
    5 locks held by syz-executor826/9605:
     #0: ffff8880a905e198 (&tty->legacy_mutex){+.+.}, at: tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     #1: ffffffff899a56c0 (rcu_read_lock){....}, at: mutex_spin_on_owner+0x0/0x330 kernel/locking/mutex.c:413
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: spin_lock include/linux/spinlock.h:338 [inline]
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: serial8250_interrupt+0x2d/0x1a0 drivers/tty/serial/8250/8250_core.c:116
     #3: ffffffff8c104048 (&port_lock_key){-.-.}, at: serial8250_handle_irq.part.0+0x24/0x330 drivers/tty/serial/8250/8250_port.c:1823
     #4: ffff8880a905e090 (&tty->ldisc_sem){++++}, at: tty_ldisc_ref+0x22/0x90 drivers/tty/tty_ldisc.c:288
    
    stack backtrace:
    CPU: 1 PID: 9605 Comm: syz-executor826 Not tainted 5.5.0-rc1-syzkaller #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x197/0x210 lib/dump_stack.c:118
     print_usage_bug.cold+0x327/0x378 kernel/locking/lockdep.c:3101
     valid_state kernel/locking/lockdep.c:3112 [inline]
     mark_lock_irq kernel/locking/lockdep.c:3309 [inline]
     mark_lock+0xbb4/0x1220 kernel/locking/lockdep.c:3666
     mark_usage kernel/locking/lockdep.c:3554 [inline]
     __lock_acquire+0x1e55/0x4a00 kernel/locking/lockdep.c:3909
     lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
     __raw_read_lock include/linux/rwlock_api_smp.h:149 [inline]
     _raw_read_lock+0x32/0x50 kernel/locking/spinlock.c:223
     sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
     sixpack_write_wakeup+0x25/0x340 drivers/net/hamradio/6pack.c:402
     tty_wakeup+0xe9/0x120 drivers/tty/tty_io.c:536
     tty_port_default_wakeup+0x2b/0x40 drivers/tty/tty_port.c:50
     tty_port_tty_wakeup+0x57/0x70 drivers/tty/tty_port.c:387
     uart_write_wakeup+0x46/0x70 drivers/tty/serial/serial_core.c:104
     serial8250_tx_chars+0x495/0xaf0 drivers/tty/serial/8250/8250_port.c:1761
     serial8250_handle_irq.part.0+0x2a2/0x330 drivers/tty/serial/8250/8250_port.c:1834
     serial8250_handle_irq drivers/tty/serial/8250/8250_port.c:1820 [inline]
     serial8250_default_handle_irq+0xc0/0x150 drivers/tty/serial/8250/8250_port.c:1850
     serial8250_interrupt+0xf1/0x1a0 drivers/tty/serial/8250/8250_core.c:126
     __handle_irq_event_percpu+0x15d/0x970 kernel/irq/handle.c:149
     handle_irq_event_percpu+0x74/0x160 kernel/irq/handle.c:189
     handle_irq_event+0xa7/0x134 kernel/irq/handle.c:206
     handle_edge_irq+0x25e/0x8d0 kernel/irq/chip.c:830
     generic_handle_irq_desc include/linux/irqdesc.h:156 [inline]
     do_IRQ+0xde/0x280 arch/x86/kernel/irq.c:250
     common_interrupt+0xf/0xf arch/x86/entry/entry_64.S:607
     </IRQ>
    RIP: 0010:cpu_relax arch/x86/include/asm/processor.h:685 [inline]
    RIP: 0010:mutex_spin_on_owner+0x247/0x330 kernel/locking/mutex.c:579
    Code: c3 be 08 00 00 00 4c 89 e7 e8 e5 06 59 00 4c 89 e0 48 c1 e8 03 42 80 3c 38 00 0f 85 e1 00 00 00 49 8b 04 24 a8 01 75 96 f3 90 <e9> 2f fe ff ff 0f 0b e8 0d 19 09 00 84 c0 0f 85 ff fd ff ff 48 c7
    RSP: 0018:ffffc90001eafa20 EFLAGS: 00000246 ORIG_RAX: ffffffffffffffd7
    RAX: 0000000000000000 RBX: ffff88809fd9e0c0 RCX: 1ffffffff13266dd
    RDX: 0000000000000000 RSI: 0000000000000008 RDI: 0000000000000000
    RBP: ffffc90001eafa60 R08: 1ffff11013d22898 R09: ffffed1013d22899
    R10: ffffed1013d22898 R11: ffff88809e9144c7 R12: ffff8880a905e138
    R13: ffff88809e9144c0 R14: 0000000000000000 R15: dffffc0000000000
     mutex_optimistic_spin kernel/locking/mutex.c:673 [inline]
     __mutex_lock_common kernel/locking/mutex.c:962 [inline]
     __mutex_lock+0x32b/0x13c0 kernel/locking/mutex.c:1106
     mutex_lock_nested+0x16/0x20 kernel/locking/mutex.c:1121
     tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     tty_release+0xb5/0xe90 drivers/tty/tty_io.c:1665
     __fput+0x2ff/0x890 fs/file_table.c:280
     ____fput+0x16/0x20 fs/file_table.c:313
     task_work_run+0x145/0x1c0 kernel/task_work.c:113
     exit_task_work include/linux/task_work.h:22 [inline]
     do_exit+0x8e7/0x2ef0 kernel/exit.c:797
     do_group_exit+0x135/0x360 kernel/exit.c:895
     __do_sys_exit_group kernel/exit.c:906 [inline]
     __se_sys_exit_group kernel/exit.c:904 [inline]
     __x64_sys_exit_group+0x44/0x50 kernel/exit.c:904
     do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x43fef8
    Code: Bad RIP value.
    RSP: 002b:00007ffdb07d2338 EFLAGS: 00000246 ORIG_RAX: 00000000000000e7
    RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 000000000043fef8
    RDX: 0000000000000000 RSI: 000000000000003c RDI: 0000000000000000
    RBP: 00000000004bf730 R08: 00000000000000e7 R09: ffffffffffffffd0
    R10: 00000000004002c8 R11: 0000000000000246 R12: 0000000000000001
    R13: 00000000006d1180 R14: 0000000000000000 R15: 0000000000000000
    
    Fixes: 6e4e2f811bad ("6pack,mkiss: fix lock inconsistency")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9b8e63d0a6e8d39bcbff3d99c8c52dab7771a68f
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Dec 12 10:32:13 2019 -0800

    6pack,mkiss: fix possible deadlock
    
    commit 5c9934b6767b16ba60be22ec3cbd4379ad64170d upstream.
    
    We got another syzbot report [1] that tells us we must use
    write_lock_irq()/write_unlock_irq() to avoid possible deadlock.
    
    [1]
    
    WARNING: inconsistent lock state
    5.5.0-rc1-syzkaller #0 Not tainted
    --------------------------------
    inconsistent {HARDIRQ-ON-W} -> {IN-HARDIRQ-R} usage.
    syz-executor826/9605 [HC1[1]:SC0[0]:HE0:SE1] takes:
    ffffffff8a128718 (disc_data_lock){+-..}, at: sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
    {HARDIRQ-ON-W} state was registered at:
      lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
      __raw_write_lock_bh include/linux/rwlock_api_smp.h:203 [inline]
      _raw_write_lock_bh+0x33/0x50 kernel/locking/spinlock.c:319
      sixpack_close+0x1d/0x250 drivers/net/hamradio/6pack.c:657
      tty_ldisc_close.isra.0+0x119/0x1a0 drivers/tty/tty_ldisc.c:489
      tty_set_ldisc+0x230/0x6b0 drivers/tty/tty_ldisc.c:585
      tiocsetd drivers/tty/tty_io.c:2337 [inline]
      tty_ioctl+0xe8d/0x14f0 drivers/tty/tty_io.c:2597
      vfs_ioctl fs/ioctl.c:47 [inline]
      file_ioctl fs/ioctl.c:545 [inline]
      do_vfs_ioctl+0x977/0x14e0 fs/ioctl.c:732
      ksys_ioctl+0xab/0xd0 fs/ioctl.c:749
      __do_sys_ioctl fs/ioctl.c:756 [inline]
      __se_sys_ioctl fs/ioctl.c:754 [inline]
      __x64_sys_ioctl+0x73/0xb0 fs/ioctl.c:754
      do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
      entry_SYSCALL_64_after_hwframe+0x49/0xbe
    irq event stamp: 3946
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] __raw_spin_unlock_irq include/linux/spinlock_api_smp.h:168 [inline]
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] _raw_spin_unlock_irq+0x23/0x80 kernel/locking/spinlock.c:199
    hardirqs last disabled at (3946): [<ffffffff8100675f>] trace_hardirqs_off_thunk+0x1a/0x1c arch/x86/entry/thunk_64.S:42
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] spin_unlock_bh include/linux/spinlock.h:383 [inline]
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] clusterip_netdev_event+0x46f/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:222
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] spin_lock_bh include/linux/spinlock.h:343 [inline]
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] clusterip_netdev_event+0x1bb/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:196
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(disc_data_lock);
      <Interrupt>
        lock(disc_data_lock);
    
     *** DEADLOCK ***
    
    5 locks held by syz-executor826/9605:
     #0: ffff8880a905e198 (&tty->legacy_mutex){+.+.}, at: tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     #1: ffffffff899a56c0 (rcu_read_lock){....}, at: mutex_spin_on_owner+0x0/0x330 kernel/locking/mutex.c:413
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: spin_lock include/linux/spinlock.h:338 [inline]
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: serial8250_interrupt+0x2d/0x1a0 drivers/tty/serial/8250/8250_core.c:116
     #3: ffffffff8c104048 (&port_lock_key){-.-.}, at: serial8250_handle_irq.part.0+0x24/0x330 drivers/tty/serial/8250/8250_port.c:1823
     #4: ffff8880a905e090 (&tty->ldisc_sem){++++}, at: tty_ldisc_ref+0x22/0x90 drivers/tty/tty_ldisc.c:288
    
    stack backtrace:
    CPU: 1 PID: 9605 Comm: syz-executor826 Not tainted 5.5.0-rc1-syzkaller #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x197/0x210 lib/dump_stack.c:118
     print_usage_bug.cold+0x327/0x378 kernel/locking/lockdep.c:3101
     valid_state kernel/locking/lockdep.c:3112 [inline]
     mark_lock_irq kernel/locking/lockdep.c:3309 [inline]
     mark_lock+0xbb4/0x1220 kernel/locking/lockdep.c:3666
     mark_usage kernel/locking/lockdep.c:3554 [inline]
     __lock_acquire+0x1e55/0x4a00 kernel/locking/lockdep.c:3909
     lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
     __raw_read_lock include/linux/rwlock_api_smp.h:149 [inline]
     _raw_read_lock+0x32/0x50 kernel/locking/spinlock.c:223
     sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
     sixpack_write_wakeup+0x25/0x340 drivers/net/hamradio/6pack.c:402
     tty_wakeup+0xe9/0x120 drivers/tty/tty_io.c:536
     tty_port_default_wakeup+0x2b/0x40 drivers/tty/tty_port.c:50
     tty_port_tty_wakeup+0x57/0x70 drivers/tty/tty_port.c:387
     uart_write_wakeup+0x46/0x70 drivers/tty/serial/serial_core.c:104
     serial8250_tx_chars+0x495/0xaf0 drivers/tty/serial/8250/8250_port.c:1761
     serial8250_handle_irq.part.0+0x2a2/0x330 drivers/tty/serial/8250/8250_port.c:1834
     serial8250_handle_irq drivers/tty/serial/8250/8250_port.c:1820 [inline]
     serial8250_default_handle_irq+0xc0/0x150 drivers/tty/serial/8250/8250_port.c:1850
     serial8250_interrupt+0xf1/0x1a0 drivers/tty/serial/8250/8250_core.c:126
     __handle_irq_event_percpu+0x15d/0x970 kernel/irq/handle.c:149
     handle_irq_event_percpu+0x74/0x160 kernel/irq/handle.c:189
     handle_irq_event+0xa7/0x134 kernel/irq/handle.c:206
     handle_edge_irq+0x25e/0x8d0 kernel/irq/chip.c:830
     generic_handle_irq_desc include/linux/irqdesc.h:156 [inline]
     do_IRQ+0xde/0x280 arch/x86/kernel/irq.c:250
     common_interrupt+0xf/0xf arch/x86/entry/entry_64.S:607
     </IRQ>
    RIP: 0010:cpu_relax arch/x86/include/asm/processor.h:685 [inline]
    RIP: 0010:mutex_spin_on_owner+0x247/0x330 kernel/locking/mutex.c:579
    Code: c3 be 08 00 00 00 4c 89 e7 e8 e5 06 59 00 4c 89 e0 48 c1 e8 03 42 80 3c 38 00 0f 85 e1 00 00 00 49 8b 04 24 a8 01 75 96 f3 90 <e9> 2f fe ff ff 0f 0b e8 0d 19 09 00 84 c0 0f 85 ff fd ff ff 48 c7
    RSP: 0018:ffffc90001eafa20 EFLAGS: 00000246 ORIG_RAX: ffffffffffffffd7
    RAX: 0000000000000000 RBX: ffff88809fd9e0c0 RCX: 1ffffffff13266dd
    RDX: 0000000000000000 RSI: 0000000000000008 RDI: 0000000000000000
    RBP: ffffc90001eafa60 R08: 1ffff11013d22898 R09: ffffed1013d22899
    R10: ffffed1013d22898 R11: ffff88809e9144c7 R12: ffff8880a905e138
    R13: ffff88809e9144c0 R14: 0000000000000000 R15: dffffc0000000000
     mutex_optimistic_spin kernel/locking/mutex.c:673 [inline]
     __mutex_lock_common kernel/locking/mutex.c:962 [inline]
     __mutex_lock+0x32b/0x13c0 kernel/locking/mutex.c:1106
     mutex_lock_nested+0x16/0x20 kernel/locking/mutex.c:1121
     tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     tty_release+0xb5/0xe90 drivers/tty/tty_io.c:1665
     __fput+0x2ff/0x890 fs/file_table.c:280
     ____fput+0x16/0x20 fs/file_table.c:313
     task_work_run+0x145/0x1c0 kernel/task_work.c:113
     exit_task_work include/linux/task_work.h:22 [inline]
     do_exit+0x8e7/0x2ef0 kernel/exit.c:797
     do_group_exit+0x135/0x360 kernel/exit.c:895
     __do_sys_exit_group kernel/exit.c:906 [inline]
     __se_sys_exit_group kernel/exit.c:904 [inline]
     __x64_sys_exit_group+0x44/0x50 kernel/exit.c:904
     do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x43fef8
    Code: Bad RIP value.
    RSP: 002b:00007ffdb07d2338 EFLAGS: 00000246 ORIG_RAX: 00000000000000e7
    RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 000000000043fef8
    RDX: 0000000000000000 RSI: 000000000000003c RDI: 0000000000000000
    RBP: 00000000004bf730 R08: 00000000000000e7 R09: ffffffffffffffd0
    R10: 00000000004002c8 R11: 0000000000000246 R12: 0000000000000001
    R13: 00000000006d1180 R14: 0000000000000000 R15: 0000000000000000
    
    Fixes: 6e4e2f811bad ("6pack,mkiss: fix lock inconsistency")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8b58905f212b4880d94b5b8ae54e5b84e311947d
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Dec 12 10:32:13 2019 -0800

    6pack,mkiss: fix possible deadlock
    
    commit 5c9934b6767b16ba60be22ec3cbd4379ad64170d upstream.
    
    We got another syzbot report [1] that tells us we must use
    write_lock_irq()/write_unlock_irq() to avoid possible deadlock.
    
    [1]
    
    WARNING: inconsistent lock state
    5.5.0-rc1-syzkaller #0 Not tainted
    --------------------------------
    inconsistent {HARDIRQ-ON-W} -> {IN-HARDIRQ-R} usage.
    syz-executor826/9605 [HC1[1]:SC0[0]:HE0:SE1] takes:
    ffffffff8a128718 (disc_data_lock){+-..}, at: sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
    {HARDIRQ-ON-W} state was registered at:
      lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
      __raw_write_lock_bh include/linux/rwlock_api_smp.h:203 [inline]
      _raw_write_lock_bh+0x33/0x50 kernel/locking/spinlock.c:319
      sixpack_close+0x1d/0x250 drivers/net/hamradio/6pack.c:657
      tty_ldisc_close.isra.0+0x119/0x1a0 drivers/tty/tty_ldisc.c:489
      tty_set_ldisc+0x230/0x6b0 drivers/tty/tty_ldisc.c:585
      tiocsetd drivers/tty/tty_io.c:2337 [inline]
      tty_ioctl+0xe8d/0x14f0 drivers/tty/tty_io.c:2597
      vfs_ioctl fs/ioctl.c:47 [inline]
      file_ioctl fs/ioctl.c:545 [inline]
      do_vfs_ioctl+0x977/0x14e0 fs/ioctl.c:732
      ksys_ioctl+0xab/0xd0 fs/ioctl.c:749
      __do_sys_ioctl fs/ioctl.c:756 [inline]
      __se_sys_ioctl fs/ioctl.c:754 [inline]
      __x64_sys_ioctl+0x73/0xb0 fs/ioctl.c:754
      do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
      entry_SYSCALL_64_after_hwframe+0x49/0xbe
    irq event stamp: 3946
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] __raw_spin_unlock_irq include/linux/spinlock_api_smp.h:168 [inline]
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] _raw_spin_unlock_irq+0x23/0x80 kernel/locking/spinlock.c:199
    hardirqs last disabled at (3946): [<ffffffff8100675f>] trace_hardirqs_off_thunk+0x1a/0x1c arch/x86/entry/thunk_64.S:42
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] spin_unlock_bh include/linux/spinlock.h:383 [inline]
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] clusterip_netdev_event+0x46f/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:222
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] spin_lock_bh include/linux/spinlock.h:343 [inline]
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] clusterip_netdev_event+0x1bb/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:196
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(disc_data_lock);
      <Interrupt>
        lock(disc_data_lock);
    
     *** DEADLOCK ***
    
    5 locks held by syz-executor826/9605:
     #0: ffff8880a905e198 (&tty->legacy_mutex){+.+.}, at: tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     #1: ffffffff899a56c0 (rcu_read_lock){....}, at: mutex_spin_on_owner+0x0/0x330 kernel/locking/mutex.c:413
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: spin_lock include/linux/spinlock.h:338 [inline]
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: serial8250_interrupt+0x2d/0x1a0 drivers/tty/serial/8250/8250_core.c:116
     #3: ffffffff8c104048 (&port_lock_key){-.-.}, at: serial8250_handle_irq.part.0+0x24/0x330 drivers/tty/serial/8250/8250_port.c:1823
     #4: ffff8880a905e090 (&tty->ldisc_sem){++++}, at: tty_ldisc_ref+0x22/0x90 drivers/tty/tty_ldisc.c:288
    
    stack backtrace:
    CPU: 1 PID: 9605 Comm: syz-executor826 Not tainted 5.5.0-rc1-syzkaller #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x197/0x210 lib/dump_stack.c:118
     print_usage_bug.cold+0x327/0x378 kernel/locking/lockdep.c:3101
     valid_state kernel/locking/lockdep.c:3112 [inline]
     mark_lock_irq kernel/locking/lockdep.c:3309 [inline]
     mark_lock+0xbb4/0x1220 kernel/locking/lockdep.c:3666
     mark_usage kernel/locking/lockdep.c:3554 [inline]
     __lock_acquire+0x1e55/0x4a00 kernel/locking/lockdep.c:3909
     lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
     __raw_read_lock include/linux/rwlock_api_smp.h:149 [inline]
     _raw_read_lock+0x32/0x50 kernel/locking/spinlock.c:223
     sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
     sixpack_write_wakeup+0x25/0x340 drivers/net/hamradio/6pack.c:402
     tty_wakeup+0xe9/0x120 drivers/tty/tty_io.c:536
     tty_port_default_wakeup+0x2b/0x40 drivers/tty/tty_port.c:50
     tty_port_tty_wakeup+0x57/0x70 drivers/tty/tty_port.c:387
     uart_write_wakeup+0x46/0x70 drivers/tty/serial/serial_core.c:104
     serial8250_tx_chars+0x495/0xaf0 drivers/tty/serial/8250/8250_port.c:1761
     serial8250_handle_irq.part.0+0x2a2/0x330 drivers/tty/serial/8250/8250_port.c:1834
     serial8250_handle_irq drivers/tty/serial/8250/8250_port.c:1820 [inline]
     serial8250_default_handle_irq+0xc0/0x150 drivers/tty/serial/8250/8250_port.c:1850
     serial8250_interrupt+0xf1/0x1a0 drivers/tty/serial/8250/8250_core.c:126
     __handle_irq_event_percpu+0x15d/0x970 kernel/irq/handle.c:149
     handle_irq_event_percpu+0x74/0x160 kernel/irq/handle.c:189
     handle_irq_event+0xa7/0x134 kernel/irq/handle.c:206
     handle_edge_irq+0x25e/0x8d0 kernel/irq/chip.c:830
     generic_handle_irq_desc include/linux/irqdesc.h:156 [inline]
     do_IRQ+0xde/0x280 arch/x86/kernel/irq.c:250
     common_interrupt+0xf/0xf arch/x86/entry/entry_64.S:607
     </IRQ>
    RIP: 0010:cpu_relax arch/x86/include/asm/processor.h:685 [inline]
    RIP: 0010:mutex_spin_on_owner+0x247/0x330 kernel/locking/mutex.c:579
    Code: c3 be 08 00 00 00 4c 89 e7 e8 e5 06 59 00 4c 89 e0 48 c1 e8 03 42 80 3c 38 00 0f 85 e1 00 00 00 49 8b 04 24 a8 01 75 96 f3 90 <e9> 2f fe ff ff 0f 0b e8 0d 19 09 00 84 c0 0f 85 ff fd ff ff 48 c7
    RSP: 0018:ffffc90001eafa20 EFLAGS: 00000246 ORIG_RAX: ffffffffffffffd7
    RAX: 0000000000000000 RBX: ffff88809fd9e0c0 RCX: 1ffffffff13266dd
    RDX: 0000000000000000 RSI: 0000000000000008 RDI: 0000000000000000
    RBP: ffffc90001eafa60 R08: 1ffff11013d22898 R09: ffffed1013d22899
    R10: ffffed1013d22898 R11: ffff88809e9144c7 R12: ffff8880a905e138
    R13: ffff88809e9144c0 R14: 0000000000000000 R15: dffffc0000000000
     mutex_optimistic_spin kernel/locking/mutex.c:673 [inline]
     __mutex_lock_common kernel/locking/mutex.c:962 [inline]
     __mutex_lock+0x32b/0x13c0 kernel/locking/mutex.c:1106
     mutex_lock_nested+0x16/0x20 kernel/locking/mutex.c:1121
     tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     tty_release+0xb5/0xe90 drivers/tty/tty_io.c:1665
     __fput+0x2ff/0x890 fs/file_table.c:280
     ____fput+0x16/0x20 fs/file_table.c:313
     task_work_run+0x145/0x1c0 kernel/task_work.c:113
     exit_task_work include/linux/task_work.h:22 [inline]
     do_exit+0x8e7/0x2ef0 kernel/exit.c:797
     do_group_exit+0x135/0x360 kernel/exit.c:895
     __do_sys_exit_group kernel/exit.c:906 [inline]
     __se_sys_exit_group kernel/exit.c:904 [inline]
     __x64_sys_exit_group+0x44/0x50 kernel/exit.c:904
     do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x43fef8
    Code: Bad RIP value.
    RSP: 002b:00007ffdb07d2338 EFLAGS: 00000246 ORIG_RAX: 00000000000000e7
    RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 000000000043fef8
    RDX: 0000000000000000 RSI: 000000000000003c RDI: 0000000000000000
    RBP: 00000000004bf730 R08: 00000000000000e7 R09: ffffffffffffffd0
    R10: 00000000004002c8 R11: 0000000000000246 R12: 0000000000000001
    R13: 00000000006d1180 R14: 0000000000000000 R15: 0000000000000000
    
    Fixes: 6e4e2f811bad ("6pack,mkiss: fix lock inconsistency")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c8c17adc7ea8f0ed28c20d49acf07a371acf1b5b
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Dec 12 10:32:13 2019 -0800

    6pack,mkiss: fix possible deadlock
    
    commit 5c9934b6767b16ba60be22ec3cbd4379ad64170d upstream.
    
    We got another syzbot report [1] that tells us we must use
    write_lock_irq()/write_unlock_irq() to avoid possible deadlock.
    
    [1]
    
    WARNING: inconsistent lock state
    5.5.0-rc1-syzkaller #0 Not tainted
    --------------------------------
    inconsistent {HARDIRQ-ON-W} -> {IN-HARDIRQ-R} usage.
    syz-executor826/9605 [HC1[1]:SC0[0]:HE0:SE1] takes:
    ffffffff8a128718 (disc_data_lock){+-..}, at: sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
    {HARDIRQ-ON-W} state was registered at:
      lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
      __raw_write_lock_bh include/linux/rwlock_api_smp.h:203 [inline]
      _raw_write_lock_bh+0x33/0x50 kernel/locking/spinlock.c:319
      sixpack_close+0x1d/0x250 drivers/net/hamradio/6pack.c:657
      tty_ldisc_close.isra.0+0x119/0x1a0 drivers/tty/tty_ldisc.c:489
      tty_set_ldisc+0x230/0x6b0 drivers/tty/tty_ldisc.c:585
      tiocsetd drivers/tty/tty_io.c:2337 [inline]
      tty_ioctl+0xe8d/0x14f0 drivers/tty/tty_io.c:2597
      vfs_ioctl fs/ioctl.c:47 [inline]
      file_ioctl fs/ioctl.c:545 [inline]
      do_vfs_ioctl+0x977/0x14e0 fs/ioctl.c:732
      ksys_ioctl+0xab/0xd0 fs/ioctl.c:749
      __do_sys_ioctl fs/ioctl.c:756 [inline]
      __se_sys_ioctl fs/ioctl.c:754 [inline]
      __x64_sys_ioctl+0x73/0xb0 fs/ioctl.c:754
      do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
      entry_SYSCALL_64_after_hwframe+0x49/0xbe
    irq event stamp: 3946
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] __raw_spin_unlock_irq include/linux/spinlock_api_smp.h:168 [inline]
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] _raw_spin_unlock_irq+0x23/0x80 kernel/locking/spinlock.c:199
    hardirqs last disabled at (3946): [<ffffffff8100675f>] trace_hardirqs_off_thunk+0x1a/0x1c arch/x86/entry/thunk_64.S:42
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] spin_unlock_bh include/linux/spinlock.h:383 [inline]
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] clusterip_netdev_event+0x46f/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:222
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] spin_lock_bh include/linux/spinlock.h:343 [inline]
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] clusterip_netdev_event+0x1bb/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:196
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(disc_data_lock);
      <Interrupt>
        lock(disc_data_lock);
    
     *** DEADLOCK ***
    
    5 locks held by syz-executor826/9605:
     #0: ffff8880a905e198 (&tty->legacy_mutex){+.+.}, at: tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     #1: ffffffff899a56c0 (rcu_read_lock){....}, at: mutex_spin_on_owner+0x0/0x330 kernel/locking/mutex.c:413
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: spin_lock include/linux/spinlock.h:338 [inline]
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: serial8250_interrupt+0x2d/0x1a0 drivers/tty/serial/8250/8250_core.c:116
     #3: ffffffff8c104048 (&port_lock_key){-.-.}, at: serial8250_handle_irq.part.0+0x24/0x330 drivers/tty/serial/8250/8250_port.c:1823
     #4: ffff8880a905e090 (&tty->ldisc_sem){++++}, at: tty_ldisc_ref+0x22/0x90 drivers/tty/tty_ldisc.c:288
    
    stack backtrace:
    CPU: 1 PID: 9605 Comm: syz-executor826 Not tainted 5.5.0-rc1-syzkaller #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x197/0x210 lib/dump_stack.c:118
     print_usage_bug.cold+0x327/0x378 kernel/locking/lockdep.c:3101
     valid_state kernel/locking/lockdep.c:3112 [inline]
     mark_lock_irq kernel/locking/lockdep.c:3309 [inline]
     mark_lock+0xbb4/0x1220 kernel/locking/lockdep.c:3666
     mark_usage kernel/locking/lockdep.c:3554 [inline]
     __lock_acquire+0x1e55/0x4a00 kernel/locking/lockdep.c:3909
     lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
     __raw_read_lock include/linux/rwlock_api_smp.h:149 [inline]
     _raw_read_lock+0x32/0x50 kernel/locking/spinlock.c:223
     sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
     sixpack_write_wakeup+0x25/0x340 drivers/net/hamradio/6pack.c:402
     tty_wakeup+0xe9/0x120 drivers/tty/tty_io.c:536
     tty_port_default_wakeup+0x2b/0x40 drivers/tty/tty_port.c:50
     tty_port_tty_wakeup+0x57/0x70 drivers/tty/tty_port.c:387
     uart_write_wakeup+0x46/0x70 drivers/tty/serial/serial_core.c:104
     serial8250_tx_chars+0x495/0xaf0 drivers/tty/serial/8250/8250_port.c:1761
     serial8250_handle_irq.part.0+0x2a2/0x330 drivers/tty/serial/8250/8250_port.c:1834
     serial8250_handle_irq drivers/tty/serial/8250/8250_port.c:1820 [inline]
     serial8250_default_handle_irq+0xc0/0x150 drivers/tty/serial/8250/8250_port.c:1850
     serial8250_interrupt+0xf1/0x1a0 drivers/tty/serial/8250/8250_core.c:126
     __handle_irq_event_percpu+0x15d/0x970 kernel/irq/handle.c:149
     handle_irq_event_percpu+0x74/0x160 kernel/irq/handle.c:189
     handle_irq_event+0xa7/0x134 kernel/irq/handle.c:206
     handle_edge_irq+0x25e/0x8d0 kernel/irq/chip.c:830
     generic_handle_irq_desc include/linux/irqdesc.h:156 [inline]
     do_IRQ+0xde/0x280 arch/x86/kernel/irq.c:250
     common_interrupt+0xf/0xf arch/x86/entry/entry_64.S:607
     </IRQ>
    RIP: 0010:cpu_relax arch/x86/include/asm/processor.h:685 [inline]
    RIP: 0010:mutex_spin_on_owner+0x247/0x330 kernel/locking/mutex.c:579
    Code: c3 be 08 00 00 00 4c 89 e7 e8 e5 06 59 00 4c 89 e0 48 c1 e8 03 42 80 3c 38 00 0f 85 e1 00 00 00 49 8b 04 24 a8 01 75 96 f3 90 <e9> 2f fe ff ff 0f 0b e8 0d 19 09 00 84 c0 0f 85 ff fd ff ff 48 c7
    RSP: 0018:ffffc90001eafa20 EFLAGS: 00000246 ORIG_RAX: ffffffffffffffd7
    RAX: 0000000000000000 RBX: ffff88809fd9e0c0 RCX: 1ffffffff13266dd
    RDX: 0000000000000000 RSI: 0000000000000008 RDI: 0000000000000000
    RBP: ffffc90001eafa60 R08: 1ffff11013d22898 R09: ffffed1013d22899
    R10: ffffed1013d22898 R11: ffff88809e9144c7 R12: ffff8880a905e138
    R13: ffff88809e9144c0 R14: 0000000000000000 R15: dffffc0000000000
     mutex_optimistic_spin kernel/locking/mutex.c:673 [inline]
     __mutex_lock_common kernel/locking/mutex.c:962 [inline]
     __mutex_lock+0x32b/0x13c0 kernel/locking/mutex.c:1106
     mutex_lock_nested+0x16/0x20 kernel/locking/mutex.c:1121
     tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     tty_release+0xb5/0xe90 drivers/tty/tty_io.c:1665
     __fput+0x2ff/0x890 fs/file_table.c:280
     ____fput+0x16/0x20 fs/file_table.c:313
     task_work_run+0x145/0x1c0 kernel/task_work.c:113
     exit_task_work include/linux/task_work.h:22 [inline]
     do_exit+0x8e7/0x2ef0 kernel/exit.c:797
     do_group_exit+0x135/0x360 kernel/exit.c:895
     __do_sys_exit_group kernel/exit.c:906 [inline]
     __se_sys_exit_group kernel/exit.c:904 [inline]
     __x64_sys_exit_group+0x44/0x50 kernel/exit.c:904
     do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x43fef8
    Code: Bad RIP value.
    RSP: 002b:00007ffdb07d2338 EFLAGS: 00000246 ORIG_RAX: 00000000000000e7
    RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 000000000043fef8
    RDX: 0000000000000000 RSI: 000000000000003c RDI: 0000000000000000
    RBP: 00000000004bf730 R08: 00000000000000e7 R09: ffffffffffffffd0
    R10: 00000000004002c8 R11: 0000000000000246 R12: 0000000000000001
    R13: 00000000006d1180 R14: 0000000000000000 R15: 0000000000000000
    
    Fixes: 6e4e2f811bad ("6pack,mkiss: fix lock inconsistency")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 521b00fe8fafaf444f895882b3a2b3f912133fc8
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Dec 12 10:32:13 2019 -0800

    6pack,mkiss: fix possible deadlock
    
    commit 5c9934b6767b16ba60be22ec3cbd4379ad64170d upstream.
    
    We got another syzbot report [1] that tells us we must use
    write_lock_irq()/write_unlock_irq() to avoid possible deadlock.
    
    [1]
    
    WARNING: inconsistent lock state
    5.5.0-rc1-syzkaller #0 Not tainted
    --------------------------------
    inconsistent {HARDIRQ-ON-W} -> {IN-HARDIRQ-R} usage.
    syz-executor826/9605 [HC1[1]:SC0[0]:HE0:SE1] takes:
    ffffffff8a128718 (disc_data_lock){+-..}, at: sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
    {HARDIRQ-ON-W} state was registered at:
      lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
      __raw_write_lock_bh include/linux/rwlock_api_smp.h:203 [inline]
      _raw_write_lock_bh+0x33/0x50 kernel/locking/spinlock.c:319
      sixpack_close+0x1d/0x250 drivers/net/hamradio/6pack.c:657
      tty_ldisc_close.isra.0+0x119/0x1a0 drivers/tty/tty_ldisc.c:489
      tty_set_ldisc+0x230/0x6b0 drivers/tty/tty_ldisc.c:585
      tiocsetd drivers/tty/tty_io.c:2337 [inline]
      tty_ioctl+0xe8d/0x14f0 drivers/tty/tty_io.c:2597
      vfs_ioctl fs/ioctl.c:47 [inline]
      file_ioctl fs/ioctl.c:545 [inline]
      do_vfs_ioctl+0x977/0x14e0 fs/ioctl.c:732
      ksys_ioctl+0xab/0xd0 fs/ioctl.c:749
      __do_sys_ioctl fs/ioctl.c:756 [inline]
      __se_sys_ioctl fs/ioctl.c:754 [inline]
      __x64_sys_ioctl+0x73/0xb0 fs/ioctl.c:754
      do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
      entry_SYSCALL_64_after_hwframe+0x49/0xbe
    irq event stamp: 3946
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] __raw_spin_unlock_irq include/linux/spinlock_api_smp.h:168 [inline]
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] _raw_spin_unlock_irq+0x23/0x80 kernel/locking/spinlock.c:199
    hardirqs last disabled at (3946): [<ffffffff8100675f>] trace_hardirqs_off_thunk+0x1a/0x1c arch/x86/entry/thunk_64.S:42
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] spin_unlock_bh include/linux/spinlock.h:383 [inline]
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] clusterip_netdev_event+0x46f/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:222
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] spin_lock_bh include/linux/spinlock.h:343 [inline]
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] clusterip_netdev_event+0x1bb/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:196
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(disc_data_lock);
      <Interrupt>
        lock(disc_data_lock);
    
     *** DEADLOCK ***
    
    5 locks held by syz-executor826/9605:
     #0: ffff8880a905e198 (&tty->legacy_mutex){+.+.}, at: tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     #1: ffffffff899a56c0 (rcu_read_lock){....}, at: mutex_spin_on_owner+0x0/0x330 kernel/locking/mutex.c:413
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: spin_lock include/linux/spinlock.h:338 [inline]
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: serial8250_interrupt+0x2d/0x1a0 drivers/tty/serial/8250/8250_core.c:116
     #3: ffffffff8c104048 (&port_lock_key){-.-.}, at: serial8250_handle_irq.part.0+0x24/0x330 drivers/tty/serial/8250/8250_port.c:1823
     #4: ffff8880a905e090 (&tty->ldisc_sem){++++}, at: tty_ldisc_ref+0x22/0x90 drivers/tty/tty_ldisc.c:288
    
    stack backtrace:
    CPU: 1 PID: 9605 Comm: syz-executor826 Not tainted 5.5.0-rc1-syzkaller #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x197/0x210 lib/dump_stack.c:118
     print_usage_bug.cold+0x327/0x378 kernel/locking/lockdep.c:3101
     valid_state kernel/locking/lockdep.c:3112 [inline]
     mark_lock_irq kernel/locking/lockdep.c:3309 [inline]
     mark_lock+0xbb4/0x1220 kernel/locking/lockdep.c:3666
     mark_usage kernel/locking/lockdep.c:3554 [inline]
     __lock_acquire+0x1e55/0x4a00 kernel/locking/lockdep.c:3909
     lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
     __raw_read_lock include/linux/rwlock_api_smp.h:149 [inline]
     _raw_read_lock+0x32/0x50 kernel/locking/spinlock.c:223
     sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
     sixpack_write_wakeup+0x25/0x340 drivers/net/hamradio/6pack.c:402
     tty_wakeup+0xe9/0x120 drivers/tty/tty_io.c:536
     tty_port_default_wakeup+0x2b/0x40 drivers/tty/tty_port.c:50
     tty_port_tty_wakeup+0x57/0x70 drivers/tty/tty_port.c:387
     uart_write_wakeup+0x46/0x70 drivers/tty/serial/serial_core.c:104
     serial8250_tx_chars+0x495/0xaf0 drivers/tty/serial/8250/8250_port.c:1761
     serial8250_handle_irq.part.0+0x2a2/0x330 drivers/tty/serial/8250/8250_port.c:1834
     serial8250_handle_irq drivers/tty/serial/8250/8250_port.c:1820 [inline]
     serial8250_default_handle_irq+0xc0/0x150 drivers/tty/serial/8250/8250_port.c:1850
     serial8250_interrupt+0xf1/0x1a0 drivers/tty/serial/8250/8250_core.c:126
     __handle_irq_event_percpu+0x15d/0x970 kernel/irq/handle.c:149
     handle_irq_event_percpu+0x74/0x160 kernel/irq/handle.c:189
     handle_irq_event+0xa7/0x134 kernel/irq/handle.c:206
     handle_edge_irq+0x25e/0x8d0 kernel/irq/chip.c:830
     generic_handle_irq_desc include/linux/irqdesc.h:156 [inline]
     do_IRQ+0xde/0x280 arch/x86/kernel/irq.c:250
     common_interrupt+0xf/0xf arch/x86/entry/entry_64.S:607
     </IRQ>
    RIP: 0010:cpu_relax arch/x86/include/asm/processor.h:685 [inline]
    RIP: 0010:mutex_spin_on_owner+0x247/0x330 kernel/locking/mutex.c:579
    Code: c3 be 08 00 00 00 4c 89 e7 e8 e5 06 59 00 4c 89 e0 48 c1 e8 03 42 80 3c 38 00 0f 85 e1 00 00 00 49 8b 04 24 a8 01 75 96 f3 90 <e9> 2f fe ff ff 0f 0b e8 0d 19 09 00 84 c0 0f 85 ff fd ff ff 48 c7
    RSP: 0018:ffffc90001eafa20 EFLAGS: 00000246 ORIG_RAX: ffffffffffffffd7
    RAX: 0000000000000000 RBX: ffff88809fd9e0c0 RCX: 1ffffffff13266dd
    RDX: 0000000000000000 RSI: 0000000000000008 RDI: 0000000000000000
    RBP: ffffc90001eafa60 R08: 1ffff11013d22898 R09: ffffed1013d22899
    R10: ffffed1013d22898 R11: ffff88809e9144c7 R12: ffff8880a905e138
    R13: ffff88809e9144c0 R14: 0000000000000000 R15: dffffc0000000000
     mutex_optimistic_spin kernel/locking/mutex.c:673 [inline]
     __mutex_lock_common kernel/locking/mutex.c:962 [inline]
     __mutex_lock+0x32b/0x13c0 kernel/locking/mutex.c:1106
     mutex_lock_nested+0x16/0x20 kernel/locking/mutex.c:1121
     tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     tty_release+0xb5/0xe90 drivers/tty/tty_io.c:1665
     __fput+0x2ff/0x890 fs/file_table.c:280
     ____fput+0x16/0x20 fs/file_table.c:313
     task_work_run+0x145/0x1c0 kernel/task_work.c:113
     exit_task_work include/linux/task_work.h:22 [inline]
     do_exit+0x8e7/0x2ef0 kernel/exit.c:797
     do_group_exit+0x135/0x360 kernel/exit.c:895
     __do_sys_exit_group kernel/exit.c:906 [inline]
     __se_sys_exit_group kernel/exit.c:904 [inline]
     __x64_sys_exit_group+0x44/0x50 kernel/exit.c:904
     do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x43fef8
    Code: Bad RIP value.
    RSP: 002b:00007ffdb07d2338 EFLAGS: 00000246 ORIG_RAX: 00000000000000e7
    RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 000000000043fef8
    RDX: 0000000000000000 RSI: 000000000000003c RDI: 0000000000000000
    RBP: 00000000004bf730 R08: 00000000000000e7 R09: ffffffffffffffd0
    R10: 00000000004002c8 R11: 0000000000000246 R12: 0000000000000001
    R13: 00000000006d1180 R14: 0000000000000000 R15: 0000000000000000
    
    Fixes: 6e4e2f811bad ("6pack,mkiss: fix lock inconsistency")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit de7999afedff02c6631feab3ea726a0e8f8c3d40
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Dec 11 09:01:40 2019 +0000

    Btrfs: fix infinite loop during nocow writeback due to race
    
    When starting writeback for a range that covers part of a preallocated
    extent, due to a race with writeback for another range that also covers
    another part of the same preallocated extent, we can end up in an infinite
    loop.
    
    Consider the following example where for inode 280 we have two dirty
    ranges:
    
      range A, from 294912 to 303103, 8192 bytes
      range B, from 348160 to 438271, 90112 bytes
    
    and we have the following file extent item layout for our inode:
    
      leaf 38895616 gen 24544 total ptrs 29 free space 13820 owner 5
          (...)
          item 27 key (280 108 200704) itemoff 14598 itemsize 53
              extent data disk bytenr 0 nr 0 type 1 (regular)
              extent data offset 0 nr 94208 ram 94208
          item 28 key (280 108 294912) itemoff 14545 itemsize 53
              extent data disk bytenr 10433052672 nr 81920 type 2 (prealloc)
              extent data offset 0 nr 81920 ram 81920
    
    Then the following happens:
    
    1) Writeback starts for range B (from 348160 to 438271), execution of
       run_delalloc_nocow() starts;
    
    2) The first iteration of run_delalloc_nocow()'s whil loop leaves us at
       the extent item at slot 28, pointing to the prealloc extent item
       covering the range from 294912 to 376831. This extent covers part of
       our range;
    
    3) An ordered extent is created against that extent, covering the file
       range from 348160 to 376831 (28672 bytes);
    
    4) We adjust 'cur_offset' to 376832 and move on to the next iteration of
       the while loop;
    
    5) The call to btrfs_lookup_file_extent() leaves us at the same leaf,
       pointing to slot 29, 1 slot after the last item (the extent item
       we processed in the previous iteration);
    
    6) Because we are a slot beyond the last item, we call btrfs_next_leaf(),
       which releases the search path before doing a another search for the
       last key of the leaf (280 108 294912);
    
    7) Right after btrfs_next_leaf() released the path, and before it did
       another search for the last key of the leaf, writeback for the range
       A (from 294912 to 303103) completes (it was previously started at
       some point);
    
    8) Upon completion of the ordered extent for range A, the prealloc extent
       we previously found got split into two extent items, one covering the
       range from 294912 to 303103 (8192 bytes), with a type of regular extent
       (and no longer prealloc) and another covering the range from 303104 to
       376831 (73728 bytes), with a type of prealloc and an offset of 8192
       bytes. So our leaf now has the following layout:
    
         leaf 38895616 gen 24544 total ptrs 31 free space 13664 owner 5
             (...)
             item 27 key (280 108 200704) itemoff 14598 itemsize 53
                 extent data disk bytenr 0 nr 0 type 1
                 extent data offset 0 nr 8192 ram 94208
             item 28 key (280 108 208896) itemoff 14545 itemsize 53
                 extent data disk bytenr 10433142784 nr 86016 type 1
                 extent data offset 0 nr 86016 ram 86016
             item 29 key (280 108 294912) itemoff 14492 itemsize 53
                 extent data disk bytenr 10433052672 nr 81920 type 1
                 extent data offset 0 nr 8192 ram 81920
             item 30 key (280 108 303104) itemoff 14439 itemsize 53
                 extent data disk bytenr 10433052672 nr 81920 type 2
                 extent data offset 8192 nr 73728 ram 81920
    
    9) After btrfs_next_leaf() returns, we have our path pointing to that same
       leaf and at slot 30, since it has a key we didn't have before and it's
       the first key greater then the key that was previously the last key of
       the leaf (key (280 108 294912));
    
    10) The extent item at slot 30 covers the range from 303104 to 376831
        which is in our target range, so we process it, despite having already
        created an ordered extent against this extent for the file range from
        348160 to 376831. This is because we skip to the next extent item only
        if its end is less than or equals to the start of our delalloc range,
        and not less than or equals to the current offset ('cur_offset');
    
    11) As a result we compute 'num_bytes' as:
    
        num_bytes = min(end + 1, extent_end) - cur_offset;
                  = min(438271 + 1, 376832) - 376832 = 0
    
    12) We then call create_io_em() for a 0 bytes range starting at offset
        376832;
    
    13) Then create_io_em() enters an infinite loop because its calls to
        btrfs_drop_extent_cache() do nothing due to the 0 length range
        passed to it. So no existing extent maps that cover the offset
        376832 get removed, and therefore calls to add_extent_mapping()
        return -EEXIST, resulting in an infinite loop. This loop from
        create_io_em() is the following:
    
        do {
            btrfs_drop_extent_cache(BTRFS_I(inode), em->start,
                                    em->start + em->len - 1, 0);
            write_lock(&em_tree->lock);
            ret = add_extent_mapping(em_tree, em, 1);
            write_unlock(&em_tree->lock);
            /*
             * The caller has taken lock_extent(), who could race with us
             * to add em?
             */
        } while (ret == -EEXIST);
    
    Also, each call to btrfs_drop_extent_cache() triggers a warning because
    the start offset passed to it (376832) is smaller then the end offset
    (376832 - 1) passed to it by -1, due to the 0 length:
    
      [258532.052621] ------------[ cut here ]------------
      [258532.052643] WARNING: CPU: 0 PID: 9987 at fs/btrfs/file.c:602 btrfs_drop_extent_cache+0x3f4/0x590 [btrfs]
      (...)
      [258532.052672] CPU: 0 PID: 9987 Comm: fsx Tainted: G        W         5.4.0-rc7-btrfs-next-64 #1
      [258532.052673] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.0-0-ga698c8995f-prebuilt.qemu.org 04/01/2014
      [258532.052691] RIP: 0010:btrfs_drop_extent_cache+0x3f4/0x590 [btrfs]
      (...)
      [258532.052695] RSP: 0018:ffffb4be0153f860 EFLAGS: 00010287
      [258532.052700] RAX: ffff975b445ee360 RBX: ffff975b44eb3e08 RCX: 0000000000000000
      [258532.052700] RDX: 0000000000038fff RSI: 0000000000039000 RDI: ffff975b445ee308
      [258532.052700] RBP: 0000000000038fff R08: 0000000000000000 R09: 0000000000000001
      [258532.052701] R10: ffff975b513c5c10 R11: 00000000e3c0cfa9 R12: 0000000000039000
      [258532.052703] R13: ffff975b445ee360 R14: 00000000ffffffef R15: ffff975b445ee308
      [258532.052705] FS:  00007f86a821de80(0000) GS:ffff975b76a00000(0000) knlGS:0000000000000000
      [258532.052707] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      [258532.052708] CR2: 00007fdacf0f3ab4 CR3: 00000001f9d26002 CR4: 00000000003606f0
      [258532.052712] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      [258532.052717] DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      [258532.052717] Call Trace:
      [258532.052718]  ? preempt_schedule_common+0x32/0x70
      [258532.052722]  ? ___preempt_schedule+0x16/0x20
      [258532.052741]  create_io_em+0xff/0x180 [btrfs]
      [258532.052767]  run_delalloc_nocow+0x942/0xb10 [btrfs]
      [258532.052791]  btrfs_run_delalloc_range+0x30b/0x520 [btrfs]
      [258532.052812]  ? find_lock_delalloc_range+0x221/0x250 [btrfs]
      [258532.052834]  writepage_delalloc+0xe4/0x140 [btrfs]
      [258532.052855]  __extent_writepage+0x110/0x4e0 [btrfs]
      [258532.052876]  extent_write_cache_pages+0x21c/0x480 [btrfs]
      [258532.052906]  extent_writepages+0x52/0xb0 [btrfs]
      [258532.052911]  do_writepages+0x23/0x80
      [258532.052915]  __filemap_fdatawrite_range+0xd2/0x110
      [258532.052938]  btrfs_fdatawrite_range+0x1b/0x50 [btrfs]
      [258532.052954]  start_ordered_ops+0x57/0xa0 [btrfs]
      [258532.052973]  ? btrfs_sync_file+0x225/0x490 [btrfs]
      [258532.052988]  btrfs_sync_file+0x225/0x490 [btrfs]
      [258532.052997]  __x64_sys_msync+0x199/0x200
      [258532.053004]  do_syscall_64+0x5c/0x250
      [258532.053007]  entry_SYSCALL_64_after_hwframe+0x49/0xbe
      [258532.053010] RIP: 0033:0x7f86a7dfd760
      (...)
      [258532.053014] RSP: 002b:00007ffd99af0368 EFLAGS: 00000246 ORIG_RAX: 000000000000001a
      [258532.053016] RAX: ffffffffffffffda RBX: 0000000000000ec9 RCX: 00007f86a7dfd760
      [258532.053017] RDX: 0000000000000004 RSI: 000000000000836c RDI: 00007f86a8221000
      [258532.053019] RBP: 0000000000021ec9 R08: 0000000000000003 R09: 00007f86a812037c
      [258532.053020] R10: 0000000000000001 R11: 0000000000000246 R12: 00000000000074a3
      [258532.053021] R13: 00007f86a8221000 R14: 000000000000836c R15: 0000000000000001
      [258532.053032] irq event stamp: 1653450494
      [258532.053035] hardirqs last  enabled at (1653450493): [<ffffffff9dec69f9>] _raw_spin_unlock_irq+0x29/0x50
      [258532.053037] hardirqs last disabled at (1653450494): [<ffffffff9d4048ea>] trace_hardirqs_off_thunk+0x1a/0x20
      [258532.053039] softirqs last  enabled at (1653449852): [<ffffffff9e200466>] __do_softirq+0x466/0x6bd
      [258532.053042] softirqs last disabled at (1653449845): [<ffffffff9d4c8a0c>] irq_exit+0xec/0x120
      [258532.053043] ---[ end trace 8476fce13d9ce20a ]---
    
    Which results in flooding dmesg/syslog since btrfs_drop_extent_cache()
    uses WARN_ON() and not WARN_ON_ONCE().
    
    So fix this issue by changing run_delalloc_nocow()'s loop to move to the
    next extent item when the current extent item ends at at offset less than
    or equals to the current offset instead of the start offset.
    
    Fixes: 80ff385665b7fc ("Btrfs: update nodatacow code v2")
    CC: stable@vger.kernel.org # 4.4+
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit a6706447f31916c4335a398c5384ca47f4e46661
Author: Hans Verkuil <hverkuil-cisco@xs4all.nl>
Date:   Wed Dec 11 17:22:24 2019 +0100

    media: pulse8-cec: locking improvements
    
    Drop the write_lock, rename config_lock to plain lock since this
    now locks access to the adapter. Use 'lock' when transmitting
    a message, ensuring that nothing interferes with the transmit.
    
    Signed-off-by: Hans Verkuil <hverkuil-cisco@xs4all.nl>
    Signed-off-by: Mauro Carvalho Chehab <mchehab+huawei@kernel.org>

commit 5c9934b6767b16ba60be22ec3cbd4379ad64170d
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Dec 12 10:32:13 2019 -0800

    6pack,mkiss: fix possible deadlock
    
    We got another syzbot report [1] that tells us we must use
    write_lock_irq()/write_unlock_irq() to avoid possible deadlock.
    
    [1]
    
    WARNING: inconsistent lock state
    5.5.0-rc1-syzkaller #0 Not tainted
    --------------------------------
    inconsistent {HARDIRQ-ON-W} -> {IN-HARDIRQ-R} usage.
    syz-executor826/9605 [HC1[1]:SC0[0]:HE0:SE1] takes:
    ffffffff8a128718 (disc_data_lock){+-..}, at: sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
    {HARDIRQ-ON-W} state was registered at:
      lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
      __raw_write_lock_bh include/linux/rwlock_api_smp.h:203 [inline]
      _raw_write_lock_bh+0x33/0x50 kernel/locking/spinlock.c:319
      sixpack_close+0x1d/0x250 drivers/net/hamradio/6pack.c:657
      tty_ldisc_close.isra.0+0x119/0x1a0 drivers/tty/tty_ldisc.c:489
      tty_set_ldisc+0x230/0x6b0 drivers/tty/tty_ldisc.c:585
      tiocsetd drivers/tty/tty_io.c:2337 [inline]
      tty_ioctl+0xe8d/0x14f0 drivers/tty/tty_io.c:2597
      vfs_ioctl fs/ioctl.c:47 [inline]
      file_ioctl fs/ioctl.c:545 [inline]
      do_vfs_ioctl+0x977/0x14e0 fs/ioctl.c:732
      ksys_ioctl+0xab/0xd0 fs/ioctl.c:749
      __do_sys_ioctl fs/ioctl.c:756 [inline]
      __se_sys_ioctl fs/ioctl.c:754 [inline]
      __x64_sys_ioctl+0x73/0xb0 fs/ioctl.c:754
      do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
      entry_SYSCALL_64_after_hwframe+0x49/0xbe
    irq event stamp: 3946
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] __raw_spin_unlock_irq include/linux/spinlock_api_smp.h:168 [inline]
    hardirqs last  enabled at (3945): [<ffffffff87c86e43>] _raw_spin_unlock_irq+0x23/0x80 kernel/locking/spinlock.c:199
    hardirqs last disabled at (3946): [<ffffffff8100675f>] trace_hardirqs_off_thunk+0x1a/0x1c arch/x86/entry/thunk_64.S:42
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] spin_unlock_bh include/linux/spinlock.h:383 [inline]
    softirqs last  enabled at (2658): [<ffffffff86a8b4df>] clusterip_netdev_event+0x46f/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:222
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] spin_lock_bh include/linux/spinlock.h:343 [inline]
    softirqs last disabled at (2656): [<ffffffff86a8b22b>] clusterip_netdev_event+0x1bb/0x670 net/ipv4/netfilter/ipt_CLUSTERIP.c:196
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(disc_data_lock);
      <Interrupt>
        lock(disc_data_lock);
    
     *** DEADLOCK ***
    
    5 locks held by syz-executor826/9605:
     #0: ffff8880a905e198 (&tty->legacy_mutex){+.+.}, at: tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     #1: ffffffff899a56c0 (rcu_read_lock){....}, at: mutex_spin_on_owner+0x0/0x330 kernel/locking/mutex.c:413
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: spin_lock include/linux/spinlock.h:338 [inline]
     #2: ffff8880a496a2b0 (&(&i->lock)->rlock){-.-.}, at: serial8250_interrupt+0x2d/0x1a0 drivers/tty/serial/8250/8250_core.c:116
     #3: ffffffff8c104048 (&port_lock_key){-.-.}, at: serial8250_handle_irq.part.0+0x24/0x330 drivers/tty/serial/8250/8250_port.c:1823
     #4: ffff8880a905e090 (&tty->ldisc_sem){++++}, at: tty_ldisc_ref+0x22/0x90 drivers/tty/tty_ldisc.c:288
    
    stack backtrace:
    CPU: 1 PID: 9605 Comm: syz-executor826 Not tainted 5.5.0-rc1-syzkaller #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x197/0x210 lib/dump_stack.c:118
     print_usage_bug.cold+0x327/0x378 kernel/locking/lockdep.c:3101
     valid_state kernel/locking/lockdep.c:3112 [inline]
     mark_lock_irq kernel/locking/lockdep.c:3309 [inline]
     mark_lock+0xbb4/0x1220 kernel/locking/lockdep.c:3666
     mark_usage kernel/locking/lockdep.c:3554 [inline]
     __lock_acquire+0x1e55/0x4a00 kernel/locking/lockdep.c:3909
     lock_acquire+0x190/0x410 kernel/locking/lockdep.c:4485
     __raw_read_lock include/linux/rwlock_api_smp.h:149 [inline]
     _raw_read_lock+0x32/0x50 kernel/locking/spinlock.c:223
     sp_get.isra.0+0x1d/0xf0 drivers/net/ppp/ppp_synctty.c:138
     sixpack_write_wakeup+0x25/0x340 drivers/net/hamradio/6pack.c:402
     tty_wakeup+0xe9/0x120 drivers/tty/tty_io.c:536
     tty_port_default_wakeup+0x2b/0x40 drivers/tty/tty_port.c:50
     tty_port_tty_wakeup+0x57/0x70 drivers/tty/tty_port.c:387
     uart_write_wakeup+0x46/0x70 drivers/tty/serial/serial_core.c:104
     serial8250_tx_chars+0x495/0xaf0 drivers/tty/serial/8250/8250_port.c:1761
     serial8250_handle_irq.part.0+0x2a2/0x330 drivers/tty/serial/8250/8250_port.c:1834
     serial8250_handle_irq drivers/tty/serial/8250/8250_port.c:1820 [inline]
     serial8250_default_handle_irq+0xc0/0x150 drivers/tty/serial/8250/8250_port.c:1850
     serial8250_interrupt+0xf1/0x1a0 drivers/tty/serial/8250/8250_core.c:126
     __handle_irq_event_percpu+0x15d/0x970 kernel/irq/handle.c:149
     handle_irq_event_percpu+0x74/0x160 kernel/irq/handle.c:189
     handle_irq_event+0xa7/0x134 kernel/irq/handle.c:206
     handle_edge_irq+0x25e/0x8d0 kernel/irq/chip.c:830
     generic_handle_irq_desc include/linux/irqdesc.h:156 [inline]
     do_IRQ+0xde/0x280 arch/x86/kernel/irq.c:250
     common_interrupt+0xf/0xf arch/x86/entry/entry_64.S:607
     </IRQ>
    RIP: 0010:cpu_relax arch/x86/include/asm/processor.h:685 [inline]
    RIP: 0010:mutex_spin_on_owner+0x247/0x330 kernel/locking/mutex.c:579
    Code: c3 be 08 00 00 00 4c 89 e7 e8 e5 06 59 00 4c 89 e0 48 c1 e8 03 42 80 3c 38 00 0f 85 e1 00 00 00 49 8b 04 24 a8 01 75 96 f3 90 <e9> 2f fe ff ff 0f 0b e8 0d 19 09 00 84 c0 0f 85 ff fd ff ff 48 c7
    RSP: 0018:ffffc90001eafa20 EFLAGS: 00000246 ORIG_RAX: ffffffffffffffd7
    RAX: 0000000000000000 RBX: ffff88809fd9e0c0 RCX: 1ffffffff13266dd
    RDX: 0000000000000000 RSI: 0000000000000008 RDI: 0000000000000000
    RBP: ffffc90001eafa60 R08: 1ffff11013d22898 R09: ffffed1013d22899
    R10: ffffed1013d22898 R11: ffff88809e9144c7 R12: ffff8880a905e138
    R13: ffff88809e9144c0 R14: 0000000000000000 R15: dffffc0000000000
     mutex_optimistic_spin kernel/locking/mutex.c:673 [inline]
     __mutex_lock_common kernel/locking/mutex.c:962 [inline]
     __mutex_lock+0x32b/0x13c0 kernel/locking/mutex.c:1106
     mutex_lock_nested+0x16/0x20 kernel/locking/mutex.c:1121
     tty_lock+0xc7/0x130 drivers/tty/tty_mutex.c:19
     tty_release+0xb5/0xe90 drivers/tty/tty_io.c:1665
     __fput+0x2ff/0x890 fs/file_table.c:280
     ____fput+0x16/0x20 fs/file_table.c:313
     task_work_run+0x145/0x1c0 kernel/task_work.c:113
     exit_task_work include/linux/task_work.h:22 [inline]
     do_exit+0x8e7/0x2ef0 kernel/exit.c:797
     do_group_exit+0x135/0x360 kernel/exit.c:895
     __do_sys_exit_group kernel/exit.c:906 [inline]
     __se_sys_exit_group kernel/exit.c:904 [inline]
     __x64_sys_exit_group+0x44/0x50 kernel/exit.c:904
     do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x43fef8
    Code: Bad RIP value.
    RSP: 002b:00007ffdb07d2338 EFLAGS: 00000246 ORIG_RAX: 00000000000000e7
    RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 000000000043fef8
    RDX: 0000000000000000 RSI: 000000000000003c RDI: 0000000000000000
    RBP: 00000000004bf730 R08: 00000000000000e7 R09: ffffffffffffffd0
    R10: 00000000004002c8 R11: 0000000000000246 R12: 0000000000000001
    R13: 00000000006d1180 R14: 0000000000000000 R15: 0000000000000000
    
    Fixes: 6e4e2f811bad ("6pack,mkiss: fix lock inconsistency")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>

commit dbb70becde5b28940366ee419e0fdd3e09af44fb
Author: Chris Mason <clm@fb.com>
Date:   Wed Jul 10 12:28:18 2019 -0700

    Btrfs: extent_write_locked_range() should attach inode->i_wb
    
    extent_write_locked_range() is used when we're falling back to buffered
    IO from inside of compression.  It allocates its own wbc and should
    associate it with the inode's i_wb to make sure the IO goes down from
    the correct cgroup.
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Chris Mason <clm@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 1d53c9e6723022b12e4a5ed4b141f67c834b7f6f
Author: Chris Mason <clm@fb.com>
Date:   Wed Jul 10 12:28:16 2019 -0700

    Btrfs: only associate the locked page with one async_chunk struct
    
    The btrfs writepages function collects a large range of pages flagged
    for delayed allocation, and then sends them down through the COW code
    for processing.  When compression is on, we allocate one async_chunk
    structure for every 512K, and then run those pages through the
    compression code for IO submission.
    
    writepages starts all of this off with a single page, locked by the
    original call to extent_write_cache_pages(), and it's important to keep
    track of this page because it has already been through
    clear_page_dirty_for_io().
    
    The btrfs async_chunk struct has a pointer to the locked_page, and when
    we're redirtying the page because compression had to fallback to
    uncompressed IO, we use page->index to decide if a given async_chunk
    struct really owns that page.
    
    But, this is racey.  If a given delalloc range is broken up into two
    async_chunks (chunkA and chunkB), we can end up with something like
    this:
    
     compress_file_range(chunkA)
     submit_compress_extents(chunkA)
     submit compressed bios(chunkA)
     put_page(locked_page)
    
                                     compress_file_range(chunkB)
                                     ...
    
    Or:
    
     async_cow_submit
      submit_compressed_extents <--- falls back to buffered writeout
       cow_file_range
        extent_clear_unlock_delalloc
         __process_pages_contig
           put_page(locked_pages)
    
                                                async_cow_submit
    
    The end result is that chunkA is completed and cleaned up before chunkB
    even starts processing.  This means we can free locked_page() and reuse
    it elsewhere.  If we get really lucky, it'll have the same page->index
    in its new home as it did before.
    
    While we're processing chunkB, we might decide we need to fall back to
    uncompressed IO, and so compress_file_range() will call
    __set_page_dirty_nobufers() on chunkB->locked_page.
    
    Without cgroups in use, this creates as a phantom dirty page, which
    isn't great but isn't the end of the world. What can happen, it can go
    through the fixup worker and the whole COW machinery again:
    
    in submit_compressed_extents():
      while (async extents) {
      ...
        cow_file_range
        if (!page_started ...)
          extent_write_locked_range
        else if (...)
          unlock_page
        continue;
    
    This hasn't been observed in practice but is still possible.
    
    With cgroups in use, we might crash in the accounting code because
    page->mapping->i_wb isn't set.
    
      BUG: unable to handle kernel NULL pointer dereference at 00000000000000d0
      IP: percpu_counter_add_batch+0x11/0x70
      PGD 66534e067 P4D 66534e067 PUD 66534f067 PMD 0
      Oops: 0000 [#1] SMP DEBUG_PAGEALLOC
      CPU: 16 PID: 2172 Comm: rm Not tainted
      RIP: 0010:percpu_counter_add_batch+0x11/0x70
      RSP: 0018:ffffc9000a97bbe0 EFLAGS: 00010286
      RAX: 0000000000000005 RBX: 0000000000000090 RCX: 0000000000026115
      RDX: 0000000000000030 RSI: ffffffffffffffff RDI: 0000000000000090
      RBP: 0000000000000000 R08: fffffffffffffff5 R09: 0000000000000000
      R10: 00000000000260c0 R11: ffff881037fc26c0 R12: ffffffffffffffff
      R13: ffff880fe4111548 R14: ffffc9000a97bc90 R15: 0000000000000001
      FS:  00007f5503ced480(0000) GS:ffff880ff7200000(0000) knlGS:0000000000000000
      CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      CR2: 00000000000000d0 CR3: 00000001e0459005 CR4: 0000000000360ee0
      DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
      Call Trace:
       account_page_cleaned+0x15b/0x1f0
       __cancel_dirty_page+0x146/0x200
       truncate_cleanup_page+0x92/0xb0
       truncate_inode_pages_range+0x202/0x7d0
       btrfs_evict_inode+0x92/0x5a0
       evict+0xc1/0x190
       do_unlinkat+0x176/0x280
       do_syscall_64+0x63/0x1a0
       entry_SYSCALL_64_after_hwframe+0x42/0xb7
    
    The fix here is to make asyc_chunk->locked_page NULL everywhere but the
    one async_chunk struct that's allowed to do things to the locked page.
    
    Link: https://lore.kernel.org/linux-btrfs/c2419d01-5c84-3fb4-189e-4db519d08796@suse.com/
    Fixes: 771ed689d2cd ("Btrfs: Optimize compressed writeback and reads")
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Chris Mason <clm@fb.com>
    [ update changelog from mail thread discussion ]
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 7dee607ed0e04500459db53001d8e02f8831f084
Author: Parav Pandit <parav@mellanox.com>
Date:   Wed Sep 18 18:50:32 2019 -0500

    net/mlx5: Support lockless FTE read lookups
    
    During connection tracking offloads with high number of connections,
    (40K connections per second), flow table group lock contention is
    observed.
    To improve the performance by reducing lock contention, lockless
    FTE read lookup is performed as described below.
    
    Each flow table entry is refcounted.
    Flow table entry is removed when refcount drops to zero.
    rhash table allows rcu protected lookup.
    Each hash table entry insertion and removal is write lock protected.
    
    Hence, it is possible to perform lockless lookup in rhash table using
    following scheme.
    
    (a) Guard FTE entry lookup per group using rcu read lock.
    (b) Before freeing the FTE entry, wait for all readers to finish
    accessing the FTE.
    
    Below example of one reader and write in parallel racing, shows
    protection in effect with rcu lock.
    
    lookup_fte_locked()
      rcu_read_lock();
      search_hash_table()
                                      existing_flow_group_write_lock();
                                      tree_put_node(fte)
                                        drop_ref_cnt(fte)
                                        del_sw_fte(fte)
                                        del_hash_table_entry();
                                        call_rcu();
                                      existing_flow_group_write_unlock();
      get_ref_cnt(fte) fails
      rcu_read_unlock();
                                      rcu grace period();
                                        [..]
                                        kmem_cache_free(fte);
    
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

commit 7d14df2d280fb7411eba2eb96682da0683ad97f6
Merge: 0bb73e42f027 6af112b11a4b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 18 17:29:31 2019 -0700

    Merge tag 'for-5.4-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux
    
    Pull btrfs updates from David Sterba:
     "This continues with work on code refactoring, sanity checks and space
      handling. There are some less user visible changes, nothing that would
      particularly stand out.
    
      User visible changes:
       - tree checker, more sanity checks of:
           - ROOT_ITEM (key, size, generation, level, alignment, flags)
           - EXTENT_ITEM and METADATA_ITEM checks (key, size, offset,
             alignment, refs)
           - tree block reference items
           - EXTENT_DATA_REF (key, hash, offset)
    
       - deprecate flag BTRFS_SUBVOL_CREATE_ASYNC for subvolume creation
         ioctl, scheduled removal in 5.7
    
       - delete stale and unused UAPI definitions
         BTRFS_DEV_REPLACE_ITEM_STATE_*
    
       - improved export of debugging information available via existing
         sysfs directory structure
    
       - try harder to delete relations between qgroups and allow to delete
         orphan entries
    
       - remove unreliable space checks before relocation starts
    
      Core:
       - space handling:
           - improved ticket reservations and other high level logic in
             order to remove special cases
           - factor flushing infrastructure and use it for different
             contexts, allows to remove some special case handling
           - reduce metadata reservation when only updating inodes
           - reduce global block reserve minimum size (affects small
             filesystems)
           - improved overcommit logic wrt global block reserve
    
       - tests:
           - fix memory leaks in extent IO tree
           - catch all TRIM range
    
      Fixes:
       - fix ENOSPC errors, leading to transaction aborts, when cloning
         extents
    
       - several fixes for inode number cache (mount option inode_cache)
    
       - fix potential soft lockups during send when traversing large trees
    
       - fix unaligned access to space cache pages with SLUB debug on
         (PowerPC)
    
      Other:
       - refactoring public/private functions, moving to new or more
         appropriate files
    
       - defines converted to enums
    
       - error handling improvements
    
       - more assertions and comments
    
       - old code deletion"
    
    * tag 'for-5.4-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux: (138 commits)
      btrfs: Relinquish CPUs in btrfs_compare_trees
      btrfs: Don't assign retval of btrfs_try_tree_write_lock/btrfs_tree_read_lock_atomic
      btrfs: create structure to encode checksum type and length
      btrfs: turn checksum type define into an enum
      btrfs: add enospc debug messages for ticket failure
      btrfs: do not account global reserve in can_overcommit
      btrfs: use btrfs_try_granting_tickets in update_global_rsv
      btrfs: always reserve our entire size for the global reserve
      btrfs: change the minimum global reserve size
      btrfs: rename btrfs_space_info_add_old_bytes
      btrfs: remove orig_bytes from reserve_ticket
      btrfs: fix may_commit_transaction to deal with no partial filling
      btrfs: rework wake_all_tickets
      btrfs: refactor the ticket wakeup code
      btrfs: stop partially refilling tickets when releasing space
      btrfs: add space reservation tracepoint for reserved bytes
      btrfs: roll tracepoint into btrfs_space_info_update helper
      btrfs: do not allow reservations if we have pending tickets
      btrfs: stop clearing EXTENT_DIRTY in inode I/O tree
      btrfs: treat RWF_{,D}SYNC writes as sync for CRCs
      ...

commit 16d2d609ff0f1aded31913e4ff887007961085f8
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:58 2019 +0800

    bcache: fix race in btree_flush_write()
    
    There is a race between mca_reap(), btree_node_free() and journal code
    btree_flush_write(), which results very rare and strange deadlock or
    panic and are very hard to reproduce.
    
    Let me explain how the race happens. In btree_flush_write() one btree
    node with oldest journal pin is selected, then it is flushed to cache
    device, the select-and-flush is a two steps operation. Between these two
    steps, there are something may happen inside the race window,
    - The selected btree node was reaped by mca_reap() and allocated to
      other requesters for other btree node.
    - The slected btree node was selected, flushed and released by mca
      shrink callback bch_mca_scan().
    When btree_flush_write() tries to flush the selected btree node, firstly
    b->write_lock is held by mutex_lock(). If the race happens and the
    memory of selected btree node is allocated to other btree node, if that
    btree node's write_lock is held already, a deadlock very probably
    happens here. A worse case is the memory of the selected btree node is
    released, then all references to this btree node (e.g. b->write_lock)
    will trigger NULL pointer deference panic.
    
    This race was introduced in commit cafe56359144 ("bcache: A block layer
    cache"), and enlarged by commit c4dc2497d50d ("bcache: fix high CPU
    occupancy during journal"), which selected 128 btree nodes and flushed
    them one-by-one in a quite long time period.
    
    Such race is not easy to reproduce before. On a Lenovo SR650 server with
    48 Xeon cores, and configure 1 NVMe SSD as cache device, a MD raid0
    device assembled by 3 NVMe SSDs as backing device, this race can be
    observed around every 10,000 times btree_flush_write() gets called. Both
    deadlock and kernel panic all happened as aftermath of the race.
    
    The idea of the fix is to add a btree flag BTREE_NODE_journal_flush. It
    is set when selecting btree nodes, and cleared after btree nodes
    flushed. Then when mca_reap() selects a btree node with this bit set,
    this btree node will be skipped. Since mca_reap() only reaps btree node
    without BTREE_NODE_journal_flush flag, such race is avoided.
    
    Once corner case should be noticed, that is btree_node_free(). It might
    be called in some error handling code path. For example the following
    code piece from btree_split(),
            2149 err_free2:
            2150         bkey_put(b->c, &n2->key);
            2151         btree_node_free(n2);
            2152         rw_unlock(true, n2);
            2153 err_free1:
            2154         bkey_put(b->c, &n1->key);
            2155         btree_node_free(n1);
            2156         rw_unlock(true, n1);
    At line 2151 and 2155, the btree node n2 and n1 are released without
    mac_reap(), so BTREE_NODE_journal_flush also needs to be checked here.
    If btree_node_free() is called directly in such error handling path,
    and the selected btree node has BTREE_NODE_journal_flush bit set, just
    delay for 1 us and retry again. In this case this btree node won't
    be skipped, just retry until the BTREE_NODE_journal_flush bit cleared,
    and free the btree node memory.
    
    Fixes: cafe56359144 ("bcache: A block layer cache")
    Signed-off-by: Coly Li <colyli@suse.de>
    Reported-and-tested-by: kbuild test robot <lkp@intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 9138558777944599716f90163b8d2b192cfe59cf
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:56 2019 +0800

    bcache: add comments for mutex_lock(&b->write_lock)
    
    When accessing or modifying BTREE_NODE_dirty bit, it is not always
    necessary to acquire b->write_lock. In bch_btree_cache_free() and
    mca_reap() acquiring b->write_lock is necessary, and this patch adds
    comments to explain why mutex_lock(&b->write_lock) is necessary for
    checking or clearing BTREE_NODE_dirty bit there.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit b113f98432aed624fd9b80af818bd87e4db83537
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:58 2019 +0800

    bcache: fix race in btree_flush_write()
    
    [ Upstream commit 50a260e859964002dab162513a10f91ae9d3bcd3 ]
    
    There is a race between mca_reap(), btree_node_free() and journal code
    btree_flush_write(), which results very rare and strange deadlock or
    panic and are very hard to reproduce.
    
    Let me explain how the race happens. In btree_flush_write() one btree
    node with oldest journal pin is selected, then it is flushed to cache
    device, the select-and-flush is a two steps operation. Between these two
    steps, there are something may happen inside the race window,
    - The selected btree node was reaped by mca_reap() and allocated to
      other requesters for other btree node.
    - The slected btree node was selected, flushed and released by mca
      shrink callback bch_mca_scan().
    When btree_flush_write() tries to flush the selected btree node, firstly
    b->write_lock is held by mutex_lock(). If the race happens and the
    memory of selected btree node is allocated to other btree node, if that
    btree node's write_lock is held already, a deadlock very probably
    happens here. A worse case is the memory of the selected btree node is
    released, then all references to this btree node (e.g. b->write_lock)
    will trigger NULL pointer deference panic.
    
    This race was introduced in commit cafe56359144 ("bcache: A block layer
    cache"), and enlarged by commit c4dc2497d50d ("bcache: fix high CPU
    occupancy during journal"), which selected 128 btree nodes and flushed
    them one-by-one in a quite long time period.
    
    Such race is not easy to reproduce before. On a Lenovo SR650 server with
    48 Xeon cores, and configure 1 NVMe SSD as cache device, a MD raid0
    device assembled by 3 NVMe SSDs as backing device, this race can be
    observed around every 10,000 times btree_flush_write() gets called. Both
    deadlock and kernel panic all happened as aftermath of the race.
    
    The idea of the fix is to add a btree flag BTREE_NODE_journal_flush. It
    is set when selecting btree nodes, and cleared after btree nodes
    flushed. Then when mca_reap() selects a btree node with this bit set,
    this btree node will be skipped. Since mca_reap() only reaps btree node
    without BTREE_NODE_journal_flush flag, such race is avoided.
    
    Once corner case should be noticed, that is btree_node_free(). It might
    be called in some error handling code path. For example the following
    code piece from btree_split(),
            2149 err_free2:
            2150         bkey_put(b->c, &n2->key);
            2151         btree_node_free(n2);
            2152         rw_unlock(true, n2);
            2153 err_free1:
            2154         bkey_put(b->c, &n1->key);
            2155         btree_node_free(n1);
            2156         rw_unlock(true, n1);
    At line 2151 and 2155, the btree node n2 and n1 are released without
    mac_reap(), so BTREE_NODE_journal_flush also needs to be checked here.
    If btree_node_free() is called directly in such error handling path,
    and the selected btree node has BTREE_NODE_journal_flush bit set, just
    delay for 1 us and retry again. In this case this btree node won't
    be skipped, just retry until the BTREE_NODE_journal_flush bit cleared,
    and free the btree node memory.
    
    Fixes: cafe56359144 ("bcache: A block layer cache")
    Signed-off-by: Coly Li <colyli@suse.de>
    Reported-and-tested-by: kbuild test robot <lkp@intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f73c35d9297698cb9ce03dc84eaae19e2e1cd7a7
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:56 2019 +0800

    bcache: add comments for mutex_lock(&b->write_lock)
    
    [ Upstream commit 41508bb7d46b74dba631017e5a702a86caf1db8c ]
    
    When accessing or modifying BTREE_NODE_dirty bit, it is not always
    necessary to acquire b->write_lock. In bch_btree_cache_free() and
    mca_reap() acquiring b->write_lock is necessary, and this patch adds
    comments to explain why mutex_lock(&b->write_lock) is necessary for
    checking or clearing BTREE_NODE_dirty bit there.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit ecf99cdea02dcc792c27a52d1cf3e1c532551479
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Tue Jul 2 15:50:08 2019 -0400

    dm thin metadata: check if in fail_io mode when setting needs_check
    
    [ Upstream commit 54fa16ee532705985e6c946da455856f18f63ee1 ]
    
    Check if in fail_io mode at start of dm_pool_metadata_set_needs_check().
    Otherwise dm_pool_metadata_set_needs_check()'s superblock_lock() can
    crash in dm_bm_write_lock() while accessing the block manager object
    that was previously destroyed as part of a failed
    dm_pool_abort_metadata() that ultimately set fail_io to begin with.
    
    Also, update DMERR() message to more accurately describe
    superblock_lock() failure.
    
    Cc: stable@vger.kernel.org
    Reported-by: Zdenek Kabelac <zkabelac@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 65e99c43e9c2fee1a1f02c100154730fbeae9717
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Sep 4 20:22:39 2019 +0300

    btrfs: Don't assign retval of btrfs_try_tree_write_lock/btrfs_tree_read_lock_atomic
    
    Those function are simple boolean predicates there is no need to assign
    their return values to interim variables. Use them directly as
    predicates. No functional changes.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 1f3ebc8253ee56bfaa883c5114fb5569c56f6197
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue Jun 4 14:05:52 2019 -0700

    rcu: Restore barrier() to rcu_read_lock() and rcu_read_unlock()
    
    Commit bb73c52bad36 ("rcu: Don't disable preemption for Tiny and Tree
    RCU readers") removed the barrier() calls from rcu_read_lock() and
    rcu_write_lock() in CONFIG_PREEMPT=n&&CONFIG_PREEMPT_COUNT=n kernels.
    Within RCU, this commit was OK, but it failed to account for things like
    get_user() that can pagefault and that can be reordered by the compiler.
    Lack of the barrier() calls in rcu_read_lock() and rcu_read_unlock()
    can cause these page faults to migrate into RCU read-side critical
    sections, which in CONFIG_PREEMPT=n kernels could result in too-short
    grace periods and arbitrary misbehavior.  Please see commit 386afc91144b
    ("spinlocks and preemption points need to be at least compiler barriers")
    and Linus's commit 66be4e66a7f4 ("rcu: locking and unlocking need to
    always be at least barriers"), this last of which restores the barrier()
    call to both rcu_read_lock() and rcu_read_unlock().
    
    This commit removes barrier() calls that are no longer needed given that
    the addition of them in Linus's commit noted above.  The combination of
    this commit and Linus's commit effectively reverts commit bb73c52bad36
    ("rcu: Don't disable preemption for Tiny and Tree RCU readers").
    
    Reported-by: Herbert Xu <herbert@gondor.apana.org.au>
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>
    [ paulmck: Fix embarrassing typo located by Alan Stern. ]

commit 8e4c803e7f5fdd611b0b9e887f40a2bd191dc26f
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Tue Jul 2 15:50:08 2019 -0400

    dm thin metadata: check if in fail_io mode when setting needs_check
    
    commit 54fa16ee532705985e6c946da455856f18f63ee1 upstream.
    
    Check if in fail_io mode at start of dm_pool_metadata_set_needs_check().
    Otherwise dm_pool_metadata_set_needs_check()'s superblock_lock() can
    crash in dm_bm_write_lock() while accessing the block manager object
    that was previously destroyed as part of a failed
    dm_pool_abort_metadata() that ultimately set fail_io to begin with.
    
    Also, update DMERR() message to more accurately describe
    superblock_lock() failure.
    
    Cc: stable@vger.kernel.org
    Reported-by: Zdenek Kabelac <zkabelac@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fdfff855cd3633680d26872771d10aeab0b7f340
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jun 20 16:49:35 2019 +0100

    afs: Fix uninitialised spinlock afs_volume::cb_break_lock
    
    [ Upstream commit 90fa9b64523a645a97edc0bdcf2d74759957eeee ]
    
    Fix the cb_break_lock spinlock in afs_volume struct by initialising it when
    the volume record is allocated.
    
    Also rename the lock to cb_v_break_lock to distinguish it from the lock of
    the same name in the afs_server struct.
    
    Without this, the following trace may be observed when a volume-break
    callback is received:
    
      INFO: trying to register non-static key.
      the code is fine but needs lockdep annotation.
      turning off the locking correctness validator.
      CPU: 2 PID: 50 Comm: kworker/2:1 Not tainted 5.2.0-rc1-fscache+ #3045
      Hardware name: ASUS All Series/H97-PLUS, BIOS 2306 10/09/2014
      Workqueue: afs SRXAFSCB_CallBack
      Call Trace:
       dump_stack+0x67/0x8e
       register_lock_class+0x23b/0x421
       ? check_usage_forwards+0x13c/0x13c
       __lock_acquire+0x89/0xf73
       lock_acquire+0x13b/0x166
       ? afs_break_callbacks+0x1b2/0x3dd
       _raw_write_lock+0x2c/0x36
       ? afs_break_callbacks+0x1b2/0x3dd
       afs_break_callbacks+0x1b2/0x3dd
       ? trace_event_raw_event_afs_server+0x61/0xac
       SRXAFSCB_CallBack+0x11f/0x16c
       process_one_work+0x2c5/0x4ee
       ? worker_thread+0x234/0x2ac
       worker_thread+0x1d8/0x2ac
       ? cancel_delayed_work_sync+0xf/0xf
       kthread+0x11f/0x127
       ? kthread_park+0x76/0x76
       ret_from_fork+0x24/0x30
    
    Fixes: 68251f0a6818 ("afs: Fix whole-volume callback handling")
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 7b0ef1ba26048ac3ca00408c6d689a717e65cf58
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jun 20 16:49:35 2019 +0100

    afs: Fix uninitialised spinlock afs_volume::cb_break_lock
    
    [ Upstream commit 90fa9b64523a645a97edc0bdcf2d74759957eeee ]
    
    Fix the cb_break_lock spinlock in afs_volume struct by initialising it when
    the volume record is allocated.
    
    Also rename the lock to cb_v_break_lock to distinguish it from the lock of
    the same name in the afs_server struct.
    
    Without this, the following trace may be observed when a volume-break
    callback is received:
    
      INFO: trying to register non-static key.
      the code is fine but needs lockdep annotation.
      turning off the locking correctness validator.
      CPU: 2 PID: 50 Comm: kworker/2:1 Not tainted 5.2.0-rc1-fscache+ #3045
      Hardware name: ASUS All Series/H97-PLUS, BIOS 2306 10/09/2014
      Workqueue: afs SRXAFSCB_CallBack
      Call Trace:
       dump_stack+0x67/0x8e
       register_lock_class+0x23b/0x421
       ? check_usage_forwards+0x13c/0x13c
       __lock_acquire+0x89/0xf73
       lock_acquire+0x13b/0x166
       ? afs_break_callbacks+0x1b2/0x3dd
       _raw_write_lock+0x2c/0x36
       ? afs_break_callbacks+0x1b2/0x3dd
       afs_break_callbacks+0x1b2/0x3dd
       ? trace_event_raw_event_afs_server+0x61/0xac
       SRXAFSCB_CallBack+0x11f/0x16c
       process_one_work+0x2c5/0x4ee
       ? worker_thread+0x234/0x2ac
       worker_thread+0x1d8/0x2ac
       ? cancel_delayed_work_sync+0xf/0xf
       kthread+0x11f/0x127
       ? kthread_park+0x76/0x76
       ret_from_fork+0x24/0x30
    
    Fixes: 68251f0a6818 ("afs: Fix whole-volume callback handling")
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 54fa16ee532705985e6c946da455856f18f63ee1
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Tue Jul 2 15:50:08 2019 -0400

    dm thin metadata: check if in fail_io mode when setting needs_check
    
    Check if in fail_io mode at start of dm_pool_metadata_set_needs_check().
    Otherwise dm_pool_metadata_set_needs_check()'s superblock_lock() can
    crash in dm_bm_write_lock() while accessing the block manager object
    that was previously destroyed as part of a failed
    dm_pool_abort_metadata() that ultimately set fail_io to begin with.
    
    Also, update DMERR() message to more accurately describe
    superblock_lock() failure.
    
    Cc: stable@vger.kernel.org
    Reported-by: Zdenek Kabelac <zkabelac@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

commit 00801ae4bb2be5f5af46502ef239ac5f4b536094
Author: David Sterba <dsterba@suse.com>
Date:   Thu May 2 16:53:47 2019 +0200

    btrfs: switch extent_buffer write_locks from atomic to int
    
    The write_locks is either 0 or 1 and always updated under the lock,
    so we don't need the atomic_t semantics.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 91be66e1318f67ed5888ca10e10cc8ffdc24f661
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:59 2019 +0800

    bcache: performance improvement for btree_flush_write()
    
    This patch improves performance for btree_flush_write() in following
    ways,
    - Use another spinlock journal.flush_write_lock to replace the very
      hot journal.lock. We don't have to use journal.lock here, selecting
      candidate btree nodes takes a lot of time, hold journal.lock here will
      block other jouranling threads and drop the overall I/O performance.
    - Only select flushing btree node from c->btree_cache list. When the
      machine has a large system memory, mca cache may have a huge number of
      cached btree nodes. Iterating all the cached nodes will take a lot
      of CPU time, and most of the nodes on c->btree_cache_freeable and
      c->btree_cache_freed lists are cleared and have need to flush. So only
      travel mca list c->btree_cache to select flushing btree node should be
      enough for most of the cases.
    - Don't iterate whole c->btree_cache list, only reversely select first
      BTREE_FLUSH_NR btree nodes to flush. Iterate all btree nodes from
      c->btree_cache and select the oldest journal pin btree nodes consumes
      huge number of CPU cycles if the list is huge (push and pop a node
      into/out of a heap is expensive). The last several dirty btree nodes
      on the tail of c->btree_cache list are earlest allocated and cached
      btree nodes, they are relative to the oldest journal pin btree nodes.
      Therefore only flushing BTREE_FLUSH_NR btree nodes from tail of
      c->btree_cache probably includes the oldest journal pin btree nodes.
    
    In my testing, the above change decreases 50%+ CPU consumption when
    journal space is full. Some times IOPS drops to 0 for 5-8 seconds,
    comparing blocking I/O for 120+ seconds in previous code, this is much
    better. Maybe there is room to improve in future, but at this momment
    the fix looks fine and performs well in my testing.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 50a260e859964002dab162513a10f91ae9d3bcd3
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:58 2019 +0800

    bcache: fix race in btree_flush_write()
    
    There is a race between mca_reap(), btree_node_free() and journal code
    btree_flush_write(), which results very rare and strange deadlock or
    panic and are very hard to reproduce.
    
    Let me explain how the race happens. In btree_flush_write() one btree
    node with oldest journal pin is selected, then it is flushed to cache
    device, the select-and-flush is a two steps operation. Between these two
    steps, there are something may happen inside the race window,
    - The selected btree node was reaped by mca_reap() and allocated to
      other requesters for other btree node.
    - The slected btree node was selected, flushed and released by mca
      shrink callback bch_mca_scan().
    When btree_flush_write() tries to flush the selected btree node, firstly
    b->write_lock is held by mutex_lock(). If the race happens and the
    memory of selected btree node is allocated to other btree node, if that
    btree node's write_lock is held already, a deadlock very probably
    happens here. A worse case is the memory of the selected btree node is
    released, then all references to this btree node (e.g. b->write_lock)
    will trigger NULL pointer deference panic.
    
    This race was introduced in commit cafe56359144 ("bcache: A block layer
    cache"), and enlarged by commit c4dc2497d50d ("bcache: fix high CPU
    occupancy during journal"), which selected 128 btree nodes and flushed
    them one-by-one in a quite long time period.
    
    Such race is not easy to reproduce before. On a Lenovo SR650 server with
    48 Xeon cores, and configure 1 NVMe SSD as cache device, a MD raid0
    device assembled by 3 NVMe SSDs as backing device, this race can be
    observed around every 10,000 times btree_flush_write() gets called. Both
    deadlock and kernel panic all happened as aftermath of the race.
    
    The idea of the fix is to add a btree flag BTREE_NODE_journal_flush. It
    is set when selecting btree nodes, and cleared after btree nodes
    flushed. Then when mca_reap() selects a btree node with this bit set,
    this btree node will be skipped. Since mca_reap() only reaps btree node
    without BTREE_NODE_journal_flush flag, such race is avoided.
    
    Once corner case should be noticed, that is btree_node_free(). It might
    be called in some error handling code path. For example the following
    code piece from btree_split(),
            2149 err_free2:
            2150         bkey_put(b->c, &n2->key);
            2151         btree_node_free(n2);
            2152         rw_unlock(true, n2);
            2153 err_free1:
            2154         bkey_put(b->c, &n1->key);
            2155         btree_node_free(n1);
            2156         rw_unlock(true, n1);
    At line 2151 and 2155, the btree node n2 and n1 are released without
    mac_reap(), so BTREE_NODE_journal_flush also needs to be checked here.
    If btree_node_free() is called directly in such error handling path,
    and the selected btree node has BTREE_NODE_journal_flush bit set, just
    delay for 1 us and retry again. In this case this btree node won't
    be skipped, just retry until the BTREE_NODE_journal_flush bit cleared,
    and free the btree node memory.
    
    Fixes: cafe56359144 ("bcache: A block layer cache")
    Signed-off-by: Coly Li <colyli@suse.de>
    Reported-and-tested-by: kbuild test robot <lkp@intel.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 41508bb7d46b74dba631017e5a702a86caf1db8c
Author: Coly Li <colyli@suse.de>
Date:   Fri Jun 28 19:59:56 2019 +0800

    bcache: add comments for mutex_lock(&b->write_lock)
    
    When accessing or modifying BTREE_NODE_dirty bit, it is not always
    necessary to acquire b->write_lock. In bch_btree_cache_free() and
    mca_reap() acquiring b->write_lock is necessary, and this patch adds
    comments to explain why mutex_lock(&b->write_lock) is necessary for
    checking or clearing BTREE_NODE_dirty bit there.
    
    Signed-off-by: Coly Li <colyli@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 90fa9b64523a645a97edc0bdcf2d74759957eeee
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jun 20 16:49:35 2019 +0100

    afs: Fix uninitialised spinlock afs_volume::cb_break_lock
    
    Fix the cb_break_lock spinlock in afs_volume struct by initialising it when
    the volume record is allocated.
    
    Also rename the lock to cb_v_break_lock to distinguish it from the lock of
    the same name in the afs_server struct.
    
    Without this, the following trace may be observed when a volume-break
    callback is received:
    
      INFO: trying to register non-static key.
      the code is fine but needs lockdep annotation.
      turning off the locking correctness validator.
      CPU: 2 PID: 50 Comm: kworker/2:1 Not tainted 5.2.0-rc1-fscache+ #3045
      Hardware name: ASUS All Series/H97-PLUS, BIOS 2306 10/09/2014
      Workqueue: afs SRXAFSCB_CallBack
      Call Trace:
       dump_stack+0x67/0x8e
       register_lock_class+0x23b/0x421
       ? check_usage_forwards+0x13c/0x13c
       __lock_acquire+0x89/0xf73
       lock_acquire+0x13b/0x166
       ? afs_break_callbacks+0x1b2/0x3dd
       _raw_write_lock+0x2c/0x36
       ? afs_break_callbacks+0x1b2/0x3dd
       afs_break_callbacks+0x1b2/0x3dd
       ? trace_event_raw_event_afs_server+0x61/0xac
       SRXAFSCB_CallBack+0x11f/0x16c
       process_one_work+0x2c5/0x4ee
       ? worker_thread+0x234/0x2ac
       worker_thread+0x1d8/0x2ac
       ? cancel_delayed_work_sync+0xf/0xf
       kthread+0x11f/0x127
       ? kthread_park+0x76/0x76
       ret_from_fork+0x24/0x30
    
    Fixes: 68251f0a6818 ("afs: Fix whole-volume callback handling")
    Signed-off-by: David Howells <dhowells@redhat.com>

commit 00f3c5a3df2c1e3dab14d0dd2b71f852d46be97f
Author: Waiman Long <longman@redhat.com>
Date:   Mon May 20 16:59:07 2019 -0400

    locking/rwsem: Always release wait_lock before waking up tasks
    
    With the use of wake_q, we can do task wakeups without holding the
    wait_lock. There is one exception in the rwsem code, though. It is
    when the writer in the slowpath detects that there are waiters ahead
    but the rwsem is not held by a writer. This can lead to a long wait_lock
    hold time especially when a large number of readers are to be woken up.
    
    Remediate this situation by releasing the wait_lock before waking
    up tasks and re-acquiring it afterward. The rwsem_try_write_lock()
    function is also modified to read the rwsem count directly to avoid
    stale count value.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: huang ying <huang.ying.caritas@gmail.com>
    Link: https://lkml.kernel.org/r/20190520205918.22251-9-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 4f23dbc1e657951e5d94c60369bc1db065961fb3
Author: Waiman Long <longman@redhat.com>
Date:   Mon May 20 16:59:06 2019 -0400

    locking/rwsem: Implement lock handoff to prevent lock starvation
    
    Because of writer lock stealing, it is possible that a constant
    stream of incoming writers will cause a waiting writer or reader to
    wait indefinitely leading to lock starvation.
    
    This patch implements a lock handoff mechanism to disable lock stealing
    and force lock handoff to the first waiter or waiters (for readers)
    in the queue after at least a 4ms waiting period unless it is a RT
    writer task which doesn't need to wait. The waiting period is used to
    avoid discouraging lock stealing too much to affect performance.
    
    The setting and clearing of the handoff bit is serialized by the
    wait_lock. So racing is not possible.
    
    A rwsem microbenchmark was run for 5 seconds on a 2-socket 40-core
    80-thread Skylake system with a v5.1 based kernel and 240 write_lock
    threads with 5us sleep critical section.
    
    Before the patch, the min/mean/max numbers of locking operations for
    the locking threads were 1/7,792/173,696. After the patch, the figures
    became 5,842/6,542/7,458.  It can be seen that the rwsem became much
    more fair, though there was a drop of about 16% in the mean locking
    operations done which was a tradeoff of having better fairness.
    
    Making the waiter set the handoff bit right after the first wakeup can
    impact performance especially with a mixed reader/writer workload. With
    the same microbenchmark with short critical section and equal number of
    reader and writer threads (40/40), the reader/writer locking operation
    counts with the current patch were:
    
      40 readers, Iterations Min/Mean/Max = 1,793/1,794/1,796
      40 writers, Iterations Min/Mean/Max = 1,793/34,956/86,081
    
    By making waiter set handoff bit immediately after wakeup:
    
      40 readers, Iterations Min/Mean/Max = 43/44/46
      40 writers, Iterations Min/Mean/Max = 43/1,263/3,191
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: huang ying <huang.ying.caritas@gmail.com>
    Link: https://lkml.kernel.org/r/20190520205918.22251-8-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d819d97ea025f8c32c12adef0ff55b2c7bf5c853
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Wed Jan 16 11:00:57 2019 -0500

    btrfs: honor path->skip_locking in backref code
    
    commit 38e3eebff643db725633657d1d87a3be019d1018 upstream.
    
    Qgroups will do the old roots lookup at delayed ref time, which could be
    while walking down the extent root while running a delayed ref.  This
    should be fine, except we specifically lock eb's in the backref walking
    code irrespective of path->skip_locking, which deadlocks the system.
    Fix up the backref code to honor path->skip_locking, nobody will be
    modifying the commit_root when we're searching so it's completely safe
    to do.
    
    This happens since fb235dc06fac ("btrfs: qgroup: Move half of the qgroup
    accounting time out of commit trans"), kernel may lockup with quota
    enabled.
    
    There is one backref trace triggered by snapshot dropping along with
    write operation in the source subvolume.  The example can be reliably
    reproduced:
    
      btrfs-cleaner   D    0  4062      2 0x80000000
      Call Trace:
       schedule+0x32/0x90
       btrfs_tree_read_lock+0x93/0x130 [btrfs]
       find_parent_nodes+0x29b/0x1170 [btrfs]
       btrfs_find_all_roots_safe+0xa8/0x120 [btrfs]
       btrfs_find_all_roots+0x57/0x70 [btrfs]
       btrfs_qgroup_trace_extent_post+0x37/0x70 [btrfs]
       btrfs_qgroup_trace_leaf_items+0x10b/0x140 [btrfs]
       btrfs_qgroup_trace_subtree+0xc8/0xe0 [btrfs]
       do_walk_down+0x541/0x5e3 [btrfs]
       walk_down_tree+0xab/0xe7 [btrfs]
       btrfs_drop_snapshot+0x356/0x71a [btrfs]
       btrfs_clean_one_deleted_snapshot+0xb8/0xf0 [btrfs]
       cleaner_kthread+0x12b/0x160 [btrfs]
       kthread+0x112/0x130
       ret_from_fork+0x27/0x50
    
    When dropping snapshots with qgroup enabled, we will trigger backref
    walk.
    
    However such backref walk at that timing is pretty dangerous, as if one
    of the parent nodes get WRITE locked by other thread, we could cause a
    dead lock.
    
    For example:
    
               FS 260     FS 261 (Dropped)
                node A        node B
               /      \      /      \
           node C      node D      node E
          /   \         /  \        /     \
      leaf F|leaf G|leaf H|leaf I|leaf J|leaf K
    
    The lock sequence would be:
    
          Thread A (cleaner)             |       Thread B (other writer)
    -----------------------------------------------------------------------
    write_lock(B)                        |
    write_lock(D)                        |
    ^^^ called by walk_down_tree()       |
                                         |       write_lock(A)
                                         |       write_lock(D) << Stall
    read_lock(H) << for backref walk     |
    read_lock(D) << lock owner is        |
                    the same thread A    |
                    so read lock is OK   |
    read_lock(A) << Stall                |
    
    So thread A hold write lock D, and needs read lock A to unlock.
    While thread B holds write lock A, while needs lock D to unlock.
    
    This will cause a deadlock.
    
    This is not only limited to snapshot dropping case.  As the backref
    walk, even only happens on commit trees, is breaking the normal top-down
    locking order, makes it deadlock prone.
    
    Fixes: fb235dc06fac ("btrfs: qgroup: Move half of the qgroup accounting time out of commit trans")
    CC: stable@vger.kernel.org # 4.14+
    Reported-and-tested-by: David Sterba <dsterba@suse.com>
    Reported-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    [ rebase to latest branch and fix lock assert bug in btrfs/007 ]
    [ backport to linux-4.19.y branch, solve minor conflicts ]
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    [ copy logs and deadlock analysis from Qu's patch ]
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9c0339dd381db5b06afd377b97c7713d9157cae9
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Wed Jan 16 11:00:57 2019 -0500

    btrfs: honor path->skip_locking in backref code
    
    commit 38e3eebff643db725633657d1d87a3be019d1018 upstream.
    
    Qgroups will do the old roots lookup at delayed ref time, which could be
    while walking down the extent root while running a delayed ref.  This
    should be fine, except we specifically lock eb's in the backref walking
    code irrespective of path->skip_locking, which deadlocks the system.
    Fix up the backref code to honor path->skip_locking, nobody will be
    modifying the commit_root when we're searching so it's completely safe
    to do.
    
    This happens since fb235dc06fac ("btrfs: qgroup: Move half of the qgroup
    accounting time out of commit trans"), kernel may lockup with quota
    enabled.
    
    There is one backref trace triggered by snapshot dropping along with
    write operation in the source subvolume.  The example can be reliably
    reproduced:
    
      btrfs-cleaner   D    0  4062      2 0x80000000
      Call Trace:
       schedule+0x32/0x90
       btrfs_tree_read_lock+0x93/0x130 [btrfs]
       find_parent_nodes+0x29b/0x1170 [btrfs]
       btrfs_find_all_roots_safe+0xa8/0x120 [btrfs]
       btrfs_find_all_roots+0x57/0x70 [btrfs]
       btrfs_qgroup_trace_extent_post+0x37/0x70 [btrfs]
       btrfs_qgroup_trace_leaf_items+0x10b/0x140 [btrfs]
       btrfs_qgroup_trace_subtree+0xc8/0xe0 [btrfs]
       do_walk_down+0x541/0x5e3 [btrfs]
       walk_down_tree+0xab/0xe7 [btrfs]
       btrfs_drop_snapshot+0x356/0x71a [btrfs]
       btrfs_clean_one_deleted_snapshot+0xb8/0xf0 [btrfs]
       cleaner_kthread+0x12b/0x160 [btrfs]
       kthread+0x112/0x130
       ret_from_fork+0x27/0x50
    
    When dropping snapshots with qgroup enabled, we will trigger backref
    walk.
    
    However such backref walk at that timing is pretty dangerous, as if one
    of the parent nodes get WRITE locked by other thread, we could cause a
    dead lock.
    
    For example:
    
               FS 260     FS 261 (Dropped)
                node A        node B
               /      \      /      \
           node C      node D      node E
          /   \         /  \        /     \
      leaf F|leaf G|leaf H|leaf I|leaf J|leaf K
    
    The lock sequence would be:
    
          Thread A (cleaner)             |       Thread B (other writer)
    -----------------------------------------------------------------------
    write_lock(B)                        |
    write_lock(D)                        |
    ^^^ called by walk_down_tree()       |
                                         |       write_lock(A)
                                         |       write_lock(D) << Stall
    read_lock(H) << for backref walk     |
    read_lock(D) << lock owner is        |
                    the same thread A    |
                    so read lock is OK   |
    read_lock(A) << Stall                |
    
    So thread A hold write lock D, and needs read lock A to unlock.
    While thread B holds write lock A, while needs lock D to unlock.
    
    This will cause a deadlock.
    
    This is not only limited to snapshot dropping case.  As the backref
    walk, even only happens on commit trees, is breaking the normal top-down
    locking order, makes it deadlock prone.
    
    Fixes: fb235dc06fac ("btrfs: qgroup: Move half of the qgroup accounting time out of commit trans")
    CC: stable@vger.kernel.org # 4.14+
    Reported-and-tested-by: David Sterba <dsterba@suse.com>
    Reported-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    [ rebase to latest branch and fix lock assert bug in btrfs/007 ]
    [ backport to linux-4.19.y branch, solve minor conflicts ]
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    [ copy logs and deadlock analysis from Qu's patch ]
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2904de1dea70671fc4cbdac796d60bd35e8aa817
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Wed Jan 16 11:00:57 2019 -0500

    btrfs: honor path->skip_locking in backref code
    
    commit 38e3eebff643db725633657d1d87a3be019d1018 upstream.
    
    Qgroups will do the old roots lookup at delayed ref time, which could be
    while walking down the extent root while running a delayed ref.  This
    should be fine, except we specifically lock eb's in the backref walking
    code irrespective of path->skip_locking, which deadlocks the system.
    Fix up the backref code to honor path->skip_locking, nobody will be
    modifying the commit_root when we're searching so it's completely safe
    to do.
    
    This happens since fb235dc06fac ("btrfs: qgroup: Move half of the qgroup
    accounting time out of commit trans"), kernel may lockup with quota
    enabled.
    
    There is one backref trace triggered by snapshot dropping along with
    write operation in the source subvolume.  The example can be reliably
    reproduced:
    
      btrfs-cleaner   D    0  4062      2 0x80000000
      Call Trace:
       schedule+0x32/0x90
       btrfs_tree_read_lock+0x93/0x130 [btrfs]
       find_parent_nodes+0x29b/0x1170 [btrfs]
       btrfs_find_all_roots_safe+0xa8/0x120 [btrfs]
       btrfs_find_all_roots+0x57/0x70 [btrfs]
       btrfs_qgroup_trace_extent_post+0x37/0x70 [btrfs]
       btrfs_qgroup_trace_leaf_items+0x10b/0x140 [btrfs]
       btrfs_qgroup_trace_subtree+0xc8/0xe0 [btrfs]
       do_walk_down+0x541/0x5e3 [btrfs]
       walk_down_tree+0xab/0xe7 [btrfs]
       btrfs_drop_snapshot+0x356/0x71a [btrfs]
       btrfs_clean_one_deleted_snapshot+0xb8/0xf0 [btrfs]
       cleaner_kthread+0x12b/0x160 [btrfs]
       kthread+0x112/0x130
       ret_from_fork+0x27/0x50
    
    When dropping snapshots with qgroup enabled, we will trigger backref
    walk.
    
    However such backref walk at that timing is pretty dangerous, as if one
    of the parent nodes get WRITE locked by other thread, we could cause a
    dead lock.
    
    For example:
    
               FS 260     FS 261 (Dropped)
                node A        node B
               /      \      /      \
           node C      node D      node E
          /   \         /  \        /     \
      leaf F|leaf G|leaf H|leaf I|leaf J|leaf K
    
    The lock sequence would be:
    
          Thread A (cleaner)             |       Thread B (other writer)
    -----------------------------------------------------------------------
    write_lock(B)                        |
    write_lock(D)                        |
    ^^^ called by walk_down_tree()       |
                                         |       write_lock(A)
                                         |       write_lock(D) << Stall
    read_lock(H) << for backref walk     |
    read_lock(D) << lock owner is        |
                    the same thread A    |
                    so read lock is OK   |
    read_lock(A) << Stall                |
    
    So thread A hold write lock D, and needs read lock A to unlock.
    While thread B holds write lock A, while needs lock D to unlock.
    
    This will cause a deadlock.
    
    This is not only limited to snapshot dropping case.  As the backref
    walk, even only happens on commit trees, is breaking the normal top-down
    locking order, makes it deadlock prone.
    
    Fixes: fb235dc06fac ("btrfs: qgroup: Move half of the qgroup accounting time out of commit trans")
    CC: stable@vger.kernel.org # 4.14+
    Reported-and-tested-by: David Sterba <dsterba@suse.com>
    Reported-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    [ rebase to latest branch and fix lock assert bug in btrfs/007 ]
    [ solve conflicts and backport to linux-5.0.y ]
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    [ copy logs and deadlock analysis from Qu's patch ]
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7296871d76087dbbe55b42ec9a4cbd7913ad94bb
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Feb 23 13:24:59 2019 -0800

    net/x25: fix a race in x25_bind()
    
    commit 797a22bd5298c2674d927893f46cadf619dad11d upstream.
    
    syzbot was able to trigger another soft lockup [1]
    
    I first thought it was the O(N^2) issue I mentioned in my
    prior fix (f657d22ee1f "net/x25: do not hold the cpu
    too long in x25_new_lci()"), but I eventually found
    that x25_bind() was not checking SOCK_ZAPPED state under
    socket lock protection.
    
    This means that multiple threads can end up calling
    x25_insert_socket() for the same socket, and corrupt x25_list
    
    [1]
    watchdog: BUG: soft lockup - CPU#0 stuck for 123s! [syz-executor.2:10492]
    Modules linked in:
    irq event stamp: 27515
    hardirqs last  enabled at (27514): [<ffffffff81006673>] trace_hardirqs_on_thunk+0x1a/0x1c
    hardirqs last disabled at (27515): [<ffffffff8100668f>] trace_hardirqs_off_thunk+0x1a/0x1c
    softirqs last  enabled at (32): [<ffffffff8632ee73>] x25_get_neigh+0xa3/0xd0 net/x25/x25_link.c:336
    softirqs last disabled at (34): [<ffffffff86324bc3>] x25_find_socket+0x23/0x140 net/x25/af_x25.c:341
    CPU: 0 PID: 10492 Comm: syz-executor.2 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__sanitizer_cov_trace_pc+0x4/0x50 kernel/kcov.c:97
    Code: f4 ff ff ff e8 11 9f ea ff 48 c7 05 12 fb e5 08 00 00 00 00 e9 c8 e9 ff ff 90 90 90 90 90 90 90 90 90 90 90 90 90 55 48 89 e5 <48> 8b 75 08 65 48 8b 04 25 40 ee 01 00 65 8b 15 38 0c 92 7e 81 e2
    RSP: 0018:ffff88806e94fc48 EFLAGS: 00000286 ORIG_RAX: ffffffffffffff13
    RAX: 1ffff1100d84dac5 RBX: 0000000000000001 RCX: ffffc90006197000
    RDX: 0000000000040000 RSI: ffffffff86324bf3 RDI: ffff88806c26d628
    RBP: ffff88806e94fc48 R08: ffff88806c1c6500 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: ffff88806c26d628
    R13: ffff888090455200 R14: dffffc0000000000 R15: 0000000000000000
    FS:  00007f3a107e4700(0000) GS:ffff8880ae800000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007f3a107e3db8 CR3: 00000000a5544000 CR4: 00000000001406f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     __x25_find_socket net/x25/af_x25.c:327 [inline]
     x25_find_socket+0x7d/0x140 net/x25/af_x25.c:342
     x25_new_lci net/x25/af_x25.c:355 [inline]
     x25_connect+0x380/0xde0 net/x25/af_x25.c:784
     __sys_connect+0x266/0x330 net/socket.c:1662
     __do_sys_connect net/socket.c:1673 [inline]
     __se_sys_connect net/socket.c:1670 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1670
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007f3a107e3c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e29
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000005
    RBP: 000000000073c040 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007f3a107e46d4
    R13: 00000000004be362 R14: 00000000004ceb98 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 10493 Comm: syz-executor.3 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x143/0x290 kernel/locking/qrwlock.c:86
    Code: 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 41 0f b6 55 00 <41> 38 d7 7c eb 84 d2 74 e7 48 89 df e8 cc aa 4e 00 eb dd be 04 00
    RSP: 0018:ffff888085c47bd8 EFLAGS: 00000206
    RAX: 0000000000000300 RBX: ffffffff89412b00 RCX: 1ffffffff1282560
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89412b00
    RBP: ffff888085c47c70 R08: 1ffffffff1282560 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: 00000000000000ff
    R13: fffffbfff1282560 R14: 1ffff11010b88f7d R15: 0000000000000003
    FS:  00007fdd04086700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007fdd04064db8 CR3: 0000000090be0000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:703
     __sys_bind+0x23f/0x290 net/socket.c:1481
     __do_sys_bind net/socket.c:1492 [inline]
     __se_sys_bind net/socket.c:1490 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1490
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    
    Fixes: 90c27297a9bf ("X.25 remove bkl in bind")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: andrew hendry <andrew.hendry@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 02c6db4f7308e4f5adf4df2ef623160bfdb18636
Author: Qu Wenruo <wqu@suse.com>
Date:   Wed Mar 20 14:27:45 2019 +0800

    btrfs: extent_io: Handle errors better in extent_write_locked_range()
    
    We can only get @ret <= 0.  Add an ASSERT() for it just in case.
    
    Then, instead of submitting the write bio even we got some error, check
    the return value first.
    
    If we have already hit some error, just clean up the corrupted or
    half-baked bio, and return error.
    
    If there is no error so far, then call flush_write_bio() and return the
    result.
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit c79adfc085c0662385cfcb55f15590303212e8e9
Author: David Sterba <dsterba@suse.com>
Date:   Fri Aug 24 16:24:26 2018 +0200

    btrfs: use assertion helpers for extent buffer write lock counters
    
    Use the helpers where open coded. On non-debug builds, the warnings will
    not trigger and extent_buffer::write_locks become unused and can be
    moved to the appropriate section, saving a few bytes.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit e3f153886702302482918a3788ba3bb24a37a4aa
Author: David Sterba <dsterba@suse.com>
Date:   Fri Aug 24 16:20:02 2018 +0200

    btrfs: add assertion helpers for extent buffer write lock counters
    
    The write_locks are a simple counter to track locking balance and used
    to assert tree locks.  Add helpers to make it conditionally work only in
    DEBUG builds.  Will be used in followup patches.
    
    Reviewed-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 873f258becca87f4dd973fe0ba09b88b737c9b14
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Thu Apr 18 10:29:48 2019 -0400

    dm thin metadata: do not write metadata if no changes occurred
    
    Otherwise, just activating a thin-pool and thin device and then
    deactivating them will cause the thin-pool metadata to be changed
    (e.g. superblock written) -- even without any metadata being changed.
    
    Add 'in_service' flag to struct dm_pool_metadata and set it in
    pmd_write_lock() because all on-disk metadata changes must take a write
    lock of pmd->root_lock.  Once 'in_service' is set it is never cleared.
    __commit_transaction() will return 0 if 'in_service' is not set.
    dm_pool_commit_metadata() is updated to use __pmd_write_lock() so that
    it isn't the sole reason for putting a thin-pool in service.
    
    Also fix dm_pool_commit_metadata() to open the next transaction if the
    return from __commit_transaction() is 0.  Not seeing why the early
    return ever made since for a return of 0 given that dm-io's async_io(),
    as used by bufio, always returns 0.
    
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

commit 6a1b1ddc6a2cfb32da8f5e75f1aa053280682a05
Author: Mike Snitzer <snitzer@redhat.com>
Date:   Mon Apr 15 16:54:36 2019 -0400

    dm thin metadata: add wrappers for managing write locking of metadata
    
    No functional change, but this prepares to hook off of pmd_write_lock()
    with additional functionality (as provided in next commit).
    
    Suggested-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>

commit 909620ff72c8fcf95b6ef1dca850b24bf016dd27
Author: Jon Maloy <jon.maloy@ericsson.com>
Date:   Thu Apr 11 21:56:28 2019 +0200

    tipc: use standard write_lock & unlock functions when creating node
    
    In the function tipc_node_create() we protect the peer capability field
    by using the node rw_lock. However, we access the lock directly instead
    of using the dedicated functions for this, as we do everywhere else in
    node.c. This cosmetic spot is fixed here.
    
    Fixes: 40999f11ce67 ("tipc: make link capability update thread safe")
    Signed-off-by: Jon Maloy <jon.maloy@ericsson.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 540f120998dff8c22e4a5ff734732f4550f2765b
Author: Vadim Pasternak <vadimp@mellanox.com>
Date:   Sun Feb 17 18:15:30 2019 +0000

    platform/mellanox: mlxreg-hotplug: Fix KASAN warning
    
    [ Upstream commit e4c275f77624961b56cce397814d9d770a45ac59 ]
    
    Fix the following KASAN warning produced when booting a 64-bit kernel:
    [   13.334750] BUG: KASAN: stack-out-of-bounds in find_first_bit+0x19/0x70
    [   13.342166] Read of size 8 at addr ffff880235067178 by task kworker/2:1/42
    [   13.342176] CPU: 2 PID: 42 Comm: kworker/2:1 Not tainted 4.20.0-rc1+ #106
    [   13.342179] Hardware name: Mellanox Technologies Ltd. MSN2740/Mellanox x86 SFF board, BIOS 5.6.5 06/07/2016
    [   13.342190] Workqueue: events deferred_probe_work_func
    [   13.342194] Call Trace:
    [   13.342206]  dump_stack+0xc7/0x15b
    [   13.342214]  ? show_regs_print_info+0x5/0x5
    [   13.342220]  ? kmsg_dump_rewind_nolock+0x59/0x59
    [   13.342234]  ? _raw_write_lock_irqsave+0x100/0x100
    [   13.351593]  print_address_description+0x73/0x260
    [   13.351603]  kasan_report+0x260/0x380
    [   13.351611]  ? find_first_bit+0x19/0x70
    [   13.351619]  find_first_bit+0x19/0x70
    [   13.351630]  mlxreg_hotplug_work_handler+0x73c/0x920 [mlxreg_hotplug]
    [   13.351639]  ? __lock_text_start+0x8/0x8
    [   13.351646]  ? _raw_write_lock_irqsave+0x80/0x100
    [   13.351656]  ? mlxreg_hotplug_remove+0x1e0/0x1e0 [mlxreg_hotplug]
    [   13.351663]  ? regmap_volatile+0x40/0xb0
    [   13.351668]  ? regcache_write+0x4c/0x90
    [   13.351676]  ? mlxplat_mlxcpld_reg_write+0x24/0x30 [mlx_platform]
    [   13.351681]  ? _regmap_write+0xea/0x220
    [   13.351688]  ? __mutex_lock_slowpath+0x10/0x10
    [   13.351696]  ? devm_add_action+0x70/0x70
    [   13.351701]  ? mutex_unlock+0x1d/0x40
    [   13.351710]  mlxreg_hotplug_probe+0x82e/0x989 [mlxreg_hotplug]
    [   13.351723]  ? mlxreg_hotplug_work_handler+0x920/0x920 [mlxreg_hotplug]
    [   13.351731]  ? sysfs_do_create_link_sd.isra.2+0xf4/0x190
    [   13.351737]  ? sysfs_rename_link_ns+0xf0/0xf0
    [   13.351743]  ? devres_close_group+0x2b0/0x2b0
    [   13.351749]  ? pinctrl_put+0x20/0x20
    [   13.351755]  ? acpi_dev_pm_attach+0x2c/0xd0
    [   13.351763]  platform_drv_probe+0x70/0xd0
    [   13.351771]  really_probe+0x480/0x6e0
    [   13.351778]  ? device_attach+0x10/0x10
    [   13.351784]  ? __lock_text_start+0x8/0x8
    [   13.351790]  ? _raw_write_lock_irqsave+0x80/0x100
    [   13.351797]  ? _raw_write_lock_irqsave+0x80/0x100
    [   13.351806]  ? __driver_attach+0x190/0x190
    [   13.351812]  driver_probe_device+0x17d/0x1a0
    [   13.351819]  ? __driver_attach+0x190/0x190
    [   13.351825]  bus_for_each_drv+0xd6/0x130
    [   13.351831]  ? bus_rescan_devices+0x20/0x20
    [   13.351837]  ? __mutex_lock_slowpath+0x10/0x10
    [   13.351845]  __device_attach+0x18c/0x230
    [   13.351852]  ? device_bind_driver+0x70/0x70
    [   13.351859]  ? __mutex_lock_slowpath+0x10/0x10
    [   13.351866]  bus_probe_device+0xea/0x110
    [   13.351874]  deferred_probe_work_func+0x1c9/0x290
    [   13.351882]  ? driver_deferred_probe_add+0x1d0/0x1d0
    [   13.351889]  ? preempt_notifier_dec+0x20/0x20
    [   13.351897]  ? read_word_at_a_time+0xe/0x20
    [   13.351904]  ? strscpy+0x151/0x290
    [   13.351912]  ? set_work_pool_and_clear_pending+0x9c/0xf0
    [   13.351918]  ? __switch_to_asm+0x34/0x70
    [   13.351924]  ? __switch_to_asm+0x40/0x70
    [   13.351929]  ? __switch_to_asm+0x34/0x70
    [   13.351935]  ? __switch_to_asm+0x40/0x70
    [   13.351942]  process_one_work+0x5cc/0xa00
    [   13.351952]  ? pwq_dec_nr_in_flight+0x1e0/0x1e0
    [   13.351960]  ? pci_mmcfg_check_reserved+0x80/0xb8
    [   13.351967]  ? run_rebalance_domains+0x250/0x250
    [   13.351980]  ? stack_access_ok+0x35/0x80
    [   13.351986]  ? deref_stack_reg+0xa1/0xe0
    [   13.351994]  ? schedule+0xcd/0x250
    [   13.352000]  ? worker_enter_idle+0x2d6/0x330
    [   13.352006]  ? __schedule+0xeb0/0xeb0
    [   13.352014]  ? fork_usermode_blob+0x130/0x130
    [   13.352019]  ? mutex_lock+0xa7/0x100
    [   13.352026]  ? _raw_spin_lock_irq+0x98/0xf0
    [   13.352032]  ? _raw_read_unlock_irqrestore+0x30/0x30
    [   13.352037] i2c i2c-2: Added multiplexed i2c bus 11
    [   13.352043]  worker_thread+0x181/0xa80
    [   13.352052]  ? __switch_to_asm+0x34/0x70
    [   13.352058]  ? __switch_to_asm+0x40/0x70
    [   13.352064]  ? process_one_work+0xa00/0xa00
    [   13.352070]  ? __switch_to_asm+0x34/0x70
    [   13.352076]  ? __switch_to_asm+0x40/0x70
    [   13.352081]  ? __switch_to_asm+0x34/0x70
    [   13.352086]  ? __switch_to_asm+0x40/0x70
    [   13.352092]  ? __switch_to_asm+0x34/0x70
    [   13.352097]  ? __switch_to_asm+0x40/0x70
    [   13.352105]  ? __schedule+0x3d6/0xeb0
    [   13.352112]  ? migrate_swap_stop+0x470/0x470
    [   13.352119]  ? save_stack+0x89/0xb0
    [   13.352127]  ? kmem_cache_alloc_trace+0xe5/0x570
    [   13.352132]  ? kthread+0x59/0x1d0
    [   13.352138]  ? ret_from_fork+0x35/0x40
    [   13.352154]  ? __schedule+0xeb0/0xeb0
    [   13.352161]  ? remove_wait_queue+0x150/0x150
    [   13.352169]  ? _raw_write_lock_irqsave+0x80/0x100
    [   13.352175]  ? __lock_text_start+0x8/0x8
    [   13.352183]  ? process_one_work+0xa00/0xa00
    [   13.352188]  kthread+0x1a4/0x1d0
    [   13.352195]  ? kthread_create_worker_on_cpu+0xc0/0xc0
    [   13.352202]  ret_from_fork+0x35/0x40
    
    [   13.353879] The buggy address belongs to the page:
    [   13.353885] page:ffffea0008d419c0 count:0 mapcount:0 mapping:0000000000000000 index:0x0
    [   13.353890] flags: 0x2ffff8000000000()
    [   13.353897] raw: 02ffff8000000000 ffffea0008d419c8 ffffea0008d419c8 0000000000000000
    [   13.353903] raw: 0000000000000000 0000000000000000 00000000ffffffff 0000000000000000
    [   13.353905] page dumped because: kasan: bad access detected
    
    [   13.353908] Memory state around the buggy address:
    [   13.353912]  ffff880235067000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [   13.353917]  ffff880235067080: 00 00 00 00 00 00 00 00 00 00 00 f1 f1 f1 f1 04
    [   13.353921] >ffff880235067100: f2 f2 f2 f2 f2 f2 f2 04 f2 f2 f2 f2 f2 f2 f2 04
    [   13.353923]                                                                 ^
    [   13.353927]  ffff880235067180: f2 f2 f2 f2 f2 f2 f2 04 f2 f2 f2 00 00 00 00 00
    [   13.353931]  ffff880235067200: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [   13.353933] ==================================================================
    
    The warning is caused by the below loop:
            for_each_set_bit(bit, (unsigned long *)&asserted, 8) {
    while "asserted" is declared as 'unsigned'.
    
    The casting of 32-bit unsigned integer pointer to a 64-bit unsigned long
    pointer. There are two problems here.
    It causes the access of four extra byte, which can corrupt memory
    The 32-bit pointer address may not be 64-bit aligned.
    
    The fix changes variable "asserted" to "unsigned long".
    
    Fixes: 1f976f6978bf ("platform/x86: Move Mellanox platform hotplug driver to platform/mellanox")
    Signed-off-by: Vadim Pasternak <vadimp@mellanox.com>
    Signed-off-by: Darren Hart (VMware) <dvhart@infradead.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 07a31820b241fff43a9ad5619db8c663f13dd6c0
Author: Vadim Pasternak <vadimp@mellanox.com>
Date:   Sun Feb 17 18:15:30 2019 +0000

    platform/mellanox: mlxreg-hotplug: Fix KASAN warning
    
    [ Upstream commit e4c275f77624961b56cce397814d9d770a45ac59 ]
    
    Fix the following KASAN warning produced when booting a 64-bit kernel:
    [   13.334750] BUG: KASAN: stack-out-of-bounds in find_first_bit+0x19/0x70
    [   13.342166] Read of size 8 at addr ffff880235067178 by task kworker/2:1/42
    [   13.342176] CPU: 2 PID: 42 Comm: kworker/2:1 Not tainted 4.20.0-rc1+ #106
    [   13.342179] Hardware name: Mellanox Technologies Ltd. MSN2740/Mellanox x86 SFF board, BIOS 5.6.5 06/07/2016
    [   13.342190] Workqueue: events deferred_probe_work_func
    [   13.342194] Call Trace:
    [   13.342206]  dump_stack+0xc7/0x15b
    [   13.342214]  ? show_regs_print_info+0x5/0x5
    [   13.342220]  ? kmsg_dump_rewind_nolock+0x59/0x59
    [   13.342234]  ? _raw_write_lock_irqsave+0x100/0x100
    [   13.351593]  print_address_description+0x73/0x260
    [   13.351603]  kasan_report+0x260/0x380
    [   13.351611]  ? find_first_bit+0x19/0x70
    [   13.351619]  find_first_bit+0x19/0x70
    [   13.351630]  mlxreg_hotplug_work_handler+0x73c/0x920 [mlxreg_hotplug]
    [   13.351639]  ? __lock_text_start+0x8/0x8
    [   13.351646]  ? _raw_write_lock_irqsave+0x80/0x100
    [   13.351656]  ? mlxreg_hotplug_remove+0x1e0/0x1e0 [mlxreg_hotplug]
    [   13.351663]  ? regmap_volatile+0x40/0xb0
    [   13.351668]  ? regcache_write+0x4c/0x90
    [   13.351676]  ? mlxplat_mlxcpld_reg_write+0x24/0x30 [mlx_platform]
    [   13.351681]  ? _regmap_write+0xea/0x220
    [   13.351688]  ? __mutex_lock_slowpath+0x10/0x10
    [   13.351696]  ? devm_add_action+0x70/0x70
    [   13.351701]  ? mutex_unlock+0x1d/0x40
    [   13.351710]  mlxreg_hotplug_probe+0x82e/0x989 [mlxreg_hotplug]
    [   13.351723]  ? mlxreg_hotplug_work_handler+0x920/0x920 [mlxreg_hotplug]
    [   13.351731]  ? sysfs_do_create_link_sd.isra.2+0xf4/0x190
    [   13.351737]  ? sysfs_rename_link_ns+0xf0/0xf0
    [   13.351743]  ? devres_close_group+0x2b0/0x2b0
    [   13.351749]  ? pinctrl_put+0x20/0x20
    [   13.351755]  ? acpi_dev_pm_attach+0x2c/0xd0
    [   13.351763]  platform_drv_probe+0x70/0xd0
    [   13.351771]  really_probe+0x480/0x6e0
    [   13.351778]  ? device_attach+0x10/0x10
    [   13.351784]  ? __lock_text_start+0x8/0x8
    [   13.351790]  ? _raw_write_lock_irqsave+0x80/0x100
    [   13.351797]  ? _raw_write_lock_irqsave+0x80/0x100
    [   13.351806]  ? __driver_attach+0x190/0x190
    [   13.351812]  driver_probe_device+0x17d/0x1a0
    [   13.351819]  ? __driver_attach+0x190/0x190
    [   13.351825]  bus_for_each_drv+0xd6/0x130
    [   13.351831]  ? bus_rescan_devices+0x20/0x20
    [   13.351837]  ? __mutex_lock_slowpath+0x10/0x10
    [   13.351845]  __device_attach+0x18c/0x230
    [   13.351852]  ? device_bind_driver+0x70/0x70
    [   13.351859]  ? __mutex_lock_slowpath+0x10/0x10
    [   13.351866]  bus_probe_device+0xea/0x110
    [   13.351874]  deferred_probe_work_func+0x1c9/0x290
    [   13.351882]  ? driver_deferred_probe_add+0x1d0/0x1d0
    [   13.351889]  ? preempt_notifier_dec+0x20/0x20
    [   13.351897]  ? read_word_at_a_time+0xe/0x20
    [   13.351904]  ? strscpy+0x151/0x290
    [   13.351912]  ? set_work_pool_and_clear_pending+0x9c/0xf0
    [   13.351918]  ? __switch_to_asm+0x34/0x70
    [   13.351924]  ? __switch_to_asm+0x40/0x70
    [   13.351929]  ? __switch_to_asm+0x34/0x70
    [   13.351935]  ? __switch_to_asm+0x40/0x70
    [   13.351942]  process_one_work+0x5cc/0xa00
    [   13.351952]  ? pwq_dec_nr_in_flight+0x1e0/0x1e0
    [   13.351960]  ? pci_mmcfg_check_reserved+0x80/0xb8
    [   13.351967]  ? run_rebalance_domains+0x250/0x250
    [   13.351980]  ? stack_access_ok+0x35/0x80
    [   13.351986]  ? deref_stack_reg+0xa1/0xe0
    [   13.351994]  ? schedule+0xcd/0x250
    [   13.352000]  ? worker_enter_idle+0x2d6/0x330
    [   13.352006]  ? __schedule+0xeb0/0xeb0
    [   13.352014]  ? fork_usermode_blob+0x130/0x130
    [   13.352019]  ? mutex_lock+0xa7/0x100
    [   13.352026]  ? _raw_spin_lock_irq+0x98/0xf0
    [   13.352032]  ? _raw_read_unlock_irqrestore+0x30/0x30
    [   13.352037] i2c i2c-2: Added multiplexed i2c bus 11
    [   13.352043]  worker_thread+0x181/0xa80
    [   13.352052]  ? __switch_to_asm+0x34/0x70
    [   13.352058]  ? __switch_to_asm+0x40/0x70
    [   13.352064]  ? process_one_work+0xa00/0xa00
    [   13.352070]  ? __switch_to_asm+0x34/0x70
    [   13.352076]  ? __switch_to_asm+0x40/0x70
    [   13.352081]  ? __switch_to_asm+0x34/0x70
    [   13.352086]  ? __switch_to_asm+0x40/0x70
    [   13.352092]  ? __switch_to_asm+0x34/0x70
    [   13.352097]  ? __switch_to_asm+0x40/0x70
    [   13.352105]  ? __schedule+0x3d6/0xeb0
    [   13.352112]  ? migrate_swap_stop+0x470/0x470
    [   13.352119]  ? save_stack+0x89/0xb0
    [   13.352127]  ? kmem_cache_alloc_trace+0xe5/0x570
    [   13.352132]  ? kthread+0x59/0x1d0
    [   13.352138]  ? ret_from_fork+0x35/0x40
    [   13.352154]  ? __schedule+0xeb0/0xeb0
    [   13.352161]  ? remove_wait_queue+0x150/0x150
    [   13.352169]  ? _raw_write_lock_irqsave+0x80/0x100
    [   13.352175]  ? __lock_text_start+0x8/0x8
    [   13.352183]  ? process_one_work+0xa00/0xa00
    [   13.352188]  kthread+0x1a4/0x1d0
    [   13.352195]  ? kthread_create_worker_on_cpu+0xc0/0xc0
    [   13.352202]  ret_from_fork+0x35/0x40
    
    [   13.353879] The buggy address belongs to the page:
    [   13.353885] page:ffffea0008d419c0 count:0 mapcount:0 mapping:0000000000000000 index:0x0
    [   13.353890] flags: 0x2ffff8000000000()
    [   13.353897] raw: 02ffff8000000000 ffffea0008d419c8 ffffea0008d419c8 0000000000000000
    [   13.353903] raw: 0000000000000000 0000000000000000 00000000ffffffff 0000000000000000
    [   13.353905] page dumped because: kasan: bad access detected
    
    [   13.353908] Memory state around the buggy address:
    [   13.353912]  ffff880235067000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [   13.353917]  ffff880235067080: 00 00 00 00 00 00 00 00 00 00 00 f1 f1 f1 f1 04
    [   13.353921] >ffff880235067100: f2 f2 f2 f2 f2 f2 f2 04 f2 f2 f2 f2 f2 f2 f2 04
    [   13.353923]                                                                 ^
    [   13.353927]  ffff880235067180: f2 f2 f2 f2 f2 f2 f2 04 f2 f2 f2 00 00 00 00 00
    [   13.353931]  ffff880235067200: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [   13.353933] ==================================================================
    
    The warning is caused by the below loop:
            for_each_set_bit(bit, (unsigned long *)&asserted, 8) {
    while "asserted" is declared as 'unsigned'.
    
    The casting of 32-bit unsigned integer pointer to a 64-bit unsigned long
    pointer. There are two problems here.
    It causes the access of four extra byte, which can corrupt memory
    The 32-bit pointer address may not be 64-bit aligned.
    
    The fix changes variable "asserted" to "unsigned long".
    
    Fixes: 1f976f6978bf ("platform/x86: Move Mellanox platform hotplug driver to platform/mellanox")
    Signed-off-by: Vadim Pasternak <vadimp@mellanox.com>
    Signed-off-by: Darren Hart (VMware) <dvhart@infradead.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit ff9d31d0d46672e201fc9ff59c42f1eef5f00c77
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Wed Mar 20 12:53:33 2019 -0500

    tracing: Remove unnecessary var_ref destroy in track_data_destroy()
    
    Commit 656fe2ba85e8 (tracing: Use hist trigger's var_ref array to
    destroy var_refs) centralized the destruction of all the var_refs
    in one place so that other code didn't have to do it.
    
    The track_data_destroy() added later ignored that and also destroyed
    the track_data var_ref, causing a double-free error flagged by KASAN.
    
    ==================================================================
    BUG: KASAN: use-after-free in destroy_hist_field+0x30/0x70
    Read of size 8 at addr ffff888086df2210 by task bash/1694
    
    CPU: 6 PID: 1694 Comm: bash Not tainted 5.1.0-rc1-test+ #15
    Hardware name: Hewlett-Packard HP Compaq Pro 6300 SFF/339A, BIOS K01 v03.03
    07/14/2016
    Call Trace:
     dump_stack+0x71/0xa0
     ? destroy_hist_field+0x30/0x70
     print_address_description.cold.3+0x9/0x1fb
     ? destroy_hist_field+0x30/0x70
     ? destroy_hist_field+0x30/0x70
     kasan_report.cold.4+0x1a/0x33
     ? __kasan_slab_free+0x100/0x150
     ? destroy_hist_field+0x30/0x70
     destroy_hist_field+0x30/0x70
     track_data_destroy+0x55/0xe0
     destroy_hist_data+0x1f0/0x350
     hist_unreg_all+0x203/0x220
     event_trigger_open+0xbb/0x130
     do_dentry_open+0x296/0x700
     ? stacktrace_count_trigger+0x30/0x30
     ? generic_permission+0x56/0x200
     ? __x64_sys_fchdir+0xd0/0xd0
     ? inode_permission+0x55/0x200
     ? security_inode_permission+0x18/0x60
     path_openat+0x633/0x22b0
     ? path_lookupat.isra.50+0x420/0x420
     ? __kasan_kmalloc.constprop.12+0xc1/0xd0
     ? kmem_cache_alloc+0xe5/0x260
     ? getname_flags+0x6c/0x2a0
     ? do_sys_open+0x149/0x2b0
     ? do_syscall_64+0x73/0x1b0
     ? entry_SYSCALL_64_after_hwframe+0x44/0xa9
     ? _raw_write_lock_bh+0xe0/0xe0
     ? __kernel_text_address+0xe/0x30
     ? unwind_get_return_address+0x2f/0x50
     ? __list_add_valid+0x2d/0x70
     ? deactivate_slab.isra.62+0x1f4/0x5a0
     ? getname_flags+0x6c/0x2a0
     ? set_track+0x76/0x120
     do_filp_open+0x11a/0x1a0
     ? may_open_dev+0x50/0x50
     ? _raw_spin_lock+0x7a/0xd0
     ? _raw_write_lock_bh+0xe0/0xe0
     ? __alloc_fd+0x10f/0x200
     do_sys_open+0x1db/0x2b0
     ? filp_open+0x50/0x50
     do_syscall_64+0x73/0x1b0
     entry_SYSCALL_64_after_hwframe+0x44/0xa9
    RIP: 0033:0x7fa7b24a4ca2
    Code: 25 00 00 41 00 3d 00 00 41 00 74 4c 48 8d 05 85 7a 0d 00 8b 00 85 c0
    75 6d 89 f2 b8 01 01 00 00 48 89 fe bf 9c ff ff ff 0f 05 <48> 3d 00 f0 ff ff
    0f 87 a2 00 00 00 48 8b 4c 24 28 64 48 33 0c 25
    RSP: 002b:00007fffbafb3af0 EFLAGS: 00000246 ORIG_RAX: 0000000000000101
    RAX: ffffffffffffffda RBX: 000055d3648ade30 RCX: 00007fa7b24a4ca2
    RDX: 0000000000000241 RSI: 000055d364a55240 RDI: 00000000ffffff9c
    RBP: 00007fffbafb3bf0 R08: 0000000000000020 R09: 0000000000000002
    R10: 00000000000001b6 R11: 0000000000000246 R12: 0000000000000000
    R13: 0000000000000003 R14: 0000000000000001 R15: 000055d364a55240
    ==================================================================
    
    So remove the track_data_destroy() destroy_hist_field() call for that
    var_ref.
    
    Link: http://lkml.kernel.org/r/1deffec420f6a16d11dd8647318d34a66d1989a9.camel@linux.intel.com
    
    Fixes: 466f4528fbc69 ("tracing: Generalize hist trigger onmax and save action")
    Reported-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

commit c1b7d9363334cee2447ddfaff18f7853ceb20e9d
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Feb 23 13:24:59 2019 -0800

    net/x25: fix a race in x25_bind()
    
    [ Upstream commit 797a22bd5298c2674d927893f46cadf619dad11d ]
    
    syzbot was able to trigger another soft lockup [1]
    
    I first thought it was the O(N^2) issue I mentioned in my
    prior fix (f657d22ee1f "net/x25: do not hold the cpu
    too long in x25_new_lci()"), but I eventually found
    that x25_bind() was not checking SOCK_ZAPPED state under
    socket lock protection.
    
    This means that multiple threads can end up calling
    x25_insert_socket() for the same socket, and corrupt x25_list
    
    [1]
    watchdog: BUG: soft lockup - CPU#0 stuck for 123s! [syz-executor.2:10492]
    Modules linked in:
    irq event stamp: 27515
    hardirqs last  enabled at (27514): [<ffffffff81006673>] trace_hardirqs_on_thunk+0x1a/0x1c
    hardirqs last disabled at (27515): [<ffffffff8100668f>] trace_hardirqs_off_thunk+0x1a/0x1c
    softirqs last  enabled at (32): [<ffffffff8632ee73>] x25_get_neigh+0xa3/0xd0 net/x25/x25_link.c:336
    softirqs last disabled at (34): [<ffffffff86324bc3>] x25_find_socket+0x23/0x140 net/x25/af_x25.c:341
    CPU: 0 PID: 10492 Comm: syz-executor.2 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__sanitizer_cov_trace_pc+0x4/0x50 kernel/kcov.c:97
    Code: f4 ff ff ff e8 11 9f ea ff 48 c7 05 12 fb e5 08 00 00 00 00 e9 c8 e9 ff ff 90 90 90 90 90 90 90 90 90 90 90 90 90 55 48 89 e5 <48> 8b 75 08 65 48 8b 04 25 40 ee 01 00 65 8b 15 38 0c 92 7e 81 e2
    RSP: 0018:ffff88806e94fc48 EFLAGS: 00000286 ORIG_RAX: ffffffffffffff13
    RAX: 1ffff1100d84dac5 RBX: 0000000000000001 RCX: ffffc90006197000
    RDX: 0000000000040000 RSI: ffffffff86324bf3 RDI: ffff88806c26d628
    RBP: ffff88806e94fc48 R08: ffff88806c1c6500 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: ffff88806c26d628
    R13: ffff888090455200 R14: dffffc0000000000 R15: 0000000000000000
    FS:  00007f3a107e4700(0000) GS:ffff8880ae800000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007f3a107e3db8 CR3: 00000000a5544000 CR4: 00000000001406f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     __x25_find_socket net/x25/af_x25.c:327 [inline]
     x25_find_socket+0x7d/0x140 net/x25/af_x25.c:342
     x25_new_lci net/x25/af_x25.c:355 [inline]
     x25_connect+0x380/0xde0 net/x25/af_x25.c:784
     __sys_connect+0x266/0x330 net/socket.c:1662
     __do_sys_connect net/socket.c:1673 [inline]
     __se_sys_connect net/socket.c:1670 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1670
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007f3a107e3c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e29
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000005
    RBP: 000000000073c040 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007f3a107e46d4
    R13: 00000000004be362 R14: 00000000004ceb98 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 10493 Comm: syz-executor.3 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x143/0x290 kernel/locking/qrwlock.c:86
    Code: 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 41 0f b6 55 00 <41> 38 d7 7c eb 84 d2 74 e7 48 89 df e8 cc aa 4e 00 eb dd be 04 00
    RSP: 0018:ffff888085c47bd8 EFLAGS: 00000206
    RAX: 0000000000000300 RBX: ffffffff89412b00 RCX: 1ffffffff1282560
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89412b00
    RBP: ffff888085c47c70 R08: 1ffffffff1282560 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: 00000000000000ff
    R13: fffffbfff1282560 R14: 1ffff11010b88f7d R15: 0000000000000003
    FS:  00007fdd04086700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007fdd04064db8 CR3: 0000000090be0000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:703
     __sys_bind+0x23f/0x290 net/socket.c:1481
     __do_sys_bind net/socket.c:1492 [inline]
     __se_sys_bind net/socket.c:1490 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1490
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    
    Fixes: 90c27297a9bf ("X.25 remove bkl in bind")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: andrew hendry <andrew.hendry@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f14e4744b29ae9ab50b589a4426478a539a23218
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Feb 23 13:24:59 2019 -0800

    net/x25: fix a race in x25_bind()
    
    [ Upstream commit 797a22bd5298c2674d927893f46cadf619dad11d ]
    
    syzbot was able to trigger another soft lockup [1]
    
    I first thought it was the O(N^2) issue I mentioned in my
    prior fix (f657d22ee1f "net/x25: do not hold the cpu
    too long in x25_new_lci()"), but I eventually found
    that x25_bind() was not checking SOCK_ZAPPED state under
    socket lock protection.
    
    This means that multiple threads can end up calling
    x25_insert_socket() for the same socket, and corrupt x25_list
    
    [1]
    watchdog: BUG: soft lockup - CPU#0 stuck for 123s! [syz-executor.2:10492]
    Modules linked in:
    irq event stamp: 27515
    hardirqs last  enabled at (27514): [<ffffffff81006673>] trace_hardirqs_on_thunk+0x1a/0x1c
    hardirqs last disabled at (27515): [<ffffffff8100668f>] trace_hardirqs_off_thunk+0x1a/0x1c
    softirqs last  enabled at (32): [<ffffffff8632ee73>] x25_get_neigh+0xa3/0xd0 net/x25/x25_link.c:336
    softirqs last disabled at (34): [<ffffffff86324bc3>] x25_find_socket+0x23/0x140 net/x25/af_x25.c:341
    CPU: 0 PID: 10492 Comm: syz-executor.2 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__sanitizer_cov_trace_pc+0x4/0x50 kernel/kcov.c:97
    Code: f4 ff ff ff e8 11 9f ea ff 48 c7 05 12 fb e5 08 00 00 00 00 e9 c8 e9 ff ff 90 90 90 90 90 90 90 90 90 90 90 90 90 55 48 89 e5 <48> 8b 75 08 65 48 8b 04 25 40 ee 01 00 65 8b 15 38 0c 92 7e 81 e2
    RSP: 0018:ffff88806e94fc48 EFLAGS: 00000286 ORIG_RAX: ffffffffffffff13
    RAX: 1ffff1100d84dac5 RBX: 0000000000000001 RCX: ffffc90006197000
    RDX: 0000000000040000 RSI: ffffffff86324bf3 RDI: ffff88806c26d628
    RBP: ffff88806e94fc48 R08: ffff88806c1c6500 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: ffff88806c26d628
    R13: ffff888090455200 R14: dffffc0000000000 R15: 0000000000000000
    FS:  00007f3a107e4700(0000) GS:ffff8880ae800000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007f3a107e3db8 CR3: 00000000a5544000 CR4: 00000000001406f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     __x25_find_socket net/x25/af_x25.c:327 [inline]
     x25_find_socket+0x7d/0x140 net/x25/af_x25.c:342
     x25_new_lci net/x25/af_x25.c:355 [inline]
     x25_connect+0x380/0xde0 net/x25/af_x25.c:784
     __sys_connect+0x266/0x330 net/socket.c:1662
     __do_sys_connect net/socket.c:1673 [inline]
     __se_sys_connect net/socket.c:1670 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1670
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007f3a107e3c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e29
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000005
    RBP: 000000000073c040 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007f3a107e46d4
    R13: 00000000004be362 R14: 00000000004ceb98 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 10493 Comm: syz-executor.3 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x143/0x290 kernel/locking/qrwlock.c:86
    Code: 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 41 0f b6 55 00 <41> 38 d7 7c eb 84 d2 74 e7 48 89 df e8 cc aa 4e 00 eb dd be 04 00
    RSP: 0018:ffff888085c47bd8 EFLAGS: 00000206
    RAX: 0000000000000300 RBX: ffffffff89412b00 RCX: 1ffffffff1282560
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89412b00
    RBP: ffff888085c47c70 R08: 1ffffffff1282560 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: 00000000000000ff
    R13: fffffbfff1282560 R14: 1ffff11010b88f7d R15: 0000000000000003
    FS:  00007fdd04086700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007fdd04064db8 CR3: 0000000090be0000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:703
     __sys_bind+0x23f/0x290 net/socket.c:1481
     __do_sys_bind net/socket.c:1492 [inline]
     __se_sys_bind net/socket.c:1490 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1490
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    
    Fixes: 90c27297a9bf ("X.25 remove bkl in bind")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: andrew hendry <andrew.hendry@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 92382cd957f41af4ed649e00575d8b20e0ff929f
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Feb 23 13:24:59 2019 -0800

    net/x25: fix a race in x25_bind()
    
    [ Upstream commit 797a22bd5298c2674d927893f46cadf619dad11d ]
    
    syzbot was able to trigger another soft lockup [1]
    
    I first thought it was the O(N^2) issue I mentioned in my
    prior fix (f657d22ee1f "net/x25: do not hold the cpu
    too long in x25_new_lci()"), but I eventually found
    that x25_bind() was not checking SOCK_ZAPPED state under
    socket lock protection.
    
    This means that multiple threads can end up calling
    x25_insert_socket() for the same socket, and corrupt x25_list
    
    [1]
    watchdog: BUG: soft lockup - CPU#0 stuck for 123s! [syz-executor.2:10492]
    Modules linked in:
    irq event stamp: 27515
    hardirqs last  enabled at (27514): [<ffffffff81006673>] trace_hardirqs_on_thunk+0x1a/0x1c
    hardirqs last disabled at (27515): [<ffffffff8100668f>] trace_hardirqs_off_thunk+0x1a/0x1c
    softirqs last  enabled at (32): [<ffffffff8632ee73>] x25_get_neigh+0xa3/0xd0 net/x25/x25_link.c:336
    softirqs last disabled at (34): [<ffffffff86324bc3>] x25_find_socket+0x23/0x140 net/x25/af_x25.c:341
    CPU: 0 PID: 10492 Comm: syz-executor.2 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__sanitizer_cov_trace_pc+0x4/0x50 kernel/kcov.c:97
    Code: f4 ff ff ff e8 11 9f ea ff 48 c7 05 12 fb e5 08 00 00 00 00 e9 c8 e9 ff ff 90 90 90 90 90 90 90 90 90 90 90 90 90 55 48 89 e5 <48> 8b 75 08 65 48 8b 04 25 40 ee 01 00 65 8b 15 38 0c 92 7e 81 e2
    RSP: 0018:ffff88806e94fc48 EFLAGS: 00000286 ORIG_RAX: ffffffffffffff13
    RAX: 1ffff1100d84dac5 RBX: 0000000000000001 RCX: ffffc90006197000
    RDX: 0000000000040000 RSI: ffffffff86324bf3 RDI: ffff88806c26d628
    RBP: ffff88806e94fc48 R08: ffff88806c1c6500 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: ffff88806c26d628
    R13: ffff888090455200 R14: dffffc0000000000 R15: 0000000000000000
    FS:  00007f3a107e4700(0000) GS:ffff8880ae800000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007f3a107e3db8 CR3: 00000000a5544000 CR4: 00000000001406f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     __x25_find_socket net/x25/af_x25.c:327 [inline]
     x25_find_socket+0x7d/0x140 net/x25/af_x25.c:342
     x25_new_lci net/x25/af_x25.c:355 [inline]
     x25_connect+0x380/0xde0 net/x25/af_x25.c:784
     __sys_connect+0x266/0x330 net/socket.c:1662
     __do_sys_connect net/socket.c:1673 [inline]
     __se_sys_connect net/socket.c:1670 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1670
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007f3a107e3c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e29
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000005
    RBP: 000000000073c040 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007f3a107e46d4
    R13: 00000000004be362 R14: 00000000004ceb98 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 10493 Comm: syz-executor.3 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x143/0x290 kernel/locking/qrwlock.c:86
    Code: 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 41 0f b6 55 00 <41> 38 d7 7c eb 84 d2 74 e7 48 89 df e8 cc aa 4e 00 eb dd be 04 00
    RSP: 0018:ffff888085c47bd8 EFLAGS: 00000206
    RAX: 0000000000000300 RBX: ffffffff89412b00 RCX: 1ffffffff1282560
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89412b00
    RBP: ffff888085c47c70 R08: 1ffffffff1282560 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: 00000000000000ff
    R13: fffffbfff1282560 R14: 1ffff11010b88f7d R15: 0000000000000003
    FS:  00007fdd04086700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007fdd04064db8 CR3: 0000000090be0000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:703
     __sys_bind+0x23f/0x290 net/socket.c:1481
     __do_sys_bind net/socket.c:1492 [inline]
     __se_sys_bind net/socket.c:1490 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1490
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    
    Fixes: 90c27297a9bf ("X.25 remove bkl in bind")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: andrew hendry <andrew.hendry@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 64a6e35ac51036d309fbcc003e4daae672fb5849
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Feb 23 13:24:59 2019 -0800

    net/x25: fix a race in x25_bind()
    
    [ Upstream commit 797a22bd5298c2674d927893f46cadf619dad11d ]
    
    syzbot was able to trigger another soft lockup [1]
    
    I first thought it was the O(N^2) issue I mentioned in my
    prior fix (f657d22ee1f "net/x25: do not hold the cpu
    too long in x25_new_lci()"), but I eventually found
    that x25_bind() was not checking SOCK_ZAPPED state under
    socket lock protection.
    
    This means that multiple threads can end up calling
    x25_insert_socket() for the same socket, and corrupt x25_list
    
    [1]
    watchdog: BUG: soft lockup - CPU#0 stuck for 123s! [syz-executor.2:10492]
    Modules linked in:
    irq event stamp: 27515
    hardirqs last  enabled at (27514): [<ffffffff81006673>] trace_hardirqs_on_thunk+0x1a/0x1c
    hardirqs last disabled at (27515): [<ffffffff8100668f>] trace_hardirqs_off_thunk+0x1a/0x1c
    softirqs last  enabled at (32): [<ffffffff8632ee73>] x25_get_neigh+0xa3/0xd0 net/x25/x25_link.c:336
    softirqs last disabled at (34): [<ffffffff86324bc3>] x25_find_socket+0x23/0x140 net/x25/af_x25.c:341
    CPU: 0 PID: 10492 Comm: syz-executor.2 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__sanitizer_cov_trace_pc+0x4/0x50 kernel/kcov.c:97
    Code: f4 ff ff ff e8 11 9f ea ff 48 c7 05 12 fb e5 08 00 00 00 00 e9 c8 e9 ff ff 90 90 90 90 90 90 90 90 90 90 90 90 90 55 48 89 e5 <48> 8b 75 08 65 48 8b 04 25 40 ee 01 00 65 8b 15 38 0c 92 7e 81 e2
    RSP: 0018:ffff88806e94fc48 EFLAGS: 00000286 ORIG_RAX: ffffffffffffff13
    RAX: 1ffff1100d84dac5 RBX: 0000000000000001 RCX: ffffc90006197000
    RDX: 0000000000040000 RSI: ffffffff86324bf3 RDI: ffff88806c26d628
    RBP: ffff88806e94fc48 R08: ffff88806c1c6500 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: ffff88806c26d628
    R13: ffff888090455200 R14: dffffc0000000000 R15: 0000000000000000
    FS:  00007f3a107e4700(0000) GS:ffff8880ae800000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007f3a107e3db8 CR3: 00000000a5544000 CR4: 00000000001406f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     __x25_find_socket net/x25/af_x25.c:327 [inline]
     x25_find_socket+0x7d/0x140 net/x25/af_x25.c:342
     x25_new_lci net/x25/af_x25.c:355 [inline]
     x25_connect+0x380/0xde0 net/x25/af_x25.c:784
     __sys_connect+0x266/0x330 net/socket.c:1662
     __do_sys_connect net/socket.c:1673 [inline]
     __se_sys_connect net/socket.c:1670 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1670
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007f3a107e3c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e29
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000005
    RBP: 000000000073c040 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007f3a107e46d4
    R13: 00000000004be362 R14: 00000000004ceb98 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 10493 Comm: syz-executor.3 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x143/0x290 kernel/locking/qrwlock.c:86
    Code: 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 41 0f b6 55 00 <41> 38 d7 7c eb 84 d2 74 e7 48 89 df e8 cc aa 4e 00 eb dd be 04 00
    RSP: 0018:ffff888085c47bd8 EFLAGS: 00000206
    RAX: 0000000000000300 RBX: ffffffff89412b00 RCX: 1ffffffff1282560
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89412b00
    RBP: ffff888085c47c70 R08: 1ffffffff1282560 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: 00000000000000ff
    R13: fffffbfff1282560 R14: 1ffff11010b88f7d R15: 0000000000000003
    FS:  00007fdd04086700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007fdd04064db8 CR3: 0000000090be0000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:703
     __sys_bind+0x23f/0x290 net/socket.c:1481
     __do_sys_bind net/socket.c:1492 [inline]
     __se_sys_bind net/socket.c:1490 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1490
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    
    Fixes: 90c27297a9bf ("X.25 remove bkl in bind")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: andrew hendry <andrew.hendry@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 13b430574e614d2c8ee432d32d371565379f9447
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Feb 23 13:24:59 2019 -0800

    net/x25: fix a race in x25_bind()
    
    [ Upstream commit 797a22bd5298c2674d927893f46cadf619dad11d ]
    
    syzbot was able to trigger another soft lockup [1]
    
    I first thought it was the O(N^2) issue I mentioned in my
    prior fix (f657d22ee1f "net/x25: do not hold the cpu
    too long in x25_new_lci()"), but I eventually found
    that x25_bind() was not checking SOCK_ZAPPED state under
    socket lock protection.
    
    This means that multiple threads can end up calling
    x25_insert_socket() for the same socket, and corrupt x25_list
    
    [1]
    watchdog: BUG: soft lockup - CPU#0 stuck for 123s! [syz-executor.2:10492]
    Modules linked in:
    irq event stamp: 27515
    hardirqs last  enabled at (27514): [<ffffffff81006673>] trace_hardirqs_on_thunk+0x1a/0x1c
    hardirqs last disabled at (27515): [<ffffffff8100668f>] trace_hardirqs_off_thunk+0x1a/0x1c
    softirqs last  enabled at (32): [<ffffffff8632ee73>] x25_get_neigh+0xa3/0xd0 net/x25/x25_link.c:336
    softirqs last disabled at (34): [<ffffffff86324bc3>] x25_find_socket+0x23/0x140 net/x25/af_x25.c:341
    CPU: 0 PID: 10492 Comm: syz-executor.2 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__sanitizer_cov_trace_pc+0x4/0x50 kernel/kcov.c:97
    Code: f4 ff ff ff e8 11 9f ea ff 48 c7 05 12 fb e5 08 00 00 00 00 e9 c8 e9 ff ff 90 90 90 90 90 90 90 90 90 90 90 90 90 55 48 89 e5 <48> 8b 75 08 65 48 8b 04 25 40 ee 01 00 65 8b 15 38 0c 92 7e 81 e2
    RSP: 0018:ffff88806e94fc48 EFLAGS: 00000286 ORIG_RAX: ffffffffffffff13
    RAX: 1ffff1100d84dac5 RBX: 0000000000000001 RCX: ffffc90006197000
    RDX: 0000000000040000 RSI: ffffffff86324bf3 RDI: ffff88806c26d628
    RBP: ffff88806e94fc48 R08: ffff88806c1c6500 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: ffff88806c26d628
    R13: ffff888090455200 R14: dffffc0000000000 R15: 0000000000000000
    FS:  00007f3a107e4700(0000) GS:ffff8880ae800000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007f3a107e3db8 CR3: 00000000a5544000 CR4: 00000000001406f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     __x25_find_socket net/x25/af_x25.c:327 [inline]
     x25_find_socket+0x7d/0x140 net/x25/af_x25.c:342
     x25_new_lci net/x25/af_x25.c:355 [inline]
     x25_connect+0x380/0xde0 net/x25/af_x25.c:784
     __sys_connect+0x266/0x330 net/socket.c:1662
     __do_sys_connect net/socket.c:1673 [inline]
     __se_sys_connect net/socket.c:1670 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1670
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007f3a107e3c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e29
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000005
    RBP: 000000000073c040 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007f3a107e46d4
    R13: 00000000004be362 R14: 00000000004ceb98 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 10493 Comm: syz-executor.3 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x143/0x290 kernel/locking/qrwlock.c:86
    Code: 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 41 0f b6 55 00 <41> 38 d7 7c eb 84 d2 74 e7 48 89 df e8 cc aa 4e 00 eb dd be 04 00
    RSP: 0018:ffff888085c47bd8 EFLAGS: 00000206
    RAX: 0000000000000300 RBX: ffffffff89412b00 RCX: 1ffffffff1282560
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89412b00
    RBP: ffff888085c47c70 R08: 1ffffffff1282560 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: 00000000000000ff
    R13: fffffbfff1282560 R14: 1ffff11010b88f7d R15: 0000000000000003
    FS:  00007fdd04086700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007fdd04064db8 CR3: 0000000090be0000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:703
     __sys_bind+0x23f/0x290 net/socket.c:1481
     __do_sys_bind net/socket.c:1492 [inline]
     __se_sys_bind net/socket.c:1490 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1490
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    
    Fixes: 90c27297a9bf ("X.25 remove bkl in bind")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: andrew hendry <andrew.hendry@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b387896bef78d5e0b6c677296cfe1ef339a57c0d
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Feb 23 13:24:59 2019 -0800

    net/x25: fix a race in x25_bind()
    
    [ Upstream commit 797a22bd5298c2674d927893f46cadf619dad11d ]
    
    syzbot was able to trigger another soft lockup [1]
    
    I first thought it was the O(N^2) issue I mentioned in my
    prior fix (f657d22ee1f "net/x25: do not hold the cpu
    too long in x25_new_lci()"), but I eventually found
    that x25_bind() was not checking SOCK_ZAPPED state under
    socket lock protection.
    
    This means that multiple threads can end up calling
    x25_insert_socket() for the same socket, and corrupt x25_list
    
    [1]
    watchdog: BUG: soft lockup - CPU#0 stuck for 123s! [syz-executor.2:10492]
    Modules linked in:
    irq event stamp: 27515
    hardirqs last  enabled at (27514): [<ffffffff81006673>] trace_hardirqs_on_thunk+0x1a/0x1c
    hardirqs last disabled at (27515): [<ffffffff8100668f>] trace_hardirqs_off_thunk+0x1a/0x1c
    softirqs last  enabled at (32): [<ffffffff8632ee73>] x25_get_neigh+0xa3/0xd0 net/x25/x25_link.c:336
    softirqs last disabled at (34): [<ffffffff86324bc3>] x25_find_socket+0x23/0x140 net/x25/af_x25.c:341
    CPU: 0 PID: 10492 Comm: syz-executor.2 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__sanitizer_cov_trace_pc+0x4/0x50 kernel/kcov.c:97
    Code: f4 ff ff ff e8 11 9f ea ff 48 c7 05 12 fb e5 08 00 00 00 00 e9 c8 e9 ff ff 90 90 90 90 90 90 90 90 90 90 90 90 90 55 48 89 e5 <48> 8b 75 08 65 48 8b 04 25 40 ee 01 00 65 8b 15 38 0c 92 7e 81 e2
    RSP: 0018:ffff88806e94fc48 EFLAGS: 00000286 ORIG_RAX: ffffffffffffff13
    RAX: 1ffff1100d84dac5 RBX: 0000000000000001 RCX: ffffc90006197000
    RDX: 0000000000040000 RSI: ffffffff86324bf3 RDI: ffff88806c26d628
    RBP: ffff88806e94fc48 R08: ffff88806c1c6500 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: ffff88806c26d628
    R13: ffff888090455200 R14: dffffc0000000000 R15: 0000000000000000
    FS:  00007f3a107e4700(0000) GS:ffff8880ae800000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007f3a107e3db8 CR3: 00000000a5544000 CR4: 00000000001406f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     __x25_find_socket net/x25/af_x25.c:327 [inline]
     x25_find_socket+0x7d/0x140 net/x25/af_x25.c:342
     x25_new_lci net/x25/af_x25.c:355 [inline]
     x25_connect+0x380/0xde0 net/x25/af_x25.c:784
     __sys_connect+0x266/0x330 net/socket.c:1662
     __do_sys_connect net/socket.c:1673 [inline]
     __se_sys_connect net/socket.c:1670 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1670
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007f3a107e3c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e29
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000005
    RBP: 000000000073c040 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007f3a107e46d4
    R13: 00000000004be362 R14: 00000000004ceb98 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 10493 Comm: syz-executor.3 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x143/0x290 kernel/locking/qrwlock.c:86
    Code: 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 41 0f b6 55 00 <41> 38 d7 7c eb 84 d2 74 e7 48 89 df e8 cc aa 4e 00 eb dd be 04 00
    RSP: 0018:ffff888085c47bd8 EFLAGS: 00000206
    RAX: 0000000000000300 RBX: ffffffff89412b00 RCX: 1ffffffff1282560
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89412b00
    RBP: ffff888085c47c70 R08: 1ffffffff1282560 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: 00000000000000ff
    R13: fffffbfff1282560 R14: 1ffff11010b88f7d R15: 0000000000000003
    FS:  00007fdd04086700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007fdd04064db8 CR3: 0000000090be0000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:703
     __sys_bind+0x23f/0x290 net/socket.c:1481
     __do_sys_bind net/socket.c:1492 [inline]
     __se_sys_bind net/socket.c:1490 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1490
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    
    Fixes: 90c27297a9bf ("X.25 remove bkl in bind")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: andrew hendry <andrew.hendry@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c42d28ce3e16dbd88e575c0acfda96d221dae2c9
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Feb 2 17:33:01 2019 +0800

    f2fs: fix potential data inconsistence of checkpoint
    
    Previously, we changed lock from cp_rwsem to node_change, it solved
    the deadlock issue which was caused by below race condition:
    
    Thread A                        Thread B
    - f2fs_setattr
     - f2fs_lock_op  -- read_lock
     - dquot_transfer
      - __dquot_transfer
       - dquot_acquire
        - commit_dqblk
         - f2fs_quota_write
          - f2fs_write_begin
           - f2fs_write_failed
                                    - write_checkpoint
                                     - block_operations
                                      - f2fs_lock_all  -- write_lock
            - f2fs_truncate_blocks
             - f2fs_lock_op  -- read_lock
    
    But it breaks the sematics of cp_rwsem, in other callers like:
    - f2fs_file_write_iter -> f2fs_write_begin -> f2fs_write_failed
    - f2fs_direct_IO -> f2fs_write_failed
    
    We allow to truncate dnode w/o cp_rwsem held, result in incorrect sit
    bitmap update, which can cause further data corruption.
    
    So this patch reverts previous fix implementation, and try to fix
    deadlock by skipping calling f2fs_truncate_blocks() in f2fs_write_failed()
    only for quota file, and keep the preallocated data/node in the tail of
    quota file, we can expecte that the preallocated space can be used to
    store quota info latter soon.
    
    Fixes: af033b2aa8a8 ("f2fs: guarantee journalled quota data by checkpoint")
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Signed-off-by: Sheng Yong <shengyong1@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

commit 38e3eebff643db725633657d1d87a3be019d1018
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Wed Jan 16 11:00:57 2019 -0500

    btrfs: honor path->skip_locking in backref code
    
    Qgroups will do the old roots lookup at delayed ref time, which could be
    while walking down the extent root while running a delayed ref.  This
    should be fine, except we specifically lock eb's in the backref walking
    code irrespective of path->skip_locking, which deadlocks the system.
    Fix up the backref code to honor path->skip_locking, nobody will be
    modifying the commit_root when we're searching so it's completely safe
    to do.
    
    This happens since fb235dc06fac ("btrfs: qgroup: Move half of the qgroup
    accounting time out of commit trans"), kernel may lockup with quota
    enabled.
    
    There is one backref trace triggered by snapshot dropping along with
    write operation in the source subvolume.  The example can be reliably
    reproduced:
    
      btrfs-cleaner   D    0  4062      2 0x80000000
      Call Trace:
       schedule+0x32/0x90
       btrfs_tree_read_lock+0x93/0x130 [btrfs]
       find_parent_nodes+0x29b/0x1170 [btrfs]
       btrfs_find_all_roots_safe+0xa8/0x120 [btrfs]
       btrfs_find_all_roots+0x57/0x70 [btrfs]
       btrfs_qgroup_trace_extent_post+0x37/0x70 [btrfs]
       btrfs_qgroup_trace_leaf_items+0x10b/0x140 [btrfs]
       btrfs_qgroup_trace_subtree+0xc8/0xe0 [btrfs]
       do_walk_down+0x541/0x5e3 [btrfs]
       walk_down_tree+0xab/0xe7 [btrfs]
       btrfs_drop_snapshot+0x356/0x71a [btrfs]
       btrfs_clean_one_deleted_snapshot+0xb8/0xf0 [btrfs]
       cleaner_kthread+0x12b/0x160 [btrfs]
       kthread+0x112/0x130
       ret_from_fork+0x27/0x50
    
    When dropping snapshots with qgroup enabled, we will trigger backref
    walk.
    
    However such backref walk at that timing is pretty dangerous, as if one
    of the parent nodes get WRITE locked by other thread, we could cause a
    dead lock.
    
    For example:
    
               FS 260     FS 261 (Dropped)
                node A        node B
               /      \      /      \
           node C      node D      node E
          /   \         /  \        /     \
      leaf F|leaf G|leaf H|leaf I|leaf J|leaf K
    
    The lock sequence would be:
    
          Thread A (cleaner)             |       Thread B (other writer)
    -----------------------------------------------------------------------
    write_lock(B)                        |
    write_lock(D)                        |
    ^^^ called by walk_down_tree()       |
                                         |       write_lock(A)
                                         |       write_lock(D) << Stall
    read_lock(H) << for backref walk     |
    read_lock(D) << lock owner is        |
                    the same thread A    |
                    so read lock is OK   |
    read_lock(A) << Stall                |
    
    So thread A hold write lock D, and needs read lock A to unlock.
    While thread B holds write lock A, while needs lock D to unlock.
    
    This will cause a deadlock.
    
    This is not only limited to snapshot dropping case.  As the backref
    walk, even only happens on commit trees, is breaking the normal top-down
    locking order, makes it deadlock prone.
    
    Fixes: fb235dc06fac ("btrfs: qgroup: Move half of the qgroup accounting time out of commit trans")
    CC: stable@vger.kernel.org # 4.14+
    Reported-and-tested-by: David Sterba <dsterba@suse.com>
    Reported-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    [ rebase to latest branch and fix lock assert bug in btrfs/007 ]
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    [ copy logs and deadlock analysis from Qu's patch ]
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 970e74d961db61eed18e33d8ecd644ee8ef7da04
Author: David Sterba <dsterba@suse.com>
Date:   Wed Apr 4 02:11:50 2018 +0200

    btrfs: simplify waiting loop in btrfs_tree_lock
    
    Currently, the number of readers and writers is checked and in case
    there are any, wait and redo the locks. There's some duplication
    before the branches go back to again label, eg. calling wait_event on
    blocking_readers twice.
    
    The sequence is transformed
    
    loop:
    * wait for readers
    * wait for writers
    * write_lock
    * check readers, unlock and wait for readers, loop
    * check writers, unlock and wait for writers, loop
    
    The new sequence is not exactly the same due to the simplification, for
    readers it's slightly faster. For the writers, original code does
    
    * wait for writers
    * (loop) wait for readers
    *        wait for writers -- again
    
    while the new goes directly to the reader check. This should behave the
    same on a contended lock with multiple writers and readers, but can
    reduce number of times we're waiting on something.
    
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit bd4691a0e866e774c9690f09fec573e845495f91
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Thu Jan 3 10:50:01 2019 +0200

    btrfs: Use ihold instead of igrab in cow_file_range_async
    
    ihold is supposed to be used when the caller already has a reference to
    the inode. In the case of cow_file_range_async this invariants holds,
    since the 3 call chains leading to this function all take a reference:
    
    btrfs_writepage  <--- does igrab
     extent_write_full_page
      __extent_writepage
       writepage_delalloc
         btrfs_run_delalloc_range
          cow_file_range_async
    
    extent_write_cache_pages <--- does igrab
     __extent_writepage (same callchain as above)
    
    and
    
    submit_compressed_extents <-- already called from async CoW submit path,
                                  which would have done ihold.
     extent_write_locked_range
      __extent_writepage
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ add comment ]
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 797a22bd5298c2674d927893f46cadf619dad11d
Author: Eric Dumazet <edumazet@google.com>
Date:   Sat Feb 23 13:24:59 2019 -0800

    net/x25: fix a race in x25_bind()
    
    syzbot was able to trigger another soft lockup [1]
    
    I first thought it was the O(N^2) issue I mentioned in my
    prior fix (f657d22ee1f "net/x25: do not hold the cpu
    too long in x25_new_lci()"), but I eventually found
    that x25_bind() was not checking SOCK_ZAPPED state under
    socket lock protection.
    
    This means that multiple threads can end up calling
    x25_insert_socket() for the same socket, and corrupt x25_list
    
    [1]
    watchdog: BUG: soft lockup - CPU#0 stuck for 123s! [syz-executor.2:10492]
    Modules linked in:
    irq event stamp: 27515
    hardirqs last  enabled at (27514): [<ffffffff81006673>] trace_hardirqs_on_thunk+0x1a/0x1c
    hardirqs last disabled at (27515): [<ffffffff8100668f>] trace_hardirqs_off_thunk+0x1a/0x1c
    softirqs last  enabled at (32): [<ffffffff8632ee73>] x25_get_neigh+0xa3/0xd0 net/x25/x25_link.c:336
    softirqs last disabled at (34): [<ffffffff86324bc3>] x25_find_socket+0x23/0x140 net/x25/af_x25.c:341
    CPU: 0 PID: 10492 Comm: syz-executor.2 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__sanitizer_cov_trace_pc+0x4/0x50 kernel/kcov.c:97
    Code: f4 ff ff ff e8 11 9f ea ff 48 c7 05 12 fb e5 08 00 00 00 00 e9 c8 e9 ff ff 90 90 90 90 90 90 90 90 90 90 90 90 90 55 48 89 e5 <48> 8b 75 08 65 48 8b 04 25 40 ee 01 00 65 8b 15 38 0c 92 7e 81 e2
    RSP: 0018:ffff88806e94fc48 EFLAGS: 00000286 ORIG_RAX: ffffffffffffff13
    RAX: 1ffff1100d84dac5 RBX: 0000000000000001 RCX: ffffc90006197000
    RDX: 0000000000040000 RSI: ffffffff86324bf3 RDI: ffff88806c26d628
    RBP: ffff88806e94fc48 R08: ffff88806c1c6500 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: ffff88806c26d628
    R13: ffff888090455200 R14: dffffc0000000000 R15: 0000000000000000
    FS:  00007f3a107e4700(0000) GS:ffff8880ae800000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007f3a107e3db8 CR3: 00000000a5544000 CR4: 00000000001406f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     __x25_find_socket net/x25/af_x25.c:327 [inline]
     x25_find_socket+0x7d/0x140 net/x25/af_x25.c:342
     x25_new_lci net/x25/af_x25.c:355 [inline]
     x25_connect+0x380/0xde0 net/x25/af_x25.c:784
     __sys_connect+0x266/0x330 net/socket.c:1662
     __do_sys_connect net/socket.c:1673 [inline]
     __se_sys_connect net/socket.c:1670 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1670
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007f3a107e3c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e29
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000005
    RBP: 000000000073c040 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007f3a107e46d4
    R13: 00000000004be362 R14: 00000000004ceb98 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 10493 Comm: syz-executor.3 Not tainted 5.0.0-rc7+ #88
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x143/0x290 kernel/locking/qrwlock.c:86
    Code: 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 41 0f b6 55 00 <41> 38 d7 7c eb 84 d2 74 e7 48 89 df e8 cc aa 4e 00 eb dd be 04 00
    RSP: 0018:ffff888085c47bd8 EFLAGS: 00000206
    RAX: 0000000000000300 RBX: ffffffff89412b00 RCX: 1ffffffff1282560
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89412b00
    RBP: ffff888085c47c70 R08: 1ffffffff1282560 R09: fffffbfff1282561
    R10: fffffbfff1282560 R11: ffffffff89412b03 R12: 00000000000000ff
    R13: fffffbfff1282560 R14: 1ffff11010b88f7d R15: 0000000000000003
    FS:  00007fdd04086700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007fdd04064db8 CR3: 0000000090be0000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:703
     __sys_bind+0x23f/0x290 net/socket.c:1481
     __do_sys_bind net/socket.c:1492 [inline]
     __se_sys_bind net/socket.c:1490 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1490
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e29
    
    Fixes: 90c27297a9bf ("X.25 remove bkl in bind")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: andrew hendry <andrew.hendry@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e4c275f77624961b56cce397814d9d770a45ac59
Author: Vadim Pasternak <vadimp@mellanox.com>
Date:   Sun Feb 17 18:15:30 2019 +0000

    platform/mellanox: mlxreg-hotplug: Fix KASAN warning
    
    Fix the following KASAN warning produced when booting a 64-bit kernel:
    [   13.334750] BUG: KASAN: stack-out-of-bounds in find_first_bit+0x19/0x70
    [   13.342166] Read of size 8 at addr ffff880235067178 by task kworker/2:1/42
    [   13.342176] CPU: 2 PID: 42 Comm: kworker/2:1 Not tainted 4.20.0-rc1+ #106
    [   13.342179] Hardware name: Mellanox Technologies Ltd. MSN2740/Mellanox x86 SFF board, BIOS 5.6.5 06/07/2016
    [   13.342190] Workqueue: events deferred_probe_work_func
    [   13.342194] Call Trace:
    [   13.342206]  dump_stack+0xc7/0x15b
    [   13.342214]  ? show_regs_print_info+0x5/0x5
    [   13.342220]  ? kmsg_dump_rewind_nolock+0x59/0x59
    [   13.342234]  ? _raw_write_lock_irqsave+0x100/0x100
    [   13.351593]  print_address_description+0x73/0x260
    [   13.351603]  kasan_report+0x260/0x380
    [   13.351611]  ? find_first_bit+0x19/0x70
    [   13.351619]  find_first_bit+0x19/0x70
    [   13.351630]  mlxreg_hotplug_work_handler+0x73c/0x920 [mlxreg_hotplug]
    [   13.351639]  ? __lock_text_start+0x8/0x8
    [   13.351646]  ? _raw_write_lock_irqsave+0x80/0x100
    [   13.351656]  ? mlxreg_hotplug_remove+0x1e0/0x1e0 [mlxreg_hotplug]
    [   13.351663]  ? regmap_volatile+0x40/0xb0
    [   13.351668]  ? regcache_write+0x4c/0x90
    [   13.351676]  ? mlxplat_mlxcpld_reg_write+0x24/0x30 [mlx_platform]
    [   13.351681]  ? _regmap_write+0xea/0x220
    [   13.351688]  ? __mutex_lock_slowpath+0x10/0x10
    [   13.351696]  ? devm_add_action+0x70/0x70
    [   13.351701]  ? mutex_unlock+0x1d/0x40
    [   13.351710]  mlxreg_hotplug_probe+0x82e/0x989 [mlxreg_hotplug]
    [   13.351723]  ? mlxreg_hotplug_work_handler+0x920/0x920 [mlxreg_hotplug]
    [   13.351731]  ? sysfs_do_create_link_sd.isra.2+0xf4/0x190
    [   13.351737]  ? sysfs_rename_link_ns+0xf0/0xf0
    [   13.351743]  ? devres_close_group+0x2b0/0x2b0
    [   13.351749]  ? pinctrl_put+0x20/0x20
    [   13.351755]  ? acpi_dev_pm_attach+0x2c/0xd0
    [   13.351763]  platform_drv_probe+0x70/0xd0
    [   13.351771]  really_probe+0x480/0x6e0
    [   13.351778]  ? device_attach+0x10/0x10
    [   13.351784]  ? __lock_text_start+0x8/0x8
    [   13.351790]  ? _raw_write_lock_irqsave+0x80/0x100
    [   13.351797]  ? _raw_write_lock_irqsave+0x80/0x100
    [   13.351806]  ? __driver_attach+0x190/0x190
    [   13.351812]  driver_probe_device+0x17d/0x1a0
    [   13.351819]  ? __driver_attach+0x190/0x190
    [   13.351825]  bus_for_each_drv+0xd6/0x130
    [   13.351831]  ? bus_rescan_devices+0x20/0x20
    [   13.351837]  ? __mutex_lock_slowpath+0x10/0x10
    [   13.351845]  __device_attach+0x18c/0x230
    [   13.351852]  ? device_bind_driver+0x70/0x70
    [   13.351859]  ? __mutex_lock_slowpath+0x10/0x10
    [   13.351866]  bus_probe_device+0xea/0x110
    [   13.351874]  deferred_probe_work_func+0x1c9/0x290
    [   13.351882]  ? driver_deferred_probe_add+0x1d0/0x1d0
    [   13.351889]  ? preempt_notifier_dec+0x20/0x20
    [   13.351897]  ? read_word_at_a_time+0xe/0x20
    [   13.351904]  ? strscpy+0x151/0x290
    [   13.351912]  ? set_work_pool_and_clear_pending+0x9c/0xf0
    [   13.351918]  ? __switch_to_asm+0x34/0x70
    [   13.351924]  ? __switch_to_asm+0x40/0x70
    [   13.351929]  ? __switch_to_asm+0x34/0x70
    [   13.351935]  ? __switch_to_asm+0x40/0x70
    [   13.351942]  process_one_work+0x5cc/0xa00
    [   13.351952]  ? pwq_dec_nr_in_flight+0x1e0/0x1e0
    [   13.351960]  ? pci_mmcfg_check_reserved+0x80/0xb8
    [   13.351967]  ? run_rebalance_domains+0x250/0x250
    [   13.351980]  ? stack_access_ok+0x35/0x80
    [   13.351986]  ? deref_stack_reg+0xa1/0xe0
    [   13.351994]  ? schedule+0xcd/0x250
    [   13.352000]  ? worker_enter_idle+0x2d6/0x330
    [   13.352006]  ? __schedule+0xeb0/0xeb0
    [   13.352014]  ? fork_usermode_blob+0x130/0x130
    [   13.352019]  ? mutex_lock+0xa7/0x100
    [   13.352026]  ? _raw_spin_lock_irq+0x98/0xf0
    [   13.352032]  ? _raw_read_unlock_irqrestore+0x30/0x30
    [   13.352037] i2c i2c-2: Added multiplexed i2c bus 11
    [   13.352043]  worker_thread+0x181/0xa80
    [   13.352052]  ? __switch_to_asm+0x34/0x70
    [   13.352058]  ? __switch_to_asm+0x40/0x70
    [   13.352064]  ? process_one_work+0xa00/0xa00
    [   13.352070]  ? __switch_to_asm+0x34/0x70
    [   13.352076]  ? __switch_to_asm+0x40/0x70
    [   13.352081]  ? __switch_to_asm+0x34/0x70
    [   13.352086]  ? __switch_to_asm+0x40/0x70
    [   13.352092]  ? __switch_to_asm+0x34/0x70
    [   13.352097]  ? __switch_to_asm+0x40/0x70
    [   13.352105]  ? __schedule+0x3d6/0xeb0
    [   13.352112]  ? migrate_swap_stop+0x470/0x470
    [   13.352119]  ? save_stack+0x89/0xb0
    [   13.352127]  ? kmem_cache_alloc_trace+0xe5/0x570
    [   13.352132]  ? kthread+0x59/0x1d0
    [   13.352138]  ? ret_from_fork+0x35/0x40
    [   13.352154]  ? __schedule+0xeb0/0xeb0
    [   13.352161]  ? remove_wait_queue+0x150/0x150
    [   13.352169]  ? _raw_write_lock_irqsave+0x80/0x100
    [   13.352175]  ? __lock_text_start+0x8/0x8
    [   13.352183]  ? process_one_work+0xa00/0xa00
    [   13.352188]  kthread+0x1a4/0x1d0
    [   13.352195]  ? kthread_create_worker_on_cpu+0xc0/0xc0
    [   13.352202]  ret_from_fork+0x35/0x40
    
    [   13.353879] The buggy address belongs to the page:
    [   13.353885] page:ffffea0008d419c0 count:0 mapcount:0 mapping:0000000000000000 index:0x0
    [   13.353890] flags: 0x2ffff8000000000()
    [   13.353897] raw: 02ffff8000000000 ffffea0008d419c8 ffffea0008d419c8 0000000000000000
    [   13.353903] raw: 0000000000000000 0000000000000000 00000000ffffffff 0000000000000000
    [   13.353905] page dumped because: kasan: bad access detected
    
    [   13.353908] Memory state around the buggy address:
    [   13.353912]  ffff880235067000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [   13.353917]  ffff880235067080: 00 00 00 00 00 00 00 00 00 00 00 f1 f1 f1 f1 04
    [   13.353921] >ffff880235067100: f2 f2 f2 f2 f2 f2 f2 04 f2 f2 f2 f2 f2 f2 f2 04
    [   13.353923]                                                                 ^
    [   13.353927]  ffff880235067180: f2 f2 f2 f2 f2 f2 f2 04 f2 f2 f2 00 00 00 00 00
    [   13.353931]  ffff880235067200: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [   13.353933] ==================================================================
    
    The warning is caused by the below loop:
            for_each_set_bit(bit, (unsigned long *)&asserted, 8) {
    while "asserted" is declared as 'unsigned'.
    
    The casting of 32-bit unsigned integer pointer to a 64-bit unsigned long
    pointer. There are two problems here.
    It causes the access of four extra byte, which can corrupt memory
    The 32-bit pointer address may not be 64-bit aligned.
    
    The fix changes variable "asserted" to "unsigned long".
    
    Fixes: 1f976f6978bf ("platform/x86: Move Mellanox platform hotplug driver to platform/mellanox")
    Signed-off-by: Vadim Pasternak <vadimp@mellanox.com>
    Signed-off-by: Darren Hart (VMware) <dvhart@infradead.org>

commit c1339bd49e72725e0a20f0b77980136e89c76e49
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Feb 8 12:41:05 2019 -0800

    net/x25: do not hold the cpu too long in x25_new_lci()
    
    commit cf657d22ee1f0e887326a92169f2e28dc932fd10 upstream.
    
    Due to quadratic behavior of x25_new_lci(), syzbot was able
    to trigger an rcu stall.
    
    Fix this by not blocking BH for the whole duration of
    the function, and inserting a reschedule point when possible.
    
    If we care enough, using a bitmap could get rid of the quadratic
    behavior.
    
    syzbot report :
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    0-...!: (10500 ticks this GP) idle=4fa/1/0x4000000000000002 softirq=283376/283376 fqs=0
    rcu:     (t=10501 jiffies g=383105 q=136)
    rcu: rcu_preempt kthread starved for 10502 jiffies! g383105 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu: RCU grace-period kthread stack dump:
    rcu_preempt     I28928    10      2 0x80000000
    Call Trace:
     context_switch kernel/sched/core.c:2844 [inline]
     __schedule+0x817/0x1cc0 kernel/sched/core.c:3485
     schedule+0x92/0x180 kernel/sched/core.c:3529
     schedule_timeout+0x4db/0xfd0 kernel/time/timer.c:1803
     rcu_gp_fqs_loop kernel/rcu/tree.c:1948 [inline]
     rcu_gp_kthread+0x956/0x17a0 kernel/rcu/tree.c:2105
     kthread+0x357/0x430 kernel/kthread.c:246
     ret_from_fork+0x3a/0x50 arch/x86/entry/entry_64.S:352
    NMI backtrace for cpu 0
    CPU: 0 PID: 8759 Comm: syz-executor2 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x172/0x1f0 lib/dump_stack.c:113
     nmi_cpu_backtrace.cold+0x63/0xa4 lib/nmi_backtrace.c:101
     nmi_trigger_cpumask_backtrace+0x1be/0x236 lib/nmi_backtrace.c:62
     arch_trigger_cpumask_backtrace+0x14/0x20 arch/x86/kernel/apic/hw_nmi.c:38
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x183/0x1cf kernel/rcu/tree.c:1211
     print_cpu_stall kernel/rcu/tree.c:1348 [inline]
     check_cpu_stall kernel/rcu/tree.c:1422 [inline]
     rcu_pending kernel/rcu/tree.c:3018 [inline]
     rcu_check_callbacks.cold+0x500/0xa4a kernel/rcu/tree.c:2521
     update_process_times+0x32/0x80 kernel/time/timer.c:1635
     tick_sched_handle+0xa2/0x190 kernel/time/tick-sched.c:161
     tick_sched_timer+0x47/0x130 kernel/time/tick-sched.c:1271
     __run_hrtimer kernel/time/hrtimer.c:1389 [inline]
     __hrtimer_run_queues+0x33e/0xde0 kernel/time/hrtimer.c:1451
     hrtimer_interrupt+0x314/0x770 kernel/time/hrtimer.c:1509
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1035 [inline]
     smp_apic_timer_interrupt+0x120/0x570 arch/x86/kernel/apic/apic.c:1060
     apic_timer_interrupt+0xf/0x20 arch/x86/entry/entry_64.S:807
     </IRQ>
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x13e/0x290 kernel/locking/qrwlock.c:86
    Code: 00 00 fc ff df 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 <41> 0f b6 55 00 41 38 d7 7c eb 84 d2 74 e7 48 89 df e8 6c 0f 4f 00
    RSP: 0018:ffff88805f117bd8 EFLAGS: 00000206 ORIG_RAX: ffffffffffffff13
    RAX: 0000000000000300 RBX: ffffffff89413ba0 RCX: 1ffffffff1282774
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89413ba0
    RBP: ffff88805f117c70 R08: 1ffffffff1282774 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: 00000000000000ff
    R13: fffffbfff1282774 R14: 1ffff1100be22f7d R15: 0000000000000003
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:705
     __sys_bind+0x23f/0x290 net/socket.c:1505
     __do_sys_bind net/socket.c:1516 [inline]
     __se_sys_bind net/socket.c:1514 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1514
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fafccd0dc78 EFLAGS: 00000246 ORIG_RAX: 0000000000000031
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000240 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fafccd0e6d4
    R13: 00000000004bdf8b R14: 00000000004ce4b8 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 8752 Comm: syz-executor4 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__x25_find_socket+0x78/0x120 net/x25/af_x25.c:328
    Code: 89 f8 48 c1 e8 03 80 3c 18 00 0f 85 a6 00 00 00 4d 8b 64 24 68 4d 85 e4 74 7f e8 03 97 3d fb 49 83 ec 68 74 74 e8 f8 96 3d fb <49> 8d bc 24 88 04 00 00 48 89 f8 48 c1 e8 03 0f b6 04 18 84 c0 74
    RSP: 0018:ffff8880639efc58 EFLAGS: 00000246
    RAX: 0000000000040000 RBX: dffffc0000000000 RCX: ffffc9000e677000
    RDX: 0000000000040000 RSI: ffffffff863244b8 RDI: ffff88806a764628
    RBP: ffff8880639efc80 R08: ffff8880a80d05c0 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: ffff88806a7645c0
    R13: 0000000000000001 R14: ffff88809f29ac00 R15: 0000000000000000
    FS:  00007fe8d0c58700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000001b32823000 CR3: 00000000672eb000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     x25_new_lci net/x25/af_x25.c:357 [inline]
     x25_connect+0x374/0xdf0 net/x25/af_x25.c:786
     __sys_connect+0x266/0x330 net/socket.c:1686
     __do_sys_connect net/socket.c:1697 [inline]
     __se_sys_connect net/socket.c:1694 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1694
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fe8d0c57c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fe8d0c586d4
    R13: 00000000004be378 R14: 00000000004ceb00 R15: 00000000ffffffff
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Andrew Hendry <andrew.hendry@gmail.com>
    Cc: linux-x25@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 74e1493e00f229a012fc87b536fa0ea9ce9b426a
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Feb 8 12:41:05 2019 -0800

    net/x25: do not hold the cpu too long in x25_new_lci()
    
    commit cf657d22ee1f0e887326a92169f2e28dc932fd10 upstream.
    
    Due to quadratic behavior of x25_new_lci(), syzbot was able
    to trigger an rcu stall.
    
    Fix this by not blocking BH for the whole duration of
    the function, and inserting a reschedule point when possible.
    
    If we care enough, using a bitmap could get rid of the quadratic
    behavior.
    
    syzbot report :
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    0-...!: (10500 ticks this GP) idle=4fa/1/0x4000000000000002 softirq=283376/283376 fqs=0
    rcu:     (t=10501 jiffies g=383105 q=136)
    rcu: rcu_preempt kthread starved for 10502 jiffies! g383105 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu: RCU grace-period kthread stack dump:
    rcu_preempt     I28928    10      2 0x80000000
    Call Trace:
     context_switch kernel/sched/core.c:2844 [inline]
     __schedule+0x817/0x1cc0 kernel/sched/core.c:3485
     schedule+0x92/0x180 kernel/sched/core.c:3529
     schedule_timeout+0x4db/0xfd0 kernel/time/timer.c:1803
     rcu_gp_fqs_loop kernel/rcu/tree.c:1948 [inline]
     rcu_gp_kthread+0x956/0x17a0 kernel/rcu/tree.c:2105
     kthread+0x357/0x430 kernel/kthread.c:246
     ret_from_fork+0x3a/0x50 arch/x86/entry/entry_64.S:352
    NMI backtrace for cpu 0
    CPU: 0 PID: 8759 Comm: syz-executor2 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x172/0x1f0 lib/dump_stack.c:113
     nmi_cpu_backtrace.cold+0x63/0xa4 lib/nmi_backtrace.c:101
     nmi_trigger_cpumask_backtrace+0x1be/0x236 lib/nmi_backtrace.c:62
     arch_trigger_cpumask_backtrace+0x14/0x20 arch/x86/kernel/apic/hw_nmi.c:38
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x183/0x1cf kernel/rcu/tree.c:1211
     print_cpu_stall kernel/rcu/tree.c:1348 [inline]
     check_cpu_stall kernel/rcu/tree.c:1422 [inline]
     rcu_pending kernel/rcu/tree.c:3018 [inline]
     rcu_check_callbacks.cold+0x500/0xa4a kernel/rcu/tree.c:2521
     update_process_times+0x32/0x80 kernel/time/timer.c:1635
     tick_sched_handle+0xa2/0x190 kernel/time/tick-sched.c:161
     tick_sched_timer+0x47/0x130 kernel/time/tick-sched.c:1271
     __run_hrtimer kernel/time/hrtimer.c:1389 [inline]
     __hrtimer_run_queues+0x33e/0xde0 kernel/time/hrtimer.c:1451
     hrtimer_interrupt+0x314/0x770 kernel/time/hrtimer.c:1509
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1035 [inline]
     smp_apic_timer_interrupt+0x120/0x570 arch/x86/kernel/apic/apic.c:1060
     apic_timer_interrupt+0xf/0x20 arch/x86/entry/entry_64.S:807
     </IRQ>
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x13e/0x290 kernel/locking/qrwlock.c:86
    Code: 00 00 fc ff df 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 <41> 0f b6 55 00 41 38 d7 7c eb 84 d2 74 e7 48 89 df e8 6c 0f 4f 00
    RSP: 0018:ffff88805f117bd8 EFLAGS: 00000206 ORIG_RAX: ffffffffffffff13
    RAX: 0000000000000300 RBX: ffffffff89413ba0 RCX: 1ffffffff1282774
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89413ba0
    RBP: ffff88805f117c70 R08: 1ffffffff1282774 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: 00000000000000ff
    R13: fffffbfff1282774 R14: 1ffff1100be22f7d R15: 0000000000000003
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:705
     __sys_bind+0x23f/0x290 net/socket.c:1505
     __do_sys_bind net/socket.c:1516 [inline]
     __se_sys_bind net/socket.c:1514 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1514
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fafccd0dc78 EFLAGS: 00000246 ORIG_RAX: 0000000000000031
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000240 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fafccd0e6d4
    R13: 00000000004bdf8b R14: 00000000004ce4b8 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 8752 Comm: syz-executor4 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__x25_find_socket+0x78/0x120 net/x25/af_x25.c:328
    Code: 89 f8 48 c1 e8 03 80 3c 18 00 0f 85 a6 00 00 00 4d 8b 64 24 68 4d 85 e4 74 7f e8 03 97 3d fb 49 83 ec 68 74 74 e8 f8 96 3d fb <49> 8d bc 24 88 04 00 00 48 89 f8 48 c1 e8 03 0f b6 04 18 84 c0 74
    RSP: 0018:ffff8880639efc58 EFLAGS: 00000246
    RAX: 0000000000040000 RBX: dffffc0000000000 RCX: ffffc9000e677000
    RDX: 0000000000040000 RSI: ffffffff863244b8 RDI: ffff88806a764628
    RBP: ffff8880639efc80 R08: ffff8880a80d05c0 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: ffff88806a7645c0
    R13: 0000000000000001 R14: ffff88809f29ac00 R15: 0000000000000000
    FS:  00007fe8d0c58700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000001b32823000 CR3: 00000000672eb000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     x25_new_lci net/x25/af_x25.c:357 [inline]
     x25_connect+0x374/0xdf0 net/x25/af_x25.c:786
     __sys_connect+0x266/0x330 net/socket.c:1686
     __do_sys_connect net/socket.c:1697 [inline]
     __se_sys_connect net/socket.c:1694 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1694
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fe8d0c57c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fe8d0c586d4
    R13: 00000000004be378 R14: 00000000004ceb00 R15: 00000000ffffffff
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Andrew Hendry <andrew.hendry@gmail.com>
    Cc: linux-x25@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c01e01adf9df0e9b32f0f5f7058b66b68155002c
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Feb 8 12:41:05 2019 -0800

    net/x25: do not hold the cpu too long in x25_new_lci()
    
    commit cf657d22ee1f0e887326a92169f2e28dc932fd10 upstream.
    
    Due to quadratic behavior of x25_new_lci(), syzbot was able
    to trigger an rcu stall.
    
    Fix this by not blocking BH for the whole duration of
    the function, and inserting a reschedule point when possible.
    
    If we care enough, using a bitmap could get rid of the quadratic
    behavior.
    
    syzbot report :
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    0-...!: (10500 ticks this GP) idle=4fa/1/0x4000000000000002 softirq=283376/283376 fqs=0
    rcu:     (t=10501 jiffies g=383105 q=136)
    rcu: rcu_preempt kthread starved for 10502 jiffies! g383105 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu: RCU grace-period kthread stack dump:
    rcu_preempt     I28928    10      2 0x80000000
    Call Trace:
     context_switch kernel/sched/core.c:2844 [inline]
     __schedule+0x817/0x1cc0 kernel/sched/core.c:3485
     schedule+0x92/0x180 kernel/sched/core.c:3529
     schedule_timeout+0x4db/0xfd0 kernel/time/timer.c:1803
     rcu_gp_fqs_loop kernel/rcu/tree.c:1948 [inline]
     rcu_gp_kthread+0x956/0x17a0 kernel/rcu/tree.c:2105
     kthread+0x357/0x430 kernel/kthread.c:246
     ret_from_fork+0x3a/0x50 arch/x86/entry/entry_64.S:352
    NMI backtrace for cpu 0
    CPU: 0 PID: 8759 Comm: syz-executor2 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x172/0x1f0 lib/dump_stack.c:113
     nmi_cpu_backtrace.cold+0x63/0xa4 lib/nmi_backtrace.c:101
     nmi_trigger_cpumask_backtrace+0x1be/0x236 lib/nmi_backtrace.c:62
     arch_trigger_cpumask_backtrace+0x14/0x20 arch/x86/kernel/apic/hw_nmi.c:38
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x183/0x1cf kernel/rcu/tree.c:1211
     print_cpu_stall kernel/rcu/tree.c:1348 [inline]
     check_cpu_stall kernel/rcu/tree.c:1422 [inline]
     rcu_pending kernel/rcu/tree.c:3018 [inline]
     rcu_check_callbacks.cold+0x500/0xa4a kernel/rcu/tree.c:2521
     update_process_times+0x32/0x80 kernel/time/timer.c:1635
     tick_sched_handle+0xa2/0x190 kernel/time/tick-sched.c:161
     tick_sched_timer+0x47/0x130 kernel/time/tick-sched.c:1271
     __run_hrtimer kernel/time/hrtimer.c:1389 [inline]
     __hrtimer_run_queues+0x33e/0xde0 kernel/time/hrtimer.c:1451
     hrtimer_interrupt+0x314/0x770 kernel/time/hrtimer.c:1509
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1035 [inline]
     smp_apic_timer_interrupt+0x120/0x570 arch/x86/kernel/apic/apic.c:1060
     apic_timer_interrupt+0xf/0x20 arch/x86/entry/entry_64.S:807
     </IRQ>
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x13e/0x290 kernel/locking/qrwlock.c:86
    Code: 00 00 fc ff df 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 <41> 0f b6 55 00 41 38 d7 7c eb 84 d2 74 e7 48 89 df e8 6c 0f 4f 00
    RSP: 0018:ffff88805f117bd8 EFLAGS: 00000206 ORIG_RAX: ffffffffffffff13
    RAX: 0000000000000300 RBX: ffffffff89413ba0 RCX: 1ffffffff1282774
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89413ba0
    RBP: ffff88805f117c70 R08: 1ffffffff1282774 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: 00000000000000ff
    R13: fffffbfff1282774 R14: 1ffff1100be22f7d R15: 0000000000000003
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:705
     __sys_bind+0x23f/0x290 net/socket.c:1505
     __do_sys_bind net/socket.c:1516 [inline]
     __se_sys_bind net/socket.c:1514 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1514
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fafccd0dc78 EFLAGS: 00000246 ORIG_RAX: 0000000000000031
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000240 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fafccd0e6d4
    R13: 00000000004bdf8b R14: 00000000004ce4b8 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 8752 Comm: syz-executor4 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__x25_find_socket+0x78/0x120 net/x25/af_x25.c:328
    Code: 89 f8 48 c1 e8 03 80 3c 18 00 0f 85 a6 00 00 00 4d 8b 64 24 68 4d 85 e4 74 7f e8 03 97 3d fb 49 83 ec 68 74 74 e8 f8 96 3d fb <49> 8d bc 24 88 04 00 00 48 89 f8 48 c1 e8 03 0f b6 04 18 84 c0 74
    RSP: 0018:ffff8880639efc58 EFLAGS: 00000246
    RAX: 0000000000040000 RBX: dffffc0000000000 RCX: ffffc9000e677000
    RDX: 0000000000040000 RSI: ffffffff863244b8 RDI: ffff88806a764628
    RBP: ffff8880639efc80 R08: ffff8880a80d05c0 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: ffff88806a7645c0
    R13: 0000000000000001 R14: ffff88809f29ac00 R15: 0000000000000000
    FS:  00007fe8d0c58700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000001b32823000 CR3: 00000000672eb000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     x25_new_lci net/x25/af_x25.c:357 [inline]
     x25_connect+0x374/0xdf0 net/x25/af_x25.c:786
     __sys_connect+0x266/0x330 net/socket.c:1686
     __do_sys_connect net/socket.c:1697 [inline]
     __se_sys_connect net/socket.c:1694 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1694
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fe8d0c57c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fe8d0c586d4
    R13: 00000000004be378 R14: 00000000004ceb00 R15: 00000000ffffffff
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Andrew Hendry <andrew.hendry@gmail.com>
    Cc: linux-x25@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4833df346832f84a583042ab26d6bb1df7d21606
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Feb 8 12:41:05 2019 -0800

    net/x25: do not hold the cpu too long in x25_new_lci()
    
    commit cf657d22ee1f0e887326a92169f2e28dc932fd10 upstream.
    
    Due to quadratic behavior of x25_new_lci(), syzbot was able
    to trigger an rcu stall.
    
    Fix this by not blocking BH for the whole duration of
    the function, and inserting a reschedule point when possible.
    
    If we care enough, using a bitmap could get rid of the quadratic
    behavior.
    
    syzbot report :
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    0-...!: (10500 ticks this GP) idle=4fa/1/0x4000000000000002 softirq=283376/283376 fqs=0
    rcu:     (t=10501 jiffies g=383105 q=136)
    rcu: rcu_preempt kthread starved for 10502 jiffies! g383105 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu: RCU grace-period kthread stack dump:
    rcu_preempt     I28928    10      2 0x80000000
    Call Trace:
     context_switch kernel/sched/core.c:2844 [inline]
     __schedule+0x817/0x1cc0 kernel/sched/core.c:3485
     schedule+0x92/0x180 kernel/sched/core.c:3529
     schedule_timeout+0x4db/0xfd0 kernel/time/timer.c:1803
     rcu_gp_fqs_loop kernel/rcu/tree.c:1948 [inline]
     rcu_gp_kthread+0x956/0x17a0 kernel/rcu/tree.c:2105
     kthread+0x357/0x430 kernel/kthread.c:246
     ret_from_fork+0x3a/0x50 arch/x86/entry/entry_64.S:352
    NMI backtrace for cpu 0
    CPU: 0 PID: 8759 Comm: syz-executor2 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x172/0x1f0 lib/dump_stack.c:113
     nmi_cpu_backtrace.cold+0x63/0xa4 lib/nmi_backtrace.c:101
     nmi_trigger_cpumask_backtrace+0x1be/0x236 lib/nmi_backtrace.c:62
     arch_trigger_cpumask_backtrace+0x14/0x20 arch/x86/kernel/apic/hw_nmi.c:38
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x183/0x1cf kernel/rcu/tree.c:1211
     print_cpu_stall kernel/rcu/tree.c:1348 [inline]
     check_cpu_stall kernel/rcu/tree.c:1422 [inline]
     rcu_pending kernel/rcu/tree.c:3018 [inline]
     rcu_check_callbacks.cold+0x500/0xa4a kernel/rcu/tree.c:2521
     update_process_times+0x32/0x80 kernel/time/timer.c:1635
     tick_sched_handle+0xa2/0x190 kernel/time/tick-sched.c:161
     tick_sched_timer+0x47/0x130 kernel/time/tick-sched.c:1271
     __run_hrtimer kernel/time/hrtimer.c:1389 [inline]
     __hrtimer_run_queues+0x33e/0xde0 kernel/time/hrtimer.c:1451
     hrtimer_interrupt+0x314/0x770 kernel/time/hrtimer.c:1509
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1035 [inline]
     smp_apic_timer_interrupt+0x120/0x570 arch/x86/kernel/apic/apic.c:1060
     apic_timer_interrupt+0xf/0x20 arch/x86/entry/entry_64.S:807
     </IRQ>
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x13e/0x290 kernel/locking/qrwlock.c:86
    Code: 00 00 fc ff df 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 <41> 0f b6 55 00 41 38 d7 7c eb 84 d2 74 e7 48 89 df e8 6c 0f 4f 00
    RSP: 0018:ffff88805f117bd8 EFLAGS: 00000206 ORIG_RAX: ffffffffffffff13
    RAX: 0000000000000300 RBX: ffffffff89413ba0 RCX: 1ffffffff1282774
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89413ba0
    RBP: ffff88805f117c70 R08: 1ffffffff1282774 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: 00000000000000ff
    R13: fffffbfff1282774 R14: 1ffff1100be22f7d R15: 0000000000000003
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:705
     __sys_bind+0x23f/0x290 net/socket.c:1505
     __do_sys_bind net/socket.c:1516 [inline]
     __se_sys_bind net/socket.c:1514 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1514
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fafccd0dc78 EFLAGS: 00000246 ORIG_RAX: 0000000000000031
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000240 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fafccd0e6d4
    R13: 00000000004bdf8b R14: 00000000004ce4b8 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 8752 Comm: syz-executor4 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__x25_find_socket+0x78/0x120 net/x25/af_x25.c:328
    Code: 89 f8 48 c1 e8 03 80 3c 18 00 0f 85 a6 00 00 00 4d 8b 64 24 68 4d 85 e4 74 7f e8 03 97 3d fb 49 83 ec 68 74 74 e8 f8 96 3d fb <49> 8d bc 24 88 04 00 00 48 89 f8 48 c1 e8 03 0f b6 04 18 84 c0 74
    RSP: 0018:ffff8880639efc58 EFLAGS: 00000246
    RAX: 0000000000040000 RBX: dffffc0000000000 RCX: ffffc9000e677000
    RDX: 0000000000040000 RSI: ffffffff863244b8 RDI: ffff88806a764628
    RBP: ffff8880639efc80 R08: ffff8880a80d05c0 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: ffff88806a7645c0
    R13: 0000000000000001 R14: ffff88809f29ac00 R15: 0000000000000000
    FS:  00007fe8d0c58700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000001b32823000 CR3: 00000000672eb000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     x25_new_lci net/x25/af_x25.c:357 [inline]
     x25_connect+0x374/0xdf0 net/x25/af_x25.c:786
     __sys_connect+0x266/0x330 net/socket.c:1686
     __do_sys_connect net/socket.c:1697 [inline]
     __se_sys_connect net/socket.c:1694 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1694
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fe8d0c57c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fe8d0c586d4
    R13: 00000000004be378 R14: 00000000004ceb00 R15: 00000000ffffffff
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Andrew Hendry <andrew.hendry@gmail.com>
    Cc: linux-x25@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 82379cf03bee3b8f7d2973363723f561dfe5c9c0
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Feb 8 12:41:05 2019 -0800

    net/x25: do not hold the cpu too long in x25_new_lci()
    
    commit cf657d22ee1f0e887326a92169f2e28dc932fd10 upstream.
    
    Due to quadratic behavior of x25_new_lci(), syzbot was able
    to trigger an rcu stall.
    
    Fix this by not blocking BH for the whole duration of
    the function, and inserting a reschedule point when possible.
    
    If we care enough, using a bitmap could get rid of the quadratic
    behavior.
    
    syzbot report :
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    0-...!: (10500 ticks this GP) idle=4fa/1/0x4000000000000002 softirq=283376/283376 fqs=0
    rcu:     (t=10501 jiffies g=383105 q=136)
    rcu: rcu_preempt kthread starved for 10502 jiffies! g383105 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu: RCU grace-period kthread stack dump:
    rcu_preempt     I28928    10      2 0x80000000
    Call Trace:
     context_switch kernel/sched/core.c:2844 [inline]
     __schedule+0x817/0x1cc0 kernel/sched/core.c:3485
     schedule+0x92/0x180 kernel/sched/core.c:3529
     schedule_timeout+0x4db/0xfd0 kernel/time/timer.c:1803
     rcu_gp_fqs_loop kernel/rcu/tree.c:1948 [inline]
     rcu_gp_kthread+0x956/0x17a0 kernel/rcu/tree.c:2105
     kthread+0x357/0x430 kernel/kthread.c:246
     ret_from_fork+0x3a/0x50 arch/x86/entry/entry_64.S:352
    NMI backtrace for cpu 0
    CPU: 0 PID: 8759 Comm: syz-executor2 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x172/0x1f0 lib/dump_stack.c:113
     nmi_cpu_backtrace.cold+0x63/0xa4 lib/nmi_backtrace.c:101
     nmi_trigger_cpumask_backtrace+0x1be/0x236 lib/nmi_backtrace.c:62
     arch_trigger_cpumask_backtrace+0x14/0x20 arch/x86/kernel/apic/hw_nmi.c:38
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x183/0x1cf kernel/rcu/tree.c:1211
     print_cpu_stall kernel/rcu/tree.c:1348 [inline]
     check_cpu_stall kernel/rcu/tree.c:1422 [inline]
     rcu_pending kernel/rcu/tree.c:3018 [inline]
     rcu_check_callbacks.cold+0x500/0xa4a kernel/rcu/tree.c:2521
     update_process_times+0x32/0x80 kernel/time/timer.c:1635
     tick_sched_handle+0xa2/0x190 kernel/time/tick-sched.c:161
     tick_sched_timer+0x47/0x130 kernel/time/tick-sched.c:1271
     __run_hrtimer kernel/time/hrtimer.c:1389 [inline]
     __hrtimer_run_queues+0x33e/0xde0 kernel/time/hrtimer.c:1451
     hrtimer_interrupt+0x314/0x770 kernel/time/hrtimer.c:1509
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1035 [inline]
     smp_apic_timer_interrupt+0x120/0x570 arch/x86/kernel/apic/apic.c:1060
     apic_timer_interrupt+0xf/0x20 arch/x86/entry/entry_64.S:807
     </IRQ>
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x13e/0x290 kernel/locking/qrwlock.c:86
    Code: 00 00 fc ff df 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 <41> 0f b6 55 00 41 38 d7 7c eb 84 d2 74 e7 48 89 df e8 6c 0f 4f 00
    RSP: 0018:ffff88805f117bd8 EFLAGS: 00000206 ORIG_RAX: ffffffffffffff13
    RAX: 0000000000000300 RBX: ffffffff89413ba0 RCX: 1ffffffff1282774
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89413ba0
    RBP: ffff88805f117c70 R08: 1ffffffff1282774 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: 00000000000000ff
    R13: fffffbfff1282774 R14: 1ffff1100be22f7d R15: 0000000000000003
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:705
     __sys_bind+0x23f/0x290 net/socket.c:1505
     __do_sys_bind net/socket.c:1516 [inline]
     __se_sys_bind net/socket.c:1514 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1514
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fafccd0dc78 EFLAGS: 00000246 ORIG_RAX: 0000000000000031
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000240 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fafccd0e6d4
    R13: 00000000004bdf8b R14: 00000000004ce4b8 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 8752 Comm: syz-executor4 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__x25_find_socket+0x78/0x120 net/x25/af_x25.c:328
    Code: 89 f8 48 c1 e8 03 80 3c 18 00 0f 85 a6 00 00 00 4d 8b 64 24 68 4d 85 e4 74 7f e8 03 97 3d fb 49 83 ec 68 74 74 e8 f8 96 3d fb <49> 8d bc 24 88 04 00 00 48 89 f8 48 c1 e8 03 0f b6 04 18 84 c0 74
    RSP: 0018:ffff8880639efc58 EFLAGS: 00000246
    RAX: 0000000000040000 RBX: dffffc0000000000 RCX: ffffc9000e677000
    RDX: 0000000000040000 RSI: ffffffff863244b8 RDI: ffff88806a764628
    RBP: ffff8880639efc80 R08: ffff8880a80d05c0 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: ffff88806a7645c0
    R13: 0000000000000001 R14: ffff88809f29ac00 R15: 0000000000000000
    FS:  00007fe8d0c58700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000001b32823000 CR3: 00000000672eb000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     x25_new_lci net/x25/af_x25.c:357 [inline]
     x25_connect+0x374/0xdf0 net/x25/af_x25.c:786
     __sys_connect+0x266/0x330 net/socket.c:1686
     __do_sys_connect net/socket.c:1697 [inline]
     __se_sys_connect net/socket.c:1694 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1694
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fe8d0c57c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fe8d0c586d4
    R13: 00000000004be378 R14: 00000000004ceb00 R15: 00000000ffffffff
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Andrew Hendry <andrew.hendry@gmail.com>
    Cc: linux-x25@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 72bd16e67ac70b439f316e83500fb73e246ee412
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Feb 8 12:41:05 2019 -0800

    net/x25: do not hold the cpu too long in x25_new_lci()
    
    commit cf657d22ee1f0e887326a92169f2e28dc932fd10 upstream.
    
    Due to quadratic behavior of x25_new_lci(), syzbot was able
    to trigger an rcu stall.
    
    Fix this by not blocking BH for the whole duration of
    the function, and inserting a reschedule point when possible.
    
    If we care enough, using a bitmap could get rid of the quadratic
    behavior.
    
    syzbot report :
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    0-...!: (10500 ticks this GP) idle=4fa/1/0x4000000000000002 softirq=283376/283376 fqs=0
    rcu:     (t=10501 jiffies g=383105 q=136)
    rcu: rcu_preempt kthread starved for 10502 jiffies! g383105 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu: RCU grace-period kthread stack dump:
    rcu_preempt     I28928    10      2 0x80000000
    Call Trace:
     context_switch kernel/sched/core.c:2844 [inline]
     __schedule+0x817/0x1cc0 kernel/sched/core.c:3485
     schedule+0x92/0x180 kernel/sched/core.c:3529
     schedule_timeout+0x4db/0xfd0 kernel/time/timer.c:1803
     rcu_gp_fqs_loop kernel/rcu/tree.c:1948 [inline]
     rcu_gp_kthread+0x956/0x17a0 kernel/rcu/tree.c:2105
     kthread+0x357/0x430 kernel/kthread.c:246
     ret_from_fork+0x3a/0x50 arch/x86/entry/entry_64.S:352
    NMI backtrace for cpu 0
    CPU: 0 PID: 8759 Comm: syz-executor2 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x172/0x1f0 lib/dump_stack.c:113
     nmi_cpu_backtrace.cold+0x63/0xa4 lib/nmi_backtrace.c:101
     nmi_trigger_cpumask_backtrace+0x1be/0x236 lib/nmi_backtrace.c:62
     arch_trigger_cpumask_backtrace+0x14/0x20 arch/x86/kernel/apic/hw_nmi.c:38
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x183/0x1cf kernel/rcu/tree.c:1211
     print_cpu_stall kernel/rcu/tree.c:1348 [inline]
     check_cpu_stall kernel/rcu/tree.c:1422 [inline]
     rcu_pending kernel/rcu/tree.c:3018 [inline]
     rcu_check_callbacks.cold+0x500/0xa4a kernel/rcu/tree.c:2521
     update_process_times+0x32/0x80 kernel/time/timer.c:1635
     tick_sched_handle+0xa2/0x190 kernel/time/tick-sched.c:161
     tick_sched_timer+0x47/0x130 kernel/time/tick-sched.c:1271
     __run_hrtimer kernel/time/hrtimer.c:1389 [inline]
     __hrtimer_run_queues+0x33e/0xde0 kernel/time/hrtimer.c:1451
     hrtimer_interrupt+0x314/0x770 kernel/time/hrtimer.c:1509
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1035 [inline]
     smp_apic_timer_interrupt+0x120/0x570 arch/x86/kernel/apic/apic.c:1060
     apic_timer_interrupt+0xf/0x20 arch/x86/entry/entry_64.S:807
     </IRQ>
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x13e/0x290 kernel/locking/qrwlock.c:86
    Code: 00 00 fc ff df 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 <41> 0f b6 55 00 41 38 d7 7c eb 84 d2 74 e7 48 89 df e8 6c 0f 4f 00
    RSP: 0018:ffff88805f117bd8 EFLAGS: 00000206 ORIG_RAX: ffffffffffffff13
    RAX: 0000000000000300 RBX: ffffffff89413ba0 RCX: 1ffffffff1282774
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89413ba0
    RBP: ffff88805f117c70 R08: 1ffffffff1282774 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: 00000000000000ff
    R13: fffffbfff1282774 R14: 1ffff1100be22f7d R15: 0000000000000003
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:705
     __sys_bind+0x23f/0x290 net/socket.c:1505
     __do_sys_bind net/socket.c:1516 [inline]
     __se_sys_bind net/socket.c:1514 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1514
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fafccd0dc78 EFLAGS: 00000246 ORIG_RAX: 0000000000000031
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000240 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fafccd0e6d4
    R13: 00000000004bdf8b R14: 00000000004ce4b8 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 8752 Comm: syz-executor4 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__x25_find_socket+0x78/0x120 net/x25/af_x25.c:328
    Code: 89 f8 48 c1 e8 03 80 3c 18 00 0f 85 a6 00 00 00 4d 8b 64 24 68 4d 85 e4 74 7f e8 03 97 3d fb 49 83 ec 68 74 74 e8 f8 96 3d fb <49> 8d bc 24 88 04 00 00 48 89 f8 48 c1 e8 03 0f b6 04 18 84 c0 74
    RSP: 0018:ffff8880639efc58 EFLAGS: 00000246
    RAX: 0000000000040000 RBX: dffffc0000000000 RCX: ffffc9000e677000
    RDX: 0000000000040000 RSI: ffffffff863244b8 RDI: ffff88806a764628
    RBP: ffff8880639efc80 R08: ffff8880a80d05c0 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: ffff88806a7645c0
    R13: 0000000000000001 R14: ffff88809f29ac00 R15: 0000000000000000
    FS:  00007fe8d0c58700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000001b32823000 CR3: 00000000672eb000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     x25_new_lci net/x25/af_x25.c:357 [inline]
     x25_connect+0x374/0xdf0 net/x25/af_x25.c:786
     __sys_connect+0x266/0x330 net/socket.c:1686
     __do_sys_connect net/socket.c:1697 [inline]
     __se_sys_connect net/socket.c:1694 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1694
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fe8d0c57c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fe8d0c586d4
    R13: 00000000004be378 R14: 00000000004ceb00 R15: 00000000ffffffff
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Andrew Hendry <andrew.hendry@gmail.com>
    Cc: linux-x25@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit cf657d22ee1f0e887326a92169f2e28dc932fd10
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Feb 8 12:41:05 2019 -0800

    net/x25: do not hold the cpu too long in x25_new_lci()
    
    Due to quadratic behavior of x25_new_lci(), syzbot was able
    to trigger an rcu stall.
    
    Fix this by not blocking BH for the whole duration of
    the function, and inserting a reschedule point when possible.
    
    If we care enough, using a bitmap could get rid of the quadratic
    behavior.
    
    syzbot report :
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    0-...!: (10500 ticks this GP) idle=4fa/1/0x4000000000000002 softirq=283376/283376 fqs=0
    rcu:     (t=10501 jiffies g=383105 q=136)
    rcu: rcu_preempt kthread starved for 10502 jiffies! g383105 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu: RCU grace-period kthread stack dump:
    rcu_preempt     I28928    10      2 0x80000000
    Call Trace:
     context_switch kernel/sched/core.c:2844 [inline]
     __schedule+0x817/0x1cc0 kernel/sched/core.c:3485
     schedule+0x92/0x180 kernel/sched/core.c:3529
     schedule_timeout+0x4db/0xfd0 kernel/time/timer.c:1803
     rcu_gp_fqs_loop kernel/rcu/tree.c:1948 [inline]
     rcu_gp_kthread+0x956/0x17a0 kernel/rcu/tree.c:2105
     kthread+0x357/0x430 kernel/kthread.c:246
     ret_from_fork+0x3a/0x50 arch/x86/entry/entry_64.S:352
    NMI backtrace for cpu 0
    CPU: 0 PID: 8759 Comm: syz-executor2 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x172/0x1f0 lib/dump_stack.c:113
     nmi_cpu_backtrace.cold+0x63/0xa4 lib/nmi_backtrace.c:101
     nmi_trigger_cpumask_backtrace+0x1be/0x236 lib/nmi_backtrace.c:62
     arch_trigger_cpumask_backtrace+0x14/0x20 arch/x86/kernel/apic/hw_nmi.c:38
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x183/0x1cf kernel/rcu/tree.c:1211
     print_cpu_stall kernel/rcu/tree.c:1348 [inline]
     check_cpu_stall kernel/rcu/tree.c:1422 [inline]
     rcu_pending kernel/rcu/tree.c:3018 [inline]
     rcu_check_callbacks.cold+0x500/0xa4a kernel/rcu/tree.c:2521
     update_process_times+0x32/0x80 kernel/time/timer.c:1635
     tick_sched_handle+0xa2/0x190 kernel/time/tick-sched.c:161
     tick_sched_timer+0x47/0x130 kernel/time/tick-sched.c:1271
     __run_hrtimer kernel/time/hrtimer.c:1389 [inline]
     __hrtimer_run_queues+0x33e/0xde0 kernel/time/hrtimer.c:1451
     hrtimer_interrupt+0x314/0x770 kernel/time/hrtimer.c:1509
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1035 [inline]
     smp_apic_timer_interrupt+0x120/0x570 arch/x86/kernel/apic/apic.c:1060
     apic_timer_interrupt+0xf/0x20 arch/x86/entry/entry_64.S:807
     </IRQ>
    RIP: 0010:__read_once_size include/linux/compiler.h:193 [inline]
    RIP: 0010:queued_write_lock_slowpath+0x13e/0x290 kernel/locking/qrwlock.c:86
    Code: 00 00 fc ff df 4c 8d 2c 01 41 83 c7 03 41 0f b6 45 00 41 38 c7 7c 08 84 c0 0f 85 0c 01 00 00 8b 03 3d 00 01 00 00 74 1a f3 90 <41> 0f b6 55 00 41 38 d7 7c eb 84 d2 74 e7 48 89 df e8 6c 0f 4f 00
    RSP: 0018:ffff88805f117bd8 EFLAGS: 00000206 ORIG_RAX: ffffffffffffff13
    RAX: 0000000000000300 RBX: ffffffff89413ba0 RCX: 1ffffffff1282774
    RDX: 0000000000000000 RSI: 0000000000000004 RDI: ffffffff89413ba0
    RBP: ffff88805f117c70 R08: 1ffffffff1282774 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: 00000000000000ff
    R13: fffffbfff1282774 R14: 1ffff1100be22f7d R15: 0000000000000003
     queued_write_lock include/asm-generic/qrwlock.h:104 [inline]
     do_raw_write_lock+0x1d6/0x290 kernel/locking/spinlock_debug.c:203
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:204 [inline]
     _raw_write_lock_bh+0x3b/0x50 kernel/locking/spinlock.c:312
     x25_insert_socket+0x21/0xe0 net/x25/af_x25.c:267
     x25_bind+0x273/0x340 net/x25/af_x25.c:705
     __sys_bind+0x23f/0x290 net/socket.c:1505
     __do_sys_bind net/socket.c:1516 [inline]
     __se_sys_bind net/socket.c:1514 [inline]
     __x64_sys_bind+0x73/0xb0 net/socket.c:1514
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fafccd0dc78 EFLAGS: 00000246 ORIG_RAX: 0000000000000031
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000240 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fafccd0e6d4
    R13: 00000000004bdf8b R14: 00000000004ce4b8 R15: 00000000ffffffff
    Sending NMI from CPU 0 to CPUs 1:
    NMI backtrace for cpu 1
    CPU: 1 PID: 8752 Comm: syz-executor4 Not tainted 5.0.0-rc4+ #51
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:__x25_find_socket+0x78/0x120 net/x25/af_x25.c:328
    Code: 89 f8 48 c1 e8 03 80 3c 18 00 0f 85 a6 00 00 00 4d 8b 64 24 68 4d 85 e4 74 7f e8 03 97 3d fb 49 83 ec 68 74 74 e8 f8 96 3d fb <49> 8d bc 24 88 04 00 00 48 89 f8 48 c1 e8 03 0f b6 04 18 84 c0 74
    RSP: 0018:ffff8880639efc58 EFLAGS: 00000246
    RAX: 0000000000040000 RBX: dffffc0000000000 RCX: ffffc9000e677000
    RDX: 0000000000040000 RSI: ffffffff863244b8 RDI: ffff88806a764628
    RBP: ffff8880639efc80 R08: ffff8880a80d05c0 R09: fffffbfff1282775
    R10: fffffbfff1282774 R11: ffffffff89413ba3 R12: ffff88806a7645c0
    R13: 0000000000000001 R14: ffff88809f29ac00 R15: 0000000000000000
    FS:  00007fe8d0c58700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000001b32823000 CR3: 00000000672eb000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     x25_new_lci net/x25/af_x25.c:357 [inline]
     x25_connect+0x374/0xdf0 net/x25/af_x25.c:786
     __sys_connect+0x266/0x330 net/socket.c:1686
     __do_sys_connect net/socket.c:1697 [inline]
     __se_sys_connect net/socket.c:1694 [inline]
     __x64_sys_connect+0x73/0xb0 net/socket.c:1694
     do_syscall_64+0x103/0x610 arch/x86/entry/common.c:290
     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    RIP: 0033:0x457e39
    Code: ad b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b8 fb ff c3 66 2e 0f 1f 84 00 00 00 00
    RSP: 002b:00007fe8d0c57c78 EFLAGS: 00000246 ORIG_RAX: 000000000000002a
    RAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000000457e39
    RDX: 0000000000000012 RSI: 0000000020000200 RDI: 0000000000000004
    RBP: 000000000073bf00 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 00007fe8d0c586d4
    R13: 00000000004be378 R14: 00000000004ceb00 R15: 00000000ffffffff
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Cc: Andrew Hendry <andrew.hendry@gmail.com>
    Cc: linux-x25@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 3a8844c298522fa22df4bdd863011e5b639a4e84
Author: Lyude Paul <lyude@redhat.com>
Date:   Fri Feb 1 19:20:01 2019 -0500

    drm/dp_mst: Fix unbalanced malloc ref in drm_dp_mst_deallocate_vcpi()
    
    In drm_dp_mst_deallocate_vcpi(), we currently unconditionally call
    drm_dp_mst_put_port_malloc() on the port that's passed to us, even if we
    never successfully allocated VCPI to it. This is contrary to what we do
    in drm_dp_mst_allocate_vcpi(), where we only call
    drm_dp_mst_get_port_malloc() on the passed port if we successfully
    allocated VCPI to it.
    
    As a result, if drm_dp_mst_allocate_vcpi() fails during a modeset and
    another successive modeset calls drm_dp_mst_deallocate_vcpi() we will
    end up dropping someone else's malloc reference to the port. Example:
    
    [  962.309260] ==================================================================
    [  962.309290] BUG: KASAN: use-after-free in drm_dp_mst_put_port_malloc+0x72/0x180 [drm_kms_helper]
    [  962.309296] Read of size 4 at addr ffff888416c30004 by task kworker/0:1H/500
    
    [  962.309308] CPU: 0 PID: 500 Comm: kworker/0:1H Tainted: G        W  O      5.0.0-rc2Lyude-Test+ #1
    [  962.309313] Hardware name: LENOVO 20L8S2N800/20L8S2N800, BIOS N22ET35W (1.12 ) 04/09/2018
    [  962.309428] Workqueue: events_highpri intel_atomic_cleanup_work [i915]
    [  962.309434] Call Trace:
    [  962.309452]  dump_stack+0xad/0x150
    [  962.309462]  ? dump_stack_print_info.cold.0+0x1b/0x1b
    [  962.309472]  ? kmsg_dump_rewind_nolock+0xd9/0xd9
    [  962.309504]  ? drm_dp_mst_put_port_malloc+0x72/0x180 [drm_kms_helper]
    [  962.309515]  print_address_description+0x6c/0x23c
    [  962.309542]  ? drm_dp_mst_put_port_malloc+0x72/0x180 [drm_kms_helper]
    [  962.309568]  ? drm_dp_mst_put_port_malloc+0x72/0x180 [drm_kms_helper]
    [  962.309577]  kasan_report.cold.3+0x1a/0x32
    [  962.309605]  ? drm_dp_mst_put_port_malloc+0x72/0x180 [drm_kms_helper]
    [  962.309631]  drm_dp_mst_put_port_malloc+0x72/0x180 [drm_kms_helper]
    [  962.309658]  ? drm_dp_mst_put_mstb_malloc+0x180/0x180 [drm_kms_helper]
    [  962.309687]  drm_dp_mst_destroy_state+0xcd/0x120 [drm_kms_helper]
    [  962.309745]  drm_atomic_state_default_clear+0x6ee/0xcc0 [drm]
    [  962.309864]  intel_atomic_state_clear+0xe/0x80 [i915]
    [  962.309928]  __drm_atomic_state_free+0x35/0xd0 [drm]
    [  962.310044]  intel_atomic_cleanup_work+0x56/0x70 [i915]
    [  962.310057]  process_one_work+0x884/0x1400
    [  962.310067]  ? drain_workqueue+0x5a0/0x5a0
    [  962.310075]  ? __schedule+0x87f/0x1e80
    [  962.310086]  ? __sched_text_start+0x8/0x8
    [  962.310095]  ? run_rebalance_domains+0x400/0x400
    [  962.310110]  ? deref_stack_reg+0xb4/0x120
    [  962.310117]  ? __read_once_size_nocheck.constprop.7+0x10/0x10
    [  962.310124]  ? worker_enter_idle+0x47f/0x6a0
    [  962.310134]  ? schedule+0xd7/0x2e0
    [  962.310141]  ? __schedule+0x1e80/0x1e80
    [  962.310148]  ? _raw_spin_lock_irq+0x9f/0x130
    [  962.310155]  ? _raw_write_unlock_irqrestore+0x110/0x110
    [  962.310164]  worker_thread+0x196/0x11e0
    [  962.310175]  ? set_load_weight+0x2e0/0x2e0
    [  962.310181]  ? __switch_to_asm+0x34/0x70
    [  962.310187]  ? __switch_to_asm+0x40/0x70
    [  962.310194]  ? process_one_work+0x1400/0x1400
    [  962.310199]  ? __switch_to_asm+0x40/0x70
    [  962.310205]  ? __switch_to_asm+0x34/0x70
    [  962.310211]  ? __switch_to_asm+0x34/0x70
    [  962.310216]  ? __switch_to_asm+0x40/0x70
    [  962.310221]  ? __switch_to_asm+0x34/0x70
    [  962.310226]  ? __switch_to_asm+0x40/0x70
    [  962.310231]  ? __switch_to_asm+0x34/0x70
    [  962.310236]  ? __switch_to_asm+0x40/0x70
    [  962.310242]  ? syscall_return_via_sysret+0xf/0x7f
    [  962.310248]  ? __switch_to_asm+0x34/0x70
    [  962.310253]  ? __switch_to_asm+0x40/0x70
    [  962.310258]  ? __switch_to_asm+0x34/0x70
    [  962.310263]  ? __switch_to_asm+0x40/0x70
    [  962.310268]  ? __switch_to_asm+0x34/0x70
    [  962.310273]  ? __switch_to_asm+0x40/0x70
    [  962.310281]  ? __schedule+0x87f/0x1e80
    [  962.310292]  ? __sched_text_start+0x8/0x8
    [  962.310300]  ? save_stack+0x8c/0xb0
    [  962.310308]  ? __kasan_kmalloc.constprop.6+0xc6/0xd0
    [  962.310313]  ? kthread+0x98/0x3a0
    [  962.310318]  ? ret_from_fork+0x35/0x40
    [  962.310334]  ? __wake_up_common+0x178/0x6f0
    [  962.310343]  ? _raw_spin_lock_irqsave+0xa4/0x140
    [  962.310349]  ? __lock_text_start+0x8/0x8
    [  962.310355]  ? _raw_write_lock_irqsave+0x70/0x130
    [  962.310360]  ? __lock_text_start+0x8/0x8
    [  962.310371]  ? process_one_work+0x1400/0x1400
    [  962.310376]  kthread+0x2e2/0x3a0
    [  962.310383]  ? kthread_create_on_node+0xc0/0xc0
    [  962.310389]  ret_from_fork+0x35/0x40
    
    [  962.310401] Allocated by task 1462:
    [  962.310410]  __kasan_kmalloc.constprop.6+0xc6/0xd0
    [  962.310437]  drm_dp_add_port+0xd60/0x1960 [drm_kms_helper]
    [  962.310464]  drm_dp_send_link_address+0x4b0/0x770 [drm_kms_helper]
    [  962.310491]  drm_dp_check_and_send_link_address+0x197/0x1f0 [drm_kms_helper]
    [  962.310515]  drm_dp_mst_link_probe_work+0x2b6/0x330 [drm_kms_helper]
    [  962.310522]  process_one_work+0x884/0x1400
    [  962.310529]  worker_thread+0x196/0x11e0
    [  962.310533]  kthread+0x2e2/0x3a0
    [  962.310538]  ret_from_fork+0x35/0x40
    
    [  962.310543] Freed by task 500:
    [  962.310550]  __kasan_slab_free+0x133/0x180
    [  962.310555]  kfree+0x92/0x1a0
    [  962.310581]  drm_dp_mst_put_port_malloc+0x14d/0x180 [drm_kms_helper]
    [  962.310693]  intel_connector_destroy+0xb2/0xe0 [i915]
    [  962.310747]  drm_mode_object_put.part.0+0x12b/0x1a0 [drm]
    [  962.310802]  drm_atomic_state_default_clear+0x1f2/0xcc0 [drm]
    [  962.310916]  intel_atomic_state_clear+0xe/0x80 [i915]
    [  962.310972]  __drm_atomic_state_free+0x35/0xd0 [drm]
    [  962.311083]  intel_atomic_cleanup_work+0x56/0x70 [i915]
    [  962.311092]  process_one_work+0x884/0x1400
    [  962.311098]  worker_thread+0x196/0x11e0
    [  962.311103]  kthread+0x2e2/0x3a0
    [  962.311108]  ret_from_fork+0x35/0x40
    
    [  962.311116] The buggy address belongs to the object at ffff888416c30000
                    which belongs to the cache kmalloc-2k of size 2048
    [  962.311122] The buggy address is located 4 bytes inside of
                    2048-byte region [ffff888416c30000, ffff888416c30800)
    [  962.311124] The buggy address belongs to the page:
    [  962.311132] page:ffffea00105b0c00 count:1 mapcount:0 mapping:ffff88841d003040 index:0x0 compound_mapcount: 0
    [  962.311142] flags: 0x8000000000010200(slab|head)
    [  962.311152] raw: 8000000000010200 dead000000000100 dead000000000200 ffff88841d003040
    [  962.311159] raw: 0000000000000000 00000000000f000f 00000001ffffffff 0000000000000000
    [  962.311162] page dumped because: kasan: bad access detected
    
    So, bail early if drm_dp_mst_deallocate_vcpi() is called on a port with
    no VCPI allocation. Additionally, clean up the surrounding kerneldoc
    while we're at it since the port is assumed to be kept around because
    the DRM driver is expected to hold a malloc reference to it, not just
    us.
    
    Changes since v1:
    * Doc changes - danvet
    
    Signed-off-by: Lyude Paul <lyude@redhat.com>
    Fixes: eceae1472467 ("drm/dp_mst: Start tracking per-port VCPI allocations")
    Reviewed-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Link: https://patchwork.freedesktop.org/patch/msgid/20190202002023.29665-2-lyude@redhat.com

commit 5db25c9eb893df8f6b93c1d97b8006d768e1b6f5
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Feb 1 01:47:53 2019 +0100

    driver core: Do not resume suppliers under device_links_write_lock()
    
    It is incorrect to call pm_runtime_get_sync() under
    device_links_write_lock(), because it may end up trying to take
    device_links_read_lock() while resuming the target device and that
    will deadlock in the non-SRCU case, so avoid that by resuming the
    supplier device in device_link_add() before calling
    device_links_write_lock().
    
    Fixes: 21d5c57b3726 ("PM / runtime: Use device links")
    Fixes: baa8809f6097 ("PM / runtime: Optimize the use of device links")
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 739aff87a8176c1193cc46ebce5ce6dd537501af
Author: Lorenzo Bianconi <lorenzo.bianconi@redhat.com>
Date:   Sun Nov 11 15:15:28 2018 +0100

    iio: imu: st_lsm6dsx: introduce locked read/write utility routines
    
    Add st_lsm6dsx_update_bits_locked, st_lsm6dsx_read_locked and
    st_lsm6dsx_write_locked utility routines in order to guarantee
    the bus access is atomic respect to reg page configuration.
    This is a preliminary patch to add i2c sensor hub support since
    i2c master registers are accessed through a reg page multiplexer
    
    Signed-off-by: Lorenzo Bianconi <lorenzo.bianconi@redhat.com>
    Signed-off-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>

commit 523983401644ebeb331c923c28c9591c07430a7d
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Wed Aug 22 05:54:37 2018 +0800

    Btrfs: kill btrfs_clear_path_blocking
    
    Btrfs's btree locking has two modes, spinning mode and blocking mode,
    while searching btree, locking is always acquired in spinning mode and
    then converted to blocking mode if necessary, and in some hot paths we may
    switch the locking back to spinning mode by btrfs_clear_path_blocking().
    
    When acquiring locks, both of reader and writer need to wait for blocking
    readers and writers to complete before doing read_lock()/write_lock().
    
    The problem is that btrfs_clear_path_blocking() needs to switch nodes
    in the path to blocking mode at first (by btrfs_set_path_blocking) to
    make lockdep happy before doing its actual clearing blocking job.
    
    When switching to blocking mode from spinning mode, it consists of
    
    step 1) bumping up blocking readers counter and
    step 2) read_unlock()/write_unlock(),
    
    this has caused serious ping-pong effect if there're a great amount of
    concurrent readers/writers, as waiters will be woken up and go to
    sleep immediately.
    
    1) Killing this kind of ping-pong results in a big improvement in my 1600k
    files creation script,
    
    MNT=/mnt/btrfs
    mkfs.btrfs -f /dev/sdf
    mount /dev/def $MNT
    time fsmark  -D  10000  -S0  -n  100000  -s  0  -L  1 -l /tmp/fs_log.txt \
            -d  $MNT/0  -d  $MNT/1 \
            -d  $MNT/2  -d  $MNT/3 \
            -d  $MNT/4  -d  $MNT/5 \
            -d  $MNT/6  -d  $MNT/7 \
            -d  $MNT/8  -d  $MNT/9 \
            -d  $MNT/10  -d  $MNT/11 \
            -d  $MNT/12  -d  $MNT/13 \
            -d  $MNT/14  -d  $MNT/15
    
    w/o patch:
    real    2m27.307s
    user    0m12.839s
    sys     13m42.831s
    
    w/ patch:
    real    1m2.273s
    user    0m15.802s
    sys     8m16.495s
    
    1.1) latency histogram from funclatency[1]
    
    Overall with the patch, there're ~50% less write lock acquisition and
    the 95% max latency that write lock takes also reduces to ~100ms from
    >500ms.
    
    --------------------------------------------
    w/o patch:
    --------------------------------------------
    Function = btrfs_tree_lock
         msecs               : count     distribution
             0 -> 1          : 2385222  |****************************************|
             2 -> 3          : 37147    |                                        |
             4 -> 7          : 20452    |                                        |
             8 -> 15         : 13131    |                                        |
            16 -> 31         : 3877     |                                        |
            32 -> 63         : 3900     |                                        |
            64 -> 127        : 2612     |                                        |
           128 -> 255        : 974      |                                        |
           256 -> 511        : 165      |                                        |
           512 -> 1023       : 13       |                                        |
    
    Function = btrfs_tree_read_lock
         msecs               : count     distribution
             0 -> 1          : 6743860  |****************************************|
             2 -> 3          : 2146     |                                        |
             4 -> 7          : 190      |                                        |
             8 -> 15         : 38       |                                        |
            16 -> 31         : 4        |                                        |
    
    --------------------------------------------
    w/ patch:
    --------------------------------------------
    Function = btrfs_tree_lock
         msecs               : count     distribution
             0 -> 1          : 1318454  |****************************************|
             2 -> 3          : 6800     |                                        |
             4 -> 7          : 3664     |                                        |
             8 -> 15         : 2145     |                                        |
            16 -> 31         : 809      |                                        |
            32 -> 63         : 219      |                                        |
            64 -> 127        : 10       |                                        |
    
    Function = btrfs_tree_read_lock
         msecs               : count     distribution
             0 -> 1          : 6854317  |****************************************|
             2 -> 3          : 2383     |                                        |
             4 -> 7          : 601      |                                        |
             8 -> 15         : 92       |                                        |
    
    2) dbench also proves the improvement,
    dbench -t 120 -D /mnt/btrfs 16
    
    w/o patch:
    Throughput 158.363 MB/sec
    
    w/ patch:
    Throughput 449.52 MB/sec
    
    3) xfstests didn't show any additional failures.
    
    One thing to note is that callers may set path->leave_spinning to have
    all nodes in the path stay in spinning mode, which means callers are
    ready to not sleep before releasing the path, but it won't cause
    problems if they don't want to sleep in blocking mode.
    
    [1]: https://github.com/iovisor/bcc/blob/master/tools/funclatency.py
    
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 4b6f8e9695da65e29f9f8ee84b39e0ba5b45e8e9
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Tue Aug 14 10:46:53 2018 +0800

    Btrfs: do not unnecessarily pass write_lock_level when processing leaf
    
    As we're going to return right after the call, it's not necessary to get
    update the new write_lock_level from unlock_up.
    
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 70cc08c44fb55b587c7485a15549e9f9a12c9405
Author: Prateek Sood <prsood@codeaurora.org>
Date:   Thu Sep 7 20:00:58 2017 +0530

    locking/rwsem-xadd: Fix missed wakeup due to reordering of load
    
    commit 9c29c31830a4eca724e137a9339137204bbb31be upstream.
    
    If a spinner is present, there is a chance that the load of
    rwsem_has_spinner() in rwsem_wake() can be reordered with
    respect to decrement of rwsem count in __up_write() leading
    to wakeup being missed:
    
     spinning writer                  up_write caller
     ---------------                  -----------------------
     [S] osq_unlock()                 [L] osq
      spin_lock(wait_lock)
      sem->count=0xFFFFFFFF00000001
                +0xFFFFFFFF00000000
      count=sem->count
      MB
                                       sem->count=0xFFFFFFFE00000001
                                                 -0xFFFFFFFF00000001
                                       spin_trylock(wait_lock)
                                       return
     rwsem_try_write_lock(count)
     spin_unlock(wait_lock)
     schedule()
    
    Reordering of atomic_long_sub_return_release() in __up_write()
    and rwsem_has_spinner() in rwsem_wake() can cause missing of
    wakeup in up_write() context. In spinning writer, sem->count
    and local variable count is 0XFFFFFFFE00000001. It would result
    in rwsem_try_write_lock() failing to acquire rwsem and spinning
    writer going to sleep in rwsem_down_write_failed().
    
    The smp_rmb() will make sure that the spinner state is
    consulted after sem->count is updated in up_write context.
    
    Signed-off-by: Prateek Sood <prsood@codeaurora.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dave@stgolabs.net
    Cc: longman@redhat.com
    Cc: parri.andrea@gmail.com
    Cc: sramana@codeaurora.org
    Link: http://lkml.kernel.org/r/1504794658-15397-1-git-send-email-prsood@codeaurora.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Amit Pundir <amit.pundir@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0cbde6c5b67307f353636f8074881fb4d1924709
Author: Prateek Sood <prsood@codeaurora.org>
Date:   Thu Sep 7 20:00:58 2017 +0530

    locking/rwsem-xadd: Fix missed wakeup due to reordering of load
    
    commit 9c29c31830a4eca724e137a9339137204bbb31be upstream.
    
    If a spinner is present, there is a chance that the load of
    rwsem_has_spinner() in rwsem_wake() can be reordered with
    respect to decrement of rwsem count in __up_write() leading
    to wakeup being missed:
    
     spinning writer                  up_write caller
     ---------------                  -----------------------
     [S] osq_unlock()                 [L] osq
      spin_lock(wait_lock)
      sem->count=0xFFFFFFFF00000001
                +0xFFFFFFFF00000000
      count=sem->count
      MB
                                       sem->count=0xFFFFFFFE00000001
                                                 -0xFFFFFFFF00000001
                                       spin_trylock(wait_lock)
                                       return
     rwsem_try_write_lock(count)
     spin_unlock(wait_lock)
     schedule()
    
    Reordering of atomic_long_sub_return_release() in __up_write()
    and rwsem_has_spinner() in rwsem_wake() can cause missing of
    wakeup in up_write() context. In spinning writer, sem->count
    and local variable count is 0XFFFFFFFE00000001. It would result
    in rwsem_try_write_lock() failing to acquire rwsem and spinning
    writer going to sleep in rwsem_down_write_failed().
    
    The smp_rmb() will make sure that the spinner state is
    consulted after sem->count is updated in up_write context.
    
    Signed-off-by: Prateek Sood <prsood@codeaurora.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dave@stgolabs.net
    Cc: longman@redhat.com
    Cc: parri.andrea@gmail.com
    Cc: sramana@codeaurora.org
    Link: http://lkml.kernel.org/r/1504794658-15397-1-git-send-email-prsood@codeaurora.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Amit Pundir <amit.pundir@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 42c625a486f367ad57d4257de6d9459daf9484a0
Author: Vlad Buslov <vladbu@mellanox.com>
Date:   Mon Aug 13 20:20:11 2018 +0300

    net: sched: act_ife: disable bh when taking ife_mod_lock
    
    Lockdep reports deadlock for following locking scenario in ife action:
    
    Task one:
    1) Executes ife action update.
    2) Takes tcfa_lock.
    3) Waits on ife_mod_lock which is already taken by task two.
    
    Task two:
    
    1) Executes any path that obtains ife_mod_lock without disabling bh (any
    path that takes ife_mod_lock while holding tcfa_lock has bh disabled) like
    loading a meta module, or creating new action.
    2) Takes ife_mod_lock.
    3) Task is preempted by rate estimator timer.
    4) Timer callback waits on tcfa_lock which is taken by task one.
    
    In described case tasks deadlock because they take same two locks in
    different order. To prevent potential deadlock reported by lockdep, always
    disable bh when obtaining ife_mod_lock.
    
    Lockdep warning:
    
    [  508.101192] =====================================================
    [  508.107708] WARNING: SOFTIRQ-safe -> SOFTIRQ-unsafe lock order detected
    [  508.114728] 4.18.0-rc8+ #646 Not tainted
    [  508.119050] -----------------------------------------------------
    [  508.125559] tc/5460 [HC0[0]:SC0[2]:HE1:SE0] is trying to acquire:
    [  508.132025] 000000005a938c68 (ife_mod_lock){++++}, at: find_ife_oplist+0x1e/0xc0 [act_ife]
    [  508.140996]
                   and this task is already holding:
    [  508.147548] 00000000d46f6c56 (&(&p->tcfa_lock)->rlock){+.-.}, at: tcf_ife_init+0x6ae/0xf40 [act_ife]
    [  508.157371] which would create a new lock dependency:
    [  508.162828]  (&(&p->tcfa_lock)->rlock){+.-.} -> (ife_mod_lock){++++}
    [  508.169572]
                   but this new dependency connects a SOFTIRQ-irq-safe lock:
    [  508.178197]  (&(&p->tcfa_lock)->rlock){+.-.}
    [  508.178201]
                   ... which became SOFTIRQ-irq-safe at:
    [  508.189771]   _raw_spin_lock+0x2c/0x40
    [  508.193906]   est_fetch_counters+0x41/0xb0
    [  508.198391]   est_timer+0x83/0x3c0
    [  508.202180]   call_timer_fn+0x16a/0x5d0
    [  508.206400]   run_timer_softirq+0x399/0x920
    [  508.210967]   __do_softirq+0x157/0x97d
    [  508.215102]   irq_exit+0x152/0x1c0
    [  508.218888]   smp_apic_timer_interrupt+0xc0/0x4e0
    [  508.223976]   apic_timer_interrupt+0xf/0x20
    [  508.228540]   cpuidle_enter_state+0xf8/0x5d0
    [  508.233198]   do_idle+0x28a/0x350
    [  508.236881]   cpu_startup_entry+0xc7/0xe0
    [  508.241296]   start_secondary+0x2e8/0x3f0
    [  508.245678]   secondary_startup_64+0xa5/0xb0
    [  508.250347]
                   to a SOFTIRQ-irq-unsafe lock:  (ife_mod_lock){++++}
    [  508.256531]
                   ... which became SOFTIRQ-irq-unsafe at:
    [  508.267279] ...
    [  508.267283]   _raw_write_lock+0x2c/0x40
    [  508.273653]   register_ife_op+0x118/0x2c0 [act_ife]
    [  508.278926]   do_one_initcall+0xf7/0x4d9
    [  508.283214]   do_init_module+0x18b/0x44e
    [  508.287521]   load_module+0x4167/0x5730
    [  508.291739]   __do_sys_finit_module+0x16d/0x1a0
    [  508.296654]   do_syscall_64+0x7a/0x3f0
    [  508.300788]   entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  508.306302]
                   other info that might help us debug this:
    
    [  508.315286]  Possible interrupt unsafe locking scenario:
    
    [  508.322771]        CPU0                    CPU1
    [  508.327681]        ----                    ----
    [  508.332604]   lock(ife_mod_lock);
    [  508.336300]                                local_irq_disable();
    [  508.342608]                                lock(&(&p->tcfa_lock)->rlock);
    [  508.349793]                                lock(ife_mod_lock);
    [  508.355990]   <Interrupt>
    [  508.358974]     lock(&(&p->tcfa_lock)->rlock);
    [  508.363803]
                    *** DEADLOCK ***
    
    [  508.370715] 2 locks held by tc/5460:
    [  508.374680]  #0: 00000000e27e4fa4 (rtnl_mutex){+.+.}, at: rtnetlink_rcv_msg+0x583/0x7b0
    [  508.383366]  #1: 00000000d46f6c56 (&(&p->tcfa_lock)->rlock){+.-.}, at: tcf_ife_init+0x6ae/0xf40 [act_ife]
    [  508.393648]
                   the dependencies between SOFTIRQ-irq-safe lock and the holding lock:
    [  508.403505] -> (&(&p->tcfa_lock)->rlock){+.-.} ops: 1001553 {
    [  508.409646]    HARDIRQ-ON-W at:
    [  508.413136]                     _raw_spin_lock_bh+0x34/0x40
    [  508.419059]                     gnet_stats_start_copy_compat+0xa2/0x230
    [  508.426021]                     gnet_stats_start_copy+0x16/0x20
    [  508.432333]                     tcf_action_copy_stats+0x95/0x1d0
    [  508.438735]                     tcf_action_dump_1+0xb0/0x4e0
    [  508.444795]                     tcf_action_dump+0xca/0x200
    [  508.450673]                     tcf_exts_dump+0xd9/0x320
    [  508.456392]                     fl_dump+0x1b7/0x4a0 [cls_flower]
    [  508.462798]                     tcf_fill_node+0x380/0x530
    [  508.468601]                     tfilter_notify+0xdf/0x1c0
    [  508.474404]                     tc_new_tfilter+0x84a/0xc90
    [  508.480270]                     rtnetlink_rcv_msg+0x5bd/0x7b0
    [  508.486419]                     netlink_rcv_skb+0x184/0x220
    [  508.492394]                     netlink_unicast+0x31b/0x460
    [  508.507411]                     netlink_sendmsg+0x3fb/0x840
    [  508.513390]                     sock_sendmsg+0x7b/0xd0
    [  508.518907]                     ___sys_sendmsg+0x4c6/0x610
    [  508.524797]                     __sys_sendmsg+0xd7/0x150
    [  508.530510]                     do_syscall_64+0x7a/0x3f0
    [  508.536201]                     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  508.543301]    IN-SOFTIRQ-W at:
    [  508.546834]                     _raw_spin_lock+0x2c/0x40
    [  508.552522]                     est_fetch_counters+0x41/0xb0
    [  508.558571]                     est_timer+0x83/0x3c0
    [  508.563912]                     call_timer_fn+0x16a/0x5d0
    [  508.569699]                     run_timer_softirq+0x399/0x920
    [  508.575840]                     __do_softirq+0x157/0x97d
    [  508.581538]                     irq_exit+0x152/0x1c0
    [  508.586882]                     smp_apic_timer_interrupt+0xc0/0x4e0
    [  508.593533]                     apic_timer_interrupt+0xf/0x20
    [  508.599686]                     cpuidle_enter_state+0xf8/0x5d0
    [  508.605895]                     do_idle+0x28a/0x350
    [  508.611147]                     cpu_startup_entry+0xc7/0xe0
    [  508.617097]                     start_secondary+0x2e8/0x3f0
    [  508.623029]                     secondary_startup_64+0xa5/0xb0
    [  508.629245]    INITIAL USE at:
    [  508.632686]                    _raw_spin_lock_bh+0x34/0x40
    [  508.638557]                    gnet_stats_start_copy_compat+0xa2/0x230
    [  508.645491]                    gnet_stats_start_copy+0x16/0x20
    [  508.651719]                    tcf_action_copy_stats+0x95/0x1d0
    [  508.657992]                    tcf_action_dump_1+0xb0/0x4e0
    [  508.663937]                    tcf_action_dump+0xca/0x200
    [  508.669716]                    tcf_exts_dump+0xd9/0x320
    [  508.675337]                    fl_dump+0x1b7/0x4a0 [cls_flower]
    [  508.681650]                    tcf_fill_node+0x380/0x530
    [  508.687366]                    tfilter_notify+0xdf/0x1c0
    [  508.693031]                    tc_new_tfilter+0x84a/0xc90
    [  508.698820]                    rtnetlink_rcv_msg+0x5bd/0x7b0
    [  508.704869]                    netlink_rcv_skb+0x184/0x220
    [  508.710758]                    netlink_unicast+0x31b/0x460
    [  508.716627]                    netlink_sendmsg+0x3fb/0x840
    [  508.722510]                    sock_sendmsg+0x7b/0xd0
    [  508.727931]                    ___sys_sendmsg+0x4c6/0x610
    [  508.733729]                    __sys_sendmsg+0xd7/0x150
    [  508.739346]                    do_syscall_64 +0x7a/0x3f0
    [  508.744943]                    entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  508.751930]  }
    [  508.753964]  ... key      at: [<ffffffff916b3e20>] __key.61145+0x0/0x40
    [  508.760946]  ... acquired at:
    [  508.764294]    _raw_read_lock+0x2f/0x40
    [  508.768513]    find_ife_oplist+0x1e/0xc0 [act_ife]
    [  508.773692]    tcf_ife_init+0x82f/0xf40 [act_ife]
    [  508.778785]    tcf_action_init_1+0x510/0x750
    [  508.783468]    tcf_action_init+0x1e8/0x340
    [  508.787938]    tcf_action_add+0xc5/0x240
    [  508.792241]    tc_ctl_action+0x203/0x2a0
    [  508.796550]    rtnetlink_rcv_msg+0x5bd/0x7b0
    [  508.801200]    netlink_rcv_skb+0x184/0x220
    [  508.805674]    netlink_unicast+0x31b/0x460
    [  508.810129]    netlink_sendmsg+0x3fb/0x840
    [  508.814611]    sock_sendmsg+0x7b/0xd0
    [  508.818665]    ___sys_sendmsg+0x4c6/0x610
    [  508.823029]    __sys_sendmsg+0xd7/0x150
    [  508.827246]    do_syscall_64+0x7a/0x3f0
    [  508.831483]    entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
                   the dependencies between the lock to be acquired
    [  508.838945]  and SOFTIRQ-irq-unsafe lock:
    [  508.851177] -> (ife_mod_lock){++++} ops: 95 {
    [  508.855920]    HARDIRQ-ON-W at:
    [  508.859478]                     _raw_write_lock+0x2c/0x40
    [  508.865264]                     register_ife_op+0x118/0x2c0 [act_ife]
    [  508.872071]                     do_one_initcall+0xf7/0x4d9
    [  508.877947]                     do_init_module+0x18b/0x44e
    [  508.883819]                     load_module+0x4167/0x5730
    [  508.889595]                     __do_sys_finit_module+0x16d/0x1a0
    [  508.896043]                     do_syscall_64+0x7a/0x3f0
    [  508.901734]                     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  508.908827]    HARDIRQ-ON-R at:
    [  508.912359]                     _raw_read_lock+0x2f/0x40
    [  508.918043]                     find_ife_oplist+0x1e/0xc0 [act_ife]
    [  508.924692]                     tcf_ife_init+0x82f/0xf40 [act_ife]
    [  508.931252]                     tcf_action_init_1+0x510/0x750
    [  508.937393]                     tcf_action_init+0x1e8/0x340
    [  508.943366]                     tcf_action_add+0xc5/0x240
    [  508.949130]                     tc_ctl_action+0x203/0x2a0
    [  508.954922]                     rtnetlink_rcv_msg+0x5bd/0x7b0
    [  508.961024]                     netlink_rcv_skb+0x184/0x220
    [  508.966970]                     netlink_unicast+0x31b/0x460
    [  508.972915]                     netlink_sendmsg+0x3fb/0x840
    [  508.978859]                     sock_sendmsg+0x7b/0xd0
    [  508.984400]                     ___sys_sendmsg+0x4c6/0x610
    [  508.990264]                     __sys_sendmsg+0xd7/0x150
    [  508.995952]                     do_syscall_64+0x7a/0x3f0
    [  509.001643]                     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  509.008722]    SOFTIRQ-ON-W at:\
    [  509.012242]                     _raw_write_lock+0x2c/0x40
    [  509.018013]                     register_ife_op+0x118/0x2c0 [act_ife]
    [  509.024841]                     do_one_initcall+0xf7/0x4d9
    [  509.030720]                     do_init_module+0x18b/0x44e
    [  509.036604]                     load_module+0x4167/0x5730
    [  509.042397]                     __do_sys_finit_module+0x16d/0x1a0
    [  509.048865]                     do_syscall_64+0x7a/0x3f0
    [  509.054551]                     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  509.061636]    SOFTIRQ-ON-R at:
    [  509.065145]                     _raw_read_lock+0x2f/0x40
    [  509.070854]                     find_ife_oplist+0x1e/0xc0 [act_ife]
    [  509.077515]                     tcf_ife_init+0x82f/0xf40 [act_ife]
    [  509.084051]                     tcf_action_init_1+0x510/0x750
    [  509.090172]                     tcf_action_init+0x1e8/0x340
    [  509.096124]                     tcf_action_add+0xc5/0x240
    [  509.101891]                     tc_ctl_action+0x203/0x2a0
    [  509.107671]                     rtnetlink_rcv_msg+0x5bd/0x7b0
    [  509.113811]                     netlink_rcv_skb+0x184/0x220
    [  509.119768]                     netlink_unicast+0x31b/0x460
    [  509.125716]                     netlink_sendmsg+0x3fb/0x840
    [  509.131668]                     sock_sendmsg+0x7b/0xd0
    [  509.137167]                     ___sys_sendmsg+0x4c6/0x610
    [  509.143010]                     __sys_sendmsg+0xd7/0x150
    [  509.148718]                     do_syscall_64+0x7a/0x3f0
    [  509.154443]                     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  509.161533]    INITIAL USE at:
    [  509.164956]                    _raw_read_lock+0x2f/0x40
    [  509.170574]                    find_ife_oplist+0x1e/0xc0 [act_ife]
    [  509.177134]                    tcf_ife_init+0x82f/0xf40 [act_ife]
    [  509.183619]                    tcf_action_init_1+0x510/0x750
    [  509.189674]                    tcf_action_init+0x1e8/0x340
    [  509.195534]                    tcf_action_add+0xc5/0x240
    [  509.201229]                    tc_ctl_action+0x203/0x2a0
    [  509.206920]                    rtnetlink_rcv_msg+0x5bd/0x7b0
    [  509.212936]                    netlink_rcv_skb+0x184/0x220
    [  509.218818]                    netlink_unicast+0x31b/0x460
    [  509.224699]                    netlink_sendmsg+0x3fb/0x840
    [  509.230581]                    sock_sendmsg+0x7b/0xd0
    [  509.235984]                    ___sys_sendmsg+0x4c6/0x610
    [  509.241791]                    __sys_sendmsg+0xd7/0x150
    [  509.247425]                    do_syscall_64+0x7a/0x3f0
    [  509.253007]                    entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  509.259975]  }
    [  509.261998]  ... key      at: [<ffffffffc1554258>] ife_mod_lock+0x18/0xffffffffffff8dc0 [act_ife]
    [  509.271569]  ... acquired at:
    [  509.274912]    _raw_read_lock+0x2f/0x40
    [  509.279134]    find_ife_oplist+0x1e/0xc0 [act_ife]
    [  509.284324]    tcf_ife_init+0x82f/0xf40 [act_ife]
    [  509.289425]    tcf_action_init_1+0x510/0x750
    [  509.294068]    tcf_action_init+0x1e8/0x340
    [  509.298553]    tcf_action_add+0xc5/0x240
    [  509.302854]    tc_ctl_action+0x203/0x2a0
    [  509.307153]    rtnetlink_rcv_msg+0x5bd/0x7b0
    [  509.311805]    netlink_rcv_skb+0x184/0x220
    [  509.316282]    netlink_unicast+0x31b/0x460
    [  509.320769]    netlink_sendmsg+0x3fb/0x840
    [  509.325248]    sock_sendmsg+0x7b/0xd0
    [  509.329290]    ___sys_sendmsg+0x4c6/0x610
    [  509.333687]    __sys_sendmsg+0xd7/0x150
    [  509.337902]    do_syscall_64+0x7a/0x3f0
    [  509.342116]    entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  509.349601]
                   stack backtrace:
    [  509.354663] CPU: 6 PID: 5460 Comm: tc Not tainted 4.18.0-rc8+ #646
    [  509.361216] Hardware name: Supermicro SYS-2028TP-DECR/X10DRT-P, BIOS 2.0b 03/30/2017
    
    Fixes: ef6980b6becb ("introduce IFE action")
    Signed-off-by: Vlad Buslov <vladbu@mellanox.com>
    Acked-by: Jamal Hadi Salim <jhs@mojatatu.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b74d2807ae0cdb17ccc45d22260fc151a1b2d46b
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue May 1 00:55:49 2018 +1000

    powerpc/powernv: Remove OPALv1 support from opal console driver
    
    opal_put_chars deals with partial writes because in OPALv1,
    opal_console_write_buffer_space did not work correctly. That firmware
    is not supported.
    
    This reworks the opal_put_chars code to no longer deal with partial
    writes by turning them into full writes. Partial write handling is still
    supported in terms of what gets returned to the caller, but it may not
    go to the console atomically. A warning message is printed in this
    case.
    
    This allows console flushing to be moved out of the opal_write_lock
    spinlock. That could cause the lock to be held for long periods if the
    console is busy (especially if it was being spammed by firmware),
    which is dangerous because the lock is taken by xmon to debug the
    system. Flushing outside the lock improves the situation a bit.
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit cf25779a3649232b4b561493f589946e68716715
Author: Davide Caratti <dcaratti@redhat.com>
Date:   Tue Jun 19 15:39:46 2018 +0200

    net/sched: act_ife: fix recursive lock and idr leak
    
    [ Upstream commit 0a889b9404c084c6fd145020c939a8f688b3e058 ]
    
    a recursive lock warning [1] can be observed with the following script,
    
     # $TC actions add action ife encode allow prio pass index 42
     IFE type 0xED3E
     # $TC actions replace action ife encode allow tcindex pass index 42
    
    in case the kernel was unable to run the last command (e.g. because of
    the impossibility to load 'act_meta_skbtcindex'). For a similar reason,
    the kernel can leak idr in the error path of tcf_ife_init(), because
    tcf_idr_release() is not called after successful idr reservation:
    
     # $TC actions add action ife encode allow tcindex index 47
     IFE type 0xED3E
     RTNETLINK answers: No such file or directory
     We have an error talking to the kernel
     # $TC actions add action ife encode allow tcindex index 47
     IFE type 0xED3E
     RTNETLINK answers: No space left on device
     We have an error talking to the kernel
     # $TC actions add action ife encode use mark 7 type 0xfefe pass index 47
     IFE type 0xFEFE
     RTNETLINK answers: No space left on device
     We have an error talking to the kernel
    
    Since tcfa_lock is already taken when the action is being edited, a call
    to tcf_idr_release() wrongly makes tcf_idr_cleanup() take the same lock
    again. On the other hand, tcf_idr_release() needs to be called in the
    error path of tcf_ife_init(), to undo the last tcf_idr_create() invocation.
    Fix both problems in tcf_ife_init().
    Since the cleanup() routine can now be called when ife->params is NULL,
    also add a NULL pointer check to avoid calling kfree_rcu(NULL, rcu).
    
     [1]
     ============================================
     WARNING: possible recursive locking detected
     4.17.0-rc4.kasan+ #417 Tainted: G            E
     --------------------------------------------
     tc/3932 is trying to acquire lock:
     000000005097c9a6 (&(&p->tcfa_lock)->rlock){+...}, at: tcf_ife_cleanup+0x19/0x80 [act_ife]
    
     but task is already holding lock:
     000000005097c9a6 (&(&p->tcfa_lock)->rlock){+...}, at: tcf_ife_init+0xf6d/0x13c0 [act_ife]
    
     other info that might help us debug this:
      Possible unsafe locking scenario:
    
            CPU0
            ----
       lock(&(&p->tcfa_lock)->rlock);
       lock(&(&p->tcfa_lock)->rlock);
    
      *** DEADLOCK ***
    
      May be due to missing lock nesting notation
    
     2 locks held by tc/3932:
      #0: 000000007ca8e990 (rtnl_mutex){+.+.}, at: tcf_ife_init+0xf61/0x13c0 [act_ife]
      #1: 000000005097c9a6 (&(&p->tcfa_lock)->rlock){+...}, at: tcf_ife_init+0xf6d/0x13c0 [act_ife]
    
     stack backtrace:
     CPU: 3 PID: 3932 Comm: tc Tainted: G            E     4.17.0-rc4.kasan+ #417
     Hardware name: Red Hat KVM, BIOS 0.5.1 01/01/2011
     Call Trace:
      dump_stack+0x9a/0xeb
      __lock_acquire+0xf43/0x34a0
      ? debug_check_no_locks_freed+0x2b0/0x2b0
      ? debug_check_no_locks_freed+0x2b0/0x2b0
      ? debug_check_no_locks_freed+0x2b0/0x2b0
      ? __mutex_lock+0x62f/0x1240
      ? kvm_sched_clock_read+0x1a/0x30
      ? sched_clock+0x5/0x10
      ? sched_clock_cpu+0x18/0x170
      ? find_held_lock+0x39/0x1d0
      ? lock_acquire+0x10b/0x330
      lock_acquire+0x10b/0x330
      ? tcf_ife_cleanup+0x19/0x80 [act_ife]
      _raw_spin_lock_bh+0x38/0x70
      ? tcf_ife_cleanup+0x19/0x80 [act_ife]
      tcf_ife_cleanup+0x19/0x80 [act_ife]
      __tcf_idr_release+0xff/0x350
      tcf_ife_init+0xdde/0x13c0 [act_ife]
      ? ife_exit_net+0x290/0x290 [act_ife]
      ? __lock_is_held+0xb4/0x140
      tcf_action_init_1+0x67b/0xad0
      ? tcf_action_dump_old+0xa0/0xa0
      ? sched_clock+0x5/0x10
      ? sched_clock_cpu+0x18/0x170
      ? kvm_sched_clock_read+0x1a/0x30
      ? sched_clock+0x5/0x10
      ? sched_clock_cpu+0x18/0x170
      ? memset+0x1f/0x40
      tcf_action_init+0x30f/0x590
      ? tcf_action_init_1+0xad0/0xad0
      ? memset+0x1f/0x40
      tc_ctl_action+0x48e/0x5e0
      ? mutex_lock_io_nested+0x1160/0x1160
      ? tca_action_gd+0x990/0x990
      ? sched_clock+0x5/0x10
      ? find_held_lock+0x39/0x1d0
      rtnetlink_rcv_msg+0x4da/0x990
      ? validate_linkmsg+0x680/0x680
      ? sched_clock_cpu+0x18/0x170
      ? find_held_lock+0x39/0x1d0
      netlink_rcv_skb+0x127/0x350
      ? validate_linkmsg+0x680/0x680
      ? netlink_ack+0x970/0x970
      ? __kmalloc_node_track_caller+0x304/0x3a0
      netlink_unicast+0x40f/0x5d0
      ? netlink_attachskb+0x580/0x580
      ? _copy_from_iter_full+0x187/0x760
      ? import_iovec+0x90/0x390
      netlink_sendmsg+0x67f/0xb50
      ? netlink_unicast+0x5d0/0x5d0
      ? copy_msghdr_from_user+0x206/0x340
      ? netlink_unicast+0x5d0/0x5d0
      sock_sendmsg+0xb3/0xf0
      ___sys_sendmsg+0x60a/0x8b0
      ? copy_msghdr_from_user+0x340/0x340
      ? lock_downgrade+0x5e0/0x5e0
      ? tty_write_lock+0x18/0x50
      ? kvm_sched_clock_read+0x1a/0x30
      ? sched_clock+0x5/0x10
      ? sched_clock_cpu+0x18/0x170
      ? find_held_lock+0x39/0x1d0
      ? lock_downgrade+0x5e0/0x5e0
      ? lock_acquire+0x10b/0x330
      ? __audit_syscall_entry+0x316/0x690
      ? current_kernel_time64+0x6b/0xd0
      ? __fget_light+0x55/0x1f0
      ? __sys_sendmsg+0xd2/0x170
      __sys_sendmsg+0xd2/0x170
      ? __ia32_sys_shutdown+0x70/0x70
      ? syscall_trace_enter+0x57a/0xd60
      ? rcu_read_lock_sched_held+0xdc/0x110
      ? __bpf_trace_sys_enter+0x10/0x10
      ? do_syscall_64+0x22/0x480
      do_syscall_64+0xa5/0x480
      entry_SYSCALL_64_after_hwframe+0x49/0xbe
     RIP: 0033:0x7fd646988ba0
     RSP: 002b:00007fffc9fab3c8 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
     RAX: ffffffffffffffda RBX: 00007fffc9fab4f0 RCX: 00007fd646988ba0
     RDX: 0000000000000000 RSI: 00007fffc9fab440 RDI: 0000000000000003
     RBP: 000000005b28c8b3 R08: 0000000000000002 R09: 0000000000000000
     R10: 00007fffc9faae20 R11: 0000000000000246 R12: 0000000000000000
     R13: 00007fffc9fab504 R14: 0000000000000001 R15: 000000000066c100
    
    Fixes: 4e8c86155010 ("net sched: net sched: ife action fix late binding")
    Fixes: ef6980b6becb ("introduce IFE action")
    Signed-off-by: Davide Caratti <dcaratti@redhat.com>
    Acked-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 264d4f88ad5ba0d3c890a70a1216b4e87e5c26ec
Author: Andrea Parri <andrea.parri@amarulasolutions.com>
Date:   Thu Jun 7 12:01:57 2018 +0200

    doc: Update synchronize_rcu() definition in whatisRCU.txt
    
    The synchronize_rcu() definition based on RW-locks in whatisRCU.txt
    does not meet the "Memory-Barrier Guarantees" in Requirements.html;
    for example, the following SB-like test:
    
        P0:                      P1:
    
        WRITE_ONCE(x, 1);        WRITE_ONCE(y, 1);
        synchronize_rcu();       smp_mb();
        r0 = READ_ONCE(y);       r1 = READ_ONCE(x);
    
    should not be allowed to reach the state "r0 = 0 AND r1 = 0", but
    the current write_lock()+write_unlock() definition can not ensure
    this.  This commit therefore inserts an smp_mb__after_spinlock()
    in order to cause this synchronize_rcu() implementation to provide
    this memory-barrier guarantee.
    
    Suggested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit e9db4ef6bf4ca9894bb324c76e01b8f1a16b2650
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Sat Jun 30 06:17:47 2018 -0700

    bpf: sockhash fix omitted bucket lock in sock_close
    
    First the sk_callback_lock() was being used to protect both the
    sock callback hooks and the psock->maps list. This got overly
    convoluted after the addition of sockhash (in sockmap it made
    some sense because masp and callbacks were tightly coupled) so
    lets split out a specific lock for maps and only use the callback
    lock for its intended purpose. This fixes a couple cases where
    we missed using maps lock when it was in fact needed. Also this
    makes it easier to follow the code because now we can put the
    locking closer to the actual code its serializing.
    
    Next, in sock_hash_delete_elem() the pattern was as follows,
    
      sock_hash_delete_elem()
         [...]
         spin_lock(bucket_lock)
         l = lookup_elem_raw()
         if (l)
            hlist_del_rcu()
            write_lock(sk_callback_lock)
             .... destroy psock ...
            write_unlock(sk_callback_lock)
         spin_unlock(bucket_lock)
    
    The ordering is necessary because we only know the {p}sock after
    dereferencing the hash table which we can't do unless we have the
    bucket lock held. Once we have the bucket lock and the psock element
    it is deleted from the hashmap to ensure any other path doing a lookup
    will fail. Finally, the refcnt is decremented and if zero the psock
    is destroyed.
    
    In parallel with the above (or free'ing the map) a tcp close event
    may trigger tcp_close(). Which at the moment omits the bucket lock
    altogether (oops!) where the flow looks like this,
    
      bpf_tcp_close()
         [...]
         write_lock(sk_callback_lock)
         for each psock->maps // list of maps this sock is part of
             hlist_del_rcu(ref_hash_node);
             .... destroy psock ...
         write_unlock(sk_callback_lock)
    
    Obviously, and demonstrated by syzbot, this is broken because
    we can have multiple threads deleting entries via hlist_del_rcu().
    
    To fix this we might be tempted to wrap the hlist operation in a
    bucket lock but that would create a lock inversion problem. In
    summary to follow locking rules the psocks maps list needs the
    sk_callback_lock (after this patch maps_lock) but we need the bucket
    lock to do the hlist_del_rcu.
    
    To resolve the lock inversion problem pop the head of the maps list
    repeatedly and remove the reference until no more are left. If a
    delete happens in parallel from the BPF API that is OK as well because
    it will do a similar action, lookup the lock in the map/hash, delete
    it from the map/hash, and dec the refcnt. We check for this case
    before doing a destroy on the psock to ensure we don't have two
    threads tearing down a psock. The new logic is as follows,
    
      bpf_tcp_close()
      e = psock_map_pop(psock->maps) // done with map lock
      bucket_lock() // lock hash list bucket
      l = lookup_elem_raw(head, hash, key, key_size);
      if (l) {
         //only get here if elmnt was not already removed
         hlist_del_rcu()
         ... destroy psock...
      }
      bucket_unlock()
    
    And finally for all the above to work add missing locking around  map
    operations per above. Then add RCU annotations and use
    rcu_dereference/rcu_assign_pointer to manage values relying on RCU so
    that the object is not free'd from sock_hash_free() while it is being
    referenced in bpf_tcp_close().
    
    Reported-by: syzbot+0ce137753c78f7b6acc1@syzkaller.appspotmail.com
    Fixes: 81110384441a ("bpf: sockmap, add hash map support")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

commit 0a889b9404c084c6fd145020c939a8f688b3e058
Author: Davide Caratti <dcaratti@redhat.com>
Date:   Tue Jun 19 15:39:46 2018 +0200

    net/sched: act_ife: fix recursive lock and idr leak
    
    a recursive lock warning [1] can be observed with the following script,
    
     # $TC actions add action ife encode allow prio pass index 42
     IFE type 0xED3E
     # $TC actions replace action ife encode allow tcindex pass index 42
    
    in case the kernel was unable to run the last command (e.g. because of
    the impossibility to load 'act_meta_skbtcindex'). For a similar reason,
    the kernel can leak idr in the error path of tcf_ife_init(), because
    tcf_idr_release() is not called after successful idr reservation:
    
     # $TC actions add action ife encode allow tcindex index 47
     IFE type 0xED3E
     RTNETLINK answers: No such file or directory
     We have an error talking to the kernel
     # $TC actions add action ife encode allow tcindex index 47
     IFE type 0xED3E
     RTNETLINK answers: No space left on device
     We have an error talking to the kernel
     # $TC actions add action ife encode use mark 7 type 0xfefe pass index 47
     IFE type 0xFEFE
     RTNETLINK answers: No space left on device
     We have an error talking to the kernel
    
    Since tcfa_lock is already taken when the action is being edited, a call
    to tcf_idr_release() wrongly makes tcf_idr_cleanup() take the same lock
    again. On the other hand, tcf_idr_release() needs to be called in the
    error path of tcf_ife_init(), to undo the last tcf_idr_create() invocation.
    Fix both problems in tcf_ife_init().
    Since the cleanup() routine can now be called when ife->params is NULL,
    also add a NULL pointer check to avoid calling kfree_rcu(NULL, rcu).
    
     [1]
     ============================================
     WARNING: possible recursive locking detected
     4.17.0-rc4.kasan+ #417 Tainted: G            E
     --------------------------------------------
     tc/3932 is trying to acquire lock:
     000000005097c9a6 (&(&p->tcfa_lock)->rlock){+...}, at: tcf_ife_cleanup+0x19/0x80 [act_ife]
    
     but task is already holding lock:
     000000005097c9a6 (&(&p->tcfa_lock)->rlock){+...}, at: tcf_ife_init+0xf6d/0x13c0 [act_ife]
    
     other info that might help us debug this:
      Possible unsafe locking scenario:
    
            CPU0
            ----
       lock(&(&p->tcfa_lock)->rlock);
       lock(&(&p->tcfa_lock)->rlock);
    
      *** DEADLOCK ***
    
      May be due to missing lock nesting notation
    
     2 locks held by tc/3932:
      #0: 000000007ca8e990 (rtnl_mutex){+.+.}, at: tcf_ife_init+0xf61/0x13c0 [act_ife]
      #1: 000000005097c9a6 (&(&p->tcfa_lock)->rlock){+...}, at: tcf_ife_init+0xf6d/0x13c0 [act_ife]
    
     stack backtrace:
     CPU: 3 PID: 3932 Comm: tc Tainted: G            E     4.17.0-rc4.kasan+ #417
     Hardware name: Red Hat KVM, BIOS 0.5.1 01/01/2011
     Call Trace:
      dump_stack+0x9a/0xeb
      __lock_acquire+0xf43/0x34a0
      ? debug_check_no_locks_freed+0x2b0/0x2b0
      ? debug_check_no_locks_freed+0x2b0/0x2b0
      ? debug_check_no_locks_freed+0x2b0/0x2b0
      ? __mutex_lock+0x62f/0x1240
      ? kvm_sched_clock_read+0x1a/0x30
      ? sched_clock+0x5/0x10
      ? sched_clock_cpu+0x18/0x170
      ? find_held_lock+0x39/0x1d0
      ? lock_acquire+0x10b/0x330
      lock_acquire+0x10b/0x330
      ? tcf_ife_cleanup+0x19/0x80 [act_ife]
      _raw_spin_lock_bh+0x38/0x70
      ? tcf_ife_cleanup+0x19/0x80 [act_ife]
      tcf_ife_cleanup+0x19/0x80 [act_ife]
      __tcf_idr_release+0xff/0x350
      tcf_ife_init+0xdde/0x13c0 [act_ife]
      ? ife_exit_net+0x290/0x290 [act_ife]
      ? __lock_is_held+0xb4/0x140
      tcf_action_init_1+0x67b/0xad0
      ? tcf_action_dump_old+0xa0/0xa0
      ? sched_clock+0x5/0x10
      ? sched_clock_cpu+0x18/0x170
      ? kvm_sched_clock_read+0x1a/0x30
      ? sched_clock+0x5/0x10
      ? sched_clock_cpu+0x18/0x170
      ? memset+0x1f/0x40
      tcf_action_init+0x30f/0x590
      ? tcf_action_init_1+0xad0/0xad0
      ? memset+0x1f/0x40
      tc_ctl_action+0x48e/0x5e0
      ? mutex_lock_io_nested+0x1160/0x1160
      ? tca_action_gd+0x990/0x990
      ? sched_clock+0x5/0x10
      ? find_held_lock+0x39/0x1d0
      rtnetlink_rcv_msg+0x4da/0x990
      ? validate_linkmsg+0x680/0x680
      ? sched_clock_cpu+0x18/0x170
      ? find_held_lock+0x39/0x1d0
      netlink_rcv_skb+0x127/0x350
      ? validate_linkmsg+0x680/0x680
      ? netlink_ack+0x970/0x970
      ? __kmalloc_node_track_caller+0x304/0x3a0
      netlink_unicast+0x40f/0x5d0
      ? netlink_attachskb+0x580/0x580
      ? _copy_from_iter_full+0x187/0x760
      ? import_iovec+0x90/0x390
      netlink_sendmsg+0x67f/0xb50
      ? netlink_unicast+0x5d0/0x5d0
      ? copy_msghdr_from_user+0x206/0x340
      ? netlink_unicast+0x5d0/0x5d0
      sock_sendmsg+0xb3/0xf0
      ___sys_sendmsg+0x60a/0x8b0
      ? copy_msghdr_from_user+0x340/0x340
      ? lock_downgrade+0x5e0/0x5e0
      ? tty_write_lock+0x18/0x50
      ? kvm_sched_clock_read+0x1a/0x30
      ? sched_clock+0x5/0x10
      ? sched_clock_cpu+0x18/0x170
      ? find_held_lock+0x39/0x1d0
      ? lock_downgrade+0x5e0/0x5e0
      ? lock_acquire+0x10b/0x330
      ? __audit_syscall_entry+0x316/0x690
      ? current_kernel_time64+0x6b/0xd0
      ? __fget_light+0x55/0x1f0
      ? __sys_sendmsg+0xd2/0x170
      __sys_sendmsg+0xd2/0x170
      ? __ia32_sys_shutdown+0x70/0x70
      ? syscall_trace_enter+0x57a/0xd60
      ? rcu_read_lock_sched_held+0xdc/0x110
      ? __bpf_trace_sys_enter+0x10/0x10
      ? do_syscall_64+0x22/0x480
      do_syscall_64+0xa5/0x480
      entry_SYSCALL_64_after_hwframe+0x49/0xbe
     RIP: 0033:0x7fd646988ba0
     RSP: 002b:00007fffc9fab3c8 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
     RAX: ffffffffffffffda RBX: 00007fffc9fab4f0 RCX: 00007fd646988ba0
     RDX: 0000000000000000 RSI: 00007fffc9fab440 RDI: 0000000000000003
     RBP: 000000005b28c8b3 R08: 0000000000000002 R09: 0000000000000000
     R10: 00007fffc9faae20 R11: 0000000000000246 R12: 0000000000000000
     R13: 00007fffc9fab504 R14: 0000000000000001 R15: 000000000066c100
    
    Fixes: 4e8c86155010 ("net sched: net sched: ife action fix late binding")
    Fixes: ef6980b6becb ("introduce IFE action")
    Signed-off-by: Davide Caratti <dcaratti@redhat.com>
    Acked-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 704996566f97e0e24c97052f81678060c213c260
Merge: e3a44fd7e633 23d0b79dfaed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 14:29:13 2018 -0700

    Merge tag 'for-4.18-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux
    
    Pull btrfs updates from David Sterba:
     "User visible features:
    
       - added support for the ioctl FS_IOC_FSGETXATTR, per-inode flags,
         successor of GET/SETFLAGS; now supports only existing flags:
         append, immutable, noatime, nodump, sync
    
       - 3 new unprivileged ioctls to allow users to enumerate subvolumes
    
       - dedupe syscall implementation does not restrict the range to 16MiB,
         though it still splits the whole range to 16MiB chunks
    
       - on user demand, rmdir() is able to delete an empty subvolume,
         export the capability in sysfs
    
       - fix inode number types in tracepoints, other cleanups
    
       - send: improved speed when dealing with a large removed directory,
         measurements show decrease from 2000 minutes to 2 minutes on a
         directory with 2 million entries
    
       - pre-commit check of superblock to detect a mysterious in-memory
         corruption
    
       - log message updates
    
      Other changes:
    
       - orphan inode cleanup improved, does no keep long-standing
         reservations that could lead up to early ENOSPC in some cases
    
       - slight improvement of handling snapshotted NOCOW files by avoiding
         some unnecessary tree searches
    
       - avoid OOM when dealing with many unmergeable small extents at flush
         time
    
       - speedup conversion of free space tree representations from/to
         bitmap/tree
    
       - code refactoring, deletion, cleanups:
          + delayed refs
          + delayed iput
          + redundant argument removals
          + memory barrier cleanups
          + remove a redundant mutex supposedly excluding several ioctls to
            run in parallel
    
       - new tracepoints for blockgroup manipulation
    
       - more sanity checks of compressed headers"
    
    * tag 'for-4.18-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/kdave/linux: (183 commits)
      btrfs: Add unprivileged version of ino_lookup ioctl
      btrfs: Add unprivileged ioctl which returns subvolume's ROOT_REF
      btrfs: Add unprivileged ioctl which returns subvolume information
      Btrfs: clean up error handling in btrfs_truncate()
      btrfs: Factor out write portion of btrfs_get_blocks_direct
      btrfs: Factor out read portion of btrfs_get_blocks_direct
      btrfs: return ENOMEM if path allocation fails in btrfs_cross_ref_exist
      btrfs: raid56: Remove VLA usage
      btrfs: return error value if create_io_em failed in cow_file_range
      btrfs: drop useless member qgroup_reserved of btrfs_pending_snapshot
      btrfs: drop unused parameter qgroup_reserved
      btrfs: balance dirty metadata pages in btrfs_finish_ordered_io
      btrfs: lift some btrfs_cross_ref_exist checks in nocow path
      btrfs: Remove fs_info argument from btrfs_uuid_tree_rem
      btrfs: Remove fs_info argument from btrfs_uuid_tree_add
      Btrfs: remove unused check of skip_locking
      Btrfs: remove always true check in unlock_up
      Btrfs: grab write lock directly if write_lock_level is the max level
      Btrfs: move get root out of btrfs_search_slot to a helper
      Btrfs: use more straightforward extent_buffer_uptodate check
      ...

commit 662c653bfda58698cf48d7143a39bd3a063fd9c6
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Fri May 18 11:00:23 2018 +0800

    Btrfs: grab write lock directly if write_lock_level is the max level
    
    Typically, when acquiring root node's lock, btrfs tries its best to get
    read lock and trade for write lock if @write_lock_level implies to do so.
    
    In case of (cow && (p->keep_locks || p->lowest_level)), write_lock_level
    is set to BTRFS_MAX_LEVEL, which means we need to acquire root node's
    write lock directly.
    
    In this particular case, the dance of acquiring read lock and then trading
    for write lock can be saved.
    
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 808e6f9dd955057615eed887f5cd1d6a9bff2eb2
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Jan 27 17:29:49 2018 +0800

    f2fs: fix to check extent cache in f2fs_drop_extent_tree
    
    [ Upstream commit bf617f7a92edc6bb2909db2bfa4576f50b280ee5 ]
    
    If noextent_cache mount option is on, we will never initialize extent tree
    in inode, but still we're going to access it in f2fs_drop_extent_tree,
    result in kernel panic as below:
    
     BUG: unable to handle kernel NULL pointer dereference at 0000000000000038
     IP: _raw_write_lock+0xc/0x30
     Call Trace:
      ? f2fs_drop_extent_tree+0x41/0x70 [f2fs]
      f2fs_fallocate+0x5a0/0xdd0 [f2fs]
      ? common_file_perm+0x47/0xc0
      ? apparmor_file_permission+0x1a/0x20
      vfs_fallocate+0x15b/0x290
      SyS_fallocate+0x44/0x70
      do_syscall_64+0x6e/0x160
      entry_SYSCALL64_slow_path+0x25/0x25
    
    This patch fixes to check extent cache status before using in
    f2fs_drop_extent_tree.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 82e93a83598be971a1d426263183cea7c853e630
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Jan 27 17:29:49 2018 +0800

    f2fs: fix to check extent cache in f2fs_drop_extent_tree
    
    [ Upstream commit bf617f7a92edc6bb2909db2bfa4576f50b280ee5 ]
    
    If noextent_cache mount option is on, we will never initialize extent tree
    in inode, but still we're going to access it in f2fs_drop_extent_tree,
    result in kernel panic as below:
    
     BUG: unable to handle kernel NULL pointer dereference at 0000000000000038
     IP: _raw_write_lock+0xc/0x30
     Call Trace:
      ? f2fs_drop_extent_tree+0x41/0x70 [f2fs]
      f2fs_fallocate+0x5a0/0xdd0 [f2fs]
      ? common_file_perm+0x47/0xc0
      ? apparmor_file_permission+0x1a/0x20
      vfs_fallocate+0x15b/0x290
      SyS_fallocate+0x44/0x70
      do_syscall_64+0x6e/0x160
      entry_SYSCALL64_slow_path+0x25/0x25
    
    This patch fixes to check extent cache status before using in
    f2fs_drop_extent_tree.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d3ab3aa8adcab55141696049936d247b7a62b40f
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Jan 27 17:29:49 2018 +0800

    f2fs: fix to check extent cache in f2fs_drop_extent_tree
    
    [ Upstream commit bf617f7a92edc6bb2909db2bfa4576f50b280ee5 ]
    
    If noextent_cache mount option is on, we will never initialize extent tree
    in inode, but still we're going to access it in f2fs_drop_extent_tree,
    result in kernel panic as below:
    
     BUG: unable to handle kernel NULL pointer dereference at 0000000000000038
     IP: _raw_write_lock+0xc/0x30
     Call Trace:
      ? f2fs_drop_extent_tree+0x41/0x70 [f2fs]
      f2fs_fallocate+0x5a0/0xdd0 [f2fs]
      ? common_file_perm+0x47/0xc0
      ? apparmor_file_permission+0x1a/0x20
      vfs_fallocate+0x15b/0x290
      SyS_fallocate+0x44/0x70
      do_syscall_64+0x6e/0x160
      entry_SYSCALL64_slow_path+0x25/0x25
    
    This patch fixes to check extent cache status before using in
    f2fs_drop_extent_tree.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2f2f95d96b8e59e29701621f94354325479cd91e
Author: Michael Bringmann <mwb@linux.vnet.ibm.com>
Date:   Tue Nov 28 16:58:40 2017 -0600

    powerpc/numa: Ensure nodes initialized for hotplug
    
    [ Upstream commit ea05ba7c559c8e5a5946c3a94a2a266e9a6680a6 ]
    
    This patch fixes some problems encountered at runtime with
    configurations that support memory-less nodes, or that hot-add CPUs
    into nodes that are memoryless during system execution after boot. The
    problems of interest include:
    
    * Nodes known to powerpc to be memoryless at boot, but to have CPUs in
      them are allowed to be 'possible' and 'online'. Memory allocations
      for those nodes are taken from another node that does have memory
      until and if memory is hot-added to the node.
    
    * Nodes which have no resources assigned at boot, but which may still
      be referenced subsequently by affinity or associativity attributes,
      are kept in the list of 'possible' nodes for powerpc. Hot-add of
      memory or CPUs to the system can reference these nodes and bring
      them online instead of redirecting the references to one of the set
      of nodes known to have memory at boot.
    
    Note that this software operates under the context of CPU hotplug. We
    are not doing memory hotplug in this code, but rather updating the
    kernel's CPU topology (i.e. arch_update_cpu_topology /
    numa_update_cpu_topology). We are initializing a node that may be used
    by CPUs or memory before it can be referenced as invalid by a CPU
    hotplug operation. CPU hotplug operations are protected by a range of
    APIs including cpu_maps_update_begin/cpu_maps_update_done,
    cpus_read/write_lock / cpus_read/write_unlock, device locks, and more.
    Memory hotplug operations, including try_online_node, are protected by
    mem_hotplug_begin/mem_hotplug_done, device locks, and more. In the
    case of CPUs being hot-added to a previously memoryless node, the
    try_online_node operation occurs wholly within the CPU locks with no
    overlap. Using HMC hot-add/hot-remove operations, we have been able to
    add and remove CPUs to any possible node without failures. HMC
    operations involve a degree self-serialization, though.
    
    Signed-off-by: Michael Bringmann <mwb@linux.vnet.ibm.com>
    Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b4e84e5aed7680dedd44727bcf8ed7fdbd409b86
Author: Michael Bringmann <mwb@linux.vnet.ibm.com>
Date:   Tue Nov 28 16:58:40 2017 -0600

    powerpc/numa: Ensure nodes initialized for hotplug
    
    [ Upstream commit ea05ba7c559c8e5a5946c3a94a2a266e9a6680a6 ]
    
    This patch fixes some problems encountered at runtime with
    configurations that support memory-less nodes, or that hot-add CPUs
    into nodes that are memoryless during system execution after boot. The
    problems of interest include:
    
    * Nodes known to powerpc to be memoryless at boot, but to have CPUs in
      them are allowed to be 'possible' and 'online'. Memory allocations
      for those nodes are taken from another node that does have memory
      until and if memory is hot-added to the node.
    
    * Nodes which have no resources assigned at boot, but which may still
      be referenced subsequently by affinity or associativity attributes,
      are kept in the list of 'possible' nodes for powerpc. Hot-add of
      memory or CPUs to the system can reference these nodes and bring
      them online instead of redirecting the references to one of the set
      of nodes known to have memory at boot.
    
    Note that this software operates under the context of CPU hotplug. We
    are not doing memory hotplug in this code, but rather updating the
    kernel's CPU topology (i.e. arch_update_cpu_topology /
    numa_update_cpu_topology). We are initializing a node that may be used
    by CPUs or memory before it can be referenced as invalid by a CPU
    hotplug operation. CPU hotplug operations are protected by a range of
    APIs including cpu_maps_update_begin/cpu_maps_update_done,
    cpus_read/write_lock / cpus_read/write_unlock, device locks, and more.
    Memory hotplug operations, including try_online_node, are protected by
    mem_hotplug_begin/mem_hotplug_done, device locks, and more. In the
    case of CPUs being hot-added to a previously memoryless node, the
    try_online_node operation occurs wholly within the CPU locks with no
    overlap. Using HMC hot-add/hot-remove operations, we have been able to
    add and remove CPUs to any possible node without failures. HMC
    operations involve a degree self-serialization, though.
    
    Signed-off-by: Michael Bringmann <mwb@linux.vnet.ibm.com>
    Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b545993694a6e3bf6e801071df00e0ad47526bb8
Author: Qu Wenruo <wqu@suse.com>
Date:   Tue Apr 24 13:03:13 2018 +0800

    btrfs: print-tree: Add eb locking status output for debug build
    
    It's pretty handy if we can get the debug output for locking status of
    an extent buffer, specially for race condition related debugging.
    
    So add the following output for btrfs_print_tree() and
    btrfs_print_leaf():
    - refs
    - write_locks (as w:%d)
    - read_locks (as r:%d)
    - blocking_writers (as bw:%d)
    - blocking_readers (as br:%d)
    - spinning_writers (as sw:%d)
    - spinning_readers (as sr:%d)
    - lock_owner
    - current->pid
    
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ update comment ]
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 1f530364e1d9aa54efbe0d83413b59321076957f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Feb 23 23:33:07 2018 +0100

    crypto: ccp - don't disable interrupts while setting up debugfs
    
    [ Upstream commit 79eb382b5e06a6dca5806465d7195d686a463ab0 ]
    
    I don't why we need take a single write lock and disable interrupts
    while setting up debugfs. This is what what happens when we try anyway:
    
    |ccp 0000:03:00.2: enabling device (0000 -> 0002)
    |BUG: sleeping function called from invalid context at kernel/locking/rwsem.c:69
    |in_atomic(): 1, irqs_disabled(): 1, pid: 3, name: kworker/0:0
    |irq event stamp: 17150
    |hardirqs last  enabled at (17149): [<0000000097a18c49>] restore_regs_and_return_to_kernel+0x0/0x23
    |hardirqs last disabled at (17150): [<000000000773b3a9>] _raw_write_lock_irqsave+0x1b/0x50
    |softirqs last  enabled at (17148): [<0000000064d56155>] __do_softirq+0x3b8/0x4c1
    |softirqs last disabled at (17125): [<0000000092633c18>] irq_exit+0xb1/0xc0
    |CPU: 0 PID: 3 Comm: kworker/0:0 Not tainted 4.16.0-rc2+ #30
    |Workqueue: events work_for_cpu_fn
    |Call Trace:
    | dump_stack+0x7d/0xb6
    | ___might_sleep+0x1eb/0x250
    | down_write+0x17/0x60
    | start_creating+0x4c/0xe0
    | debugfs_create_dir+0x9/0x100
    | ccp5_debugfs_setup+0x191/0x1b0
    | ccp5_init+0x8a7/0x8c0
    | ccp_dev_init+0xb8/0xe0
    | sp_init+0x6c/0x90
    | sp_pci_probe+0x26e/0x590
    | local_pci_probe+0x3f/0x90
    | work_for_cpu_fn+0x11/0x20
    | process_one_work+0x1ff/0x650
    | worker_thread+0x1d4/0x3a0
    | kthread+0xfe/0x130
    | ret_from_fork+0x27/0x50
    
    If any locking is required, a simple mutex will do it.
    
    Cc: Gary R Hook <gary.hook@amd.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Gary R Hook <gary.hook@amd.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 41b6f0ffb3ac1d486bcd2545f0b1d25189d78695
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Feb 23 23:33:07 2018 +0100

    crypto: ccp - don't disable interrupts while setting up debugfs
    
    [ Upstream commit 79eb382b5e06a6dca5806465d7195d686a463ab0 ]
    
    I don't why we need take a single write lock and disable interrupts
    while setting up debugfs. This is what what happens when we try anyway:
    
    |ccp 0000:03:00.2: enabling device (0000 -> 0002)
    |BUG: sleeping function called from invalid context at kernel/locking/rwsem.c:69
    |in_atomic(): 1, irqs_disabled(): 1, pid: 3, name: kworker/0:0
    |irq event stamp: 17150
    |hardirqs last  enabled at (17149): [<0000000097a18c49>] restore_regs_and_return_to_kernel+0x0/0x23
    |hardirqs last disabled at (17150): [<000000000773b3a9>] _raw_write_lock_irqsave+0x1b/0x50
    |softirqs last  enabled at (17148): [<0000000064d56155>] __do_softirq+0x3b8/0x4c1
    |softirqs last disabled at (17125): [<0000000092633c18>] irq_exit+0xb1/0xc0
    |CPU: 0 PID: 3 Comm: kworker/0:0 Not tainted 4.16.0-rc2+ #30
    |Workqueue: events work_for_cpu_fn
    |Call Trace:
    | dump_stack+0x7d/0xb6
    | ___might_sleep+0x1eb/0x250
    | down_write+0x17/0x60
    | start_creating+0x4c/0xe0
    | debugfs_create_dir+0x9/0x100
    | ccp5_debugfs_setup+0x191/0x1b0
    | ccp5_init+0x8a7/0x8c0
    | ccp_dev_init+0xb8/0xe0
    | sp_init+0x6c/0x90
    | sp_pci_probe+0x26e/0x590
    | local_pci_probe+0x3f/0x90
    | work_for_cpu_fn+0x11/0x20
    | process_one_work+0x1ff/0x650
    | worker_thread+0x1d4/0x3a0
    | kthread+0xfe/0x130
    | ret_from_fork+0x27/0x50
    
    If any locking is required, a simple mutex will do it.
    
    Cc: Gary R Hook <gary.hook@amd.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Gary R Hook <gary.hook@amd.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0759c1c2f140ec56a9fec96dddf9e5beb89f3b48
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Wed Feb 1 20:49:35 2017 -0500

    jbd2: fix use after free in kjournald2()
    
    [ Upstream commit dbfcef6b0f4012c57bc0b6e0e660d5ed12a5eaed ]
    
    Below is the synchronization issue between unmount and kjournald2
    contexts, which results into use after free issue in kjournald2().
    Fix this issue by using journal->j_state_lock to synchronize the
    wait_event() done in journal_kill_thread() and the wake_up() done
    in kjournald2().
    
    TASK 1:
    umount cmd:
       |--jbd2_journal_destroy() {
           |--journal_kill_thread() {
                write_lock(&journal->j_state_lock);
                journal->j_flags |= JBD2_UNMOUNT;
                ...
                write_unlock(&journal->j_state_lock);
                wake_up(&journal->j_wait_commit);      TASK 2 wakes up here:
                                                       kjournald2() {
                                                         ...
                                                         checks JBD2_UNMOUNT flag and calls goto end-loop;
                                                         ...
                                                         end_loop:
                                                           write_unlock(&journal->j_state_lock);
                                                           journal->j_task = NULL; --> If this thread gets
                                                           pre-empted here, then TASK 1 wait_event will
                                                           exit even before this thread is completely
                                                           done.
                wait_event(journal->j_wait_done_commit, journal->j_task == NULL);
                ...
                write_lock(&journal->j_state_lock);
                write_unlock(&journal->j_state_lock);
              }
           |--kfree(journal);
         }
    }
                                                           wake_up(&journal->j_wait_done_commit); --> this step
                                                           now results into use after free issue.
                                                       }
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>

commit 7f7b79ed69748b1f9c9822b0d067180d845f90bd
Author: Jia-Ju Bai <baijiaju1990@163.com>
Date:   Thu Jun 1 16:18:10 2017 +0800

    qlcnic: Fix a sleep-in-atomic bug in qlcnic_82xx_hw_write_wx_2M and qlcnic_82xx_hw_read_wx_2M
    
    [ Upstream commit 5ea6d691aac6c93b790f0905e3460d44cc4c449b ]
    
    The driver may sleep under a write spin lock, and the function
    call path is:
    qlcnic_82xx_hw_write_wx_2M (acquire the lock by write_lock_irqsave)
      crb_win_lock
        qlcnic_pcie_sem_lock
          usleep_range
    qlcnic_82xx_hw_read_wx_2M (acquire the lock by write_lock_irqsave)
      crb_win_lock
        qlcnic_pcie_sem_lock
          usleep_range
    
    To fix it, the usleep_range is replaced with udelay.
    
    Signed-off-by: Jia-Ju Bai <baijiaju1990@163.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>

commit 7a107c0f55a3b4c6f84a4323df5610360bde1684
Author: Kirill Tkhai <ktkhai@virtuozzo.com>
Date:   Thu Apr 5 14:58:06 2018 +0300

    fasync: Fix deadlock between task-context and interrupt-context kill_fasync()
    
    I observed the following deadlock between them:
    
    [task 1]                          [task 2]                         [task 3]
    kill_fasync()                     mm_update_next_owner()           copy_process()
     spin_lock_irqsave(&fa->fa_lock)   read_lock(&tasklist_lock)        write_lock_irq(&tasklist_lock)
      send_sigio()                    <IRQ>                             ...
       read_lock(&fown->lock)         kill_fasync()                     ...
        read_lock(&tasklist_lock)      spin_lock_irqsave(&fa->fa_lock)  ...
    
    Task 1 can't acquire read locked tasklist_lock, since there is
    already task 3 expressed its wish to take the lock exclusive.
    Task 2 holds the read locked lock, but it can't take the spin lock.
    
    Also, there is possible another deadlock (which I haven't observed):
    
    [task 1]                            [task 2]
    f_getown()                          kill_fasync()
     read_lock(&f_own->lock)             spin_lock_irqsave(&fa->fa_lock,)
     <IRQ>                               send_sigio()                     write_lock_irq(&f_own->lock)
      kill_fasync()                       read_lock(&fown->lock)
       spin_lock_irqsave(&fa->fa_lock,)
    
    Actually, we do not need exclusive fa->fa_lock in kill_fasync_rcu(),
    as it guarantees fa->fa_file->f_owner integrity only. It may seem,
    that it used to give a task a small possibility to receive two sequential
    signals, if there are two parallel kill_fasync() callers, and task
    handles the first signal fastly, but the behaviour won't become
    different, since there is exclusive sighand lock in do_send_sig_info().
    
    The patch converts fa_lock into rwlock_t, and this fixes two above
    deadlocks, as rwlock is allowed to be taken from interrupt handler
    by qrwlock design.
    
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Signed-off-by: Jeff Layton <jlayton@redhat.com>

commit cff3a5f282ff1813352a146c487f4d8b472c250e
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Wed Feb 1 20:49:35 2017 -0500

    jbd2: fix use after free in kjournald2()
    
    commit dbfcef6b0f4012c57bc0b6e0e660d5ed12a5eaed upstream.
    
    Below is the synchronization issue between unmount and kjournald2
    contexts, which results into use after free issue in kjournald2().
    Fix this issue by using journal->j_state_lock to synchronize the
    wait_event() done in journal_kill_thread() and the wake_up() done
    in kjournald2().
    
    TASK 1:
    umount cmd:
       |--jbd2_journal_destroy() {
           |--journal_kill_thread() {
                write_lock(&journal->j_state_lock);
                journal->j_flags |= JBD2_UNMOUNT;
                ...
                write_unlock(&journal->j_state_lock);
                wake_up(&journal->j_wait_commit);      TASK 2 wakes up here:
                                                       kjournald2() {
                                                         ...
                                                         checks JBD2_UNMOUNT flag and calls goto end-loop;
                                                         ...
                                                         end_loop:
                                                           write_unlock(&journal->j_state_lock);
                                                           journal->j_task = NULL; --> If this thread gets
                                                           pre-empted here, then TASK 1 wait_event will
                                                           exit even before this thread is completely
                                                           done.
                wait_event(journal->j_wait_done_commit, journal->j_task == NULL);
                ...
                write_lock(&journal->j_state_lock);
                write_unlock(&journal->j_state_lock);
              }
           |--kfree(journal);
         }
    }
                                                           wake_up(&journal->j_wait_done_commit); --> this step
                                                           now results into use after free issue.
                                                       }
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Cc: Amit Pundir <amit.pundir@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 87dfe99e129679c23bebda36b83cb223285afc2a
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Wed Feb 1 20:49:35 2017 -0500

    jbd2: fix use after free in kjournald2()
    
    commit dbfcef6b0f4012c57bc0b6e0e660d5ed12a5eaed upstream.
    
    Below is the synchronization issue between unmount and kjournald2
    contexts, which results into use after free issue in kjournald2().
    Fix this issue by using journal->j_state_lock to synchronize the
    wait_event() done in journal_kill_thread() and the wake_up() done
    in kjournald2().
    
    TASK 1:
    umount cmd:
       |--jbd2_journal_destroy() {
           |--journal_kill_thread() {
                write_lock(&journal->j_state_lock);
                journal->j_flags |= JBD2_UNMOUNT;
                ...
                write_unlock(&journal->j_state_lock);
                wake_up(&journal->j_wait_commit);      TASK 2 wakes up here:
                                                       kjournald2() {
                                                         ...
                                                         checks JBD2_UNMOUNT flag and calls goto end-loop;
                                                         ...
                                                         end_loop:
                                                           write_unlock(&journal->j_state_lock);
                                                           journal->j_task = NULL; --> If this thread gets
                                                           pre-empted here, then TASK 1 wait_event will
                                                           exit even before this thread is completely
                                                           done.
                wait_event(journal->j_wait_done_commit, journal->j_task == NULL);
                ...
                write_lock(&journal->j_state_lock);
                write_unlock(&journal->j_state_lock);
              }
           |--kfree(journal);
         }
    }
                                                           wake_up(&journal->j_wait_done_commit); --> this step
                                                           now results into use after free issue.
                                                       }
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Cc: Amit Pundir <amit.pundir@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f47bd1b5605e2a5cb89853e3fc595b7d84f4db4c
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Wed Feb 1 20:49:35 2017 -0500

    jbd2: fix use after free in kjournald2()
    
    commit dbfcef6b0f4012c57bc0b6e0e660d5ed12a5eaed upstream.
    
    Below is the synchronization issue between unmount and kjournald2
    contexts, which results into use after free issue in kjournald2().
    Fix this issue by using journal->j_state_lock to synchronize the
    wait_event() done in journal_kill_thread() and the wake_up() done
    in kjournald2().
    
    TASK 1:
    umount cmd:
       |--jbd2_journal_destroy() {
           |--journal_kill_thread() {
                write_lock(&journal->j_state_lock);
                journal->j_flags |= JBD2_UNMOUNT;
                ...
                write_unlock(&journal->j_state_lock);
                wake_up(&journal->j_wait_commit);      TASK 2 wakes up here:
                                                       kjournald2() {
                                                         ...
                                                         checks JBD2_UNMOUNT flag and calls goto end-loop;
                                                         ...
                                                         end_loop:
                                                           write_unlock(&journal->j_state_lock);
                                                           journal->j_task = NULL; --> If this thread gets
                                                           pre-empted here, then TASK 1 wait_event will
                                                           exit even before this thread is completely
                                                           done.
                wait_event(journal->j_wait_done_commit, journal->j_task == NULL);
                ...
                write_lock(&journal->j_state_lock);
                write_unlock(&journal->j_state_lock);
              }
           |--kfree(journal);
         }
    }
                                                           wake_up(&journal->j_wait_done_commit); --> this step
                                                           now results into use after free issue.
                                                       }
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Cc: Amit Pundir <amit.pundir@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0bddd43ac2001d87471117ff29e789aa3bcfd18b
Author: Michael Bringmann <mwb@linux.vnet.ibm.com>
Date:   Tue Nov 28 16:58:40 2017 -0600

    powerpc/numa: Ensure nodes initialized for hotplug
    
    
    [ Upstream commit ea05ba7c559c8e5a5946c3a94a2a266e9a6680a6 ]
    
    This patch fixes some problems encountered at runtime with
    configurations that support memory-less nodes, or that hot-add CPUs
    into nodes that are memoryless during system execution after boot. The
    problems of interest include:
    
    * Nodes known to powerpc to be memoryless at boot, but to have CPUs in
      them are allowed to be 'possible' and 'online'. Memory allocations
      for those nodes are taken from another node that does have memory
      until and if memory is hot-added to the node.
    
    * Nodes which have no resources assigned at boot, but which may still
      be referenced subsequently by affinity or associativity attributes,
      are kept in the list of 'possible' nodes for powerpc. Hot-add of
      memory or CPUs to the system can reference these nodes and bring
      them online instead of redirecting the references to one of the set
      of nodes known to have memory at boot.
    
    Note that this software operates under the context of CPU hotplug. We
    are not doing memory hotplug in this code, but rather updating the
    kernel's CPU topology (i.e. arch_update_cpu_topology /
    numa_update_cpu_topology). We are initializing a node that may be used
    by CPUs or memory before it can be referenced as invalid by a CPU
    hotplug operation. CPU hotplug operations are protected by a range of
    APIs including cpu_maps_update_begin/cpu_maps_update_done,
    cpus_read/write_lock / cpus_read/write_unlock, device locks, and more.
    Memory hotplug operations, including try_online_node, are protected by
    mem_hotplug_begin/mem_hotplug_done, device locks, and more. In the
    case of CPUs being hot-added to a previously memoryless node, the
    try_online_node operation occurs wholly within the CPU locks with no
    overlap. Using HMC hot-add/hot-remove operations, we have been able to
    add and remove CPUs to any possible node without failures. HMC
    operations involve a degree self-serialization, though.
    
    Signed-off-by: Michael Bringmann <mwb@linux.vnet.ibm.com>
    Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4bd1ca48c141577f836860b6634ed0c398af25ce
Author: Jia-Ju Bai <baijiaju1990@163.com>
Date:   Thu Jun 1 16:18:10 2017 +0800

    qlcnic: Fix a sleep-in-atomic bug in qlcnic_82xx_hw_write_wx_2M and qlcnic_82xx_hw_read_wx_2M
    
    
    [ Upstream commit 5ea6d691aac6c93b790f0905e3460d44cc4c449b ]
    
    The driver may sleep under a write spin lock, and the function
    call path is:
    qlcnic_82xx_hw_write_wx_2M (acquire the lock by write_lock_irqsave)
      crb_win_lock
        qlcnic_pcie_sem_lock
          usleep_range
    qlcnic_82xx_hw_read_wx_2M (acquire the lock by write_lock_irqsave)
      crb_win_lock
        qlcnic_pcie_sem_lock
          usleep_range
    
    To fix it, the usleep_range is replaced with udelay.
    
    Signed-off-by: Jia-Ju Bai <baijiaju1990@163.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 819bb2c00f329088a5f9291f48aeda740b3dcec9
Author: Jia-Ju Bai <baijiaju1990@163.com>
Date:   Thu Jun 1 16:18:10 2017 +0800

    qlcnic: Fix a sleep-in-atomic bug in qlcnic_82xx_hw_write_wx_2M and qlcnic_82xx_hw_read_wx_2M
    
    
    [ Upstream commit 5ea6d691aac6c93b790f0905e3460d44cc4c449b ]
    
    The driver may sleep under a write spin lock, and the function
    call path is:
    qlcnic_82xx_hw_write_wx_2M (acquire the lock by write_lock_irqsave)
      crb_win_lock
        qlcnic_pcie_sem_lock
          usleep_range
    qlcnic_82xx_hw_read_wx_2M (acquire the lock by write_lock_irqsave)
      crb_win_lock
        qlcnic_pcie_sem_lock
          usleep_range
    
    To fix it, the usleep_range is replaced with udelay.
    
    Signed-off-by: Jia-Ju Bai <baijiaju1990@163.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d78b198c02c4276ad93af106f7f38904bd45d6cb
Author: Jia-Ju Bai <baijiaju1990@163.com>
Date:   Thu Jun 1 16:18:10 2017 +0800

    qlcnic: Fix a sleep-in-atomic bug in qlcnic_82xx_hw_write_wx_2M and qlcnic_82xx_hw_read_wx_2M
    
    
    [ Upstream commit 5ea6d691aac6c93b790f0905e3460d44cc4c449b ]
    
    The driver may sleep under a write spin lock, and the function
    call path is:
    qlcnic_82xx_hw_write_wx_2M (acquire the lock by write_lock_irqsave)
      crb_win_lock
        qlcnic_pcie_sem_lock
          usleep_range
    qlcnic_82xx_hw_read_wx_2M (acquire the lock by write_lock_irqsave)
      crb_win_lock
        qlcnic_pcie_sem_lock
          usleep_range
    
    To fix it, the usleep_range is replaced with udelay.
    
    Signed-off-by: Jia-Ju Bai <baijiaju1990@163.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e13d781171fb7d2a4b424af4d81fec49f203e551
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Mar 23 07:56:58 2018 -0700

    ipv6: fix possible deadlock in rt6_age_examine_exception()
    
    commit 1bfa26ff8c4b7512f4e4efa6df211239223033d4 upstream.
    
    syzbot reported a LOCKDEP splat [1] in rt6_age_examine_exception()
    
    rt6_age_examine_exception() is called while rt6_exception_lock is held.
    This lock is the lower one in the lock hierarchy, thus we can not
    call dst_neigh_lookup() function, as it can fallback to neigh_create()
    
    We should instead do a pure RCU lookup. As a bonus we avoid
    a pair of atomic operations on neigh refcount.
    
    [1]
    
    WARNING: possible circular locking dependency detected
    4.16.0-rc4+ #277 Not tainted
    
    syz-executor7/4015 is trying to acquire lock:
     (&ndev->lock){++--}, at: [<00000000416dce19>] __ipv6_dev_mc_dec+0x45/0x350 net/ipv6/mcast.c:928
    
    but task is already holding lock:
     (&tbl->lock){++-.}, at: [<00000000b5cb1d65>] neigh_ifdown+0x3d/0x250 net/core/neighbour.c:292
    
    which lock already depends on the new lock.
    
    the existing dependency chain (in reverse order) is:
    
    -> #3 (&tbl->lock){++-.}:
           __raw_write_lock_bh include/linux/rwlock_api_smp.h:203 [inline]
           _raw_write_lock_bh+0x31/0x40 kernel/locking/spinlock.c:312
           __neigh_create+0x87e/0x1d90 net/core/neighbour.c:528
           neigh_create include/net/neighbour.h:315 [inline]
           ip6_neigh_lookup+0x9a7/0xba0 net/ipv6/route.c:228
           dst_neigh_lookup include/net/dst.h:405 [inline]
           rt6_age_examine_exception net/ipv6/route.c:1609 [inline]
           rt6_age_exceptions+0x381/0x660 net/ipv6/route.c:1645
           fib6_age+0xfb/0x140 net/ipv6/ip6_fib.c:2033
           fib6_clean_node+0x389/0x580 net/ipv6/ip6_fib.c:1919
           fib6_walk_continue+0x46c/0x8a0 net/ipv6/ip6_fib.c:1845
           fib6_walk+0x91/0xf0 net/ipv6/ip6_fib.c:1893
           fib6_clean_tree+0x1e6/0x340 net/ipv6/ip6_fib.c:1970
           __fib6_clean_all+0x1f4/0x3a0 net/ipv6/ip6_fib.c:1986
           fib6_clean_all net/ipv6/ip6_fib.c:1997 [inline]
           fib6_run_gc+0x16b/0x3c0 net/ipv6/ip6_fib.c:2053
           ndisc_netdev_event+0x3c2/0x4a0 net/ipv6/ndisc.c:1781
           notifier_call_chain+0x136/0x2c0 kernel/notifier.c:93
           __raw_notifier_call_chain kernel/notifier.c:394 [inline]
           raw_notifier_call_chain+0x2d/0x40 kernel/notifier.c:401
           call_netdevice_notifiers_info+0x32/0x70 net/core/dev.c:1707
           call_netdevice_notifiers net/core/dev.c:1725 [inline]
           __dev_notify_flags+0x262/0x430 net/core/dev.c:6960
           dev_change_flags+0xf5/0x140 net/core/dev.c:6994
           devinet_ioctl+0x126a/0x1ac0 net/ipv4/devinet.c:1080
           inet_ioctl+0x184/0x310 net/ipv4/af_inet.c:919
           sock_do_ioctl+0xef/0x390 net/socket.c:957
           sock_ioctl+0x36b/0x610 net/socket.c:1081
           vfs_ioctl fs/ioctl.c:46 [inline]
           do_vfs_ioctl+0x1b1/0x1520 fs/ioctl.c:686
           SYSC_ioctl fs/ioctl.c:701 [inline]
           SyS_ioctl+0x8f/0xc0 fs/ioctl.c:692
           do_syscall_64+0x281/0x940 arch/x86/entry/common.c:287
           entry_SYSCALL_64_after_hwframe+0x42/0xb7
    
    -> #2 (rt6_exception_lock){+.-.}:
           __raw_spin_lock_bh include/linux/spinlock_api_smp.h:135 [inline]
           _raw_spin_lock_bh+0x31/0x40 kernel/locking/spinlock.c:168
           spin_lock_bh include/linux/spinlock.h:315 [inline]
           rt6_flush_exceptions+0x21/0x210 net/ipv6/route.c:1367
           fib6_del_route net/ipv6/ip6_fib.c:1677 [inline]
           fib6_del+0x624/0x12c0 net/ipv6/ip6_fib.c:1761
           __ip6_del_rt+0xc7/0x120 net/ipv6/route.c:2980
           ip6_del_rt+0x132/0x1a0 net/ipv6/route.c:2993
           __ipv6_dev_ac_dec+0x3b1/0x600 net/ipv6/anycast.c:332
           ipv6_dev_ac_dec net/ipv6/anycast.c:345 [inline]
           ipv6_sock_ac_close+0x2b4/0x3e0 net/ipv6/anycast.c:200
           inet6_release+0x48/0x70 net/ipv6/af_inet6.c:433
           sock_release+0x8d/0x1e0 net/socket.c:594
           sock_close+0x16/0x20 net/socket.c:1149
           __fput+0x327/0x7e0 fs/file_table.c:209
           ____fput+0x15/0x20 fs/file_table.c:243
           task_work_run+0x199/0x270 kernel/task_work.c:113
           exit_task_work include/linux/task_work.h:22 [inline]
           do_exit+0x9bb/0x1ad0 kernel/exit.c:865
           do_group_exit+0x149/0x400 kernel/exit.c:968
           get_signal+0x73a/0x16d0 kernel/signal.c:2469
           do_signal+0x90/0x1e90 arch/x86/kernel/signal.c:809
           exit_to_usermode_loop+0x258/0x2f0 arch/x86/entry/common.c:162
           prepare_exit_to_usermode arch/x86/entry/common.c:196 [inline]
           syscall_return_slowpath arch/x86/entry/common.c:265 [inline]
           do_syscall_64+0x6ec/0x940 arch/x86/entry/common.c:292
           entry_SYSCALL_64_after_hwframe+0x42/0xb7
    
    -> #1 (&(&tb->tb6_lock)->rlock){+.-.}:
           __raw_spin_lock_bh include/linux/spinlock_api_smp.h:135 [inline]
           _raw_spin_lock_bh+0x31/0x40 kernel/locking/spinlock.c:168
           spin_lock_bh include/linux/spinlock.h:315 [inline]
           __ip6_ins_rt+0x56/0x90 net/ipv6/route.c:1007
           ip6_route_add+0x141/0x190 net/ipv6/route.c:2955
           addrconf_prefix_route+0x44f/0x620 net/ipv6/addrconf.c:2359
           fixup_permanent_addr net/ipv6/addrconf.c:3368 [inline]
           addrconf_permanent_addr net/ipv6/addrconf.c:3391 [inline]
           addrconf_notify+0x1ad2/0x2310 net/ipv6/addrconf.c:3460
           notifier_call_chain+0x136/0x2c0 kernel/notifier.c:93
           __raw_notifier_call_chain kernel/notifier.c:394 [inline]
           raw_notifier_call_chain+0x2d/0x40 kernel/notifier.c:401
           call_netdevice_notifiers_info+0x32/0x70 net/core/dev.c:1707
           call_netdevice_notifiers net/core/dev.c:1725 [inline]
           __dev_notify_flags+0x15d/0x430 net/core/dev.c:6958
           dev_change_flags+0xf5/0x140 net/core/dev.c:6994
           do_setlink+0xa22/0x3bb0 net/core/rtnetlink.c:2357
           rtnl_newlink+0xf37/0x1a50 net/core/rtnetlink.c:2965
           rtnetlink_rcv_msg+0x57f/0xb10 net/core/rtnetlink.c:4641
           netlink_rcv_skb+0x14b/0x380 net/netlink/af_netlink.c:2444
           rtnetlink_rcv+0x1c/0x20 net/core/rtnetlink.c:4659
           netlink_unicast_kernel net/netlink/af_netlink.c:1308 [inline]
           netlink_unicast+0x4c4/0x6b0 net/netlink/af_netlink.c:1334
           netlink_sendmsg+0xa4a/0xe60 net/netlink/af_netlink.c:1897
           sock_sendmsg_nosec net/socket.c:629 [inline]
           sock_sendmsg+0xca/0x110 net/socket.c:639
           ___sys_sendmsg+0x767/0x8b0 net/socket.c:2047
           __sys_sendmsg+0xe5/0x210 net/socket.c:2081
           SYSC_sendmsg net/socket.c:2092 [inline]
           SyS_sendmsg+0x2d/0x50 net/socket.c:2088
           do_syscall_64+0x281/0x940 arch/x86/entry/common.c:287
           entry_SYSCALL_64_after_hwframe+0x42/0xb7
    
    -> #0 (&ndev->lock){++--}:
           lock_acquire+0x1d5/0x580 kernel/locking/lockdep.c:3920
           __raw_write_lock_bh include/linux/rwlock_api_smp.h:203 [inline]
           _raw_write_lock_bh+0x31/0x40 kernel/locking/spinlock.c:312
           __ipv6_dev_mc_dec+0x45/0x350 net/ipv6/mcast.c:928
           ipv6_dev_mc_dec+0x110/0x1f0 net/ipv6/mcast.c:961
           pndisc_destructor+0x21a/0x340 net/ipv6/ndisc.c:392
           pneigh_ifdown net/core/neighbour.c:695 [inline]
           neigh_ifdown+0x149/0x250 net/core/neighbour.c:294
           rt6_disable_ip+0x537/0x700 net/ipv6/route.c:3874
           addrconf_ifdown+0x14b/0x14f0 net/ipv6/addrconf.c:3633
           addrconf_notify+0x5f8/0x2310 net/ipv6/addrconf.c:3557
           notifier_call_chain+0x136/0x2c0 kernel/notifier.c:93
           __raw_notifier_call_chain kernel/notifier.c:394 [inline]
           raw_notifier_call_chain+0x2d/0x40 kernel/notifier.c:401
           call_netdevice_notifiers_info+0x32/0x70 net/core/dev.c:1707
           call_netdevice_notifiers net/core/dev.c:1725 [inline]
           __dev_notify_flags+0x262/0x430 net/core/dev.c:6960
           dev_change_flags+0xf5/0x140 net/core/dev.c:6994
           devinet_ioctl+0x126a/0x1ac0 net/ipv4/devinet.c:1080
           inet_ioctl+0x184/0x310 net/ipv4/af_inet.c:919
           packet_ioctl+0x1ff/0x310 net/packet/af_packet.c:4066
           sock_do_ioctl+0xef/0x390 net/socket.c:957
           sock_ioctl+0x36b/0x610 net/socket.c:1081
           vfs_ioctl fs/ioctl.c:46 [inline]
           do_vfs_ioctl+0x1b1/0x1520 fs/ioctl.c:686
           SYSC_ioctl fs/ioctl.c:701 [inline]
           SyS_ioctl+0x8f/0xc0 fs/ioctl.c:692
           do_syscall_64+0x281/0x940 arch/x86/entry/common.c:287
           entry_SYSCALL_64_after_hwframe+0x42/0xb7
    
    other info that might help us debug this:
    
    Chain exists of:
      &ndev->lock --> rt6_exception_lock --> &tbl->lock
    
     Possible unsafe locking scenario:
    
           CPU0                    CPU1
           ----                    ----
      lock(&tbl->lock);
                                   lock(rt6_exception_lock);
                                   lock(&tbl->lock);
      lock(&ndev->lock);
    
     *** DEADLOCK ***
    
    2 locks held by syz-executor7/4015:
     #0:  (rtnl_mutex){+.+.}, at: [<00000000a2f16daa>] rtnl_lock+0x17/0x20 net/core/rtnetlink.c:74
     #1:  (&tbl->lock){++-.}, at: [<00000000b5cb1d65>] neigh_ifdown+0x3d/0x250 net/core/neighbour.c:292
    
    stack backtrace:
    CPU: 0 PID: 4015 Comm: syz-executor7 Not tainted 4.16.0-rc4+ #277
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     __dump_stack lib/dump_stack.c:17 [inline]
     dump_stack+0x194/0x24d lib/dump_stack.c:53
     print_circular_bug.isra.38+0x2cd/0x2dc kernel/locking/lockdep.c:1223
     check_prev_add kernel/locking/lockdep.c:1863 [inline]
     check_prevs_add kernel/locking/lockdep.c:1976 [inline]
     validate_chain kernel/locking/lockdep.c:2417 [inline]
     __lock_acquire+0x30a8/0x3e00 kernel/locking/lockdep.c:3431
     lock_acquire+0x1d5/0x580 kernel/locking/lockdep.c:3920
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:203 [inline]
     _raw_write_lock_bh+0x31/0x40 kernel/locking/spinlock.c:312
     __ipv6_dev_mc_dec+0x45/0x350 net/ipv6/mcast.c:928
     ipv6_dev_mc_dec+0x110/0x1f0 net/ipv6/mcast.c:961
     pndisc_destructor+0x21a/0x340 net/ipv6/ndisc.c:392
     pneigh_ifdown net/core/neighbour.c:695 [inline]
     neigh_ifdown+0x149/0x250 net/core/neighbour.c:294
     rt6_disable_ip+0x537/0x700 net/ipv6/route.c:3874
     addrconf_ifdown+0x14b/0x14f0 net/ipv6/addrconf.c:3633
     addrconf_notify+0x5f8/0x2310 net/ipv6/addrconf.c:3557
     notifier_call_chain+0x136/0x2c0 kernel/notifier.c:93
     __raw_notifier_call_chain kernel/notifier.c:394 [inline]
     raw_notifier_call_chain+0x2d/0x40 kernel/notifier.c:401
     call_netdevice_notifiers_info+0x32/0x70 net/core/dev.c:1707
     call_netdevice_notifiers net/core/dev.c:1725 [inline]
     __dev_notify_flags+0x262/0x430 net/core/dev.c:6960
     dev_change_flags+0xf5/0x140 net/core/dev.c:6994
     devinet_ioctl+0x126a/0x1ac0 net/ipv4/devinet.c:1080
     inet_ioctl+0x184/0x310 net/ipv4/af_inet.c:919
     packet_ioctl+0x1ff/0x310 net/packet/af_packet.c:4066
     sock_do_ioctl+0xef/0x390 net/socket.c:957
     sock_ioctl+0x36b/0x610 net/socket.c:1081
     vfs_ioctl fs/ioctl.c:46 [inline]
     do_vfs_ioctl+0x1b1/0x1520 fs/ioctl.c:686
     SYSC_ioctl fs/ioctl.c:701 [inline]
     SyS_ioctl+0x8f/0xc0 fs/ioctl.c:692
     do_syscall_64+0x281/0x940 arch/x86/entry/common.c:287
     entry_SYSCALL_64_after_hwframe+0x42/0xb7
    
    Fixes: c757faa8bfa2 ("ipv6: prepare fib6_age() for exception table")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Wei Wang <weiwan@google.com>
    Cc: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Wei Wang <weiwan@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2cd1083d79a0a8c223af430ca97884c28a1e2fc0
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Mar 22 16:22:41 2018 +0100

    iommu/amd: Make amd_iommu_devtable_lock a spin_lock
    
    Before commit 0bb6e243d7fb ("iommu/amd: Support IOMMU_DOMAIN_DMA type
    allocation") amd_iommu_devtable_lock had a read_lock() user but now
    there are none. In fact, after the mentioned commit we had only
    write_lock() user of the lock. Since there is no reason to keep it as
    writer lock, change its type to a spin_lock.
    I *think* that we might even be able to remove the lock because all its
    current user seem to have their own protection.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

commit 1bfa26ff8c4b7512f4e4efa6df211239223033d4
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Mar 23 07:56:58 2018 -0700

    ipv6: fix possible deadlock in rt6_age_examine_exception()
    
    syzbot reported a LOCKDEP splat [1] in rt6_age_examine_exception()
    
    rt6_age_examine_exception() is called while rt6_exception_lock is held.
    This lock is the lower one in the lock hierarchy, thus we can not
    call dst_neigh_lookup() function, as it can fallback to neigh_create()
    
    We should instead do a pure RCU lookup. As a bonus we avoid
    a pair of atomic operations on neigh refcount.
    
    [1]
    
    WARNING: possible circular locking dependency detected
    4.16.0-rc4+ #277 Not tainted
    
    syz-executor7/4015 is trying to acquire lock:
     (&ndev->lock){++--}, at: [<00000000416dce19>] __ipv6_dev_mc_dec+0x45/0x350 net/ipv6/mcast.c:928
    
    but task is already holding lock:
     (&tbl->lock){++-.}, at: [<00000000b5cb1d65>] neigh_ifdown+0x3d/0x250 net/core/neighbour.c:292
    
    which lock already depends on the new lock.
    
    the existing dependency chain (in reverse order) is:
    
    -> #3 (&tbl->lock){++-.}:
           __raw_write_lock_bh include/linux/rwlock_api_smp.h:203 [inline]
           _raw_write_lock_bh+0x31/0x40 kernel/locking/spinlock.c:312
           __neigh_create+0x87e/0x1d90 net/core/neighbour.c:528
           neigh_create include/net/neighbour.h:315 [inline]
           ip6_neigh_lookup+0x9a7/0xba0 net/ipv6/route.c:228
           dst_neigh_lookup include/net/dst.h:405 [inline]
           rt6_age_examine_exception net/ipv6/route.c:1609 [inline]
           rt6_age_exceptions+0x381/0x660 net/ipv6/route.c:1645
           fib6_age+0xfb/0x140 net/ipv6/ip6_fib.c:2033
           fib6_clean_node+0x389/0x580 net/ipv6/ip6_fib.c:1919
           fib6_walk_continue+0x46c/0x8a0 net/ipv6/ip6_fib.c:1845
           fib6_walk+0x91/0xf0 net/ipv6/ip6_fib.c:1893
           fib6_clean_tree+0x1e6/0x340 net/ipv6/ip6_fib.c:1970
           __fib6_clean_all+0x1f4/0x3a0 net/ipv6/ip6_fib.c:1986
           fib6_clean_all net/ipv6/ip6_fib.c:1997 [inline]
           fib6_run_gc+0x16b/0x3c0 net/ipv6/ip6_fib.c:2053
           ndisc_netdev_event+0x3c2/0x4a0 net/ipv6/ndisc.c:1781
           notifier_call_chain+0x136/0x2c0 kernel/notifier.c:93
           __raw_notifier_call_chain kernel/notifier.c:394 [inline]
           raw_notifier_call_chain+0x2d/0x40 kernel/notifier.c:401
           call_netdevice_notifiers_info+0x32/0x70 net/core/dev.c:1707
           call_netdevice_notifiers net/core/dev.c:1725 [inline]
           __dev_notify_flags+0x262/0x430 net/core/dev.c:6960
           dev_change_flags+0xf5/0x140 net/core/dev.c:6994
           devinet_ioctl+0x126a/0x1ac0 net/ipv4/devinet.c:1080
           inet_ioctl+0x184/0x310 net/ipv4/af_inet.c:919
           sock_do_ioctl+0xef/0x390 net/socket.c:957
           sock_ioctl+0x36b/0x610 net/socket.c:1081
           vfs_ioctl fs/ioctl.c:46 [inline]
           do_vfs_ioctl+0x1b1/0x1520 fs/ioctl.c:686
           SYSC_ioctl fs/ioctl.c:701 [inline]
           SyS_ioctl+0x8f/0xc0 fs/ioctl.c:692
           do_syscall_64+0x281/0x940 arch/x86/entry/common.c:287
           entry_SYSCALL_64_after_hwframe+0x42/0xb7
    
    -> #2 (rt6_exception_lock){+.-.}:
           __raw_spin_lock_bh include/linux/spinlock_api_smp.h:135 [inline]
           _raw_spin_lock_bh+0x31/0x40 kernel/locking/spinlock.c:168
           spin_lock_bh include/linux/spinlock.h:315 [inline]
           rt6_flush_exceptions+0x21/0x210 net/ipv6/route.c:1367
           fib6_del_route net/ipv6/ip6_fib.c:1677 [inline]
           fib6_del+0x624/0x12c0 net/ipv6/ip6_fib.c:1761
           __ip6_del_rt+0xc7/0x120 net/ipv6/route.c:2980
           ip6_del_rt+0x132/0x1a0 net/ipv6/route.c:2993
           __ipv6_dev_ac_dec+0x3b1/0x600 net/ipv6/anycast.c:332
           ipv6_dev_ac_dec net/ipv6/anycast.c:345 [inline]
           ipv6_sock_ac_close+0x2b4/0x3e0 net/ipv6/anycast.c:200
           inet6_release+0x48/0x70 net/ipv6/af_inet6.c:433
           sock_release+0x8d/0x1e0 net/socket.c:594
           sock_close+0x16/0x20 net/socket.c:1149
           __fput+0x327/0x7e0 fs/file_table.c:209
           ____fput+0x15/0x20 fs/file_table.c:243
           task_work_run+0x199/0x270 kernel/task_work.c:113
           exit_task_work include/linux/task_work.h:22 [inline]
           do_exit+0x9bb/0x1ad0 kernel/exit.c:865
           do_group_exit+0x149/0x400 kernel/exit.c:968
           get_signal+0x73a/0x16d0 kernel/signal.c:2469
           do_signal+0x90/0x1e90 arch/x86/kernel/signal.c:809
           exit_to_usermode_loop+0x258/0x2f0 arch/x86/entry/common.c:162
           prepare_exit_to_usermode arch/x86/entry/common.c:196 [inline]
           syscall_return_slowpath arch/x86/entry/common.c:265 [inline]
           do_syscall_64+0x6ec/0x940 arch/x86/entry/common.c:292
           entry_SYSCALL_64_after_hwframe+0x42/0xb7
    
    -> #1 (&(&tb->tb6_lock)->rlock){+.-.}:
           __raw_spin_lock_bh include/linux/spinlock_api_smp.h:135 [inline]
           _raw_spin_lock_bh+0x31/0x40 kernel/locking/spinlock.c:168
           spin_lock_bh include/linux/spinlock.h:315 [inline]
           __ip6_ins_rt+0x56/0x90 net/ipv6/route.c:1007
           ip6_route_add+0x141/0x190 net/ipv6/route.c:2955
           addrconf_prefix_route+0x44f/0x620 net/ipv6/addrconf.c:2359
           fixup_permanent_addr net/ipv6/addrconf.c:3368 [inline]
           addrconf_permanent_addr net/ipv6/addrconf.c:3391 [inline]
           addrconf_notify+0x1ad2/0x2310 net/ipv6/addrconf.c:3460
           notifier_call_chain+0x136/0x2c0 kernel/notifier.c:93
           __raw_notifier_call_chain kernel/notifier.c:394 [inline]
           raw_notifier_call_chain+0x2d/0x40 kernel/notifier.c:401
           call_netdevice_notifiers_info+0x32/0x70 net/core/dev.c:1707
           call_netdevice_notifiers net/core/dev.c:1725 [inline]
           __dev_notify_flags+0x15d/0x430 net/core/dev.c:6958
           dev_change_flags+0xf5/0x140 net/core/dev.c:6994
           do_setlink+0xa22/0x3bb0 net/core/rtnetlink.c:2357
           rtnl_newlink+0xf37/0x1a50 net/core/rtnetlink.c:2965
           rtnetlink_rcv_msg+0x57f/0xb10 net/core/rtnetlink.c:4641
           netlink_rcv_skb+0x14b/0x380 net/netlink/af_netlink.c:2444
           rtnetlink_rcv+0x1c/0x20 net/core/rtnetlink.c:4659
           netlink_unicast_kernel net/netlink/af_netlink.c:1308 [inline]
           netlink_unicast+0x4c4/0x6b0 net/netlink/af_netlink.c:1334
           netlink_sendmsg+0xa4a/0xe60 net/netlink/af_netlink.c:1897
           sock_sendmsg_nosec net/socket.c:629 [inline]
           sock_sendmsg+0xca/0x110 net/socket.c:639
           ___sys_sendmsg+0x767/0x8b0 net/socket.c:2047
           __sys_sendmsg+0xe5/0x210 net/socket.c:2081
           SYSC_sendmsg net/socket.c:2092 [inline]
           SyS_sendmsg+0x2d/0x50 net/socket.c:2088
           do_syscall_64+0x281/0x940 arch/x86/entry/common.c:287
           entry_SYSCALL_64_after_hwframe+0x42/0xb7
    
    -> #0 (&ndev->lock){++--}:
           lock_acquire+0x1d5/0x580 kernel/locking/lockdep.c:3920
           __raw_write_lock_bh include/linux/rwlock_api_smp.h:203 [inline]
           _raw_write_lock_bh+0x31/0x40 kernel/locking/spinlock.c:312
           __ipv6_dev_mc_dec+0x45/0x350 net/ipv6/mcast.c:928
           ipv6_dev_mc_dec+0x110/0x1f0 net/ipv6/mcast.c:961
           pndisc_destructor+0x21a/0x340 net/ipv6/ndisc.c:392
           pneigh_ifdown net/core/neighbour.c:695 [inline]
           neigh_ifdown+0x149/0x250 net/core/neighbour.c:294
           rt6_disable_ip+0x537/0x700 net/ipv6/route.c:3874
           addrconf_ifdown+0x14b/0x14f0 net/ipv6/addrconf.c:3633
           addrconf_notify+0x5f8/0x2310 net/ipv6/addrconf.c:3557
           notifier_call_chain+0x136/0x2c0 kernel/notifier.c:93
           __raw_notifier_call_chain kernel/notifier.c:394 [inline]
           raw_notifier_call_chain+0x2d/0x40 kernel/notifier.c:401
           call_netdevice_notifiers_info+0x32/0x70 net/core/dev.c:1707
           call_netdevice_notifiers net/core/dev.c:1725 [inline]
           __dev_notify_flags+0x262/0x430 net/core/dev.c:6960
           dev_change_flags+0xf5/0x140 net/core/dev.c:6994
           devinet_ioctl+0x126a/0x1ac0 net/ipv4/devinet.c:1080
           inet_ioctl+0x184/0x310 net/ipv4/af_inet.c:919
           packet_ioctl+0x1ff/0x310 net/packet/af_packet.c:4066
           sock_do_ioctl+0xef/0x390 net/socket.c:957
           sock_ioctl+0x36b/0x610 net/socket.c:1081
           vfs_ioctl fs/ioctl.c:46 [inline]
           do_vfs_ioctl+0x1b1/0x1520 fs/ioctl.c:686
           SYSC_ioctl fs/ioctl.c:701 [inline]
           SyS_ioctl+0x8f/0xc0 fs/ioctl.c:692
           do_syscall_64+0x281/0x940 arch/x86/entry/common.c:287
           entry_SYSCALL_64_after_hwframe+0x42/0xb7
    
    other info that might help us debug this:
    
    Chain exists of:
      &ndev->lock --> rt6_exception_lock --> &tbl->lock
    
     Possible unsafe locking scenario:
    
           CPU0                    CPU1
           ----                    ----
      lock(&tbl->lock);
                                   lock(rt6_exception_lock);
                                   lock(&tbl->lock);
      lock(&ndev->lock);
    
     *** DEADLOCK ***
    
    2 locks held by syz-executor7/4015:
     #0:  (rtnl_mutex){+.+.}, at: [<00000000a2f16daa>] rtnl_lock+0x17/0x20 net/core/rtnetlink.c:74
     #1:  (&tbl->lock){++-.}, at: [<00000000b5cb1d65>] neigh_ifdown+0x3d/0x250 net/core/neighbour.c:292
    
    stack backtrace:
    CPU: 0 PID: 4015 Comm: syz-executor7 Not tainted 4.16.0-rc4+ #277
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Call Trace:
     __dump_stack lib/dump_stack.c:17 [inline]
     dump_stack+0x194/0x24d lib/dump_stack.c:53
     print_circular_bug.isra.38+0x2cd/0x2dc kernel/locking/lockdep.c:1223
     check_prev_add kernel/locking/lockdep.c:1863 [inline]
     check_prevs_add kernel/locking/lockdep.c:1976 [inline]
     validate_chain kernel/locking/lockdep.c:2417 [inline]
     __lock_acquire+0x30a8/0x3e00 kernel/locking/lockdep.c:3431
     lock_acquire+0x1d5/0x580 kernel/locking/lockdep.c:3920
     __raw_write_lock_bh include/linux/rwlock_api_smp.h:203 [inline]
     _raw_write_lock_bh+0x31/0x40 kernel/locking/spinlock.c:312
     __ipv6_dev_mc_dec+0x45/0x350 net/ipv6/mcast.c:928
     ipv6_dev_mc_dec+0x110/0x1f0 net/ipv6/mcast.c:961
     pndisc_destructor+0x21a/0x340 net/ipv6/ndisc.c:392
     pneigh_ifdown net/core/neighbour.c:695 [inline]
     neigh_ifdown+0x149/0x250 net/core/neighbour.c:294
     rt6_disable_ip+0x537/0x700 net/ipv6/route.c:3874
     addrconf_ifdown+0x14b/0x14f0 net/ipv6/addrconf.c:3633
     addrconf_notify+0x5f8/0x2310 net/ipv6/addrconf.c:3557
     notifier_call_chain+0x136/0x2c0 kernel/notifier.c:93
     __raw_notifier_call_chain kernel/notifier.c:394 [inline]
     raw_notifier_call_chain+0x2d/0x40 kernel/notifier.c:401
     call_netdevice_notifiers_info+0x32/0x70 net/core/dev.c:1707
     call_netdevice_notifiers net/core/dev.c:1725 [inline]
     __dev_notify_flags+0x262/0x430 net/core/dev.c:6960
     dev_change_flags+0xf5/0x140 net/core/dev.c:6994
     devinet_ioctl+0x126a/0x1ac0 net/ipv4/devinet.c:1080
     inet_ioctl+0x184/0x310 net/ipv4/af_inet.c:919
     packet_ioctl+0x1ff/0x310 net/packet/af_packet.c:4066
     sock_do_ioctl+0xef/0x390 net/socket.c:957
     sock_ioctl+0x36b/0x610 net/socket.c:1081
     vfs_ioctl fs/ioctl.c:46 [inline]
     do_vfs_ioctl+0x1b1/0x1520 fs/ioctl.c:686
     SYSC_ioctl fs/ioctl.c:701 [inline]
     SyS_ioctl+0x8f/0xc0 fs/ioctl.c:692
     do_syscall_64+0x281/0x940 arch/x86/entry/common.c:287
     entry_SYSCALL_64_after_hwframe+0x42/0xb7
    
    Fixes: c757faa8bfa2 ("ipv6: prepare fib6_age() for exception table")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Wei Wang <weiwan@google.com>
    Cc: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Wei Wang <weiwan@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit bf617f7a92edc6bb2909db2bfa4576f50b280ee5
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Jan 27 17:29:49 2018 +0800

    f2fs: fix to check extent cache in f2fs_drop_extent_tree
    
    If noextent_cache mount option is on, we will never initialize extent tree
    in inode, but still we're going to access it in f2fs_drop_extent_tree,
    result in kernel panic as below:
    
     BUG: unable to handle kernel NULL pointer dereference at 0000000000000038
     IP: _raw_write_lock+0xc/0x30
     Call Trace:
      ? f2fs_drop_extent_tree+0x41/0x70 [f2fs]
      f2fs_fallocate+0x5a0/0xdd0 [f2fs]
      ? common_file_perm+0x47/0xc0
      ? apparmor_file_permission+0x1a/0x20
      vfs_fallocate+0x15b/0x290
      SyS_fallocate+0x44/0x70
      do_syscall_64+0x6e/0x160
      entry_SYSCALL64_slow_path+0x25/0x25
    
    This patch fixes to check extent cache status before using in
    f2fs_drop_extent_tree.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

commit 79eb382b5e06a6dca5806465d7195d686a463ab0
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Feb 23 23:33:07 2018 +0100

    crypto: ccp - don't disable interrupts while setting up debugfs
    
    I don't why we need take a single write lock and disable interrupts
    while setting up debugfs. This is what what happens when we try anyway:
    
    |ccp 0000:03:00.2: enabling device (0000 -> 0002)
    |BUG: sleeping function called from invalid context at kernel/locking/rwsem.c:69
    |in_atomic(): 1, irqs_disabled(): 1, pid: 3, name: kworker/0:0
    |irq event stamp: 17150
    |hardirqs last  enabled at (17149): [<0000000097a18c49>] restore_regs_and_return_to_kernel+0x0/0x23
    |hardirqs last disabled at (17150): [<000000000773b3a9>] _raw_write_lock_irqsave+0x1b/0x50
    |softirqs last  enabled at (17148): [<0000000064d56155>] __do_softirq+0x3b8/0x4c1
    |softirqs last disabled at (17125): [<0000000092633c18>] irq_exit+0xb1/0xc0
    |CPU: 0 PID: 3 Comm: kworker/0:0 Not tainted 4.16.0-rc2+ #30
    |Workqueue: events work_for_cpu_fn
    |Call Trace:
    | dump_stack+0x7d/0xb6
    | ___might_sleep+0x1eb/0x250
    | down_write+0x17/0x60
    | start_creating+0x4c/0xe0
    | debugfs_create_dir+0x9/0x100
    | ccp5_debugfs_setup+0x191/0x1b0
    | ccp5_init+0x8a7/0x8c0
    | ccp_dev_init+0xb8/0xe0
    | sp_init+0x6c/0x90
    | sp_pci_probe+0x26e/0x590
    | local_pci_probe+0x3f/0x90
    | work_for_cpu_fn+0x11/0x20
    | process_one_work+0x1ff/0x650
    | worker_thread+0x1d4/0x3a0
    | kthread+0xfe/0x130
    | ret_from_fork+0x27/0x50
    
    If any locking is required, a simple mutex will do it.
    
    Cc: Gary R Hook <gary.hook@amd.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Gary R Hook <gary.hook@amd.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 294975841483c08e84572713f348cd51b8408021
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Feb 5 22:18:11 2018 -0500

    tracing: Fix parsing of globs with a wildcard at the beginning
    
    commit 07234021410bbc27b7c86c18de98616c29fbe667 upstream.
    
    Al Viro reported:
    
        For substring - sure, but what about something like "*a*b" and "a*b"?
        AFAICS, filter_parse_regex() ends up with identical results in both
        cases - MATCH_GLOB and *search = "a*b".  And no way for the caller
        to tell one from another.
    
    Testing this with the following:
    
     # cd /sys/kernel/tracing
     # echo '*raw*lock' > set_ftrace_filter
     bash: echo: write error: Invalid argument
    
    With this patch:
    
     # echo '*raw*lock' > set_ftrace_filter
     # cat set_ftrace_filter
    _raw_read_trylock
    _raw_write_trylock
    _raw_read_unlock
    _raw_spin_unlock
    _raw_write_unlock
    _raw_spin_trylock
    _raw_spin_lock
    _raw_write_lock
    _raw_read_lock
    
    Al recommended not setting the search buffer to skip the first '*' unless we
    know we are not using MATCH_GLOB. This implements his suggested logic.
    
    Link: http://lkml.kernel.org/r/20180127170748.GF13338@ZenIV.linux.org.uk
    
    Cc: stable@vger.kernel.org
    Fixes: 60f1d5e3bac44 ("ftrace: Support full glob matching")
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reported-by: Al Viro <viro@ZenIV.linux.org.uk>
    Suggsted-by: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 95f92d0a0ca9dd0f4a92e9eb02b2b7b3d257d46f
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Feb 5 22:18:11 2018 -0500

    tracing: Fix parsing of globs with a wildcard at the beginning
    
    commit 07234021410bbc27b7c86c18de98616c29fbe667 upstream.
    
    Al Viro reported:
    
        For substring - sure, but what about something like "*a*b" and "a*b"?
        AFAICS, filter_parse_regex() ends up with identical results in both
        cases - MATCH_GLOB and *search = "a*b".  And no way for the caller
        to tell one from another.
    
    Testing this with the following:
    
     # cd /sys/kernel/tracing
     # echo '*raw*lock' > set_ftrace_filter
     bash: echo: write error: Invalid argument
    
    With this patch:
    
     # echo '*raw*lock' > set_ftrace_filter
     # cat set_ftrace_filter
    _raw_read_trylock
    _raw_write_trylock
    _raw_read_unlock
    _raw_spin_unlock
    _raw_write_unlock
    _raw_spin_trylock
    _raw_spin_lock
    _raw_write_lock
    _raw_read_lock
    
    Al recommended not setting the search buffer to skip the first '*' unless we
    know we are not using MATCH_GLOB. This implements his suggested logic.
    
    Link: http://lkml.kernel.org/r/20180127170748.GF13338@ZenIV.linux.org.uk
    
    Cc: stable@vger.kernel.org
    Fixes: 60f1d5e3bac44 ("ftrace: Support full glob matching")
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reported-by: Al Viro <viro@ZenIV.linux.org.uk>
    Suggsted-by: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 07234021410bbc27b7c86c18de98616c29fbe667
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Feb 5 22:18:11 2018 -0500

    tracing: Fix parsing of globs with a wildcard at the beginning
    
    Al Viro reported:
    
        For substring - sure, but what about something like "*a*b" and "a*b"?
        AFAICS, filter_parse_regex() ends up with identical results in both
        cases - MATCH_GLOB and *search = "a*b".  And no way for the caller
        to tell one from another.
    
    Testing this with the following:
    
     # cd /sys/kernel/tracing
     # echo '*raw*lock' > set_ftrace_filter
     bash: echo: write error: Invalid argument
    
    With this patch:
    
     # echo '*raw*lock' > set_ftrace_filter
     # cat set_ftrace_filter
    _raw_read_trylock
    _raw_write_trylock
    _raw_read_unlock
    _raw_spin_unlock
    _raw_write_unlock
    _raw_spin_trylock
    _raw_spin_lock
    _raw_write_lock
    _raw_read_lock
    
    Al recommended not setting the search buffer to skip the first '*' unless we
    know we are not using MATCH_GLOB. This implements his suggested logic.
    
    Link: http://lkml.kernel.org/r/20180127170748.GF13338@ZenIV.linux.org.uk
    
    Cc: stable@vger.kernel.org
    Fixes: 60f1d5e3bac44 ("ftrace: Support full glob matching")
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reported-by: Al Viro <viro@ZenIV.linux.org.uk>
    Suggsted-by: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

commit ea05ba7c559c8e5a5946c3a94a2a266e9a6680a6
Author: Michael Bringmann <mwb@linux.vnet.ibm.com>
Date:   Tue Nov 28 16:58:40 2017 -0600

    powerpc/numa: Ensure nodes initialized for hotplug
    
    This patch fixes some problems encountered at runtime with
    configurations that support memory-less nodes, or that hot-add CPUs
    into nodes that are memoryless during system execution after boot. The
    problems of interest include:
    
    * Nodes known to powerpc to be memoryless at boot, but to have CPUs in
      them are allowed to be 'possible' and 'online'. Memory allocations
      for those nodes are taken from another node that does have memory
      until and if memory is hot-added to the node.
    
    * Nodes which have no resources assigned at boot, but which may still
      be referenced subsequently by affinity or associativity attributes,
      are kept in the list of 'possible' nodes for powerpc. Hot-add of
      memory or CPUs to the system can reference these nodes and bring
      them online instead of redirecting the references to one of the set
      of nodes known to have memory at boot.
    
    Note that this software operates under the context of CPU hotplug. We
    are not doing memory hotplug in this code, but rather updating the
    kernel's CPU topology (i.e. arch_update_cpu_topology /
    numa_update_cpu_topology). We are initializing a node that may be used
    by CPUs or memory before it can be referenced as invalid by a CPU
    hotplug operation. CPU hotplug operations are protected by a range of
    APIs including cpu_maps_update_begin/cpu_maps_update_done,
    cpus_read/write_lock / cpus_read/write_unlock, device locks, and more.
    Memory hotplug operations, including try_online_node, are protected by
    mem_hotplug_begin/mem_hotplug_done, device locks, and more. In the
    case of CPUs being hot-added to a previously memoryless node, the
    try_online_node operation occurs wholly within the CPU locks with no
    overlap. Using HMC hot-add/hot-remove operations, we have been able to
    add and remove CPUs to any possible node without failures. HMC
    operations involve a degree self-serialization, though.
    
    Signed-off-by: Michael Bringmann <mwb@linux.vnet.ibm.com>
    Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 5e3ee23648a20dfaf72eeb88f884aae25ea7d8fb
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Fri Dec 8 15:55:58 2017 +0200

    btrfs: sink extent_write_locked_range tree parameter
    
    This function is called only from submit_compressed_extents and the
    io tree being passed is always that of the inode. But we are also
    passing the inode, so just move getting the io tree pointer in
    extent_write_locked_range to simplify the signature.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 916b929831a92a2a432274cd264311893f22a46d
Author: David Sterba <dsterba@suse.com>
Date:   Fri Jun 23 03:47:28 2017 +0200

    btrfs: sink get_extent parameter to extent_write_locked_range
    
    There's only one caller.
    
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 01f196945a21b3eec37317e3bc5cf35f95f95063
Author: Sascha Hauer <s.hauer@pengutronix.de>
Date:   Fri Nov 24 12:17:14 2017 +0100

    ubi: Fix copy/paste error in function documentation
    
    The function documentation of leb_write_trylock is copied from
    leb_write_lock. Replace the function name with the correct one.
    
    Signed-off-by: Sascha Hauer <s.hauer@pengutronix.de>
    Signed-off-by: Richard Weinberger <richard@nod.at>

commit 47d3d7ac656a1ffb9d0f0d3c845663ed6fd7e78d
Author: Tom Herbert <tom@quantonium.net>
Date:   Mon Oct 30 14:16:00 2017 -0700

    ipv6: Implement limits on Hop-by-Hop and Destination options
    
    RFC 8200 (IPv6) defines Hop-by-Hop options and Destination options
    extension headers. Both of these carry a list of TLVs which is
    only limited by the maximum length of the extension header (2048
    bytes). By the spec a host must process all the TLVs in these
    options, however these could be used as a fairly obvious
    denial of service attack. I think this could in fact be
    a significant DOS vector on the Internet, one mitigating
    factor might be that many FWs drop all packets with EH (and
    obviously this is only IPv6) so an Internet wide attack might not
    be so effective (yet!).
    
    By my calculation, the worse case packet with TLVs in a standard
    1500 byte MTU packet that would be processed by the stack contains
    1282 invidual TLVs (including pad TLVS) or 724 two byte TLVs. I
    wrote a quick test program that floods a whole bunch of these
    packets to a host and sure enough there is substantial time spent
    in ip6_parse_tlv. These packets contain nothing but unknown TLVS
    (that are ignored), TLV padding, and bogus UDP header with zero
    payload length.
    
      25.38%  [kernel]                    [k] __fib6_clean_all
      21.63%  [kernel]                    [k] ip6_parse_tlv
       4.21%  [kernel]                    [k] __local_bh_enable_ip
       2.18%  [kernel]                    [k] ip6_pol_route.isra.39
       1.98%  [kernel]                    [k] fib6_walk_continue
       1.88%  [kernel]                    [k] _raw_write_lock_bh
       1.65%  [kernel]                    [k] dst_release
    
    This patch adds configurable limits to Destination and Hop-by-Hop
    options. There are three limits that may be set:
      - Limit the number of options in a Hop-by-Hop or Destination options
        extension header.
      - Limit the byte length of a Hop-by-Hop or Destination options
        extension header.
      - Disallow unrecognized options in a Hop-by-Hop or Destination
        options extension header.
    
    The limits are set in corresponding sysctls:
    
      ipv6.sysctl.max_dst_opts_cnt
      ipv6.sysctl.max_hbh_opts_cnt
      ipv6.sysctl.max_dst_opts_len
      ipv6.sysctl.max_hbh_opts_len
    
    If a max_*_opts_cnt is less than zero then unknown TLVs are disallowed.
    The number of known TLVs that are allowed is the absolute value of
    this number.
    
    If a limit is exceeded when processing an extension header the packet is
    dropped.
    
    Default values are set to 8 for options counts, and set to INT_MAX
    for maximum length. Note the choice to limit options to 8 is an
    arbitrary guess (roughly based on the fact that the stack supports
    three HBH options and just one destination option).
    
    These limits have being proposed in draft-ietf-6man-rfc6434-bis.
    
    Tested (by Martin Lau)
    
    I tested out 1 thread (i.e. one raw_udp process).
    
    I changed the net.ipv6.max_dst_(opts|hbh)_number between 8 to 2048.
    With sysctls setting to 2048, the softirq% is packed to 100%.
    With 8, the softirq% is almost unnoticable from mpstat.
    
    v2;
      - Code and documention cleanup.
      - Change references of RFC2460 to be RFC8200.
      - Add reference to RFC6434-bis where the limits will be in standard.
    
    Signed-off-by: Tom Herbert <tom@quantonium.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit dd9640717f3f6abef2b16ba3cbc6eeb203172124
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Fri Mar 31 15:12:01 2017 -0700

    hugetlbfs: initialize shared policy as part of inode allocation
    
    
    [ Upstream commit 4742a35d9de745e867405b4311e1aac412f0ace1 ]
    
    Any time after inode allocation, destroy_inode can be called.  The
    hugetlbfs inode contains a shared_policy structure, and
    mpol_free_shared_policy is unconditionally called as part of
    hugetlbfs_destroy_inode.  Initialize the policy as part of inode
    allocation so that any quick (error path) calls to destroy_inode will be
    handed an initialized policy.
    
    syzkaller fuzzer found this bug, that resulted in the following:
    
        BUG: KASAN: user-memory-access in atomic_inc
        include/asm-generic/atomic-instrumented.h:87 [inline] at addr
        000000131730bd7a
        BUG: KASAN: user-memory-access in __lock_acquire+0x21a/0x3a80
        kernel/locking/lockdep.c:3239 at addr 000000131730bd7a
        Write of size 4 by task syz-executor6/14086
        CPU: 3 PID: 14086 Comm: syz-executor6 Not tainted 4.11.0-rc3+ #364
        Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011
        Call Trace:
         atomic_inc include/asm-generic/atomic-instrumented.h:87 [inline]
         __lock_acquire+0x21a/0x3a80 kernel/locking/lockdep.c:3239
         lock_acquire+0x1ee/0x590 kernel/locking/lockdep.c:3762
         __raw_write_lock include/linux/rwlock_api_smp.h:210 [inline]
         _raw_write_lock+0x33/0x50 kernel/locking/spinlock.c:295
         mpol_free_shared_policy+0x43/0xb0 mm/mempolicy.c:2536
         hugetlbfs_destroy_inode+0xca/0x120 fs/hugetlbfs/inode.c:952
         alloc_inode+0x10d/0x180 fs/inode.c:216
         new_inode_pseudo+0x69/0x190 fs/inode.c:889
         new_inode+0x1c/0x40 fs/inode.c:918
         hugetlbfs_get_inode+0x40/0x420 fs/hugetlbfs/inode.c:734
         hugetlb_file_setup+0x329/0x9f0 fs/hugetlbfs/inode.c:1282
         newseg+0x422/0xd30 ipc/shm.c:575
         ipcget_new ipc/util.c:285 [inline]
         ipcget+0x21e/0x580 ipc/util.c:639
         SYSC_shmget ipc/shm.c:673 [inline]
         SyS_shmget+0x158/0x230 ipc/shm.c:657
         entry_SYSCALL_64_fastpath+0x1f/0xc2
    
    Analysis provided by Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    
    Link: http://lkml.kernel.org/r/1490477850-7944-1-git-send-email-mike.kravetz@oracle.com
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9c29c31830a4eca724e137a9339137204bbb31be
Author: Prateek Sood <prsood@codeaurora.org>
Date:   Thu Sep 7 20:00:58 2017 +0530

    locking/rwsem-xadd: Fix missed wakeup due to reordering of load
    
    If a spinner is present, there is a chance that the load of
    rwsem_has_spinner() in rwsem_wake() can be reordered with
    respect to decrement of rwsem count in __up_write() leading
    to wakeup being missed:
    
     spinning writer                  up_write caller
     ---------------                  -----------------------
     [S] osq_unlock()                 [L] osq
      spin_lock(wait_lock)
      sem->count=0xFFFFFFFF00000001
                +0xFFFFFFFF00000000
      count=sem->count
      MB
                                       sem->count=0xFFFFFFFE00000001
                                                 -0xFFFFFFFF00000001
                                       spin_trylock(wait_lock)
                                       return
     rwsem_try_write_lock(count)
     spin_unlock(wait_lock)
     schedule()
    
    Reordering of atomic_long_sub_return_release() in __up_write()
    and rwsem_has_spinner() in rwsem_wake() can cause missing of
    wakeup in up_write() context. In spinning writer, sem->count
    and local variable count is 0XFFFFFFFE00000001. It would result
    in rwsem_try_write_lock() failing to acquire rwsem and spinning
    writer going to sleep in rwsem_down_write_failed().
    
    The smp_rmb() will make sure that the spinner state is
    consulted after sem->count is updated in up_write context.
    
    Signed-off-by: Prateek Sood <prsood@codeaurora.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dave@stgolabs.net
    Cc: longman@redhat.com
    Cc: parri.andrea@gmail.com
    Cc: sramana@codeaurora.org
    Link: http://lkml.kernel.org/r/1504794658-15397-1-git-send-email-prsood@codeaurora.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 8d07678e12833213b72804f839c6c662b526977d
Author: Kirill Tkhai <ktkhai@virtuozzo.com>
Date:   Fri May 12 19:11:31 2017 +0300

    pid_ns: Fix race between setns'ed fork() and zap_pid_ns_processes()
    
    commit 3fd37226216620c1a468afa999739d5016fbc349 upstream.
    
    Imagine we have a pid namespace and a task from its parent's pid_ns,
    which made setns() to the pid namespace. The task is doing fork(),
    while the pid namespace's child reaper is dying. We have the race
    between them:
    
    Task from parent pid_ns             Child reaper
    copy_process()                      ..
      alloc_pid()                       ..
      ..                                zap_pid_ns_processes()
      ..                                  disable_pid_allocation()
      ..                                  read_lock(&tasklist_lock)
      ..                                  iterate over pids in pid_ns
      ..                                    kill tasks linked to pids
      ..                                  read_unlock(&tasklist_lock)
      write_lock_irq(&tasklist_lock);   ..
      attach_pid(p, PIDTYPE_PID);       ..
      ..                                ..
    
    So, just created task p won't receive SIGKILL signal,
    and the pid namespace will be in contradictory state.
    Only manual kill will help there, but does the userspace
    care about this? I suppose, the most users just inject
    a task into a pid namespace and wait a SIGCHLD from it.
    
    The patch fixes the problem. It simply checks for
    (pid_ns->nr_hashed & PIDNS_HASH_ADDING) in copy_process().
    We do it under the tasklist_lock, and can't skip
    PIDNS_HASH_ADDING as noted by Oleg:
    
    "zap_pid_ns_processes() does disable_pid_allocation()
    and then takes tasklist_lock to kill the whole namespace.
    Given that copy_process() checks PIDNS_HASH_ADDING
    under write_lock(tasklist) they can't race;
    if copy_process() takes this lock first, the new child will
    be killed, otherwise copy_process() can't miss
    the change in ->nr_hashed."
    
    If allocation is disabled, we just return -ENOMEM
    like it's made for such cases in alloc_pid().
    
    v2: Do not move disable_pid_allocation(), do not
    introduce a new variable in copy_process() and simplify
    the patch as suggested by Oleg Nesterov.
    Account the problem with double irq enabling
    found by Eric W. Biederman.
    
    Fixes: c876ad768215 ("pidns: Stop pid allocation when init dies")
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    CC: Andrew Morton <akpm@linux-foundation.org>
    CC: Ingo Molnar <mingo@kernel.org>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Oleg Nesterov <oleg@redhat.com>
    CC: Mike Rapoport <rppt@linux.vnet.ibm.com>
    CC: Michal Hocko <mhocko@suse.com>
    CC: Andy Lutomirski <luto@kernel.org>
    CC: "Eric W. Biederman" <ebiederm@xmission.com>
    CC: Andrei Vagin <avagin@openvz.org>
    CC: Cyrill Gorcunov <gorcunov@openvz.org>
    CC: Serge Hallyn <serge@hallyn.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    [bwh: Backported to 3.16: the proper cleanup label is bad_fork_free_pid, not
     bad_fork_cancel_cgroup]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 941154bd6937a710ae9193a3c733c0029e5ae7b8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:04 2017 +0200

    watchdog/hardlockup/perf: Prevent CPU hotplug deadlock
    
    The following deadlock is possible in the watchdog hotplug code:
    
      cpus_write_lock()
        ...
          takedown_cpu()
            smpboot_park_threads()
              smpboot_park_thread()
                kthread_park()
                  ->park() := watchdog_disable()
                    watchdog_nmi_disable()
                      perf_event_release_kernel();
                        put_event()
                          _free_event()
                            ->destroy() := hw_perf_event_destroy()
                              x86_release_hardware()
                                release_ds_buffers()
                                  get_online_cpus()
    
    when a per cpu watchdog perf event is destroyed which drops the last
    reference to the PMU hardware. The cleanup code there invokes
    get_online_cpus() which instantly deadlocks because the hotplug percpu
    rwsem is write locked.
    
    To solve this add a deferring mechanism:
    
      cpus_write_lock()
                               kthread_park()
                                watchdog_nmi_disable(deferred)
                                  perf_event_disable(event);
                                  move_event_to_deferred(event);
                               ....
      cpus_write_unlock()
      cleaup_deferred_events()
        perf_event_release_kernel()
    
    This is still properly serialized against concurrent hotplug via the
    cpu_add_remove_lock, which is held by the task which initiated the hotplug
    event.
    
    This is also used to handle event destruction when the watchdog threads are
    parked via other mechanisms than CPU hotplug.
    
    Analyzed-by: Peter Zijlstra <peterz@infradead.org>
    
    Reported-by: Borislav Petkov <bp@alien8.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194146.884469246@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 20d853fd0703b1d73c35a22024c0d4fcbcc57c8c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:03 2017 +0200

    watchdog/hardlockup/perf: Remove broken self disable on failure
    
    The self disabling feature is broken vs. CPU hotplug locking:
    
    CPU 0                      CPU 1
    cpus_write_lock();
     cpu_up(1)
       wait_for_completion()
                               ....
                               unpark_watchdog()
                               ->unpark()
                                 perf_event_create() <- fails
                                   watchdog_enable &= ~NMI_WATCHDOG;
                               ....
    cpus_write_unlock();
                               CPU 2
    cpus_write_lock()
     cpu_down(2)
       wait_for_completion()
                               wakeup(watchdog);
                                 watchdog()
                                 if (!(watchdog_enable & NMI_WATCHDOG))
                                    watchdog_nmi_disable()
                                      perf_event_disable()
                                      ....
                                      cpus_read_lock();
    
                               stop_smpboot_threads()
                                 park_watchdog();
                                   wait_for_completion(watchdog->parked);
    
    Result: End of hotplug and instantaneous full lockup of the machine.
    
    There is a similar problem with disabling the watchdog via the user space
    interface as the sysctl function fiddles with watchdog_enable directly.
    
    It's very debatable whether this is required at all. If the watchdog works
    nicely on N CPUs and it fails to enable on the N + 1 CPU either during
    hotplug or because the user space interface disabled it via sysctl cpumask
    and then some perf user grabbed the counter which is then unavailable for
    the watchdog when the sysctl cpumask gets changed back.
    
    There is no real justification for this.
    
    One of the reasons WHY this is done is the utter stupidity of the init code
    of the perf NMI watchdog. Instead of checking upfront at boot whether PERF
    is available and functional at all, it just does this check at run time
    over and over when user space fiddles with the sysctl. That's broken beyond
    repair along with the idiotic error code dependent warn level printks and
    the even more silly printk rate limiting.
    
    If the init code checks whether perf works at boot time, then this mess can
    be more or less avoided completely. Perf does not come magically into life
    at runtime. Brain usage while coding is overrated.
    
    Remove the cruft and add a temporary safe guard which gets removed later.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194146.806708429@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 29b8ea35b0c1d4d6c4ab4e3c3ccfb774a22a3bb7
Author: Xin Long <lucien.xin@gmail.com>
Date:   Sat Jun 10 14:48:14 2017 +0800

    sctp: disable BH in sctp_for_each_endpoint
    
    
    [ Upstream commit 581409dacc9176b0de1f6c4ca8d66e13aa8e1b29 ]
    
    Now sctp holds read_lock when foreach sctp_ep_hashtable without disabling
    BH. If CPU schedules to another thread A at this moment, the thread A may
    be trying to hold the write_lock with disabling BH.
    
    As BH is disabled and CPU cannot schedule back to the thread holding the
    read_lock, while the thread A keeps waiting for the read_lock. A dead
    lock would be triggered by this.
    
    This patch is to fix this dead lock by calling read_lock_bh instead to
    disable BH when holding the read_lock in sctp_for_each_endpoint.
    
    Fixes: 626d16f50f39 ("sctp: export some apis or variables for sctp_diag and reuse some for proc")
    Reported-by: Xiumei Mu <xmu@redhat.com>
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Acked-by: Marcelo Ricardo Leitner <marcelo.leitner@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8cda426a7cfa61b902c4335d1d1ab945bbcb41b6
Author: Xin Long <lucien.xin@gmail.com>
Date:   Sat Jun 10 14:48:14 2017 +0800

    sctp: disable BH in sctp_for_each_endpoint
    
    
    [ Upstream commit 581409dacc9176b0de1f6c4ca8d66e13aa8e1b29 ]
    
    Now sctp holds read_lock when foreach sctp_ep_hashtable without disabling
    BH. If CPU schedules to another thread A at this moment, the thread A may
    be trying to hold the write_lock with disabling BH.
    
    As BH is disabled and CPU cannot schedule back to the thread holding the
    read_lock, while the thread A keeps waiting for the read_lock. A dead
    lock would be triggered by this.
    
    This patch is to fix this dead lock by calling read_lock_bh instead to
    disable BH when holding the read_lock in sctp_for_each_endpoint.
    
    Fixes: 626d16f50f39 ("sctp: export some apis or variables for sctp_diag and reuse some for proc")
    Reported-by: Xiumei Mu <xmu@redhat.com>
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Acked-by: Marcelo Ricardo Leitner <marcelo.leitner@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 581409dacc9176b0de1f6c4ca8d66e13aa8e1b29
Author: Xin Long <lucien.xin@gmail.com>
Date:   Sat Jun 10 14:48:14 2017 +0800

    sctp: disable BH in sctp_for_each_endpoint
    
    Now sctp holds read_lock when foreach sctp_ep_hashtable without disabling
    BH. If CPU schedules to another thread A at this moment, the thread A may
    be trying to hold the write_lock with disabling BH.
    
    As BH is disabled and CPU cannot schedule back to the thread holding the
    read_lock, while the thread A keeps waiting for the read_lock. A dead
    lock would be triggered by this.
    
    This patch is to fix this dead lock by calling read_lock_bh instead to
    disable BH when holding the read_lock in sctp_for_each_endpoint.
    
    Fixes: 626d16f50f39 ("sctp: export some apis or variables for sctp_diag and reuse some for proc")
    Reported-by: Xiumei Mu <xmu@redhat.com>
    Signed-off-by: Xin Long <lucien.xin@gmail.com>
    Acked-by: Marcelo Ricardo Leitner <marcelo.leitner@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 1c2287b99f5c97cb692c7e30b856e9166d713e1f
Author: Steffen Maier <maier@linux.vnet.ibm.com>
Date:   Fri Dec 9 17:16:33 2016 +0100

    scsi: zfcp: fix rport unblock race with LUN recovery
    
    commit 6f2ce1c6af37191640ee3ff6e8fc39ea10352f4c upstream.
    
    It is unavoidable that zfcp_scsi_queuecommand() has to finish requests
    with DID_IMM_RETRY (like fc_remote_port_chkready()) during the time
    window when zfcp detected an unavailable rport but
    fc_remote_port_delete(), which is asynchronous via
    zfcp_scsi_schedule_rport_block(), has not yet blocked the rport.
    
    However, for the case when the rport becomes available again, we should
    prevent unblocking the rport too early.  In contrast to other FCP LLDDs,
    zfcp has to open each LUN with the FCP channel hardware before it can
    send I/O to a LUN.  So if a port already has LUNs attached and we
    unblock the rport just after port recovery, recoveries of LUNs behind
    this port can still be pending which in turn force
    zfcp_scsi_queuecommand() to unnecessarily finish requests with
    DID_IMM_RETRY.
    
    This also opens a time window with unblocked rport (until the followup
    LUN reopen recovery has finished).  If a scsi_cmnd timeout occurs during
    this time window fc_timed_out() cannot work as desired and such command
    would indeed time out and trigger scsi_eh. This prevents a clean and
    timely path failover.  This should not happen if the path issue can be
    recovered on FC transport layer such as path issues involving RSCNs.
    
    Fix this by only calling zfcp_scsi_schedule_rport_register(), to
    asynchronously trigger fc_remote_port_add(), after all LUN recoveries as
    children of the rport have finished and no new recoveries of equal or
    higher order were triggered meanwhile.  Finished intentionally includes
    any recovery result no matter if successful or failed (still unblock
    rport so other successful LUNs work).  For simplicity, we check after
    each finished LUN recovery if there is another LUN recovery pending on
    the same port and then do nothing.  We handle the special case of a
    successful recovery of a port without LUN children the same way without
    changing this case's semantics.
    
    For debugging we introduce 2 new trace records written if the rport
    unblock attempt was aborted due to still unfinished or freshly triggered
    recovery. The records are only written above the default trace level.
    
    Benjamin noticed the important special case of new recovery that can be
    triggered between having given up the erp_lock and before calling
    zfcp_erp_action_cleanup() within zfcp_erp_strategy().  We must avoid the
    following sequence:
    
    ERP thread                 rport_work      other context
    -------------------------  --------------  --------------------------------
    port is unblocked, rport still blocked,
     due to pending/running ERP action,
     so ((port->status & ...UNBLOCK) != 0)
     and (port->rport == NULL)
    unlock ERP
    zfcp_erp_action_cleanup()
    case ZFCP_ERP_ACTION_REOPEN_LUN:
    zfcp_erp_try_rport_unblock()
    ((status & ...UNBLOCK) != 0) [OLD!]
                                               zfcp_erp_port_reopen()
                                               lock ERP
                                               zfcp_erp_port_block()
                                               port->status clear ...UNBLOCK
                                               unlock ERP
                                               zfcp_scsi_schedule_rport_block()
                                               port->rport_task = RPORT_DEL
                                               queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task != RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_block()
                               if (!port->rport) return
    zfcp_scsi_schedule_rport_register()
    port->rport_task = RPORT_ADD
    queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task == RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_register()
                               (port->rport == NULL)
                               rport = fc_remote_port_add()
                               port->rport = rport;
    
    Now the rport was erroneously unblocked while the zfcp_port is blocked.
    This is another situation we want to avoid due to scsi_eh
    potential. This state would at least remain until the new recovery from
    the other context finished successfully, or potentially forever if it
    failed.  In order to close this race, we take the erp_lock inside
    zfcp_erp_try_rport_unblock() when checking the status of zfcp_port or
    LUN.  With that, the possible corresponding rport state sequences would
    be: (unblock[ERP thread],block[other context]) if the ERP thread gets
    erp_lock first and still sees ((port->status & ...UNBLOCK) != 0),
    (block[other context],NOP[ERP thread]) if the ERP thread gets erp_lock
    after the other context has already cleard ...UNBLOCK from port->status.
    
    Since checking fields of struct erp_action is unsafe because they could
    have been overwritten (re-used for new recovery) meanwhile, we only
    check status of zfcp_port and LUN since these are only changed under
    erp_lock elsewhere. Regarding the check of the proper status flags (port
    or port_forced are similar to the shown adapter recovery):
    
    [zfcp_erp_adapter_shutdown()]
    zfcp_erp_adapter_reopen()
     zfcp_erp_adapter_block()
      * clear UNBLOCK ---------------------------------------+
     zfcp_scsi_schedule_rports_block()                       |
     write_lock_irqsave(&adapter->erp_lock, flags);-------+  |
     zfcp_erp_action_enqueue()                            |  |
      zfcp_erp_setup_act()                                |  |
       * set ERP_INUSE -----------------------------------|--|--+
     write_unlock_irqrestore(&adapter->erp_lock, flags);--+  |  |
    .context-switch.                                         |  |
    zfcp_erp_thread()                                        |  |
     zfcp_erp_strategy()                                     |  |
      write_lock_irqsave(&adapter->erp_lock, flags);------+  |  |
      ...                                                 |  |  |
      zfcp_erp_strategy_check_target()                    |  |  |
       zfcp_erp_strategy_check_adapter()                  |  |  |
        zfcp_erp_adapter_unblock()                        |  |  |
         * set UNBLOCK -----------------------------------|--+  |
      zfcp_erp_action_dequeue()                           |     |
       * clear ERP_INUSE ---------------------------------|-----+
      ...                                                 |
      write_unlock_irqrestore(&adapter->erp_lock, flags);-+
    
    Hence, we should check for both UNBLOCK and ERP_INUSE because they are
    interleaved.  Also we need to explicitly check ERP_FAILED for the link
    down case which currently does not clear the UNBLOCK flag in
    zfcp_fsf_link_down_info_eval().
    
    Signed-off-by: Steffen Maier <maier@linux.vnet.ibm.com>
    Fixes: 8830271c4819 ("[SCSI] zfcp: Dont fail SCSI commands when transitioning to blocked fc_rport")
    Fixes: a2fa0aede07c ("[SCSI] zfcp: Block FC transport rports early on errors")
    Fixes: 5f852be9e11d ("[SCSI] zfcp: Fix deadlock between zfcp ERP and SCSI")
    Fixes: 338151e06608 ("[SCSI] zfcp: make use of fc_remote_port_delete when target port is unavailable")
    Fixes: 3859f6a248cb ("[PATCH] zfcp: add rports to enable scsi_add_device to work again")
    Reviewed-by: Benjamin Block <bblock@linux.vnet.ibm.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Willy Tarreau <w@1wt.eu>

commit 5ea6d691aac6c93b790f0905e3460d44cc4c449b
Author: Jia-Ju Bai <baijiaju1990@163.com>
Date:   Thu Jun 1 16:18:10 2017 +0800

    qlcnic: Fix a sleep-in-atomic bug in qlcnic_82xx_hw_write_wx_2M and qlcnic_82xx_hw_read_wx_2M
    
    The driver may sleep under a write spin lock, and the function
    call path is:
    qlcnic_82xx_hw_write_wx_2M (acquire the lock by write_lock_irqsave)
      crb_win_lock
        qlcnic_pcie_sem_lock
          usleep_range
    qlcnic_82xx_hw_read_wx_2M (acquire the lock by write_lock_irqsave)
      crb_win_lock
        qlcnic_pcie_sem_lock
          usleep_range
    
    To fix it, the usleep_range is replaced with udelay.
    
    Signed-off-by: Jia-Ju Bai <baijiaju1990@163.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 15e4ed2a46587a8e3085299ae443c96459ce8856
Author: Kirill Tkhai <ktkhai@virtuozzo.com>
Date:   Fri May 12 19:11:31 2017 +0300

    pid_ns: Fix race between setns'ed fork() and zap_pid_ns_processes()
    
    commit 3fd37226216620c1a468afa999739d5016fbc349 upstream.
    
    Imagine we have a pid namespace and a task from its parent's pid_ns,
    which made setns() to the pid namespace. The task is doing fork(),
    while the pid namespace's child reaper is dying. We have the race
    between them:
    
    Task from parent pid_ns             Child reaper
    copy_process()                      ..
      alloc_pid()                       ..
      ..                                zap_pid_ns_processes()
      ..                                  disable_pid_allocation()
      ..                                  read_lock(&tasklist_lock)
      ..                                  iterate over pids in pid_ns
      ..                                    kill tasks linked to pids
      ..                                  read_unlock(&tasklist_lock)
      write_lock_irq(&tasklist_lock);   ..
      attach_pid(p, PIDTYPE_PID);       ..
      ..                                ..
    
    So, just created task p won't receive SIGKILL signal,
    and the pid namespace will be in contradictory state.
    Only manual kill will help there, but does the userspace
    care about this? I suppose, the most users just inject
    a task into a pid namespace and wait a SIGCHLD from it.
    
    The patch fixes the problem. It simply checks for
    (pid_ns->nr_hashed & PIDNS_HASH_ADDING) in copy_process().
    We do it under the tasklist_lock, and can't skip
    PIDNS_HASH_ADDING as noted by Oleg:
    
    "zap_pid_ns_processes() does disable_pid_allocation()
    and then takes tasklist_lock to kill the whole namespace.
    Given that copy_process() checks PIDNS_HASH_ADDING
    under write_lock(tasklist) they can't race;
    if copy_process() takes this lock first, the new child will
    be killed, otherwise copy_process() can't miss
    the change in ->nr_hashed."
    
    If allocation is disabled, we just return -ENOMEM
    like it's made for such cases in alloc_pid().
    
    v2: Do not move disable_pid_allocation(), do not
    introduce a new variable in copy_process() and simplify
    the patch as suggested by Oleg Nesterov.
    Account the problem with double irq enabling
    found by Eric W. Biederman.
    
    Fixes: c876ad768215 ("pidns: Stop pid allocation when init dies")
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    CC: Andrew Morton <akpm@linux-foundation.org>
    CC: Ingo Molnar <mingo@kernel.org>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Oleg Nesterov <oleg@redhat.com>
    CC: Mike Rapoport <rppt@linux.vnet.ibm.com>
    CC: Michal Hocko <mhocko@suse.com>
    CC: Andy Lutomirski <luto@kernel.org>
    CC: "Eric W. Biederman" <ebiederm@xmission.com>
    CC: Andrei Vagin <avagin@openvz.org>
    CC: Cyrill Gorcunov <gorcunov@openvz.org>
    CC: Serge Hallyn <serge@hallyn.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2ea2f891fa85a6b8fd2fd6991e16844be39da888
Author: Kirill Tkhai <ktkhai@virtuozzo.com>
Date:   Fri May 12 19:11:31 2017 +0300

    pid_ns: Fix race between setns'ed fork() and zap_pid_ns_processes()
    
    commit 3fd37226216620c1a468afa999739d5016fbc349 upstream.
    
    Imagine we have a pid namespace and a task from its parent's pid_ns,
    which made setns() to the pid namespace. The task is doing fork(),
    while the pid namespace's child reaper is dying. We have the race
    between them:
    
    Task from parent pid_ns             Child reaper
    copy_process()                      ..
      alloc_pid()                       ..
      ..                                zap_pid_ns_processes()
      ..                                  disable_pid_allocation()
      ..                                  read_lock(&tasklist_lock)
      ..                                  iterate over pids in pid_ns
      ..                                    kill tasks linked to pids
      ..                                  read_unlock(&tasklist_lock)
      write_lock_irq(&tasklist_lock);   ..
      attach_pid(p, PIDTYPE_PID);       ..
      ..                                ..
    
    So, just created task p won't receive SIGKILL signal,
    and the pid namespace will be in contradictory state.
    Only manual kill will help there, but does the userspace
    care about this? I suppose, the most users just inject
    a task into a pid namespace and wait a SIGCHLD from it.
    
    The patch fixes the problem. It simply checks for
    (pid_ns->nr_hashed & PIDNS_HASH_ADDING) in copy_process().
    We do it under the tasklist_lock, and can't skip
    PIDNS_HASH_ADDING as noted by Oleg:
    
    "zap_pid_ns_processes() does disable_pid_allocation()
    and then takes tasklist_lock to kill the whole namespace.
    Given that copy_process() checks PIDNS_HASH_ADDING
    under write_lock(tasklist) they can't race;
    if copy_process() takes this lock first, the new child will
    be killed, otherwise copy_process() can't miss
    the change in ->nr_hashed."
    
    If allocation is disabled, we just return -ENOMEM
    like it's made for such cases in alloc_pid().
    
    v2: Do not move disable_pid_allocation(), do not
    introduce a new variable in copy_process() and simplify
    the patch as suggested by Oleg Nesterov.
    Account the problem with double irq enabling
    found by Eric W. Biederman.
    
    Fixes: c876ad768215 ("pidns: Stop pid allocation when init dies")
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    CC: Andrew Morton <akpm@linux-foundation.org>
    CC: Ingo Molnar <mingo@kernel.org>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Oleg Nesterov <oleg@redhat.com>
    CC: Mike Rapoport <rppt@linux.vnet.ibm.com>
    CC: Michal Hocko <mhocko@suse.com>
    CC: Andy Lutomirski <luto@kernel.org>
    CC: "Eric W. Biederman" <ebiederm@xmission.com>
    CC: Andrei Vagin <avagin@openvz.org>
    CC: Cyrill Gorcunov <gorcunov@openvz.org>
    CC: Serge Hallyn <serge@hallyn.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6a70a5833ecc9147d8257e80f39e11d582810082
Author: Kirill Tkhai <ktkhai@virtuozzo.com>
Date:   Fri May 12 19:11:31 2017 +0300

    pid_ns: Fix race between setns'ed fork() and zap_pid_ns_processes()
    
    commit 3fd37226216620c1a468afa999739d5016fbc349 upstream.
    
    Imagine we have a pid namespace and a task from its parent's pid_ns,
    which made setns() to the pid namespace. The task is doing fork(),
    while the pid namespace's child reaper is dying. We have the race
    between them:
    
    Task from parent pid_ns             Child reaper
    copy_process()                      ..
      alloc_pid()                       ..
      ..                                zap_pid_ns_processes()
      ..                                  disable_pid_allocation()
      ..                                  read_lock(&tasklist_lock)
      ..                                  iterate over pids in pid_ns
      ..                                    kill tasks linked to pids
      ..                                  read_unlock(&tasklist_lock)
      write_lock_irq(&tasklist_lock);   ..
      attach_pid(p, PIDTYPE_PID);       ..
      ..                                ..
    
    So, just created task p won't receive SIGKILL signal,
    and the pid namespace will be in contradictory state.
    Only manual kill will help there, but does the userspace
    care about this? I suppose, the most users just inject
    a task into a pid namespace and wait a SIGCHLD from it.
    
    The patch fixes the problem. It simply checks for
    (pid_ns->nr_hashed & PIDNS_HASH_ADDING) in copy_process().
    We do it under the tasklist_lock, and can't skip
    PIDNS_HASH_ADDING as noted by Oleg:
    
    "zap_pid_ns_processes() does disable_pid_allocation()
    and then takes tasklist_lock to kill the whole namespace.
    Given that copy_process() checks PIDNS_HASH_ADDING
    under write_lock(tasklist) they can't race;
    if copy_process() takes this lock first, the new child will
    be killed, otherwise copy_process() can't miss
    the change in ->nr_hashed."
    
    If allocation is disabled, we just return -ENOMEM
    like it's made for such cases in alloc_pid().
    
    v2: Do not move disable_pid_allocation(), do not
    introduce a new variable in copy_process() and simplify
    the patch as suggested by Oleg Nesterov.
    Account the problem with double irq enabling
    found by Eric W. Biederman.
    
    Fixes: c876ad768215 ("pidns: Stop pid allocation when init dies")
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    CC: Andrew Morton <akpm@linux-foundation.org>
    CC: Ingo Molnar <mingo@kernel.org>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Oleg Nesterov <oleg@redhat.com>
    CC: Mike Rapoport <rppt@linux.vnet.ibm.com>
    CC: Michal Hocko <mhocko@suse.com>
    CC: Andy Lutomirski <luto@kernel.org>
    CC: "Eric W. Biederman" <ebiederm@xmission.com>
    CC: Andrei Vagin <avagin@openvz.org>
    CC: Cyrill Gorcunov <gorcunov@openvz.org>
    CC: Serge Hallyn <serge@hallyn.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 925bb1ce47f429f69aad35876df7ecd8c53deb7e
Author: Vegard Nossum <vegard.nossum@oracle.com>
Date:   Thu May 11 12:18:52 2017 +0200

    tty: fix port buffer locking
    
    tty_insert_flip_string_fixed_flag() is racy against itself when called
    from the ioctl(TCXONC, TCION/TCIOFF) path [1] and the flush_to_ldisc()
    workqueue path [2].
    
    The problem is that port->buf.tail->used is modified without consistent
    locking; the ioctl path takes tty->atomic_write_lock, whereas the workqueue
    path takes ldata->output_lock.
    
    We cannot simply take ldata->output_lock, since that is specific to the
    N_TTY line discipline.
    
    It might seem natural to try to take port->buf.lock inside
    tty_insert_flip_string_fixed_flag() and friends (where port->buf is
    actually used/modified), but this creates problems for flush_to_ldisc()
    which takes it before grabbing tty->ldisc_sem, o_tty->termios_rwsem,
    and ldata->output_lock.
    
    Therefore, the simplest solution for now seems to be to take
    tty->atomic_write_lock inside tty_port_default_receive_buf(). This lock
    is also used in the write path [3] with a consistent ordering.
    
    [1]: Call Trace:
     tty_insert_flip_string_fixed_flag
     pty_write
     tty_send_xchar                     // down_read(&o_tty->termios_rwsem)
                                        // mutex_lock(&tty->atomic_write_lock)
     n_tty_ioctl_helper
     n_tty_ioctl
     tty_ioctl                          // down_read(&tty->ldisc_sem)
     do_vfs_ioctl
     SyS_ioctl
    
    [2]: Workqueue: events_unbound flush_to_ldisc
    Call Trace:
     tty_insert_flip_string_fixed_flag
     pty_write
     tty_put_char
     __process_echoes
     commit_echoes                      // mutex_lock(&ldata->output_lock)
     n_tty_receive_buf_common
     n_tty_receive_buf2
     tty_ldisc_receive_buf              // down_read(&o_tty->termios_rwsem)
     tty_port_default_receive_buf       // down_read(&tty->ldisc_sem)
     flush_to_ldisc                     // mutex_lock(&port->buf.lock)
     process_one_work
    
    [3]: Call Trace:
     tty_insert_flip_string_fixed_flag
     pty_write
     n_tty_write                        // mutex_lock(&ldata->output_lock)
                                        // down_read(&tty->termios_rwsem)
     do_tty_write (inline)              // mutex_lock(&tty->atomic_write_lock)
     tty_write                          // down_read(&tty->ldisc_sem)
     __vfs_write
     vfs_write
     SyS_write
    
    The bug can result in about a dozen different crashes depending on what
    exactly gets corrupted when port->buf.tail->used points outside the
    buffer.
    
    The patch passes my LOCKDEP/PROVE_LOCKING testing but more testing is
    always welcome.
    
    Found using syzkaller.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Vegard Nossum <vegard.nossum@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit dec2c92880cc5435381d50e3045ef018a762a917
Author: Dean Jenkins <Dean_Jenkins@mentor.com>
Date:   Fri May 5 16:27:06 2017 +0100

    Bluetooth: hci_ldisc: Use rwlocking to avoid closing proto races
    
    When HCI_UART_PROTO_READY is in the set state, the Data Link protocol
    layer (proto) is bound to the HCI UART driver. This state allows the
    registered proto function pointers to be used by the HCI UART driver.
    
    When unbinding (closing) the Data Link protocol layer, the proto
    function pointers much be prevented from being used immediately before
    running the proto close function pointer. Otherwise, there is a risk
    that a proto non-close function pointer is used during or after the
    proto close function pointer is used. The consequences are likely to
    be a kernel crash because the proto close function pointer will free
    resources used in the Data Link protocol layer.
    
    Therefore, add a reader writer lock (rwlock) solution to prevent the
    close proto function pointer from running by using write_lock_irqsave()
    whilst the other proto function pointers are protected using
    read_lock(). This means HCI_UART_PROTO_READY can safely be cleared
    in the knowledge that no proto function pointers are running.
    
    When flag HCI_UART_PROTO_READY is put into the clear state,
    proto close function pointer can safely be run. Note
    flag HCI_UART_PROTO_SET being in the set state prevents the proto
    open function pointer from being run so there is no race condition
    between proto open and close function pointers.
    
    Signed-off-by: Dean Jenkins <Dean_Jenkins@mentor.com>
    Signed-off-by: Marcel Holtmann <marcel@holtmann.org>

commit 3fd37226216620c1a468afa999739d5016fbc349
Author: Kirill Tkhai <ktkhai@virtuozzo.com>
Date:   Fri May 12 19:11:31 2017 +0300

    pid_ns: Fix race between setns'ed fork() and zap_pid_ns_processes()
    
    Imagine we have a pid namespace and a task from its parent's pid_ns,
    which made setns() to the pid namespace. The task is doing fork(),
    while the pid namespace's child reaper is dying. We have the race
    between them:
    
    Task from parent pid_ns             Child reaper
    copy_process()                      ..
      alloc_pid()                       ..
      ..                                zap_pid_ns_processes()
      ..                                  disable_pid_allocation()
      ..                                  read_lock(&tasklist_lock)
      ..                                  iterate over pids in pid_ns
      ..                                    kill tasks linked to pids
      ..                                  read_unlock(&tasklist_lock)
      write_lock_irq(&tasklist_lock);   ..
      attach_pid(p, PIDTYPE_PID);       ..
      ..                                ..
    
    So, just created task p won't receive SIGKILL signal,
    and the pid namespace will be in contradictory state.
    Only manual kill will help there, but does the userspace
    care about this? I suppose, the most users just inject
    a task into a pid namespace and wait a SIGCHLD from it.
    
    The patch fixes the problem. It simply checks for
    (pid_ns->nr_hashed & PIDNS_HASH_ADDING) in copy_process().
    We do it under the tasklist_lock, and can't skip
    PIDNS_HASH_ADDING as noted by Oleg:
    
    "zap_pid_ns_processes() does disable_pid_allocation()
    and then takes tasklist_lock to kill the whole namespace.
    Given that copy_process() checks PIDNS_HASH_ADDING
    under write_lock(tasklist) they can't race;
    if copy_process() takes this lock first, the new child will
    be killed, otherwise copy_process() can't miss
    the change in ->nr_hashed."
    
    If allocation is disabled, we just return -ENOMEM
    like it's made for such cases in alloc_pid().
    
    v2: Do not move disable_pid_allocation(), do not
    introduce a new variable in copy_process() and simplify
    the patch as suggested by Oleg Nesterov.
    Account the problem with double irq enabling
    found by Eric W. Biederman.
    
    Fixes: c876ad768215 ("pidns: Stop pid allocation when init dies")
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    CC: Andrew Morton <akpm@linux-foundation.org>
    CC: Ingo Molnar <mingo@kernel.org>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Oleg Nesterov <oleg@redhat.com>
    CC: Mike Rapoport <rppt@linux.vnet.ibm.com>
    CC: Michal Hocko <mhocko@suse.com>
    CC: Andy Lutomirski <luto@kernel.org>
    CC: "Eric W. Biederman" <ebiederm@xmission.com>
    CC: Andrei Vagin <avagin@openvz.org>
    CC: Cyrill Gorcunov <gorcunov@openvz.org>
    CC: Serge Hallyn <serge@hallyn.com>
    Cc: stable@vger.kernel.org
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

commit a90dfdc2dea1ce8ab87e3949c3c6bab3c1e36137
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Mon Apr 24 16:51:13 2017 +0900

    scsi: sd: sd_zbc: Rename sd_zbc_setup_write_cmnd
    
    Rename sd_zbc_setup_write_cmnd() to sd_zbc_write_lock_zone() to be clear
    about what the function actually does. To be consistent, also rename
    sd_zbc_cancel_write_cmnd() to sd_zbc_write_unlock_zone().
    
    No functional change is introduced by this patch.
    
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Reviewed-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit f0df317b2bb383301bab159b96f1559f81ca51ef
Author: Stanislaw Gruszka <sgruszka@redhat.com>
Date:   Wed Feb 8 12:18:09 2017 +0100

    rt2x00usb: do not anchor rx and tx urb's
    
    commit 93c7018ec16bb83399dd4db61c361a6d6aba0d5a upstream.
    
    We might kill TX or RX urb during rt2x00usb_flush_entry(), what can
    cause anchor list corruption like shown below:
    
    [ 2074.035633] WARNING: CPU: 2 PID: 14480 at lib/list_debug.c:33 __list_add+0xac/0xc0
    [ 2074.035634] list_add corruption. prev->next should be next (ffff88020f362c28), but was dead000000000100. (prev=ffff8801d161bb70).
    <snip>
    [ 2074.035670] Call Trace:
    [ 2074.035672]  [<ffffffff813bde47>] dump_stack+0x63/0x8c
    [ 2074.035674]  [<ffffffff810a2231>] __warn+0xd1/0xf0
    [ 2074.035676]  [<ffffffff810a22af>] warn_slowpath_fmt+0x5f/0x80
    [ 2074.035678]  [<ffffffffa073855d>] ? rt2x00usb_register_write_lock+0x3d/0x60 [rt2800usb]
    [ 2074.035679]  [<ffffffff813dbe4c>] __list_add+0xac/0xc0
    [ 2074.035681]  [<ffffffff81591c6c>] usb_anchor_urb+0x4c/0xa0
    [ 2074.035683]  [<ffffffffa07322af>] rt2x00usb_kick_rx_entry+0xaf/0x100 [rt2x00usb]
    [ 2074.035684]  [<ffffffffa0732322>] rt2x00usb_clear_entry+0x22/0x30 [rt2x00usb]
    
    To fix do not anchor TX and RX urb's, it is not needed as during
    shutdown we kill those urbs in rt2x00usb_free_entries().
    
    Cc: Vishal Thanki <vishalthanki@gmail.com>
    Fixes: 8b4c0009313f ("rt2x00usb: Use usb anchor to manage URB")
    Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Kalle Valo <kvalo@codeaurora.org>
    Signed-off-by: Amit Pundir <amit.pundir@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 612973c554045ea4ee7901fcd6d8c44b016fa297
Author: Stanislaw Gruszka <sgruszka@redhat.com>
Date:   Wed Feb 8 12:18:09 2017 +0100

    rt2x00usb: do not anchor rx and tx urb's
    
    commit 93c7018ec16bb83399dd4db61c361a6d6aba0d5a upstream.
    
    We might kill TX or RX urb during rt2x00usb_flush_entry(), what can
    cause anchor list corruption like shown below:
    
    [ 2074.035633] WARNING: CPU: 2 PID: 14480 at lib/list_debug.c:33 __list_add+0xac/0xc0
    [ 2074.035634] list_add corruption. prev->next should be next (ffff88020f362c28), but was dead000000000100. (prev=ffff8801d161bb70).
    <snip>
    [ 2074.035670] Call Trace:
    [ 2074.035672]  [<ffffffff813bde47>] dump_stack+0x63/0x8c
    [ 2074.035674]  [<ffffffff810a2231>] __warn+0xd1/0xf0
    [ 2074.035676]  [<ffffffff810a22af>] warn_slowpath_fmt+0x5f/0x80
    [ 2074.035678]  [<ffffffffa073855d>] ? rt2x00usb_register_write_lock+0x3d/0x60 [rt2800usb]
    [ 2074.035679]  [<ffffffff813dbe4c>] __list_add+0xac/0xc0
    [ 2074.035681]  [<ffffffff81591c6c>] usb_anchor_urb+0x4c/0xa0
    [ 2074.035683]  [<ffffffffa07322af>] rt2x00usb_kick_rx_entry+0xaf/0x100 [rt2x00usb]
    [ 2074.035684]  [<ffffffffa0732322>] rt2x00usb_clear_entry+0x22/0x30 [rt2x00usb]
    
    To fix do not anchor TX and RX urb's, it is not needed as during
    shutdown we kill those urbs in rt2x00usb_free_entries().
    
    Cc: Vishal Thanki <vishalthanki@gmail.com>
    Fixes: 8b4c0009313f ("rt2x00usb: Use usb anchor to manage URB")
    Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Kalle Valo <kvalo@codeaurora.org>
    Signed-off-by: Amit Pundir <amit.pundir@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f2b42866b1bb30c1f62a84f6d829b8789495da95
Author: Rabin Vincent <rabinv@axis.com>
Date:   Wed Apr 5 15:14:08 2017 +0200

    MIPS: perf: fix deadlock
    
    mipsxx_pmu_handle_shared_irq() calls irq_work_run() while holding the
    pmuint_rwlock for read.  irq_work_run() can, via perf_pending_event(),
    call try_to_wake_up() which can try to take rq->lock.
    
    However, perf can also call perf_pmu_enable() (and thus take the
    pmuint_rwlock for write) while holding the rq->lock, from
    finish_task_switch() via perf_event_context_sched_in().
    
    This leads to an ABBA deadlock:
    
     PID: 3855   TASK: 8f7ce288  CPU: 2   COMMAND: "process"
      #0 [89c39ac8] __delay at 803b5be4
      #1 [89c39ac8] do_raw_spin_lock at 8008fdcc
      #2 [89c39af8] try_to_wake_up at 8006e47c
      #3 [89c39b38] pollwake at 8018eab0
      #4 [89c39b68] __wake_up_common at 800879f4
      #5 [89c39b98] __wake_up at 800880e4
      #6 [89c39bc8] perf_event_wakeup at 8012109c
      #7 [89c39be8] perf_pending_event at 80121184
      #8 [89c39c08] irq_work_run_list at 801151f0
      #9 [89c39c38] irq_work_run at 80115274
     #10 [89c39c50] mipsxx_pmu_handle_shared_irq at 8002cc7c
    
     PID: 1481   TASK: 8eaac6a8  CPU: 3   COMMAND: "process"
      #0 [8de7f900] do_raw_write_lock at 800900e0
      #1 [8de7f918] perf_event_context_sched_in at 80122310
      #2 [8de7f938] __perf_event_task_sched_in at 80122608
      #3 [8de7f958] finish_task_switch at 8006b8a4
      #4 [8de7f998] __schedule at 805e4dc4
      #5 [8de7f9f8] schedule at 805e5558
      #6 [8de7fa10] schedule_hrtimeout_range_clock at 805e9984
      #7 [8de7fa70] poll_schedule_timeout at 8018e8f8
      #8 [8de7fa88] do_select at 8018f338
      #9 [8de7fd88] core_sys_select at 8018f5cc
     #10 [8de7fee0] sys_select at 8018f854
     #11 [8de7ff28] syscall_common at 80028fc8
    
    The lock seems to be there to protect the hardware counters so there is
    no need to hold it across irq_work_run().
    
    Signed-off-by: Rabin Vincent <rabinv@axis.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 4742a35d9de745e867405b4311e1aac412f0ace1
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Fri Mar 31 15:12:01 2017 -0700

    hugetlbfs: initialize shared policy as part of inode allocation
    
    Any time after inode allocation, destroy_inode can be called.  The
    hugetlbfs inode contains a shared_policy structure, and
    mpol_free_shared_policy is unconditionally called as part of
    hugetlbfs_destroy_inode.  Initialize the policy as part of inode
    allocation so that any quick (error path) calls to destroy_inode will be
    handed an initialized policy.
    
    syzkaller fuzzer found this bug, that resulted in the following:
    
        BUG: KASAN: user-memory-access in atomic_inc
        include/asm-generic/atomic-instrumented.h:87 [inline] at addr
        000000131730bd7a
        BUG: KASAN: user-memory-access in __lock_acquire+0x21a/0x3a80
        kernel/locking/lockdep.c:3239 at addr 000000131730bd7a
        Write of size 4 by task syz-executor6/14086
        CPU: 3 PID: 14086 Comm: syz-executor6 Not tainted 4.11.0-rc3+ #364
        Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011
        Call Trace:
         atomic_inc include/asm-generic/atomic-instrumented.h:87 [inline]
         __lock_acquire+0x21a/0x3a80 kernel/locking/lockdep.c:3239
         lock_acquire+0x1ee/0x590 kernel/locking/lockdep.c:3762
         __raw_write_lock include/linux/rwlock_api_smp.h:210 [inline]
         _raw_write_lock+0x33/0x50 kernel/locking/spinlock.c:295
         mpol_free_shared_policy+0x43/0xb0 mm/mempolicy.c:2536
         hugetlbfs_destroy_inode+0xca/0x120 fs/hugetlbfs/inode.c:952
         alloc_inode+0x10d/0x180 fs/inode.c:216
         new_inode_pseudo+0x69/0x190 fs/inode.c:889
         new_inode+0x1c/0x40 fs/inode.c:918
         hugetlbfs_get_inode+0x40/0x420 fs/hugetlbfs/inode.c:734
         hugetlb_file_setup+0x329/0x9f0 fs/hugetlbfs/inode.c:1282
         newseg+0x422/0xd30 ipc/shm.c:575
         ipcget_new ipc/util.c:285 [inline]
         ipcget+0x21e/0x580 ipc/util.c:639
         SYSC_shmget ipc/shm.c:673 [inline]
         SyS_shmget+0x158/0x230 ipc/shm.c:657
         entry_SYSCALL_64_fastpath+0x1f/0xc2
    
    Analysis provided by Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    
    Link: http://lkml.kernel.org/r/1490477850-7944-1-git-send-email-mike.kravetz@oracle.com
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 044a8ceeb012d956a02cc7587b19a795d4deca79
Author: Steffen Maier <maier@linux.vnet.ibm.com>
Date:   Fri Dec 9 17:16:33 2016 +0100

    scsi: zfcp: fix rport unblock race with LUN recovery
    
    commit 6f2ce1c6af37191640ee3ff6e8fc39ea10352f4c upstream.
    
    It is unavoidable that zfcp_scsi_queuecommand() has to finish requests
    with DID_IMM_RETRY (like fc_remote_port_chkready()) during the time
    window when zfcp detected an unavailable rport but
    fc_remote_port_delete(), which is asynchronous via
    zfcp_scsi_schedule_rport_block(), has not yet blocked the rport.
    
    However, for the case when the rport becomes available again, we should
    prevent unblocking the rport too early.  In contrast to other FCP LLDDs,
    zfcp has to open each LUN with the FCP channel hardware before it can
    send I/O to a LUN.  So if a port already has LUNs attached and we
    unblock the rport just after port recovery, recoveries of LUNs behind
    this port can still be pending which in turn force
    zfcp_scsi_queuecommand() to unnecessarily finish requests with
    DID_IMM_RETRY.
    
    This also opens a time window with unblocked rport (until the followup
    LUN reopen recovery has finished).  If a scsi_cmnd timeout occurs during
    this time window fc_timed_out() cannot work as desired and such command
    would indeed time out and trigger scsi_eh. This prevents a clean and
    timely path failover.  This should not happen if the path issue can be
    recovered on FC transport layer such as path issues involving RSCNs.
    
    Fix this by only calling zfcp_scsi_schedule_rport_register(), to
    asynchronously trigger fc_remote_port_add(), after all LUN recoveries as
    children of the rport have finished and no new recoveries of equal or
    higher order were triggered meanwhile.  Finished intentionally includes
    any recovery result no matter if successful or failed (still unblock
    rport so other successful LUNs work).  For simplicity, we check after
    each finished LUN recovery if there is another LUN recovery pending on
    the same port and then do nothing.  We handle the special case of a
    successful recovery of a port without LUN children the same way without
    changing this case's semantics.
    
    For debugging we introduce 2 new trace records written if the rport
    unblock attempt was aborted due to still unfinished or freshly triggered
    recovery. The records are only written above the default trace level.
    
    Benjamin noticed the important special case of new recovery that can be
    triggered between having given up the erp_lock and before calling
    zfcp_erp_action_cleanup() within zfcp_erp_strategy().  We must avoid the
    following sequence:
    
    ERP thread                 rport_work      other context
    -------------------------  --------------  --------------------------------
    port is unblocked, rport still blocked,
     due to pending/running ERP action,
     so ((port->status & ...UNBLOCK) != 0)
     and (port->rport == NULL)
    unlock ERP
    zfcp_erp_action_cleanup()
    case ZFCP_ERP_ACTION_REOPEN_LUN:
    zfcp_erp_try_rport_unblock()
    ((status & ...UNBLOCK) != 0) [OLD!]
                                               zfcp_erp_port_reopen()
                                               lock ERP
                                               zfcp_erp_port_block()
                                               port->status clear ...UNBLOCK
                                               unlock ERP
                                               zfcp_scsi_schedule_rport_block()
                                               port->rport_task = RPORT_DEL
                                               queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task != RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_block()
                               if (!port->rport) return
    zfcp_scsi_schedule_rport_register()
    port->rport_task = RPORT_ADD
    queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task == RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_register()
                               (port->rport == NULL)
                               rport = fc_remote_port_add()
                               port->rport = rport;
    
    Now the rport was erroneously unblocked while the zfcp_port is blocked.
    This is another situation we want to avoid due to scsi_eh
    potential. This state would at least remain until the new recovery from
    the other context finished successfully, or potentially forever if it
    failed.  In order to close this race, we take the erp_lock inside
    zfcp_erp_try_rport_unblock() when checking the status of zfcp_port or
    LUN.  With that, the possible corresponding rport state sequences would
    be: (unblock[ERP thread],block[other context]) if the ERP thread gets
    erp_lock first and still sees ((port->status & ...UNBLOCK) != 0),
    (block[other context],NOP[ERP thread]) if the ERP thread gets erp_lock
    after the other context has already cleard ...UNBLOCK from port->status.
    
    Since checking fields of struct erp_action is unsafe because they could
    have been overwritten (re-used for new recovery) meanwhile, we only
    check status of zfcp_port and LUN since these are only changed under
    erp_lock elsewhere. Regarding the check of the proper status flags (port
    or port_forced are similar to the shown adapter recovery):
    
    [zfcp_erp_adapter_shutdown()]
    zfcp_erp_adapter_reopen()
     zfcp_erp_adapter_block()
      * clear UNBLOCK ---------------------------------------+
     zfcp_scsi_schedule_rports_block()                       |
     write_lock_irqsave(&adapter->erp_lock, flags);-------+  |
     zfcp_erp_action_enqueue()                            |  |
      zfcp_erp_setup_act()                                |  |
       * set ERP_INUSE -----------------------------------|--|--+
     write_unlock_irqrestore(&adapter->erp_lock, flags);--+  |  |
    .context-switch.                                         |  |
    zfcp_erp_thread()                                        |  |
     zfcp_erp_strategy()                                     |  |
      write_lock_irqsave(&adapter->erp_lock, flags);------+  |  |
      ...                                                 |  |  |
      zfcp_erp_strategy_check_target()                    |  |  |
       zfcp_erp_strategy_check_adapter()                  |  |  |
        zfcp_erp_adapter_unblock()                        |  |  |
         * set UNBLOCK -----------------------------------|--+  |
      zfcp_erp_action_dequeue()                           |     |
       * clear ERP_INUSE ---------------------------------|-----+
      ...                                                 |
      write_unlock_irqrestore(&adapter->erp_lock, flags);-+
    
    Hence, we should check for both UNBLOCK and ERP_INUSE because they are
    interleaved.  Also we need to explicitly check ERP_FAILED for the link
    down case which currently does not clear the UNBLOCK flag in
    zfcp_fsf_link_down_info_eval().
    
    Signed-off-by: Steffen Maier <maier@linux.vnet.ibm.com>
    Fixes: 8830271c4819 ("[SCSI] zfcp: Dont fail SCSI commands when transitioning to blocked fc_rport")
    Fixes: a2fa0aede07c ("[SCSI] zfcp: Block FC transport rports early on errors")
    Fixes: 5f852be9e11d ("[SCSI] zfcp: Fix deadlock between zfcp ERP and SCSI")
    Fixes: 338151e06608 ("[SCSI] zfcp: make use of fc_remote_port_delete when target port is unavailable")
    Fixes: 3859f6a248cb ("[PATCH] zfcp: add rports to enable scsi_add_device to work again")
    Reviewed-by: Benjamin Block <bblock@linux.vnet.ibm.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit fffadf3cf34c662a94d64fa7f3dc7a26063c07bd
Author: Steffen Maier <maier@linux.vnet.ibm.com>
Date:   Fri Dec 9 17:16:33 2016 +0100

    scsi: zfcp: fix rport unblock race with LUN recovery
    
    commit 6f2ce1c6af37191640ee3ff6e8fc39ea10352f4c upstream.
    
    It is unavoidable that zfcp_scsi_queuecommand() has to finish requests
    with DID_IMM_RETRY (like fc_remote_port_chkready()) during the time
    window when zfcp detected an unavailable rport but
    fc_remote_port_delete(), which is asynchronous via
    zfcp_scsi_schedule_rport_block(), has not yet blocked the rport.
    
    However, for the case when the rport becomes available again, we should
    prevent unblocking the rport too early.  In contrast to other FCP LLDDs,
    zfcp has to open each LUN with the FCP channel hardware before it can
    send I/O to a LUN.  So if a port already has LUNs attached and we
    unblock the rport just after port recovery, recoveries of LUNs behind
    this port can still be pending which in turn force
    zfcp_scsi_queuecommand() to unnecessarily finish requests with
    DID_IMM_RETRY.
    
    This also opens a time window with unblocked rport (until the followup
    LUN reopen recovery has finished).  If a scsi_cmnd timeout occurs during
    this time window fc_timed_out() cannot work as desired and such command
    would indeed time out and trigger scsi_eh. This prevents a clean and
    timely path failover.  This should not happen if the path issue can be
    recovered on FC transport layer such as path issues involving RSCNs.
    
    Fix this by only calling zfcp_scsi_schedule_rport_register(), to
    asynchronously trigger fc_remote_port_add(), after all LUN recoveries as
    children of the rport have finished and no new recoveries of equal or
    higher order were triggered meanwhile.  Finished intentionally includes
    any recovery result no matter if successful or failed (still unblock
    rport so other successful LUNs work).  For simplicity, we check after
    each finished LUN recovery if there is another LUN recovery pending on
    the same port and then do nothing.  We handle the special case of a
    successful recovery of a port without LUN children the same way without
    changing this case's semantics.
    
    For debugging we introduce 2 new trace records written if the rport
    unblock attempt was aborted due to still unfinished or freshly triggered
    recovery. The records are only written above the default trace level.
    
    Benjamin noticed the important special case of new recovery that can be
    triggered between having given up the erp_lock and before calling
    zfcp_erp_action_cleanup() within zfcp_erp_strategy().  We must avoid the
    following sequence:
    
    ERP thread                 rport_work      other context
    -------------------------  --------------  --------------------------------
    port is unblocked, rport still blocked,
     due to pending/running ERP action,
     so ((port->status & ...UNBLOCK) != 0)
     and (port->rport == NULL)
    unlock ERP
    zfcp_erp_action_cleanup()
    case ZFCP_ERP_ACTION_REOPEN_LUN:
    zfcp_erp_try_rport_unblock()
    ((status & ...UNBLOCK) != 0) [OLD!]
                                               zfcp_erp_port_reopen()
                                               lock ERP
                                               zfcp_erp_port_block()
                                               port->status clear ...UNBLOCK
                                               unlock ERP
                                               zfcp_scsi_schedule_rport_block()
                                               port->rport_task = RPORT_DEL
                                               queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task != RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_block()
                               if (!port->rport) return
    zfcp_scsi_schedule_rport_register()
    port->rport_task = RPORT_ADD
    queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task == RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_register()
                               (port->rport == NULL)
                               rport = fc_remote_port_add()
                               port->rport = rport;
    
    Now the rport was erroneously unblocked while the zfcp_port is blocked.
    This is another situation we want to avoid due to scsi_eh
    potential. This state would at least remain until the new recovery from
    the other context finished successfully, or potentially forever if it
    failed.  In order to close this race, we take the erp_lock inside
    zfcp_erp_try_rport_unblock() when checking the status of zfcp_port or
    LUN.  With that, the possible corresponding rport state sequences would
    be: (unblock[ERP thread],block[other context]) if the ERP thread gets
    erp_lock first and still sees ((port->status & ...UNBLOCK) != 0),
    (block[other context],NOP[ERP thread]) if the ERP thread gets erp_lock
    after the other context has already cleard ...UNBLOCK from port->status.
    
    Since checking fields of struct erp_action is unsafe because they could
    have been overwritten (re-used for new recovery) meanwhile, we only
    check status of zfcp_port and LUN since these are only changed under
    erp_lock elsewhere. Regarding the check of the proper status flags (port
    or port_forced are similar to the shown adapter recovery):
    
    [zfcp_erp_adapter_shutdown()]
    zfcp_erp_adapter_reopen()
     zfcp_erp_adapter_block()
      * clear UNBLOCK ---------------------------------------+
     zfcp_scsi_schedule_rports_block()                       |
     write_lock_irqsave(&adapter->erp_lock, flags);-------+  |
     zfcp_erp_action_enqueue()                            |  |
      zfcp_erp_setup_act()                                |  |
       * set ERP_INUSE -----------------------------------|--|--+
     write_unlock_irqrestore(&adapter->erp_lock, flags);--+  |  |
    .context-switch.                                         |  |
    zfcp_erp_thread()                                        |  |
     zfcp_erp_strategy()                                     |  |
      write_lock_irqsave(&adapter->erp_lock, flags);------+  |  |
      ...                                                 |  |  |
      zfcp_erp_strategy_check_target()                    |  |  |
       zfcp_erp_strategy_check_adapter()                  |  |  |
        zfcp_erp_adapter_unblock()                        |  |  |
         * set UNBLOCK -----------------------------------|--+  |
      zfcp_erp_action_dequeue()                           |     |
       * clear ERP_INUSE ---------------------------------|-----+
      ...                                                 |
      write_unlock_irqrestore(&adapter->erp_lock, flags);-+
    
    Hence, we should check for both UNBLOCK and ERP_INUSE because they are
    interleaved.  Also we need to explicitly check ERP_FAILED for the link
    down case which currently does not clear the UNBLOCK flag in
    zfcp_fsf_link_down_info_eval().
    
    Signed-off-by: Steffen Maier <maier@linux.vnet.ibm.com>
    Fixes: 8830271c4819 ("[SCSI] zfcp: Dont fail SCSI commands when transitioning to blocked fc_rport")
    Fixes: a2fa0aede07c ("[SCSI] zfcp: Block FC transport rports early on errors")
    Fixes: 5f852be9e11d ("[SCSI] zfcp: Fix deadlock between zfcp ERP and SCSI")
    Fixes: 338151e06608 ("[SCSI] zfcp: make use of fc_remote_port_delete when target port is unavailable")
    Fixes: 3859f6a248cb ("[PATCH] zfcp: add rports to enable scsi_add_device to work again")
    Reviewed-by: Benjamin Block <bblock@linux.vnet.ibm.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit acf50db444f7a9f751365ad0a65cb5cc3ed21f52
Author: Mike Galbraith <efault@gmx.de>
Date:   Mon Aug 13 15:21:23 2012 +0200

    reiserfs: Unlock superblock before calling reiserfs_quota_on_mount()
    
    commit 420902c9d086848a7548c83e0a49021514bd71b7 upstream.
    
    If we hold the superblock lock while calling reiserfs_quota_on_mount(), we can
    deadlock our own worker - mount blocks kworker/3:2, sleeps forever more.
    
    crash> ps|grep UN
        715      2   3  ffff880220734d30  UN   0.0       0      0  [kworker/3:2]
       9369   9341   2  ffff88021ffb7560  UN   1.3  493404 123184  Xorg
       9665   9664   3  ffff880225b92ab0  UN   0.0   47368    812  udisks-daemon
      10635  10403   3  ffff880222f22c70  UN   0.0   14904    936  mount
    crash> bt ffff880220734d30
    PID: 715    TASK: ffff880220734d30  CPU: 3   COMMAND: "kworker/3:2"
     #0 [ffff8802244c3c20] schedule at ffffffff8144584b
     #1 [ffff8802244c3cc8] __rt_mutex_slowlock at ffffffff814472b3
     #2 [ffff8802244c3d28] rt_mutex_slowlock at ffffffff814473f5
     #3 [ffff8802244c3dc8] reiserfs_write_lock at ffffffffa05f28fd [reiserfs]
     #4 [ffff8802244c3de8] flush_async_commits at ffffffffa05ec91d [reiserfs]
     #5 [ffff8802244c3e08] process_one_work at ffffffff81073726
     #6 [ffff8802244c3e68] worker_thread at ffffffff81073eba
     #7 [ffff8802244c3ec8] kthread at ffffffff810782e0
     #8 [ffff8802244c3f48] kernel_thread_helper at ffffffff81450064
    crash> rd ffff8802244c3cc8 10
    ffff8802244c3cc8:  ffffffff814472b3 ffff880222f23250   .rD.....P2."....
    ffff8802244c3cd8:  0000000000000000 0000000000000286   ................
    ffff8802244c3ce8:  ffff8802244c3d30 ffff880220734d80   0=L$.....Ms ....
    ffff8802244c3cf8:  ffff880222e8f628 0000000000000000   (.."............
    ffff8802244c3d08:  0000000000000000 0000000000000002   ................
    crash> struct rt_mutex ffff880222e8f628
    struct rt_mutex {
      wait_lock = {
        raw_lock = {
          slock = 65537
        }
      },
      wait_list = {
        node_list = {
          next = 0xffff8802244c3d48,
          prev = 0xffff8802244c3d48
        }
      },
      owner = 0xffff880222f22c71,
      save_state = 0
    }
    crash> bt 0xffff880222f22c70
    PID: 10635  TASK: ffff880222f22c70  CPU: 3   COMMAND: "mount"
     #0 [ffff8802216a9868] schedule at ffffffff8144584b
     #1 [ffff8802216a9910] schedule_timeout at ffffffff81446865
     #2 [ffff8802216a99a0] wait_for_common at ffffffff81445f74
     #3 [ffff8802216a9a30] flush_work at ffffffff810712d3
     #4 [ffff8802216a9ab0] schedule_on_each_cpu at ffffffff81074463
     #5 [ffff8802216a9ae0] invalidate_bdev at ffffffff81178aba
     #6 [ffff8802216a9af0] vfs_load_quota_inode at ffffffff811a3632
     #7 [ffff8802216a9b50] dquot_quota_on_mount at ffffffff811a375c
     #8 [ffff8802216a9b80] finish_unfinished at ffffffffa05dd8b0 [reiserfs]
     #9 [ffff8802216a9cc0] reiserfs_fill_super at ffffffffa05de825 [reiserfs]
        RIP: 00007f7b9303997a  RSP: 00007ffff443c7a8  RFLAGS: 00010202
        RAX: 00000000000000a5  RBX: ffffffff8144ef12  RCX: 00007f7b932e9ee0
        RDX: 00007f7b93d9a400  RSI: 00007f7b93d9a3e0  RDI: 00007f7b93d9a3c0
        RBP: 00007f7b93d9a2c0   R8: 00007f7b93d9a550   R9: 0000000000000001
        R10: ffffffffc0ed040e  R11: 0000000000000202  R12: 000000000000040e
        R13: 0000000000000000  R14: 00000000c0ed040e  R15: 00007ffff443ca20
        ORIG_RAX: 00000000000000a5  CS: 0033  SS: 002b
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Mike Galbraith <mgalbraith@suse.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 675e634a9786ce753f58afd8f84dcdd637c29f2e
Author: Mike Galbraith <efault@gmx.de>
Date:   Mon Aug 13 15:21:23 2012 +0200

    reiserfs: Unlock superblock before calling reiserfs_quota_on_mount()
    
    commit 420902c9d086848a7548c83e0a49021514bd71b7 upstream.
    
    If we hold the superblock lock while calling reiserfs_quota_on_mount(), we can
    deadlock our own worker - mount blocks kworker/3:2, sleeps forever more.
    
    crash> ps|grep UN
        715      2   3  ffff880220734d30  UN   0.0       0      0  [kworker/3:2]
       9369   9341   2  ffff88021ffb7560  UN   1.3  493404 123184  Xorg
       9665   9664   3  ffff880225b92ab0  UN   0.0   47368    812  udisks-daemon
      10635  10403   3  ffff880222f22c70  UN   0.0   14904    936  mount
    crash> bt ffff880220734d30
    PID: 715    TASK: ffff880220734d30  CPU: 3   COMMAND: "kworker/3:2"
     #0 [ffff8802244c3c20] schedule at ffffffff8144584b
     #1 [ffff8802244c3cc8] __rt_mutex_slowlock at ffffffff814472b3
     #2 [ffff8802244c3d28] rt_mutex_slowlock at ffffffff814473f5
     #3 [ffff8802244c3dc8] reiserfs_write_lock at ffffffffa05f28fd [reiserfs]
     #4 [ffff8802244c3de8] flush_async_commits at ffffffffa05ec91d [reiserfs]
     #5 [ffff8802244c3e08] process_one_work at ffffffff81073726
     #6 [ffff8802244c3e68] worker_thread at ffffffff81073eba
     #7 [ffff8802244c3ec8] kthread at ffffffff810782e0
     #8 [ffff8802244c3f48] kernel_thread_helper at ffffffff81450064
    crash> rd ffff8802244c3cc8 10
    ffff8802244c3cc8:  ffffffff814472b3 ffff880222f23250   .rD.....P2."....
    ffff8802244c3cd8:  0000000000000000 0000000000000286   ................
    ffff8802244c3ce8:  ffff8802244c3d30 ffff880220734d80   0=L$.....Ms ....
    ffff8802244c3cf8:  ffff880222e8f628 0000000000000000   (.."............
    ffff8802244c3d08:  0000000000000000 0000000000000002   ................
    crash> struct rt_mutex ffff880222e8f628
    struct rt_mutex {
      wait_lock = {
        raw_lock = {
          slock = 65537
        }
      },
      wait_list = {
        node_list = {
          next = 0xffff8802244c3d48,
          prev = 0xffff8802244c3d48
        }
      },
      owner = 0xffff880222f22c71,
      save_state = 0
    }
    crash> bt 0xffff880222f22c70
    PID: 10635  TASK: ffff880222f22c70  CPU: 3   COMMAND: "mount"
     #0 [ffff8802216a9868] schedule at ffffffff8144584b
     #1 [ffff8802216a9910] schedule_timeout at ffffffff81446865
     #2 [ffff8802216a99a0] wait_for_common at ffffffff81445f74
     #3 [ffff8802216a9a30] flush_work at ffffffff810712d3
     #4 [ffff8802216a9ab0] schedule_on_each_cpu at ffffffff81074463
     #5 [ffff8802216a9ae0] invalidate_bdev at ffffffff81178aba
     #6 [ffff8802216a9af0] vfs_load_quota_inode at ffffffff811a3632
     #7 [ffff8802216a9b50] dquot_quota_on_mount at ffffffff811a375c
     #8 [ffff8802216a9b80] finish_unfinished at ffffffffa05dd8b0 [reiserfs]
     #9 [ffff8802216a9cc0] reiserfs_fill_super at ffffffffa05de825 [reiserfs]
        RIP: 00007f7b9303997a  RSP: 00007ffff443c7a8  RFLAGS: 00010202
        RAX: 00000000000000a5  RBX: ffffffff8144ef12  RCX: 00007f7b932e9ee0
        RDX: 00007f7b93d9a400  RSI: 00007f7b93d9a3e0  RDI: 00007f7b93d9a3c0
        RBP: 00007f7b93d9a2c0   R8: 00007f7b93d9a550   R9: 0000000000000001
        R10: ffffffffc0ed040e  R11: 0000000000000202  R12: 000000000000040e
        R13: 0000000000000000  R14: 00000000c0ed040e  R15: 00007ffff443ca20
        ORIG_RAX: 00000000000000a5  CS: 0033  SS: 002b
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Mike Galbraith <mgalbraith@suse.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit ee89427388b9ffde431d0331a157620f95b7dd2b
Author: Mike Galbraith <efault@gmx.de>
Date:   Mon Aug 13 15:21:23 2012 +0200

    reiserfs: Unlock superblock before calling reiserfs_quota_on_mount()
    
    commit 420902c9d086848a7548c83e0a49021514bd71b7 upstream.
    
    If we hold the superblock lock while calling reiserfs_quota_on_mount(), we can
    deadlock our own worker - mount blocks kworker/3:2, sleeps forever more.
    
    crash> ps|grep UN
        715      2   3  ffff880220734d30  UN   0.0       0      0  [kworker/3:2]
       9369   9341   2  ffff88021ffb7560  UN   1.3  493404 123184  Xorg
       9665   9664   3  ffff880225b92ab0  UN   0.0   47368    812  udisks-daemon
      10635  10403   3  ffff880222f22c70  UN   0.0   14904    936  mount
    crash> bt ffff880220734d30
    PID: 715    TASK: ffff880220734d30  CPU: 3   COMMAND: "kworker/3:2"
     #0 [ffff8802244c3c20] schedule at ffffffff8144584b
     #1 [ffff8802244c3cc8] __rt_mutex_slowlock at ffffffff814472b3
     #2 [ffff8802244c3d28] rt_mutex_slowlock at ffffffff814473f5
     #3 [ffff8802244c3dc8] reiserfs_write_lock at ffffffffa05f28fd [reiserfs]
     #4 [ffff8802244c3de8] flush_async_commits at ffffffffa05ec91d [reiserfs]
     #5 [ffff8802244c3e08] process_one_work at ffffffff81073726
     #6 [ffff8802244c3e68] worker_thread at ffffffff81073eba
     #7 [ffff8802244c3ec8] kthread at ffffffff810782e0
     #8 [ffff8802244c3f48] kernel_thread_helper at ffffffff81450064
    crash> rd ffff8802244c3cc8 10
    ffff8802244c3cc8:  ffffffff814472b3 ffff880222f23250   .rD.....P2."....
    ffff8802244c3cd8:  0000000000000000 0000000000000286   ................
    ffff8802244c3ce8:  ffff8802244c3d30 ffff880220734d80   0=L$.....Ms ....
    ffff8802244c3cf8:  ffff880222e8f628 0000000000000000   (.."............
    ffff8802244c3d08:  0000000000000000 0000000000000002   ................
    crash> struct rt_mutex ffff880222e8f628
    struct rt_mutex {
      wait_lock = {
        raw_lock = {
          slock = 65537
        }
      },
      wait_list = {
        node_list = {
          next = 0xffff8802244c3d48,
          prev = 0xffff8802244c3d48
        }
      },
      owner = 0xffff880222f22c71,
      save_state = 0
    }
    crash> bt 0xffff880222f22c70
    PID: 10635  TASK: ffff880222f22c70  CPU: 3   COMMAND: "mount"
     #0 [ffff8802216a9868] schedule at ffffffff8144584b
     #1 [ffff8802216a9910] schedule_timeout at ffffffff81446865
     #2 [ffff8802216a99a0] wait_for_common at ffffffff81445f74
     #3 [ffff8802216a9a30] flush_work at ffffffff810712d3
     #4 [ffff8802216a9ab0] schedule_on_each_cpu at ffffffff81074463
     #5 [ffff8802216a9ae0] invalidate_bdev at ffffffff81178aba
     #6 [ffff8802216a9af0] vfs_load_quota_inode at ffffffff811a3632
     #7 [ffff8802216a9b50] dquot_quota_on_mount at ffffffff811a375c
     #8 [ffff8802216a9b80] finish_unfinished at ffffffffa05dd8b0 [reiserfs]
     #9 [ffff8802216a9cc0] reiserfs_fill_super at ffffffffa05de825 [reiserfs]
        RIP: 00007f7b9303997a  RSP: 00007ffff443c7a8  RFLAGS: 00010202
        RAX: 00000000000000a5  RBX: ffffffff8144ef12  RCX: 00007f7b932e9ee0
        RDX: 00007f7b93d9a400  RSI: 00007f7b93d9a3e0  RDI: 00007f7b93d9a3c0
        RBP: 00007f7b93d9a2c0   R8: 00007f7b93d9a550   R9: 0000000000000001
        R10: ffffffffc0ed040e  R11: 0000000000000202  R12: 000000000000040e
        R13: 0000000000000000  R14: 00000000c0ed040e  R15: 00007ffff443ca20
        ORIG_RAX: 00000000000000a5  CS: 0033  SS: 002b
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Mike Galbraith <mgalbraith@suse.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Willy Tarreau <w@1wt.eu>

commit 93c7018ec16bb83399dd4db61c361a6d6aba0d5a
Author: Stanislaw Gruszka <sgruszka@redhat.com>
Date:   Wed Feb 8 12:18:09 2017 +0100

    rt2x00usb: do not anchor rx and tx urb's
    
    We might kill TX or RX urb during rt2x00usb_flush_entry(), what can
    cause anchor list corruption like shown below:
    
    [ 2074.035633] WARNING: CPU: 2 PID: 14480 at lib/list_debug.c:33 __list_add+0xac/0xc0
    [ 2074.035634] list_add corruption. prev->next should be next (ffff88020f362c28), but was dead000000000100. (prev=ffff8801d161bb70).
    <snip>
    [ 2074.035670] Call Trace:
    [ 2074.035672]  [<ffffffff813bde47>] dump_stack+0x63/0x8c
    [ 2074.035674]  [<ffffffff810a2231>] __warn+0xd1/0xf0
    [ 2074.035676]  [<ffffffff810a22af>] warn_slowpath_fmt+0x5f/0x80
    [ 2074.035678]  [<ffffffffa073855d>] ? rt2x00usb_register_write_lock+0x3d/0x60 [rt2800usb]
    [ 2074.035679]  [<ffffffff813dbe4c>] __list_add+0xac/0xc0
    [ 2074.035681]  [<ffffffff81591c6c>] usb_anchor_urb+0x4c/0xa0
    [ 2074.035683]  [<ffffffffa07322af>] rt2x00usb_kick_rx_entry+0xaf/0x100 [rt2x00usb]
    [ 2074.035684]  [<ffffffffa0732322>] rt2x00usb_clear_entry+0x22/0x30 [rt2x00usb]
    
    To fix do not anchor TX and RX urb's, it is not needed as during
    shutdown we kill those urbs in rt2x00usb_free_entries().
    
    Cc: Vishal Thanki <vishalthanki@gmail.com>
    Fixes: 8b4c0009313f ("rt2x00usb: Use usb anchor to manage URB")
    Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Kalle Valo <kvalo@codeaurora.org>

commit dbfcef6b0f4012c57bc0b6e0e660d5ed12a5eaed
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Wed Feb 1 20:49:35 2017 -0500

    jbd2: fix use after free in kjournald2()
    
    Below is the synchronization issue between unmount and kjournald2
    contexts, which results into use after free issue in kjournald2().
    Fix this issue by using journal->j_state_lock to synchronize the
    wait_event() done in journal_kill_thread() and the wake_up() done
    in kjournald2().
    
    TASK 1:
    umount cmd:
       |--jbd2_journal_destroy() {
           |--journal_kill_thread() {
                write_lock(&journal->j_state_lock);
                journal->j_flags |= JBD2_UNMOUNT;
                ...
                write_unlock(&journal->j_state_lock);
                wake_up(&journal->j_wait_commit);      TASK 2 wakes up here:
                                                       kjournald2() {
                                                         ...
                                                         checks JBD2_UNMOUNT flag and calls goto end-loop;
                                                         ...
                                                         end_loop:
                                                           write_unlock(&journal->j_state_lock);
                                                           journal->j_task = NULL; --> If this thread gets
                                                           pre-empted here, then TASK 1 wait_event will
                                                           exit even before this thread is completely
                                                           done.
                wait_event(journal->j_wait_done_commit, journal->j_task == NULL);
                ...
                write_lock(&journal->j_state_lock);
                write_unlock(&journal->j_state_lock);
              }
           |--kfree(journal);
         }
    }
                                                           wake_up(&journal->j_wait_done_commit); --> this step
                                                           now results into use after free issue.
                                                       }
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

commit 197663e65f08e1b6a18982984a0ae1559a76df7d
Author: Steffen Maier <maier@linux.vnet.ibm.com>
Date:   Fri Dec 9 17:16:33 2016 +0100

    scsi: zfcp: fix rport unblock race with LUN recovery
    
    commit 6f2ce1c6af37191640ee3ff6e8fc39ea10352f4c upstream.
    
    It is unavoidable that zfcp_scsi_queuecommand() has to finish requests
    with DID_IMM_RETRY (like fc_remote_port_chkready()) during the time
    window when zfcp detected an unavailable rport but
    fc_remote_port_delete(), which is asynchronous via
    zfcp_scsi_schedule_rport_block(), has not yet blocked the rport.
    
    However, for the case when the rport becomes available again, we should
    prevent unblocking the rport too early.  In contrast to other FCP LLDDs,
    zfcp has to open each LUN with the FCP channel hardware before it can
    send I/O to a LUN.  So if a port already has LUNs attached and we
    unblock the rport just after port recovery, recoveries of LUNs behind
    this port can still be pending which in turn force
    zfcp_scsi_queuecommand() to unnecessarily finish requests with
    DID_IMM_RETRY.
    
    This also opens a time window with unblocked rport (until the followup
    LUN reopen recovery has finished).  If a scsi_cmnd timeout occurs during
    this time window fc_timed_out() cannot work as desired and such command
    would indeed time out and trigger scsi_eh. This prevents a clean and
    timely path failover.  This should not happen if the path issue can be
    recovered on FC transport layer such as path issues involving RSCNs.
    
    Fix this by only calling zfcp_scsi_schedule_rport_register(), to
    asynchronously trigger fc_remote_port_add(), after all LUN recoveries as
    children of the rport have finished and no new recoveries of equal or
    higher order were triggered meanwhile.  Finished intentionally includes
    any recovery result no matter if successful or failed (still unblock
    rport so other successful LUNs work).  For simplicity, we check after
    each finished LUN recovery if there is another LUN recovery pending on
    the same port and then do nothing.  We handle the special case of a
    successful recovery of a port without LUN children the same way without
    changing this case's semantics.
    
    For debugging we introduce 2 new trace records written if the rport
    unblock attempt was aborted due to still unfinished or freshly triggered
    recovery. The records are only written above the default trace level.
    
    Benjamin noticed the important special case of new recovery that can be
    triggered between having given up the erp_lock and before calling
    zfcp_erp_action_cleanup() within zfcp_erp_strategy().  We must avoid the
    following sequence:
    
    ERP thread                 rport_work      other context
    -------------------------  --------------  --------------------------------
    port is unblocked, rport still blocked,
     due to pending/running ERP action,
     so ((port->status & ...UNBLOCK) != 0)
     and (port->rport == NULL)
    unlock ERP
    zfcp_erp_action_cleanup()
    case ZFCP_ERP_ACTION_REOPEN_LUN:
    zfcp_erp_try_rport_unblock()
    ((status & ...UNBLOCK) != 0) [OLD!]
                                               zfcp_erp_port_reopen()
                                               lock ERP
                                               zfcp_erp_port_block()
                                               port->status clear ...UNBLOCK
                                               unlock ERP
                                               zfcp_scsi_schedule_rport_block()
                                               port->rport_task = RPORT_DEL
                                               queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task != RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_block()
                               if (!port->rport) return
    zfcp_scsi_schedule_rport_register()
    port->rport_task = RPORT_ADD
    queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task == RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_register()
                               (port->rport == NULL)
                               rport = fc_remote_port_add()
                               port->rport = rport;
    
    Now the rport was erroneously unblocked while the zfcp_port is blocked.
    This is another situation we want to avoid due to scsi_eh
    potential. This state would at least remain until the new recovery from
    the other context finished successfully, or potentially forever if it
    failed.  In order to close this race, we take the erp_lock inside
    zfcp_erp_try_rport_unblock() when checking the status of zfcp_port or
    LUN.  With that, the possible corresponding rport state sequences would
    be: (unblock[ERP thread],block[other context]) if the ERP thread gets
    erp_lock first and still sees ((port->status & ...UNBLOCK) != 0),
    (block[other context],NOP[ERP thread]) if the ERP thread gets erp_lock
    after the other context has already cleard ...UNBLOCK from port->status.
    
    Since checking fields of struct erp_action is unsafe because they could
    have been overwritten (re-used for new recovery) meanwhile, we only
    check status of zfcp_port and LUN since these are only changed under
    erp_lock elsewhere. Regarding the check of the proper status flags (port
    or port_forced are similar to the shown adapter recovery):
    
    [zfcp_erp_adapter_shutdown()]
    zfcp_erp_adapter_reopen()
     zfcp_erp_adapter_block()
      * clear UNBLOCK ---------------------------------------+
     zfcp_scsi_schedule_rports_block()                       |
     write_lock_irqsave(&adapter->erp_lock, flags);-------+  |
     zfcp_erp_action_enqueue()                            |  |
      zfcp_erp_setup_act()                                |  |
       * set ERP_INUSE -----------------------------------|--|--+
     write_unlock_irqrestore(&adapter->erp_lock, flags);--+  |  |
    .context-switch.                                         |  |
    zfcp_erp_thread()                                        |  |
     zfcp_erp_strategy()                                     |  |
      write_lock_irqsave(&adapter->erp_lock, flags);------+  |  |
      ...                                                 |  |  |
      zfcp_erp_strategy_check_target()                    |  |  |
       zfcp_erp_strategy_check_adapter()                  |  |  |
        zfcp_erp_adapter_unblock()                        |  |  |
         * set UNBLOCK -----------------------------------|--+  |
      zfcp_erp_action_dequeue()                           |     |
       * clear ERP_INUSE ---------------------------------|-----+
      ...                                                 |
      write_unlock_irqrestore(&adapter->erp_lock, flags);-+
    
    Hence, we should check for both UNBLOCK and ERP_INUSE because they are
    interleaved.  Also we need to explicitly check ERP_FAILED for the link
    down case which currently does not clear the UNBLOCK flag in
    zfcp_fsf_link_down_info_eval().
    
    Signed-off-by: Steffen Maier <maier@linux.vnet.ibm.com>
    Fixes: 8830271c4819 ("[SCSI] zfcp: Dont fail SCSI commands when transitioning to blocked fc_rport")
    Fixes: a2fa0aede07c ("[SCSI] zfcp: Block FC transport rports early on errors")
    Fixes: 5f852be9e11d ("[SCSI] zfcp: Fix deadlock between zfcp ERP and SCSI")
    Fixes: 338151e06608 ("[SCSI] zfcp: make use of fc_remote_port_delete when target port is unavailable")
    Fixes: 3859f6a248cb ("[PATCH] zfcp: add rports to enable scsi_add_device to work again")
    Reviewed-by: Benjamin Block <bblock@linux.vnet.ibm.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit 2a940b853ef679cd981e38f23c756d4335a3375c
Author: Steffen Maier <maier@linux.vnet.ibm.com>
Date:   Fri Dec 9 17:16:33 2016 +0100

    scsi: zfcp: fix rport unblock race with LUN recovery
    
    commit 6f2ce1c6af37191640ee3ff6e8fc39ea10352f4c upstream.
    
    It is unavoidable that zfcp_scsi_queuecommand() has to finish requests
    with DID_IMM_RETRY (like fc_remote_port_chkready()) during the time
    window when zfcp detected an unavailable rport but
    fc_remote_port_delete(), which is asynchronous via
    zfcp_scsi_schedule_rport_block(), has not yet blocked the rport.
    
    However, for the case when the rport becomes available again, we should
    prevent unblocking the rport too early.  In contrast to other FCP LLDDs,
    zfcp has to open each LUN with the FCP channel hardware before it can
    send I/O to a LUN.  So if a port already has LUNs attached and we
    unblock the rport just after port recovery, recoveries of LUNs behind
    this port can still be pending which in turn force
    zfcp_scsi_queuecommand() to unnecessarily finish requests with
    DID_IMM_RETRY.
    
    This also opens a time window with unblocked rport (until the followup
    LUN reopen recovery has finished).  If a scsi_cmnd timeout occurs during
    this time window fc_timed_out() cannot work as desired and such command
    would indeed time out and trigger scsi_eh. This prevents a clean and
    timely path failover.  This should not happen if the path issue can be
    recovered on FC transport layer such as path issues involving RSCNs.
    
    Fix this by only calling zfcp_scsi_schedule_rport_register(), to
    asynchronously trigger fc_remote_port_add(), after all LUN recoveries as
    children of the rport have finished and no new recoveries of equal or
    higher order were triggered meanwhile.  Finished intentionally includes
    any recovery result no matter if successful or failed (still unblock
    rport so other successful LUNs work).  For simplicity, we check after
    each finished LUN recovery if there is another LUN recovery pending on
    the same port and then do nothing.  We handle the special case of a
    successful recovery of a port without LUN children the same way without
    changing this case's semantics.
    
    For debugging we introduce 2 new trace records written if the rport
    unblock attempt was aborted due to still unfinished or freshly triggered
    recovery. The records are only written above the default trace level.
    
    Benjamin noticed the important special case of new recovery that can be
    triggered between having given up the erp_lock and before calling
    zfcp_erp_action_cleanup() within zfcp_erp_strategy().  We must avoid the
    following sequence:
    
    ERP thread                 rport_work      other context
    -------------------------  --------------  --------------------------------
    port is unblocked, rport still blocked,
     due to pending/running ERP action,
     so ((port->status & ...UNBLOCK) != 0)
     and (port->rport == NULL)
    unlock ERP
    zfcp_erp_action_cleanup()
    case ZFCP_ERP_ACTION_REOPEN_LUN:
    zfcp_erp_try_rport_unblock()
    ((status & ...UNBLOCK) != 0) [OLD!]
                                               zfcp_erp_port_reopen()
                                               lock ERP
                                               zfcp_erp_port_block()
                                               port->status clear ...UNBLOCK
                                               unlock ERP
                                               zfcp_scsi_schedule_rport_block()
                                               port->rport_task = RPORT_DEL
                                               queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task != RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_block()
                               if (!port->rport) return
    zfcp_scsi_schedule_rport_register()
    port->rport_task = RPORT_ADD
    queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task == RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_register()
                               (port->rport == NULL)
                               rport = fc_remote_port_add()
                               port->rport = rport;
    
    Now the rport was erroneously unblocked while the zfcp_port is blocked.
    This is another situation we want to avoid due to scsi_eh
    potential. This state would at least remain until the new recovery from
    the other context finished successfully, or potentially forever if it
    failed.  In order to close this race, we take the erp_lock inside
    zfcp_erp_try_rport_unblock() when checking the status of zfcp_port or
    LUN.  With that, the possible corresponding rport state sequences would
    be: (unblock[ERP thread],block[other context]) if the ERP thread gets
    erp_lock first and still sees ((port->status & ...UNBLOCK) != 0),
    (block[other context],NOP[ERP thread]) if the ERP thread gets erp_lock
    after the other context has already cleard ...UNBLOCK from port->status.
    
    Since checking fields of struct erp_action is unsafe because they could
    have been overwritten (re-used for new recovery) meanwhile, we only
    check status of zfcp_port and LUN since these are only changed under
    erp_lock elsewhere. Regarding the check of the proper status flags (port
    or port_forced are similar to the shown adapter recovery):
    
    [zfcp_erp_adapter_shutdown()]
    zfcp_erp_adapter_reopen()
     zfcp_erp_adapter_block()
      * clear UNBLOCK ---------------------------------------+
     zfcp_scsi_schedule_rports_block()                       |
     write_lock_irqsave(&adapter->erp_lock, flags);-------+  |
     zfcp_erp_action_enqueue()                            |  |
      zfcp_erp_setup_act()                                |  |
       * set ERP_INUSE -----------------------------------|--|--+
     write_unlock_irqrestore(&adapter->erp_lock, flags);--+  |  |
    .context-switch.                                         |  |
    zfcp_erp_thread()                                        |  |
     zfcp_erp_strategy()                                     |  |
      write_lock_irqsave(&adapter->erp_lock, flags);------+  |  |
      ...                                                 |  |  |
      zfcp_erp_strategy_check_target()                    |  |  |
       zfcp_erp_strategy_check_adapter()                  |  |  |
        zfcp_erp_adapter_unblock()                        |  |  |
         * set UNBLOCK -----------------------------------|--+  |
      zfcp_erp_action_dequeue()                           |     |
       * clear ERP_INUSE ---------------------------------|-----+
      ...                                                 |
      write_unlock_irqrestore(&adapter->erp_lock, flags);-+
    
    Hence, we should check for both UNBLOCK and ERP_INUSE because they are
    interleaved.  Also we need to explicitly check ERP_FAILED for the link
    down case which currently does not clear the UNBLOCK flag in
    zfcp_fsf_link_down_info_eval().
    
    Signed-off-by: Steffen Maier <maier@linux.vnet.ibm.com>
    Fixes: 8830271c4819 ("[SCSI] zfcp: Dont fail SCSI commands when transitioning to blocked fc_rport")
    Fixes: a2fa0aede07c ("[SCSI] zfcp: Block FC transport rports early on errors")
    Fixes: 5f852be9e11d ("[SCSI] zfcp: Fix deadlock between zfcp ERP and SCSI")
    Fixes: 338151e06608 ("[SCSI] zfcp: make use of fc_remote_port_delete when target port is unavailable")
    Fixes: 3859f6a248cb ("[PATCH] zfcp: add rports to enable scsi_add_device to work again")
    Reviewed-by: Benjamin Block <bblock@linux.vnet.ibm.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6d675dffd425a68e6647e77647eb9976fd6868a1
Author: Steffen Maier <maier@linux.vnet.ibm.com>
Date:   Fri Dec 9 17:16:33 2016 +0100

    scsi: zfcp: fix rport unblock race with LUN recovery
    
    commit 6f2ce1c6af37191640ee3ff6e8fc39ea10352f4c upstream.
    
    It is unavoidable that zfcp_scsi_queuecommand() has to finish requests
    with DID_IMM_RETRY (like fc_remote_port_chkready()) during the time
    window when zfcp detected an unavailable rport but
    fc_remote_port_delete(), which is asynchronous via
    zfcp_scsi_schedule_rport_block(), has not yet blocked the rport.
    
    However, for the case when the rport becomes available again, we should
    prevent unblocking the rport too early.  In contrast to other FCP LLDDs,
    zfcp has to open each LUN with the FCP channel hardware before it can
    send I/O to a LUN.  So if a port already has LUNs attached and we
    unblock the rport just after port recovery, recoveries of LUNs behind
    this port can still be pending which in turn force
    zfcp_scsi_queuecommand() to unnecessarily finish requests with
    DID_IMM_RETRY.
    
    This also opens a time window with unblocked rport (until the followup
    LUN reopen recovery has finished).  If a scsi_cmnd timeout occurs during
    this time window fc_timed_out() cannot work as desired and such command
    would indeed time out and trigger scsi_eh. This prevents a clean and
    timely path failover.  This should not happen if the path issue can be
    recovered on FC transport layer such as path issues involving RSCNs.
    
    Fix this by only calling zfcp_scsi_schedule_rport_register(), to
    asynchronously trigger fc_remote_port_add(), after all LUN recoveries as
    children of the rport have finished and no new recoveries of equal or
    higher order were triggered meanwhile.  Finished intentionally includes
    any recovery result no matter if successful or failed (still unblock
    rport so other successful LUNs work).  For simplicity, we check after
    each finished LUN recovery if there is another LUN recovery pending on
    the same port and then do nothing.  We handle the special case of a
    successful recovery of a port without LUN children the same way without
    changing this case's semantics.
    
    For debugging we introduce 2 new trace records written if the rport
    unblock attempt was aborted due to still unfinished or freshly triggered
    recovery. The records are only written above the default trace level.
    
    Benjamin noticed the important special case of new recovery that can be
    triggered between having given up the erp_lock and before calling
    zfcp_erp_action_cleanup() within zfcp_erp_strategy().  We must avoid the
    following sequence:
    
    ERP thread                 rport_work      other context
    -------------------------  --------------  --------------------------------
    port is unblocked, rport still blocked,
     due to pending/running ERP action,
     so ((port->status & ...UNBLOCK) != 0)
     and (port->rport == NULL)
    unlock ERP
    zfcp_erp_action_cleanup()
    case ZFCP_ERP_ACTION_REOPEN_LUN:
    zfcp_erp_try_rport_unblock()
    ((status & ...UNBLOCK) != 0) [OLD!]
                                               zfcp_erp_port_reopen()
                                               lock ERP
                                               zfcp_erp_port_block()
                                               port->status clear ...UNBLOCK
                                               unlock ERP
                                               zfcp_scsi_schedule_rport_block()
                                               port->rport_task = RPORT_DEL
                                               queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task != RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_block()
                               if (!port->rport) return
    zfcp_scsi_schedule_rport_register()
    port->rport_task = RPORT_ADD
    queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task == RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_register()
                               (port->rport == NULL)
                               rport = fc_remote_port_add()
                               port->rport = rport;
    
    Now the rport was erroneously unblocked while the zfcp_port is blocked.
    This is another situation we want to avoid due to scsi_eh
    potential. This state would at least remain until the new recovery from
    the other context finished successfully, or potentially forever if it
    failed.  In order to close this race, we take the erp_lock inside
    zfcp_erp_try_rport_unblock() when checking the status of zfcp_port or
    LUN.  With that, the possible corresponding rport state sequences would
    be: (unblock[ERP thread],block[other context]) if the ERP thread gets
    erp_lock first and still sees ((port->status & ...UNBLOCK) != 0),
    (block[other context],NOP[ERP thread]) if the ERP thread gets erp_lock
    after the other context has already cleard ...UNBLOCK from port->status.
    
    Since checking fields of struct erp_action is unsafe because they could
    have been overwritten (re-used for new recovery) meanwhile, we only
    check status of zfcp_port and LUN since these are only changed under
    erp_lock elsewhere. Regarding the check of the proper status flags (port
    or port_forced are similar to the shown adapter recovery):
    
    [zfcp_erp_adapter_shutdown()]
    zfcp_erp_adapter_reopen()
     zfcp_erp_adapter_block()
      * clear UNBLOCK ---------------------------------------+
     zfcp_scsi_schedule_rports_block()                       |
     write_lock_irqsave(&adapter->erp_lock, flags);-------+  |
     zfcp_erp_action_enqueue()                            |  |
      zfcp_erp_setup_act()                                |  |
       * set ERP_INUSE -----------------------------------|--|--+
     write_unlock_irqrestore(&adapter->erp_lock, flags);--+  |  |
    .context-switch.                                         |  |
    zfcp_erp_thread()                                        |  |
     zfcp_erp_strategy()                                     |  |
      write_lock_irqsave(&adapter->erp_lock, flags);------+  |  |
      ...                                                 |  |  |
      zfcp_erp_strategy_check_target()                    |  |  |
       zfcp_erp_strategy_check_adapter()                  |  |  |
        zfcp_erp_adapter_unblock()                        |  |  |
         * set UNBLOCK -----------------------------------|--+  |
      zfcp_erp_action_dequeue()                           |     |
       * clear ERP_INUSE ---------------------------------|-----+
      ...                                                 |
      write_unlock_irqrestore(&adapter->erp_lock, flags);-+
    
    Hence, we should check for both UNBLOCK and ERP_INUSE because they are
    interleaved.  Also we need to explicitly check ERP_FAILED for the link
    down case which currently does not clear the UNBLOCK flag in
    zfcp_fsf_link_down_info_eval().
    
    Signed-off-by: Steffen Maier <maier@linux.vnet.ibm.com>
    Fixes: 8830271c4819 ("[SCSI] zfcp: Dont fail SCSI commands when transitioning to blocked fc_rport")
    Fixes: a2fa0aede07c ("[SCSI] zfcp: Block FC transport rports early on errors")
    Fixes: 5f852be9e11d ("[SCSI] zfcp: Fix deadlock between zfcp ERP and SCSI")
    Fixes: 338151e06608 ("[SCSI] zfcp: make use of fc_remote_port_delete when target port is unavailable")
    Fixes: 3859f6a248cb ("[PATCH] zfcp: add rports to enable scsi_add_device to work again")
    Reviewed-by: Benjamin Block <bblock@linux.vnet.ibm.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 565ae61d8995916da836b98f8c2f12a4192525fa
Author: Steffen Maier <maier@linux.vnet.ibm.com>
Date:   Fri Dec 9 17:16:33 2016 +0100

    scsi: zfcp: fix rport unblock race with LUN recovery
    
    commit 6f2ce1c6af37191640ee3ff6e8fc39ea10352f4c upstream.
    
    It is unavoidable that zfcp_scsi_queuecommand() has to finish requests
    with DID_IMM_RETRY (like fc_remote_port_chkready()) during the time
    window when zfcp detected an unavailable rport but
    fc_remote_port_delete(), which is asynchronous via
    zfcp_scsi_schedule_rport_block(), has not yet blocked the rport.
    
    However, for the case when the rport becomes available again, we should
    prevent unblocking the rport too early.  In contrast to other FCP LLDDs,
    zfcp has to open each LUN with the FCP channel hardware before it can
    send I/O to a LUN.  So if a port already has LUNs attached and we
    unblock the rport just after port recovery, recoveries of LUNs behind
    this port can still be pending which in turn force
    zfcp_scsi_queuecommand() to unnecessarily finish requests with
    DID_IMM_RETRY.
    
    This also opens a time window with unblocked rport (until the followup
    LUN reopen recovery has finished).  If a scsi_cmnd timeout occurs during
    this time window fc_timed_out() cannot work as desired and such command
    would indeed time out and trigger scsi_eh. This prevents a clean and
    timely path failover.  This should not happen if the path issue can be
    recovered on FC transport layer such as path issues involving RSCNs.
    
    Fix this by only calling zfcp_scsi_schedule_rport_register(), to
    asynchronously trigger fc_remote_port_add(), after all LUN recoveries as
    children of the rport have finished and no new recoveries of equal or
    higher order were triggered meanwhile.  Finished intentionally includes
    any recovery result no matter if successful or failed (still unblock
    rport so other successful LUNs work).  For simplicity, we check after
    each finished LUN recovery if there is another LUN recovery pending on
    the same port and then do nothing.  We handle the special case of a
    successful recovery of a port without LUN children the same way without
    changing this case's semantics.
    
    For debugging we introduce 2 new trace records written if the rport
    unblock attempt was aborted due to still unfinished or freshly triggered
    recovery. The records are only written above the default trace level.
    
    Benjamin noticed the important special case of new recovery that can be
    triggered between having given up the erp_lock and before calling
    zfcp_erp_action_cleanup() within zfcp_erp_strategy().  We must avoid the
    following sequence:
    
    ERP thread                 rport_work      other context
    -------------------------  --------------  --------------------------------
    port is unblocked, rport still blocked,
     due to pending/running ERP action,
     so ((port->status & ...UNBLOCK) != 0)
     and (port->rport == NULL)
    unlock ERP
    zfcp_erp_action_cleanup()
    case ZFCP_ERP_ACTION_REOPEN_LUN:
    zfcp_erp_try_rport_unblock()
    ((status & ...UNBLOCK) != 0) [OLD!]
                                               zfcp_erp_port_reopen()
                                               lock ERP
                                               zfcp_erp_port_block()
                                               port->status clear ...UNBLOCK
                                               unlock ERP
                                               zfcp_scsi_schedule_rport_block()
                                               port->rport_task = RPORT_DEL
                                               queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task != RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_block()
                               if (!port->rport) return
    zfcp_scsi_schedule_rport_register()
    port->rport_task = RPORT_ADD
    queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task == RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_register()
                               (port->rport == NULL)
                               rport = fc_remote_port_add()
                               port->rport = rport;
    
    Now the rport was erroneously unblocked while the zfcp_port is blocked.
    This is another situation we want to avoid due to scsi_eh
    potential. This state would at least remain until the new recovery from
    the other context finished successfully, or potentially forever if it
    failed.  In order to close this race, we take the erp_lock inside
    zfcp_erp_try_rport_unblock() when checking the status of zfcp_port or
    LUN.  With that, the possible corresponding rport state sequences would
    be: (unblock[ERP thread],block[other context]) if the ERP thread gets
    erp_lock first and still sees ((port->status & ...UNBLOCK) != 0),
    (block[other context],NOP[ERP thread]) if the ERP thread gets erp_lock
    after the other context has already cleard ...UNBLOCK from port->status.
    
    Since checking fields of struct erp_action is unsafe because they could
    have been overwritten (re-used for new recovery) meanwhile, we only
    check status of zfcp_port and LUN since these are only changed under
    erp_lock elsewhere. Regarding the check of the proper status flags (port
    or port_forced are similar to the shown adapter recovery):
    
    [zfcp_erp_adapter_shutdown()]
    zfcp_erp_adapter_reopen()
     zfcp_erp_adapter_block()
      * clear UNBLOCK ---------------------------------------+
     zfcp_scsi_schedule_rports_block()                       |
     write_lock_irqsave(&adapter->erp_lock, flags);-------+  |
     zfcp_erp_action_enqueue()                            |  |
      zfcp_erp_setup_act()                                |  |
       * set ERP_INUSE -----------------------------------|--|--+
     write_unlock_irqrestore(&adapter->erp_lock, flags);--+  |  |
    .context-switch.                                         |  |
    zfcp_erp_thread()                                        |  |
     zfcp_erp_strategy()                                     |  |
      write_lock_irqsave(&adapter->erp_lock, flags);------+  |  |
      ...                                                 |  |  |
      zfcp_erp_strategy_check_target()                    |  |  |
       zfcp_erp_strategy_check_adapter()                  |  |  |
        zfcp_erp_adapter_unblock()                        |  |  |
         * set UNBLOCK -----------------------------------|--+  |
      zfcp_erp_action_dequeue()                           |     |
       * clear ERP_INUSE ---------------------------------|-----+
      ...                                                 |
      write_unlock_irqrestore(&adapter->erp_lock, flags);-+
    
    Hence, we should check for both UNBLOCK and ERP_INUSE because they are
    interleaved.  Also we need to explicitly check ERP_FAILED for the link
    down case which currently does not clear the UNBLOCK flag in
    zfcp_fsf_link_down_info_eval().
    
    Signed-off-by: Steffen Maier <maier@linux.vnet.ibm.com>
    Fixes: 8830271c4819 ("[SCSI] zfcp: Dont fail SCSI commands when transitioning to blocked fc_rport")
    Fixes: a2fa0aede07c ("[SCSI] zfcp: Block FC transport rports early on errors")
    Fixes: 5f852be9e11d ("[SCSI] zfcp: Fix deadlock between zfcp ERP and SCSI")
    Fixes: 338151e06608 ("[SCSI] zfcp: make use of fc_remote_port_delete when target port is unavailable")
    Fixes: 3859f6a248cb ("[PATCH] zfcp: add rports to enable scsi_add_device to work again")
    Reviewed-by: Benjamin Block <bblock@linux.vnet.ibm.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6f2ce1c6af37191640ee3ff6e8fc39ea10352f4c
Author: Steffen Maier <maier@linux.vnet.ibm.com>
Date:   Fri Dec 9 17:16:33 2016 +0100

    scsi: zfcp: fix rport unblock race with LUN recovery
    
    It is unavoidable that zfcp_scsi_queuecommand() has to finish requests
    with DID_IMM_RETRY (like fc_remote_port_chkready()) during the time
    window when zfcp detected an unavailable rport but
    fc_remote_port_delete(), which is asynchronous via
    zfcp_scsi_schedule_rport_block(), has not yet blocked the rport.
    
    However, for the case when the rport becomes available again, we should
    prevent unblocking the rport too early.  In contrast to other FCP LLDDs,
    zfcp has to open each LUN with the FCP channel hardware before it can
    send I/O to a LUN.  So if a port already has LUNs attached and we
    unblock the rport just after port recovery, recoveries of LUNs behind
    this port can still be pending which in turn force
    zfcp_scsi_queuecommand() to unnecessarily finish requests with
    DID_IMM_RETRY.
    
    This also opens a time window with unblocked rport (until the followup
    LUN reopen recovery has finished).  If a scsi_cmnd timeout occurs during
    this time window fc_timed_out() cannot work as desired and such command
    would indeed time out and trigger scsi_eh. This prevents a clean and
    timely path failover.  This should not happen if the path issue can be
    recovered on FC transport layer such as path issues involving RSCNs.
    
    Fix this by only calling zfcp_scsi_schedule_rport_register(), to
    asynchronously trigger fc_remote_port_add(), after all LUN recoveries as
    children of the rport have finished and no new recoveries of equal or
    higher order were triggered meanwhile.  Finished intentionally includes
    any recovery result no matter if successful or failed (still unblock
    rport so other successful LUNs work).  For simplicity, we check after
    each finished LUN recovery if there is another LUN recovery pending on
    the same port and then do nothing.  We handle the special case of a
    successful recovery of a port without LUN children the same way without
    changing this case's semantics.
    
    For debugging we introduce 2 new trace records written if the rport
    unblock attempt was aborted due to still unfinished or freshly triggered
    recovery. The records are only written above the default trace level.
    
    Benjamin noticed the important special case of new recovery that can be
    triggered between having given up the erp_lock and before calling
    zfcp_erp_action_cleanup() within zfcp_erp_strategy().  We must avoid the
    following sequence:
    
    ERP thread                 rport_work      other context
    -------------------------  --------------  --------------------------------
    port is unblocked, rport still blocked,
     due to pending/running ERP action,
     so ((port->status & ...UNBLOCK) != 0)
     and (port->rport == NULL)
    unlock ERP
    zfcp_erp_action_cleanup()
    case ZFCP_ERP_ACTION_REOPEN_LUN:
    zfcp_erp_try_rport_unblock()
    ((status & ...UNBLOCK) != 0) [OLD!]
                                               zfcp_erp_port_reopen()
                                               lock ERP
                                               zfcp_erp_port_block()
                                               port->status clear ...UNBLOCK
                                               unlock ERP
                                               zfcp_scsi_schedule_rport_block()
                                               port->rport_task = RPORT_DEL
                                               queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task != RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_block()
                               if (!port->rport) return
    zfcp_scsi_schedule_rport_register()
    port->rport_task = RPORT_ADD
    queue_work(rport_work)
                               zfcp_scsi_rport_work()
                               (port->rport_task == RPORT_ADD)
                               port->rport_task = RPORT_NONE
                               zfcp_scsi_rport_register()
                               (port->rport == NULL)
                               rport = fc_remote_port_add()
                               port->rport = rport;
    
    Now the rport was erroneously unblocked while the zfcp_port is blocked.
    This is another situation we want to avoid due to scsi_eh
    potential. This state would at least remain until the new recovery from
    the other context finished successfully, or potentially forever if it
    failed.  In order to close this race, we take the erp_lock inside
    zfcp_erp_try_rport_unblock() when checking the status of zfcp_port or
    LUN.  With that, the possible corresponding rport state sequences would
    be: (unblock[ERP thread],block[other context]) if the ERP thread gets
    erp_lock first and still sees ((port->status & ...UNBLOCK) != 0),
    (block[other context],NOP[ERP thread]) if the ERP thread gets erp_lock
    after the other context has already cleard ...UNBLOCK from port->status.
    
    Since checking fields of struct erp_action is unsafe because they could
    have been overwritten (re-used for new recovery) meanwhile, we only
    check status of zfcp_port and LUN since these are only changed under
    erp_lock elsewhere. Regarding the check of the proper status flags (port
    or port_forced are similar to the shown adapter recovery):
    
    [zfcp_erp_adapter_shutdown()]
    zfcp_erp_adapter_reopen()
     zfcp_erp_adapter_block()
      * clear UNBLOCK ---------------------------------------+
     zfcp_scsi_schedule_rports_block()                       |
     write_lock_irqsave(&adapter->erp_lock, flags);-------+  |
     zfcp_erp_action_enqueue()                            |  |
      zfcp_erp_setup_act()                                |  |
       * set ERP_INUSE -----------------------------------|--|--+
     write_unlock_irqrestore(&adapter->erp_lock, flags);--+  |  |
    .context-switch.                                         |  |
    zfcp_erp_thread()                                        |  |
     zfcp_erp_strategy()                                     |  |
      write_lock_irqsave(&adapter->erp_lock, flags);------+  |  |
      ...                                                 |  |  |
      zfcp_erp_strategy_check_target()                    |  |  |
       zfcp_erp_strategy_check_adapter()                  |  |  |
        zfcp_erp_adapter_unblock()                        |  |  |
         * set UNBLOCK -----------------------------------|--+  |
      zfcp_erp_action_dequeue()                           |     |
       * clear ERP_INUSE ---------------------------------|-----+
      ...                                                 |
      write_unlock_irqrestore(&adapter->erp_lock, flags);-+
    
    Hence, we should check for both UNBLOCK and ERP_INUSE because they are
    interleaved.  Also we need to explicitly check ERP_FAILED for the link
    down case which currently does not clear the UNBLOCK flag in
    zfcp_fsf_link_down_info_eval().
    
    Signed-off-by: Steffen Maier <maier@linux.vnet.ibm.com>
    Fixes: 8830271c4819 ("[SCSI] zfcp: Dont fail SCSI commands when transitioning to blocked fc_rport")
    Fixes: a2fa0aede07c ("[SCSI] zfcp: Block FC transport rports early on errors")
    Fixes: 5f852be9e11d ("[SCSI] zfcp: Fix deadlock between zfcp ERP and SCSI")
    Fixes: 338151e06608 ("[SCSI] zfcp: make use of fc_remote_port_delete when target port is unavailable")
    Fixes: 3859f6a248cb ("[PATCH] zfcp: add rports to enable scsi_add_device to work again")
    Cc: <stable@vger.kernel.org> #2.6.32+
    Reviewed-by: Benjamin Block <bblock@linux.vnet.ibm.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit 05ffc951392df57edecc2519327b169210c3df75
Author: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
Date:   Wed Nov 2 05:08:30 2016 -0400

    locking/mutex: Break out of expensive busy-loop on {mutex,rwsem}_spin_on_owner() when owner vCPU is preempted
    
    An over-committed guest with more vCPUs than pCPUs has a heavy overload
    in the two spin_on_owner. This blames on the lock holder preemption
    issue.
    
    Break out of the loop if the vCPU is preempted: if vcpu_is_preempted(cpu)
    is true.
    
    test-case:
    perf record -a perf bench sched messaging -g 400 -p && perf report
    
    before patch:
    20.68%  sched-messaging  [kernel.vmlinux]  [k] mutex_spin_on_owner
     8.45%  sched-messaging  [kernel.vmlinux]  [k] mutex_unlock
     4.12%  sched-messaging  [kernel.vmlinux]  [k] system_call
     3.01%  sched-messaging  [kernel.vmlinux]  [k] system_call_common
     2.83%  sched-messaging  [kernel.vmlinux]  [k] copypage_power7
     2.64%  sched-messaging  [kernel.vmlinux]  [k] rwsem_spin_on_owner
     2.00%  sched-messaging  [kernel.vmlinux]  [k] osq_lock
    
    after patch:
     9.99%  sched-messaging  [kernel.vmlinux]  [k] mutex_unlock
     5.28%  sched-messaging  [unknown]         [H] 0xc0000000000768e0
     4.27%  sched-messaging  [kernel.vmlinux]  [k] __copy_tofrom_user_power7
     3.77%  sched-messaging  [kernel.vmlinux]  [k] copypage_power7
     3.24%  sched-messaging  [kernel.vmlinux]  [k] _raw_write_lock_irq
     3.02%  sched-messaging  [kernel.vmlinux]  [k] system_call
     2.69%  sched-messaging  [kernel.vmlinux]  [k] wait_consider_task
    
    Tested-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: David.Laight@ACULAB.COM
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: benh@kernel.crashing.org
    Cc: boqun.feng@gmail.com
    Cc: bsingharora@gmail.com
    Cc: dave@stgolabs.net
    Cc: kernellwp@gmail.com
    Cc: konrad.wilk@oracle.com
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: mpe@ellerman.id.au
    Cc: paulmck@linux.vnet.ibm.com
    Cc: paulus@samba.org
    Cc: rkrcmar@redhat.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: will.deacon@arm.com
    Cc: xen-devel-request@lists.xenproject.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1478077718-37424-4-git-send-email-xinhui.pan@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 5aff60a191e579ae00ae5ca6ce16c13b687bc8a3
Author: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
Date:   Wed Nov 2 05:08:29 2016 -0400

    locking/osq: Break out of spin-wait busy waiting loop for a preempted vCPU in osq_lock()
    
    An over-committed guest with more vCPUs than pCPUs has a heavy overload
    in osq_lock().
    
    This is because if vCPU-A holds the osq lock and yields out, vCPU-B ends
    up waiting for per_cpu node->locked to be set. IOW, vCPU-B waits for
    vCPU-A to run and unlock the osq lock.
    
    Use the new vcpu_is_preempted(cpu) interface to detect if a vCPU is
    currently running or not, and break out of the spin-loop if so.
    
    test case:
    
     $ perf record -a perf bench sched messaging -g 400 -p && perf report
    
     before patch:
     18.09%  sched-messaging  [kernel.vmlinux]  [k] osq_lock
     12.28%  sched-messaging  [kernel.vmlinux]  [k] rwsem_spin_on_owner
      5.27%  sched-messaging  [kernel.vmlinux]  [k] mutex_unlock
      3.89%  sched-messaging  [kernel.vmlinux]  [k] wait_consider_task
      3.64%  sched-messaging  [kernel.vmlinux]  [k] _raw_write_lock_irq
      3.41%  sched-messaging  [kernel.vmlinux]  [k] mutex_spin_on_owner.is
      2.49%  sched-messaging  [kernel.vmlinux]  [k] system_call
    
     after patch:
     20.68%  sched-messaging  [kernel.vmlinux]  [k] mutex_spin_on_owner
      8.45%  sched-messaging  [kernel.vmlinux]  [k] mutex_unlock
      4.12%  sched-messaging  [kernel.vmlinux]  [k] system_call
      3.01%  sched-messaging  [kernel.vmlinux]  [k] system_call_common
      2.83%  sched-messaging  [kernel.vmlinux]  [k] copypage_power7
      2.64%  sched-messaging  [kernel.vmlinux]  [k] rwsem_spin_on_owner
      2.00%  sched-messaging  [kernel.vmlinux]  [k] osq_lock
    
    Suggested-by: Boqun Feng <boqun.feng@gmail.com>
    Tested-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: David.Laight@ACULAB.COM
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: benh@kernel.crashing.org
    Cc: bsingharora@gmail.com
    Cc: dave@stgolabs.net
    Cc: kernellwp@gmail.com
    Cc: konrad.wilk@oracle.com
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: mpe@ellerman.id.au
    Cc: paulmck@linux.vnet.ibm.com
    Cc: paulus@samba.org
    Cc: rkrcmar@redhat.com
    Cc: virtualization@lists.linux-foundation.org
    Cc: will.deacon@arm.com
    Cc: xen-devel-request@lists.xenproject.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1478077718-37424-3-git-send-email-xinhui.pan@linux.vnet.ibm.com
    [ Translated to English. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 643a9a21c6ba06b01fb1b33a7f6bbc4bfaafcd07
Author: Mike Galbraith <efault@gmx.de>
Date:   Mon Aug 13 15:21:23 2012 +0200

    reiserfs: Unlock superblock before calling reiserfs_quota_on_mount()
    
    commit 420902c9d086848a7548c83e0a49021514bd71b7 upstream.
    
    If we hold the superblock lock while calling reiserfs_quota_on_mount(), we can
    deadlock our own worker - mount blocks kworker/3:2, sleeps forever more.
    
    crash> ps|grep UN
        715      2   3  ffff880220734d30  UN   0.0       0      0  [kworker/3:2]
       9369   9341   2  ffff88021ffb7560  UN   1.3  493404 123184  Xorg
       9665   9664   3  ffff880225b92ab0  UN   0.0   47368    812  udisks-daemon
      10635  10403   3  ffff880222f22c70  UN   0.0   14904    936  mount
    crash> bt ffff880220734d30
    PID: 715    TASK: ffff880220734d30  CPU: 3   COMMAND: "kworker/3:2"
     #0 [ffff8802244c3c20] schedule at ffffffff8144584b
     #1 [ffff8802244c3cc8] __rt_mutex_slowlock at ffffffff814472b3
     #2 [ffff8802244c3d28] rt_mutex_slowlock at ffffffff814473f5
     #3 [ffff8802244c3dc8] reiserfs_write_lock at ffffffffa05f28fd [reiserfs]
     #4 [ffff8802244c3de8] flush_async_commits at ffffffffa05ec91d [reiserfs]
     #5 [ffff8802244c3e08] process_one_work at ffffffff81073726
     #6 [ffff8802244c3e68] worker_thread at ffffffff81073eba
     #7 [ffff8802244c3ec8] kthread at ffffffff810782e0
     #8 [ffff8802244c3f48] kernel_thread_helper at ffffffff81450064
    crash> rd ffff8802244c3cc8 10
    ffff8802244c3cc8:  ffffffff814472b3 ffff880222f23250   .rD.....P2."....
    ffff8802244c3cd8:  0000000000000000 0000000000000286   ................
    ffff8802244c3ce8:  ffff8802244c3d30 ffff880220734d80   0=L$.....Ms ....
    ffff8802244c3cf8:  ffff880222e8f628 0000000000000000   (.."............
    ffff8802244c3d08:  0000000000000000 0000000000000002   ................
    crash> struct rt_mutex ffff880222e8f628
    struct rt_mutex {
      wait_lock = {
        raw_lock = {
          slock = 65537
        }
      },
      wait_list = {
        node_list = {
          next = 0xffff8802244c3d48,
          prev = 0xffff8802244c3d48
        }
      },
      owner = 0xffff880222f22c71,
      save_state = 0
    }
    crash> bt 0xffff880222f22c70
    PID: 10635  TASK: ffff880222f22c70  CPU: 3   COMMAND: "mount"
     #0 [ffff8802216a9868] schedule at ffffffff8144584b
     #1 [ffff8802216a9910] schedule_timeout at ffffffff81446865
     #2 [ffff8802216a99a0] wait_for_common at ffffffff81445f74
     #3 [ffff8802216a9a30] flush_work at ffffffff810712d3
     #4 [ffff8802216a9ab0] schedule_on_each_cpu at ffffffff81074463
     #5 [ffff8802216a9ae0] invalidate_bdev at ffffffff81178aba
     #6 [ffff8802216a9af0] vfs_load_quota_inode at ffffffff811a3632
     #7 [ffff8802216a9b50] dquot_quota_on_mount at ffffffff811a375c
     #8 [ffff8802216a9b80] finish_unfinished at ffffffffa05dd8b0 [reiserfs]
     #9 [ffff8802216a9cc0] reiserfs_fill_super at ffffffffa05de825 [reiserfs]
        RIP: 00007f7b9303997a  RSP: 00007ffff443c7a8  RFLAGS: 00010202
        RAX: 00000000000000a5  RBX: ffffffff8144ef12  RCX: 00007f7b932e9ee0
        RDX: 00007f7b93d9a400  RSI: 00007f7b93d9a3e0  RDI: 00007f7b93d9a3c0
        RBP: 00007f7b93d9a2c0   R8: 00007f7b93d9a550   R9: 0000000000000001
        R10: ffffffffc0ed040e  R11: 0000000000000202  R12: 000000000000040e
        R13: 0000000000000000  R14: 00000000c0ed040e  R15: 00007ffff443ca20
        ORIG_RAX: 00000000000000a5  CS: 0033  SS: 002b
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Mike Galbraith <mgalbraith@suse.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit 5c55afa3cd8102ddaace98ea7f002558c1cc4aaa
Author: Mike Galbraith <efault@gmx.de>
Date:   Mon Aug 13 15:21:23 2012 +0200

    reiserfs: Unlock superblock before calling reiserfs_quota_on_mount()
    
    commit 420902c9d086848a7548c83e0a49021514bd71b7 upstream.
    
    If we hold the superblock lock while calling reiserfs_quota_on_mount(), we can
    deadlock our own worker - mount blocks kworker/3:2, sleeps forever more.
    
    crash> ps|grep UN
        715      2   3  ffff880220734d30  UN   0.0       0      0  [kworker/3:2]
       9369   9341   2  ffff88021ffb7560  UN   1.3  493404 123184  Xorg
       9665   9664   3  ffff880225b92ab0  UN   0.0   47368    812  udisks-daemon
      10635  10403   3  ffff880222f22c70  UN   0.0   14904    936  mount
    crash> bt ffff880220734d30
    PID: 715    TASK: ffff880220734d30  CPU: 3   COMMAND: "kworker/3:2"
     #0 [ffff8802244c3c20] schedule at ffffffff8144584b
     #1 [ffff8802244c3cc8] __rt_mutex_slowlock at ffffffff814472b3
     #2 [ffff8802244c3d28] rt_mutex_slowlock at ffffffff814473f5
     #3 [ffff8802244c3dc8] reiserfs_write_lock at ffffffffa05f28fd [reiserfs]
     #4 [ffff8802244c3de8] flush_async_commits at ffffffffa05ec91d [reiserfs]
     #5 [ffff8802244c3e08] process_one_work at ffffffff81073726
     #6 [ffff8802244c3e68] worker_thread at ffffffff81073eba
     #7 [ffff8802244c3ec8] kthread at ffffffff810782e0
     #8 [ffff8802244c3f48] kernel_thread_helper at ffffffff81450064
    crash> rd ffff8802244c3cc8 10
    ffff8802244c3cc8:  ffffffff814472b3 ffff880222f23250   .rD.....P2."....
    ffff8802244c3cd8:  0000000000000000 0000000000000286   ................
    ffff8802244c3ce8:  ffff8802244c3d30 ffff880220734d80   0=L$.....Ms ....
    ffff8802244c3cf8:  ffff880222e8f628 0000000000000000   (.."............
    ffff8802244c3d08:  0000000000000000 0000000000000002   ................
    crash> struct rt_mutex ffff880222e8f628
    struct rt_mutex {
      wait_lock = {
        raw_lock = {
          slock = 65537
        }
      },
      wait_list = {
        node_list = {
          next = 0xffff8802244c3d48,
          prev = 0xffff8802244c3d48
        }
      },
      owner = 0xffff880222f22c71,
      save_state = 0
    }
    crash> bt 0xffff880222f22c70
    PID: 10635  TASK: ffff880222f22c70  CPU: 3   COMMAND: "mount"
     #0 [ffff8802216a9868] schedule at ffffffff8144584b
     #1 [ffff8802216a9910] schedule_timeout at ffffffff81446865
     #2 [ffff8802216a99a0] wait_for_common at ffffffff81445f74
     #3 [ffff8802216a9a30] flush_work at ffffffff810712d3
     #4 [ffff8802216a9ab0] schedule_on_each_cpu at ffffffff81074463
     #5 [ffff8802216a9ae0] invalidate_bdev at ffffffff81178aba
     #6 [ffff8802216a9af0] vfs_load_quota_inode at ffffffff811a3632
     #7 [ffff8802216a9b50] dquot_quota_on_mount at ffffffff811a375c
     #8 [ffff8802216a9b80] finish_unfinished at ffffffffa05dd8b0 [reiserfs]
     #9 [ffff8802216a9cc0] reiserfs_fill_super at ffffffffa05de825 [reiserfs]
        RIP: 00007f7b9303997a  RSP: 00007ffff443c7a8  RFLAGS: 00010202
        RAX: 00000000000000a5  RBX: ffffffff8144ef12  RCX: 00007f7b932e9ee0
        RDX: 00007f7b93d9a400  RSI: 00007f7b93d9a3e0  RDI: 00007f7b93d9a3c0
        RBP: 00007f7b93d9a2c0   R8: 00007f7b93d9a550   R9: 0000000000000001
        R10: ffffffffc0ed040e  R11: 0000000000000202  R12: 000000000000040e
        R13: 0000000000000000  R14: 00000000c0ed040e  R15: 00007ffff443ca20
        ORIG_RAX: 00000000000000a5  CS: 0033  SS: 002b
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Mike Galbraith <mgalbraith@suse.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5c16520bdcd41740abcadb58133637faecf713c9
Author: Mike Galbraith <efault@gmx.de>
Date:   Mon Aug 13 15:21:23 2012 +0200

    reiserfs: Unlock superblock before calling reiserfs_quota_on_mount()
    
    commit 420902c9d086848a7548c83e0a49021514bd71b7 upstream.
    
    If we hold the superblock lock while calling reiserfs_quota_on_mount(), we can
    deadlock our own worker - mount blocks kworker/3:2, sleeps forever more.
    
    crash> ps|grep UN
        715      2   3  ffff880220734d30  UN   0.0       0      0  [kworker/3:2]
       9369   9341   2  ffff88021ffb7560  UN   1.3  493404 123184  Xorg
       9665   9664   3  ffff880225b92ab0  UN   0.0   47368    812  udisks-daemon
      10635  10403   3  ffff880222f22c70  UN   0.0   14904    936  mount
    crash> bt ffff880220734d30
    PID: 715    TASK: ffff880220734d30  CPU: 3   COMMAND: "kworker/3:2"
     #0 [ffff8802244c3c20] schedule at ffffffff8144584b
     #1 [ffff8802244c3cc8] __rt_mutex_slowlock at ffffffff814472b3
     #2 [ffff8802244c3d28] rt_mutex_slowlock at ffffffff814473f5
     #3 [ffff8802244c3dc8] reiserfs_write_lock at ffffffffa05f28fd [reiserfs]
     #4 [ffff8802244c3de8] flush_async_commits at ffffffffa05ec91d [reiserfs]
     #5 [ffff8802244c3e08] process_one_work at ffffffff81073726
     #6 [ffff8802244c3e68] worker_thread at ffffffff81073eba
     #7 [ffff8802244c3ec8] kthread at ffffffff810782e0
     #8 [ffff8802244c3f48] kernel_thread_helper at ffffffff81450064
    crash> rd ffff8802244c3cc8 10
    ffff8802244c3cc8:  ffffffff814472b3 ffff880222f23250   .rD.....P2."....
    ffff8802244c3cd8:  0000000000000000 0000000000000286   ................
    ffff8802244c3ce8:  ffff8802244c3d30 ffff880220734d80   0=L$.....Ms ....
    ffff8802244c3cf8:  ffff880222e8f628 0000000000000000   (.."............
    ffff8802244c3d08:  0000000000000000 0000000000000002   ................
    crash> struct rt_mutex ffff880222e8f628
    struct rt_mutex {
      wait_lock = {
        raw_lock = {
          slock = 65537
        }
      },
      wait_list = {
        node_list = {
          next = 0xffff8802244c3d48,
          prev = 0xffff8802244c3d48
        }
      },
      owner = 0xffff880222f22c71,
      save_state = 0
    }
    crash> bt 0xffff880222f22c70
    PID: 10635  TASK: ffff880222f22c70  CPU: 3   COMMAND: "mount"
     #0 [ffff8802216a9868] schedule at ffffffff8144584b
     #1 [ffff8802216a9910] schedule_timeout at ffffffff81446865
     #2 [ffff8802216a99a0] wait_for_common at ffffffff81445f74
     #3 [ffff8802216a9a30] flush_work at ffffffff810712d3
     #4 [ffff8802216a9ab0] schedule_on_each_cpu at ffffffff81074463
     #5 [ffff8802216a9ae0] invalidate_bdev at ffffffff81178aba
     #6 [ffff8802216a9af0] vfs_load_quota_inode at ffffffff811a3632
     #7 [ffff8802216a9b50] dquot_quota_on_mount at ffffffff811a375c
     #8 [ffff8802216a9b80] finish_unfinished at ffffffffa05dd8b0 [reiserfs]
     #9 [ffff8802216a9cc0] reiserfs_fill_super at ffffffffa05de825 [reiserfs]
        RIP: 00007f7b9303997a  RSP: 00007ffff443c7a8  RFLAGS: 00010202
        RAX: 00000000000000a5  RBX: ffffffff8144ef12  RCX: 00007f7b932e9ee0
        RDX: 00007f7b93d9a400  RSI: 00007f7b93d9a3e0  RDI: 00007f7b93d9a3c0
        RBP: 00007f7b93d9a2c0   R8: 00007f7b93d9a550   R9: 0000000000000001
        R10: ffffffffc0ed040e  R11: 0000000000000202  R12: 000000000000040e
        R13: 0000000000000000  R14: 00000000c0ed040e  R15: 00007ffff443ca20
        ORIG_RAX: 00000000000000a5  CS: 0033  SS: 002b
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Mike Galbraith <mgalbraith@suse.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 41466db87ee886e2410aac25b84849838a8b2ef0
Author: Mike Galbraith <efault@gmx.de>
Date:   Mon Aug 13 15:21:23 2012 +0200

    reiserfs: Unlock superblock before calling reiserfs_quota_on_mount()
    
    commit 420902c9d086848a7548c83e0a49021514bd71b7 upstream.
    
    If we hold the superblock lock while calling reiserfs_quota_on_mount(), we can
    deadlock our own worker - mount blocks kworker/3:2, sleeps forever more.
    
    crash> ps|grep UN
        715      2   3  ffff880220734d30  UN   0.0       0      0  [kworker/3:2]
       9369   9341   2  ffff88021ffb7560  UN   1.3  493404 123184  Xorg
       9665   9664   3  ffff880225b92ab0  UN   0.0   47368    812  udisks-daemon
      10635  10403   3  ffff880222f22c70  UN   0.0   14904    936  mount
    crash> bt ffff880220734d30
    PID: 715    TASK: ffff880220734d30  CPU: 3   COMMAND: "kworker/3:2"
     #0 [ffff8802244c3c20] schedule at ffffffff8144584b
     #1 [ffff8802244c3cc8] __rt_mutex_slowlock at ffffffff814472b3
     #2 [ffff8802244c3d28] rt_mutex_slowlock at ffffffff814473f5
     #3 [ffff8802244c3dc8] reiserfs_write_lock at ffffffffa05f28fd [reiserfs]
     #4 [ffff8802244c3de8] flush_async_commits at ffffffffa05ec91d [reiserfs]
     #5 [ffff8802244c3e08] process_one_work at ffffffff81073726
     #6 [ffff8802244c3e68] worker_thread at ffffffff81073eba
     #7 [ffff8802244c3ec8] kthread at ffffffff810782e0
     #8 [ffff8802244c3f48] kernel_thread_helper at ffffffff81450064
    crash> rd ffff8802244c3cc8 10
    ffff8802244c3cc8:  ffffffff814472b3 ffff880222f23250   .rD.....P2."....
    ffff8802244c3cd8:  0000000000000000 0000000000000286   ................
    ffff8802244c3ce8:  ffff8802244c3d30 ffff880220734d80   0=L$.....Ms ....
    ffff8802244c3cf8:  ffff880222e8f628 0000000000000000   (.."............
    ffff8802244c3d08:  0000000000000000 0000000000000002   ................
    crash> struct rt_mutex ffff880222e8f628
    struct rt_mutex {
      wait_lock = {
        raw_lock = {
          slock = 65537
        }
      },
      wait_list = {
        node_list = {
          next = 0xffff8802244c3d48,
          prev = 0xffff8802244c3d48
        }
      },
      owner = 0xffff880222f22c71,
      save_state = 0
    }
    crash> bt 0xffff880222f22c70
    PID: 10635  TASK: ffff880222f22c70  CPU: 3   COMMAND: "mount"
     #0 [ffff8802216a9868] schedule at ffffffff8144584b
     #1 [ffff8802216a9910] schedule_timeout at ffffffff81446865
     #2 [ffff8802216a99a0] wait_for_common at ffffffff81445f74
     #3 [ffff8802216a9a30] flush_work at ffffffff810712d3
     #4 [ffff8802216a9ab0] schedule_on_each_cpu at ffffffff81074463
     #5 [ffff8802216a9ae0] invalidate_bdev at ffffffff81178aba
     #6 [ffff8802216a9af0] vfs_load_quota_inode at ffffffff811a3632
     #7 [ffff8802216a9b50] dquot_quota_on_mount at ffffffff811a375c
     #8 [ffff8802216a9b80] finish_unfinished at ffffffffa05dd8b0 [reiserfs]
     #9 [ffff8802216a9cc0] reiserfs_fill_super at ffffffffa05de825 [reiserfs]
        RIP: 00007f7b9303997a  RSP: 00007ffff443c7a8  RFLAGS: 00010202
        RAX: 00000000000000a5  RBX: ffffffff8144ef12  RCX: 00007f7b932e9ee0
        RDX: 00007f7b93d9a400  RSI: 00007f7b93d9a3e0  RDI: 00007f7b93d9a3c0
        RBP: 00007f7b93d9a2c0   R8: 00007f7b93d9a550   R9: 0000000000000001
        R10: ffffffffc0ed040e  R11: 0000000000000202  R12: 000000000000040e
        R13: 0000000000000000  R14: 00000000c0ed040e  R15: 00007ffff443ca20
        ORIG_RAX: 00000000000000a5  CS: 0033  SS: 002b
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Mike Galbraith <mgalbraith@suse.de>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d2fee58a3bb15b2b8f1eaff14aa3432cf0f35d8c
Author: Bob Peterson <rpeterso@redhat.com>
Date:   Mon Oct 10 09:19:52 2016 -0400

    dlm: remove lock_sock to avoid scheduling while atomic
    
    Before this patch, functions save_callbacks and restore_callbacks
    called function lock_sock and release_sock to prevent other processes
    from messing with the struct sock while the callbacks were saved and
    restored. However, function add_sock calls write_lock_bh prior to
    calling it save_callbacks, which disables preempts. So the call to
    lock_sock would try to schedule when we can't schedule.
    
    Signed-off-by: Bob Peterson <rpeterso@redhat.com>
    Signed-off-by: David Teigland <teigland@redhat.com>

commit 420902c9d086848a7548c83e0a49021514bd71b7
Author: Mike Galbraith <efault@gmx.de>
Date:   Mon Aug 13 15:21:23 2012 +0200

    reiserfs: Unlock superblock before calling reiserfs_quota_on_mount()
    
    If we hold the superblock lock while calling reiserfs_quota_on_mount(), we can
    deadlock our own worker - mount blocks kworker/3:2, sleeps forever more.
    
    crash> ps|grep UN
        715      2   3  ffff880220734d30  UN   0.0       0      0  [kworker/3:2]
       9369   9341   2  ffff88021ffb7560  UN   1.3  493404 123184  Xorg
       9665   9664   3  ffff880225b92ab0  UN   0.0   47368    812  udisks-daemon
      10635  10403   3  ffff880222f22c70  UN   0.0   14904    936  mount
    crash> bt ffff880220734d30
    PID: 715    TASK: ffff880220734d30  CPU: 3   COMMAND: "kworker/3:2"
     #0 [ffff8802244c3c20] schedule at ffffffff8144584b
     #1 [ffff8802244c3cc8] __rt_mutex_slowlock at ffffffff814472b3
     #2 [ffff8802244c3d28] rt_mutex_slowlock at ffffffff814473f5
     #3 [ffff8802244c3dc8] reiserfs_write_lock at ffffffffa05f28fd [reiserfs]
     #4 [ffff8802244c3de8] flush_async_commits at ffffffffa05ec91d [reiserfs]
     #5 [ffff8802244c3e08] process_one_work at ffffffff81073726
     #6 [ffff8802244c3e68] worker_thread at ffffffff81073eba
     #7 [ffff8802244c3ec8] kthread at ffffffff810782e0
     #8 [ffff8802244c3f48] kernel_thread_helper at ffffffff81450064
    crash> rd ffff8802244c3cc8 10
    ffff8802244c3cc8:  ffffffff814472b3 ffff880222f23250   .rD.....P2."....
    ffff8802244c3cd8:  0000000000000000 0000000000000286   ................
    ffff8802244c3ce8:  ffff8802244c3d30 ffff880220734d80   0=L$.....Ms ....
    ffff8802244c3cf8:  ffff880222e8f628 0000000000000000   (.."............
    ffff8802244c3d08:  0000000000000000 0000000000000002   ................
    crash> struct rt_mutex ffff880222e8f628
    struct rt_mutex {
      wait_lock = {
        raw_lock = {
          slock = 65537
        }
      },
      wait_list = {
        node_list = {
          next = 0xffff8802244c3d48,
          prev = 0xffff8802244c3d48
        }
      },
      owner = 0xffff880222f22c71,
      save_state = 0
    }
    crash> bt 0xffff880222f22c70
    PID: 10635  TASK: ffff880222f22c70  CPU: 3   COMMAND: "mount"
     #0 [ffff8802216a9868] schedule at ffffffff8144584b
     #1 [ffff8802216a9910] schedule_timeout at ffffffff81446865
     #2 [ffff8802216a99a0] wait_for_common at ffffffff81445f74
     #3 [ffff8802216a9a30] flush_work at ffffffff810712d3
     #4 [ffff8802216a9ab0] schedule_on_each_cpu at ffffffff81074463
     #5 [ffff8802216a9ae0] invalidate_bdev at ffffffff81178aba
     #6 [ffff8802216a9af0] vfs_load_quota_inode at ffffffff811a3632
     #7 [ffff8802216a9b50] dquot_quota_on_mount at ffffffff811a375c
     #8 [ffff8802216a9b80] finish_unfinished at ffffffffa05dd8b0 [reiserfs]
     #9 [ffff8802216a9cc0] reiserfs_fill_super at ffffffffa05de825 [reiserfs]
        RIP: 00007f7b9303997a  RSP: 00007ffff443c7a8  RFLAGS: 00010202
        RAX: 00000000000000a5  RBX: ffffffff8144ef12  RCX: 00007f7b932e9ee0
        RDX: 00007f7b93d9a400  RSI: 00007f7b93d9a3e0  RDI: 00007f7b93d9a3c0
        RBP: 00007f7b93d9a2c0   R8: 00007f7b93d9a550   R9: 0000000000000001
        R10: ffffffffc0ed040e  R11: 0000000000000202  R12: 000000000000040e
        R13: 0000000000000000  R14: 00000000c0ed040e  R15: 00007ffff443ca20
        ORIG_RAX: 00000000000000a5  CS: 0033  SS: 002b
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Mike Galbraith <mgalbraith@suse.de>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Jan Kara <jack@suse.cz>

commit ae0119f5f73b1e9cf7177fbbeea68d74c5751def
Author: Xie XiuQi <xiexiuqi@huawei.com>
Date:   Tue Sep 6 16:55:38 2016 +0800

    drm: fix signed integer overflow
    
    Use 1UL for unsigned long, or we'll meet a overflow issue with UBSAN.
    
    [   15.589489] UBSAN: Undefined behaviour in drivers/gpu/drm/drm_hashtab.c:145:35
    [   15.589500] signed integer overflow:
    [   15.589999] -2147483648 - 1 cannot be represented in type 'int'
    [   15.590434] CPU: 2 PID: 294 Comm: plymouthd Not tainted 3.10.0-327.28.3.el7.x86_64 #1
    [   15.590653] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 01/07/2011
    [   15.591001]  1ffff1000670fe83 000000000d6b385e ffff88003387f3e0 ffffffff81ee3140
    [   15.591028]  ffff88003387f3f8 ffffffff81ee31fd ffffffffa032f460 ffff88003387f560
    [   15.591044]  ffffffff81ee46e2 0000002d00000009 0000000000000001 0000000041b58ab3
    [   15.591059] Call Trace:
    [   15.591078]  [<ffffffff81ee3140>] dump_stack+0x1e/0x20
    [   15.591093]  [<ffffffff81ee31fd>] ubsan_epilogue+0x12/0x55
    [   15.591109]  [<ffffffff81ee46e2>] handle_overflow+0x1ba/0x215
    [   15.591126]  [<ffffffff81ee4528>] ? __ubsan_handle_negate_overflow+0x162/0x162
    [   15.591146]  [<ffffffff8103416c>] ? print_context_stack+0x9c/0x160
    [   15.591163]  [<ffffffff81031df2>] ? dump_trace+0x252/0x750
    [   15.591181]  [<ffffffff81739023>] ? __list_add+0x93/0x160
    [   15.591197]  [<ffffffff81ee4798>] __ubsan_handle_sub_overflow+0x2a/0x31
    [   15.591261]  [<ffffffffa0282140>] drm_ht_just_insert_please+0x1e0/0x200 [drm]
    [   15.591290]  [<ffffffffa0528c7a>] ttm_base_object_init+0x10a/0x270 [ttm]
    [   15.591316]  [<ffffffffa052a34c>] ttm_vt_lock+0x28c/0x3a0 [ttm]
    [   15.591343]  [<ffffffffa052a0c0>] ? ttm_write_lock+0x180/0x180 [ttm]
    [   15.591362]  [<ffffffff81419526>] ? kasan_unpoison_shadow+0x36/0x50
    [   15.591379]  [<ffffffff81419526>] ? kasan_unpoison_shadow+0x36/0x50
    [   15.591396]  [<ffffffff81419526>] ? kasan_unpoison_shadow+0x36/0x50
    [   15.591413]  [<ffffffff81419526>] ? kasan_unpoison_shadow+0x36/0x50
    [   15.591442]  [<ffffffffa061cbe1>] vmw_master_set+0x121/0x470 [vmwgfx]
    [   15.591459]  [<ffffffff811773a5>] ? __init_waitqueue_head+0x45/0x70
    [   15.591487]  [<ffffffffa061cac0>] ? vmw_master_drop+0x310/0x310 [vmwgfx]
    [   15.591535]  [<ffffffffa026946a>] drm_open+0x92a/0xc00 [drm]
    [   15.591563]  [<ffffffffa0619ff0>] ? vmw_driver_open+0x170/0x170 [vmwgfx]
    [   15.591610]  [<ffffffffa0268b40>] ? drm_poll+0xe0/0xe0 [drm]
    [   15.591661]  [<ffffffffa02797b4>] drm_stub_open+0x224/0x330 [drm]
    [   15.591711]  [<ffffffffa0279590>] ? drm_minor_acquire+0x240/0x240 [drm]
    [   15.591727]  [<ffffffff8145fa8a>] chrdev_open+0x1fa/0x3f0
    [   15.591742]  [<ffffffff8145f890>] ? cdev_put+0x50/0x50
    [   15.591761]  [<ffffffff814f6dc3>] ? __fsnotify_parent+0x53/0x210
    [   15.591778]  [<ffffffff8144fde1>] do_dentry_open+0x351/0x670
    [   15.591792]  [<ffffffff8145f890>] ? cdev_put+0x50/0x50
    [   15.591807]  [<ffffffff814503c2>] vfs_open+0xa2/0x170
    [   15.591824]  [<ffffffff8147b5df>] do_last+0xccf/0x2c80
    [   15.591842]  [<ffffffff8147a910>] ? filename_create+0x320/0x320
    [   15.591858]  [<ffffffff81472549>] ? path_init+0x1b9/0xa90
    [   15.591875]  [<ffffffff81472390>] ? mountpoint_last+0x9a0/0x9a0
    [   15.591894]  [<ffffffff815f9ccf>] ? selinux_file_alloc_security+0xcf/0x130
    [   15.591911]  [<ffffffff8147d777>] path_openat+0x1e7/0xcc0
    [   15.591927]  [<ffffffff81031df2>] ? dump_trace+0x252/0x750
    [   15.591943]  [<ffffffff8147d590>] ? do_last+0x2c80/0x2c80
    [   15.591959]  [<ffffffff81739023>] ? __list_add+0x93/0x160
    [   15.591974]  [<ffffffff8104b48d>] ? save_stack_trace+0x7d/0xb0
    [   15.591989]  [<ffffffff81480824>] do_filp_open+0xa4/0x160
    [   15.592004]  [<ffffffff81480780>] ? user_path_mountpoint_at+0x50/0x50
    [   15.592022]  [<ffffffff8149d755>] ? __alloc_fd+0x175/0x300
    [   15.592039]  [<ffffffff81453127>] do_sys_open+0x1b7/0x3f0
    [   15.592054]  [<ffffffff81452f70>] ? filp_open+0x80/0x80
    [   15.592070]  [<ffffffff81453392>] SyS_open+0x32/0x40
    [   15.592088]  [<ffffffff81f08989>] system_call_fastpath+0x16/0x1b
    
    Signed-off-by: Xie XiuQi <xiexiuqi@huawei.com>
    [seanpaul tweaked subject to remove "gpu/"]
    Signed-off-by: Sean Paul <seanpaul@chromium.org>
    Link: http://patchwork.freedesktop.org/patch/msgid/1473152138-25335-1-git-send-email-xiexiuqi@huawei.com

commit 4141b36ab16d7a66b4cf712f2d21eba61c5927e5
Author: Steffen Klassert <steffen.klassert@secunet.com>
Date:   Wed Aug 24 13:08:40 2016 +0200

    xfrm: Fix xfrm_policy_lock imbalance
    
    An earlier patch accidentally replaced a write_lock_bh
    with a spin_unlock_bh. Fix this by using spin_lock_bh
    instead.
    
    Fixes: 9d0380df6217 ("xfrm: policy: convert policy_lock to spinlock")
    Signed-off-by: Steffen Klassert <steffen.klassert@secunet.com>

commit 1a7af12ae0d7926a4a27225d6a0817761929cb2c
Author: Ben Dooks <ben.dooks@codethink.co.uk>
Date:   Tue Jun 7 17:22:17 2016 +0100

    gpio: bcm-kona: fix bcm_kona_gpio_reset() warnings
    
    commit b66b2a0adf0e48973b582e055758b9907a7eee7c upstream.
    
    The bcm_kona_gpio_reset() calls bcm_kona_gpio_write_lock_regs()
    with what looks like the wrong parameter. The write_lock_regs
    function takes a pointer to the registers, not the bcm_kona_gpio
    structure.
    
    Fix the warning, and probably bug by changing the function to
    pass reg_base instead of kona_gpio, fixing the following warning:
    
    drivers/gpio/gpio-bcm-kona.c:550:47: warning: incorrect type in argument 1
      (different address spaces)
      expected void [noderef] <asn:2>*reg_base
      got struct bcm_kona_gpio *kona_gpio
      warning: incorrect type in argument 1 (different address spaces)
      expected void [noderef] <asn:2>*reg_base
      got struct bcm_kona_gpio *kona_gpio
    
    Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
    Acked-by: Ray Jui <ray.jui@broadcom.com>
    Reviewed-by: Markus Mayer <mmayer@broadcom.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 48b4800a1c6af2cdda344ea4e2c843dcc1f6afc9
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 26 15:23:31 2016 -0700

    zsmalloc: page migration support
    
    This patch introduces run-time migration feature for zspage.
    
    For migration, VM uses page.lru field so it would be better to not use
    page.next field which is unified with page.lru for own purpose.  For
    that, firstly, we can get first object offset of the page via runtime
    calculation instead of using page.index so we can use page.index as link
    for page chaining instead of page.next.
    
    In case of huge object, it stores handle to page.index instead of next
    link of page chaining because huge object doesn't need to next link for
    page chaining.  So get_next_page need to identify huge object to return
    NULL.  For it, this patch uses PG_owner_priv_1 flag of the page flag.
    
    For migration, it supports three functions
    
    * zs_page_isolate
    
    It isolates a zspage which includes a subpage VM want to migrate from
    class so anyone cannot allocate new object from the zspage.
    
    We could try to isolate a zspage by the number of subpage so subsequent
    isolation trial of other subpage of the zpsage shouldn't fail.  For
    that, we introduce zspage.isolated count.  With that, zs_page_isolate
    can know whether zspage is already isolated or not for migration so if
    it is isolated for migration, subsequent isolation trial can be
    successful without trying further isolation.
    
    * zs_page_migrate
    
    First of all, it holds write-side zspage->lock to prevent migrate other
    subpage in zspage.  Then, lock all objects in the page VM want to
    migrate.  The reason we should lock all objects in the page is due to
    race between zs_map_object and zs_page_migrate.
    
      zs_map_object                         zs_page_migrate
    
      pin_tag(handle)
      obj = handle_to_obj(handle)
      obj_to_location(obj, &page, &obj_idx);
    
                                            write_lock(&zspage->lock)
                                            if (!trypin_tag(handle))
                                                    goto unpin_object
    
      zspage = get_zspage(page);
      read_lock(&zspage->lock);
    
    If zs_page_migrate doesn't do trypin_tag, zs_map_object's page can be
    stale by migration so it goes crash.
    
    If it locks all of objects successfully, it copies content from old page
    to new one, finally, create new zspage chain with new page.  And if it's
    last isolated subpage in the zspage, put the zspage back to class.
    
    * zs_page_putback
    
    It returns isolated zspage to right fullness_group list if it fails to
    migrate a page.  If it find a zspage is ZS_EMPTY, it queues zspage
    freeing to workqueue.  See below about async zspage freeing.
    
    This patch introduces asynchronous zspage free.  The reason to need it
    is we need page_lock to clear PG_movable but unfortunately, zs_free path
    should be atomic so the apporach is try to grab page_lock.  If it got
    page_lock of all of pages successfully, it can free zspage immediately.
    Otherwise, it queues free request and free zspage via workqueue in
    process context.
    
    If zs_free finds the zspage is isolated when it try to free zspage, it
    delays the freeing until zs_page_putback finds it so it will free free
    the zspage finally.
    
    In this patch, we expand fullness_list from ZS_EMPTY to ZS_FULL.  First
    of all, it will use ZS_EMPTY list for delay freeing.  And with adding
    ZS_FULL list, it makes to identify whether zspage is isolated or not via
    list_empty(&zspage->list) test.
    
    [minchan@kernel.org: zsmalloc: keep first object offset in struct page]
      Link: http://lkml.kernel.org/r/1465788015-23195-1-git-send-email-minchan@kernel.org
    [minchan@kernel.org: zsmalloc: zspage sanity check]
      Link: http://lkml.kernel.org/r/20160603010129.GC3304@bbox
    Link: http://lkml.kernel.org/r/1464736881-24886-12-git-send-email-minchan@kernel.org
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ac63dc3fb6c8f20f50944c3c6214ffe1490a28c4
Author: Ben Dooks <ben.dooks@codethink.co.uk>
Date:   Tue Jun 7 17:22:17 2016 +0100

    gpio: bcm-kona: fix bcm_kona_gpio_reset() warnings
    
    commit b66b2a0adf0e48973b582e055758b9907a7eee7c upstream.
    
    The bcm_kona_gpio_reset() calls bcm_kona_gpio_write_lock_regs()
    with what looks like the wrong parameter. The write_lock_regs
    function takes a pointer to the registers, not the bcm_kona_gpio
    structure.
    
    Fix the warning, and probably bug by changing the function to
    pass reg_base instead of kona_gpio, fixing the following warning:
    
    drivers/gpio/gpio-bcm-kona.c:550:47: warning: incorrect type in argument 1
      (different address spaces)
      expected void [noderef] <asn:2>*reg_base
      got struct bcm_kona_gpio *kona_gpio
      warning: incorrect type in argument 1 (different address spaces)
      expected void [noderef] <asn:2>*reg_base
      got struct bcm_kona_gpio *kona_gpio
    
    Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
    Acked-by: Ray Jui <ray.jui@broadcom.com>
    Reviewed-by: Markus Mayer <mmayer@broadcom.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e1c35534e3684e25053f5caf6e032956894e8b1f
Author: Ben Dooks <ben.dooks@codethink.co.uk>
Date:   Tue Jun 7 17:22:17 2016 +0100

    gpio: bcm-kona: fix bcm_kona_gpio_reset() warnings
    
    commit b66b2a0adf0e48973b582e055758b9907a7eee7c upstream.
    
    The bcm_kona_gpio_reset() calls bcm_kona_gpio_write_lock_regs()
    with what looks like the wrong parameter. The write_lock_regs
    function takes a pointer to the registers, not the bcm_kona_gpio
    structure.
    
    Fix the warning, and probably bug by changing the function to
    pass reg_base instead of kona_gpio, fixing the following warning:
    
    drivers/gpio/gpio-bcm-kona.c:550:47: warning: incorrect type in argument 1
      (different address spaces)
      expected void [noderef] <asn:2>*reg_base
      got struct bcm_kona_gpio *kona_gpio
      warning: incorrect type in argument 1 (different address spaces)
      expected void [noderef] <asn:2>*reg_base
      got struct bcm_kona_gpio *kona_gpio
    
    Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
    Acked-by: Ray Jui <ray.jui@broadcom.com>
    Reviewed-by: Markus Mayer <mmayer@broadcom.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1f9737df6d4e31106df4dd929cb3f936d2ce955d
Author: Ben Dooks <ben.dooks@codethink.co.uk>
Date:   Tue Jun 7 17:22:17 2016 +0100

    gpio: bcm-kona: fix bcm_kona_gpio_reset() warnings
    
    [ Upstream commit b66b2a0adf0e48973b582e055758b9907a7eee7c ]
    
    The bcm_kona_gpio_reset() calls bcm_kona_gpio_write_lock_regs()
    with what looks like the wrong parameter. The write_lock_regs
    function takes a pointer to the registers, not the bcm_kona_gpio
    structure.
    
    Fix the warning, and probably bug by changing the function to
    pass reg_base instead of kona_gpio, fixing the following warning:
    
    drivers/gpio/gpio-bcm-kona.c:550:47: warning: incorrect type in argument 1
      (different address spaces)
      expected void [noderef] <asn:2>*reg_base
      got struct bcm_kona_gpio *kona_gpio
      warning: incorrect type in argument 1 (different address spaces)
      expected void [noderef] <asn:2>*reg_base
      got struct bcm_kona_gpio *kona_gpio
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
    Acked-by: Ray Jui <ray.jui@broadcom.com>
    Reviewed-by: Markus Mayer <mmayer@broadcom.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>

commit 8921c300bb0f67ddced9e50330b894556c72c87d
Author: Ben Dooks <ben.dooks@codethink.co.uk>
Date:   Tue Jun 7 17:22:17 2016 +0100

    gpio: bcm-kona: fix bcm_kona_gpio_reset() warnings
    
    [ Upstream commit b66b2a0adf0e48973b582e055758b9907a7eee7c ]
    
    The bcm_kona_gpio_reset() calls bcm_kona_gpio_write_lock_regs()
    with what looks like the wrong parameter. The write_lock_regs
    function takes a pointer to the registers, not the bcm_kona_gpio
    structure.
    
    Fix the warning, and probably bug by changing the function to
    pass reg_base instead of kona_gpio, fixing the following warning:
    
    drivers/gpio/gpio-bcm-kona.c:550:47: warning: incorrect type in argument 1
      (different address spaces)
      expected void [noderef] <asn:2>*reg_base
      got struct bcm_kona_gpio *kona_gpio
      warning: incorrect type in argument 1 (different address spaces)
      expected void [noderef] <asn:2>*reg_base
      got struct bcm_kona_gpio *kona_gpio
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
    Acked-by: Ray Jui <ray.jui@broadcom.com>
    Reviewed-by: Markus Mayer <mmayer@broadcom.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>

commit b66b2a0adf0e48973b582e055758b9907a7eee7c
Author: Ben Dooks <ben.dooks@codethink.co.uk>
Date:   Tue Jun 7 17:22:17 2016 +0100

    gpio: bcm-kona: fix bcm_kona_gpio_reset() warnings
    
    The bcm_kona_gpio_reset() calls bcm_kona_gpio_write_lock_regs()
    with what looks like the wrong parameter. The write_lock_regs
    function takes a pointer to the registers, not the bcm_kona_gpio
    structure.
    
    Fix the warning, and probably bug by changing the function to
    pass reg_base instead of kona_gpio, fixing the following warning:
    
    drivers/gpio/gpio-bcm-kona.c:550:47: warning: incorrect type in argument 1
      (different address spaces)
      expected void [noderef] <asn:2>*reg_base
      got struct bcm_kona_gpio *kona_gpio
      warning: incorrect type in argument 1 (different address spaces)
      expected void [noderef] <asn:2>*reg_base
      got struct bcm_kona_gpio *kona_gpio
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
    Acked-by: Ray Jui <ray.jui@broadcom.com>
    Reviewed-by: Markus Mayer <mmayer@broadcom.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 6112a300c9e41993cc0dc56ac393743d28381284
Author: Soumya PN <soumya.p.n@hpe.com>
Date:   Tue May 17 21:31:14 2016 +0530

    ftrace: Don't disable irqs when taking the tasklist_lock read_lock
    
    In ftrace.c inside the function alloc_retstack_tasklist() (which will be
    invoked when function_graph tracing is on) the tasklist_lock is being
    held as reader while iterating through a list of threads. Here the lock
    is being held as reader with irqs disabled. The tasklist_lock is never
    write_locked in interrupt context so it is safe to not disable interrupts
    for the duration of read_lock in this block which, can be significant,
    given the block of code iterates through all threads. Hence changing the
    code to call read_lock() and read_unlock() instead of read_lock_irqsave()
    and read_unlock_irqrestore().
    
    A similar change was made in commits: 8063e41d2ffc ("tracing: Change
    syscall_*regfunc() to check PF_KTHREAD and use for_each_process_thread()")'
    and 3472eaa1f12e ("sched: normalize_rt_tasks(): Don't use _irqsave for
    tasklist_lock, use task_rq_lock()")'
    
    Link: http://lkml.kernel.org/r/1463500874-77480-1-git-send-email-soumya.p.n@hpe.com
    
    Signed-off-by: Soumya PN <soumya.p.n@hpe.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 84e0e38744c5d814650e4acec34cea585d04cc96
Author: Vaibhav Agarwal <vaibhav.agarwal@linaro.org>
Date:   Wed May 4 16:29:19 2016 +0530

    greybus: audio:gb_manager: Use proper locking around kobject_xxx
    
    read/write_lock_irqsave mechanism was used to protect modules
    list & kobject_xxx() in gb_audio_manager. Since kobject_xxx calls
    can sleep spin_lock variants can't be used there. So use rw_sem
    for protecting modules_list.
    
    Signed-off-by: Vaibhav Agarwal <vaibhav.agarwal@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@google.com>

commit 9f88ecf6c707ef3a6e26a8bd58e948728d2093d6
Author: Subash Abhinov Kasiviswanathan <subashab@codeaurora.org>
Date:   Tue Feb 2 02:11:10 2016 +0000

    ipv6: addrconf: Fix recursive spin lock call
    
    [ Upstream commit 16186a82de1fdd868255448274e64ae2616e2640 ]
    
    A rcu stall with the following backtrace was seen on a system with
    forwarding, optimistic_dad and use_optimistic set. To reproduce,
    set these flags and allow ipv6 autoconf.
    
    This occurs because the device write_lock is acquired while already
    holding the read_lock. Back trace below -
    
    INFO: rcu_preempt self-detected stall on CPU { 1}  (t=2100 jiffies
     g=3992 c=3991 q=4471)
    <6> Task dump for CPU 1:
    <2> kworker/1:0     R  running task    12168    15   2 0x00000002
    <2> Workqueue: ipv6_addrconf addrconf_dad_work
    <6> Call trace:
    <2> [<ffffffc000084da8>] el1_irq+0x68/0xdc
    <2> [<ffffffc000cc4e0c>] _raw_write_lock_bh+0x20/0x30
    <2> [<ffffffc000bc5dd8>] __ipv6_dev_ac_inc+0x64/0x1b4
    <2> [<ffffffc000bcbd2c>] addrconf_join_anycast+0x9c/0xc4
    <2> [<ffffffc000bcf9f0>] __ipv6_ifa_notify+0x160/0x29c
    <2> [<ffffffc000bcfb7c>] ipv6_ifa_notify+0x50/0x70
    <2> [<ffffffc000bd035c>] addrconf_dad_work+0x314/0x334
    <2> [<ffffffc0000b64c8>] process_one_work+0x244/0x3fc
    <2> [<ffffffc0000b7324>] worker_thread+0x2f8/0x418
    <2> [<ffffffc0000bb40c>] kthread+0xe0/0xec
    
    v2: do addrconf_dad_kick inside read lock and then acquire write
    lock for ipv6_ifa_notify as suggested by Eric
    
    Fixes: 7fd2561e4ebdd ("net: ipv6: Add a sysctl to make optimistic
    addresses useful candidates")
    
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Erik Kline <ek@google.com>
    Cc: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: Subash Abhinov Kasiviswanathan <subashab@codeaurora.org>
    Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>

commit cdbc66828d250b463c0ca3a1e715965b57f2c1a3
Author: Subash Abhinov Kasiviswanathan <subashab@codeaurora.org>
Date:   Tue Feb 2 02:11:10 2016 +0000

    ipv6: addrconf: Fix recursive spin lock call
    
    [ Upstream commit 16186a82de1fdd868255448274e64ae2616e2640 ]
    
    A rcu stall with the following backtrace was seen on a system with
    forwarding, optimistic_dad and use_optimistic set. To reproduce,
    set these flags and allow ipv6 autoconf.
    
    This occurs because the device write_lock is acquired while already
    holding the read_lock. Back trace below -
    
    INFO: rcu_preempt self-detected stall on CPU { 1}  (t=2100 jiffies
     g=3992 c=3991 q=4471)
    <6> Task dump for CPU 1:
    <2> kworker/1:0     R  running task    12168    15   2 0x00000002
    <2> Workqueue: ipv6_addrconf addrconf_dad_work
    <6> Call trace:
    <2> [<ffffffc000084da8>] el1_irq+0x68/0xdc
    <2> [<ffffffc000cc4e0c>] _raw_write_lock_bh+0x20/0x30
    <2> [<ffffffc000bc5dd8>] __ipv6_dev_ac_inc+0x64/0x1b4
    <2> [<ffffffc000bcbd2c>] addrconf_join_anycast+0x9c/0xc4
    <2> [<ffffffc000bcf9f0>] __ipv6_ifa_notify+0x160/0x29c
    <2> [<ffffffc000bcfb7c>] ipv6_ifa_notify+0x50/0x70
    <2> [<ffffffc000bd035c>] addrconf_dad_work+0x314/0x334
    <2> [<ffffffc0000b64c8>] process_one_work+0x244/0x3fc
    <2> [<ffffffc0000b7324>] worker_thread+0x2f8/0x418
    <2> [<ffffffc0000bb40c>] kthread+0xe0/0xec
    
    v2: do addrconf_dad_kick inside read lock and then acquire write
    lock for ipv6_ifa_notify as suggested by Eric
    
    Fixes: 7fd2561e4ebdd ("net: ipv6: Add a sysctl to make optimistic
    addresses useful candidates")
    
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Erik Kline <ek@google.com>
    Cc: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: Subash Abhinov Kasiviswanathan <subashab@codeaurora.org>
    Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fb87262aec0685ef7fe89ae50a5ee900d48db4d4
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Feb 3 19:17:27 2016 +0000

    Btrfs: fix hang on extent buffer lock caused by the inode_paths ioctl
    
    [ Upstream commit 0c0fe3b0fa45082cd752553fdb3a4b42503a118e ]
    
    While doing some tests I ran into an hang on an extent buffer's rwlock
    that produced the following trace:
    
    [39389.800012] NMI watchdog: BUG: soft lockup - CPU#15 stuck for 22s! [fdm-stress:32166]
    [39389.800016] NMI watchdog: BUG: soft lockup - CPU#14 stuck for 22s! [fdm-stress:32165]
    [39389.800016] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800016] irq event stamp: 0
    [39389.800016] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800016] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800016] CPU: 14 PID: 32165 Comm: fdm-stress Not tainted 4.4.0-rc6-btrfs-next-18+ #1
    [39389.800016] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800016] task: ffff880175b1ca40 ti: ffff8800a185c000 task.ti: ffff8800a185c000
    [39389.800016] RIP: 0010:[<ffffffff810902af>]  [<ffffffff810902af>] queued_spin_lock_slowpath+0x57/0x158
    [39389.800016] RSP: 0018:ffff8800a185fb80  EFLAGS: 00000202
    [39389.800016] RAX: 0000000000000101 RBX: ffff8801710c4e9c RCX: 0000000000000101
    [39389.800016] RDX: 0000000000000100 RSI: 0000000000000001 RDI: 0000000000000001
    [39389.800016] RBP: ffff8800a185fb98 R08: 0000000000000001 R09: 0000000000000000
    [39389.800016] R10: ffff8800a185fb68 R11: 6db6db6db6db6db7 R12: ffff8801710c4e98
    [39389.800016] R13: ffff880175b1ca40 R14: ffff8800a185fc10 R15: ffff880175b1ca40
    [39389.800016] FS:  00007f6d37fff700(0000) GS:ffff8802be9c0000(0000) knlGS:0000000000000000
    [39389.800016] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800016] CR2: 00007f6d300019b8 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800016] Stack:
    [39389.800016]  ffff8801710c4e98 ffff8801710c4e98 ffff880175b1ca40 ffff8800a185fbb0
    [39389.800016]  ffffffff81091e11 ffff8801710c4e98 ffff8800a185fbc8 ffffffff81091895
    [39389.800016]  ffff8801710c4e98 ffff8800a185fbe8 ffffffff81486c5c ffffffffa067288c
    [39389.800016] Call Trace:
    [39389.800016]  [<ffffffff81091e11>] queued_read_lock_slowpath+0x46/0x60
    [39389.800016]  [<ffffffff81091895>] do_raw_read_lock+0x3e/0x41
    [39389.800016]  [<ffffffff81486c5c>] _raw_read_lock+0x3d/0x44
    [39389.800016]  [<ffffffffa067288c>] ? btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa067288c>] btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa0622ced>] ? btrfs_find_item+0xa7/0xd2 [btrfs]
    [39389.800016]  [<ffffffffa069363f>] btrfs_ref_to_path+0xd6/0x174 [btrfs]
    [39389.800016]  [<ffffffffa0693730>] inode_to_path+0x53/0xa2 [btrfs]
    [39389.800016]  [<ffffffffa0693e2e>] paths_from_inode+0x117/0x2ec [btrfs]
    [39389.800016]  [<ffffffffa0670cff>] btrfs_ioctl+0xd5b/0x2793 [btrfs]
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff81276727>] ? __this_cpu_preempt_check+0x13/0x15
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800016]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800016]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800016]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800016]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800016] Code: b9 01 01 00 00 f7 c6 00 ff ff ff 75 32 83 fe 01 89 ca 89 f0 0f 45 d7 f0 0f b1 13 39 f0 74 04 89 c6 eb e2 ff ca 0f 84 fa 00 00 00 <8b> 03 84 c0 74 04 f3 90 eb f6 66 c7 03 01 00 e9 e6 00 00 00 e8
    [39389.800012] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800012] irq event stamp: 0
    [39389.800012] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800012] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800012] CPU: 15 PID: 32166 Comm: fdm-stress Tainted: G             L  4.4.0-rc6-btrfs-next-18+ #1
    [39389.800012] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800012] task: ffff880179294380 ti: ffff880034a60000 task.ti: ffff880034a60000
    [39389.800012] RIP: 0010:[<ffffffff81091e8d>]  [<ffffffff81091e8d>] queued_write_lock_slowpath+0x62/0x72
    [39389.800012] RSP: 0018:ffff880034a639f0  EFLAGS: 00000206
    [39389.800012] RAX: 0000000000000101 RBX: ffff8801710c4e98 RCX: 0000000000000000
    [39389.800012] RDX: 00000000000000ff RSI: 0000000000000000 RDI: ffff8801710c4e9c
    [39389.800012] RBP: ffff880034a639f8 R08: 0000000000000001 R09: 0000000000000000
    [39389.800012] R10: ffff880034a639b0 R11: 0000000000001000 R12: ffff8801710c4e98
    [39389.800012] R13: 0000000000000001 R14: ffff880172cbc000 R15: ffff8801710c4e00
    [39389.800012] FS:  00007f6d377fe700(0000) GS:ffff8802be9e0000(0000) knlGS:0000000000000000
    [39389.800012] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800012] CR2: 00007f6d3d3c1000 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800012] Stack:
    [39389.800012]  ffff8801710c4e98 ffff880034a63a10 ffffffff81091963 ffff8801710c4e98
    [39389.800012]  ffff880034a63a30 ffffffff81486f1b ffffffffa0672cb3 ffff8801710c4e00
    [39389.800012]  ffff880034a63a78 ffffffffa0672cb3 ffff8801710c4e00 ffff880034a63a58
    [39389.800012] Call Trace:
    [39389.800012]  [<ffffffff81091963>] do_raw_write_lock+0x72/0x8c
    [39389.800012]  [<ffffffff81486f1b>] _raw_write_lock+0x3a/0x41
    [39389.800012]  [<ffffffffa0672cb3>] ? btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa0672cb3>] btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa061aeba>] ? rcu_read_unlock+0x5b/0x5d [btrfs]
    [39389.800012]  [<ffffffffa061ce13>] ? btrfs_root_node+0xda/0xe6 [btrfs]
    [39389.800012]  [<ffffffffa061ce83>] btrfs_lock_root_node+0x22/0x42 [btrfs]
    [39389.800012]  [<ffffffffa062046b>] btrfs_search_slot+0x1b8/0x758 [btrfs]
    [39389.800012]  [<ffffffff810fc6b0>] ? time_hardirqs_on+0x15/0x28
    [39389.800012]  [<ffffffffa06365db>] btrfs_lookup_inode+0x31/0x95 [btrfs]
    [39389.800012]  [<ffffffff8108d62f>] ? trace_hardirqs_on+0xd/0xf
    [39389.800012]  [<ffffffff8148482b>] ? mutex_lock_nested+0x397/0x3bc
    [39389.800012]  [<ffffffffa068821b>] __btrfs_update_delayed_inode+0x59/0x1c0 [btrfs]
    [39389.800012]  [<ffffffffa068858e>] __btrfs_commit_inode_delayed_items+0x194/0x5aa [btrfs]
    [39389.800012]  [<ffffffff81486ab7>] ? _raw_spin_unlock+0x31/0x44
    [39389.800012]  [<ffffffffa0688a48>] __btrfs_run_delayed_items+0xa4/0x15c [btrfs]
    [39389.800012]  [<ffffffffa0688d62>] btrfs_run_delayed_items+0x11/0x13 [btrfs]
    [39389.800012]  [<ffffffffa064048e>] btrfs_commit_transaction+0x234/0x96e [btrfs]
    [39389.800012]  [<ffffffffa0618d10>] btrfs_sync_fs+0x145/0x1ad [btrfs]
    [39389.800012]  [<ffffffffa0671176>] btrfs_ioctl+0x11d2/0x2793 [btrfs]
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800012]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800012]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800012]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800012]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800012] Code: f0 0f b1 13 85 c0 75 ef eb 2a f3 90 8a 03 84 c0 75 f8 f0 0f b0 13 84 c0 75 f0 ba ff 00 00 00 eb 0a f0 0f b1 13 ff c8 74 0b f3 90 <8b> 03 83 f8 01 75 f7 eb ed c6 43 04 00 5b 5d c3 0f 1f 44 00 00
    
    This happens because in the code path executed by the inode_paths ioctl we
    end up nesting two calls to read lock a leaf's rwlock when after the first
    call to read_lock() and before the second call to read_lock(), another
    task (running the delayed items as part of a transaction commit) has
    already called write_lock() against the leaf's rwlock. This situation is
    illustrated by the following diagram:
    
             Task A                       Task B
    
      btrfs_ref_to_path()               btrfs_commit_transaction()
        read_lock(&eb->lock);
    
                                          btrfs_run_delayed_items()
                                            __btrfs_commit_inode_delayed_items()
                                              __btrfs_update_delayed_inode()
                                                btrfs_lookup_inode()
    
                                                  write_lock(&eb->lock);
                                                    --> task waits for lock
    
        read_lock(&eb->lock);
        --> makes this task hang
            forever (and task B too
            of course)
    
    So fix this by avoiding doing the nested read lock, which is easily
    avoidable. This issue does not happen if task B calls write_lock() after
    task A does the second call to read_lock(), however there does not seem
    to exist anything in the documentation that mentions what is the expected
    behaviour for recursive locking of rwlocks (leaving the idea that doing
    so is not a good usage of rwlocks).
    
    Also, as a side effect necessary for this fix, make sure we do not
    needlessly read lock extent buffers when the input path has skip_locking
    set (used when called from send).
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>

commit 0ad9b20415a461332611666301e7812900a15ad4
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Mon Feb 29 21:17:11 2016 +0200

    net/mlx5e: Fix soft lockup when HW Timestamping is enabled
    
    Readers/Writers lock for SW timecounter was acquired without disabling
    interrupts on local CPU.
    
    The problematic scenario:
    * HW timestamping is enabled
    * Timestamp overflow periodic service task is running on local CPU and
      holding write_lock for SW timecounter
    * Completion arrives, triggers interrupt for local CPU.
      Interrupt routine calls napi_schedule(), which triggers rx/tx
      skb process.
      An attempt to read SW timecounter using read_lock is done, which is
      already locked by a writer on the same CPU and cause soft lockup.
    
    Add irqsave/irqrestore for when using the readers/writers lock for
    writing.
    
    Fixes: ef9814deafd0 ('net/mlx5e: Add HW timestamping (TS) support')
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 79179a68458a7fddcf35adb16d1f7a7880aa41e7
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Feb 3 19:17:27 2016 +0000

    Btrfs: fix hang on extent buffer lock caused by the inode_paths ioctl
    
    [ Upstream commit 0c0fe3b0fa45082cd752553fdb3a4b42503a118e ]
    
    While doing some tests I ran into an hang on an extent buffer's rwlock
    that produced the following trace:
    
    [39389.800012] NMI watchdog: BUG: soft lockup - CPU#15 stuck for 22s! [fdm-stress:32166]
    [39389.800016] NMI watchdog: BUG: soft lockup - CPU#14 stuck for 22s! [fdm-stress:32165]
    [39389.800016] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800016] irq event stamp: 0
    [39389.800016] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800016] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800016] CPU: 14 PID: 32165 Comm: fdm-stress Not tainted 4.4.0-rc6-btrfs-next-18+ #1
    [39389.800016] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800016] task: ffff880175b1ca40 ti: ffff8800a185c000 task.ti: ffff8800a185c000
    [39389.800016] RIP: 0010:[<ffffffff810902af>]  [<ffffffff810902af>] queued_spin_lock_slowpath+0x57/0x158
    [39389.800016] RSP: 0018:ffff8800a185fb80  EFLAGS: 00000202
    [39389.800016] RAX: 0000000000000101 RBX: ffff8801710c4e9c RCX: 0000000000000101
    [39389.800016] RDX: 0000000000000100 RSI: 0000000000000001 RDI: 0000000000000001
    [39389.800016] RBP: ffff8800a185fb98 R08: 0000000000000001 R09: 0000000000000000
    [39389.800016] R10: ffff8800a185fb68 R11: 6db6db6db6db6db7 R12: ffff8801710c4e98
    [39389.800016] R13: ffff880175b1ca40 R14: ffff8800a185fc10 R15: ffff880175b1ca40
    [39389.800016] FS:  00007f6d37fff700(0000) GS:ffff8802be9c0000(0000) knlGS:0000000000000000
    [39389.800016] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800016] CR2: 00007f6d300019b8 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800016] Stack:
    [39389.800016]  ffff8801710c4e98 ffff8801710c4e98 ffff880175b1ca40 ffff8800a185fbb0
    [39389.800016]  ffffffff81091e11 ffff8801710c4e98 ffff8800a185fbc8 ffffffff81091895
    [39389.800016]  ffff8801710c4e98 ffff8800a185fbe8 ffffffff81486c5c ffffffffa067288c
    [39389.800016] Call Trace:
    [39389.800016]  [<ffffffff81091e11>] queued_read_lock_slowpath+0x46/0x60
    [39389.800016]  [<ffffffff81091895>] do_raw_read_lock+0x3e/0x41
    [39389.800016]  [<ffffffff81486c5c>] _raw_read_lock+0x3d/0x44
    [39389.800016]  [<ffffffffa067288c>] ? btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa067288c>] btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa0622ced>] ? btrfs_find_item+0xa7/0xd2 [btrfs]
    [39389.800016]  [<ffffffffa069363f>] btrfs_ref_to_path+0xd6/0x174 [btrfs]
    [39389.800016]  [<ffffffffa0693730>] inode_to_path+0x53/0xa2 [btrfs]
    [39389.800016]  [<ffffffffa0693e2e>] paths_from_inode+0x117/0x2ec [btrfs]
    [39389.800016]  [<ffffffffa0670cff>] btrfs_ioctl+0xd5b/0x2793 [btrfs]
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff81276727>] ? __this_cpu_preempt_check+0x13/0x15
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800016]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800016]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800016]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800016]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800016] Code: b9 01 01 00 00 f7 c6 00 ff ff ff 75 32 83 fe 01 89 ca 89 f0 0f 45 d7 f0 0f b1 13 39 f0 74 04 89 c6 eb e2 ff ca 0f 84 fa 00 00 00 <8b> 03 84 c0 74 04 f3 90 eb f6 66 c7 03 01 00 e9 e6 00 00 00 e8
    [39389.800012] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800012] irq event stamp: 0
    [39389.800012] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800012] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800012] CPU: 15 PID: 32166 Comm: fdm-stress Tainted: G             L  4.4.0-rc6-btrfs-next-18+ #1
    [39389.800012] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800012] task: ffff880179294380 ti: ffff880034a60000 task.ti: ffff880034a60000
    [39389.800012] RIP: 0010:[<ffffffff81091e8d>]  [<ffffffff81091e8d>] queued_write_lock_slowpath+0x62/0x72
    [39389.800012] RSP: 0018:ffff880034a639f0  EFLAGS: 00000206
    [39389.800012] RAX: 0000000000000101 RBX: ffff8801710c4e98 RCX: 0000000000000000
    [39389.800012] RDX: 00000000000000ff RSI: 0000000000000000 RDI: ffff8801710c4e9c
    [39389.800012] RBP: ffff880034a639f8 R08: 0000000000000001 R09: 0000000000000000
    [39389.800012] R10: ffff880034a639b0 R11: 0000000000001000 R12: ffff8801710c4e98
    [39389.800012] R13: 0000000000000001 R14: ffff880172cbc000 R15: ffff8801710c4e00
    [39389.800012] FS:  00007f6d377fe700(0000) GS:ffff8802be9e0000(0000) knlGS:0000000000000000
    [39389.800012] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800012] CR2: 00007f6d3d3c1000 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800012] Stack:
    [39389.800012]  ffff8801710c4e98 ffff880034a63a10 ffffffff81091963 ffff8801710c4e98
    [39389.800012]  ffff880034a63a30 ffffffff81486f1b ffffffffa0672cb3 ffff8801710c4e00
    [39389.800012]  ffff880034a63a78 ffffffffa0672cb3 ffff8801710c4e00 ffff880034a63a58
    [39389.800012] Call Trace:
    [39389.800012]  [<ffffffff81091963>] do_raw_write_lock+0x72/0x8c
    [39389.800012]  [<ffffffff81486f1b>] _raw_write_lock+0x3a/0x41
    [39389.800012]  [<ffffffffa0672cb3>] ? btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa0672cb3>] btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa061aeba>] ? rcu_read_unlock+0x5b/0x5d [btrfs]
    [39389.800012]  [<ffffffffa061ce13>] ? btrfs_root_node+0xda/0xe6 [btrfs]
    [39389.800012]  [<ffffffffa061ce83>] btrfs_lock_root_node+0x22/0x42 [btrfs]
    [39389.800012]  [<ffffffffa062046b>] btrfs_search_slot+0x1b8/0x758 [btrfs]
    [39389.800012]  [<ffffffff810fc6b0>] ? time_hardirqs_on+0x15/0x28
    [39389.800012]  [<ffffffffa06365db>] btrfs_lookup_inode+0x31/0x95 [btrfs]
    [39389.800012]  [<ffffffff8108d62f>] ? trace_hardirqs_on+0xd/0xf
    [39389.800012]  [<ffffffff8148482b>] ? mutex_lock_nested+0x397/0x3bc
    [39389.800012]  [<ffffffffa068821b>] __btrfs_update_delayed_inode+0x59/0x1c0 [btrfs]
    [39389.800012]  [<ffffffffa068858e>] __btrfs_commit_inode_delayed_items+0x194/0x5aa [btrfs]
    [39389.800012]  [<ffffffff81486ab7>] ? _raw_spin_unlock+0x31/0x44
    [39389.800012]  [<ffffffffa0688a48>] __btrfs_run_delayed_items+0xa4/0x15c [btrfs]
    [39389.800012]  [<ffffffffa0688d62>] btrfs_run_delayed_items+0x11/0x13 [btrfs]
    [39389.800012]  [<ffffffffa064048e>] btrfs_commit_transaction+0x234/0x96e [btrfs]
    [39389.800012]  [<ffffffffa0618d10>] btrfs_sync_fs+0x145/0x1ad [btrfs]
    [39389.800012]  [<ffffffffa0671176>] btrfs_ioctl+0x11d2/0x2793 [btrfs]
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800012]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800012]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800012]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800012]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800012] Code: f0 0f b1 13 85 c0 75 ef eb 2a f3 90 8a 03 84 c0 75 f8 f0 0f b0 13 84 c0 75 f0 ba ff 00 00 00 eb 0a f0 0f b1 13 ff c8 74 0b f3 90 <8b> 03 83 f8 01 75 f7 eb ed c6 43 04 00 5b 5d c3 0f 1f 44 00 00
    
    This happens because in the code path executed by the inode_paths ioctl we
    end up nesting two calls to read lock a leaf's rwlock when after the first
    call to read_lock() and before the second call to read_lock(), another
    task (running the delayed items as part of a transaction commit) has
    already called write_lock() against the leaf's rwlock. This situation is
    illustrated by the following diagram:
    
             Task A                       Task B
    
      btrfs_ref_to_path()               btrfs_commit_transaction()
        read_lock(&eb->lock);
    
                                          btrfs_run_delayed_items()
                                            __btrfs_commit_inode_delayed_items()
                                              __btrfs_update_delayed_inode()
                                                btrfs_lookup_inode()
    
                                                  write_lock(&eb->lock);
                                                    --> task waits for lock
    
        read_lock(&eb->lock);
        --> makes this task hang
            forever (and task B too
            of course)
    
    So fix this by avoiding doing the nested read lock, which is easily
    avoidable. This issue does not happen if task B calls write_lock() after
    task A does the second call to read_lock(), however there does not seem
    to exist anything in the documentation that mentions what is the expected
    behaviour for recursive locking of rwlocks (leaving the idea that doing
    so is not a good usage of rwlocks).
    
    Also, as a side effect necessary for this fix, make sure we do not
    needlessly read lock extent buffers when the input path has skip_locking
    set (used when called from send).
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>

commit e8eced78e0252040c4e6bb633b40afb11a176416
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Feb 3 19:17:27 2016 +0000

    Btrfs: fix hang on extent buffer lock caused by the inode_paths ioctl
    
    commit 0c0fe3b0fa45082cd752553fdb3a4b42503a118e upstream.
    
    While doing some tests I ran into an hang on an extent buffer's rwlock
    that produced the following trace:
    
    [39389.800012] NMI watchdog: BUG: soft lockup - CPU#15 stuck for 22s! [fdm-stress:32166]
    [39389.800016] NMI watchdog: BUG: soft lockup - CPU#14 stuck for 22s! [fdm-stress:32165]
    [39389.800016] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800016] irq event stamp: 0
    [39389.800016] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800016] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800016] CPU: 14 PID: 32165 Comm: fdm-stress Not tainted 4.4.0-rc6-btrfs-next-18+ #1
    [39389.800016] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800016] task: ffff880175b1ca40 ti: ffff8800a185c000 task.ti: ffff8800a185c000
    [39389.800016] RIP: 0010:[<ffffffff810902af>]  [<ffffffff810902af>] queued_spin_lock_slowpath+0x57/0x158
    [39389.800016] RSP: 0018:ffff8800a185fb80  EFLAGS: 00000202
    [39389.800016] RAX: 0000000000000101 RBX: ffff8801710c4e9c RCX: 0000000000000101
    [39389.800016] RDX: 0000000000000100 RSI: 0000000000000001 RDI: 0000000000000001
    [39389.800016] RBP: ffff8800a185fb98 R08: 0000000000000001 R09: 0000000000000000
    [39389.800016] R10: ffff8800a185fb68 R11: 6db6db6db6db6db7 R12: ffff8801710c4e98
    [39389.800016] R13: ffff880175b1ca40 R14: ffff8800a185fc10 R15: ffff880175b1ca40
    [39389.800016] FS:  00007f6d37fff700(0000) GS:ffff8802be9c0000(0000) knlGS:0000000000000000
    [39389.800016] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800016] CR2: 00007f6d300019b8 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800016] Stack:
    [39389.800016]  ffff8801710c4e98 ffff8801710c4e98 ffff880175b1ca40 ffff8800a185fbb0
    [39389.800016]  ffffffff81091e11 ffff8801710c4e98 ffff8800a185fbc8 ffffffff81091895
    [39389.800016]  ffff8801710c4e98 ffff8800a185fbe8 ffffffff81486c5c ffffffffa067288c
    [39389.800016] Call Trace:
    [39389.800016]  [<ffffffff81091e11>] queued_read_lock_slowpath+0x46/0x60
    [39389.800016]  [<ffffffff81091895>] do_raw_read_lock+0x3e/0x41
    [39389.800016]  [<ffffffff81486c5c>] _raw_read_lock+0x3d/0x44
    [39389.800016]  [<ffffffffa067288c>] ? btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa067288c>] btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa0622ced>] ? btrfs_find_item+0xa7/0xd2 [btrfs]
    [39389.800016]  [<ffffffffa069363f>] btrfs_ref_to_path+0xd6/0x174 [btrfs]
    [39389.800016]  [<ffffffffa0693730>] inode_to_path+0x53/0xa2 [btrfs]
    [39389.800016]  [<ffffffffa0693e2e>] paths_from_inode+0x117/0x2ec [btrfs]
    [39389.800016]  [<ffffffffa0670cff>] btrfs_ioctl+0xd5b/0x2793 [btrfs]
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff81276727>] ? __this_cpu_preempt_check+0x13/0x15
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800016]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800016]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800016]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800016]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800016] Code: b9 01 01 00 00 f7 c6 00 ff ff ff 75 32 83 fe 01 89 ca 89 f0 0f 45 d7 f0 0f b1 13 39 f0 74 04 89 c6 eb e2 ff ca 0f 84 fa 00 00 00 <8b> 03 84 c0 74 04 f3 90 eb f6 66 c7 03 01 00 e9 e6 00 00 00 e8
    [39389.800012] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800012] irq event stamp: 0
    [39389.800012] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800012] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800012] CPU: 15 PID: 32166 Comm: fdm-stress Tainted: G             L  4.4.0-rc6-btrfs-next-18+ #1
    [39389.800012] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800012] task: ffff880179294380 ti: ffff880034a60000 task.ti: ffff880034a60000
    [39389.800012] RIP: 0010:[<ffffffff81091e8d>]  [<ffffffff81091e8d>] queued_write_lock_slowpath+0x62/0x72
    [39389.800012] RSP: 0018:ffff880034a639f0  EFLAGS: 00000206
    [39389.800012] RAX: 0000000000000101 RBX: ffff8801710c4e98 RCX: 0000000000000000
    [39389.800012] RDX: 00000000000000ff RSI: 0000000000000000 RDI: ffff8801710c4e9c
    [39389.800012] RBP: ffff880034a639f8 R08: 0000000000000001 R09: 0000000000000000
    [39389.800012] R10: ffff880034a639b0 R11: 0000000000001000 R12: ffff8801710c4e98
    [39389.800012] R13: 0000000000000001 R14: ffff880172cbc000 R15: ffff8801710c4e00
    [39389.800012] FS:  00007f6d377fe700(0000) GS:ffff8802be9e0000(0000) knlGS:0000000000000000
    [39389.800012] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800012] CR2: 00007f6d3d3c1000 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800012] Stack:
    [39389.800012]  ffff8801710c4e98 ffff880034a63a10 ffffffff81091963 ffff8801710c4e98
    [39389.800012]  ffff880034a63a30 ffffffff81486f1b ffffffffa0672cb3 ffff8801710c4e00
    [39389.800012]  ffff880034a63a78 ffffffffa0672cb3 ffff8801710c4e00 ffff880034a63a58
    [39389.800012] Call Trace:
    [39389.800012]  [<ffffffff81091963>] do_raw_write_lock+0x72/0x8c
    [39389.800012]  [<ffffffff81486f1b>] _raw_write_lock+0x3a/0x41
    [39389.800012]  [<ffffffffa0672cb3>] ? btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa0672cb3>] btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa061aeba>] ? rcu_read_unlock+0x5b/0x5d [btrfs]
    [39389.800012]  [<ffffffffa061ce13>] ? btrfs_root_node+0xda/0xe6 [btrfs]
    [39389.800012]  [<ffffffffa061ce83>] btrfs_lock_root_node+0x22/0x42 [btrfs]
    [39389.800012]  [<ffffffffa062046b>] btrfs_search_slot+0x1b8/0x758 [btrfs]
    [39389.800012]  [<ffffffff810fc6b0>] ? time_hardirqs_on+0x15/0x28
    [39389.800012]  [<ffffffffa06365db>] btrfs_lookup_inode+0x31/0x95 [btrfs]
    [39389.800012]  [<ffffffff8108d62f>] ? trace_hardirqs_on+0xd/0xf
    [39389.800012]  [<ffffffff8148482b>] ? mutex_lock_nested+0x397/0x3bc
    [39389.800012]  [<ffffffffa068821b>] __btrfs_update_delayed_inode+0x59/0x1c0 [btrfs]
    [39389.800012]  [<ffffffffa068858e>] __btrfs_commit_inode_delayed_items+0x194/0x5aa [btrfs]
    [39389.800012]  [<ffffffff81486ab7>] ? _raw_spin_unlock+0x31/0x44
    [39389.800012]  [<ffffffffa0688a48>] __btrfs_run_delayed_items+0xa4/0x15c [btrfs]
    [39389.800012]  [<ffffffffa0688d62>] btrfs_run_delayed_items+0x11/0x13 [btrfs]
    [39389.800012]  [<ffffffffa064048e>] btrfs_commit_transaction+0x234/0x96e [btrfs]
    [39389.800012]  [<ffffffffa0618d10>] btrfs_sync_fs+0x145/0x1ad [btrfs]
    [39389.800012]  [<ffffffffa0671176>] btrfs_ioctl+0x11d2/0x2793 [btrfs]
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800012]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800012]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800012]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800012]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800012] Code: f0 0f b1 13 85 c0 75 ef eb 2a f3 90 8a 03 84 c0 75 f8 f0 0f b0 13 84 c0 75 f0 ba ff 00 00 00 eb 0a f0 0f b1 13 ff c8 74 0b f3 90 <8b> 03 83 f8 01 75 f7 eb ed c6 43 04 00 5b 5d c3 0f 1f 44 00 00
    
    This happens because in the code path executed by the inode_paths ioctl we
    end up nesting two calls to read lock a leaf's rwlock when after the first
    call to read_lock() and before the second call to read_lock(), another
    task (running the delayed items as part of a transaction commit) has
    already called write_lock() against the leaf's rwlock. This situation is
    illustrated by the following diagram:
    
             Task A                       Task B
    
      btrfs_ref_to_path()               btrfs_commit_transaction()
        read_lock(&eb->lock);
    
                                          btrfs_run_delayed_items()
                                            __btrfs_commit_inode_delayed_items()
                                              __btrfs_update_delayed_inode()
                                                btrfs_lookup_inode()
    
                                                  write_lock(&eb->lock);
                                                    --> task waits for lock
    
        read_lock(&eb->lock);
        --> makes this task hang
            forever (and task B too
            of course)
    
    So fix this by avoiding doing the nested read lock, which is easily
    avoidable. This issue does not happen if task B calls write_lock() after
    task A does the second call to read_lock(), however there does not seem
    to exist anything in the documentation that mentions what is the expected
    behaviour for recursive locking of rwlocks (leaving the idea that doing
    so is not a good usage of rwlocks).
    
    Also, as a side effect necessary for this fix, make sure we do not
    needlessly read lock extent buffers when the input path has skip_locking
    set (used when called from send).
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5d5b6db240e5689761a92e294c0a18b1afff20d1
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Feb 3 19:17:27 2016 +0000

    Btrfs: fix hang on extent buffer lock caused by the inode_paths ioctl
    
    commit 0c0fe3b0fa45082cd752553fdb3a4b42503a118e upstream.
    
    While doing some tests I ran into an hang on an extent buffer's rwlock
    that produced the following trace:
    
    [39389.800012] NMI watchdog: BUG: soft lockup - CPU#15 stuck for 22s! [fdm-stress:32166]
    [39389.800016] NMI watchdog: BUG: soft lockup - CPU#14 stuck for 22s! [fdm-stress:32165]
    [39389.800016] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800016] irq event stamp: 0
    [39389.800016] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800016] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800016] CPU: 14 PID: 32165 Comm: fdm-stress Not tainted 4.4.0-rc6-btrfs-next-18+ #1
    [39389.800016] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800016] task: ffff880175b1ca40 ti: ffff8800a185c000 task.ti: ffff8800a185c000
    [39389.800016] RIP: 0010:[<ffffffff810902af>]  [<ffffffff810902af>] queued_spin_lock_slowpath+0x57/0x158
    [39389.800016] RSP: 0018:ffff8800a185fb80  EFLAGS: 00000202
    [39389.800016] RAX: 0000000000000101 RBX: ffff8801710c4e9c RCX: 0000000000000101
    [39389.800016] RDX: 0000000000000100 RSI: 0000000000000001 RDI: 0000000000000001
    [39389.800016] RBP: ffff8800a185fb98 R08: 0000000000000001 R09: 0000000000000000
    [39389.800016] R10: ffff8800a185fb68 R11: 6db6db6db6db6db7 R12: ffff8801710c4e98
    [39389.800016] R13: ffff880175b1ca40 R14: ffff8800a185fc10 R15: ffff880175b1ca40
    [39389.800016] FS:  00007f6d37fff700(0000) GS:ffff8802be9c0000(0000) knlGS:0000000000000000
    [39389.800016] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800016] CR2: 00007f6d300019b8 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800016] Stack:
    [39389.800016]  ffff8801710c4e98 ffff8801710c4e98 ffff880175b1ca40 ffff8800a185fbb0
    [39389.800016]  ffffffff81091e11 ffff8801710c4e98 ffff8800a185fbc8 ffffffff81091895
    [39389.800016]  ffff8801710c4e98 ffff8800a185fbe8 ffffffff81486c5c ffffffffa067288c
    [39389.800016] Call Trace:
    [39389.800016]  [<ffffffff81091e11>] queued_read_lock_slowpath+0x46/0x60
    [39389.800016]  [<ffffffff81091895>] do_raw_read_lock+0x3e/0x41
    [39389.800016]  [<ffffffff81486c5c>] _raw_read_lock+0x3d/0x44
    [39389.800016]  [<ffffffffa067288c>] ? btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa067288c>] btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa0622ced>] ? btrfs_find_item+0xa7/0xd2 [btrfs]
    [39389.800016]  [<ffffffffa069363f>] btrfs_ref_to_path+0xd6/0x174 [btrfs]
    [39389.800016]  [<ffffffffa0693730>] inode_to_path+0x53/0xa2 [btrfs]
    [39389.800016]  [<ffffffffa0693e2e>] paths_from_inode+0x117/0x2ec [btrfs]
    [39389.800016]  [<ffffffffa0670cff>] btrfs_ioctl+0xd5b/0x2793 [btrfs]
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff81276727>] ? __this_cpu_preempt_check+0x13/0x15
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800016]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800016]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800016]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800016]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800016] Code: b9 01 01 00 00 f7 c6 00 ff ff ff 75 32 83 fe 01 89 ca 89 f0 0f 45 d7 f0 0f b1 13 39 f0 74 04 89 c6 eb e2 ff ca 0f 84 fa 00 00 00 <8b> 03 84 c0 74 04 f3 90 eb f6 66 c7 03 01 00 e9 e6 00 00 00 e8
    [39389.800012] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800012] irq event stamp: 0
    [39389.800012] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800012] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800012] CPU: 15 PID: 32166 Comm: fdm-stress Tainted: G             L  4.4.0-rc6-btrfs-next-18+ #1
    [39389.800012] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800012] task: ffff880179294380 ti: ffff880034a60000 task.ti: ffff880034a60000
    [39389.800012] RIP: 0010:[<ffffffff81091e8d>]  [<ffffffff81091e8d>] queued_write_lock_slowpath+0x62/0x72
    [39389.800012] RSP: 0018:ffff880034a639f0  EFLAGS: 00000206
    [39389.800012] RAX: 0000000000000101 RBX: ffff8801710c4e98 RCX: 0000000000000000
    [39389.800012] RDX: 00000000000000ff RSI: 0000000000000000 RDI: ffff8801710c4e9c
    [39389.800012] RBP: ffff880034a639f8 R08: 0000000000000001 R09: 0000000000000000
    [39389.800012] R10: ffff880034a639b0 R11: 0000000000001000 R12: ffff8801710c4e98
    [39389.800012] R13: 0000000000000001 R14: ffff880172cbc000 R15: ffff8801710c4e00
    [39389.800012] FS:  00007f6d377fe700(0000) GS:ffff8802be9e0000(0000) knlGS:0000000000000000
    [39389.800012] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800012] CR2: 00007f6d3d3c1000 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800012] Stack:
    [39389.800012]  ffff8801710c4e98 ffff880034a63a10 ffffffff81091963 ffff8801710c4e98
    [39389.800012]  ffff880034a63a30 ffffffff81486f1b ffffffffa0672cb3 ffff8801710c4e00
    [39389.800012]  ffff880034a63a78 ffffffffa0672cb3 ffff8801710c4e00 ffff880034a63a58
    [39389.800012] Call Trace:
    [39389.800012]  [<ffffffff81091963>] do_raw_write_lock+0x72/0x8c
    [39389.800012]  [<ffffffff81486f1b>] _raw_write_lock+0x3a/0x41
    [39389.800012]  [<ffffffffa0672cb3>] ? btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa0672cb3>] btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa061aeba>] ? rcu_read_unlock+0x5b/0x5d [btrfs]
    [39389.800012]  [<ffffffffa061ce13>] ? btrfs_root_node+0xda/0xe6 [btrfs]
    [39389.800012]  [<ffffffffa061ce83>] btrfs_lock_root_node+0x22/0x42 [btrfs]
    [39389.800012]  [<ffffffffa062046b>] btrfs_search_slot+0x1b8/0x758 [btrfs]
    [39389.800012]  [<ffffffff810fc6b0>] ? time_hardirqs_on+0x15/0x28
    [39389.800012]  [<ffffffffa06365db>] btrfs_lookup_inode+0x31/0x95 [btrfs]
    [39389.800012]  [<ffffffff8108d62f>] ? trace_hardirqs_on+0xd/0xf
    [39389.800012]  [<ffffffff8148482b>] ? mutex_lock_nested+0x397/0x3bc
    [39389.800012]  [<ffffffffa068821b>] __btrfs_update_delayed_inode+0x59/0x1c0 [btrfs]
    [39389.800012]  [<ffffffffa068858e>] __btrfs_commit_inode_delayed_items+0x194/0x5aa [btrfs]
    [39389.800012]  [<ffffffff81486ab7>] ? _raw_spin_unlock+0x31/0x44
    [39389.800012]  [<ffffffffa0688a48>] __btrfs_run_delayed_items+0xa4/0x15c [btrfs]
    [39389.800012]  [<ffffffffa0688d62>] btrfs_run_delayed_items+0x11/0x13 [btrfs]
    [39389.800012]  [<ffffffffa064048e>] btrfs_commit_transaction+0x234/0x96e [btrfs]
    [39389.800012]  [<ffffffffa0618d10>] btrfs_sync_fs+0x145/0x1ad [btrfs]
    [39389.800012]  [<ffffffffa0671176>] btrfs_ioctl+0x11d2/0x2793 [btrfs]
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800012]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800012]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800012]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800012]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800012] Code: f0 0f b1 13 85 c0 75 ef eb 2a f3 90 8a 03 84 c0 75 f8 f0 0f b0 13 84 c0 75 f0 ba ff 00 00 00 eb 0a f0 0f b1 13 ff c8 74 0b f3 90 <8b> 03 83 f8 01 75 f7 eb ed c6 43 04 00 5b 5d c3 0f 1f 44 00 00
    
    This happens because in the code path executed by the inode_paths ioctl we
    end up nesting two calls to read lock a leaf's rwlock when after the first
    call to read_lock() and before the second call to read_lock(), another
    task (running the delayed items as part of a transaction commit) has
    already called write_lock() against the leaf's rwlock. This situation is
    illustrated by the following diagram:
    
             Task A                       Task B
    
      btrfs_ref_to_path()               btrfs_commit_transaction()
        read_lock(&eb->lock);
    
                                          btrfs_run_delayed_items()
                                            __btrfs_commit_inode_delayed_items()
                                              __btrfs_update_delayed_inode()
                                                btrfs_lookup_inode()
    
                                                  write_lock(&eb->lock);
                                                    --> task waits for lock
    
        read_lock(&eb->lock);
        --> makes this task hang
            forever (and task B too
            of course)
    
    So fix this by avoiding doing the nested read lock, which is easily
    avoidable. This issue does not happen if task B calls write_lock() after
    task A does the second call to read_lock(), however there does not seem
    to exist anything in the documentation that mentions what is the expected
    behaviour for recursive locking of rwlocks (leaving the idea that doing
    so is not a good usage of rwlocks).
    
    Also, as a side effect necessary for this fix, make sure we do not
    needlessly read lock extent buffers when the input path has skip_locking
    set (used when called from send).
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 590a2f0b8c5d10279bf8cb6d07ca426e6086b349
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Feb 3 19:17:27 2016 +0000

    Btrfs: fix hang on extent buffer lock caused by the inode_paths ioctl
    
    commit 0c0fe3b0fa45082cd752553fdb3a4b42503a118e upstream.
    
    While doing some tests I ran into an hang on an extent buffer's rwlock
    that produced the following trace:
    
    [39389.800012] NMI watchdog: BUG: soft lockup - CPU#15 stuck for 22s! [fdm-stress:32166]
    [39389.800016] NMI watchdog: BUG: soft lockup - CPU#14 stuck for 22s! [fdm-stress:32165]
    [39389.800016] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800016] irq event stamp: 0
    [39389.800016] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800016] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800016] CPU: 14 PID: 32165 Comm: fdm-stress Not tainted 4.4.0-rc6-btrfs-next-18+ #1
    [39389.800016] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800016] task: ffff880175b1ca40 ti: ffff8800a185c000 task.ti: ffff8800a185c000
    [39389.800016] RIP: 0010:[<ffffffff810902af>]  [<ffffffff810902af>] queued_spin_lock_slowpath+0x57/0x158
    [39389.800016] RSP: 0018:ffff8800a185fb80  EFLAGS: 00000202
    [39389.800016] RAX: 0000000000000101 RBX: ffff8801710c4e9c RCX: 0000000000000101
    [39389.800016] RDX: 0000000000000100 RSI: 0000000000000001 RDI: 0000000000000001
    [39389.800016] RBP: ffff8800a185fb98 R08: 0000000000000001 R09: 0000000000000000
    [39389.800016] R10: ffff8800a185fb68 R11: 6db6db6db6db6db7 R12: ffff8801710c4e98
    [39389.800016] R13: ffff880175b1ca40 R14: ffff8800a185fc10 R15: ffff880175b1ca40
    [39389.800016] FS:  00007f6d37fff700(0000) GS:ffff8802be9c0000(0000) knlGS:0000000000000000
    [39389.800016] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800016] CR2: 00007f6d300019b8 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800016] Stack:
    [39389.800016]  ffff8801710c4e98 ffff8801710c4e98 ffff880175b1ca40 ffff8800a185fbb0
    [39389.800016]  ffffffff81091e11 ffff8801710c4e98 ffff8800a185fbc8 ffffffff81091895
    [39389.800016]  ffff8801710c4e98 ffff8800a185fbe8 ffffffff81486c5c ffffffffa067288c
    [39389.800016] Call Trace:
    [39389.800016]  [<ffffffff81091e11>] queued_read_lock_slowpath+0x46/0x60
    [39389.800016]  [<ffffffff81091895>] do_raw_read_lock+0x3e/0x41
    [39389.800016]  [<ffffffff81486c5c>] _raw_read_lock+0x3d/0x44
    [39389.800016]  [<ffffffffa067288c>] ? btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa067288c>] btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa0622ced>] ? btrfs_find_item+0xa7/0xd2 [btrfs]
    [39389.800016]  [<ffffffffa069363f>] btrfs_ref_to_path+0xd6/0x174 [btrfs]
    [39389.800016]  [<ffffffffa0693730>] inode_to_path+0x53/0xa2 [btrfs]
    [39389.800016]  [<ffffffffa0693e2e>] paths_from_inode+0x117/0x2ec [btrfs]
    [39389.800016]  [<ffffffffa0670cff>] btrfs_ioctl+0xd5b/0x2793 [btrfs]
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff81276727>] ? __this_cpu_preempt_check+0x13/0x15
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800016]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800016]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800016]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800016]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800016] Code: b9 01 01 00 00 f7 c6 00 ff ff ff 75 32 83 fe 01 89 ca 89 f0 0f 45 d7 f0 0f b1 13 39 f0 74 04 89 c6 eb e2 ff ca 0f 84 fa 00 00 00 <8b> 03 84 c0 74 04 f3 90 eb f6 66 c7 03 01 00 e9 e6 00 00 00 e8
    [39389.800012] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800012] irq event stamp: 0
    [39389.800012] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800012] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800012] CPU: 15 PID: 32166 Comm: fdm-stress Tainted: G             L  4.4.0-rc6-btrfs-next-18+ #1
    [39389.800012] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800012] task: ffff880179294380 ti: ffff880034a60000 task.ti: ffff880034a60000
    [39389.800012] RIP: 0010:[<ffffffff81091e8d>]  [<ffffffff81091e8d>] queued_write_lock_slowpath+0x62/0x72
    [39389.800012] RSP: 0018:ffff880034a639f0  EFLAGS: 00000206
    [39389.800012] RAX: 0000000000000101 RBX: ffff8801710c4e98 RCX: 0000000000000000
    [39389.800012] RDX: 00000000000000ff RSI: 0000000000000000 RDI: ffff8801710c4e9c
    [39389.800012] RBP: ffff880034a639f8 R08: 0000000000000001 R09: 0000000000000000
    [39389.800012] R10: ffff880034a639b0 R11: 0000000000001000 R12: ffff8801710c4e98
    [39389.800012] R13: 0000000000000001 R14: ffff880172cbc000 R15: ffff8801710c4e00
    [39389.800012] FS:  00007f6d377fe700(0000) GS:ffff8802be9e0000(0000) knlGS:0000000000000000
    [39389.800012] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800012] CR2: 00007f6d3d3c1000 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800012] Stack:
    [39389.800012]  ffff8801710c4e98 ffff880034a63a10 ffffffff81091963 ffff8801710c4e98
    [39389.800012]  ffff880034a63a30 ffffffff81486f1b ffffffffa0672cb3 ffff8801710c4e00
    [39389.800012]  ffff880034a63a78 ffffffffa0672cb3 ffff8801710c4e00 ffff880034a63a58
    [39389.800012] Call Trace:
    [39389.800012]  [<ffffffff81091963>] do_raw_write_lock+0x72/0x8c
    [39389.800012]  [<ffffffff81486f1b>] _raw_write_lock+0x3a/0x41
    [39389.800012]  [<ffffffffa0672cb3>] ? btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa0672cb3>] btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa061aeba>] ? rcu_read_unlock+0x5b/0x5d [btrfs]
    [39389.800012]  [<ffffffffa061ce13>] ? btrfs_root_node+0xda/0xe6 [btrfs]
    [39389.800012]  [<ffffffffa061ce83>] btrfs_lock_root_node+0x22/0x42 [btrfs]
    [39389.800012]  [<ffffffffa062046b>] btrfs_search_slot+0x1b8/0x758 [btrfs]
    [39389.800012]  [<ffffffff810fc6b0>] ? time_hardirqs_on+0x15/0x28
    [39389.800012]  [<ffffffffa06365db>] btrfs_lookup_inode+0x31/0x95 [btrfs]
    [39389.800012]  [<ffffffff8108d62f>] ? trace_hardirqs_on+0xd/0xf
    [39389.800012]  [<ffffffff8148482b>] ? mutex_lock_nested+0x397/0x3bc
    [39389.800012]  [<ffffffffa068821b>] __btrfs_update_delayed_inode+0x59/0x1c0 [btrfs]
    [39389.800012]  [<ffffffffa068858e>] __btrfs_commit_inode_delayed_items+0x194/0x5aa [btrfs]
    [39389.800012]  [<ffffffff81486ab7>] ? _raw_spin_unlock+0x31/0x44
    [39389.800012]  [<ffffffffa0688a48>] __btrfs_run_delayed_items+0xa4/0x15c [btrfs]
    [39389.800012]  [<ffffffffa0688d62>] btrfs_run_delayed_items+0x11/0x13 [btrfs]
    [39389.800012]  [<ffffffffa064048e>] btrfs_commit_transaction+0x234/0x96e [btrfs]
    [39389.800012]  [<ffffffffa0618d10>] btrfs_sync_fs+0x145/0x1ad [btrfs]
    [39389.800012]  [<ffffffffa0671176>] btrfs_ioctl+0x11d2/0x2793 [btrfs]
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800012]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800012]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800012]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800012]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800012] Code: f0 0f b1 13 85 c0 75 ef eb 2a f3 90 8a 03 84 c0 75 f8 f0 0f b0 13 84 c0 75 f0 ba ff 00 00 00 eb 0a f0 0f b1 13 ff c8 74 0b f3 90 <8b> 03 83 f8 01 75 f7 eb ed c6 43 04 00 5b 5d c3 0f 1f 44 00 00
    
    This happens because in the code path executed by the inode_paths ioctl we
    end up nesting two calls to read lock a leaf's rwlock when after the first
    call to read_lock() and before the second call to read_lock(), another
    task (running the delayed items as part of a transaction commit) has
    already called write_lock() against the leaf's rwlock. This situation is
    illustrated by the following diagram:
    
             Task A                       Task B
    
      btrfs_ref_to_path()               btrfs_commit_transaction()
        read_lock(&eb->lock);
    
                                          btrfs_run_delayed_items()
                                            __btrfs_commit_inode_delayed_items()
                                              __btrfs_update_delayed_inode()
                                                btrfs_lookup_inode()
    
                                                  write_lock(&eb->lock);
                                                    --> task waits for lock
    
        read_lock(&eb->lock);
        --> makes this task hang
            forever (and task B too
            of course)
    
    So fix this by avoiding doing the nested read lock, which is easily
    avoidable. This issue does not happen if task B calls write_lock() after
    task A does the second call to read_lock(), however there does not seem
    to exist anything in the documentation that mentions what is the expected
    behaviour for recursive locking of rwlocks (leaving the idea that doing
    so is not a good usage of rwlocks).
    
    Also, as a side effect necessary for this fix, make sure we do not
    needlessly read lock extent buffers when the input path has skip_locking
    set (used when called from send).
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b10a62d6c00ee83c2614fda9e8d8e7178ef53020
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Feb 3 19:17:27 2016 +0000

    Btrfs: fix hang on extent buffer lock caused by the inode_paths ioctl
    
    commit 0c0fe3b0fa45082cd752553fdb3a4b42503a118e upstream.
    
    While doing some tests I ran into an hang on an extent buffer's rwlock
    that produced the following trace:
    
    [39389.800012] NMI watchdog: BUG: soft lockup - CPU#15 stuck for 22s! [fdm-stress:32166]
    [39389.800016] NMI watchdog: BUG: soft lockup - CPU#14 stuck for 22s! [fdm-stress:32165]
    [39389.800016] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800016] irq event stamp: 0
    [39389.800016] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800016] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800016] CPU: 14 PID: 32165 Comm: fdm-stress Not tainted 4.4.0-rc6-btrfs-next-18+ #1
    [39389.800016] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800016] task: ffff880175b1ca40 ti: ffff8800a185c000 task.ti: ffff8800a185c000
    [39389.800016] RIP: 0010:[<ffffffff810902af>]  [<ffffffff810902af>] queued_spin_lock_slowpath+0x57/0x158
    [39389.800016] RSP: 0018:ffff8800a185fb80  EFLAGS: 00000202
    [39389.800016] RAX: 0000000000000101 RBX: ffff8801710c4e9c RCX: 0000000000000101
    [39389.800016] RDX: 0000000000000100 RSI: 0000000000000001 RDI: 0000000000000001
    [39389.800016] RBP: ffff8800a185fb98 R08: 0000000000000001 R09: 0000000000000000
    [39389.800016] R10: ffff8800a185fb68 R11: 6db6db6db6db6db7 R12: ffff8801710c4e98
    [39389.800016] R13: ffff880175b1ca40 R14: ffff8800a185fc10 R15: ffff880175b1ca40
    [39389.800016] FS:  00007f6d37fff700(0000) GS:ffff8802be9c0000(0000) knlGS:0000000000000000
    [39389.800016] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800016] CR2: 00007f6d300019b8 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800016] Stack:
    [39389.800016]  ffff8801710c4e98 ffff8801710c4e98 ffff880175b1ca40 ffff8800a185fbb0
    [39389.800016]  ffffffff81091e11 ffff8801710c4e98 ffff8800a185fbc8 ffffffff81091895
    [39389.800016]  ffff8801710c4e98 ffff8800a185fbe8 ffffffff81486c5c ffffffffa067288c
    [39389.800016] Call Trace:
    [39389.800016]  [<ffffffff81091e11>] queued_read_lock_slowpath+0x46/0x60
    [39389.800016]  [<ffffffff81091895>] do_raw_read_lock+0x3e/0x41
    [39389.800016]  [<ffffffff81486c5c>] _raw_read_lock+0x3d/0x44
    [39389.800016]  [<ffffffffa067288c>] ? btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa067288c>] btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa0622ced>] ? btrfs_find_item+0xa7/0xd2 [btrfs]
    [39389.800016]  [<ffffffffa069363f>] btrfs_ref_to_path+0xd6/0x174 [btrfs]
    [39389.800016]  [<ffffffffa0693730>] inode_to_path+0x53/0xa2 [btrfs]
    [39389.800016]  [<ffffffffa0693e2e>] paths_from_inode+0x117/0x2ec [btrfs]
    [39389.800016]  [<ffffffffa0670cff>] btrfs_ioctl+0xd5b/0x2793 [btrfs]
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff81276727>] ? __this_cpu_preempt_check+0x13/0x15
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800016]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800016]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800016]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800016]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800016] Code: b9 01 01 00 00 f7 c6 00 ff ff ff 75 32 83 fe 01 89 ca 89 f0 0f 45 d7 f0 0f b1 13 39 f0 74 04 89 c6 eb e2 ff ca 0f 84 fa 00 00 00 <8b> 03 84 c0 74 04 f3 90 eb f6 66 c7 03 01 00 e9 e6 00 00 00 e8
    [39389.800012] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800012] irq event stamp: 0
    [39389.800012] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800012] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800012] CPU: 15 PID: 32166 Comm: fdm-stress Tainted: G             L  4.4.0-rc6-btrfs-next-18+ #1
    [39389.800012] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800012] task: ffff880179294380 ti: ffff880034a60000 task.ti: ffff880034a60000
    [39389.800012] RIP: 0010:[<ffffffff81091e8d>]  [<ffffffff81091e8d>] queued_write_lock_slowpath+0x62/0x72
    [39389.800012] RSP: 0018:ffff880034a639f0  EFLAGS: 00000206
    [39389.800012] RAX: 0000000000000101 RBX: ffff8801710c4e98 RCX: 0000000000000000
    [39389.800012] RDX: 00000000000000ff RSI: 0000000000000000 RDI: ffff8801710c4e9c
    [39389.800012] RBP: ffff880034a639f8 R08: 0000000000000001 R09: 0000000000000000
    [39389.800012] R10: ffff880034a639b0 R11: 0000000000001000 R12: ffff8801710c4e98
    [39389.800012] R13: 0000000000000001 R14: ffff880172cbc000 R15: ffff8801710c4e00
    [39389.800012] FS:  00007f6d377fe700(0000) GS:ffff8802be9e0000(0000) knlGS:0000000000000000
    [39389.800012] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800012] CR2: 00007f6d3d3c1000 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800012] Stack:
    [39389.800012]  ffff8801710c4e98 ffff880034a63a10 ffffffff81091963 ffff8801710c4e98
    [39389.800012]  ffff880034a63a30 ffffffff81486f1b ffffffffa0672cb3 ffff8801710c4e00
    [39389.800012]  ffff880034a63a78 ffffffffa0672cb3 ffff8801710c4e00 ffff880034a63a58
    [39389.800012] Call Trace:
    [39389.800012]  [<ffffffff81091963>] do_raw_write_lock+0x72/0x8c
    [39389.800012]  [<ffffffff81486f1b>] _raw_write_lock+0x3a/0x41
    [39389.800012]  [<ffffffffa0672cb3>] ? btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa0672cb3>] btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa061aeba>] ? rcu_read_unlock+0x5b/0x5d [btrfs]
    [39389.800012]  [<ffffffffa061ce13>] ? btrfs_root_node+0xda/0xe6 [btrfs]
    [39389.800012]  [<ffffffffa061ce83>] btrfs_lock_root_node+0x22/0x42 [btrfs]
    [39389.800012]  [<ffffffffa062046b>] btrfs_search_slot+0x1b8/0x758 [btrfs]
    [39389.800012]  [<ffffffff810fc6b0>] ? time_hardirqs_on+0x15/0x28
    [39389.800012]  [<ffffffffa06365db>] btrfs_lookup_inode+0x31/0x95 [btrfs]
    [39389.800012]  [<ffffffff8108d62f>] ? trace_hardirqs_on+0xd/0xf
    [39389.800012]  [<ffffffff8148482b>] ? mutex_lock_nested+0x397/0x3bc
    [39389.800012]  [<ffffffffa068821b>] __btrfs_update_delayed_inode+0x59/0x1c0 [btrfs]
    [39389.800012]  [<ffffffffa068858e>] __btrfs_commit_inode_delayed_items+0x194/0x5aa [btrfs]
    [39389.800012]  [<ffffffff81486ab7>] ? _raw_spin_unlock+0x31/0x44
    [39389.800012]  [<ffffffffa0688a48>] __btrfs_run_delayed_items+0xa4/0x15c [btrfs]
    [39389.800012]  [<ffffffffa0688d62>] btrfs_run_delayed_items+0x11/0x13 [btrfs]
    [39389.800012]  [<ffffffffa064048e>] btrfs_commit_transaction+0x234/0x96e [btrfs]
    [39389.800012]  [<ffffffffa0618d10>] btrfs_sync_fs+0x145/0x1ad [btrfs]
    [39389.800012]  [<ffffffffa0671176>] btrfs_ioctl+0x11d2/0x2793 [btrfs]
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800012]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800012]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800012]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800012]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800012] Code: f0 0f b1 13 85 c0 75 ef eb 2a f3 90 8a 03 84 c0 75 f8 f0 0f b0 13 84 c0 75 f0 ba ff 00 00 00 eb 0a f0 0f b1 13 ff c8 74 0b f3 90 <8b> 03 83 f8 01 75 f7 eb ed c6 43 04 00 5b 5d c3 0f 1f 44 00 00
    
    This happens because in the code path executed by the inode_paths ioctl we
    end up nesting two calls to read lock a leaf's rwlock when after the first
    call to read_lock() and before the second call to read_lock(), another
    task (running the delayed items as part of a transaction commit) has
    already called write_lock() against the leaf's rwlock. This situation is
    illustrated by the following diagram:
    
             Task A                       Task B
    
      btrfs_ref_to_path()               btrfs_commit_transaction()
        read_lock(&eb->lock);
    
                                          btrfs_run_delayed_items()
                                            __btrfs_commit_inode_delayed_items()
                                              __btrfs_update_delayed_inode()
                                                btrfs_lookup_inode()
    
                                                  write_lock(&eb->lock);
                                                    --> task waits for lock
    
        read_lock(&eb->lock);
        --> makes this task hang
            forever (and task B too
            of course)
    
    So fix this by avoiding doing the nested read lock, which is easily
    avoidable. This issue does not happen if task B calls write_lock() after
    task A does the second call to read_lock(), however there does not seem
    to exist anything in the documentation that mentions what is the expected
    behaviour for recursive locking of rwlocks (leaving the idea that doing
    so is not a good usage of rwlocks).
    
    Also, as a side effect necessary for this fix, make sure we do not
    needlessly read lock extent buffers when the input path has skip_locking
    set (used when called from send).
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit 2c0d636da649546fde114db32cd70b7f56e8d11b
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Feb 3 19:17:27 2016 +0000

    Btrfs: fix hang on extent buffer lock caused by the inode_paths ioctl
    
    commit 0c0fe3b0fa45082cd752553fdb3a4b42503a118e upstream.
    
    While doing some tests I ran into an hang on an extent buffer's rwlock
    that produced the following trace:
    
    [39389.800012] NMI watchdog: BUG: soft lockup - CPU#15 stuck for 22s! [fdm-stress:32166]
    [39389.800016] NMI watchdog: BUG: soft lockup - CPU#14 stuck for 22s! [fdm-stress:32165]
    [39389.800016] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800016] irq event stamp: 0
    [39389.800016] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800016] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800016] CPU: 14 PID: 32165 Comm: fdm-stress Not tainted 4.4.0-rc6-btrfs-next-18+ #1
    [39389.800016] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800016] task: ffff880175b1ca40 ti: ffff8800a185c000 task.ti: ffff8800a185c000
    [39389.800016] RIP: 0010:[<ffffffff810902af>]  [<ffffffff810902af>] queued_spin_lock_slowpath+0x57/0x158
    [39389.800016] RSP: 0018:ffff8800a185fb80  EFLAGS: 00000202
    [39389.800016] RAX: 0000000000000101 RBX: ffff8801710c4e9c RCX: 0000000000000101
    [39389.800016] RDX: 0000000000000100 RSI: 0000000000000001 RDI: 0000000000000001
    [39389.800016] RBP: ffff8800a185fb98 R08: 0000000000000001 R09: 0000000000000000
    [39389.800016] R10: ffff8800a185fb68 R11: 6db6db6db6db6db7 R12: ffff8801710c4e98
    [39389.800016] R13: ffff880175b1ca40 R14: ffff8800a185fc10 R15: ffff880175b1ca40
    [39389.800016] FS:  00007f6d37fff700(0000) GS:ffff8802be9c0000(0000) knlGS:0000000000000000
    [39389.800016] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800016] CR2: 00007f6d300019b8 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800016] Stack:
    [39389.800016]  ffff8801710c4e98 ffff8801710c4e98 ffff880175b1ca40 ffff8800a185fbb0
    [39389.800016]  ffffffff81091e11 ffff8801710c4e98 ffff8800a185fbc8 ffffffff81091895
    [39389.800016]  ffff8801710c4e98 ffff8800a185fbe8 ffffffff81486c5c ffffffffa067288c
    [39389.800016] Call Trace:
    [39389.800016]  [<ffffffff81091e11>] queued_read_lock_slowpath+0x46/0x60
    [39389.800016]  [<ffffffff81091895>] do_raw_read_lock+0x3e/0x41
    [39389.800016]  [<ffffffff81486c5c>] _raw_read_lock+0x3d/0x44
    [39389.800016]  [<ffffffffa067288c>] ? btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa067288c>] btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa0622ced>] ? btrfs_find_item+0xa7/0xd2 [btrfs]
    [39389.800016]  [<ffffffffa069363f>] btrfs_ref_to_path+0xd6/0x174 [btrfs]
    [39389.800016]  [<ffffffffa0693730>] inode_to_path+0x53/0xa2 [btrfs]
    [39389.800016]  [<ffffffffa0693e2e>] paths_from_inode+0x117/0x2ec [btrfs]
    [39389.800016]  [<ffffffffa0670cff>] btrfs_ioctl+0xd5b/0x2793 [btrfs]
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff81276727>] ? __this_cpu_preempt_check+0x13/0x15
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800016]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800016]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800016]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800016]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800016] Code: b9 01 01 00 00 f7 c6 00 ff ff ff 75 32 83 fe 01 89 ca 89 f0 0f 45 d7 f0 0f b1 13 39 f0 74 04 89 c6 eb e2 ff ca 0f 84 fa 00 00 00 <8b> 03 84 c0 74 04 f3 90 eb f6 66 c7 03 01 00 e9 e6 00 00 00 e8
    [39389.800012] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800012] irq event stamp: 0
    [39389.800012] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800012] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800012] CPU: 15 PID: 32166 Comm: fdm-stress Tainted: G             L  4.4.0-rc6-btrfs-next-18+ #1
    [39389.800012] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800012] task: ffff880179294380 ti: ffff880034a60000 task.ti: ffff880034a60000
    [39389.800012] RIP: 0010:[<ffffffff81091e8d>]  [<ffffffff81091e8d>] queued_write_lock_slowpath+0x62/0x72
    [39389.800012] RSP: 0018:ffff880034a639f0  EFLAGS: 00000206
    [39389.800012] RAX: 0000000000000101 RBX: ffff8801710c4e98 RCX: 0000000000000000
    [39389.800012] RDX: 00000000000000ff RSI: 0000000000000000 RDI: ffff8801710c4e9c
    [39389.800012] RBP: ffff880034a639f8 R08: 0000000000000001 R09: 0000000000000000
    [39389.800012] R10: ffff880034a639b0 R11: 0000000000001000 R12: ffff8801710c4e98
    [39389.800012] R13: 0000000000000001 R14: ffff880172cbc000 R15: ffff8801710c4e00
    [39389.800012] FS:  00007f6d377fe700(0000) GS:ffff8802be9e0000(0000) knlGS:0000000000000000
    [39389.800012] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800012] CR2: 00007f6d3d3c1000 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800012] Stack:
    [39389.800012]  ffff8801710c4e98 ffff880034a63a10 ffffffff81091963 ffff8801710c4e98
    [39389.800012]  ffff880034a63a30 ffffffff81486f1b ffffffffa0672cb3 ffff8801710c4e00
    [39389.800012]  ffff880034a63a78 ffffffffa0672cb3 ffff8801710c4e00 ffff880034a63a58
    [39389.800012] Call Trace:
    [39389.800012]  [<ffffffff81091963>] do_raw_write_lock+0x72/0x8c
    [39389.800012]  [<ffffffff81486f1b>] _raw_write_lock+0x3a/0x41
    [39389.800012]  [<ffffffffa0672cb3>] ? btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa0672cb3>] btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa061aeba>] ? rcu_read_unlock+0x5b/0x5d [btrfs]
    [39389.800012]  [<ffffffffa061ce13>] ? btrfs_root_node+0xda/0xe6 [btrfs]
    [39389.800012]  [<ffffffffa061ce83>] btrfs_lock_root_node+0x22/0x42 [btrfs]
    [39389.800012]  [<ffffffffa062046b>] btrfs_search_slot+0x1b8/0x758 [btrfs]
    [39389.800012]  [<ffffffff810fc6b0>] ? time_hardirqs_on+0x15/0x28
    [39389.800012]  [<ffffffffa06365db>] btrfs_lookup_inode+0x31/0x95 [btrfs]
    [39389.800012]  [<ffffffff8108d62f>] ? trace_hardirqs_on+0xd/0xf
    [39389.800012]  [<ffffffff8148482b>] ? mutex_lock_nested+0x397/0x3bc
    [39389.800012]  [<ffffffffa068821b>] __btrfs_update_delayed_inode+0x59/0x1c0 [btrfs]
    [39389.800012]  [<ffffffffa068858e>] __btrfs_commit_inode_delayed_items+0x194/0x5aa [btrfs]
    [39389.800012]  [<ffffffff81486ab7>] ? _raw_spin_unlock+0x31/0x44
    [39389.800012]  [<ffffffffa0688a48>] __btrfs_run_delayed_items+0xa4/0x15c [btrfs]
    [39389.800012]  [<ffffffffa0688d62>] btrfs_run_delayed_items+0x11/0x13 [btrfs]
    [39389.800012]  [<ffffffffa064048e>] btrfs_commit_transaction+0x234/0x96e [btrfs]
    [39389.800012]  [<ffffffffa0618d10>] btrfs_sync_fs+0x145/0x1ad [btrfs]
    [39389.800012]  [<ffffffffa0671176>] btrfs_ioctl+0x11d2/0x2793 [btrfs]
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800012]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800012]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800012]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800012]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800012] Code: f0 0f b1 13 85 c0 75 ef eb 2a f3 90 8a 03 84 c0 75 f8 f0 0f b0 13 84 c0 75 f0 ba ff 00 00 00 eb 0a f0 0f b1 13 ff c8 74 0b f3 90 <8b> 03 83 f8 01 75 f7 eb ed c6 43 04 00 5b 5d c3 0f 1f 44 00 00
    
    This happens because in the code path executed by the inode_paths ioctl we
    end up nesting two calls to read lock a leaf's rwlock when after the first
    call to read_lock() and before the second call to read_lock(), another
    task (running the delayed items as part of a transaction commit) has
    already called write_lock() against the leaf's rwlock. This situation is
    illustrated by the following diagram:
    
             Task A                       Task B
    
      btrfs_ref_to_path()               btrfs_commit_transaction()
        read_lock(&eb->lock);
    
                                          btrfs_run_delayed_items()
                                            __btrfs_commit_inode_delayed_items()
                                              __btrfs_update_delayed_inode()
                                                btrfs_lookup_inode()
    
                                                  write_lock(&eb->lock);
                                                    --> task waits for lock
    
        read_lock(&eb->lock);
        --> makes this task hang
            forever (and task B too
            of course)
    
    So fix this by avoiding doing the nested read lock, which is easily
    avoidable. This issue does not happen if task B calls write_lock() after
    task A does the second call to read_lock(), however there does not seem
    to exist anything in the documentation that mentions what is the expected
    behaviour for recursive locking of rwlocks (leaving the idea that doing
    so is not a good usage of rwlocks).
    
    Also, as a side effect necessary for this fix, make sure we do not
    needlessly read lock extent buffers when the input path has skip_locking
    set (used when called from send).
    
    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit 16186a82de1fdd868255448274e64ae2616e2640
Author: Subash Abhinov Kasiviswanathan <subashab@codeaurora.org>
Date:   Tue Feb 2 02:11:10 2016 +0000

    ipv6: addrconf: Fix recursive spin lock call
    
    A rcu stall with the following backtrace was seen on a system with
    forwarding, optimistic_dad and use_optimistic set. To reproduce,
    set these flags and allow ipv6 autoconf.
    
    This occurs because the device write_lock is acquired while already
    holding the read_lock. Back trace below -
    
    INFO: rcu_preempt self-detected stall on CPU { 1}  (t=2100 jiffies
     g=3992 c=3991 q=4471)
    <6> Task dump for CPU 1:
    <2> kworker/1:0     R  running task    12168    15   2 0x00000002
    <2> Workqueue: ipv6_addrconf addrconf_dad_work
    <6> Call trace:
    <2> [<ffffffc000084da8>] el1_irq+0x68/0xdc
    <2> [<ffffffc000cc4e0c>] _raw_write_lock_bh+0x20/0x30
    <2> [<ffffffc000bc5dd8>] __ipv6_dev_ac_inc+0x64/0x1b4
    <2> [<ffffffc000bcbd2c>] addrconf_join_anycast+0x9c/0xc4
    <2> [<ffffffc000bcf9f0>] __ipv6_ifa_notify+0x160/0x29c
    <2> [<ffffffc000bcfb7c>] ipv6_ifa_notify+0x50/0x70
    <2> [<ffffffc000bd035c>] addrconf_dad_work+0x314/0x334
    <2> [<ffffffc0000b64c8>] process_one_work+0x244/0x3fc
    <2> [<ffffffc0000b7324>] worker_thread+0x2f8/0x418
    <2> [<ffffffc0000bb40c>] kthread+0xe0/0xec
    
    v2: do addrconf_dad_kick inside read lock and then acquire write
    lock for ipv6_ifa_notify as suggested by Eric
    
    Fixes: 7fd2561e4ebdd ("net: ipv6: Add a sysctl to make optimistic
    addresses useful candidates")
    
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Erik Kline <ek@google.com>
    Cc: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: Subash Abhinov Kasiviswanathan <subashab@codeaurora.org>
    Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Acked-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 0c0fe3b0fa45082cd752553fdb3a4b42503a118e
Author: Filipe Manana <fdmanana@suse.com>
Date:   Wed Feb 3 19:17:27 2016 +0000

    Btrfs: fix hang on extent buffer lock caused by the inode_paths ioctl
    
    While doing some tests I ran into an hang on an extent buffer's rwlock
    that produced the following trace:
    
    [39389.800012] NMI watchdog: BUG: soft lockup - CPU#15 stuck for 22s! [fdm-stress:32166]
    [39389.800016] NMI watchdog: BUG: soft lockup - CPU#14 stuck for 22s! [fdm-stress:32165]
    [39389.800016] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800016] irq event stamp: 0
    [39389.800016] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800016] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800016] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800016] CPU: 14 PID: 32165 Comm: fdm-stress Not tainted 4.4.0-rc6-btrfs-next-18+ #1
    [39389.800016] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800016] task: ffff880175b1ca40 ti: ffff8800a185c000 task.ti: ffff8800a185c000
    [39389.800016] RIP: 0010:[<ffffffff810902af>]  [<ffffffff810902af>] queued_spin_lock_slowpath+0x57/0x158
    [39389.800016] RSP: 0018:ffff8800a185fb80  EFLAGS: 00000202
    [39389.800016] RAX: 0000000000000101 RBX: ffff8801710c4e9c RCX: 0000000000000101
    [39389.800016] RDX: 0000000000000100 RSI: 0000000000000001 RDI: 0000000000000001
    [39389.800016] RBP: ffff8800a185fb98 R08: 0000000000000001 R09: 0000000000000000
    [39389.800016] R10: ffff8800a185fb68 R11: 6db6db6db6db6db7 R12: ffff8801710c4e98
    [39389.800016] R13: ffff880175b1ca40 R14: ffff8800a185fc10 R15: ffff880175b1ca40
    [39389.800016] FS:  00007f6d37fff700(0000) GS:ffff8802be9c0000(0000) knlGS:0000000000000000
    [39389.800016] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800016] CR2: 00007f6d300019b8 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800016] Stack:
    [39389.800016]  ffff8801710c4e98 ffff8801710c4e98 ffff880175b1ca40 ffff8800a185fbb0
    [39389.800016]  ffffffff81091e11 ffff8801710c4e98 ffff8800a185fbc8 ffffffff81091895
    [39389.800016]  ffff8801710c4e98 ffff8800a185fbe8 ffffffff81486c5c ffffffffa067288c
    [39389.800016] Call Trace:
    [39389.800016]  [<ffffffff81091e11>] queued_read_lock_slowpath+0x46/0x60
    [39389.800016]  [<ffffffff81091895>] do_raw_read_lock+0x3e/0x41
    [39389.800016]  [<ffffffff81486c5c>] _raw_read_lock+0x3d/0x44
    [39389.800016]  [<ffffffffa067288c>] ? btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa067288c>] btrfs_tree_read_lock+0x54/0x125 [btrfs]
    [39389.800016]  [<ffffffffa0622ced>] ? btrfs_find_item+0xa7/0xd2 [btrfs]
    [39389.800016]  [<ffffffffa069363f>] btrfs_ref_to_path+0xd6/0x174 [btrfs]
    [39389.800016]  [<ffffffffa0693730>] inode_to_path+0x53/0xa2 [btrfs]
    [39389.800016]  [<ffffffffa0693e2e>] paths_from_inode+0x117/0x2ec [btrfs]
    [39389.800016]  [<ffffffffa0670cff>] btrfs_ioctl+0xd5b/0x2793 [btrfs]
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff81276727>] ? __this_cpu_preempt_check+0x13/0x15
    [39389.800016]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800016]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800016]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800016]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800016]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800016]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800016] Code: b9 01 01 00 00 f7 c6 00 ff ff ff 75 32 83 fe 01 89 ca 89 f0 0f 45 d7 f0 0f b1 13 39 f0 74 04 89 c6 eb e2 ff ca 0f 84 fa 00 00 00 <8b> 03 84 c0 74 04 f3 90 eb f6 66 c7 03 01 00 e9 e6 00 00 00 e8
    [39389.800012] Modules linked in: btrfs dm_mod ppdev xor sha256_generic hmac raid6_pq drbg ansi_cprng aesni_intel i2c_piix4 acpi_cpufreq aes_x86_64 ablk_helper tpm_tis parport_pc i2c_core sg cryptd evdev psmouse lrw tpm parport gf128mul serio_raw pcspkr glue_helper processor button loop autofs4 ext4 crc16 mbcache jbd2 sd_mod sr_mod cdrom ata_generic virtio_scsi ata_piix libata virtio_pci virtio_ring crc32c_intel scsi_mod e1000 virtio floppy [last unloaded: btrfs]
    [39389.800012] irq event stamp: 0
    [39389.800012] hardirqs last  enabled at (0): [<          (null)>]           (null)
    [39389.800012] hardirqs last disabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last  enabled at (0): [<ffffffff8104e58d>] copy_process+0x638/0x1a35
    [39389.800012] softirqs last disabled at (0): [<          (null)>]           (null)
    [39389.800012] CPU: 15 PID: 32166 Comm: fdm-stress Tainted: G             L  4.4.0-rc6-btrfs-next-18+ #1
    [39389.800012] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS by qemu-project.org 04/01/2014
    [39389.800012] task: ffff880179294380 ti: ffff880034a60000 task.ti: ffff880034a60000
    [39389.800012] RIP: 0010:[<ffffffff81091e8d>]  [<ffffffff81091e8d>] queued_write_lock_slowpath+0x62/0x72
    [39389.800012] RSP: 0018:ffff880034a639f0  EFLAGS: 00000206
    [39389.800012] RAX: 0000000000000101 RBX: ffff8801710c4e98 RCX: 0000000000000000
    [39389.800012] RDX: 00000000000000ff RSI: 0000000000000000 RDI: ffff8801710c4e9c
    [39389.800012] RBP: ffff880034a639f8 R08: 0000000000000001 R09: 0000000000000000
    [39389.800012] R10: ffff880034a639b0 R11: 0000000000001000 R12: ffff8801710c4e98
    [39389.800012] R13: 0000000000000001 R14: ffff880172cbc000 R15: ffff8801710c4e00
    [39389.800012] FS:  00007f6d377fe700(0000) GS:ffff8802be9e0000(0000) knlGS:0000000000000000
    [39389.800012] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [39389.800012] CR2: 00007f6d3d3c1000 CR3: 0000000037c93000 CR4: 00000000001406e0
    [39389.800012] Stack:
    [39389.800012]  ffff8801710c4e98 ffff880034a63a10 ffffffff81091963 ffff8801710c4e98
    [39389.800012]  ffff880034a63a30 ffffffff81486f1b ffffffffa0672cb3 ffff8801710c4e00
    [39389.800012]  ffff880034a63a78 ffffffffa0672cb3 ffff8801710c4e00 ffff880034a63a58
    [39389.800012] Call Trace:
    [39389.800012]  [<ffffffff81091963>] do_raw_write_lock+0x72/0x8c
    [39389.800012]  [<ffffffff81486f1b>] _raw_write_lock+0x3a/0x41
    [39389.800012]  [<ffffffffa0672cb3>] ? btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa0672cb3>] btrfs_tree_lock+0x119/0x251 [btrfs]
    [39389.800012]  [<ffffffffa061aeba>] ? rcu_read_unlock+0x5b/0x5d [btrfs]
    [39389.800012]  [<ffffffffa061ce13>] ? btrfs_root_node+0xda/0xe6 [btrfs]
    [39389.800012]  [<ffffffffa061ce83>] btrfs_lock_root_node+0x22/0x42 [btrfs]
    [39389.800012]  [<ffffffffa062046b>] btrfs_search_slot+0x1b8/0x758 [btrfs]
    [39389.800012]  [<ffffffff810fc6b0>] ? time_hardirqs_on+0x15/0x28
    [39389.800012]  [<ffffffffa06365db>] btrfs_lookup_inode+0x31/0x95 [btrfs]
    [39389.800012]  [<ffffffff8108d62f>] ? trace_hardirqs_on+0xd/0xf
    [39389.800012]  [<ffffffff8148482b>] ? mutex_lock_nested+0x397/0x3bc
    [39389.800012]  [<ffffffffa068821b>] __btrfs_update_delayed_inode+0x59/0x1c0 [btrfs]
    [39389.800012]  [<ffffffffa068858e>] __btrfs_commit_inode_delayed_items+0x194/0x5aa [btrfs]
    [39389.800012]  [<ffffffff81486ab7>] ? _raw_spin_unlock+0x31/0x44
    [39389.800012]  [<ffffffffa0688a48>] __btrfs_run_delayed_items+0xa4/0x15c [btrfs]
    [39389.800012]  [<ffffffffa0688d62>] btrfs_run_delayed_items+0x11/0x13 [btrfs]
    [39389.800012]  [<ffffffffa064048e>] btrfs_commit_transaction+0x234/0x96e [btrfs]
    [39389.800012]  [<ffffffffa0618d10>] btrfs_sync_fs+0x145/0x1ad [btrfs]
    [39389.800012]  [<ffffffffa0671176>] btrfs_ioctl+0x11d2/0x2793 [btrfs]
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff81140261>] ? __might_fault+0x4c/0xa7
    [39389.800012]  [<ffffffff8108a8b0>] ? arch_local_irq_save+0x9/0xc
    [39389.800012]  [<ffffffff8118b3d4>] ? rcu_read_unlock+0x3e/0x5d
    [39389.800012]  [<ffffffff811822f8>] do_vfs_ioctl+0x42b/0x4ea
    [39389.800012]  [<ffffffff8118b4f3>] ? __fget_light+0x62/0x71
    [39389.800012]  [<ffffffff8118240e>] SyS_ioctl+0x57/0x79
    [39389.800012]  [<ffffffff814872d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    [39389.800012] Code: f0 0f b1 13 85 c0 75 ef eb 2a f3 90 8a 03 84 c0 75 f8 f0 0f b0 13 84 c0 75 f0 ba ff 00 00 00 eb 0a f0 0f b1 13 ff c8 74 0b f3 90 <8b> 03 83 f8 01 75 f7 eb ed c6 43 04 00 5b 5d c3 0f 1f 44 00 00
    
    This happens because in the code path executed by the inode_paths ioctl we
    end up nesting two calls to read lock a leaf's rwlock when after the first
    call to read_lock() and before the second call to read_lock(), another
    task (running the delayed items as part of a transaction commit) has
    already called write_lock() against the leaf's rwlock. This situation is
    illustrated by the following diagram:
    
             Task A                       Task B
    
      btrfs_ref_to_path()               btrfs_commit_transaction()
        read_lock(&eb->lock);
    
                                          btrfs_run_delayed_items()
                                            __btrfs_commit_inode_delayed_items()
                                              __btrfs_update_delayed_inode()
                                                btrfs_lookup_inode()
    
                                                  write_lock(&eb->lock);
                                                    --> task waits for lock
    
        read_lock(&eb->lock);
        --> makes this task hang
            forever (and task B too
            of course)
    
    So fix this by avoiding doing the nested read lock, which is easily
    avoidable. This issue does not happen if task B calls write_lock() after
    task A does the second call to read_lock(), however there does not seem
    to exist anything in the documentation that mentions what is the expected
    behaviour for recursive locking of rwlocks (leaving the idea that doing
    so is not a good usage of rwlocks).
    
    Also, as a side effect necessary for this fix, make sure we do not
    needlessly read lock extent buffers when the input path has skip_locking
    set (used when called from send).
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Filipe Manana <fdmanana@suse.com>

commit 23fe87e5a17ab204a85988d2f6d49e0a71e15a62
Author: Thomas Hellstrom <thellstrom@vmware.com>
Date:   Fri Nov 20 11:43:50 2015 -0800

    drm/ttm: Fixed a read/write lock imbalance
    
    commit 025af189fb44250206dd8a32fa4a682392af3301 upstream.
    
    In ttm_write_lock(), the uninterruptible path should call
    __ttm_write_lock() not __ttm_read_lock().  This fixes a vmwgfx hang
    on F23 start up.
    
    syeh: Extracted this from one of Thomas' internal patches.
    
    Signed-off-by: Thomas Hellstrom <thellstrom@vmware.com>
    Reviewed-by: Sinclair Yeh <syeh@vmware.com>
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit 1633bf118bfbc641a7d3a4bbb0b2b20c9c60f8d7
Merge: 33c152972e62 3d8c38af1493
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jan 5 14:11:51 2016 -0500

    Merge branch 'mlx5e-tstamp'
    
    Saeed Mahameed says:
    
    ====================
    Introduce mlx5 ethernet timestamping
    
    This patch series introduces the support for ConnectX-4 timestamping
    and the PTP kernel interface.
    
    Changes from V2:
    net/mlx5_core: Introduce access function to read internal_timer
            - Remove one line function
            - Change function name
    
    net/mlx5e: Add HW timestamping (TS) support:
            - Data path performance optimization (caching tstamp struct in rq,sq)
            - Change read/write_lock_irqsave to read/write_lock
            - Move ioctl functions to en_clock file
            - Changed overflow start algorithm according to comments from Richard
            - Move timestamp init/cleanup to open/close ndos.
    
    In details:
    
    1st patch prevents the driver from modifying skb->data and SKB CB in
    device xmit function.
    
    2nd patch adds the needed low level helpers for:
            - Fetching the hardware clock (hardware internal timer)
            - Parsing CQEs timestamps
            - Device frequency capability
    
    3rd patch adds new en_clock.c file that handles all needed timestamping
    operations:
            - Internal clock structure initialization and other helper functions
            - Added the needed ioctl for setting/getting the current timestamping
              configuration.
            - used this configuration in RX/TX data path to fill the SKB with
              the timestamp.
    
    4th patch Introduces PTP (PHC) support.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit ad404016553666c23829234b69db4253de46983b
Author: Thomas Hellstrom <thellstrom@vmware.com>
Date:   Fri Nov 20 11:43:50 2015 -0800

    drm/ttm: Fixed a read/write lock imbalance
    
    commit 025af189fb44250206dd8a32fa4a682392af3301 upstream.
    
    In ttm_write_lock(), the uninterruptible path should call
    __ttm_write_lock() not __ttm_read_lock().  This fixes a vmwgfx hang
    on F23 start up.
    
    syeh: Extracted this from one of Thomas' internal patches.
    
    Signed-off-by: Thomas Hellstrom <thellstrom@vmware.com>
    Reviewed-by: Sinclair Yeh <syeh@vmware.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit fdaac6a92778b95ad41e65b6f9a88b454899d8d4
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Nov 11 11:51:06 2015 -0800

    ipv6: Avoid creating RTF_CACHE from a rt that is not managed by fib6 tree
    
    [ Upstream commit 0d3f6d297bfb7af24d0508460fdb3d1ec4903fa3 ]
    
    The original bug report:
    https://bugzilla.redhat.com/show_bug.cgi?id=1272571
    
    The setup has a IPv4 GRE tunnel running in a IPSec.  The bug
    happens when ndisc starts sending router solicitation at the gre
    interface.  The simplified oops stack is like:
    
    __lock_acquire+0x1b2/0x1c30
    lock_acquire+0xb9/0x140
    _raw_write_lock_bh+0x3f/0x50
    __ip6_ins_rt+0x2e/0x60
    ip6_ins_rt+0x49/0x50
    ~~~~~~~~
    __ip6_rt_update_pmtu.part.54+0x145/0x250
    ip6_rt_update_pmtu+0x2e/0x40
    ~~~~~~~~
    ip_tunnel_xmit+0x1f1/0xf40
    __gre_xmit+0x7a/0x90
    ipgre_xmit+0x15a/0x220
    dev_hard_start_xmit+0x2bd/0x480
    __dev_queue_xmit+0x696/0x730
    dev_queue_xmit+0x10/0x20
    neigh_direct_output+0x11/0x20
    ip6_finish_output2+0x21f/0x770
    ip6_finish_output+0xa7/0x1d0
    ip6_output+0x56/0x190
    ~~~~~~~~
    ndisc_send_skb+0x1d9/0x400
    ndisc_send_rs+0x88/0xc0
    ~~~~~~~~
    
    The rt passed to ip6_rt_update_pmtu() is created by
    icmp6_dst_alloc() and it is not managed by the fib6 tree,
    so its rt6i_table == NULL.  When __ip6_rt_update_pmtu() creates
    a RTF_CACHE clone, the newly created clone also has rt6i_table == NULL
    and it causes the ip6_ins_rt() oops.
    
    During pmtu update, we only want to create a RTF_CACHE clone
    from a rt which is currently managed (or owned) by the
    fib6 tree.  It means either rt->rt6i_node != NULL or
    rt is a RTF_PCPU clone.
    
    It is worth to note that rt6i_table may not be NULL even it is
    not (yet) managed by the fib6 tree (e.g. addrconf_dst_alloc()).
    Hence, rt6i_node is a better check instead of rt6i_table.
    
    Fixes: 45e4fd26683c ("ipv6: Only create RTF_CACHE routes after encountering pmtu")
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Reported-by: Chris Siebenmann <cks-rhbugzilla@cs.toronto.edu>
    Cc: Chris Siebenmann <cks-rhbugzilla@cs.toronto.edu>
    Cc: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 726dd3c9c8b89e804d2a0211f4fbcf48cecd1847
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Nov 11 11:51:06 2015 -0800

    ipv6: Avoid creating RTF_CACHE from a rt that is not managed by fib6 tree
    
    [ Upstream commit 0d3f6d297bfb7af24d0508460fdb3d1ec4903fa3 ]
    
    The original bug report:
    https://bugzilla.redhat.com/show_bug.cgi?id=1272571
    
    The setup has a IPv4 GRE tunnel running in a IPSec.  The bug
    happens when ndisc starts sending router solicitation at the gre
    interface.  The simplified oops stack is like:
    
    __lock_acquire+0x1b2/0x1c30
    lock_acquire+0xb9/0x140
    _raw_write_lock_bh+0x3f/0x50
    __ip6_ins_rt+0x2e/0x60
    ip6_ins_rt+0x49/0x50
    ~~~~~~~~
    __ip6_rt_update_pmtu.part.54+0x145/0x250
    ip6_rt_update_pmtu+0x2e/0x40
    ~~~~~~~~
    ip_tunnel_xmit+0x1f1/0xf40
    __gre_xmit+0x7a/0x90
    ipgre_xmit+0x15a/0x220
    dev_hard_start_xmit+0x2bd/0x480
    __dev_queue_xmit+0x696/0x730
    dev_queue_xmit+0x10/0x20
    neigh_direct_output+0x11/0x20
    ip6_finish_output2+0x21f/0x770
    ip6_finish_output+0xa7/0x1d0
    ip6_output+0x56/0x190
    ~~~~~~~~
    ndisc_send_skb+0x1d9/0x400
    ndisc_send_rs+0x88/0xc0
    ~~~~~~~~
    
    The rt passed to ip6_rt_update_pmtu() is created by
    icmp6_dst_alloc() and it is not managed by the fib6 tree,
    so its rt6i_table == NULL.  When __ip6_rt_update_pmtu() creates
    a RTF_CACHE clone, the newly created clone also has rt6i_table == NULL
    and it causes the ip6_ins_rt() oops.
    
    During pmtu update, we only want to create a RTF_CACHE clone
    from a rt which is currently managed (or owned) by the
    fib6 tree.  It means either rt->rt6i_node != NULL or
    rt is a RTF_PCPU clone.
    
    It is worth to note that rt6i_table may not be NULL even it is
    not (yet) managed by the fib6 tree (e.g. addrconf_dst_alloc()).
    Hence, rt6i_node is a better check instead of rt6i_table.
    
    Fixes: 45e4fd26683c ("ipv6: Only create RTF_CACHE routes after encountering pmtu")
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Reported-by: Chris Siebenmann <cks-rhbugzilla@cs.toronto.edu>
    Cc: Chris Siebenmann <cks-rhbugzilla@cs.toronto.edu>
    Cc: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2be56f43e40a65249132d153b14b8c33cb635d4d
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Wed Nov 11 08:03:54 2015 -0500

    tty: Fix tty_send_xchar() lock order inversion
    
    commit ee0c1a65cf95230d5eb3d9de94fd2ead9a428c67 upstream.
    
    The correct lock order is atomic_write_lock => termios_rwsem, as
    established by tty_write() => n_tty_write().
    
    Fixes: c274f6ef1c666 ("tty: Hold termios_rwsem for tcflow(TCIxxx)")
    Reported-and-Tested-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0163804569a78f87cf47cf5a0891c7e829bd101c
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Wed Nov 11 08:03:54 2015 -0500

    tty: Fix tty_send_xchar() lock order inversion
    
    commit ee0c1a65cf95230d5eb3d9de94fd2ead9a428c67 upstream.
    
    The correct lock order is atomic_write_lock => termios_rwsem, as
    established by tty_write() => n_tty_write().
    
    Fixes: c274f6ef1c666 ("tty: Hold termios_rwsem for tcflow(TCIxxx)")
    Reported-and-Tested-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit aa23ddc33c318841c3e856665e4c7fd0b432a1f5
Author: David Herrmann <dh.herrmann@gmail.com>
Date:   Wed Oct 21 11:47:43 2015 +0200

    netlink: fix locking around NETLINK_LIST_MEMBERSHIPS
    
    [ Upstream commit 47191d65b647af5eb5c82ede70ed4c24b1e93ef4 ]
    
    Currently, NETLINK_LIST_MEMBERSHIPS grabs the netlink table while copying
    the membership state to user-space. However, grabing the netlink table is
    effectively a write_lock_irq(), and as such we should not be triggering
    page-faults in the critical section.
    
    This can be easily reproduced by the following snippet:
        int s = socket(AF_NETLINK, SOCK_RAW, NETLINK_ROUTE);
        void *p = mmap(0, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANON, -1, 0);
        int r = getsockopt(s, 0x10e, 9, p, (void*)((char*)p + 4092));
    
    This should work just fine, but currently triggers EFAULT and a possible
    WARN_ON below handle_mm_fault().
    
    Fix this by reducing locking of NETLINK_LIST_MEMBERSHIPS to a read-side
    lock. The write-lock was overkill in the first place, and the read-lock
    allows page-faults just fine.
    
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: David Herrmann <dh.herrmann@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ff4fcbdf43bc4ae760ef1e7782e233156ed90a75
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Wed Nov 11 08:03:54 2015 -0500

    tty: Fix tty_send_xchar() lock order inversion
    
    commit ee0c1a65cf95230d5eb3d9de94fd2ead9a428c67 upstream.
    
    The correct lock order is atomic_write_lock => termios_rwsem, as
    established by tty_write() => n_tty_write().
    
    Fixes: c274f6ef1c666 ("tty: Hold termios_rwsem for tcflow(TCIxxx)")
    Reported-and-Tested-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 760de7914e27781abb44564449c761ea4440f982
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Nov 30 16:26:44 2015 -0800

    f2fs: avoid deadlock in f2fs_shrink_extent_tree
    
    While handling extent trees, we can enter into a reclaiming path anytime.
    If it tries to release some extent nodes in the same extent tree,
    write_lock(&et->lock) would be hanged.
    In order to avoid the deadlock, we can just skip it.
    
    Note that, if it is an unreferenced tree, we should get write_lock(&et->lock)
    successfully and release all of therein nodes.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

commit 025af189fb44250206dd8a32fa4a682392af3301
Author: Thomas Hellstrom <thellstrom@vmware.com>
Date:   Fri Nov 20 11:43:50 2015 -0800

    drm/ttm: Fixed a read/write lock imbalance
    
    In ttm_write_lock(), the uninterruptible path should call
    __ttm_write_lock() not __ttm_read_lock().  This fixes a vmwgfx hang
    on F23 start up.
    
    syeh: Extracted this from one of Thomas' internal patches.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Thomas Hellstrom <thellstrom@vmware.com>
    Reviewed-by: Sinclair Yeh <syeh@vmware.com>

commit ee0c1a65cf95230d5eb3d9de94fd2ead9a428c67
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Wed Nov 11 08:03:54 2015 -0500

    tty: Fix tty_send_xchar() lock order inversion
    
    The correct lock order is atomic_write_lock => termios_rwsem, as
    established by tty_write() => n_tty_write().
    
    Fixes: c274f6ef1c666 ("tty: Hold termios_rwsem for tcflow(TCIxxx)")
    Reported-and-Tested-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: <stable@vger.kernel.org> # v3.18+
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0874f8ec9b0a462cc8787b7bb4b7824a1690da10
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Mon Nov 16 15:26:51 2015 -0800

    qla2xxx: Fix rwlock recursion
    
    This patch fixes the following kernel bug:
    
    kernel:BUG: rwlock recursion on CPU#2, insmod/39333, ffff8803e998cb28
    kernel: Call Trace:
    kernel: [<ffffffff812bce44>] dump_stack+0x48/0x64
    kernel: [<ffffffff810a8047>] rwlock_bug+0x67/0x70
    kernel: [<ffffffff810a833a>] do_raw_write_lock+0x8a/0xa0
    kernel: [<ffffffff815f3033>] _raw_write_lock_irqsave+0x63/0x80
    kernel: [<ffffffffa08087c8>] qla82xx_rd_32+0xe8/0x140 [qla2xxx]
    kernel: [<ffffffffa0808845>] qla82xx_crb_win_lock+0x25/0x60 [qla2xxx]
    kernel: [<ffffffffa0808976>] qla82xx_wr_32+0xf6/0x150 [qla2xxx]
    kernel: [<ffffffffa0808ac0>] qla82xx_disable_intrs+0x50/0x80 [qla2xxx]
    kernel: [<ffffffffa080630a>] qla82xx_reset_chip+0x1a/0x20 [qla2xxx]
    kernel: [<ffffffffa07d6ef2>] qla2x00_initialize_adapter+0x132/0x420 [qla2xxx]
    kernel: [<ffffffffa08087c8>] qla82xx_rd_32+0xe8/0x140 [qla2xxx]
    kernel: [<ffffffffa0808845>] qla82xx_crb_win_lock+0x25/0x60 [qla2xxx]
    kernel: [<ffffffffa0808976>] qla82xx_wr_32+0xf6/0x150 [qla2xxx]
    kernel: [<ffffffffa0808ac0>] qla82xx_disable_intrs+0x50/0x80 [qla2xxx]
    kernel: [<ffffffffa080630a>] qla82xx_reset_chip+0x1a/0x20 [qla2xxx]
    kernel: [<ffffffffa07d6ef2>] qla2x00_initialize_adapter+0x132/0x420 [qla2xxx]
    kernel: [<ffffffffa07c964e>] qla2x00_probe_one+0xefe/0x2130 [qla2xxx]
    kernel: [<ffffffff8130052c>] local_pci_probe+0x4c/0xa0
    kernel: [<ffffffff81300603>] pci_call_probe+0x83/0xa0
    kernel: [<ffffffff813008cf>] pci_device_probe+0x7f/0xb0
    kernel: [<ffffffff813e2e83>] really_probe+0x133/0x390
    kernel: [<ffffffff813e3139>] driver_probe_device+0x59/0xd0
    kernel: [<ffffffff813e3251>] __driver_attach+0xa1/0xb0
    kernel: [<ffffffff813e0cdd>] bus_for_each_dev+0x8d/0xb0
    kernel: [<ffffffff813e28ee>] driver_attach+0x1e/0x20
    kernel: [<ffffffff813e2252>] bus_add_driver+0x1d2/0x290
    kernel: [<ffffffff813e3970>] driver_register+0x60/0xe0
    kernel: [<ffffffff813009e4>] __pci_register_driver+0x64/0x70
    kernel: [<ffffffffa04bc1cb>] qla2x00_module_init+0x1cb/0x21b [qla2xxx]
    kernel: [<ffffffff8100027d>] do_one_initcall+0xad/0x1c0
    kernel: [<ffffffff810e2859>] do_init_module+0x69/0x210
    kernel: [<ffffffff810e4e5c>] load_module+0x5cc/0x750
    kernel: [<ffffffff810e5162>] SyS_init_module+0x92/0xc0
    kernel: [<ffffffff815f37d7>] entry_SYSCALL_64_fastpath+0x12/0x6f
    
    Fixes: 8dfa4b5a9b44 ("qla2xxx: Fix sparse annotation")
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reported-by: Himanshu Madhani <himanshu.madhani@qlogic.com>
    Tested-by: Himanshu Madhani <himanshu.madhani@qlogic.com>
    Reviewed-by: Himanshu Madhani <himanshu.madhani@qlogic.com>
    Reviewed-by: Chad Dupuis <chad.dupuis@qlogic.com>
    Cc: Giridhar Malavali <giridhar.malavali@qlogic.com>
    Cc: Xose Vazquez Perez <xose.vazquez@gmail.com>
    Cc: stable <stable@vger.kernel.org> # v4.3+
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit 0d3f6d297bfb7af24d0508460fdb3d1ec4903fa3
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Nov 11 11:51:06 2015 -0800

    ipv6: Avoid creating RTF_CACHE from a rt that is not managed by fib6 tree
    
    The original bug report:
    https://bugzilla.redhat.com/show_bug.cgi?id=1272571
    
    The setup has a IPv4 GRE tunnel running in a IPSec.  The bug
    happens when ndisc starts sending router solicitation at the gre
    interface.  The simplified oops stack is like:
    
    __lock_acquire+0x1b2/0x1c30
    lock_acquire+0xb9/0x140
    _raw_write_lock_bh+0x3f/0x50
    __ip6_ins_rt+0x2e/0x60
    ip6_ins_rt+0x49/0x50
    ~~~~~~~~
    __ip6_rt_update_pmtu.part.54+0x145/0x250
    ip6_rt_update_pmtu+0x2e/0x40
    ~~~~~~~~
    ip_tunnel_xmit+0x1f1/0xf40
    __gre_xmit+0x7a/0x90
    ipgre_xmit+0x15a/0x220
    dev_hard_start_xmit+0x2bd/0x480
    __dev_queue_xmit+0x696/0x730
    dev_queue_xmit+0x10/0x20
    neigh_direct_output+0x11/0x20
    ip6_finish_output2+0x21f/0x770
    ip6_finish_output+0xa7/0x1d0
    ip6_output+0x56/0x190
    ~~~~~~~~
    ndisc_send_skb+0x1d9/0x400
    ndisc_send_rs+0x88/0xc0
    ~~~~~~~~
    
    The rt passed to ip6_rt_update_pmtu() is created by
    icmp6_dst_alloc() and it is not managed by the fib6 tree,
    so its rt6i_table == NULL.  When __ip6_rt_update_pmtu() creates
    a RTF_CACHE clone, the newly created clone also has rt6i_table == NULL
    and it causes the ip6_ins_rt() oops.
    
    During pmtu update, we only want to create a RTF_CACHE clone
    from a rt which is currently managed (or owned) by the
    fib6 tree.  It means either rt->rt6i_node != NULL or
    rt is a RTF_PCPU clone.
    
    It is worth to note that rt6i_table may not be NULL even it is
    not (yet) managed by the fib6 tree (e.g. addrconf_dst_alloc()).
    Hence, rt6i_node is a better check instead of rt6i_table.
    
    Fixes: 45e4fd26683c ("ipv6: Only create RTF_CACHE routes after encountering pmtu")
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Reported-by: Chris Siebenmann <cks-rhbugzilla@cs.toronto.edu>
    Cc: Chris Siebenmann <cks-rhbugzilla@cs.toronto.edu>
    Cc: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 47191d65b647af5eb5c82ede70ed4c24b1e93ef4
Author: David Herrmann <dh.herrmann@gmail.com>
Date:   Wed Oct 21 11:47:43 2015 +0200

    netlink: fix locking around NETLINK_LIST_MEMBERSHIPS
    
    Currently, NETLINK_LIST_MEMBERSHIPS grabs the netlink table while copying
    the membership state to user-space. However, grabing the netlink table is
    effectively a write_lock_irq(), and as such we should not be triggering
    page-faults in the critical section.
    
    This can be easily reproduced by the following snippet:
        int s = socket(AF_NETLINK, SOCK_RAW, NETLINK_ROUTE);
        void *p = mmap(0, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANON, -1, 0);
        int r = getsockopt(s, 0x10e, 9, p, (void*)((char*)p + 4092));
    
    This should work just fine, but currently triggers EFAULT and a possible
    WARN_ON below handle_mm_fault().
    
    Fix this by reducing locking of NETLINK_LIST_MEMBERSHIPS to a read-side
    lock. The write-lock was overkill in the first place, and the read-lock
    allows page-faults just fine.
    
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: David Herrmann <dh.herrmann@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e0af21c56ddd592b33f74f986a2cb4478b10786e
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Sep 11 16:09:56 2015 +0200

    s390/spinlock: use correct barriers
    
    _raw_write_lock_wait first sets the high order bit to indicate a
    pending writer and then waits for the reader to drop to zero.
    smp_rmb by definition only orders reads against reads. Let's use
    a full smp_mb instead. As right now smp_rmb is implemented
    as full serialization, this needs no stable backport, but this
    patch will be necessary if we reimplement smp_rmb.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit 9a6fbaeb68f43f252c204e2ecf87a5d984b35a9d
Author: Weilong Chen <chenweilong@huawei.com>
Date:   Mon Sep 14 09:27:08 2015 +0800

    ipv6: add check for blackhole or prohibited entry in rt6_redire
    
    There's a check for ip6_null_entry, but it's not enough if the config
    CONFIG_IPV6_MULTIPLE_TABLES is selected. Blackhole or prohibited entries
    should also be ignored.
    
    This path is for kernel before v3.6, as there's a commit b94f1c0
    use icmpv6_notify() instead of rt6_redirect() and rt6_redirect has
    been deleted.
    
    The oops as follow:
        [exception RIP: do_raw_write_lock+12]
        RIP: ffffffff8122c42c  RSP: ffff880666e45820  RFLAGS: 00010282
        RAX: ffff8801207bffd8  RBX: 0000000000000018  RCX: 0000000000000000
        RDX: 0000000000000000  RSI: ffff880666e45898  RDI: 0000000000000018
        RBP: ffff880666e45830   R8: 000000000000001e   R9: 0000000006000000
        R10: ffff88011796b8a0  R11: 0000000000000004  R12: ffff88010391ed00
        R13: 0000000000000000  R14: ffff880666e45898  R15: ffff88011796b890
        ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018
        [ffff880666e45838] _raw_write_lock_bh at ffffffff81450b39
        [ffff880666e45858] __ip6_ins_rt at ffffffff813ed8c1
        [ffff880666e45888] ip6_ins_rt at ffffffff813eef58
        [ffff880666e458b8] rt6_redirect at ffffffff813f0b84
        [ffff880666e45958] ndisc_rcv at ffffffff813f95d8
        [ffff880666e45a08] icmpv6_rcv at ffffffff814000e8
        [ffff880666e45ae8] ip6_input_finish at ffffffff813e43bb
        [ffff880666e45b38] ip6_input at ffffffff813e4b08
        [ffff880666e45b68] ipv6_rcv at ffffffff813e4969
        [ffff880666e45bc8] __netif_receive_skb at ffffffff8135158a
        [ffff880666e45c38] dev_gro_receive at ffffffff81351cb0
        [ffff880666e45c78] napi_gro_receive at ffffffff81351fc5
        [ffff880666e45cb8] tg3_rx at ffffffffa0bfb354 [tg]
        [ffff880666e45d88] tg3_poll_work at ffffffffa0c07857 [tg]
        [ffff880666e45e18] tg3_poll_msix at ffffffffa0c07d1b [tg]
        [ffff880666e45e68] net_rx_action at ffffffff81352219
        [ffff880666e45ec8] __do_softirq at ffffffff8103e5a1
        [ffff880666e45f38] call_softirq at ffffffff81459c4c
        [ffff880666e45f50] do_softirq at ffffffff8100413d
        [ffff880666e45f80] do_IRQ at ffffffff81003cce
    This happened when ip6_route_redirect found a rt which was set
    blackhole, the rt had a NULL rt6i_table argument which is accessed by
    __ip6_ins_rt() when trying to lock rt6i_table->tb6_lock caused a BUG:
    "BUG: unable to handle kernel NULL pointer"
    
    Signed-off-by: Weilong Chen <chenweilong@huawei.com>

commit 03db3a2d81e6e84f3ed3cb9e087cae17d762642b
Author: Matan Barak <matanb@mellanox.com>
Date:   Thu Jul 30 18:33:26 2015 +0300

    IB/core: Add RoCE GID table management
    
    RoCE GIDs are based on IP addresses configured on Ethernet net-devices
    which relate to the RDMA (RoCE) device port.
    
    Currently, each of the low-level drivers that support RoCE (ocrdma,
    mlx4) manages its own RoCE port GID table. As there's nothing which is
    essentially vendor specific, we generalize that, and enhance the RDMA
    core GID cache to do this job.
    
    In order to populate the GID table, we listen for events:
    
    (a) netdev up/down/change_addr events - if a netdev is built onto
        our RoCE device, we need to add/delete its IPs. This involves
        adding all GIDs related to this ndev, add default GIDs, etc.
    
    (b) inet events - add new GIDs (according to the IP addresses)
        to the table.
    
    For programming the port RoCE GID table, providers must implement
    the add_gid and del_gid callbacks.
    
    RoCE GID management requires us to state the associated net_device
    alongside the GID. This information is necessary in order to manage
    the GID table. For example, when a net_device is removed, its
    associated GIDs need to be removed as well.
    
    RoCE mandates generating a default GID for each port, based on the
    related net-device's IPv6 link local. In contrast to the GID based on
    the regular IPv6 link-local (as we generate GID per IP address),
    the default GID is also available when the net device is down (in
    order to support loopback).
    
    Locking is done as follows:
    The patch modify the GID table code both for new RoCE drivers
    implementing the add_gid/del_gid callbacks and for current RoCE and
    IB drivers that do not. The flows for updating the table are
    different, so the locking requirements are too.
    
    While updating RoCE GID table, protection against multiple writers is
    achieved via mutex_lock(&table->lock). Since writing to a table
    requires us to find an entry (possible a free entry) in the table and
    then modify it, this mutex protects both the find_gid and write_gid
    ensuring the atomicity of the action.
    Each entry in the GID cache is protected by rwlock. In RoCE, writing
    (usually results from netdev notifier) involves invoking the vendor's
    add_gid and del_gid callbacks, which could sleep.
    Therefore, an invalid flag is added for each entry. Updates for RoCE are
    done via a workqueue, thus sleeping is permitted.
    
    In IB, updates are done in write_lock_irq(&device->cache.lock), thus
    write_gid isn't allowed to sleep and add_gid/del_gid are not called.
    
    When passing net-device into/out-of the GID cache, the device
    is always passed held (dev_hold).
    
    The code uses a single work item for updating all RDMA devices,
    following a netdev or inet notifier.
    
    The patch moves the cache from being a client (which was incorrect,
    as the cache is part of the IB infrastructure) to being explicitly
    initialized/freed when a device is registered/removed.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

commit 1f979b117b17978b70f20bd981f0791012b1cddf
Merge: 776829de90c5 9c7370a166b4
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Aug 17 14:28:04 2015 -0700

    Merge branch 'ipv6_percpu_rt_deadlock'
    
    Martin KaFai Lau says:
    
    ====================
    ipv6: Fix a potential deadlock when creating pcpu rt
    
    v1 -> v2:
    A minor change in the commit message of patch 2.
    
    This patch series fixes a potential deadlock when creating a pcpu rt.
    It happens when dst_alloc() decided to run gc. Something like this:
    
    read_lock(&table->tb6_lock);
    ip6_rt_pcpu_alloc()
    => dst_alloc()
    => ip6_dst_gc()
    => write_lock(&table->tb6_lock); /* oops */
    
    Patch 1 and 2 are some prep works.
    Patch 3 is the fix.
    
    Original report: https://bugzilla.kernel.org/show_bug.cgi?id=102291
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 9c7370a166b4e157137bfbfe2ad296d57147547c
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Aug 14 11:05:54 2015 -0700

    ipv6: Fix a potential deadlock when creating pcpu rt
    
    rt6_make_pcpu_route() is called under read_lock(&table->tb6_lock).
    rt6_make_pcpu_route() calls ip6_rt_pcpu_alloc(rt) which then
    calls dst_alloc().  dst_alloc() _may_ call ip6_dst_gc() which takes
    the write_lock(&tabl->tb6_lock).  A visualized version:
    
    read_lock(&table->tb6_lock);
    rt6_make_pcpu_route();
    => ip6_rt_pcpu_alloc();
    => dst_alloc();
    => ip6_dst_gc();
    => write_lock(&table->tb6_lock); /* oops */
    
    The fix is to do a read_unlock first before calling ip6_rt_pcpu_alloc().
    
    A reported stack:
    
    [141625.537638] INFO: rcu_sched self-detected stall on CPU { 27}  (t=60000 jiffies g=4159086 c=4159085 q=2139)
    [141625.547469] Task dump for CPU 27:
    [141625.550881] mtr             R  running task        0 22121  22081 0x00000008
    [141625.558069]  0000000000000000 ffff88103f363d98 ffffffff8106e488 000000000000001b
    [141625.565641]  ffffffff81684900 ffff88103f363db8 ffffffff810702b0 0000000008000000
    [141625.573220]  ffffffff81684900 ffff88103f363de8 ffffffff8108df9f ffff88103f375a00
    [141625.580803] Call Trace:
    [141625.583345]  <IRQ>  [<ffffffff8106e488>] sched_show_task+0xc1/0xc6
    [141625.589650]  [<ffffffff810702b0>] dump_cpu_task+0x35/0x39
    [141625.595144]  [<ffffffff8108df9f>] rcu_dump_cpu_stacks+0x6a/0x8c
    [141625.601320]  [<ffffffff81090606>] rcu_check_callbacks+0x1f6/0x5d4
    [141625.607669]  [<ffffffff810940c8>] update_process_times+0x2a/0x4f
    [141625.613925]  [<ffffffff8109fbee>] tick_sched_handle+0x32/0x3e
    [141625.619923]  [<ffffffff8109fc2f>] tick_sched_timer+0x35/0x5c
    [141625.625830]  [<ffffffff81094a1f>] __hrtimer_run_queues+0x8f/0x18d
    [141625.632171]  [<ffffffff81094c9e>] hrtimer_interrupt+0xa0/0x166
    [141625.638258]  [<ffffffff8102bf2a>] local_apic_timer_interrupt+0x4e/0x52
    [141625.645036]  [<ffffffff8102c36f>] smp_apic_timer_interrupt+0x39/0x4a
    [141625.651643]  [<ffffffff8140b9e8>] apic_timer_interrupt+0x68/0x70
    [141625.657895]  <EOI>  [<ffffffff81346ee8>] ? dst_destroy+0x7c/0xb5
    [141625.664188]  [<ffffffff813d45b5>] ? fib6_flush_trees+0x20/0x20
    [141625.670272]  [<ffffffff81082b45>] ? queue_write_lock_slowpath+0x60/0x6f
    [141625.677140]  [<ffffffff8140aa33>] _raw_write_lock_bh+0x23/0x25
    [141625.683218]  [<ffffffff813d4553>] __fib6_clean_all+0x40/0x82
    [141625.689124]  [<ffffffff813d45b5>] ? fib6_flush_trees+0x20/0x20
    [141625.695207]  [<ffffffff813d6058>] fib6_clean_all+0xe/0x10
    [141625.700854]  [<ffffffff813d60d3>] fib6_run_gc+0x79/0xc8
    [141625.706329]  [<ffffffff813d0510>] ip6_dst_gc+0x85/0xf9
    [141625.711718]  [<ffffffff81346d68>] dst_alloc+0x55/0x159
    [141625.717105]  [<ffffffff813d09b5>] __ip6_dst_alloc.isra.32+0x19/0x63
    [141625.723620]  [<ffffffff813d1830>] ip6_pol_route+0x36a/0x3e8
    [141625.729441]  [<ffffffff813d18d6>] ip6_pol_route_output+0x11/0x13
    [141625.735700]  [<ffffffff813f02c8>] fib6_rule_action+0xa7/0x1bf
    [141625.741698]  [<ffffffff813d18c5>] ? ip6_pol_route_input+0x17/0x17
    [141625.748043]  [<ffffffff81357c48>] fib_rules_lookup+0xb5/0x12a
    [141625.754050]  [<ffffffff81141628>] ? poll_select_copy_remaining+0xf9/0xf9
    [141625.761002]  [<ffffffff813f0535>] fib6_rule_lookup+0x37/0x5c
    [141625.766914]  [<ffffffff813d18c5>] ? ip6_pol_route_input+0x17/0x17
    [141625.773260]  [<ffffffff813d008c>] ip6_route_output+0x7a/0x82
    [141625.779177]  [<ffffffff813c44c8>] ip6_dst_lookup_tail+0x53/0x112
    [141625.785437]  [<ffffffff813c45c3>] ip6_dst_lookup_flow+0x2a/0x6b
    [141625.791604]  [<ffffffff813ddaab>] rawv6_sendmsg+0x407/0x9b6
    [141625.797423]  [<ffffffff813d7914>] ? do_ipv6_setsockopt.isra.8+0xd87/0xde2
    [141625.804464]  [<ffffffff8139d4b4>] inet_sendmsg+0x57/0x8e
    [141625.810028]  [<ffffffff81329ba3>] sock_sendmsg+0x2e/0x3c
    [141625.815588]  [<ffffffff8132be57>] SyS_sendto+0xfe/0x143
    [141625.821063]  [<ffffffff813dd551>] ? rawv6_setsockopt+0x5e/0x67
    [141625.827146]  [<ffffffff8132c9f8>] ? sock_common_setsockopt+0xf/0x11
    [141625.833660]  [<ffffffff8132c08c>] ? SyS_setsockopt+0x81/0xa2
    [141625.839565]  [<ffffffff8140ac17>] entry_SYSCALL_64_fastpath+0x12/0x6a
    
    Fixes: d52d3997f843 ("pv6: Create percpu rt6_info")
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    CC: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Reported-by: Steinar H. Gunderson <sgunderson@bigfoot.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 70d2db7b21684bb2b808e78fbab8e594dd7e50ce
Merge: 205845a34763 8d6c31bf5741
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Jul 27 01:08:26 2015 -0700

    Merge branch 'rt6_probe_write_lock'
    
    Martin KaFai Lau says:
    
    ====================
    ipv6: Avoid rt6_probe() taking writer lock in the fast path
    
    v1 -> v2:
    1. Separate the code re-arrangement into another patch
    2. Fix style
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 990edb428c2c85c22ca770330437db7183cbe8b5
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Jul 24 09:57:42 2015 -0700

    ipv6: Re-arrange code in rt6_probe()
    
    It is a prep work for the next patch to remove write_lock
    from rt6_probe().
    
    1. Reduce the number of if(neigh) check.  From 4 to 1.
    2. Bring the write_(un)lock() closer to the operations that the
       lock is protecting.
    
    Hopefully, the above make rt6_probe() more readable.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Cc: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Cc: Julian Anastasov <ja@ssi.bg>
    Cc: YOSHIFUJI Hideaki <hideaki.yoshifuji@miraclelinux.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 454d3a2500a4eb33be85dde3bfba9e5f6b5efadc
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jul 22 17:59:11 2015 +0200

    cpufreq: Remove cpufreq_rwsem
    
    cpufreq_rwsem was introduced in commit 6eed9404ab3c4 ("cpufreq: Use
    rwsem for protecting critical sections) in order to replace
    try_module_get() on the cpu-freq driver. That try_module_get() worked
    well until the refcount was so heavily used that module removal became
    more or less impossible.
    
    Though when looking at the various (undocumented) protection
    mechanisms in that code, the randomly sprinkeled around cpufreq_rwsem
    locking sites are superfluous.
    
    The policy, which is acquired in cpufreq_cpu_get() and released in
    cpufreq_cpu_put() is sufficiently protected already.
    
      cpufreq_cpu_get(cpu)
        /* Protects against concurrent driver removal */
        read_lock_irqsave(&cpufreq_driver_lock, flags);
        policy = per_cpu(cpufreq_cpu_data, cpu);
        kobject_get(&policy->kobj);
        read_unlock_irqrestore(&cpufreq_driver_lock, flags);
    
    The reference on the policy serializes versus module unload already:
    
      cpufreq_unregister_driver()
        subsys_interface_unregister()
          __cpufreq_remove_dev_finish()
            per_cpu(cpufreq_cpu_data) = NULL;
            cpufreq_policy_put_kobj()
    
    If there is a reference held on the policy, i.e. obtained prior to the
    unregister call, then cpufreq_policy_put_kobj() will wait until that
    reference is dropped. So once subsys_interface_unregister() returns
    there is no policy pointer in flight and no new reference can be
    obtained. So that rwsem protection is useless.
    
    The other usage of cpufreq_rwsem in show()/store() of the sysfs
    interface is redundant as well because sysfs already does the proper
    kobject_get()/put() pairs.
    
    That leaves CPU hotplug versus module removal. The current
    down_write() around the write_lock() in cpufreq_unregister_driver() is
    silly at best as it protects actually nothing.
    
    The trivial solution to this is to prevent hotplug across
    cpufreq_unregister_driver completely.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 405963b6a57c60040bc1dad2597f7f4b897954d1
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Tue Jun 9 11:19:13 2015 -0400

    locking/qrwlock: Don't contend with readers when setting _QW_WAITING
    
    The current cmpxchg() loop in setting the _QW_WAITING flag for writers
    in queue_write_lock_slowpath() will contend with incoming readers
    causing possibly extra cmpxchg() operations that are wasteful. This
    patch changes the code to do a byte cmpxchg() to eliminate contention
    with new readers.
    
    A multithreaded microbenchmark running 5M read_lock/write_lock loop
    on a 8-socket 80-core Westmere-EX machine running 4.0 based kernel
    with the qspinlock patch have the following execution times (in ms)
    with and without the patch:
    
    With R:W ratio = 5:1
    
            Threads    w/o patch    with patch      % change
            -------    ---------    ----------      --------
               2         990            895           -9.6%
               3        2136           1912          -10.5%
               4        3166           2830          -10.6%
               5        3953           3629           -8.2%
               6        4628           4405           -4.8%
               7        5344           5197           -2.8%
               8        6065           6004           -1.0%
               9        6826           6811           -0.2%
              10        7599           7599            0.0%
              15        9757           9766           +0.1%
              20       13767          13817           +0.4%
    
    With small number of contending threads, this patch can improve
    locking performance by up to 10%. With more contending threads,
    however, the gain diminishes.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1433863153-30722-3-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit dfcce791fb0ad06f3f0b745a23160b9d8858fe39
Author: Kirill Tkhai <ktkhai@parallels.com>
Date:   Thu Apr 16 12:48:01 2015 -0700

    fs/exec.c:de_thread: move notify_count write under lock
    
    We set sig->notify_count = -1 between RELEASE and ACQUIRE operations:
    
            spin_unlock_irq(lock);
            ...
            if (!thread_group_leader(tsk)) {
                    ...
                    for (;;) {
                            sig->notify_count = -1;
                            write_lock_irq(&tasklist_lock);
    
    There are no restriction on it so other processors may see this STORE
    mixed with other STOREs in both areas limited by the spinlocks.
    
    Probably, it may be reordered with the above
    
            sig->group_exit_task = tsk;
            sig->notify_count = zap_other_threads(tsk);
    
    in some way.
    
    Set it under tasklist_lock locked to be sure nothing will be reordered.
    
    Signed-off-by: Kirill Tkhai <ktkhai@parallels.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 7a215f89a0335582292ec6f3edaa3abd570da75a
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Fri Jan 30 01:14:25 2015 -0800

    locking/rwsem: Set lock ownership ASAP
    
    In order to optimize the spinning step, we need to set the lock
    owner as soon as the lock is acquired; after a successful counter
    cmpxchg operation, that is. This is particularly useful as rwsems
    need to set the owner to nil for readers, so there is a greater
    chance of falling out of the spinning. Currently we only set the
    owner much later in the game, in the more generic level -- latency
    can be specially bad when waiting for a node->next pointer when
    releasing the osq in up_write calls.
    
    As such, update the owner inside rwsem_try_write_lock (when the
    lock is obtained after blocking) and rwsem_try_write_lock_unqueued
    (when the lock is obtained while spinning). This requires creating
    a new internal rwsem.h header to share the owner related calls.
    
    Also cleanup some headers for mutex and rwsem.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Jason Low <jason.low2@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Link: http://lkml.kernel.org/r/1422609267-15102-4-git-send-email-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 08eee69fcf6baea543a2b4d2a2fcba0e61aa3160
Author: Minchan Kim <minchan@kernel.org>
Date:   Thu Feb 12 15:00:45 2015 -0800

    zram: remove init_lock in zram_make_request
    
    Admin could reset zram during I/O operation going on so we have used
    zram->init_lock as read-side lock in I/O path to prevent sudden zram
    meta freeing.
    
    However, the init_lock is really troublesome.  We can't do call
    zram_meta_alloc under init_lock due to lockdep splat because
    zram_rw_page is one of the function under reclaim path and hold it as
    read_lock while other places in process context hold it as write_lock.
    So, we have used allocation out of the lock to avoid lockdep warn but
    it's not good for readability and fainally, I met another lockdep splat
    between init_lock and cpu_hotplug from kmem_cache_destroy during working
    zsmalloc compaction.  :(
    
    Yes, the ideal is to remove horrible init_lock of zram in rw path.  This
    patch removes it in rw path and instead, add atomic refcount for meta
    lifetime management and completion to free meta in process context.
    It's important to free meta in process context because some of resource
    destruction needs mutex lock, which could be held if we releases the
    resource in reclaim context so it's deadlock, again.
    
    As a bonus, we could remove init_done check in rw path because
    zram_meta_get will do a role for it, instead.
    
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Ganesh Mahendran <opensource.ganesh@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 85572d7c75fd5b9fa3fc911e1c99c68ec74903a0
Author: NeilBrown <neilb@suse.de>
Date:   Mon Dec 15 12:56:56 2014 +1100

    md: rename mddev->write_lock to mddev->lock
    
    This lock is used for (slightly) more than helping with writing
    superblocks, and it will soon be extended further.  So the
    name is inappropriate.
    
    Also, the _irq variant hasn't been needed since 2.6.37 as it is
    never taking from interrupt or bh context.
    
    So:
      -rename write_lock to lock
      -document what it protects
      -remove _irq ... except in md_flush_request() as there
         is no wait_event_lock() (with no _irq).  This can be
         cleaned up after appropriate changes to wait.h.
    
    Signed-off-by: NeilBrown <neilb@suse.de>

commit 482a3767e5087f6e6ad2486a6655aaa5f3d59301
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:55:20 2014 -0800

    exit: reparent: call forget_original_parent() under tasklist_lock
    
    Shift "release dead children" loop from forget_original_parent() to its
    caller, exit_notify().  It is safe to reap them even if our parent reaps
    us right after we drop tasklist_lock, those children no longer have any
    connection to the exiting task.
    
    And this allows us to avoid write_lock_irq(tasklist_lock) right after it
    was released by forget_original_parent(), we can simply call it with
    tasklist_lock held.
    
    While at it, move the comment about forget_original_parent() up to
    this function.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit c920f72d13325bea8a52d95e4d18b5f9a86a19b5
Author: Chris Mason <clm@fb.com>
Date:   Wed Nov 19 10:25:09 2014 -0800

    btrfs: fix lockups from btrfs_clear_path_blocking
    
    commit f82c458a2c3ffb94b431fc6ad791a79df1b3713e upstream.
    
    The fair reader/writer locks mean that btrfs_clear_path_blocking needs
    to strictly follow lock ordering rules even when we already have
    blocking locks on a given path.
    
    Before we can clear a blocking lock on the path, we need to make sure
    all of the locks have been converted to blocking.  This will remove lock
    inversions against anyone spinning in write_lock() against the buffers
    we're trying to get read locks on.  These inversions didn't exist before
    the fair read/writer locks, but now we need to be more careful.
    
    We papered over this deadlock in the past by changing
    btrfs_try_read_lock() to be a true trylock against both the spinlock and
    the blocking lock.  This was slower, and not sufficient to fix all the
    deadlocks.  This patch adds a btrfs_tree_read_lock_atomic(), which
    basically means get the spinlock but trylock on the blocking lock.
    
    Signed-off-by: Chris Mason <clm@fb.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Reported-by: Patrick Schmid <schmid@phys.ethz.ch>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 038134933e1139688ec1ebd769a504fde086cd3e
Author: Chris Mason <clm@fb.com>
Date:   Wed Nov 19 10:25:09 2014 -0800

    btrfs: fix lockups from btrfs_clear_path_blocking
    
    commit f82c458a2c3ffb94b431fc6ad791a79df1b3713e upstream.
    
    The fair reader/writer locks mean that btrfs_clear_path_blocking needs
    to strictly follow lock ordering rules even when we already have
    blocking locks on a given path.
    
    Before we can clear a blocking lock on the path, we need to make sure
    all of the locks have been converted to blocking.  This will remove lock
    inversions against anyone spinning in write_lock() against the buffers
    we're trying to get read locks on.  These inversions didn't exist before
    the fair read/writer locks, but now we need to be more careful.
    
    We papered over this deadlock in the past by changing
    btrfs_try_read_lock() to be a true trylock against both the spinlock and
    the blocking lock.  This was slower, and not sufficient to fix all the
    deadlocks.  This patch adds a btrfs_tree_read_lock_atomic(), which
    basically means get the spinlock but trylock on the blocking lock.
    
    Signed-off-by: Chris Mason <clm@fb.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Reported-by: Patrick Schmid <schmid@phys.ethz.ch>
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit 856839b76836a2ee524a8638f568275da57f719c
Author: Eunbong Song <eunb.song@samsung.com>
Date:   Wed Oct 22 06:39:56 2014 +0000

    MIPS: Add arch_trigger_all_cpu_backtrace() function
    
    Currently, arch_trigger_all_cpu_backtrace() is defined in only x86 and
    sparc which have an NMI.  But in case of softlockup, it could be possible
    to dump backtrace of all cpus. and this could be helpful for debugging.
    
    for example, if system has 2 cpus.
    
            CPU 0                           CPU 1
     acquire read_lock()
    
                                    try to do write_lock()
    
     ,,,
     missing read_unlock()
    
    In this case, softlockup will occur becasuse CPU 0 does not call
    read_unlock().  And dump_stack() print only backtrace for "CPU 0". If
    CPU1's backtrace is printed it's very helpful.
    
    [ralf@linux-mips.org: Fixed whitespace and formatting issues.]
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/8200/

commit f82c458a2c3ffb94b431fc6ad791a79df1b3713e
Author: Chris Mason <clm@fb.com>
Date:   Wed Nov 19 10:25:09 2014 -0800

    btrfs: fix lockups from btrfs_clear_path_blocking
    
    The fair reader/writer locks mean that btrfs_clear_path_blocking needs
    to strictly follow lock ordering rules even when we already have
    blocking locks on a given path.
    
    Before we can clear a blocking lock on the path, we need to make sure
    all of the locks have been converted to blocking.  This will remove lock
    inversions against anyone spinning in write_lock() against the buffers
    we're trying to get read locks on.  These inversions didn't exist before
    the fair read/writer locks, but now we need to be more careful.
    
    We papered over this deadlock in the past by changing
    btrfs_try_read_lock() to be a true trylock against both the spinlock and
    the blocking lock.  This was slower, and not sufficient to fix all the
    deadlocks.  This patch adds a btrfs_tree_read_lock_atomic(), which
    basically means get the spinlock but trylock on the blocking lock.
    
    Signed-off-by: Chris Mason <clm@fb.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Reported-by: Patrick Schmid <schmid@phys.ethz.ch>
    cc: stable@vger.kernel.org #v3.15+

commit 369e2b84e4eed08e5368abc3bc4277d500a186ea
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Thu Oct 16 14:19:49 2014 -0400

    tty: Remove sparse lock annotations from tty_write_lock()/_unlock()
    
    sparse lock annotations cannot represent conditional acquire, such
    as mutex_lock_interruptible() or mutex_trylock(), and produce sparse
    warnings at _every_ correct call site.
    
    Remove lock annotations from tty_write_lock() and tty_write_unlock().
    
    Fixes sparse warnings:
    drivers/tty/tty_io.c:1083:13: warning: context imbalance in 'tty_write_unlock' - wrong count at exit
    drivers/tty/tty_io.c:1090:12: warning: context imbalance in 'tty_write_lock' - wrong count at exit
    drivers/tty/tty_io.c:1211:17: warning: context imbalance in 'tty_write_message' - unexpected unlock
    drivers/tty/tty_io.c:1233:16: warning: context imbalance in 'tty_write' - different lock contexts for basic block
    drivers/tty/tty_io.c:1285:5: warning: context imbalance in 'tty_send_xchar' - different lock contexts for basic block
    drivers/tty/tty_io.c:2653:12: warning: context imbalance in 'send_break' - different lock contexts for basic block
    
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 136d5258b2bc4ffae99cb69874a76624c26fbfad
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Wed Sep 10 15:06:34 2014 -0400

    tty: Move and rename send_prio_char() as tty_send_xchar()
    
    Relocate the file-scope function, send_prio_char(), as a global
    helper tty_send_xchar(). Remove the global declarations for
    tty_write_lock()/tty_write_unlock(), as these are file-scope only now.
    
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f0bab73cb539fb803c4d419951e8d28aa4964f8f
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Wed Aug 6 13:22:01 2014 -0400

    locking/lockdep: Restrict the use of recursive read_lock() with qrwlock
    
    Unlike the original unfair rwlock implementation, queued rwlock
    will grant lock according to the chronological sequence of the lock
    requests except when the lock requester is in the interrupt context.
    Consequently, recursive read_lock calls will now hang the process if
    there is a write_lock call somewhere in between the read_lock calls.
    
    This patch updates the lockdep implementation to look for recursive
    read_lock calls. A new read state (3) is used to mark those read_lock
    call that cannot be recursively called except in the interrupt
    context. The new read state does exhaust the 2 bits available in
    held_lock:read bit field. The addition of any new read state in the
    future may require a redesign of how all those bits are squeezed
    together in the held_lock structure.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Maarten Lankhorst <maarten.lankhorst@canonical.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1407345722-61615-2-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 161427758577d5b5299c0b897730235a1fa950cc
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:37 2014 +0200

    USB: cdc-acm: fix write and resume race
    
    commit e144ed28bed10684f9aaec6325ed974d53f76110 upstream.
    
    Fix race between write() and resume() due to improper locking that could
    lead to writes being reordered.
    
    Resume must be done atomically and susp_count be protected by the
    write_lock in order to prevent racing with write(). This could otherwise
    lead to writes being reordered if write() grabs the write_lock after
    susp_count is decremented, but before the delayed urb is submitted.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    [bwh: Backported to 3.2:
     - Adjust context
     - Move mutex_lock(acm->mutex) above acquisition of spinlocks]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 3627c07522db92848b8d6414616315028084f5f2
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:36 2014 +0200

    USB: cdc-acm: fix write and suspend race
    
    commit 5a345c20c17d87099224a4be12e69e5bd7023dca upstream.
    
    Fix race between write() and suspend() which could lead to writes being
    dropped (or I/O while suspended) if the device is runtime suspended
    while a write request is being processed.
    
    Specifically, suspend() releases the write_lock after determining the
    device is idle but before incrementing the susp_count, thus leaving a
    window where a concurrent write() can submit an urb.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit b728ca06029d085a1585c1926610f26de93b9146
Author: Kirill Tkhai <ktkhai@parallels.com>
Date:   Wed Jun 25 12:19:55 2014 +0400

    sched: Rework check_for_tasks()
    
    1) Iterate thru all of threads in the system.
       Check for all threads, not only for group leaders.
    
    2) Check for p->on_rq instead of p->state and cputime.
       Preempted task in !TASK_RUNNING state  OR just
       created task may be queued, that we want to be
       reported too.
    
    3) Use read_lock() instead of write_lock().
       This function does not change any structures, and
       read_lock() is enough.
    
    Signed-off-by: Kirill Tkhai <ktkhai@parallels.com>
    Reviewed-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ben Segall <bsegall@google.com>
    Cc: Fabian Frederick <fabf@skynet.be>
    Cc: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Cc: Konstantin Khorenko <khorenko@parallels.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael wang <wangyun@linux.vnet.ibm.com>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Todd E Brandt <todd.e.brandt@linux.intel.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1403684395.3462.44.camel@tkhai
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 529ce0fc0bae6fa9520f348d2d5bbc94bf1b2dec
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:37 2014 +0200

    USB: cdc-acm: fix write and resume race
    
    commit e144ed28bed10684f9aaec6325ed974d53f76110 upstream.
    
    Fix race between write() and resume() due to improper locking that could
    lead to writes being reordered.
    
    Resume must be done atomically and susp_count be protected by the
    write_lock in order to prevent racing with write(). This could otherwise
    lead to writes being reordered if write() grabs the write_lock after
    susp_count is decremented, but before the delayed urb is submitted.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit 06c2f546fe195c3a0178a2ec4f5c0ebf2167a97c
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:36 2014 +0200

    USB: cdc-acm: fix write and suspend race
    
    commit 5a345c20c17d87099224a4be12e69e5bd7023dca upstream.
    
    Fix race between write() and suspend() which could lead to writes being
    dropped (or I/O while suspended) if the device is runtime suspended
    while a write request is being processed.
    
    Specifically, suspend() releases the write_lock after determining the
    device is idle but before incrementing the susp_count, thus leaving a
    window where a concurrent write() can submit an urb.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit e2b5e89b636fa1bd5f359bb9a9857866233785e2
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:37 2014 +0200

    USB: cdc-acm: fix write and resume race
    
    commit e144ed28bed10684f9aaec6325ed974d53f76110 upstream.
    
    Fix race between write() and resume() due to improper locking that could
    lead to writes being reordered.
    
    Resume must be done atomically and susp_count be protected by the
    write_lock in order to prevent racing with write(). This could otherwise
    lead to writes being reordered if write() grabs the write_lock after
    susp_count is decremented, but before the delayed urb is submitted.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7408a522713f2bb330bf36f606d2ea01b6281e5e
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:36 2014 +0200

    USB: cdc-acm: fix write and suspend race
    
    commit 5a345c20c17d87099224a4be12e69e5bd7023dca upstream.
    
    Fix race between write() and suspend() which could lead to writes being
    dropped (or I/O while suspended) if the device is runtime suspended
    while a write request is being processed.
    
    Specifically, suspend() releases the write_lock after determining the
    device is idle but before incrementing the susp_count, thus leaving a
    window where a concurrent write() can submit an urb.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 39d43e4ede7dbb4001a025e157688139c1e2dbdb
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:37 2014 +0200

    USB: cdc-acm: fix write and resume race
    
    commit e144ed28bed10684f9aaec6325ed974d53f76110 upstream.
    
    Fix race between write() and resume() due to improper locking that could
    lead to writes being reordered.
    
    Resume must be done atomically and susp_count be protected by the
    write_lock in order to prevent racing with write(). This could otherwise
    lead to writes being reordered if write() grabs the write_lock after
    susp_count is decremented, but before the delayed urb is submitted.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2d0227bd3d32c96be8c14fd860b0de9fbc87b155
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:36 2014 +0200

    USB: cdc-acm: fix write and suspend race
    
    commit 5a345c20c17d87099224a4be12e69e5bd7023dca upstream.
    
    Fix race between write() and suspend() which could lead to writes being
    dropped (or I/O while suspended) if the device is runtime suspended
    while a write request is being processed.
    
    Specifically, suspend() releases the write_lock after determining the
    device is idle but before incrementing the susp_count, thus leaving a
    window where a concurrent write() can submit an urb.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1b3fe0068c8d5efd5d81ce6f5616d9cec1921e3a
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:37 2014 +0200

    USB: cdc-acm: fix write and resume race
    
    commit e144ed28bed10684f9aaec6325ed974d53f76110 upstream.
    
    Fix race between write() and resume() due to improper locking that could
    lead to writes being reordered.
    
    Resume must be done atomically and susp_count be protected by the
    write_lock in order to prevent racing with write(). This could otherwise
    lead to writes being reordered if write() grabs the write_lock after
    susp_count is decremented, but before the delayed urb is submitted.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c5ece48205da2cfe6b5fbfb82d305283c05299f1
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:36 2014 +0200

    USB: cdc-acm: fix write and suspend race
    
    commit 5a345c20c17d87099224a4be12e69e5bd7023dca upstream.
    
    Fix race between write() and suspend() which could lead to writes being
    dropped (or I/O while suspended) if the device is runtime suspended
    while a write request is being processed.
    
    Specifically, suspend() releases the write_lock after determining the
    device is idle but before incrementing the susp_count, thus leaving a
    window where a concurrent write() can submit an urb.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 97e72c4aa615b5ed6a7c4b22b9fa766679c749bc
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:37 2014 +0200

    USB: cdc-acm: fix write and resume race
    
    commit e144ed28bed10684f9aaec6325ed974d53f76110 upstream.
    
    Fix race between write() and resume() due to improper locking that could
    lead to writes being reordered.
    
    Resume must be done atomically and susp_count be protected by the
    write_lock in order to prevent racing with write(). This could otherwise
    lead to writes being reordered if write() grabs the write_lock after
    susp_count is decremented, but before the delayed urb is submitted.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit be7978bb5b91ef7168a93cf91acd672dfb65f0d0
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:36 2014 +0200

    USB: cdc-acm: fix write and suspend race
    
    commit 5a345c20c17d87099224a4be12e69e5bd7023dca upstream.
    
    Fix race between write() and suspend() which could lead to writes being
    dropped (or I/O while suspended) if the device is runtime suspended
    while a write request is being processed.
    
    Specifically, suspend() releases the write_lock after determining the
    device is idle but before incrementing the susp_count, thus leaving a
    window where a concurrent write() can submit an urb.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f31e799459659ae88c341aeac16a8a5efb1271d4
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Mon Jun 23 11:28:51 2014 -0400

    selinux: no recursive read_lock of policy_rwlock in security_genfs_sid()
    
    With the introduction of fair queued rwlock, recursive read_lock()
    may hang the offending process if there is a write_lock() somewhere
    in between.
    
    With recursive read_lock checking enabled, the following error was
    reported:
    
    =============================================
    [ INFO: possible recursive locking detected ]
    3.16.0-rc1 #2 Tainted: G            E
    ---------------------------------------------
    load_policy/708 is trying to acquire lock:
     (policy_rwlock){.+.+..}, at: [<ffffffff8125b32a>]
    security_genfs_sid+0x3a/0x170
    
    but task is already holding lock:
     (policy_rwlock){.+.+..}, at: [<ffffffff8125b48c>]
    security_fs_use+0x2c/0x110
    
    other info that might help us debug this:
     Possible unsafe locking scenario:
    
           CPU0
           ----
      lock(policy_rwlock);
      lock(policy_rwlock);
    
    This patch fixes the occurrence of recursive read_lock() of
    policy_rwlock by adding a helper function __security_genfs_sid()
    which requires caller to take the lock before calling it. The
    security_fs_use() was then modified to call the new helper function.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Acked-by:  Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Paul Moore <pmoore@redhat.com>

commit b050f9f6ddefe5de9c130fda6493ccaacd5168ba
Author: Filipe Manana <fdmanana@gmail.com>
Date:   Thu Jun 12 02:47:37 2014 +0100

    Btrfs: fix qgroups sanity test crash or hang
    
    Often when running the qgroups sanity test, a crash or a hang happened.
    This is because the extent buffer the test uses for the root node doesn't
    have an header level explicitly set, making it have a random level value.
    This is a problem when it's not zero for the btrfs_search_slot() calls
    the test ends up doing, resulting in crashes or hangs such as the following:
    
    [ 6454.127192] Btrfs loaded, debug=on, assert=on, integrity-checker=on
    (...)
    [ 6454.127760] BTRFS: selftest: Running qgroup tests
    [ 6454.127964] BTRFS: selftest: Running test_test_no_shared_qgroup
    [ 6454.127966] BTRFS: selftest: Qgroup basic add
    [ 6480.152005] BUG: soft lockup - CPU#0 stuck for 23s! [modprobe:5383]
    [ 6480.152005] Modules linked in: btrfs(+) xor raid6_pq binfmt_misc nfsd auth_rpcgss oid_registry nfs_acl nfs lockd fscache sunrpc i2c_piix4 i2c_core pcspkr evbug psmouse serio_raw e1000 [last unloaded: btrfs]
    [ 6480.152005] irq event stamp: 188448
    [ 6480.152005] hardirqs last  enabled at (188447): [<ffffffff8168ef5c>] restore_args+0x0/0x30
    [ 6480.152005] hardirqs last disabled at (188448): [<ffffffff81698e6a>] apic_timer_interrupt+0x6a/0x80
    [ 6480.152005] softirqs last  enabled at (188446): [<ffffffff810516cf>] __do_softirq+0x1cf/0x450
    [ 6480.152005] softirqs last disabled at (188441): [<ffffffff81051c25>] irq_exit+0xb5/0xc0
    [ 6480.152005] CPU: 0 PID: 5383 Comm: modprobe Not tainted 3.15.0-rc8-fdm-btrfs-next-33+ #4
    [ 6480.152005] Hardware name: Bochs Bochs, BIOS Bochs 01/01/2011
    [ 6480.152005] task: ffff8802146125a0 ti: ffff8800d0d00000 task.ti: ffff8800d0d00000
    [ 6480.152005] RIP: 0010:[<ffffffff81349a63>]  [<ffffffff81349a63>] __write_lock_failed+0x13/0x20
    [ 6480.152005] RSP: 0018:ffff8800d0d038e8  EFLAGS: 00000287
    [ 6480.152005] RAX: 0000000000000000 RBX: ffffffff8168ef5c RCX: 000005deb8525852
    [ 6480.152005] RDX: 0000000000000000 RSI: 0000000000001d45 RDI: ffff8802105000b8
    [ 6480.152005] RBP: ffff8800d0d038e8 R08: fffffe12710f63db R09: ffffffffa03196fb
    [ 6480.152005] R10: ffff8802146125a0 R11: ffff880214612e28 R12: ffff8800d0d03858
    [ 6480.152005] R13: 0000000000000000 R14: ffff8800d0d00000 R15: ffff8802146125a0
    [ 6480.152005] FS:  00007f14ff804700(0000) GS:ffff880215e00000(0000) knlGS:0000000000000000
    [ 6480.152005] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
    [ 6480.152005] CR2: 00007fff4df0dac8 CR3: 00000000d1796000 CR4: 00000000000006f0
    [ 6480.152005] Stack:
    [ 6480.152005]  ffff8800d0d03908 ffffffff810ae967 0000000000000001 ffff8802105000b8
    [ 6480.152005]  ffff8800d0d03938 ffffffff8168e57e ffffffffa0319c16 0000000000000007
    [ 6480.152005]  ffff880210500000 ffff880210500100 ffff8800d0d039b8 ffffffffa0319c16
    [ 6480.152005] Call Trace:
    [ 6480.152005]  [<ffffffff810ae967>] do_raw_write_lock+0x47/0xa0
    [ 6480.152005]  [<ffffffff8168e57e>] _raw_write_lock+0x5e/0x80
    [ 6480.152005]  [<ffffffffa0319c16>] ? btrfs_tree_lock+0x116/0x270 [btrfs]
    [ 6480.152005]  [<ffffffffa0319c16>] btrfs_tree_lock+0x116/0x270 [btrfs]
    [ 6480.152005]  [<ffffffffa02b2acb>] btrfs_lock_root_node+0x3b/0x50 [btrfs]
    [ 6480.152005]  [<ffffffffa02b81a6>] btrfs_search_slot+0x916/0xa20 [btrfs]
    [ 6480.152005]  [<ffffffff811a727f>] ? create_object+0x23f/0x300
    [ 6480.152005]  [<ffffffffa02b9958>] btrfs_insert_empty_items+0x78/0xd0 [btrfs]
    [ 6480.152005]  [<ffffffffa036041a>] insert_normal_tree_ref.constprop.4+0xa2/0x19a [btrfs]
    [ 6480.152005]  [<ffffffffa03605c3>] test_no_shared_qgroup+0xb1/0x1ca [btrfs]
    [ 6480.152005]  [<ffffffff8108cad6>] ? local_clock+0x16/0x30
    [ 6480.152005]  [<ffffffffa035ef8e>] btrfs_test_qgroups+0x1ae/0x1d7 [btrfs]
    [ 6480.152005]  [<ffffffffa03a69d2>] ? ftrace_define_fields_btrfs_space_reservation+0xfd/0xfd [btrfs]
    [ 6480.152005]  [<ffffffffa03a6a86>] init_btrfs_fs+0xb4/0x153 [btrfs]
    [ 6480.152005]  [<ffffffff81000352>] do_one_initcall+0x102/0x150
    [ 6480.152005]  [<ffffffff8103d223>] ? set_memory_nx+0x43/0x50
    [ 6480.152005]  [<ffffffff81682668>] ? set_section_ro_nx+0x6d/0x74
    [ 6480.152005]  [<ffffffff810d91cc>] load_module+0x1cdc/0x2630
    (...)
    
    Therefore initialize the extent buffer as an empty leaf (level 0).
    
    Issue easy to reproduce when btrfs is built as a module via:
    
        $ for ((i = 1; i <= 1000000; i++)); do rmmod btrfs; modprobe btrfs; done
    
    Signed-off-by: Filipe David Borba Manana <fdmanana@gmail.com>
    Signed-off-by: Chris Mason <clm@fb.com>

commit b1d42efc217fdc1a6a704b344fd902ae52a012c8
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:48 2014 +0200

    USB: cdc-acm: minimise no-suspend window during shutdown
    
    Now that acm_set_control() handles runtime PM properly, the only
    remaining reason for the PM operations in shutdown is to clear the
    needs_remote_wakeup flag before the final put.
    
    Note that this also means that we now need to grab the write_lock to
    prevent racing with resume.
    
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e144ed28bed10684f9aaec6325ed974d53f76110
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:37 2014 +0200

    USB: cdc-acm: fix write and resume race
    
    Fix race between write() and resume() due to improper locking that could
    lead to writes being reordered.
    
    Resume must be done atomically and susp_count be protected by the
    write_lock in order to prevent racing with write(). This could otherwise
    lead to writes being reordered if write() grabs the write_lock after
    susp_count is decremented, but before the delayed urb is submitted.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Cc: <stable@vger.kernel.org>    # v2.6.27
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5a345c20c17d87099224a4be12e69e5bd7023dca
Author: Johan Hovold <johan@kernel.org>
Date:   Mon May 26 19:23:36 2014 +0200

    USB: cdc-acm: fix write and suspend race
    
    Fix race between write() and suspend() which could lead to writes being
    dropped (or I/O while suspended) if the device is runtime suspended
    while a write request is being processed.
    
    Specifically, suspend() releases the write_lock after determining the
    device is idle but before incrementing the susp_count, thus leaving a
    window where a concurrent write() can submit an urb.
    
    Fixes: 11ea859d64b6 ("USB: additional power savings for cdc-acm devices
    that support remote wakeup")
    
    Cc: <stable@vger.kernel.org>        # v2.6.27
    Signed-off-by: Johan Hovold <jhovold@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 939c5ae4029e1679bb93f7d09afb8c831db985bd
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri May 16 09:35:02 2014 +0200

    s390/rwlock: add missing local_irq_restore calls
    
    The out of line _raw_read_lock_wait_flags/_raw_write_lock_wait_flags
    functions for the arch_read_lock_flags/arch_write_lock_flags  calls
    fail to re-enable the interrupts after another unsuccessful try to
    get the lock with compare-and-swap. The following wait would be
    done with interrupts disabled which is suboptimal.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit a14cc8a6f5bc2e3c491edc92c07e17ba2f94f1d0
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Sat May 3 14:04:59 2014 +0200

    n_tty: Fix n_tty_write crash when echoing in raw mode
    
    The tty atomic_write_lock does not provide an exclusion guarantee for
    the tty driver if the termios settings are LECHO & !OPOST.  And since
    it is unexpected and not allowed to call TTY buffer helpers like
    tty_insert_flip_string concurrently, this may lead to crashes when
    concurrect writers call pty_write. In that case the following two
    writers:
    * the ECHOing from a workqueue and
    * pty_write from the process
    race and can overflow the corresponding TTY buffer like follows.
    
    If we look into tty_insert_flip_string_fixed_flag, there is:
      int space = __tty_buffer_request_room(port, goal, flags);
      struct tty_buffer *tb = port->buf.tail;
      ...
      memcpy(char_buf_ptr(tb, tb->used), chars, space);
      ...
      tb->used += space;
    
    so the race of the two can result in something like this:
                  A                                B
    __tty_buffer_request_room
                                      __tty_buffer_request_room
    memcpy(buf(tb->used), ...)
    tb->used += space;
                                      memcpy(buf(tb->used), ...) ->BOOM
    
    B's memcpy is past the tty_buffer due to the previous A's tb->used
    increment.
    
    Since the N_TTY line discipline input processing can output
    concurrently with a tty write, obtain the N_TTY ldisc output_lock to
    serialize echo output with normal tty writes.  This ensures the tty
    buffer helper tty_insert_flip_string is not called concurrently and
    everything is fine.
    
    Note that this is nicely reproducible by an ordinary user using
    forkpty and some setup around that (raw termios + ECHO). And it is
    present in kernels at least after commit
    d945cb9cce20ac7143c2de8d88b187f62db99bdc (pty: Rework the pty layer to
    use the normal buffering logic) in 2.6.31-rc3.
    
    js: add more info to the commit log
    js: switch to bool
    js: lock unconditionally
    js: lock only the tty->ops->write call
    
    References: CVE-2014-0196
    Reported-and-tested-by: Jiri Slaby <jslaby@suse.cz>
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    (cherry picked from commit 4291086b1f081b869c6d79e5b7441633dc3ace00)
    [wt: 2.6.32 has no n_tty_data, so output_lock is in tty, not tty->disc_data]
    Signed-off-by: Willy Tarreau <w@1wt.eu>

commit 1e5099713cefc67aa562f6d8fe43444f41baf52d
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Sat May 3 14:04:59 2014 +0200

    n_tty: Fix n_tty_write crash when echoing in raw mode
    
    commit 4291086b1f081b869c6d79e5b7441633dc3ace00 upstream.
    
    The tty atomic_write_lock does not provide an exclusion guarantee for
    the tty driver if the termios settings are LECHO & !OPOST.  And since
    it is unexpected and not allowed to call TTY buffer helpers like
    tty_insert_flip_string concurrently, this may lead to crashes when
    concurrect writers call pty_write. In that case the following two
    writers:
    * the ECHOing from a workqueue and
    * pty_write from the process
    race and can overflow the corresponding TTY buffer like follows.
    
    If we look into tty_insert_flip_string_fixed_flag, there is:
      int space = __tty_buffer_request_room(port, goal, flags);
      struct tty_buffer *tb = port->buf.tail;
      ...
      memcpy(char_buf_ptr(tb, tb->used), chars, space);
      ...
      tb->used += space;
    
    so the race of the two can result in something like this:
                  A                                B
    __tty_buffer_request_room
                                      __tty_buffer_request_room
    memcpy(buf(tb->used), ...)
    tb->used += space;
                                      memcpy(buf(tb->used), ...) ->BOOM
    
    B's memcpy is past the tty_buffer due to the previous A's tb->used
    increment.
    
    Since the N_TTY line discipline input processing can output
    concurrently with a tty write, obtain the N_TTY ldisc output_lock to
    serialize echo output with normal tty writes.  This ensures the tty
    buffer helper tty_insert_flip_string is not called concurrently and
    everything is fine.
    
    Note that this is nicely reproducible by an ordinary user using
    forkpty and some setup around that (raw termios + ECHO). And it is
    present in kernels at least after commit
    d945cb9cce20ac7143c2de8d88b187f62db99bdc (pty: Rework the pty layer to
    use the normal buffering logic) in 2.6.31-rc3.
    
    js: add more info to the commit log
    js: switch to bool
    js: lock unconditionally
    js: lock only the tty->ops->write call
    
    References: CVE-2014-0196
    Reported-and-tested-by: Jiri Slaby <jslaby@suse.cz>
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    [bwh: Backported to 3.2: output_lock is a member of struct tty_struct]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 664c0fc651b6ca2ab43ddbb73bbda2acdb2c9915
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Sat May 3 14:04:59 2014 +0200

    n_tty: Fix n_tty_write crash when echoing in raw mode
    
    commit 4291086b1f081b869c6d79e5b7441633dc3ace00 upstream.
    
    The tty atomic_write_lock does not provide an exclusion guarantee for
    the tty driver if the termios settings are LECHO & !OPOST.  And since
    it is unexpected and not allowed to call TTY buffer helpers like
    tty_insert_flip_string concurrently, this may lead to crashes when
    concurrect writers call pty_write. In that case the following two
    writers:
    * the ECHOing from a workqueue and
    * pty_write from the process
    race and can overflow the corresponding TTY buffer like follows.
    
    If we look into tty_insert_flip_string_fixed_flag, there is:
      int space = __tty_buffer_request_room(port, goal, flags);
      struct tty_buffer *tb = port->buf.tail;
      ...
      memcpy(char_buf_ptr(tb, tb->used), chars, space);
      ...
      tb->used += space;
    
    so the race of the two can result in something like this:
                  A                                B
    __tty_buffer_request_room
                                      __tty_buffer_request_room
    memcpy(buf(tb->used), ...)
    tb->used += space;
                                      memcpy(buf(tb->used), ...) ->BOOM
    
    B's memcpy is past the tty_buffer due to the previous A's tb->used
    increment.
    
    Since the N_TTY line discipline input processing can output
    concurrently with a tty write, obtain the N_TTY ldisc output_lock to
    serialize echo output with normal tty writes.  This ensures the tty
    buffer helper tty_insert_flip_string is not called concurrently and
    everything is fine.
    
    Note that this is nicely reproducible by an ordinary user using
    forkpty and some setup around that (raw termios + ECHO). And it is
    present in kernels at least after commit
    d945cb9cce20ac7143c2de8d88b187f62db99bdc (pty: Rework the pty layer to
    use the normal buffering logic) in 2.6.31-rc3.
    
    js: add more info to the commit log
    js: switch to bool
    js: lock unconditionally
    js: lock only the tty->ops->write call
    
    References: CVE-2014-0196
    Reported-and-tested-by: Jiri Slaby <jslaby@suse.cz>
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    [bwh: Backported to 3.2: output_lock is a member of struct tty_struct]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 61461fa9182895c6396ee9704d80fe8ff9d1135d
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Sat May 3 14:04:59 2014 +0200

    n_tty: Fix n_tty_write crash when echoing in raw mode
    
    commit 4291086b1f081b869c6d79e5b7441633dc3ace00 upstream.
    
    The tty atomic_write_lock does not provide an exclusion guarantee for
    the tty driver if the termios settings are LECHO & !OPOST.  And since
    it is unexpected and not allowed to call TTY buffer helpers like
    tty_insert_flip_string concurrently, this may lead to crashes when
    concurrect writers call pty_write. In that case the following two
    writers:
    * the ECHOing from a workqueue and
    * pty_write from the process
    race and can overflow the corresponding TTY buffer like follows.
    
    If we look into tty_insert_flip_string_fixed_flag, there is:
      int space = __tty_buffer_request_room(port, goal, flags);
      struct tty_buffer *tb = port->buf.tail;
      ...
      memcpy(char_buf_ptr(tb, tb->used), chars, space);
      ...
      tb->used += space;
    
    so the race of the two can result in something like this:
                  A                                B
    __tty_buffer_request_room
                                      __tty_buffer_request_room
    memcpy(buf(tb->used), ...)
    tb->used += space;
                                      memcpy(buf(tb->used), ...) ->BOOM
    
    B's memcpy is past the tty_buffer due to the previous A's tb->used
    increment.
    
    Since the N_TTY line discipline input processing can output
    concurrently with a tty write, obtain the N_TTY ldisc output_lock to
    serialize echo output with normal tty writes.  This ensures the tty
    buffer helper tty_insert_flip_string is not called concurrently and
    everything is fine.
    
    Note that this is nicely reproducible by an ordinary user using
    forkpty and some setup around that (raw termios + ECHO). And it is
    present in kernels at least after commit
    d945cb9cce20ac7143c2de8d88b187f62db99bdc (pty: Rework the pty layer to
    use the normal buffering logic) in 2.6.31-rc3.
    
    js: add more info to the commit log
    js: switch to bool
    js: lock unconditionally
    js: lock only the tty->ops->write call
    
    References: CVE-2014-0196
    Reported-and-tested-by: Jiri Slaby <jslaby@suse.cz>
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit 9d4e7a24d77a05fb5c4e4121051a8d80501c74d3
Author: Cho KyongHo <pullip.cho@samsung.com>
Date:   Mon May 12 11:44:57 2014 +0530

    iommu/exynos: Change rwlock to spinlock
    
    Since acquiring read_lock is not more frequent than write_lock, it is
    not beneficial to use rwlock, this commit changes rwlock to spinlock.
    
    Reviewed-by: Grant Grundler <grundler@chromium.org>
    Signed-off-by: Cho KyongHo <pullip.cho@samsung.com>
    Signed-off-by: Shaik Ameer Basha <shaik.ameer@samsung.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

commit abb5100737bba3f82b5514350fea89ca361ac66c
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Sat May 3 14:04:59 2014 +0200

    n_tty: Fix n_tty_write crash when echoing in raw mode
    
    commit 4291086b1f081b869c6d79e5b7441633dc3ace00 upstream.
    
    The tty atomic_write_lock does not provide an exclusion guarantee for
    the tty driver if the termios settings are LECHO & !OPOST.  And since
    it is unexpected and not allowed to call TTY buffer helpers like
    tty_insert_flip_string concurrently, this may lead to crashes when
    concurrect writers call pty_write. In that case the following two
    writers:
    * the ECHOing from a workqueue and
    * pty_write from the process
    race and can overflow the corresponding TTY buffer like follows.
    
    If we look into tty_insert_flip_string_fixed_flag, there is:
      int space = __tty_buffer_request_room(port, goal, flags);
      struct tty_buffer *tb = port->buf.tail;
      ...
      memcpy(char_buf_ptr(tb, tb->used), chars, space);
      ...
      tb->used += space;
    
    so the race of the two can result in something like this:
                  A                                B
    __tty_buffer_request_room
                                      __tty_buffer_request_room
    memcpy(buf(tb->used), ...)
    tb->used += space;
                                      memcpy(buf(tb->used), ...) ->BOOM
    
    B's memcpy is past the tty_buffer due to the previous A's tb->used
    increment.
    
    Since the N_TTY line discipline input processing can output
    concurrently with a tty write, obtain the N_TTY ldisc output_lock to
    serialize echo output with normal tty writes.  This ensures the tty
    buffer helper tty_insert_flip_string is not called concurrently and
    everything is fine.
    
    Note that this is nicely reproducible by an ordinary user using
    forkpty and some setup around that (raw termios + ECHO). And it is
    present in kernels at least after commit
    d945cb9cce20ac7143c2de8d88b187f62db99bdc (pty: Rework the pty layer to
    use the normal buffering logic) in 2.6.31-rc3.
    
    js: add more info to the commit log
    js: switch to bool
    js: lock unconditionally
    js: lock only the tty->ops->write call
    
    References: CVE-2014-0196
    Reported-and-tested-by: Jiri Slaby <jslaby@suse.cz>
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6d194d6e0d832e622d87b8dab4e2122c555a82e9
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Sat May 3 14:04:59 2014 +0200

    n_tty: Fix n_tty_write crash when echoing in raw mode
    
    commit 4291086b1f081b869c6d79e5b7441633dc3ace00 upstream.
    
    The tty atomic_write_lock does not provide an exclusion guarantee for
    the tty driver if the termios settings are LECHO & !OPOST.  And since
    it is unexpected and not allowed to call TTY buffer helpers like
    tty_insert_flip_string concurrently, this may lead to crashes when
    concurrect writers call pty_write. In that case the following two
    writers:
    * the ECHOing from a workqueue and
    * pty_write from the process
    race and can overflow the corresponding TTY buffer like follows.
    
    If we look into tty_insert_flip_string_fixed_flag, there is:
      int space = __tty_buffer_request_room(port, goal, flags);
      struct tty_buffer *tb = port->buf.tail;
      ...
      memcpy(char_buf_ptr(tb, tb->used), chars, space);
      ...
      tb->used += space;
    
    so the race of the two can result in something like this:
                  A                                B
    __tty_buffer_request_room
                                      __tty_buffer_request_room
    memcpy(buf(tb->used), ...)
    tb->used += space;
                                      memcpy(buf(tb->used), ...) ->BOOM
    
    B's memcpy is past the tty_buffer due to the previous A's tb->used
    increment.
    
    Since the N_TTY line discipline input processing can output
    concurrently with a tty write, obtain the N_TTY ldisc output_lock to
    serialize echo output with normal tty writes.  This ensures the tty
    buffer helper tty_insert_flip_string is not called concurrently and
    everything is fine.
    
    Note that this is nicely reproducible by an ordinary user using
    forkpty and some setup around that (raw termios + ECHO). And it is
    present in kernels at least after commit
    d945cb9cce20ac7143c2de8d88b187f62db99bdc (pty: Rework the pty layer to
    use the normal buffering logic) in 2.6.31-rc3.
    
    js: add more info to the commit log
    js: switch to bool
    js: lock unconditionally
    js: lock only the tty->ops->write call
    
    References: CVE-2014-0196
    Reported-and-tested-by: Jiri Slaby <jslaby@suse.cz>
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4291086b1f081b869c6d79e5b7441633dc3ace00
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Sat May 3 14:04:59 2014 +0200

    n_tty: Fix n_tty_write crash when echoing in raw mode
    
    The tty atomic_write_lock does not provide an exclusion guarantee for
    the tty driver if the termios settings are LECHO & !OPOST.  And since
    it is unexpected and not allowed to call TTY buffer helpers like
    tty_insert_flip_string concurrently, this may lead to crashes when
    concurrect writers call pty_write. In that case the following two
    writers:
    * the ECHOing from a workqueue and
    * pty_write from the process
    race and can overflow the corresponding TTY buffer like follows.
    
    If we look into tty_insert_flip_string_fixed_flag, there is:
      int space = __tty_buffer_request_room(port, goal, flags);
      struct tty_buffer *tb = port->buf.tail;
      ...
      memcpy(char_buf_ptr(tb, tb->used), chars, space);
      ...
      tb->used += space;
    
    so the race of the two can result in something like this:
                  A                                B
    __tty_buffer_request_room
                                      __tty_buffer_request_room
    memcpy(buf(tb->used), ...)
    tb->used += space;
                                      memcpy(buf(tb->used), ...) ->BOOM
    
    B's memcpy is past the tty_buffer due to the previous A's tb->used
    increment.
    
    Since the N_TTY line discipline input processing can output
    concurrently with a tty write, obtain the N_TTY ldisc output_lock to
    serialize echo output with normal tty writes.  This ensures the tty
    buffer helper tty_insert_flip_string is not called concurrently and
    everything is fine.
    
    Note that this is nicely reproducible by an ordinary user using
    forkpty and some setup around that (raw termios + ECHO). And it is
    present in kernels at least after commit
    d945cb9cce20ac7143c2de8d88b187f62db99bdc (pty: Rework the pty layer to
    use the normal buffering logic) in 2.6.31-rc3.
    
    js: add more info to the commit log
    js: switch to bool
    js: lock unconditionally
    js: lock only the tty->ops->write call
    
    References: CVE-2014-0196
    Reported-and-tested-by: Jiri Slaby <jslaby@suse.cz>
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 56b30770b27d54d68ad51eccc6d888282b568cee
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Thu Jan 23 01:44:55 2014 -0800

    bcache: Kill btree_io_wq
    
    With the locking rework in the last patch, this shouldn't be needed anymore -
    btree_node_write_work() only takes b->write_lock which is never held for very
    long.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

commit 2a285686c109816ba71a00b9278262cf02648258
Author: Kent Overstreet <kmo@daterainc.com>
Date:   Tue Mar 4 16:42:42 2014 -0800

    bcache: btree locking rework
    
    Add a new lock, b->write_lock, which is required to actually modify - or write -
    a btree node; this lock is only held for short durations.
    
    This means we can write out a btree node without taking b->lock, which _is_ held
    for long durations - solving a deadlock when btree_flush_write() (from the
    journalling code) is called with a btree node locked.
    
    Right now just occurs in bch_btree_set_root(), but with an upcoming journalling
    rework is going to happen a lot more.
    
    This also turns b->lock is now more of a read/intent lock instead of a
    read/write lock - but not completely, since it still blocks readers. May turn it
    into a real intent lock at some point in the future.
    
    Signed-off-by: Kent Overstreet <kmo@daterainc.com>

commit bd39784b505cd491d5ec8641f57fe35aeec32b4b
Author: Larry Finger <Larry.Finger@lwfinger.net>
Date:   Wed Sep 26 12:32:02 2012 -0500

    b43legacy: Fix crash on unload when firmware not available
    
    commit 2d838bb608e2d1f6cb4280e76748cb812dc822e7 upstream.
    
    When b43legacy is loaded without the firmware being available, a following
    unload generates a kernel NULL pointer dereference BUG as follows:
    
    [  214.330789] BUG: unable to handle kernel NULL pointer dereference at 0000004c
    [  214.330997] IP: [<c104c395>] drain_workqueue+0x15/0x170
    [  214.331179] *pde = 00000000
    [  214.331311] Oops: 0000 [#1] SMP
    [  214.331471] Modules linked in: b43legacy(-) ssb pcmcia mac80211 cfg80211 af_packet mperf arc4 ppdev sr_mod cdrom sg shpchp yenta_socket pcmcia_rsrc pci_hotplug pcmcia_core battery parport_pc parport floppy container ac button edd autofs4 ohci_hcd ehci_hcd usbcore usb_common thermal processor scsi_dh_rdac scsi_dh_hp_sw scsi_dh_emc scsi_dh_alua scsi_dh fan thermal_sys hwmon ata_generic pata_ali libata [last unloaded: cfg80211]
    [  214.333421] Pid: 3639, comm: modprobe Not tainted 3.6.0-rc6-wl+ #163 Source Technology VIC 9921/ALI Based Notebook
    [  214.333580] EIP: 0060:[<c104c395>] EFLAGS: 00010246 CPU: 0
    [  214.333687] EIP is at drain_workqueue+0x15/0x170
    [  214.333788] EAX: c162ac40 EBX: cdfb8360 ECX: 0000002a EDX: 00002a2a
    [  214.333890] ESI: 00000000 EDI: 00000000 EBP: cd767e7c ESP: cd767e5c
    [  214.333957]  DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
    [  214.333957] CR0: 8005003b CR2: 0000004c CR3: 0c96a000 CR4: 00000090
    [  214.333957] DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
    [  214.333957] DR6: ffff0ff0 DR7: 00000400
    [  214.333957] Process modprobe (pid: 3639, ti=cd766000 task=cf802e90 task.ti=cd766000)
    [  214.333957] Stack:
    [  214.333957]  00000292 cd767e74 c12c5e09 00000296 00000296 cdfb8360 cdfb9220 00000000
    [  214.333957]  cd767e90 c104c4fd cdfb8360 cdfb9220 cd682800 cd767ea4 d0c10184 cd682800
    [  214.333957]  cd767ea4 cba31064 cd767eb8 d0867908 cba31064 d087e09c cd96f034 cd767ec4
    [  214.333957] Call Trace:
    [  214.333957]  [<c12c5e09>] ? skb_dequeue+0x49/0x60
    [  214.333957]  [<c104c4fd>] destroy_workqueue+0xd/0x150
    [  214.333957]  [<d0c10184>] ieee80211_unregister_hw+0xc4/0x100 [mac80211]
    [  214.333957]  [<d0867908>] b43legacy_remove+0x78/0x80 [b43legacy]
    [  214.333957]  [<d083654d>] ssb_device_remove+0x1d/0x30 [ssb]
    [  214.333957]  [<c126f15a>] __device_release_driver+0x5a/0xb0
    [  214.333957]  [<c126fb07>] driver_detach+0x87/0x90
    [  214.333957]  [<c126ef4c>] bus_remove_driver+0x6c/0xe0
    [  214.333957]  [<c1270120>] driver_unregister+0x40/0x70
    [  214.333957]  [<d083686b>] ssb_driver_unregister+0xb/0x10 [ssb]
    [  214.333957]  [<d087c488>] b43legacy_exit+0xd/0xf [b43legacy]
    [  214.333957]  [<c1089dde>] sys_delete_module+0x14e/0x2b0
    [  214.333957]  [<c110a4a7>] ? vfs_write+0xf7/0x150
    [  214.333957]  [<c1240050>] ? tty_write_lock+0x50/0x50
    [  214.333957]  [<c110a6f8>] ? sys_write+0x38/0x70
    [  214.333957]  [<c1397c55>] syscall_call+0x7/0xb
    [  214.333957] Code: bc 27 00 00 00 00 a1 74 61 56 c1 55 89 e5 e8 a3 fc ff ff 5d c3 90 55 89 e5 57 56 89 c6 53 b8 40 ac 62 c1 83 ec 14 e8 bb b7 34 00 <8b> 46 4c 8d 50 01 85 c0 89 56 4c 75 03 83 0e 40 80 05 40 ac 62
    [  214.333957] EIP: [<c104c395>] drain_workqueue+0x15/0x170 SS:ESP 0068:cd767e5c
    [  214.333957] CR2: 000000000000004c
    [  214.341110] ---[ end trace c7e90ec026d875a6 ]---Index: wireless-testing/drivers/net/wireless/b43legacy/main.c
    
    The problem is fixed by making certain that the ucode pointer is not NULL
    before deregistering the driver in mac80211.
    
    Signed-off-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

commit 52028704126b5597775cc788028385556af1f85c
Author: Rashika Kheria <rashika.kheria@gmail.com>
Date:   Mon Jan 6 22:15:56 2014 +0530

    drivers: gpu: Remove unused function in ttm_lock.c
    
    Remove unused function ttm_write_lock_downgrade() from
    drm/ttm/ttm_lock.c.
    
    This eliminates the following warning in drm/ttm/ttm_lock.c:
    drivers/gpu/drm/ttm/ttm_lock.c:189:6: warning: no previous prototype for ‘ttm_write_lock_downgrade’ [-Wmissing-prototypes]
    
    Signed-off-by: Rashika Kheria <rashika.kheria@gmail.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Reviewed-by: Thomas Hellstrom <thellstrom@vmware.com>

commit 721a769c034204e8e5f6ca4ecbff249e0225333b
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Mon Sep 23 16:50:42 2013 -0400

    reiserfs: fix race with flush_used_journal_lists and flush_journal_list
    
    There are two locks involved in managing the journal lists. The general
    reiserfs_write_lock and the journal->j_flush_mutex.
    
    While flush_journal_list is sleeping to acquire the j_flush_mutex or to
    submit a block for write, it will drop the write lock. This allows
    another thread to acquire the write lock and ultimately call
    flush_used_journal_lists to traverse the list of journal lists and
    select one for flushing. It can select the journal_list that has just
    had flush_journal_list called on it in the original thread and call it
    again with the same journal_list.
    
    The second thread then drops the write lock to acquire j_flush_mutex and
    the first thread reacquires it and continues execution and eventually
    clears and frees the journal list before dropping j_flush_mutex and
    returning.
    
    The second thread acquires j_flush_mutex and ends up operating on a
    journal_list that has already been released. If the memory hasn't
    been reused, we'll soon after hit a BUG_ON because the transaction id
    has already been cleared. If it's been reused, we'll crash in other
    fun ways.
    
    Since flush_journal_list will synchronize on j_flush_mutex, we can fix
    the race by taking a proper reference in flush_used_journal_lists
    and checking to see if it's still valid after the mutex is taken. It's
    safe to iterate the list of journal lists and pick a list with
    just the write lock as long as a reference is taken on the journal list
    before we drop the lock. We already have code to handle whether a
    transaction has been flushed already so we can use that to handle the
    race and get rid of the trans_id BUG_ON.
    
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: Jan Kara <jack@suse.cz>

commit feded5077ba27265ce2f317eeb89dbed64674fed
Author: Roland Dreier <roland@purestorage.com>
Date:   Mon Aug 5 17:55:01 2013 -0700

    sg: Fix user memory corruption when SG_IO is interrupted by a signal
    
    commit 35dc248383bbab0a7203fca4d722875bc81ef091 upstream.
    
    There is a nasty bug in the SCSI SG_IO ioctl that in some circumstances
    leads to one process writing data into the address space of some other
    random unrelated process if the ioctl is interrupted by a signal.
    What happens is the following:
    
     - A process issues an SG_IO ioctl with direction DXFER_FROM_DEV (ie the
       underlying SCSI command will transfer data from the SCSI device to
       the buffer provided in the ioctl)
    
     - Before the command finishes, a signal is sent to the process waiting
       in the ioctl.  This will end up waking up the sg_ioctl() code:
    
                    result = wait_event_interruptible(sfp->read_wait,
                            (srp_done(sfp, srp) || sdp->detached));
    
       but neither srp_done() nor sdp->detached is true, so we end up just
       setting srp->orphan and returning to userspace:
    
                    srp->orphan = 1;
                    write_unlock_irq(&sfp->rq_list_lock);
                    return result;  /* -ERESTARTSYS because signal hit process */
    
       At this point the original process is done with the ioctl and
       blithely goes ahead handling the signal, reissuing the ioctl, etc.
    
     - Eventually, the SCSI command issued by the first ioctl finishes and
       ends up in sg_rq_end_io().  At the end of that function, we run through:
    
            write_lock_irqsave(&sfp->rq_list_lock, iflags);
            if (unlikely(srp->orphan)) {
                    if (sfp->keep_orphan)
                            srp->sg_io_owned = 0;
                    else
                            done = 0;
            }
            srp->done = done;
            write_unlock_irqrestore(&sfp->rq_list_lock, iflags);
    
            if (likely(done)) {
                    /* Now wake up any sg_read() that is waiting for this
                     * packet.
                     */
                    wake_up_interruptible(&sfp->read_wait);
                    kill_fasync(&sfp->async_qp, SIGPOLL, POLL_IN);
                    kref_put(&sfp->f_ref, sg_remove_sfp);
            } else {
                    INIT_WORK(&srp->ew.work, sg_rq_end_io_usercontext);
                    schedule_work(&srp->ew.work);
            }
    
       Since srp->orphan *is* set, we set done to 0 (assuming the
       userspace app has not set keep_orphan via an SG_SET_KEEP_ORPHAN
       ioctl), and therefore we end up scheduling sg_rq_end_io_usercontext()
       to run in a workqueue.
    
     - In workqueue context we go through sg_rq_end_io_usercontext() ->
       sg_finish_rem_req() -> blk_rq_unmap_user() -> ... ->
       bio_uncopy_user() -> __bio_copy_iov() -> copy_to_user().
    
       The key point here is that we are doing copy_to_user() on a
       workqueue -- that is, we're on a kernel thread with current->mm
       equal to whatever random previous user process was scheduled before
       this kernel thread.  So we end up copying whatever data the SCSI
       command returned to the virtual address of the buffer passed into
       the original ioctl, but it's quite likely we do this copying into a
       different address space!
    
    As suggested by James Bottomley <James.Bottomley@hansenpartnership.com>,
    add a check for current->mm (which is NULL if we're on a kernel thread
    without a real userspace address space) in bio_uncopy_user(), and skip
    the copy if we're on a kernel thread.
    
    There's no reason that I can think of for any caller of bio_uncopy_user()
    to want to do copying on a kernel thread with a random active userspace
    address space.
    
    Huge thanks to Costa Sapuntzakis <costa@purestorage.com> for the
    original pointer to this bug in the sg code.
    
    Signed-off-by: Roland Dreier <roland@purestorage.com>
    Tested-by: David Milburn <dmilburn@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: James Bottomley <JBottomley@Parallels.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit d3ba21877b9488abffd4528aa4b316ec5af27ea3
Author: Roland Dreier <roland@purestorage.com>
Date:   Mon Aug 5 17:55:01 2013 -0700

    SCSI: sg: Fix user memory corruption when SG_IO is interrupted by a signal
    
    commit 35dc248383bbab0a7203fca4d722875bc81ef091 upstream.
    
    There is a nasty bug in the SCSI SG_IO ioctl that in some circumstances
    leads to one process writing data into the address space of some other
    random unrelated process if the ioctl is interrupted by a signal.
    What happens is the following:
    
     - A process issues an SG_IO ioctl with direction DXFER_FROM_DEV (ie the
       underlying SCSI command will transfer data from the SCSI device to
       the buffer provided in the ioctl)
    
     - Before the command finishes, a signal is sent to the process waiting
       in the ioctl.  This will end up waking up the sg_ioctl() code:
    
                    result = wait_event_interruptible(sfp->read_wait,
                            (srp_done(sfp, srp) || sdp->detached));
    
       but neither srp_done() nor sdp->detached is true, so we end up just
       setting srp->orphan and returning to userspace:
    
                    srp->orphan = 1;
                    write_unlock_irq(&sfp->rq_list_lock);
                    return result;  /* -ERESTARTSYS because signal hit process */
    
       At this point the original process is done with the ioctl and
       blithely goes ahead handling the signal, reissuing the ioctl, etc.
    
     - Eventually, the SCSI command issued by the first ioctl finishes and
       ends up in sg_rq_end_io().  At the end of that function, we run through:
    
            write_lock_irqsave(&sfp->rq_list_lock, iflags);
            if (unlikely(srp->orphan)) {
                    if (sfp->keep_orphan)
                            srp->sg_io_owned = 0;
                    else
                            done = 0;
            }
            srp->done = done;
            write_unlock_irqrestore(&sfp->rq_list_lock, iflags);
    
            if (likely(done)) {
                    /* Now wake up any sg_read() that is waiting for this
                     * packet.
                     */
                    wake_up_interruptible(&sfp->read_wait);
                    kill_fasync(&sfp->async_qp, SIGPOLL, POLL_IN);
                    kref_put(&sfp->f_ref, sg_remove_sfp);
            } else {
                    INIT_WORK(&srp->ew.work, sg_rq_end_io_usercontext);
                    schedule_work(&srp->ew.work);
            }
    
       Since srp->orphan *is* set, we set done to 0 (assuming the
       userspace app has not set keep_orphan via an SG_SET_KEEP_ORPHAN
       ioctl), and therefore we end up scheduling sg_rq_end_io_usercontext()
       to run in a workqueue.
    
     - In workqueue context we go through sg_rq_end_io_usercontext() ->
       sg_finish_rem_req() -> blk_rq_unmap_user() -> ... ->
       bio_uncopy_user() -> __bio_copy_iov() -> copy_to_user().
    
       The key point here is that we are doing copy_to_user() on a
       workqueue -- that is, we're on a kernel thread with current->mm
       equal to whatever random previous user process was scheduled before
       this kernel thread.  So we end up copying whatever data the SCSI
       command returned to the virtual address of the buffer passed into
       the original ioctl, but it's quite likely we do this copying into a
       different address space!
    
    As suggested by James Bottomley <James.Bottomley@hansenpartnership.com>,
    add a check for current->mm (which is NULL if we're on a kernel thread
    without a real userspace address space) in bio_uncopy_user(), and skip
    the copy if we're on a kernel thread.
    
    There's no reason that I can think of for any caller of bio_uncopy_user()
    to want to do copying on a kernel thread with a random active userspace
    address space.
    
    Huge thanks to Costa Sapuntzakis <costa@purestorage.com> for the
    original pointer to this bug in the sg code.
    
    Signed-off-by: Roland Dreier <roland@purestorage.com>
    Tested-by: David Milburn <dmilburn@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: James Bottomley <JBottomley@Parallels.com>
    [lizf: backported to 3.4:
     - Use __bio_for_each_segment() instead of bio_for_each_segment_all()]
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit eb18ce5b78b1efb313a14532d2883420163e681a
Author: Roland Dreier <roland@purestorage.com>
Date:   Mon Aug 5 17:55:01 2013 -0700

    SCSI: sg: Fix user memory corruption when SG_IO is interrupted by a signal
    
    commit 35dc248383bbab0a7203fca4d722875bc81ef091 upstream.
    
    There is a nasty bug in the SCSI SG_IO ioctl that in some circumstances
    leads to one process writing data into the address space of some other
    random unrelated process if the ioctl is interrupted by a signal.
    What happens is the following:
    
     - A process issues an SG_IO ioctl with direction DXFER_FROM_DEV (ie the
       underlying SCSI command will transfer data from the SCSI device to
       the buffer provided in the ioctl)
    
     - Before the command finishes, a signal is sent to the process waiting
       in the ioctl.  This will end up waking up the sg_ioctl() code:
    
                    result = wait_event_interruptible(sfp->read_wait,
                            (srp_done(sfp, srp) || sdp->detached));
    
       but neither srp_done() nor sdp->detached is true, so we end up just
       setting srp->orphan and returning to userspace:
    
                    srp->orphan = 1;
                    write_unlock_irq(&sfp->rq_list_lock);
                    return result;  /* -ERESTARTSYS because signal hit process */
    
       At this point the original process is done with the ioctl and
       blithely goes ahead handling the signal, reissuing the ioctl, etc.
    
     - Eventually, the SCSI command issued by the first ioctl finishes and
       ends up in sg_rq_end_io().  At the end of that function, we run through:
    
            write_lock_irqsave(&sfp->rq_list_lock, iflags);
            if (unlikely(srp->orphan)) {
                    if (sfp->keep_orphan)
                            srp->sg_io_owned = 0;
                    else
                            done = 0;
            }
            srp->done = done;
            write_unlock_irqrestore(&sfp->rq_list_lock, iflags);
    
            if (likely(done)) {
                    /* Now wake up any sg_read() that is waiting for this
                     * packet.
                     */
                    wake_up_interruptible(&sfp->read_wait);
                    kill_fasync(&sfp->async_qp, SIGPOLL, POLL_IN);
                    kref_put(&sfp->f_ref, sg_remove_sfp);
            } else {
                    INIT_WORK(&srp->ew.work, sg_rq_end_io_usercontext);
                    schedule_work(&srp->ew.work);
            }
    
       Since srp->orphan *is* set, we set done to 0 (assuming the
       userspace app has not set keep_orphan via an SG_SET_KEEP_ORPHAN
       ioctl), and therefore we end up scheduling sg_rq_end_io_usercontext()
       to run in a workqueue.
    
     - In workqueue context we go through sg_rq_end_io_usercontext() ->
       sg_finish_rem_req() -> blk_rq_unmap_user() -> ... ->
       bio_uncopy_user() -> __bio_copy_iov() -> copy_to_user().
    
       The key point here is that we are doing copy_to_user() on a
       workqueue -- that is, we're on a kernel thread with current->mm
       equal to whatever random previous user process was scheduled before
       this kernel thread.  So we end up copying whatever data the SCSI
       command returned to the virtual address of the buffer passed into
       the original ioctl, but it's quite likely we do this copying into a
       different address space!
    
    As suggested by James Bottomley <James.Bottomley@hansenpartnership.com>,
    add a check for current->mm (which is NULL if we're on a kernel thread
    without a real userspace address space) in bio_uncopy_user(), and skip
    the copy if we're on a kernel thread.
    
    There's no reason that I can think of for any caller of bio_uncopy_user()
    to want to do copying on a kernel thread with a random active userspace
    address space.
    
    Huge thanks to Costa Sapuntzakis <costa@purestorage.com> for the
    original pointer to this bug in the sg code.
    
    Signed-off-by: Roland Dreier <roland@purestorage.com>
    Tested-by: David Milburn <dmilburn@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: James Bottomley <JBottomley@Parallels.com>
    [lizf: backported to 3.4:
     - Use __bio_for_each_segment() instead of bio_for_each_segment_all()]
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 32b8d5f874e1c6ca88ec4f2d10cd885b3d70cf17
Author: Roland Dreier <roland@purestorage.com>
Date:   Mon Aug 5 17:55:01 2013 -0700

    SCSI: sg: Fix user memory corruption when SG_IO is interrupted by a signal
    
    commit 35dc248383bbab0a7203fca4d722875bc81ef091 upstream.
    
    There is a nasty bug in the SCSI SG_IO ioctl that in some circumstances
    leads to one process writing data into the address space of some other
    random unrelated process if the ioctl is interrupted by a signal.
    What happens is the following:
    
     - A process issues an SG_IO ioctl with direction DXFER_FROM_DEV (ie the
       underlying SCSI command will transfer data from the SCSI device to
       the buffer provided in the ioctl)
    
     - Before the command finishes, a signal is sent to the process waiting
       in the ioctl.  This will end up waking up the sg_ioctl() code:
    
                    result = wait_event_interruptible(sfp->read_wait,
                            (srp_done(sfp, srp) || sdp->detached));
    
       but neither srp_done() nor sdp->detached is true, so we end up just
       setting srp->orphan and returning to userspace:
    
                    srp->orphan = 1;
                    write_unlock_irq(&sfp->rq_list_lock);
                    return result;  /* -ERESTARTSYS because signal hit process */
    
       At this point the original process is done with the ioctl and
       blithely goes ahead handling the signal, reissuing the ioctl, etc.
    
     - Eventually, the SCSI command issued by the first ioctl finishes and
       ends up in sg_rq_end_io().  At the end of that function, we run through:
    
            write_lock_irqsave(&sfp->rq_list_lock, iflags);
            if (unlikely(srp->orphan)) {
                    if (sfp->keep_orphan)
                            srp->sg_io_owned = 0;
                    else
                            done = 0;
            }
            srp->done = done;
            write_unlock_irqrestore(&sfp->rq_list_lock, iflags);
    
            if (likely(done)) {
                    /* Now wake up any sg_read() that is waiting for this
                     * packet.
                     */
                    wake_up_interruptible(&sfp->read_wait);
                    kill_fasync(&sfp->async_qp, SIGPOLL, POLL_IN);
                    kref_put(&sfp->f_ref, sg_remove_sfp);
            } else {
                    INIT_WORK(&srp->ew.work, sg_rq_end_io_usercontext);
                    schedule_work(&srp->ew.work);
            }
    
       Since srp->orphan *is* set, we set done to 0 (assuming the
       userspace app has not set keep_orphan via an SG_SET_KEEP_ORPHAN
       ioctl), and therefore we end up scheduling sg_rq_end_io_usercontext()
       to run in a workqueue.
    
     - In workqueue context we go through sg_rq_end_io_usercontext() ->
       sg_finish_rem_req() -> blk_rq_unmap_user() -> ... ->
       bio_uncopy_user() -> __bio_copy_iov() -> copy_to_user().
    
       The key point here is that we are doing copy_to_user() on a
       workqueue -- that is, we're on a kernel thread with current->mm
       equal to whatever random previous user process was scheduled before
       this kernel thread.  So we end up copying whatever data the SCSI
       command returned to the virtual address of the buffer passed into
       the original ioctl, but it's quite likely we do this copying into a
       different address space!
    
    As suggested by James Bottomley <James.Bottomley@hansenpartnership.com>,
    add a check for current->mm (which is NULL if we're on a kernel thread
    without a real userspace address space) in bio_uncopy_user(), and skip
    the copy if we're on a kernel thread.
    
    There's no reason that I can think of for any caller of bio_uncopy_user()
    to want to do copying on a kernel thread with a random active userspace
    address space.
    
    Huge thanks to Costa Sapuntzakis <costa@purestorage.com> for the
    original pointer to this bug in the sg code.
    
    Signed-off-by: Roland Dreier <roland@purestorage.com>
    Tested-by: David Milburn <dmilburn@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: James Bottomley <JBottomley@Parallels.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 35dc248383bbab0a7203fca4d722875bc81ef091
Author: Roland Dreier <roland@purestorage.com>
Date:   Mon Aug 5 17:55:01 2013 -0700

    [SCSI] sg: Fix user memory corruption when SG_IO is interrupted by a signal
    
    There is a nasty bug in the SCSI SG_IO ioctl that in some circumstances
    leads to one process writing data into the address space of some other
    random unrelated process if the ioctl is interrupted by a signal.
    What happens is the following:
    
     - A process issues an SG_IO ioctl with direction DXFER_FROM_DEV (ie the
       underlying SCSI command will transfer data from the SCSI device to
       the buffer provided in the ioctl)
    
     - Before the command finishes, a signal is sent to the process waiting
       in the ioctl.  This will end up waking up the sg_ioctl() code:
    
                    result = wait_event_interruptible(sfp->read_wait,
                            (srp_done(sfp, srp) || sdp->detached));
    
       but neither srp_done() nor sdp->detached is true, so we end up just
       setting srp->orphan and returning to userspace:
    
                    srp->orphan = 1;
                    write_unlock_irq(&sfp->rq_list_lock);
                    return result;  /* -ERESTARTSYS because signal hit process */
    
       At this point the original process is done with the ioctl and
       blithely goes ahead handling the signal, reissuing the ioctl, etc.
    
     - Eventually, the SCSI command issued by the first ioctl finishes and
       ends up in sg_rq_end_io().  At the end of that function, we run through:
    
            write_lock_irqsave(&sfp->rq_list_lock, iflags);
            if (unlikely(srp->orphan)) {
                    if (sfp->keep_orphan)
                            srp->sg_io_owned = 0;
                    else
                            done = 0;
            }
            srp->done = done;
            write_unlock_irqrestore(&sfp->rq_list_lock, iflags);
    
            if (likely(done)) {
                    /* Now wake up any sg_read() that is waiting for this
                     * packet.
                     */
                    wake_up_interruptible(&sfp->read_wait);
                    kill_fasync(&sfp->async_qp, SIGPOLL, POLL_IN);
                    kref_put(&sfp->f_ref, sg_remove_sfp);
            } else {
                    INIT_WORK(&srp->ew.work, sg_rq_end_io_usercontext);
                    schedule_work(&srp->ew.work);
            }
    
       Since srp->orphan *is* set, we set done to 0 (assuming the
       userspace app has not set keep_orphan via an SG_SET_KEEP_ORPHAN
       ioctl), and therefore we end up scheduling sg_rq_end_io_usercontext()
       to run in a workqueue.
    
     - In workqueue context we go through sg_rq_end_io_usercontext() ->
       sg_finish_rem_req() -> blk_rq_unmap_user() -> ... ->
       bio_uncopy_user() -> __bio_copy_iov() -> copy_to_user().
    
       The key point here is that we are doing copy_to_user() on a
       workqueue -- that is, we're on a kernel thread with current->mm
       equal to whatever random previous user process was scheduled before
       this kernel thread.  So we end up copying whatever data the SCSI
       command returned to the virtual address of the buffer passed into
       the original ioctl, but it's quite likely we do this copying into a
       different address space!
    
    As suggested by James Bottomley <James.Bottomley@hansenpartnership.com>,
    add a check for current->mm (which is NULL if we're on a kernel thread
    without a real userspace address space) in bio_uncopy_user(), and skip
    the copy if we're on a kernel thread.
    
    There's no reason that I can think of for any caller of bio_uncopy_user()
    to want to do copying on a kernel thread with a random active userspace
    address space.
    
    Huge thanks to Costa Sapuntzakis <costa@purestorage.com> for the
    original pointer to this bug in the sg code.
    
    Signed-off-by: Roland Dreier <roland@purestorage.com>
    Tested-by: David Milburn <dmilburn@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: James Bottomley <JBottomley@Parallels.com>

commit e12eeedd93b12913386430ead7dc6b2b948ce31f
Author: Larry Finger <Larry.Finger@lwfinger.net>
Date:   Wed Sep 26 12:32:02 2012 -0500

    b43legacy: Fix crash on unload when firmware not available
    
    commit 2d838bb608e2d1f6cb4280e76748cb812dc822e7 upstream.
    
    When b43legacy is loaded without the firmware being available, a following
    unload generates a kernel NULL pointer dereference BUG as follows:
    
    [  214.330789] BUG: unable to handle kernel NULL pointer dereference at 0000004c
    [  214.330997] IP: [<c104c395>] drain_workqueue+0x15/0x170
    [  214.331179] *pde = 00000000
    [  214.331311] Oops: 0000 [#1] SMP
    [  214.331471] Modules linked in: b43legacy(-) ssb pcmcia mac80211 cfg80211 af_packet mperf arc4 ppdev sr_mod cdrom sg shpchp yenta_socket pcmcia_rsrc pci_hotplug pcmcia_core battery parport_pc parport floppy container ac button edd autofs4 ohci_hcd ehci_hcd usbcore usb_common thermal processor scsi_dh_rdac scsi_dh_hp_sw scsi_dh_emc scsi_dh_alua scsi_dh fan thermal_sys hwmon ata_generic pata_ali libata [last unloaded: cfg80211]
    [  214.333421] Pid: 3639, comm: modprobe Not tainted 3.6.0-rc6-wl+ #163 Source Technology VIC 9921/ALI Based Notebook
    [  214.333580] EIP: 0060:[<c104c395>] EFLAGS: 00010246 CPU: 0
    [  214.333687] EIP is at drain_workqueue+0x15/0x170
    [  214.333788] EAX: c162ac40 EBX: cdfb8360 ECX: 0000002a EDX: 00002a2a
    [  214.333890] ESI: 00000000 EDI: 00000000 EBP: cd767e7c ESP: cd767e5c
    [  214.333957]  DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
    [  214.333957] CR0: 8005003b CR2: 0000004c CR3: 0c96a000 CR4: 00000090
    [  214.333957] DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
    [  214.333957] DR6: ffff0ff0 DR7: 00000400
    [  214.333957] Process modprobe (pid: 3639, ti=cd766000 task=cf802e90 task.ti=cd766000)
    [  214.333957] Stack:
    [  214.333957]  00000292 cd767e74 c12c5e09 00000296 00000296 cdfb8360 cdfb9220 00000000
    [  214.333957]  cd767e90 c104c4fd cdfb8360 cdfb9220 cd682800 cd767ea4 d0c10184 cd682800
    [  214.333957]  cd767ea4 cba31064 cd767eb8 d0867908 cba31064 d087e09c cd96f034 cd767ec4
    [  214.333957] Call Trace:
    [  214.333957]  [<c12c5e09>] ? skb_dequeue+0x49/0x60
    [  214.333957]  [<c104c4fd>] destroy_workqueue+0xd/0x150
    [  214.333957]  [<d0c10184>] ieee80211_unregister_hw+0xc4/0x100 [mac80211]
    [  214.333957]  [<d0867908>] b43legacy_remove+0x78/0x80 [b43legacy]
    [  214.333957]  [<d083654d>] ssb_device_remove+0x1d/0x30 [ssb]
    [  214.333957]  [<c126f15a>] __device_release_driver+0x5a/0xb0
    [  214.333957]  [<c126fb07>] driver_detach+0x87/0x90
    [  214.333957]  [<c126ef4c>] bus_remove_driver+0x6c/0xe0
    [  214.333957]  [<c1270120>] driver_unregister+0x40/0x70
    [  214.333957]  [<d083686b>] ssb_driver_unregister+0xb/0x10 [ssb]
    [  214.333957]  [<d087c488>] b43legacy_exit+0xd/0xf [b43legacy]
    [  214.333957]  [<c1089dde>] sys_delete_module+0x14e/0x2b0
    [  214.333957]  [<c110a4a7>] ? vfs_write+0xf7/0x150
    [  214.333957]  [<c1240050>] ? tty_write_lock+0x50/0x50
    [  214.333957]  [<c110a6f8>] ? sys_write+0x38/0x70
    [  214.333957]  [<c1397c55>] syscall_call+0x7/0xb
    [  214.333957] Code: bc 27 00 00 00 00 a1 74 61 56 c1 55 89 e5 e8 a3 fc ff ff 5d c3 90 55 89 e5 57 56 89 c6 53 b8 40 ac 62 c1 83 ec 14 e8 bb b7 34 00 <8b> 46 4c 8d 50 01 85 c0 89 56 4c 75 03 83 0e 40 80 05 40 ac 62
    [  214.333957] EIP: [<c104c395>] drain_workqueue+0x15/0x170 SS:ESP 0068:cd767e5c
    [  214.333957] CR2: 000000000000004c
    [  214.341110] ---[ end trace c7e90ec026d875a6 ]---Index: wireless-testing/drivers/net/wireless/b43legacy/main.c
    
    The problem is fixed by making certain that the ucode pointer is not NULL
    before deregistering the driver in mac80211.
    
    Signed-off-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 28c684cee153e764af0a49a95e14b70463c3c69c
Author: Larry Finger <Larry.Finger@lwfinger.net>
Date:   Wed Sep 26 12:32:02 2012 -0500

    b43legacy: Fix crash on unload when firmware not available
    
    commit 2d838bb608e2d1f6cb4280e76748cb812dc822e7 upstream.
    
    When b43legacy is loaded without the firmware being available, a following
    unload generates a kernel NULL pointer dereference BUG as follows:
    
    [  214.330789] BUG: unable to handle kernel NULL pointer dereference at 0000004c
    [  214.330997] IP: [<c104c395>] drain_workqueue+0x15/0x170
    [  214.331179] *pde = 00000000
    [  214.331311] Oops: 0000 [#1] SMP
    [  214.331471] Modules linked in: b43legacy(-) ssb pcmcia mac80211 cfg80211 af_packet mperf arc4 ppdev sr_mod cdrom sg shpchp yenta_socket pcmcia_rsrc pci_hotplug pcmcia_core battery parport_pc parport floppy container ac button edd autofs4 ohci_hcd ehci_hcd usbcore usb_common thermal processor scsi_dh_rdac scsi_dh_hp_sw scsi_dh_emc scsi_dh_alua scsi_dh fan thermal_sys hwmon ata_generic pata_ali libata [last unloaded: cfg80211]
    [  214.333421] Pid: 3639, comm: modprobe Not tainted 3.6.0-rc6-wl+ #163 Source Technology VIC 9921/ALI Based Notebook
    [  214.333580] EIP: 0060:[<c104c395>] EFLAGS: 00010246 CPU: 0
    [  214.333687] EIP is at drain_workqueue+0x15/0x170
    [  214.333788] EAX: c162ac40 EBX: cdfb8360 ECX: 0000002a EDX: 00002a2a
    [  214.333890] ESI: 00000000 EDI: 00000000 EBP: cd767e7c ESP: cd767e5c
    [  214.333957]  DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
    [  214.333957] CR0: 8005003b CR2: 0000004c CR3: 0c96a000 CR4: 00000090
    [  214.333957] DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
    [  214.333957] DR6: ffff0ff0 DR7: 00000400
    [  214.333957] Process modprobe (pid: 3639, ti=cd766000 task=cf802e90 task.ti=cd766000)
    [  214.333957] Stack:
    [  214.333957]  00000292 cd767e74 c12c5e09 00000296 00000296 cdfb8360 cdfb9220 00000000
    [  214.333957]  cd767e90 c104c4fd cdfb8360 cdfb9220 cd682800 cd767ea4 d0c10184 cd682800
    [  214.333957]  cd767ea4 cba31064 cd767eb8 d0867908 cba31064 d087e09c cd96f034 cd767ec4
    [  214.333957] Call Trace:
    [  214.333957]  [<c12c5e09>] ? skb_dequeue+0x49/0x60
    [  214.333957]  [<c104c4fd>] destroy_workqueue+0xd/0x150
    [  214.333957]  [<d0c10184>] ieee80211_unregister_hw+0xc4/0x100 [mac80211]
    [  214.333957]  [<d0867908>] b43legacy_remove+0x78/0x80 [b43legacy]
    [  214.333957]  [<d083654d>] ssb_device_remove+0x1d/0x30 [ssb]
    [  214.333957]  [<c126f15a>] __device_release_driver+0x5a/0xb0
    [  214.333957]  [<c126fb07>] driver_detach+0x87/0x90
    [  214.333957]  [<c126ef4c>] bus_remove_driver+0x6c/0xe0
    [  214.333957]  [<c1270120>] driver_unregister+0x40/0x70
    [  214.333957]  [<d083686b>] ssb_driver_unregister+0xb/0x10 [ssb]
    [  214.333957]  [<d087c488>] b43legacy_exit+0xd/0xf [b43legacy]
    [  214.333957]  [<c1089dde>] sys_delete_module+0x14e/0x2b0
    [  214.333957]  [<c110a4a7>] ? vfs_write+0xf7/0x150
    [  214.333957]  [<c1240050>] ? tty_write_lock+0x50/0x50
    [  214.333957]  [<c110a6f8>] ? sys_write+0x38/0x70
    [  214.333957]  [<c1397c55>] syscall_call+0x7/0xb
    [  214.333957] Code: bc 27 00 00 00 00 a1 74 61 56 c1 55 89 e5 e8 a3 fc ff ff 5d c3 90 55 89 e5 57 56 89 c6 53 b8 40 ac 62 c1 83 ec 14 e8 bb b7 34 00 <8b> 46 4c 8d 50 01 85 c0 89 56 4c 75 03 83 0e 40 80 05 40 ac 62
    [  214.333957] EIP: [<c104c395>] drain_workqueue+0x15/0x170 SS:ESP 0068:cd767e5c
    [  214.333957] CR2: 000000000000004c
    [  214.341110] ---[ end trace c7e90ec026d875a6 ]---Index: wireless-testing/drivers/net/wireless/b43legacy/main.c
    
    The problem is fixed by making certain that the ucode pointer is not NULL
    before deregistering the driver in mac80211.
    
    Signed-off-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Willy Tarreau <w@1wt.eu>

commit 825f787bb49676083b97c1de1f8f2f8f26b5c908
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Mon Apr 29 15:08:19 2013 -0700

    resource: add release_mem_region_adjustable()
    
    Add release_mem_region_adjustable(), which releases a requested region
    from a currently busy memory resource.  This interface adjusts the
    matched memory resource accordingly even if the requested region does
    not match exactly but still fits into.
    
    This new interface is intended for memory hot-delete.  During bootup,
    memory resources are inserted from the boot descriptor table, such as
    EFI Memory Table and e820.  Each memory resource entry usually covers
    the whole contigous memory range.  Memory hot-delete request, on the
    other hand, may target to a particular range of memory resource, and its
    size can be much smaller than the whole contiguous memory.  Since the
    existing release interfaces like __release_region() require a requested
    region to be exactly matched to a resource entry, they do not allow a
    partial resource to be released.
    
    This new interface is restrictive (i.e.  release under certain
    conditions), which is consistent with other release interfaces,
    __release_region() and __release_resource().  Additional release
    conditions, such as an overlapping region to a resource entry, can be
    supported after they are confirmed as valid cases.
    
    There is no change to the existing interfaces since their restriction is
    valid for I/O resources.
    
    [akpm@linux-foundation.org: use GFP_ATOMIC under write_lock()]
    [akpm@linux-foundation.org: switch back to GFP_KERNEL, less buggily]
    [akpm@linux-foundation.org: remove unneeded and wrong kfree(), per Toshi]
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Reviewed-by : Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Reviewed-by: Ram Pai <linuxram@us.ibm.com>
    Cc: T Makphaibulchoke <tmac@hp.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 2bd2c92cf07cc4a373bf316c75b78ac465fefd35
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Wed Apr 17 15:23:13 2013 -0400

    mutex: Queue mutex spinners with MCS lock to reduce cacheline contention
    
    The current mutex spinning code (with MUTEX_SPIN_ON_OWNER option
    turned on) allow multiple tasks to spin on a single mutex
    concurrently. A potential problem with the current approach is
    that when the mutex becomes available, all the spinning tasks
    will try to acquire the mutex more or less simultaneously. As a
    result, there will be a lot of cacheline bouncing especially on
    systems with a large number of CPUs.
    
    This patch tries to reduce this kind of contention by putting
    the mutex spinners into a queue so that only the first one in
    the queue will try to acquire the mutex. This will reduce
    contention and allow all the tasks to move forward faster.
    
    The queuing of mutex spinners is done using an MCS lock based
    implementation which will further reduce contention on the mutex
    cacheline than a similar ticket spinlock based implementation.
    This patch will add a new field into the mutex data structure
    for holding the MCS lock. This expands the mutex size by 8 bytes
    for 64-bit system and 4 bytes for 32-bit system. This overhead
    will be avoid if the MUTEX_SPIN_ON_OWNER option is turned off.
    
    The following table shows the jobs per minute (JPM) scalability
    data on an 8-node 80-core Westmere box with a 3.7.10 kernel. The
    numactl command is used to restrict the running of the fserver
    workloads to 1/2/4/8 nodes with hyperthreading off.
    
    +-----------------+-----------+-----------+-------------+----------+
    |  Configuration  | Mean JPM  | Mean JPM  |  Mean JPM   | % Change |
    |                 | w/o patch | patch 1   | patches 1&2 |  1->1&2  |
    +-----------------+------------------------------------------------+
    |                 |              User Range 1100 - 2000            |
    +-----------------+------------------------------------------------+
    | 8 nodes, HT off |  227972   |  227237   |   305043    |  +34.2%  |
    | 4 nodes, HT off |  393503   |  381558   |   394650    |   +3.4%  |
    | 2 nodes, HT off |  334957   |  325240   |   338853    |   +4.2%  |
    | 1 node , HT off |  198141   |  197972   |   198075    |   +0.1%  |
    +-----------------+------------------------------------------------+
    |                 |              User Range 200 - 1000             |
    +-----------------+------------------------------------------------+
    | 8 nodes, HT off |  282325   |  312870   |   332185    |   +6.2%  |
    | 4 nodes, HT off |  390698   |  378279   |   393419    |   +4.0%  |
    | 2 nodes, HT off |  336986   |  326543   |   340260    |   +4.2%  |
    | 1 node , HT off |  197588   |  197622   |   197582    |    0.0%  |
    +-----------------+-----------+-----------+-------------+----------+
    
    At low user range 10-100, the JPM differences were within +/-1%.
    So they are not that interesting.
    
    The fserver workload uses mutex spinning extensively. With just
    the mutex change in the first patch, there is no noticeable
    change in performance.  Rather, there is a slight drop in
    performance. This mutex spinning patch more than recovers the
    lost performance and show a significant increase of +30% at high
    user load with the full 8 nodes. Similar improvements were also
    seen in a 3.8 kernel.
    
    The table below shows the %time spent by different kernel
    functions as reported by perf when running the fserver workload
    at 1500 users with all 8 nodes.
    
    +-----------------------+-----------+---------+-------------+
    |        Function       |  % time   | % time  |   % time    |
    |                       | w/o patch | patch 1 | patches 1&2 |
    +-----------------------+-----------+---------+-------------+
    | __read_lock_failed    |  34.96%   | 34.91%  |   29.14%    |
    | __write_lock_failed   |  10.14%   | 10.68%  |    7.51%    |
    | mutex_spin_on_owner   |   3.62%   |  3.42%  |    2.33%    |
    | mspin_lock            |    N/A    |   N/A   |    9.90%    |
    | __mutex_lock_slowpath |   1.46%   |  0.81%  |    0.14%    |
    | _raw_spin_lock        |   2.25%   |  2.50%  |    1.10%    |
    +-----------------------+-----------+---------+-------------+
    
    The fserver workload for an 8-node system is dominated by the
    contention in the read/write lock. Mutex contention also plays a
    role. With the first patch only, mutex contention is down (as
    shown by the __mutex_lock_slowpath figure) which help a little
    bit. We saw only a few percents improvement with that.
    
    By applying patch 2 as well, the single mutex_spin_on_owner
    figure is now split out into an additional mspin_lock figure.
    The time increases from 3.42% to 11.23%. It shows a great
    reduction in contention among the spinners leading to a 30%
    improvement. The time ratio 9.9/2.33=4.3 indicates that there
    are on average 4+ spinners waiting in the spin_lock loop for
    each spinner in the mutex_spin_on_owner loop. Contention in
    other locking functions also go down by quite a lot.
    
    The table below shows the performance change of both patches 1 &
    2 over patch 1 alone in other AIM7 workloads (at 8 nodes,
    hyperthreading off).
    
    +--------------+---------------+----------------+-----------------+
    |   Workload   | mean % change | mean % change  | mean % change   |
    |              | 10-100 users  | 200-1000 users | 1100-2000 users |
    +--------------+---------------+----------------+-----------------+
    | alltests     |      0.0%     |     -0.8%      |     +0.6%       |
    | five_sec     |     -0.3%     |     +0.8%      |     +0.8%       |
    | high_systime |     +0.4%     |     +2.4%      |     +2.1%       |
    | new_fserver  |     +0.1%     |    +14.1%      |    +34.2%       |
    | shared       |     -0.5%     |     -0.3%      |     -0.4%       |
    | short        |     -1.7%     |     -9.8%      |     -8.3%       |
    +--------------+---------------+----------------+-----------------+
    
    The short workload is the only one that shows a decline in
    performance probably due to the spinner locking and queuing
    overhead.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Reviewed-by: Davidlohr Bueso <davidlohr.bueso@hp.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chandramouleeswaran Aswin <aswin@hp.com>
    Cc: Norton Scott J <scott.norton@hp.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1366226594-5506-4-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 8e0bf542fadde9a9ef58c46fa1411dd6cdfb3b14
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Wed Mar 20 13:49:48 2013 +0000

    Btrfs: fix locking on ROOT_REPLACE operations in tree mod log
    
    commit d9abbf1c3131b679379762700201ae69367f3f62 upstream.
    
    To resolve backrefs, ROOT_REPLACE operations in the tree mod log are
    required to be tied to at least one KEY_REMOVE_WHILE_FREEING operation.
    Therefore, those operations must be enclosed by tree_mod_log_write_lock()
    and tree_mod_log_write_unlock() calls.
    
    Those calls are private to the tree_mod_log_* functions, which means that
    removal of the elements of an old root node must be logged from
    tree_mod_log_insert_root. This partly reverts and corrects commit ba1bfbd5
    (Btrfs: fix a tree mod logging issue for root replacement operations).
    
    This fixes the brand-new version of xfstest 276 as of commit cfe73f71.
    
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9a7a71b1d0968fc2bd602b7481cde1d4872e01ff
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 21 22:51:38 2013 +0000

    timekeeping: Split timekeeper_lock into lock and seqcount
    
    We want to shorten the seqcount write hold time. So split the seqlock
    into a lock and a seqcount.
    
    Open code the seqwrite_lock in the places which matter and drop the
    sequence counter update where it's pointless.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [jstultz: Merge fixups from CLOCK_TAI collisions]
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit d9abbf1c3131b679379762700201ae69367f3f62
Author: Jan Schmidt <list.btrfs@jan-o-sch.net>
Date:   Wed Mar 20 13:49:48 2013 +0000

    Btrfs: fix locking on ROOT_REPLACE operations in tree mod log
    
    To resolve backrefs, ROOT_REPLACE operations in the tree mod log are
    required to be tied to at least one KEY_REMOVE_WHILE_FREEING operation.
    Therefore, those operations must be enclosed by tree_mod_log_write_lock()
    and tree_mod_log_write_unlock() calls.
    
    Those calls are private to the tree_mod_log_* functions, which means that
    removal of the elements of an old root node must be logged from
    tree_mod_log_insert_root. This partly reverts and corrects commit ba1bfbd5
    (Btrfs: fix a tree mod logging issue for root replacement operations).
    
    This fixes the brand-new version of xfstest 276 as of commit cfe73f71.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Jan Schmidt <list.btrfs@jan-o-sch.net>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>
    Signed-off-by: Chris Mason <chris.mason@fusionio.com>

commit acb0549acc270c8206ecfdd35d34fc349c3457a0
Author: Antti Palosaari <crope@iki.fi>
Date:   Tue Feb 26 13:01:48 2013 -0300

    [media] dvb_usb_v2: locked versions of USB bulk IO functions
    
    Implement:
    dvb_usbv2_generic_rw_locked()
    dvb_usbv2_generic_write_locked()
    Caller must hold device lock when locked versions are called.
    
    Signed-off-by: Antti Palosaari <crope@iki.fi>
    Signed-off-by: Mauro Carvalho Chehab <mchehab@redhat.com>

commit b59340c2c0508d280f10658ad662fa56a39c74c2
Author: nikolay@redhat.com <nikolay@redhat.com>
Date:   Mon Feb 18 07:59:02 2013 +0000

    bonding: Fix race condition between bond_enslave() and bond_3ad_update_lacp_rate()
    
    port->slave can be NULL since it's being initialized in bond_enslave
    thus dereferencing a NULL pointer in bond_3ad_update_lacp_rate()
    Also fix a minor bug, which could cause a port not to have
    AD_STATE_LACP_TIMEOUT since there's no sync between
    bond_3ad_update_lacp_rate() and bond_3ad_bind_slave(), by changing
    the read_lock to a write_lock_bh in bond_3ad_update_lacp_rate().
    
    Signed-off-by: Nikolay Aleksandrov <nikolay@redhat.com>
    Signed-off-by: Jay Vosburgh <fubar@us.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b5f0e251211531180989a32ba598968104562c84
Author: Jan Kara <jack@suse.cz>
Date:   Fri Dec 21 00:15:51 2012 -0500

    jbd2: fix assertion failure in jbd2_journal_flush()
    
    commit d7961c7fa4d2e3c3f12be67e21ba8799b5a7238a upstream.
    
    The following race is possible between start_this_handle() and someone
    calling jbd2_journal_flush().
    
    Process A                              Process B
    start_this_handle().
      if (journal->j_barrier_count) # false
      if (!journal->j_running_transaction) { #true
        read_unlock(&journal->j_state_lock);
                                           jbd2_journal_lock_updates()
                                           jbd2_journal_flush()
                                             write_lock(&journal->j_state_lock);
                                             if (journal->j_running_transaction) {
                                               # false
                                             ... wait for committing trans ...
                                             write_unlock(&journal->j_state_lock);
        ...
        write_lock(&journal->j_state_lock);
        if (!journal->j_running_transaction) { # true
          jbd2_get_transaction(journal, new_transaction);
        write_unlock(&journal->j_state_lock);
        goto repeat; # eventually blocks on j_barrier_count > 0
                                             ...
                                             J_ASSERT(!journal->j_running_transaction);
                                               # fails
    
    We fix the race by rechecking j_barrier_count after reacquiring j_state_lock
    in exclusive mode.
    
    Reported-by: yjwsignal@empal.com
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3a06c3ff07f7a00a48edbe264987cc33f409a6d5
Author: Jan Kara <jack@suse.cz>
Date:   Fri Dec 21 00:15:51 2012 -0500

    jbd2: fix assertion failure in jbd2_journal_flush()
    
    commit d7961c7fa4d2e3c3f12be67e21ba8799b5a7238a upstream.
    
    The following race is possible between start_this_handle() and someone
    calling jbd2_journal_flush().
    
    Process A                              Process B
    start_this_handle().
      if (journal->j_barrier_count) # false
      if (!journal->j_running_transaction) { #true
        read_unlock(&journal->j_state_lock);
                                           jbd2_journal_lock_updates()
                                           jbd2_journal_flush()
                                             write_lock(&journal->j_state_lock);
                                             if (journal->j_running_transaction) {
                                               # false
                                             ... wait for committing trans ...
                                             write_unlock(&journal->j_state_lock);
        ...
        write_lock(&journal->j_state_lock);
        if (!journal->j_running_transaction) { # true
          jbd2_get_transaction(journal, new_transaction);
        write_unlock(&journal->j_state_lock);
        goto repeat; # eventually blocks on j_barrier_count > 0
                                             ...
                                             J_ASSERT(!journal->j_running_transaction);
                                               # fails
    
    We fix the race by rechecking j_barrier_count after reacquiring j_state_lock
    in exclusive mode.
    
    Reported-by: yjwsignal@empal.com
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7c558b7eb837ca7bc014af2f5509be143c3c3670
Author: Jan Kara <jack@suse.cz>
Date:   Fri Dec 21 00:15:51 2012 -0500

    jbd2: fix assertion failure in jbd2_journal_flush()
    
    commit d7961c7fa4d2e3c3f12be67e21ba8799b5a7238a upstream.
    
    The following race is possible between start_this_handle() and someone
    calling jbd2_journal_flush().
    
    Process A                              Process B
    start_this_handle().
      if (journal->j_barrier_count) # false
      if (!journal->j_running_transaction) { #true
        read_unlock(&journal->j_state_lock);
                                           jbd2_journal_lock_updates()
                                           jbd2_journal_flush()
                                             write_lock(&journal->j_state_lock);
                                             if (journal->j_running_transaction) {
                                               # false
                                             ... wait for committing trans ...
                                             write_unlock(&journal->j_state_lock);
        ...
        write_lock(&journal->j_state_lock);
        if (!journal->j_running_transaction) { # true
          jbd2_get_transaction(journal, new_transaction);
        write_unlock(&journal->j_state_lock);
        goto repeat; # eventually blocks on j_barrier_count > 0
                                             ...
                                             J_ASSERT(!journal->j_running_transaction);
                                               # fails
    
    We fix the race by rechecking j_barrier_count after reacquiring j_state_lock
    in exclusive mode.
    
    Reported-by: yjwsignal@empal.com
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7a55283222cdd70c1cd7a33df0db1e0c96462ac9
Author: Jan Kara <jack@suse.cz>
Date:   Fri Dec 21 00:15:51 2012 -0500

    jbd2: fix assertion failure in jbd2_journal_flush()
    
    commit d7961c7fa4d2e3c3f12be67e21ba8799b5a7238a upstream.
    
    The following race is possible between start_this_handle() and someone
    calling jbd2_journal_flush().
    
    Process A                              Process B
    start_this_handle().
      if (journal->j_barrier_count) # false
      if (!journal->j_running_transaction) { #true
        read_unlock(&journal->j_state_lock);
                                           jbd2_journal_lock_updates()
                                           jbd2_journal_flush()
                                             write_lock(&journal->j_state_lock);
                                             if (journal->j_running_transaction) {
                                               # false
                                             ... wait for committing trans ...
                                             write_unlock(&journal->j_state_lock);
        ...
        write_lock(&journal->j_state_lock);
        if (!journal->j_running_transaction) { # true
          jbd2_get_transaction(journal, new_transaction);
        write_unlock(&journal->j_state_lock);
        goto repeat; # eventually blocks on j_barrier_count > 0
                                             ...
                                             J_ASSERT(!journal->j_running_transaction);
                                               # fails
    
    We fix the race by rechecking j_barrier_count after reacquiring j_state_lock
    in exclusive mode.
    
    Reported-by: yjwsignal@empal.com
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 9650388b5c56578fdccc79c57a8c82fb92b8e7f1
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Dec 21 07:32:10 2012 +0000

    ipv4: arp: fix a lockdep splat in arp_solicit()
    
    Yan Burman reported following lockdep warning :
    
    =============================================
    [ INFO: possible recursive locking detected ]
    3.7.0+ #24 Not tainted
    ---------------------------------------------
    swapper/1/0 is trying to acquire lock:
      (&n->lock){++--..}, at: [<ffffffff8139f56e>] __neigh_event_send
    +0x2e/0x2f0
    
    but task is already holding lock:
      (&n->lock){++--..}, at: [<ffffffff813f63f4>] arp_solicit+0x1d4/0x280
    
    other info that might help us debug this:
      Possible unsafe locking scenario:
    
            CPU0
            ----
       lock(&n->lock);
       lock(&n->lock);
    
      *** DEADLOCK ***
    
      May be due to missing lock nesting notation
    
    4 locks held by swapper/1/0:
      #0:  (((&n->timer))){+.-...}, at: [<ffffffff8104b350>]
    call_timer_fn+0x0/0x1c0
      #1:  (&n->lock){++--..}, at: [<ffffffff813f63f4>] arp_solicit
    +0x1d4/0x280
      #2:  (rcu_read_lock_bh){.+....}, at: [<ffffffff81395400>]
    dev_queue_xmit+0x0/0x5d0
      #3:  (rcu_read_lock_bh){.+....}, at: [<ffffffff813cb41e>]
    ip_finish_output+0x13e/0x640
    
    stack backtrace:
    Pid: 0, comm: swapper/1 Not tainted 3.7.0+ #24
    Call Trace:
      <IRQ>  [<ffffffff8108c7ac>] validate_chain+0xdcc/0x11f0
      [<ffffffff8108d570>] ? __lock_acquire+0x440/0xc30
      [<ffffffff81120565>] ? kmem_cache_free+0xe5/0x1c0
      [<ffffffff8108d570>] __lock_acquire+0x440/0xc30
      [<ffffffff813c3570>] ? inet_getpeer+0x40/0x600
      [<ffffffff8108d570>] ? __lock_acquire+0x440/0xc30
      [<ffffffff8139f56e>] ? __neigh_event_send+0x2e/0x2f0
      [<ffffffff8108ddf5>] lock_acquire+0x95/0x140
      [<ffffffff8139f56e>] ? __neigh_event_send+0x2e/0x2f0
      [<ffffffff8108d570>] ? __lock_acquire+0x440/0xc30
      [<ffffffff81448d4b>] _raw_write_lock_bh+0x3b/0x50
      [<ffffffff8139f56e>] ? __neigh_event_send+0x2e/0x2f0
      [<ffffffff8139f56e>] __neigh_event_send+0x2e/0x2f0
      [<ffffffff8139f99b>] neigh_resolve_output+0x16b/0x270
      [<ffffffff813cb62d>] ip_finish_output+0x34d/0x640
      [<ffffffff813cb41e>] ? ip_finish_output+0x13e/0x640
      [<ffffffffa046f146>] ? vxlan_xmit+0x556/0xbec [vxlan]
      [<ffffffff813cb9a0>] ip_output+0x80/0xf0
      [<ffffffff813ca368>] ip_local_out+0x28/0x80
      [<ffffffffa046f25a>] vxlan_xmit+0x66a/0xbec [vxlan]
      [<ffffffffa046f146>] ? vxlan_xmit+0x556/0xbec [vxlan]
      [<ffffffff81394a50>] ? skb_gso_segment+0x2b0/0x2b0
      [<ffffffff81449355>] ? _raw_spin_unlock_irqrestore+0x65/0x80
      [<ffffffff81394c57>] ? dev_queue_xmit_nit+0x207/0x270
      [<ffffffff813950c8>] dev_hard_start_xmit+0x298/0x5d0
      [<ffffffff813956f3>] dev_queue_xmit+0x2f3/0x5d0
      [<ffffffff81395400>] ? dev_hard_start_xmit+0x5d0/0x5d0
      [<ffffffff813f5788>] arp_xmit+0x58/0x60
      [<ffffffff813f59db>] arp_send+0x3b/0x40
      [<ffffffff813f6424>] arp_solicit+0x204/0x280
      [<ffffffff813a1a70>] ? neigh_add+0x310/0x310
      [<ffffffff8139f515>] neigh_probe+0x45/0x70
      [<ffffffff813a1c10>] neigh_timer_handler+0x1a0/0x2a0
      [<ffffffff8104b3cf>] call_timer_fn+0x7f/0x1c0
      [<ffffffff8104b350>] ? detach_if_pending+0x120/0x120
      [<ffffffff8104b748>] run_timer_softirq+0x238/0x2b0
      [<ffffffff813a1a70>] ? neigh_add+0x310/0x310
      [<ffffffff81043e51>] __do_softirq+0x101/0x280
      [<ffffffff814518cc>] call_softirq+0x1c/0x30
      [<ffffffff81003b65>] do_softirq+0x85/0xc0
      [<ffffffff81043a7e>] irq_exit+0x9e/0xc0
      [<ffffffff810264f8>] smp_apic_timer_interrupt+0x68/0xa0
      [<ffffffff8145122f>] apic_timer_interrupt+0x6f/0x80
      <EOI>  [<ffffffff8100a054>] ? mwait_idle+0xa4/0x1c0
      [<ffffffff8100a04b>] ? mwait_idle+0x9b/0x1c0
      [<ffffffff8100a6a9>] cpu_idle+0x89/0xe0
      [<ffffffff81441127>] start_secondary+0x1b2/0x1b6
    
    Bug is from arp_solicit(), releasing the neigh lock after arp_send()
    In case of vxlan, we eventually need to write lock a neigh lock later.
    
    Its a false positive, but we can get rid of it without lockdep
    annotations.
    
    We can instead use neigh_ha_snapshot() helper.
    
    Reported-by: Yan Burman <yanb@mellanox.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit d7961c7fa4d2e3c3f12be67e21ba8799b5a7238a
Author: Jan Kara <jack@suse.cz>
Date:   Fri Dec 21 00:15:51 2012 -0500

    jbd2: fix assertion failure in jbd2_journal_flush()
    
    The following race is possible between start_this_handle() and someone
    calling jbd2_journal_flush().
    
    Process A                              Process B
    start_this_handle().
      if (journal->j_barrier_count) # false
      if (!journal->j_running_transaction) { #true
        read_unlock(&journal->j_state_lock);
                                           jbd2_journal_lock_updates()
                                           jbd2_journal_flush()
                                             write_lock(&journal->j_state_lock);
                                             if (journal->j_running_transaction) {
                                               # false
                                             ... wait for committing trans ...
                                             write_unlock(&journal->j_state_lock);
        ...
        write_lock(&journal->j_state_lock);
        if (!journal->j_running_transaction) { # true
          jbd2_get_transaction(journal, new_transaction);
        write_unlock(&journal->j_state_lock);
        goto repeat; # eventually blocks on j_barrier_count > 0
                                             ...
                                             J_ASSERT(!journal->j_running_transaction);
                                               # fails
    
    We fix the race by rechecking j_barrier_count after reacquiring j_state_lock
    in exclusive mode.
    
    Reported-by: yjwsignal@empal.com
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>
    Cc: stable@vger.kernel.org

commit e97c5b609880d97313b13eb71830fca62cee50c2
Author: Jim Quinlan <jim2101024@gmail.com>
Date:   Thu Sep 6 11:36:56 2012 -0400

    MIPS: Make irqflags.h functions preempt-safe for non-mipsr2 cpus
    
    For non MIPSr2 processors, such as the BMIPS 5000, calls to
    arch_local_irq_disable() and others may be preempted, and in doing
    so a stale value may be restored to c0_status.  This fix disables
    preemption for such processors prior to the call and enables it
    after the call.
    
    Those functions that needed this fix have been "outlined" to
    mips-atomic.c, as they are no longer good candidates for inlining.
    
    This bug was observed in a BMIPS 5000, occuring once every few hours
    in a continuous reboot test.  It was traced to the write_lock_irq()
    function which was being invoked in release_task() in exit.c.
    By placing a number of "nops" inbetween the mfc0/mtc0 pair in
    arch_local_irq_disable(), which is called by write_lock_irq(), we
    were able to greatly increase the occurance of this bug.  Similarly,
    the application of this commit silenced the bug.
    
    Signed-off-by: Jim Quinlan <jim2101024@gmail.com>
    Cc: linux-mips@linux-mips.org
    Cc: David Daney <ddaney.cavm@gmail.com>
    Cc: Kevin Cernekee cernekee@gmail.com
    Cc: Jim Quinlan <jim2101024@gmail.com>
    Patchwork: https://patchwork.linux-mips.org/patch/4321/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit bddc7152f68bc1e0ee1f55a8055e33531f384101
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Thu Oct 18 22:26:42 2012 +0200

    TTY: move ldisc data from tty_struct: locks
    
    atomic_write_lock is not n_tty specific, so move it up in the
    tty_struct.
    
    And since these are the last ones to move, remove also the comment
    saying there are some ldisc' members. There are none now.
    
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Acked-by: Alan Cox <alan@linux.intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6a29b109b9fb4c76bf55851223ae59e3322d9616
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Jul 17 18:05:25 2012 -0400

    ntp: Fix leap-second hrtimer livelock
    
    This is a backport of 6b43ae8a619d17c4935c3320d2ef9e92bdeed05d
    
    This should have been backported when it was commited, but I
    mistook the problem as requiring the ntp_lock changes
    that landed in 3.4 in order for it to occur.
    
    Unfortunately the same issue can happen (with only one cpu)
    as follows:
    do_adjtimex()
     write_seqlock_irq(&xtime_lock);
      process_adjtimex_modes()
       process_adj_status()
        ntp_start_leap_timer()
         hrtimer_start()
          hrtimer_reprogram()
           tick_program_event()
            clockevents_program_event()
             ktime_get()
              seq = req_seqbegin(xtime_lock); [DEADLOCK]
    
    This deadlock will no always occur, as it requires the
    leap_timer to force a hrtimer_reprogram which only happens
    if its set and there's no sooner timer to expire.
    
    NOTE: This patch, being faithful to the original commit,
    introduces a bug (we don't update wall_to_monotonic),
    which will be resovled by backporting a following fix.
    
    Original commit message below:
    
    Since commit 7dffa3c673fbcf835cd7be80bb4aec8ad3f51168 the ntp
    subsystem has used an hrtimer for triggering the leapsecond
    adjustment. However, this can cause a potential livelock.
    
    Thomas diagnosed this as the following pattern:
    CPU 0                                                    CPU 1
    do_adjtimex()
      spin_lock_irq(&ntp_lock);
        process_adjtimex_modes();                            timer_interrupt()
          process_adj_status();                                do_timer()
            ntp_start_leap_timer();                             write_lock(&xtime_lock);
              hrtimer_start();                                  update_wall_time();
                 hrtimer_reprogram();                            ntp_tick_length()
                   tick_program_event()                            spin_lock(&ntp_lock);
                     clockevents_program_event()
                       ktime_get()
                         seq = req_seqbegin(xtime_lock);
    
    This patch tries to avoid the problem by reverting back to not using
    an hrtimer to inject leapseconds, and instead we handle the leapsecond
    processing in the second_overflow() function.
    
    The downside to this change is that on systems that support highres
    timers, the leap second processing will occur on a HZ tick boundary,
    (ie: ~1-10ms, depending on HZ)  after the leap second instead of
    possibly sooner (~34us in my tests w/ x86_64 lapic).
    
    This patch applies on top of tip/timers/core.
    
    CC: Sasha Levin <levinsasha928@gmail.com>
    CC: Thomas Gleixner <tglx@linutronix.de>
    Reported-by: Sasha Levin <levinsasha928@gmail.com>
    Diagnoised-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Sasha Levin <levinsasha928@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linux Kernel <linux-kernel@vger.kernel.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Willy Tarreau <w@1wt.eu>

commit 2975303676cea3f3d7cc076e32c1542637ef7edc
Author: Larry Finger <Larry.Finger@lwfinger.net>
Date:   Wed Sep 26 12:32:02 2012 -0500

    b43legacy: Fix crash on unload when firmware not available
    
    commit 2d838bb608e2d1f6cb4280e76748cb812dc822e7 upstream.
    
    When b43legacy is loaded without the firmware being available, a following
    unload generates a kernel NULL pointer dereference BUG as follows:
    
    [  214.330789] BUG: unable to handle kernel NULL pointer dereference at 0000004c
    [  214.330997] IP: [<c104c395>] drain_workqueue+0x15/0x170
    [  214.331179] *pde = 00000000
    [  214.331311] Oops: 0000 [#1] SMP
    [  214.331471] Modules linked in: b43legacy(-) ssb pcmcia mac80211 cfg80211 af_packet mperf arc4 ppdev sr_mod cdrom sg shpchp yenta_socket pcmcia_rsrc pci_hotplug pcmcia_core battery parport_pc parport floppy container ac button edd autofs4 ohci_hcd ehci_hcd usbcore usb_common thermal processor scsi_dh_rdac scsi_dh_hp_sw scsi_dh_emc scsi_dh_alua scsi_dh fan thermal_sys hwmon ata_generic pata_ali libata [last unloaded: cfg80211]
    [  214.333421] Pid: 3639, comm: modprobe Not tainted 3.6.0-rc6-wl+ #163 Source Technology VIC 9921/ALI Based Notebook
    [  214.333580] EIP: 0060:[<c104c395>] EFLAGS: 00010246 CPU: 0
    [  214.333687] EIP is at drain_workqueue+0x15/0x170
    [  214.333788] EAX: c162ac40 EBX: cdfb8360 ECX: 0000002a EDX: 00002a2a
    [  214.333890] ESI: 00000000 EDI: 00000000 EBP: cd767e7c ESP: cd767e5c
    [  214.333957]  DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
    [  214.333957] CR0: 8005003b CR2: 0000004c CR3: 0c96a000 CR4: 00000090
    [  214.333957] DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
    [  214.333957] DR6: ffff0ff0 DR7: 00000400
    [  214.333957] Process modprobe (pid: 3639, ti=cd766000 task=cf802e90 task.ti=cd766000)
    [  214.333957] Stack:
    [  214.333957]  00000292 cd767e74 c12c5e09 00000296 00000296 cdfb8360 cdfb9220 00000000
    [  214.333957]  cd767e90 c104c4fd cdfb8360 cdfb9220 cd682800 cd767ea4 d0c10184 cd682800
    [  214.333957]  cd767ea4 cba31064 cd767eb8 d0867908 cba31064 d087e09c cd96f034 cd767ec4
    [  214.333957] Call Trace:
    [  214.333957]  [<c12c5e09>] ? skb_dequeue+0x49/0x60
    [  214.333957]  [<c104c4fd>] destroy_workqueue+0xd/0x150
    [  214.333957]  [<d0c10184>] ieee80211_unregister_hw+0xc4/0x100 [mac80211]
    [  214.333957]  [<d0867908>] b43legacy_remove+0x78/0x80 [b43legacy]
    [  214.333957]  [<d083654d>] ssb_device_remove+0x1d/0x30 [ssb]
    [  214.333957]  [<c126f15a>] __device_release_driver+0x5a/0xb0
    [  214.333957]  [<c126fb07>] driver_detach+0x87/0x90
    [  214.333957]  [<c126ef4c>] bus_remove_driver+0x6c/0xe0
    [  214.333957]  [<c1270120>] driver_unregister+0x40/0x70
    [  214.333957]  [<d083686b>] ssb_driver_unregister+0xb/0x10 [ssb]
    [  214.333957]  [<d087c488>] b43legacy_exit+0xd/0xf [b43legacy]
    [  214.333957]  [<c1089dde>] sys_delete_module+0x14e/0x2b0
    [  214.333957]  [<c110a4a7>] ? vfs_write+0xf7/0x150
    [  214.333957]  [<c1240050>] ? tty_write_lock+0x50/0x50
    [  214.333957]  [<c110a6f8>] ? sys_write+0x38/0x70
    [  214.333957]  [<c1397c55>] syscall_call+0x7/0xb
    [  214.333957] Code: bc 27 00 00 00 00 a1 74 61 56 c1 55 89 e5 e8 a3 fc ff ff 5d c3 90 55 89 e5 57 56 89 c6 53 b8 40 ac 62 c1 83 ec 14 e8 bb b7 34 00 <8b> 46 4c 8d 50 01 85 c0 89 56 4c 75 03 83 0e 40 80 05 40 ac 62
    [  214.333957] EIP: [<c104c395>] drain_workqueue+0x15/0x170 SS:ESP 0068:cd767e5c
    [  214.333957] CR2: 000000000000004c
    [  214.341110] ---[ end trace c7e90ec026d875a6 ]---Index: wireless-testing/drivers/net/wireless/b43legacy/main.c
    
    The problem is fixed by making certain that the ucode pointer is not NULL
    before deregistering the driver in mac80211.
    
    Signed-off-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 163449e1dfc3973020d91e61d958da4714c3f23a
Author: Larry Finger <Larry.Finger@lwfinger.net>
Date:   Wed Sep 26 12:32:02 2012 -0500

    b43legacy: Fix crash on unload when firmware not available
    
    commit 2d838bb608e2d1f6cb4280e76748cb812dc822e7 upstream.
    
    When b43legacy is loaded without the firmware being available, a following
    unload generates a kernel NULL pointer dereference BUG as follows:
    
    [  214.330789] BUG: unable to handle kernel NULL pointer dereference at 0000004c
    [  214.330997] IP: [<c104c395>] drain_workqueue+0x15/0x170
    [  214.331179] *pde = 00000000
    [  214.331311] Oops: 0000 [#1] SMP
    [  214.331471] Modules linked in: b43legacy(-) ssb pcmcia mac80211 cfg80211 af_packet mperf arc4 ppdev sr_mod cdrom sg shpchp yenta_socket pcmcia_rsrc pci_hotplug pcmcia_core battery parport_pc parport floppy container ac button edd autofs4 ohci_hcd ehci_hcd usbcore usb_common thermal processor scsi_dh_rdac scsi_dh_hp_sw scsi_dh_emc scsi_dh_alua scsi_dh fan thermal_sys hwmon ata_generic pata_ali libata [last unloaded: cfg80211]
    [  214.333421] Pid: 3639, comm: modprobe Not tainted 3.6.0-rc6-wl+ #163 Source Technology VIC 9921/ALI Based Notebook
    [  214.333580] EIP: 0060:[<c104c395>] EFLAGS: 00010246 CPU: 0
    [  214.333687] EIP is at drain_workqueue+0x15/0x170
    [  214.333788] EAX: c162ac40 EBX: cdfb8360 ECX: 0000002a EDX: 00002a2a
    [  214.333890] ESI: 00000000 EDI: 00000000 EBP: cd767e7c ESP: cd767e5c
    [  214.333957]  DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
    [  214.333957] CR0: 8005003b CR2: 0000004c CR3: 0c96a000 CR4: 00000090
    [  214.333957] DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
    [  214.333957] DR6: ffff0ff0 DR7: 00000400
    [  214.333957] Process modprobe (pid: 3639, ti=cd766000 task=cf802e90 task.ti=cd766000)
    [  214.333957] Stack:
    [  214.333957]  00000292 cd767e74 c12c5e09 00000296 00000296 cdfb8360 cdfb9220 00000000
    [  214.333957]  cd767e90 c104c4fd cdfb8360 cdfb9220 cd682800 cd767ea4 d0c10184 cd682800
    [  214.333957]  cd767ea4 cba31064 cd767eb8 d0867908 cba31064 d087e09c cd96f034 cd767ec4
    [  214.333957] Call Trace:
    [  214.333957]  [<c12c5e09>] ? skb_dequeue+0x49/0x60
    [  214.333957]  [<c104c4fd>] destroy_workqueue+0xd/0x150
    [  214.333957]  [<d0c10184>] ieee80211_unregister_hw+0xc4/0x100 [mac80211]
    [  214.333957]  [<d0867908>] b43legacy_remove+0x78/0x80 [b43legacy]
    [  214.333957]  [<d083654d>] ssb_device_remove+0x1d/0x30 [ssb]
    [  214.333957]  [<c126f15a>] __device_release_driver+0x5a/0xb0
    [  214.333957]  [<c126fb07>] driver_detach+0x87/0x90
    [  214.333957]  [<c126ef4c>] bus_remove_driver+0x6c/0xe0
    [  214.333957]  [<c1270120>] driver_unregister+0x40/0x70
    [  214.333957]  [<d083686b>] ssb_driver_unregister+0xb/0x10 [ssb]
    [  214.333957]  [<d087c488>] b43legacy_exit+0xd/0xf [b43legacy]
    [  214.333957]  [<c1089dde>] sys_delete_module+0x14e/0x2b0
    [  214.333957]  [<c110a4a7>] ? vfs_write+0xf7/0x150
    [  214.333957]  [<c1240050>] ? tty_write_lock+0x50/0x50
    [  214.333957]  [<c110a6f8>] ? sys_write+0x38/0x70
    [  214.333957]  [<c1397c55>] syscall_call+0x7/0xb
    [  214.333957] Code: bc 27 00 00 00 00 a1 74 61 56 c1 55 89 e5 e8 a3 fc ff ff 5d c3 90 55 89 e5 57 56 89 c6 53 b8 40 ac 62 c1 83 ec 14 e8 bb b7 34 00 <8b> 46 4c 8d 50 01 85 c0 89 56 4c 75 03 83 0e 40 80 05 40 ac 62
    [  214.333957] EIP: [<c104c395>] drain_workqueue+0x15/0x170 SS:ESP 0068:cd767e5c
    [  214.333957] CR2: 000000000000004c
    [  214.341110] ---[ end trace c7e90ec026d875a6 ]---Index: wireless-testing/drivers/net/wireless/b43legacy/main.c
    
    The problem is fixed by making certain that the ucode pointer is not NULL
    before deregistering the driver in mac80211.
    
    Signed-off-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit dc8276b241ad415b2602c4a7309e5b518bb09c32
Author: Larry Finger <Larry.Finger@lwfinger.net>
Date:   Wed Sep 26 12:32:02 2012 -0500

    b43legacy: Fix crash on unload when firmware not available
    
    commit 2d838bb608e2d1f6cb4280e76748cb812dc822e7 upstream.
    
    When b43legacy is loaded without the firmware being available, a following
    unload generates a kernel NULL pointer dereference BUG as follows:
    
    [  214.330789] BUG: unable to handle kernel NULL pointer dereference at 0000004c
    [  214.330997] IP: [<c104c395>] drain_workqueue+0x15/0x170
    [  214.331179] *pde = 00000000
    [  214.331311] Oops: 0000 [#1] SMP
    [  214.331471] Modules linked in: b43legacy(-) ssb pcmcia mac80211 cfg80211 af_packet mperf arc4 ppdev sr_mod cdrom sg shpchp yenta_socket pcmcia_rsrc pci_hotplug pcmcia_core battery parport_pc parport floppy container ac button edd autofs4 ohci_hcd ehci_hcd usbcore usb_common thermal processor scsi_dh_rdac scsi_dh_hp_sw scsi_dh_emc scsi_dh_alua scsi_dh fan thermal_sys hwmon ata_generic pata_ali libata [last unloaded: cfg80211]
    [  214.333421] Pid: 3639, comm: modprobe Not tainted 3.6.0-rc6-wl+ #163 Source Technology VIC 9921/ALI Based Notebook
    [  214.333580] EIP: 0060:[<c104c395>] EFLAGS: 00010246 CPU: 0
    [  214.333687] EIP is at drain_workqueue+0x15/0x170
    [  214.333788] EAX: c162ac40 EBX: cdfb8360 ECX: 0000002a EDX: 00002a2a
    [  214.333890] ESI: 00000000 EDI: 00000000 EBP: cd767e7c ESP: cd767e5c
    [  214.333957]  DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
    [  214.333957] CR0: 8005003b CR2: 0000004c CR3: 0c96a000 CR4: 00000090
    [  214.333957] DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
    [  214.333957] DR6: ffff0ff0 DR7: 00000400
    [  214.333957] Process modprobe (pid: 3639, ti=cd766000 task=cf802e90 task.ti=cd766000)
    [  214.333957] Stack:
    [  214.333957]  00000292 cd767e74 c12c5e09 00000296 00000296 cdfb8360 cdfb9220 00000000
    [  214.333957]  cd767e90 c104c4fd cdfb8360 cdfb9220 cd682800 cd767ea4 d0c10184 cd682800
    [  214.333957]  cd767ea4 cba31064 cd767eb8 d0867908 cba31064 d087e09c cd96f034 cd767ec4
    [  214.333957] Call Trace:
    [  214.333957]  [<c12c5e09>] ? skb_dequeue+0x49/0x60
    [  214.333957]  [<c104c4fd>] destroy_workqueue+0xd/0x150
    [  214.333957]  [<d0c10184>] ieee80211_unregister_hw+0xc4/0x100 [mac80211]
    [  214.333957]  [<d0867908>] b43legacy_remove+0x78/0x80 [b43legacy]
    [  214.333957]  [<d083654d>] ssb_device_remove+0x1d/0x30 [ssb]
    [  214.333957]  [<c126f15a>] __device_release_driver+0x5a/0xb0
    [  214.333957]  [<c126fb07>] driver_detach+0x87/0x90
    [  214.333957]  [<c126ef4c>] bus_remove_driver+0x6c/0xe0
    [  214.333957]  [<c1270120>] driver_unregister+0x40/0x70
    [  214.333957]  [<d083686b>] ssb_driver_unregister+0xb/0x10 [ssb]
    [  214.333957]  [<d087c488>] b43legacy_exit+0xd/0xf [b43legacy]
    [  214.333957]  [<c1089dde>] sys_delete_module+0x14e/0x2b0
    [  214.333957]  [<c110a4a7>] ? vfs_write+0xf7/0x150
    [  214.333957]  [<c1240050>] ? tty_write_lock+0x50/0x50
    [  214.333957]  [<c110a6f8>] ? sys_write+0x38/0x70
    [  214.333957]  [<c1397c55>] syscall_call+0x7/0xb
    [  214.333957] Code: bc 27 00 00 00 00 a1 74 61 56 c1 55 89 e5 e8 a3 fc ff ff 5d c3 90 55 89 e5 57 56 89 c6 53 b8 40 ac 62 c1 83 ec 14 e8 bb b7 34 00 <8b> 46 4c 8d 50 01 85 c0 89 56 4c 75 03 83 0e 40 80 05 40 ac 62
    [  214.333957] EIP: [<c104c395>] drain_workqueue+0x15/0x170 SS:ESP 0068:cd767e5c
    [  214.333957] CR2: 000000000000004c
    [  214.341110] ---[ end trace c7e90ec026d875a6 ]---Index: wireless-testing/drivers/net/wireless/b43legacy/main.c
    
    The problem is fixed by making certain that the ucode pointer is not NULL
    before deregistering the driver in mac80211.
    
    Signed-off-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f39a3e8d1462627890731d0d077509546cc113e4
Author: Larry Finger <Larry.Finger@lwfinger.net>
Date:   Wed Sep 26 12:32:02 2012 -0500

    b43legacy: Fix crash on unload when firmware not available
    
    commit 2d838bb608e2d1f6cb4280e76748cb812dc822e7 upstream.
    
    When b43legacy is loaded without the firmware being available, a following
    unload generates a kernel NULL pointer dereference BUG as follows:
    
    [  214.330789] BUG: unable to handle kernel NULL pointer dereference at 0000004c
    [  214.330997] IP: [<c104c395>] drain_workqueue+0x15/0x170
    [  214.331179] *pde = 00000000
    [  214.331311] Oops: 0000 [#1] SMP
    [  214.331471] Modules linked in: b43legacy(-) ssb pcmcia mac80211 cfg80211 af_packet mperf arc4 ppdev sr_mod cdrom sg shpchp yenta_socket pcmcia_rsrc pci_hotplug pcmcia_core battery parport_pc parport floppy container ac button edd autofs4 ohci_hcd ehci_hcd usbcore usb_common thermal processor scsi_dh_rdac scsi_dh_hp_sw scsi_dh_emc scsi_dh_alua scsi_dh fan thermal_sys hwmon ata_generic pata_ali libata [last unloaded: cfg80211]
    [  214.333421] Pid: 3639, comm: modprobe Not tainted 3.6.0-rc6-wl+ #163 Source Technology VIC 9921/ALI Based Notebook
    [  214.333580] EIP: 0060:[<c104c395>] EFLAGS: 00010246 CPU: 0
    [  214.333687] EIP is at drain_workqueue+0x15/0x170
    [  214.333788] EAX: c162ac40 EBX: cdfb8360 ECX: 0000002a EDX: 00002a2a
    [  214.333890] ESI: 00000000 EDI: 00000000 EBP: cd767e7c ESP: cd767e5c
    [  214.333957]  DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
    [  214.333957] CR0: 8005003b CR2: 0000004c CR3: 0c96a000 CR4: 00000090
    [  214.333957] DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
    [  214.333957] DR6: ffff0ff0 DR7: 00000400
    [  214.333957] Process modprobe (pid: 3639, ti=cd766000 task=cf802e90 task.ti=cd766000)
    [  214.333957] Stack:
    [  214.333957]  00000292 cd767e74 c12c5e09 00000296 00000296 cdfb8360 cdfb9220 00000000
    [  214.333957]  cd767e90 c104c4fd cdfb8360 cdfb9220 cd682800 cd767ea4 d0c10184 cd682800
    [  214.333957]  cd767ea4 cba31064 cd767eb8 d0867908 cba31064 d087e09c cd96f034 cd767ec4
    [  214.333957] Call Trace:
    [  214.333957]  [<c12c5e09>] ? skb_dequeue+0x49/0x60
    [  214.333957]  [<c104c4fd>] destroy_workqueue+0xd/0x150
    [  214.333957]  [<d0c10184>] ieee80211_unregister_hw+0xc4/0x100 [mac80211]
    [  214.333957]  [<d0867908>] b43legacy_remove+0x78/0x80 [b43legacy]
    [  214.333957]  [<d083654d>] ssb_device_remove+0x1d/0x30 [ssb]
    [  214.333957]  [<c126f15a>] __device_release_driver+0x5a/0xb0
    [  214.333957]  [<c126fb07>] driver_detach+0x87/0x90
    [  214.333957]  [<c126ef4c>] bus_remove_driver+0x6c/0xe0
    [  214.333957]  [<c1270120>] driver_unregister+0x40/0x70
    [  214.333957]  [<d083686b>] ssb_driver_unregister+0xb/0x10 [ssb]
    [  214.333957]  [<d087c488>] b43legacy_exit+0xd/0xf [b43legacy]
    [  214.333957]  [<c1089dde>] sys_delete_module+0x14e/0x2b0
    [  214.333957]  [<c110a4a7>] ? vfs_write+0xf7/0x150
    [  214.333957]  [<c1240050>] ? tty_write_lock+0x50/0x50
    [  214.333957]  [<c110a6f8>] ? sys_write+0x38/0x70
    [  214.333957]  [<c1397c55>] syscall_call+0x7/0xb
    [  214.333957] Code: bc 27 00 00 00 00 a1 74 61 56 c1 55 89 e5 e8 a3 fc ff ff 5d c3 90 55 89 e5 57 56 89 c6 53 b8 40 ac 62 c1 83 ec 14 e8 bb b7 34 00 <8b> 46 4c 8d 50 01 85 c0 89 56 4c 75 03 83 0e 40 80 05 40 ac 62
    [  214.333957] EIP: [<c104c395>] drain_workqueue+0x15/0x170 SS:ESP 0068:cd767e5c
    [  214.333957] CR2: 000000000000004c
    [  214.341110] ---[ end trace c7e90ec026d875a6 ]---Index: wireless-testing/drivers/net/wireless/b43legacy/main.c
    
    The problem is fixed by making certain that the ucode pointer is not NULL
    before deregistering the driver in mac80211.
    
    Signed-off-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ff44c6e36dc9dcc02652a1105b120bdf08cea9f7
Author: Josef Bacik <jbacik@fusionio.com>
Date:   Fri Sep 14 12:59:20 2012 -0400

    Btrfs: do not hold the write_lock on the extent tree while logging
    
    Dave Sterba pointed out a sleeping while atomic bug while doing fsync.  This
    is because I'm an idiot and didn't realize that rwlock's were spin locks, so
    we've been holding this thing while doing allocations and such which is not
    good.  This patch fixes this by dropping the write lock before we do
    anything heavy and re-acquire it when it is done.  We also need to take a
    ref on the em's in case their corresponding pages are evicted and mark them
    as being logged so that releasepage does not remove them and doesn't remove
    them from our local list.  Thanks,
    
    Reported-by: Dave Sterba <dave@jikos.cz>
    Signed-off-by: Josef Bacik <jbacik@fusionio.com>

commit 2d838bb608e2d1f6cb4280e76748cb812dc822e7
Author: Larry Finger <Larry.Finger@lwfinger.net>
Date:   Wed Sep 26 12:32:02 2012 -0500

    b43legacy: Fix crash on unload when firmware not available
    
    When b43legacy is loaded without the firmware being available, a following
    unload generates a kernel NULL pointer dereference BUG as follows:
    
    [  214.330789] BUG: unable to handle kernel NULL pointer dereference at 0000004c
    [  214.330997] IP: [<c104c395>] drain_workqueue+0x15/0x170
    [  214.331179] *pde = 00000000
    [  214.331311] Oops: 0000 [#1] SMP
    [  214.331471] Modules linked in: b43legacy(-) ssb pcmcia mac80211 cfg80211 af_packet mperf arc4 ppdev sr_mod cdrom sg shpchp yenta_socket pcmcia_rsrc pci_hotplug pcmcia_core battery parport_pc parport floppy container ac button edd autofs4 ohci_hcd ehci_hcd usbcore usb_common thermal processor scsi_dh_rdac scsi_dh_hp_sw scsi_dh_emc scsi_dh_alua scsi_dh fan thermal_sys hwmon ata_generic pata_ali libata [last unloaded: cfg80211]
    [  214.333421] Pid: 3639, comm: modprobe Not tainted 3.6.0-rc6-wl+ #163 Source Technology VIC 9921/ALI Based Notebook
    [  214.333580] EIP: 0060:[<c104c395>] EFLAGS: 00010246 CPU: 0
    [  214.333687] EIP is at drain_workqueue+0x15/0x170
    [  214.333788] EAX: c162ac40 EBX: cdfb8360 ECX: 0000002a EDX: 00002a2a
    [  214.333890] ESI: 00000000 EDI: 00000000 EBP: cd767e7c ESP: cd767e5c
    [  214.333957]  DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
    [  214.333957] CR0: 8005003b CR2: 0000004c CR3: 0c96a000 CR4: 00000090
    [  214.333957] DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
    [  214.333957] DR6: ffff0ff0 DR7: 00000400
    [  214.333957] Process modprobe (pid: 3639, ti=cd766000 task=cf802e90 task.ti=cd766000)
    [  214.333957] Stack:
    [  214.333957]  00000292 cd767e74 c12c5e09 00000296 00000296 cdfb8360 cdfb9220 00000000
    [  214.333957]  cd767e90 c104c4fd cdfb8360 cdfb9220 cd682800 cd767ea4 d0c10184 cd682800
    [  214.333957]  cd767ea4 cba31064 cd767eb8 d0867908 cba31064 d087e09c cd96f034 cd767ec4
    [  214.333957] Call Trace:
    [  214.333957]  [<c12c5e09>] ? skb_dequeue+0x49/0x60
    [  214.333957]  [<c104c4fd>] destroy_workqueue+0xd/0x150
    [  214.333957]  [<d0c10184>] ieee80211_unregister_hw+0xc4/0x100 [mac80211]
    [  214.333957]  [<d0867908>] b43legacy_remove+0x78/0x80 [b43legacy]
    [  214.333957]  [<d083654d>] ssb_device_remove+0x1d/0x30 [ssb]
    [  214.333957]  [<c126f15a>] __device_release_driver+0x5a/0xb0
    [  214.333957]  [<c126fb07>] driver_detach+0x87/0x90
    [  214.333957]  [<c126ef4c>] bus_remove_driver+0x6c/0xe0
    [  214.333957]  [<c1270120>] driver_unregister+0x40/0x70
    [  214.333957]  [<d083686b>] ssb_driver_unregister+0xb/0x10 [ssb]
    [  214.333957]  [<d087c488>] b43legacy_exit+0xd/0xf [b43legacy]
    [  214.333957]  [<c1089dde>] sys_delete_module+0x14e/0x2b0
    [  214.333957]  [<c110a4a7>] ? vfs_write+0xf7/0x150
    [  214.333957]  [<c1240050>] ? tty_write_lock+0x50/0x50
    [  214.333957]  [<c110a6f8>] ? sys_write+0x38/0x70
    [  214.333957]  [<c1397c55>] syscall_call+0x7/0xb
    [  214.333957] Code: bc 27 00 00 00 00 a1 74 61 56 c1 55 89 e5 e8 a3 fc ff ff 5d c3 90 55 89 e5 57 56 89 c6 53 b8 40 ac 62 c1 83 ec 14 e8 bb b7 34 00 <8b> 46 4c 8d 50 01 85 c0 89 56 4c 75 03 83 0e 40 80 05 40 ac 62
    [  214.333957] EIP: [<c104c395>] drain_workqueue+0x15/0x170 SS:ESP 0068:cd767e5c
    [  214.333957] CR2: 000000000000004c
    [  214.341110] ---[ end trace c7e90ec026d875a6 ]---Index: wireless-testing/drivers/net/wireless/b43legacy/main.c
    
    The problem is fixed by making certain that the ucode pointer is not NULL
    before deregistering the driver in mac80211.
    
    Signed-off-by: Larry Finger <Larry.Finger@lwfinger.net>
    Cc: Stable <stable@vger.kernel.org>   [v 3.3.0+]
    Signed-off-by: John W. Linville <linville@tuxdriver.com>

commit 50b78b2a6500d0e97c204c1b6c51df8c17358bbe
Author: Szymon Janc <szymon.janc@tieto.com>
Date:   Wed Sep 26 14:22:10 2012 +0200

    NFC: Fix sleeping in atomic when releasing socket
    
    nfc_llcp_socket_release is calling lock_sock/release_sock while holding
    write lock for rwlock. Use bh_lock/unlock_sock instead.
    
    BUG: sleeping function called from invalid context at net/core/sock.c:2138
    in_atomic(): 1, irqs_disabled(): 0, pid: 56, name: kworker/1:1
    4 locks held by kworker/1:1/56:
    Pid: 56, comm: kworker/1:1 Not tainted 3.5.0-999-nfc+ #7
    Call Trace:
    [<ffffffff810952c5>] __might_sleep+0x145/0x200
    [<ffffffff815d7686>] lock_sock_nested+0x36/0xa0
    [<ffffffff81731569>] ? _raw_write_lock+0x49/0x50
    [<ffffffffa04aa100>] ? nfc_llcp_socket_release+0x30/0x200 [nfc]
    [<ffffffffa04aa122>] nfc_llcp_socket_release+0x52/0x200 [nfc]
    [<ffffffffa04ab9f0>] nfc_llcp_mac_is_down+0x20/0x30 [nfc]
    [<ffffffffa04a6fea>] nfc_dep_link_down+0xaa/0xf0 [nfc]
    [<ffffffffa04a9bb5>] nfc_llcp_timeout_work+0x15/0x20 [nfc]
    [<ffffffff810825f7>] process_one_work+0x197/0x7c0
    [<ffffffff81082596>] ? process_one_work+0x136/0x7c0
    [<ffffffff8172fbc9>] ? __schedule+0x419/0x9c0
    [<ffffffffa04a9ba0>] ? nfc_llcp_build_gb+0x1b0/0x1b0 [nfc]
    [<ffffffff81083090>] worker_thread+0x190/0x4c0
    [<ffffffff81082f00>] ? rescuer_thread+0x2a0/0x2a0
    [<ffffffff81088d1e>] kthread+0xae/0xc0
    [<ffffffff810caafd>] ? trace_hardirqs_on+0xd/0x10
    [<ffffffff8173acc4>] kernel_thread_helper+0x4/0x10
    [<ffffffff81732174>] ? retint_restore_args+0x13/0x13
    [<ffffffff81088c70>] ? flush_kthread_worker+0x150/0x150
    [<ffffffff8173acc0>] ? gs_change+0x13/0x13
    
    Signed-off-by: Szymon Janc <szymon.janc@tieto.com>
    Signed-off-by: Samuel Ortiz <sameo@linux.intel.com>

commit b3f68f16dbcde6fcdf0fd27695391ff7e9d41233
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Sun Aug 26 21:12:14 2012 +0200

    task_work: Revert "hold task_lock around checks in keyctl"
    
    This reverts commit d35abdb28824cf74f0a106a0f9c6f3ff700a35bf.
    
    task_lock() was added to ensure exit_mm() and thus exit_task_work() is
    not possible before task_work_add().
    
    This is wrong, task_lock() must not be nested with write_lock(tasklist).
    And this is no longer needed, task_work_add() now fails if it is called
    after exit_task_work().
    
    Reported-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20120826191214.GA4231@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 838955464ce6579492c1b863264ad6cfe79a9a14
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Jul 17 17:49:21 2012 -0400

    ntp: Fix leap-second hrtimer livelock
    
    commit 6b43ae8a619d17c4935c3320d2ef9e92bdeed05d upstream.
    
    This should have been backported when it was commited, but I
    mistook the problem as requiring the ntp_lock changes
    that landed in 3.4 in order for it to occur.
    
    Unfortunately the same issue can happen (with only one cpu)
    as follows:
    do_adjtimex()
     write_seqlock_irq(&xtime_lock);
      process_adjtimex_modes()
       process_adj_status()
        ntp_start_leap_timer()
         hrtimer_start()
          hrtimer_reprogram()
           tick_program_event()
            clockevents_program_event()
             ktime_get()
              seq = req_seqbegin(xtime_lock); [DEADLOCK]
    
    This deadlock will no always occur, as it requires the
    leap_timer to force a hrtimer_reprogram which only happens
    if its set and there's no sooner timer to expire.
    
    NOTE: This patch, being faithful to the original commit,
    introduces a bug (we don't update wall_to_monotonic),
    which will be resovled by backporting a following fix.
    
    Original commit message below:
    
    Since commit 7dffa3c673fbcf835cd7be80bb4aec8ad3f51168 the ntp
    subsystem has used an hrtimer for triggering the leapsecond
    adjustment. However, this can cause a potential livelock.
    
    Thomas diagnosed this as the following pattern:
    CPU 0                                                    CPU 1
    do_adjtimex()
      spin_lock_irq(&ntp_lock);
        process_adjtimex_modes();                            timer_interrupt()
          process_adj_status();                                do_timer()
            ntp_start_leap_timer();                             write_lock(&xtime_lock);
              hrtimer_start();                                  update_wall_time();
                 hrtimer_reprogram();                            ntp_tick_length()
                   tick_program_event()                            spin_lock(&ntp_lock);
                     clockevents_program_event()
                       ktime_get()
                         seq = req_seqbegin(xtime_lock);
    
    This patch tries to avoid the problem by reverting back to not using
    an hrtimer to inject leapseconds, and instead we handle the leapsecond
    processing in the second_overflow() function.
    
    The downside to this change is that on systems that support highres
    timers, the leap second processing will occur on a HZ tick boundary,
    (ie: ~1-10ms, depending on HZ)  after the leap second instead of
    possibly sooner (~34us in my tests w/ x86_64 lapic).
    
    This patch applies on top of tip/timers/core.
    
    CC: Sasha Levin <levinsasha928@gmail.com>
    CC: Thomas Gleixner <tglx@linutronix.de>
    Reported-by: Sasha Levin <levinsasha928@gmail.com>
    Diagnoised-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Sasha Levin <levinsasha928@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linux Kernel <linux-kernel@vger.kernel.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

commit 6b0c81b3be114a93f79bd4c5639ade5107d77c21
Author: David Rientjes <rientjes@google.com>
Date:   Tue Jul 31 16:43:45 2012 -0700

    mm, oom: reduce dependency on tasklist_lock
    
    Since exiting tasks require write_lock_irq(&tasklist_lock) several times,
    try to reduce the amount of time the readside is held for oom kills.  This
    makes the interface with the memcg oom handler more consistent since it
    now never needs to take tasklist_lock unnecessarily.
    
    The only time the oom killer now takes tasklist_lock is when iterating the
    children of the selected task, everything else is protected by
    rcu_read_lock().
    
    This requires that a reference to the selected process, p, is grabbed
    before calling oom_kill_process().  It may release it and grab a reference
    on another one of p's threads if !p->mm, but it also guarantees that it
    will release the reference before returning.
    
    [hughd@google.com: fix duplicate put_task_struct()]
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 384ef0e62e409e52c80adef5b1ff83075377c19e
Author: Joe Thornber <ejt@redhat.com>
Date:   Fri Jul 27 15:08:09 2012 +0100

    dm persistent data: tidy transaction manager creation fns
    
    Tidy the transaction manager creation functions.
    
    They no longer lock the superblock.  Superblock locking is pulled out to
    the caller.
    
    Also export dm_bm_write_lock_zero.
    
    Signed-off-by: Joe Thornber <ejt@redhat.com>
    Signed-off-by: Mike Snitzer <snitzer@redhat.com>
    Signed-off-by: Alasdair G Kergon <agk@redhat.com>

commit a57ccabee60519dd90051266c00d038055b93878
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Jul 17 03:05:14 2012 -0400

    ntp: Fix leap-second hrtimer livelock
    
    This is a backport of 6b43ae8a619d17c4935c3320d2ef9e92bdeed05d
    
    This should have been backported when it was commited, but I
    mistook the problem as requiring the ntp_lock changes
    that landed in 3.4 in order for it to occur.
    
    Unfortunately the same issue can happen (with only one cpu)
    as follows:
    do_adjtimex()
     write_seqlock_irq(&xtime_lock);
      process_adjtimex_modes()
       process_adj_status()
        ntp_start_leap_timer()
         hrtimer_start()
          hrtimer_reprogram()
           tick_program_event()
            clockevents_program_event()
             ktime_get()
              seq = req_seqbegin(xtime_lock); [DEADLOCK]
    
    This deadlock will no always occur, as it requires the
    leap_timer to force a hrtimer_reprogram which only happens
    if its set and there's no sooner timer to expire.
    
    NOTE: This patch, being faithful to the original commit,
    introduces a bug (we don't update wall_to_monotonic),
    which will be resovled by backporting a following fix.
    
    Original commit message below:
    
    Since commit 7dffa3c673fbcf835cd7be80bb4aec8ad3f51168 the ntp
    subsystem has used an hrtimer for triggering the leapsecond
    adjustment. However, this can cause a potential livelock.
    
    Thomas diagnosed this as the following pattern:
    CPU 0                                                    CPU 1
    do_adjtimex()
      spin_lock_irq(&ntp_lock);
        process_adjtimex_modes();                            timer_interrupt()
          process_adj_status();                                do_timer()
            ntp_start_leap_timer();                             write_lock(&xtime_lock);
              hrtimer_start();                                  update_wall_time();
                 hrtimer_reprogram();                            ntp_tick_length()
                   tick_program_event()                            spin_lock(&ntp_lock);
                     clockevents_program_event()
                       ktime_get()
                         seq = req_seqbegin(xtime_lock);
    
    This patch tries to avoid the problem by reverting back to not using
    an hrtimer to inject leapseconds, and instead we handle the leapsecond
    processing in the second_overflow() function.
    
    The downside to this change is that on systems that support highres
    timers, the leap second processing will occur on a HZ tick boundary,
    (ie: ~1-10ms, depending on HZ)  after the leap second instead of
    possibly sooner (~34us in my tests w/ x86_64 lapic).
    
    This patch applies on top of tip/timers/core.
    
    CC: Sasha Levin <levinsasha928@gmail.com>
    CC: Thomas Gleixner <tglx@linutronix.de>
    Reported-by: Sasha Levin <levinsasha928@gmail.com>
    Diagnoised-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Sasha Levin <levinsasha928@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linux Kernel <linux-kernel@vger.kernel.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 9c24771f844b6f0708a72cd116953e0a128e5d2a
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Jul 17 13:33:48 2012 -0400

    ntp: Fix leap-second hrtimer livelock
    
    This is a backport of 6b43ae8a619d17c4935c3320d2ef9e92bdeed05d
    
    This should have been backported when it was commited, but I
    mistook the problem as requiring the ntp_lock changes
    that landed in 3.4 in order for it to occur.
    
    Unfortunately the same issue can happen (with only one cpu)
    as follows:
    do_adjtimex()
     write_seqlock_irq(&xtime_lock);
      process_adjtimex_modes()
       process_adj_status()
        ntp_start_leap_timer()
         hrtimer_start()
          hrtimer_reprogram()
           tick_program_event()
            clockevents_program_event()
             ktime_get()
              seq = req_seqbegin(xtime_lock); [DEADLOCK]
    
    This deadlock will no always occur, as it requires the
    leap_timer to force a hrtimer_reprogram which only happens
    if its set and there's no sooner timer to expire.
    
    NOTE: This patch, being faithful to the original commit,
    introduces a bug (we don't update wall_to_monotonic),
    which will be resovled by backporting a following fix.
    
    Original commit message below:
    
    Since commit 7dffa3c673fbcf835cd7be80bb4aec8ad3f51168 the ntp
    subsystem has used an hrtimer for triggering the leapsecond
    adjustment. However, this can cause a potential livelock.
    
    Thomas diagnosed this as the following pattern:
    CPU 0                                                    CPU 1
    do_adjtimex()
      spin_lock_irq(&ntp_lock);
        process_adjtimex_modes();                            timer_interrupt()
          process_adj_status();                                do_timer()
            ntp_start_leap_timer();                             write_lock(&xtime_lock);
              hrtimer_start();                                  update_wall_time();
                 hrtimer_reprogram();                            ntp_tick_length()
                   tick_program_event()                            spin_lock(&ntp_lock);
                     clockevents_program_event()
                       ktime_get()
                         seq = req_seqbegin(xtime_lock);
    
    This patch tries to avoid the problem by reverting back to not using
    an hrtimer to inject leapseconds, and instead we handle the leapsecond
    processing in the second_overflow() function.
    
    The downside to this change is that on systems that support highres
    timers, the leap second processing will occur on a HZ tick boundary,
    (ie: ~1-10ms, depending on HZ)  after the leap second instead of
    possibly sooner (~34us in my tests w/ x86_64 lapic).
    
    This patch applies on top of tip/timers/core.
    
    CC: Sasha Levin <levinsasha928@gmail.com>
    CC: Thomas Gleixner <tglx@linutronix.de>
    Reported-by: Sasha Levin <levinsasha928@gmail.com>
    Diagnoised-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Sasha Levin <levinsasha928@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ee0b2dd6344911d7769f9fd638d30f45e66b8410
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Jun 12 00:47:58 2012 +0200

    NFC: Return from rawsock_release when sk is NULL
    
    commit 03e934f620101ca2cfc9383bd76172dd3e1f8567 upstream.
    
    Sasha Levin reported following panic :
    
    [ 2136.383310] BUG: unable to handle kernel NULL pointer dereference at
    00000000000003b0
    [ 2136.384022] IP: [<ffffffff8114e400>] __lock_acquire+0xc0/0x4b0
    [ 2136.384022] PGD 131c4067 PUD 11c0c067 PMD 0
    [ 2136.388106] Oops: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC
    [ 2136.388106] CPU 1
    [ 2136.388106] Pid: 24855, comm: trinity-child1 Tainted: G        W
    3.5.0-rc2-sasha-00015-g7b268f7 #374
    [ 2136.388106] RIP: 0010:[<ffffffff8114e400>]  [<ffffffff8114e400>]
    __lock_acquire+0xc0/0x4b0
    [ 2136.388106] RSP: 0018:ffff8800130b3ca8  EFLAGS: 00010046
    [ 2136.388106] RAX: 0000000000000086 RBX: ffff88001186b000 RCX:
    0000000000000000
    [ 2136.388106] RDX: 0000000000000000 RSI: 0000000000000000 RDI:
    0000000000000000
    [ 2136.388106] RBP: ffff8800130b3d08 R08: 0000000000000001 R09:
    0000000000000000
    [ 2136.388106] R10: 0000000000000000 R11: 0000000000000001 R12:
    0000000000000002
    [ 2136.388106] R13: 00000000000003b0 R14: 0000000000000000 R15:
    0000000000000000
    [ 2136.388106] FS:  00007fa5b1bd4700(0000) GS:ffff88001b800000(0000)
    knlGS:0000000000000000
    [ 2136.388106] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [ 2136.388106] CR2: 00000000000003b0 CR3: 0000000011d1f000 CR4:
    00000000000406e0
    [ 2136.388106] DR0: 0000000000000000 DR1: 0000000000000000 DR2:
    0000000000000000
    [ 2136.388106] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7:
    0000000000000400
    [ 2136.388106] Process trinity-child1 (pid: 24855, threadinfo
    ffff8800130b2000, task ffff88001186b000)
    [ 2136.388106] Stack:
    [ 2136.388106]  ffff8800130b3cd8 ffffffff81121785 ffffffff81236774
    000080d000000001
    [ 2136.388106]  ffff88001b9d6c00 00000000001d6c00 ffffffff130b3d08
    ffff88001186b000
    [ 2136.388106]  0000000000000000 0000000000000002 0000000000000000
    0000000000000000
    [ 2136.388106] Call Trace:
    [ 2136.388106]  [<ffffffff81121785>] ? sched_clock_local+0x25/0x90
    [ 2136.388106]  [<ffffffff81236774>] ? get_empty_filp+0x74/0x220
    [ 2136.388106]  [<ffffffff8114e97a>] lock_acquire+0x18a/0x1e0
    [ 2136.388106]  [<ffffffff836b37df>] ? rawsock_release+0x4f/0xa0
    [ 2136.388106]  [<ffffffff837c0ef0>] _raw_write_lock_bh+0x40/0x80
    [ 2136.388106]  [<ffffffff836b37df>] ? rawsock_release+0x4f/0xa0
    [ 2136.388106]  [<ffffffff836b37df>] rawsock_release+0x4f/0xa0
    [ 2136.388106]  [<ffffffff8321cfe8>] sock_release+0x18/0x70
    [ 2136.388106]  [<ffffffff8321d069>] sock_close+0x29/0x30
    [ 2136.388106]  [<ffffffff81236bca>] __fput+0x11a/0x2c0
    [ 2136.388106]  [<ffffffff81236d85>] fput+0x15/0x20
    [ 2136.388106]  [<ffffffff8321de34>] sys_accept4+0x1b4/0x200
    [ 2136.388106]  [<ffffffff837c165c>] ? _raw_spin_unlock_irq+0x4c/0x80
    [ 2136.388106]  [<ffffffff837c1669>] ? _raw_spin_unlock_irq+0x59/0x80
    [ 2136.388106]  [<ffffffff837c2565>] ? sysret_check+0x22/0x5d
    [ 2136.388106]  [<ffffffff8321de8b>] sys_accept+0xb/0x10
    [ 2136.388106]  [<ffffffff837c2539>] system_call_fastpath+0x16/0x1b
    [ 2136.388106] Code: ec 04 00 0f 85 ea 03 00 00 be d5 0b 00 00 48 c7 c7
    8a c1 40 84 e8 b1 a5 f8 ff 31 c0 e9 d4 03 00 00 66 2e 0f 1f 84 00 00 00
    00 00 <49> 81 7d 00 60 73 5e 85 b8 01 00 00 00 44 0f 44 e0 83 fe 01 77
    [ 2136.388106] RIP  [<ffffffff8114e400>] __lock_acquire+0xc0/0x4b0
    [ 2136.388106]  RSP <ffff8800130b3ca8>
    [ 2136.388106] CR2: 00000000000003b0
    [ 2136.388106] ---[ end trace 6d450e935ee18982 ]---
    [ 2136.388106] Kernel panic - not syncing: Fatal exception in interrupt
    
    rawsock_release() should test if sock->sk is NULL before calling
    sock_orphan()/sock_put()
    
    Reported-by: Sasha Levin <levinsasha928@gmail.com>
    Tested-by: Sasha Levin <levinsasha928@gmail.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Samuel Ortiz <sameo@linux.intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit faa95fde43f758d0a9e0e3d8751dac79aae1f08d
Author: Axel Lin <axel.lin@gmail.com>
Date:   Wed Jul 11 19:44:13 2012 +0800

    regulator: tps65910: Remvoe tps65910_reg_[read|modify_bits|read_locked|write_locked] functions
    
    The tps65910 mfd driver has been converted to regmap APIs.
    This patch adds tps65910_reg_update_bits() in include/linux/mfd/tps65910.h.
    Thus we can use tps65910_reg_read/tps65910_reg_write/tps65910_reg_update_bits
    directly and remove tps65910_reg_[read|modify_bits|read_locked|write_locked]
    functions. With this change, we can also remove the mutex in struct tps65910_reg.
    
    Signed-off-by: Axel Lin <axel.lin@gmail.com>
    Tested-by: Laxman Dewangan <ldewangan@nvidia.com>
    Signed-off-by: Mark Brown <broonie@opensource.wolfsonmicro.com>

commit b82b566a46ef61f26f461f0c55e6588b2cc93384
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Jun 12 00:47:58 2012 +0200

    NFC: Return from rawsock_release when sk is NULL
    
    commit 03e934f620101ca2cfc9383bd76172dd3e1f8567 upstream.
    
    Sasha Levin reported following panic :
    
    [ 2136.383310] BUG: unable to handle kernel NULL pointer dereference at
    00000000000003b0
    [ 2136.384022] IP: [<ffffffff8114e400>] __lock_acquire+0xc0/0x4b0
    [ 2136.384022] PGD 131c4067 PUD 11c0c067 PMD 0
    [ 2136.388106] Oops: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC
    [ 2136.388106] CPU 1
    [ 2136.388106] Pid: 24855, comm: trinity-child1 Tainted: G        W
    3.5.0-rc2-sasha-00015-g7b268f7 #374
    [ 2136.388106] RIP: 0010:[<ffffffff8114e400>]  [<ffffffff8114e400>]
    __lock_acquire+0xc0/0x4b0
    [ 2136.388106] RSP: 0018:ffff8800130b3ca8  EFLAGS: 00010046
    [ 2136.388106] RAX: 0000000000000086 RBX: ffff88001186b000 RCX:
    0000000000000000
    [ 2136.388106] RDX: 0000000000000000 RSI: 0000000000000000 RDI:
    0000000000000000
    [ 2136.388106] RBP: ffff8800130b3d08 R08: 0000000000000001 R09:
    0000000000000000
    [ 2136.388106] R10: 0000000000000000 R11: 0000000000000001 R12:
    0000000000000002
    [ 2136.388106] R13: 00000000000003b0 R14: 0000000000000000 R15:
    0000000000000000
    [ 2136.388106] FS:  00007fa5b1bd4700(0000) GS:ffff88001b800000(0000)
    knlGS:0000000000000000
    [ 2136.388106] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [ 2136.388106] CR2: 00000000000003b0 CR3: 0000000011d1f000 CR4:
    00000000000406e0
    [ 2136.388106] DR0: 0000000000000000 DR1: 0000000000000000 DR2:
    0000000000000000
    [ 2136.388106] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7:
    0000000000000400
    [ 2136.388106] Process trinity-child1 (pid: 24855, threadinfo
    ffff8800130b2000, task ffff88001186b000)
    [ 2136.388106] Stack:
    [ 2136.388106]  ffff8800130b3cd8 ffffffff81121785 ffffffff81236774
    000080d000000001
    [ 2136.388106]  ffff88001b9d6c00 00000000001d6c00 ffffffff130b3d08
    ffff88001186b000
    [ 2136.388106]  0000000000000000 0000000000000002 0000000000000000
    0000000000000000
    [ 2136.388106] Call Trace:
    [ 2136.388106]  [<ffffffff81121785>] ? sched_clock_local+0x25/0x90
    [ 2136.388106]  [<ffffffff81236774>] ? get_empty_filp+0x74/0x220
    [ 2136.388106]  [<ffffffff8114e97a>] lock_acquire+0x18a/0x1e0
    [ 2136.388106]  [<ffffffff836b37df>] ? rawsock_release+0x4f/0xa0
    [ 2136.388106]  [<ffffffff837c0ef0>] _raw_write_lock_bh+0x40/0x80
    [ 2136.388106]  [<ffffffff836b37df>] ? rawsock_release+0x4f/0xa0
    [ 2136.388106]  [<ffffffff836b37df>] rawsock_release+0x4f/0xa0
    [ 2136.388106]  [<ffffffff8321cfe8>] sock_release+0x18/0x70
    [ 2136.388106]  [<ffffffff8321d069>] sock_close+0x29/0x30
    [ 2136.388106]  [<ffffffff81236bca>] __fput+0x11a/0x2c0
    [ 2136.388106]  [<ffffffff81236d85>] fput+0x15/0x20
    [ 2136.388106]  [<ffffffff8321de34>] sys_accept4+0x1b4/0x200
    [ 2136.388106]  [<ffffffff837c165c>] ? _raw_spin_unlock_irq+0x4c/0x80
    [ 2136.388106]  [<ffffffff837c1669>] ? _raw_spin_unlock_irq+0x59/0x80
    [ 2136.388106]  [<ffffffff837c2565>] ? sysret_check+0x22/0x5d
    [ 2136.388106]  [<ffffffff8321de8b>] sys_accept+0xb/0x10
    [ 2136.388106]  [<ffffffff837c2539>] system_call_fastpath+0x16/0x1b
    [ 2136.388106] Code: ec 04 00 0f 85 ea 03 00 00 be d5 0b 00 00 48 c7 c7
    8a c1 40 84 e8 b1 a5 f8 ff 31 c0 e9 d4 03 00 00 66 2e 0f 1f 84 00 00 00
    00 00 <49> 81 7d 00 60 73 5e 85 b8 01 00 00 00 44 0f 44 e0 83 fe 01 77
    [ 2136.388106] RIP  [<ffffffff8114e400>] __lock_acquire+0xc0/0x4b0
    [ 2136.388106]  RSP <ffff8800130b3ca8>
    [ 2136.388106] CR2: 00000000000003b0
    [ 2136.388106] ---[ end trace 6d450e935ee18982 ]---
    [ 2136.388106] Kernel panic - not syncing: Fatal exception in interrupt
    
    rawsock_release() should test if sock->sk is NULL before calling
    sock_orphan()/sock_put()
    
    Reported-by: Sasha Levin <levinsasha928@gmail.com>
    Tested-by: Sasha Levin <levinsasha928@gmail.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Samuel Ortiz <sameo@linux.intel.com>
    [bwh: Backported to 3.2: keep using nfc_dbg(), not pr_debug()]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 03e934f620101ca2cfc9383bd76172dd3e1f8567
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Jun 12 00:47:58 2012 +0200

    NFC: Return from rawsock_release when sk is NULL
    
    Sasha Levin reported following panic :
    
    [ 2136.383310] BUG: unable to handle kernel NULL pointer dereference at
    00000000000003b0
    [ 2136.384022] IP: [<ffffffff8114e400>] __lock_acquire+0xc0/0x4b0
    [ 2136.384022] PGD 131c4067 PUD 11c0c067 PMD 0
    [ 2136.388106] Oops: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC
    [ 2136.388106] CPU 1
    [ 2136.388106] Pid: 24855, comm: trinity-child1 Tainted: G        W
    3.5.0-rc2-sasha-00015-g7b268f7 #374
    [ 2136.388106] RIP: 0010:[<ffffffff8114e400>]  [<ffffffff8114e400>]
    __lock_acquire+0xc0/0x4b0
    [ 2136.388106] RSP: 0018:ffff8800130b3ca8  EFLAGS: 00010046
    [ 2136.388106] RAX: 0000000000000086 RBX: ffff88001186b000 RCX:
    0000000000000000
    [ 2136.388106] RDX: 0000000000000000 RSI: 0000000000000000 RDI:
    0000000000000000
    [ 2136.388106] RBP: ffff8800130b3d08 R08: 0000000000000001 R09:
    0000000000000000
    [ 2136.388106] R10: 0000000000000000 R11: 0000000000000001 R12:
    0000000000000002
    [ 2136.388106] R13: 00000000000003b0 R14: 0000000000000000 R15:
    0000000000000000
    [ 2136.388106] FS:  00007fa5b1bd4700(0000) GS:ffff88001b800000(0000)
    knlGS:0000000000000000
    [ 2136.388106] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [ 2136.388106] CR2: 00000000000003b0 CR3: 0000000011d1f000 CR4:
    00000000000406e0
    [ 2136.388106] DR0: 0000000000000000 DR1: 0000000000000000 DR2:
    0000000000000000
    [ 2136.388106] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7:
    0000000000000400
    [ 2136.388106] Process trinity-child1 (pid: 24855, threadinfo
    ffff8800130b2000, task ffff88001186b000)
    [ 2136.388106] Stack:
    [ 2136.388106]  ffff8800130b3cd8 ffffffff81121785 ffffffff81236774
    000080d000000001
    [ 2136.388106]  ffff88001b9d6c00 00000000001d6c00 ffffffff130b3d08
    ffff88001186b000
    [ 2136.388106]  0000000000000000 0000000000000002 0000000000000000
    0000000000000000
    [ 2136.388106] Call Trace:
    [ 2136.388106]  [<ffffffff81121785>] ? sched_clock_local+0x25/0x90
    [ 2136.388106]  [<ffffffff81236774>] ? get_empty_filp+0x74/0x220
    [ 2136.388106]  [<ffffffff8114e97a>] lock_acquire+0x18a/0x1e0
    [ 2136.388106]  [<ffffffff836b37df>] ? rawsock_release+0x4f/0xa0
    [ 2136.388106]  [<ffffffff837c0ef0>] _raw_write_lock_bh+0x40/0x80
    [ 2136.388106]  [<ffffffff836b37df>] ? rawsock_release+0x4f/0xa0
    [ 2136.388106]  [<ffffffff836b37df>] rawsock_release+0x4f/0xa0
    [ 2136.388106]  [<ffffffff8321cfe8>] sock_release+0x18/0x70
    [ 2136.388106]  [<ffffffff8321d069>] sock_close+0x29/0x30
    [ 2136.388106]  [<ffffffff81236bca>] __fput+0x11a/0x2c0
    [ 2136.388106]  [<ffffffff81236d85>] fput+0x15/0x20
    [ 2136.388106]  [<ffffffff8321de34>] sys_accept4+0x1b4/0x200
    [ 2136.388106]  [<ffffffff837c165c>] ? _raw_spin_unlock_irq+0x4c/0x80
    [ 2136.388106]  [<ffffffff837c1669>] ? _raw_spin_unlock_irq+0x59/0x80
    [ 2136.388106]  [<ffffffff837c2565>] ? sysret_check+0x22/0x5d
    [ 2136.388106]  [<ffffffff8321de8b>] sys_accept+0xb/0x10
    [ 2136.388106]  [<ffffffff837c2539>] system_call_fastpath+0x16/0x1b
    [ 2136.388106] Code: ec 04 00 0f 85 ea 03 00 00 be d5 0b 00 00 48 c7 c7
    8a c1 40 84 e8 b1 a5 f8 ff 31 c0 e9 d4 03 00 00 66 2e 0f 1f 84 00 00 00
    00 00 <49> 81 7d 00 60 73 5e 85 b8 01 00 00 00 44 0f 44 e0 83 fe 01 77
    [ 2136.388106] RIP  [<ffffffff8114e400>] __lock_acquire+0xc0/0x4b0
    [ 2136.388106]  RSP <ffff8800130b3ca8>
    [ 2136.388106] CR2: 00000000000003b0
    [ 2136.388106] ---[ end trace 6d450e935ee18982 ]---
    [ 2136.388106] Kernel panic - not syncing: Fatal exception in interrupt
    
    rawsock_release() should test if sock->sk is NULL before calling
    sock_orphan()/sock_put()
    
    Reported-by: Sasha Levin <levinsasha928@gmail.com>
    Tested-by: Sasha Levin <levinsasha928@gmail.com>
    Cc: stable@kernel.org
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Samuel Ortiz <sameo@linux.intel.com>

commit b026f0c7a312ef36edb451d2b66c543b9488b4fd
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jul 1 17:30:00 2011 -0700

    6pack,mkiss: fix lock inconsistency
    
    commit 6e4e2f811bade330126d4029c88c831784a7efd9 upstream.
    
    Lockdep found a locking inconsistency in the mkiss_close function:
    
    > kernel: [ INFO: inconsistent lock state ]
    > kernel: 2.6.39.1 #3
    > kernel: ---------------------------------
    > kernel: inconsistent {IN-SOFTIRQ-R} -> {SOFTIRQ-ON-W} usage.
    > kernel: ax25ipd/2813 [HC0[0]:SC0[0]:HE1:SE1] takes:
    > kernel: (disc_data_lock){+++?.-}, at: [<ffffffffa018552b>] mkiss_close+0x1b/0x90 [mkiss]
    > kernel: {IN-SOFTIRQ-R} state was registered at:
    
    The message hints that disc_data_lock is aquired with softirqs disabled,
    but does not itself disable softirqs, which can in rare circumstances
    lead to a deadlock.
    The same problem is present in the 6pack driver, this patch fixes both
    by using write_lock_bh instead of write_lock.
    
    Reported-by: Bernard F6BVP <f6bvp@free.fr>
    Tested-by: Bernard F6BVP <f6bvp@free.fr>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Ralf Baechle<ralf@linux-mips.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

commit 724d7ad550fb6ed11ce40425eb0206f90faf665a
Author: Alexander Stein <alexander.stein@systec-electronic.com>
Date:   Wed Jun 22 17:05:33 2011 +0200

    pch_dma: Fix channel locking
    
    commit 70f18915846f092e0e1c988f1726a532fa3ab3a1 upstream.
    
    Fix for the following INFO message
    
    =================================
    [ INFO: inconsistent lock state ]
    2.6.39+ #89
    ---------------------------------
    inconsistent {HARDIRQ-ON-W} -> {IN-HARDIRQ-W} usage.
    rs232/822 [HC1[1]:SC0[0]:HE0:SE1] takes:
     (&(&pd_chan->lock)->rlock){?.....}, at: [<c123b9a1>] pdc_desc_get+0x16/0xab
    {HARDIRQ-ON-W} state was registered at:
      [<c104fe28>] mark_irqflags+0xbd/0x11a
      [<c1050386>] __lock_acquire+0x501/0x6bb
      [<c1050945>] lock_acquire+0x63/0x7b
      [<c131c51d>] _raw_spin_lock_bh+0x43/0x51
      [<c123bee4>] pd_alloc_chan_resources+0x92/0x11e
      [<c123ad62>] dma_chan_get+0x9b/0x107
      [<c123b2d1>] __dma_request_channel+0x61/0xdc
      [<c11ba24b>] pch_request_dma+0x61/0x19e
      [<c11bb3b8>] pch_uart_startup+0x16a/0x1a2
      [<c11b8446>] uart_startup+0x87/0x147
      [<c11b9183>] uart_open+0x117/0x13e
      [<c11a5c7d>] tty_open+0x23c/0x34c
      [<c1097705>] chrdev_open+0x140/0x15f
      [<c10930a6>] __dentry_open.clone.14+0x14a/0x22b
      [<c1093dfb>] nameidata_to_filp+0x36/0x40
      [<c109f28b>] do_last+0x513/0x635
      [<c109f4af>] path_openat+0x9c/0x2aa
      [<c109f6e4>] do_filp_open+0x27/0x69
      [<c1093f02>] do_sys_open+0xfd/0x184
      [<c1093fad>] sys_open+0x24/0x2a
      [<c131d58c>] sysenter_do_call+0x12/0x32
    irq event stamp: 2522
    hardirqs last  enabled at (2521): [<c131ca3b>] _raw_spin_unlock_irqrestore+0x36/0x52
    hardirqs last disabled at (2522): [<c131db27>] common_interrupt+0x27/0x34
    softirqs last  enabled at (2354): [<c102fa11>] __do_softirq+0x10a/0x11a
    softirqs last disabled at (2299): [<c10041a4>] do_softirq+0x57/0xa4
    
    other info that might help us debug this:
    2 locks held by rs232/822:
     #0:  (&tty->atomic_write_lock){+.+.+.}, at: [<c11a4b7a>] tty_write_lock+0x14/0x3c
     #1:  (&port_lock_key){-.....}, at: [<c11bad72>] pch_uart_interrupt+0x17/0x1e9
    
    stack backtrace:
    Pid: 822, comm: rs232 Not tainted 2.6.39+ #89
    Call Trace:
     [<c1319f90>] ? printk+0x19/0x1b
     [<c104f893>] print_usage_bug+0x184/0x18f
     [<c104e5b1>] ? print_irq_inversion_bug+0x10e/0x10e
     [<c104f943>] mark_lock_irq+0xa5/0x1f6
     [<c104fc9c>] mark_lock+0x208/0x2d7
     [<c104fdc0>] mark_irqflags+0x55/0x11a
     [<c1050386>] __lock_acquire+0x501/0x6bb
     [<c10042ee>] ? dump_trace+0x92/0xb6
     [<c1050945>] lock_acquire+0x63/0x7b
     [<c123b9a1>] ? pdc_desc_get+0x16/0xab
     [<c131c2d0>] _raw_spin_lock+0x3e/0x4c
     [<c123b9a1>] ? pdc_desc_get+0x16/0xab
     [<c123b9a1>] pdc_desc_get+0x16/0xab
     [<c10504d8>] ? __lock_acquire+0x653/0x6bb
     [<c123bb2c>] pd_prep_slave_sg+0x7c/0x1cb
     [<c1006c3f>] ? nommu_map_sg+0x6e/0x81
     [<c11bace6>] dma_handle_tx+0x2cf/0x344
     [<c11bad72>] ? pch_uart_interrupt+0x17/0x1e9
     [<c11baebb>] pch_uart_interrupt+0x160/0x1e9
     [<c10642fb>] handle_irq_event_percpu+0x25/0x127
     [<c1064429>] handle_irq_event+0x2c/0x43
     [<c1065e0d>] ? handle_fasteoi_irq+0x84/0x84
     [<c1065eb9>] handle_edge_irq+0xac/0xce
     <IRQ>  [<c1003ecb>] ? do_IRQ+0x38/0x9d
     [<c131db2e>] ? common_interrupt+0x2e/0x34
     [<c105007b>] ? __lock_acquire+0x1f6/0x6bb
     [<c131ca3d>] ? _raw_spin_unlock_irqrestore+0x38/0x52
     [<c11b798b>] ? uart_start+0x2d/0x32
     [<c11b7998>] ? uart_flush_chars+0x8/0xa
     [<c11a7962>] ? n_tty_write+0x12c/0x1c6
     [<c1027a73>] ? try_to_wake_up+0x251/0x251
     [<c11a4d0b>] ? tty_write+0x169/0x1dc
     [<c11a7836>] ? n_tty_ioctl+0xb7/0xb7
     [<c1094841>] ? vfs_write+0x91/0x10d
     [<c11a4ba2>] ? tty_write_lock+0x3c/0x3c
     [<c1094a69>] ? sys_write+0x3e/0x63
     [<c131d58c>] ? sysenter_do_call+0x12/0x32
    
    Signed-off-by: Alexander Stein <alexander.stein@systec-electronic.com>
    Tested-by: Tomoya MORINAGA <tomoya-linux@dsn.okisemi.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>
    Signed-off-by: Tomoya MORINAGA <tomoya.rohm@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f7c79f30cb2d3883488e70cafc9e3a7edd4b9fdb
Author: Chris Mason <chris.mason@oracle.com>
Date:   Mon Mar 19 15:54:38 2012 -0400

    Btrfs: adjust the write_lock_level as we unlock
    
    btrfs_search_slot sometimes needs write locks on high levels of
    the tree.  It remembers the highest level that needs a write lock
    and will use that for all future searches through the tree in a given
    call.
    
    But, very often we'll just cow the top level or the level below and we
    won't really need write locks on the root again after that.  This patch
    changes things to adjust the write lock requirement as it unlocks
    levels.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

commit 6b43ae8a619d17c4935c3320d2ef9e92bdeed05d
Author: John Stultz <john.stultz@linaro.org>
Date:   Thu Mar 15 13:04:03 2012 -0700

    ntp: Fix leap-second hrtimer livelock
    
    Since commit 7dffa3c673fbcf835cd7be80bb4aec8ad3f51168 the ntp
    subsystem has used an hrtimer for triggering the leapsecond
    adjustment. However, this can cause a potential livelock.
    
    Thomas diagnosed this as the following pattern:
    CPU 0                                                    CPU 1
    do_adjtimex()
      spin_lock_irq(&ntp_lock);
        process_adjtimex_modes();                            timer_interrupt()
          process_adj_status();                                do_timer()
            ntp_start_leap_timer();                             write_lock(&xtime_lock);
              hrtimer_start();                                  update_wall_time();
                 hrtimer_reprogram();                            ntp_tick_length()
                   tick_program_event()                            spin_lock(&ntp_lock);
                     clockevents_program_event()
                       ktime_get()
                         seq = req_seqbegin(xtime_lock);
    
    This patch tries to avoid the problem by reverting back to not using
    an hrtimer to inject leapseconds, and instead we handle the leapsecond
    processing in the second_overflow() function.
    
    The downside to this change is that on systems that support highres
    timers, the leap second processing will occur on a HZ tick boundary,
    (ie: ~1-10ms, depending on HZ)  after the leap second instead of
    possibly sooner (~34us in my tests w/ x86_64 lapic).
    
    This patch applies on top of tip/timers/core.
    
    CC: Sasha Levin <levinsasha928@gmail.com>
    CC: Thomas Gleixner <tglx@linutronix.de>
    Reported-by: Sasha Levin <levinsasha928@gmail.com>
    Diagnoised-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Sasha Levin <levinsasha928@gmail.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit bf097aaff45eba2244ca3a61e531ec6f2801f6b2
Author: Jan Kara <jack@suse.cz>
Date:   Tue Feb 14 13:28:01 2012 +0100

    quota: Make quota code not call tty layer with dqptr_sem held
    
    dqptr_sem can be called from slab reclaim. tty layer uses GFP_KERNEL mask for
    allocation so it can end up calling slab reclaim. Given quota code can call
    into tty layer to print warning this creates possibility for lock inversion
    between tty->atomic_write_lock and dqptr_sem.
    
    Using direct printing of warnings from quota layer is obsolete but since it's
    easy enough to change quota code to not hold any locks when printing warnings,
    let's just do it. It seems like a good thing to do even when we use netlink
    layer to transmit warnings to userspace.
    
    Reported-by: Markus <M4rkusXXL@web.de>
    Signed-off-by: Jan Kara <jack@suse.cz>

commit 3ce3230a0cff484e5130153f244d4fb8a56b3a8b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Feb 8 03:37:27 2012 +0100

    cgroup: Walk task list under tasklist_lock in cgroup_enable_task_cg_list
    
    Walking through the tasklist in cgroup_enable_task_cg_list() inside
    an RCU read side critical section is not enough because:
    
    - RCU is not (yet) safe against while_each_thread()
    
    - If we use only RCU, a forking task that has passed cgroup_post_fork()
      without seeing use_task_css_set_links == 1 is not guaranteed to have
      its child immediately visible in the tasklist if we walk through it
      remotely with RCU. In this case it will be missing in its css_set's
      task list.
    
    Thus we need to traverse the list (unfortunately) under the
    tasklist_lock. It makes us safe against while_each_thread() and also
    make sure we see all forked task that have been added to the tasklist.
    
    As a secondary effect, reading and writing use_task_css_set_links are
    now well ordered against tasklist traversing and modification. The new
    layout is:
    
    CPU 0                                      CPU 1
    
    use_task_css_set_links = 1                write_lock(tasklist_lock)
    read_lock(tasklist_lock)                  add task to tasklist
    do_each_thread() {                        write_unlock(tasklist_lock)
            add thread to css set links       if (use_task_css_set_links)
    } while_each_thread()                         add thread to css set links
    read_unlock(tasklist_lock)
    
    If CPU 0 traverse the list after the task has been added to the tasklist
    then it is correctly added to the css set links. OTOH if CPU 0 traverse
    the tasklist before the new task had the opportunity to be added to the
    tasklist because it was too early in the fork process, then CPU 1
    catches up and add the task to the css set links after it added the task
    to the tasklist. The right value of use_task_css_set_links is guaranteed
    to be visible from CPU 1 due to the LOCK/UNLOCK implicit barrier properties:
    the read_unlock on CPU 0 makes the write on use_task_css_set_links happening
    and the write_lock on CPU 1 make the read of use_task_css_set_links that comes
    afterward to return the correct value.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Mandeep Singh Baines <msb@chromium.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 9b467e6ebebbe75288aeb7e816ffbb5d35d6eaa3
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 10 15:11:11 2012 -0800

    reiserfs: don't lock root inode searching
    
    Nothing requires that we lock the filesystem until the root inode is
    provided.
    
    Also iget5_locked() triggers a warning because we are holding the
    filesystem lock while allocating the inode, which result in a lockdep
    suspicion that we have a lock inversion against the reclaim path:
    
    [ 1986.896979] =================================
    [ 1986.896990] [ INFO: inconsistent lock state ]
    [ 1986.896997] 3.1.1-main #8
    [ 1986.897001] ---------------------------------
    [ 1986.897007] inconsistent {RECLAIM_FS-ON-W} -> {IN-RECLAIM_FS-W} usage.
    [ 1986.897016] kswapd0/16 [HC0[0]:SC0[0]:HE1:SE1] takes:
    [ 1986.897023]  (&REISERFS_SB(s)->lock){+.+.?.}, at: [<c01f8bd4>] reiserfs_write_lock+0x20/0x2a
    [ 1986.897044] {RECLAIM_FS-ON-W} state was registered at:
    [ 1986.897050]   [<c014a5b9>] mark_held_locks+0xae/0xd0
    [ 1986.897060]   [<c014aab3>] lockdep_trace_alloc+0x7d/0x91
    [ 1986.897068]   [<c0190ee0>] kmem_cache_alloc+0x1a/0x93
    [ 1986.897078]   [<c01e7728>] reiserfs_alloc_inode+0x13/0x3d
    [ 1986.897088]   [<c01a5b06>] alloc_inode+0x14/0x5f
    [ 1986.897097]   [<c01a5cb9>] iget5_locked+0x62/0x13a
    [ 1986.897106]   [<c01e99e0>] reiserfs_fill_super+0x410/0x8b9
    [ 1986.897114]   [<c01953da>] mount_bdev+0x10b/0x159
    [ 1986.897123]   [<c01e764d>] get_super_block+0x10/0x12
    [ 1986.897131]   [<c0195b38>] mount_fs+0x59/0x12d
    [ 1986.897138]   [<c01a80d1>] vfs_kern_mount+0x45/0x7a
    [ 1986.897147]   [<c01a83e3>] do_kern_mount+0x2f/0xb0
    [ 1986.897155]   [<c01a987a>] do_mount+0x5c2/0x612
    [ 1986.897163]   [<c01a9a72>] sys_mount+0x61/0x8f
    [ 1986.897170]   [<c044060c>] sysenter_do_call+0x12/0x32
    [ 1986.897181] irq event stamp: 7509691
    [ 1986.897186] hardirqs last  enabled at (7509691): [<c0190f34>] kmem_cache_alloc+0x6e/0x93
    [ 1986.897197] hardirqs last disabled at (7509690): [<c0190eea>] kmem_cache_alloc+0x24/0x93
    [ 1986.897209] softirqs last  enabled at (7508896): [<c01294bd>] __do_softirq+0xee/0xfd
    [ 1986.897222] softirqs last disabled at (7508859): [<c01030ed>] do_softirq+0x50/0x9d
    [ 1986.897234]
    [ 1986.897235] other info that might help us debug this:
    [ 1986.897242]  Possible unsafe locking scenario:
    [ 1986.897244]
    [ 1986.897250]        CPU0
    [ 1986.897254]        ----
    [ 1986.897257]   lock(&REISERFS_SB(s)->lock);
    [ 1986.897265] <Interrupt>
    [ 1986.897269]     lock(&REISERFS_SB(s)->lock);
    [ 1986.897276]
    [ 1986.897277]  *** DEADLOCK ***
    [ 1986.897278]
    [ 1986.897286] no locks held by kswapd0/16.
    [ 1986.897291]
    [ 1986.897292] stack backtrace:
    [ 1986.897299] Pid: 16, comm: kswapd0 Not tainted 3.1.1-main #8
    [ 1986.897306] Call Trace:
    [ 1986.897314]  [<c0439e76>] ? printk+0xf/0x11
    [ 1986.897324]  [<c01482d1>] print_usage_bug+0x20e/0x21a
    [ 1986.897332]  [<c01479b8>] ? print_irq_inversion_bug+0x172/0x172
    [ 1986.897341]  [<c014855c>] mark_lock+0x27f/0x483
    [ 1986.897349]  [<c0148d88>] __lock_acquire+0x628/0x1472
    [ 1986.897358]  [<c0149fae>] lock_acquire+0x47/0x5e
    [ 1986.897366]  [<c01f8bd4>] ? reiserfs_write_lock+0x20/0x2a
    [ 1986.897384]  [<c01f8bd4>] ? reiserfs_write_lock+0x20/0x2a
    [ 1986.897397]  [<c043b5ef>] mutex_lock_nested+0x35/0x26f
    [ 1986.897409]  [<c01f8bd4>] ? reiserfs_write_lock+0x20/0x2a
    [ 1986.897421]  [<c01f8bd4>] reiserfs_write_lock+0x20/0x2a
    [ 1986.897433]  [<c01e2edd>] map_block_for_writepage+0xc9/0x590
    [ 1986.897448]  [<c01b1706>] ? create_empty_buffers+0x33/0x8f
    [ 1986.897461]  [<c0121124>] ? get_parent_ip+0xb/0x31
    [ 1986.897472]  [<c043ef7f>] ? sub_preempt_count+0x81/0x8e
    [ 1986.897485]  [<c043cae0>] ? _raw_spin_unlock+0x27/0x3d
    [ 1986.897496]  [<c0121124>] ? get_parent_ip+0xb/0x31
    [ 1986.897508]  [<c01e355d>] reiserfs_writepage+0x1b9/0x3e7
    [ 1986.897521]  [<c0173b40>] ? clear_page_dirty_for_io+0xcb/0xde
    [ 1986.897533]  [<c014a6e3>] ? trace_hardirqs_on_caller+0x108/0x138
    [ 1986.897546]  [<c014a71e>] ? trace_hardirqs_on+0xb/0xd
    [ 1986.897559]  [<c0177b38>] shrink_page_list+0x34f/0x5e2
    [ 1986.897572]  [<c01780a7>] shrink_inactive_list+0x172/0x22c
    [ 1986.897585]  [<c0178464>] shrink_zone+0x303/0x3b1
    [ 1986.897597]  [<c043cae0>] ? _raw_spin_unlock+0x27/0x3d
    [ 1986.897611]  [<c01788c9>] kswapd+0x3b7/0x5f2
    
    The deadlock shouldn't happen since we are doing that allocation in the
    mount path, the filesystem is not available for any reclaim.  Still the
    warning is annoying.
    
    To solve this, acquire the lock later only where we need it, right before
    calling reiserfs_read_locked_inode() that wants to lock to walk the tree.
    
    Reported-by: Knut Petersen <Knut_Petersen@t-online.de>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Jan Kara <jack@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 27c57858531c4829a1446ebb5fd606d07846b2e5
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu Dec 22 02:45:29 2011 +0530

    VFS: Fix race between CPU hotplug and lglocks
    
    commit e30e2fdfe56288576ee9e04dbb06b4bd5f282203 upstream.
    
    Currently, the *_global_[un]lock_online() routines are not at all synchronized
    with CPU hotplug. Soft-lockups detected as a consequence of this race was
    reported earlier at https://lkml.org/lkml/2011/8/24/185. (Thanks to Cong Meng
    for finding out that the root-cause of this issue is the race condition
    between br_write_[un]lock() and CPU hotplug, which results in the lock states
    getting messed up).
    
    Fixing this race by just adding {get,put}_online_cpus() at appropriate places
    in *_global_[un]lock_online() is not a good option, because, then suddenly
    br_write_[un]lock() would become blocking, whereas they have been kept as
    non-blocking all this time, and we would want to keep them that way.
    
    So, overall, we want to ensure 3 things:
    1. br_write_lock() and br_write_unlock() must remain as non-blocking.
    2. The corresponding lock and unlock of the per-cpu spinlocks must not happen
       for different sets of CPUs.
    3. Either prevent any new CPU online operation in between this lock-unlock, or
       ensure that the newly onlined CPU does not proceed with its corresponding
       per-cpu spinlock unlocked.
    
    To achieve all this:
    (a) We introduce a new spinlock that is taken by the *_global_lock_online()
        routine and released by the *_global_unlock_online() routine.
    (b) We register a callback for CPU hotplug notifications, and this callback
        takes the same spinlock as above.
    (c) We maintain a bitmap which is close to the cpu_online_mask, and once it is
        initialized in the lock_init() code, all future updates to it are done in
        the callback, under the above spinlock.
    (d) The above bitmap is used (instead of cpu_online_mask) while locking and
        unlocking the per-cpu locks.
    
    The callback takes the spinlock upon the CPU_UP_PREPARE event. So, if the
    br_write_lock-unlock sequence is in progress, the callback keeps spinning,
    thus preventing the CPU online operation till the lock-unlock sequence is
    complete. This takes care of requirement (3).
    
    The bitmap that we maintain remains unmodified throughout the lock-unlock
    sequence, since all updates to it are managed by the callback, which takes
    the same spinlock as the one taken by the lock code and released only by the
    unlock routine. Combining this with (d) above, satisfies requirement (2).
    
    Overall, since we use a spinlock (mentioned in (a)) to prevent CPU hotplug
    operations from racing with br_write_lock-unlock, requirement (1) is also
    taken care of.
    
    By the way, it is to be noted that a CPU offline operation can actually run
    in parallel with our lock-unlock sequence, because our callback doesn't react
    to notifications earlier than CPU_DEAD (in order to maintain our bitmap
    properly). And this means, since we use our own bitmap (which is stale, on
    purpose) during the lock-unlock sequence, we could end up unlocking the
    per-cpu lock of an offline CPU (because we had locked it earlier, when the
    CPU was online), in order to satisfy requirement (2). But this is harmless,
    though it looks a bit awkward.
    
    Debugged-by: Cong Meng <mc@linux.vnet.ibm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit f3545737cf06d342d34483b7a8421d0bb90b9c1b
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu Dec 22 02:45:29 2011 +0530

    VFS: Fix race between CPU hotplug and lglocks
    
    commit e30e2fdfe56288576ee9e04dbb06b4bd5f282203 upstream.
    
    Currently, the *_global_[un]lock_online() routines are not at all synchronized
    with CPU hotplug. Soft-lockups detected as a consequence of this race was
    reported earlier at https://lkml.org/lkml/2011/8/24/185. (Thanks to Cong Meng
    for finding out that the root-cause of this issue is the race condition
    between br_write_[un]lock() and CPU hotplug, which results in the lock states
    getting messed up).
    
    Fixing this race by just adding {get,put}_online_cpus() at appropriate places
    in *_global_[un]lock_online() is not a good option, because, then suddenly
    br_write_[un]lock() would become blocking, whereas they have been kept as
    non-blocking all this time, and we would want to keep them that way.
    
    So, overall, we want to ensure 3 things:
    1. br_write_lock() and br_write_unlock() must remain as non-blocking.
    2. The corresponding lock and unlock of the per-cpu spinlocks must not happen
       for different sets of CPUs.
    3. Either prevent any new CPU online operation in between this lock-unlock, or
       ensure that the newly onlined CPU does not proceed with its corresponding
       per-cpu spinlock unlocked.
    
    To achieve all this:
    (a) We introduce a new spinlock that is taken by the *_global_lock_online()
        routine and released by the *_global_unlock_online() routine.
    (b) We register a callback for CPU hotplug notifications, and this callback
        takes the same spinlock as above.
    (c) We maintain a bitmap which is close to the cpu_online_mask, and once it is
        initialized in the lock_init() code, all future updates to it are done in
        the callback, under the above spinlock.
    (d) The above bitmap is used (instead of cpu_online_mask) while locking and
        unlocking the per-cpu locks.
    
    The callback takes the spinlock upon the CPU_UP_PREPARE event. So, if the
    br_write_lock-unlock sequence is in progress, the callback keeps spinning,
    thus preventing the CPU online operation till the lock-unlock sequence is
    complete. This takes care of requirement (3).
    
    The bitmap that we maintain remains unmodified throughout the lock-unlock
    sequence, since all updates to it are managed by the callback, which takes
    the same spinlock as the one taken by the lock code and released only by the
    unlock routine. Combining this with (d) above, satisfies requirement (2).
    
    Overall, since we use a spinlock (mentioned in (a)) to prevent CPU hotplug
    operations from racing with br_write_lock-unlock, requirement (1) is also
    taken care of.
    
    By the way, it is to be noted that a CPU offline operation can actually run
    in parallel with our lock-unlock sequence, because our callback doesn't react
    to notifications earlier than CPU_DEAD (in order to maintain our bitmap
    properly). And this means, since we use our own bitmap (which is stale, on
    purpose) during the lock-unlock sequence, we could end up unlocking the
    per-cpu lock of an offline CPU (because we had locked it earlier, when the
    CPU was online), in order to satisfy requirement (2). But this is harmless,
    though it looks a bit awkward.
    
    Debugged-by: Cong Meng <mc@linux.vnet.ibm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 9837d8e982b7e87a7207f90618e45d460e196e6a
Author: Jan Kara <jack@suse.cz>
Date:   Wed Jan 4 22:03:11 2012 -0500

    jbd2: fix hung processes in jbd2_journal_lock_updates()
    
    Toshiyuki Okajima found out that when running
    
    for ((i=0; i < 100000; i++)); do
            if ((i%2 == 0)); then
                    chattr +j /mnt/file
            else
                    chattr -j /mnt/file
            fi
            echo "0" >> /mnt/file
    done
    
    process sometimes hangs indefinitely in jbd2_journal_lock_updates().
    
    Toshiyuki identified that the following race happens:
    
    jbd2_journal_lock_updates()            |jbd2_journal_stop()
    ---------------------------------------+---------------------------------------
     write_lock(&journal->j_state_lock)    |    .
     ++journal->j_barrier_count            |    .
     spin_lock(&tran->t_handle_lock)       |    .
     atomic_read(&tran->t_updates) //not 0 |
                                           | atomic_dec_and_test(&tran->t_updates)
                                           |    // t_updates = 0
                                           | wake_up(&journal->j_wait_updates)
     prepare_to_wait()                     |    // no process is woken up.
     spin_unlock(&tran->t_handle_lock)     |
     write_unlock(&journal->j_state_lock)  |
     schedule() // never return            |
    
    We fix the problem by first calling prepare_to_wait() and only after that
    checking t_updates in jbd2_journal_lock_updates().
    
    Reported-and-analyzed-by: Toshiyuki Okajima <toshi.okajima@jp.fujitsu.com>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

commit 32b293a53deeb220769f9a29357cb151cfb8ee26
Author: Josh Hunt <joshhunt00@gmail.com>
Date:   Wed Dec 28 13:23:07 2011 +0000

    IPv6: Avoid taking write lock for /proc/net/ipv6_route
    
    During some debugging I needed to look into how /proc/net/ipv6_route
    operated and in my digging I found its calling fib6_clean_all() which uses
    "write_lock_bh(&table->tb6_lock)" before doing the walk of the table. I
    found this on 2.6.32, but reading the code I believe the same basic idea
    exists currently. Looking at the rtnetlink code they are only calling
    "read_lock_bh(&table->tb6_lock);" via fib6_dump_table(). While I realize
    reading from proc isn't the recommended way of fetching the ipv6 route
    table; taking a write lock seems unnecessary and would probably cause
    network performance issues.
    
    To verify this I loaded up the ipv6 route table and then ran iperf in 3
    cases:
      * doing nothing
      * reading ipv6 route table via proc
        (while :; do cat /proc/net/ipv6_route > /dev/null; done)
      * reading ipv6 route table via rtnetlink
        (while :; do ip -6 route show table all > /dev/null; done)
    
    * Load the ipv6 route table up with:
      * for ((i = 0;i < 4000;i++)); do ip route add unreachable 2000::$i; done
    
    * iperf commands:
      * client: iperf -i 1 -V -c <ipv6 addr>
      * server: iperf -V -s
    
    * iperf results - 3 runs each (in Mbits/sec)
      * nothing: client: 927,927,927 server: 927,927,927
      * proc: client: 179,97,96,113 server: 142,112,133
      * iproute: client: 928,927,928 server: 927,927,927
    
    lock_stat shows taking the write lock is causing the slowdown. Using this
    info I decided to write a version of fib6_clean_all() which replaces
    write_lock_bh(&table->tb6_lock) with read_lock_bh(&table->tb6_lock). With
    this new function I see the same results as with my rtnetlink iperf test.
    
    Signed-off-by: Josh Hunt <joshhunt00@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e30e2fdfe56288576ee9e04dbb06b4bd5f282203
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu Dec 22 02:45:29 2011 +0530

    VFS: Fix race between CPU hotplug and lglocks
    
    Currently, the *_global_[un]lock_online() routines are not at all synchronized
    with CPU hotplug. Soft-lockups detected as a consequence of this race was
    reported earlier at https://lkml.org/lkml/2011/8/24/185. (Thanks to Cong Meng
    for finding out that the root-cause of this issue is the race condition
    between br_write_[un]lock() and CPU hotplug, which results in the lock states
    getting messed up).
    
    Fixing this race by just adding {get,put}_online_cpus() at appropriate places
    in *_global_[un]lock_online() is not a good option, because, then suddenly
    br_write_[un]lock() would become blocking, whereas they have been kept as
    non-blocking all this time, and we would want to keep them that way.
    
    So, overall, we want to ensure 3 things:
    1. br_write_lock() and br_write_unlock() must remain as non-blocking.
    2. The corresponding lock and unlock of the per-cpu spinlocks must not happen
       for different sets of CPUs.
    3. Either prevent any new CPU online operation in between this lock-unlock, or
       ensure that the newly onlined CPU does not proceed with its corresponding
       per-cpu spinlock unlocked.
    
    To achieve all this:
    (a) We introduce a new spinlock that is taken by the *_global_lock_online()
        routine and released by the *_global_unlock_online() routine.
    (b) We register a callback for CPU hotplug notifications, and this callback
        takes the same spinlock as above.
    (c) We maintain a bitmap which is close to the cpu_online_mask, and once it is
        initialized in the lock_init() code, all future updates to it are done in
        the callback, under the above spinlock.
    (d) The above bitmap is used (instead of cpu_online_mask) while locking and
        unlocking the per-cpu locks.
    
    The callback takes the spinlock upon the CPU_UP_PREPARE event. So, if the
    br_write_lock-unlock sequence is in progress, the callback keeps spinning,
    thus preventing the CPU online operation till the lock-unlock sequence is
    complete. This takes care of requirement (3).
    
    The bitmap that we maintain remains unmodified throughout the lock-unlock
    sequence, since all updates to it are managed by the callback, which takes
    the same spinlock as the one taken by the lock code and released only by the
    unlock routine. Combining this with (d) above, satisfies requirement (2).
    
    Overall, since we use a spinlock (mentioned in (a)) to prevent CPU hotplug
    operations from racing with br_write_lock-unlock, requirement (1) is also
    taken care of.
    
    By the way, it is to be noted that a CPU offline operation can actually run
    in parallel with our lock-unlock sequence, because our callback doesn't react
    to notifications earlier than CPU_DEAD (in order to maintain our bitmap
    properly). And this means, since we use our own bitmap (which is stale, on
    purpose) during the lock-unlock sequence, we could end up unlocking the
    per-cpu lock of an offline CPU (because we had locked it earlier, when the
    CPU was online), in order to satisfy requirement (2). But this is harmless,
    though it looks a bit awkward.
    
    Debugged-by: Cong Meng <mc@linux.vnet.ibm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Cc: stable@vger.kernel.org

commit 257058ae2b971646b96ab3a15605ac69186e562a
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Dec 12 18:12:21 2011 -0800

    threadgroup: rename signal->threadgroup_fork_lock to ->group_rwsem
    
    Make the following renames to prepare for extension of threadgroup
    locking.
    
    * s/signal->threadgroup_fork_lock/signal->group_rwsem/
    * s/threadgroup_fork_read_lock()/threadgroup_change_begin()/
    * s/threadgroup_fork_read_unlock()/threadgroup_change_end()/
    * s/threadgroup_fork_write_lock()/threadgroup_lock()/
    * s/threadgroup_fork_write_unlock()/threadgroup_unlock()/
    
    This patch doesn't cause any behavior change.
    
    -v2: Rename threadgroup_change_done() to threadgroup_change_end() per
         KAMEZAWA's suggestion.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Menage <paul@paulmenage.org>

commit c8efe8c2805748f93add31d0463252c57f27a0ab
Author: Mikael Pettersson <mikpe@it.uu.se>
Date:   Mon Aug 15 10:11:50 2011 +0000

    sparc32: unbreak arch_write_unlock()
    
    commit 3f6aa0b113846a8628baa649af422cfc6fb1d786 upstream.
    
    The sparc32 version of arch_write_unlock() is just a plain assignment.
    Unfortunately this allows the compiler to schedule side-effects in a
    protected region to occur after the HW-level unlock, which is broken.
    E.g., the following trivial test case gets miscompiled:
    
            #include <linux/spinlock.h>
            rwlock_t lock;
            int counter;
            void foo(void) { write_lock(&lock); ++counter; write_unlock(&lock); }
    
    Fixed by adding a compiler memory barrier to arch_write_unlock().  The
    sparc64 version combines the barrier and assignment into a single asm(),
    and implements the operation as a static inline, so that's what I did too.
    
    Compile-tested with sparc32_defconfig + CONFIG_SMP=y.
    
    Signed-off-by: Mikael Pettersson <mikpe@it.uu.se>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 7fe1e169829030a8257be59636f82c45753ca941
Author: Mikael Pettersson <mikpe@it.uu.se>
Date:   Mon Aug 15 10:11:50 2011 +0000

    sparc32: unbreak arch_write_unlock()
    
    commit 3f6aa0b113846a8628baa649af422cfc6fb1d786 upstream.
    
    The sparc32 version of arch_write_unlock() is just a plain assignment.
    Unfortunately this allows the compiler to schedule side-effects in a
    protected region to occur after the HW-level unlock, which is broken.
    E.g., the following trivial test case gets miscompiled:
    
            #include <linux/spinlock.h>
            rwlock_t lock;
            int counter;
            void foo(void) { write_lock(&lock); ++counter; write_unlock(&lock); }
    
    Fixed by adding a compiler memory barrier to arch_write_unlock().  The
    sparc64 version combines the barrier and assignment into a single asm(),
    and implements the operation as a static inline, so that's what I did too.
    
    Compile-tested with sparc32_defconfig + CONFIG_SMP=y.
    
    Signed-off-by: Mikael Pettersson <mikpe@it.uu.se>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 3f6aa0b113846a8628baa649af422cfc6fb1d786
Author: Mikael Pettersson <mikpe@it.uu.se>
Date:   Mon Aug 15 10:11:50 2011 +0000

    sparc32: unbreak arch_write_unlock()
    
    The sparc32 version of arch_write_unlock() is just a plain assignment.
    Unfortunately this allows the compiler to schedule side-effects in a
    protected region to occur after the HW-level unlock, which is broken.
    E.g., the following trivial test case gets miscompiled:
    
            #include <linux/spinlock.h>
            rwlock_t lock;
            int counter;
            void foo(void) { write_lock(&lock); ++counter; write_unlock(&lock); }
    
    Fixed by adding a compiler memory barrier to arch_write_unlock().  The
    sparc64 version combines the barrier and assignment into a single asm(),
    and implements the operation as a static inline, so that's what I did too.
    
    Compile-tested with sparc32_defconfig + CONFIG_SMP=y.
    
    Signed-off-by: Mikael Pettersson <mikpe@it.uu.se>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 95cb3f2eefee6dd22468318a0d986598f6e9827d
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jul 1 17:30:00 2011 -0700

    6pack,mkiss: fix lock inconsistency
    
    commit 6e4e2f811bade330126d4029c88c831784a7efd9 upstream.
    
    Lockdep found a locking inconsistency in the mkiss_close function:
    
    > kernel: [ INFO: inconsistent lock state ]
    > kernel: 2.6.39.1 #3
    > kernel: ---------------------------------
    > kernel: inconsistent {IN-SOFTIRQ-R} -> {SOFTIRQ-ON-W} usage.
    > kernel: ax25ipd/2813 [HC0[0]:SC0[0]:HE1:SE1] takes:
    > kernel: (disc_data_lock){+++?.-}, at: [<ffffffffa018552b>] mkiss_close+0x1b/0x90 [mkiss]
    > kernel: {IN-SOFTIRQ-R} state was registered at:
    
    The message hints that disc_data_lock is aquired with softirqs disabled,
    but does not itself disable softirqs, which can in rare circumstances
    lead to a deadlock.
    The same problem is present in the 6pack driver, this patch fixes both
    by using write_lock_bh instead of write_lock.
    
    Reported-by: Bernard F6BVP <f6bvp@free.fr>
    Tested-by: Bernard F6BVP <f6bvp@free.fr>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Ralf Baechle<ralf@linux-mips.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

commit 70f18915846f092e0e1c988f1726a532fa3ab3a1
Author: Alexander Stein <alexander.stein@systec-electronic.com>
Date:   Wed Jun 22 17:05:33 2011 +0200

    pch_dma: Fix channel locking
    
    Fix for the following INFO message
    
    =================================
    [ INFO: inconsistent lock state ]
    2.6.39+ #89
    ---------------------------------
    inconsistent {HARDIRQ-ON-W} -> {IN-HARDIRQ-W} usage.
    rs232/822 [HC1[1]:SC0[0]:HE0:SE1] takes:
     (&(&pd_chan->lock)->rlock){?.....}, at: [<c123b9a1>] pdc_desc_get+0x16/0xab
    {HARDIRQ-ON-W} state was registered at:
      [<c104fe28>] mark_irqflags+0xbd/0x11a
      [<c1050386>] __lock_acquire+0x501/0x6bb
      [<c1050945>] lock_acquire+0x63/0x7b
      [<c131c51d>] _raw_spin_lock_bh+0x43/0x51
      [<c123bee4>] pd_alloc_chan_resources+0x92/0x11e
      [<c123ad62>] dma_chan_get+0x9b/0x107
      [<c123b2d1>] __dma_request_channel+0x61/0xdc
      [<c11ba24b>] pch_request_dma+0x61/0x19e
      [<c11bb3b8>] pch_uart_startup+0x16a/0x1a2
      [<c11b8446>] uart_startup+0x87/0x147
      [<c11b9183>] uart_open+0x117/0x13e
      [<c11a5c7d>] tty_open+0x23c/0x34c
      [<c1097705>] chrdev_open+0x140/0x15f
      [<c10930a6>] __dentry_open.clone.14+0x14a/0x22b
      [<c1093dfb>] nameidata_to_filp+0x36/0x40
      [<c109f28b>] do_last+0x513/0x635
      [<c109f4af>] path_openat+0x9c/0x2aa
      [<c109f6e4>] do_filp_open+0x27/0x69
      [<c1093f02>] do_sys_open+0xfd/0x184
      [<c1093fad>] sys_open+0x24/0x2a
      [<c131d58c>] sysenter_do_call+0x12/0x32
    irq event stamp: 2522
    hardirqs last  enabled at (2521): [<c131ca3b>] _raw_spin_unlock_irqrestore+0x36/0x52
    hardirqs last disabled at (2522): [<c131db27>] common_interrupt+0x27/0x34
    softirqs last  enabled at (2354): [<c102fa11>] __do_softirq+0x10a/0x11a
    softirqs last disabled at (2299): [<c10041a4>] do_softirq+0x57/0xa4
    
    other info that might help us debug this:
    2 locks held by rs232/822:
     #0:  (&tty->atomic_write_lock){+.+.+.}, at: [<c11a4b7a>] tty_write_lock+0x14/0x3c
     #1:  (&port_lock_key){-.....}, at: [<c11bad72>] pch_uart_interrupt+0x17/0x1e9
    
    stack backtrace:
    Pid: 822, comm: rs232 Not tainted 2.6.39+ #89
    Call Trace:
     [<c1319f90>] ? printk+0x19/0x1b
     [<c104f893>] print_usage_bug+0x184/0x18f
     [<c104e5b1>] ? print_irq_inversion_bug+0x10e/0x10e
     [<c104f943>] mark_lock_irq+0xa5/0x1f6
     [<c104fc9c>] mark_lock+0x208/0x2d7
     [<c104fdc0>] mark_irqflags+0x55/0x11a
     [<c1050386>] __lock_acquire+0x501/0x6bb
     [<c10042ee>] ? dump_trace+0x92/0xb6
     [<c1050945>] lock_acquire+0x63/0x7b
     [<c123b9a1>] ? pdc_desc_get+0x16/0xab
     [<c131c2d0>] _raw_spin_lock+0x3e/0x4c
     [<c123b9a1>] ? pdc_desc_get+0x16/0xab
     [<c123b9a1>] pdc_desc_get+0x16/0xab
     [<c10504d8>] ? __lock_acquire+0x653/0x6bb
     [<c123bb2c>] pd_prep_slave_sg+0x7c/0x1cb
     [<c1006c3f>] ? nommu_map_sg+0x6e/0x81
     [<c11bace6>] dma_handle_tx+0x2cf/0x344
     [<c11bad72>] ? pch_uart_interrupt+0x17/0x1e9
     [<c11baebb>] pch_uart_interrupt+0x160/0x1e9
     [<c10642fb>] handle_irq_event_percpu+0x25/0x127
     [<c1064429>] handle_irq_event+0x2c/0x43
     [<c1065e0d>] ? handle_fasteoi_irq+0x84/0x84
     [<c1065eb9>] handle_edge_irq+0xac/0xce
     <IRQ>  [<c1003ecb>] ? do_IRQ+0x38/0x9d
     [<c131db2e>] ? common_interrupt+0x2e/0x34
     [<c105007b>] ? __lock_acquire+0x1f6/0x6bb
     [<c131ca3d>] ? _raw_spin_unlock_irqrestore+0x38/0x52
     [<c11b798b>] ? uart_start+0x2d/0x32
     [<c11b7998>] ? uart_flush_chars+0x8/0xa
     [<c11a7962>] ? n_tty_write+0x12c/0x1c6
     [<c1027a73>] ? try_to_wake_up+0x251/0x251
     [<c11a4d0b>] ? tty_write+0x169/0x1dc
     [<c11a7836>] ? n_tty_ioctl+0xb7/0xb7
     [<c1094841>] ? vfs_write+0x91/0x10d
     [<c11a4ba2>] ? tty_write_lock+0x3c/0x3c
     [<c1094a69>] ? sys_write+0x3e/0x63
     [<c131d58c>] ? sysenter_do_call+0x12/0x32
    
    Signed-off-by: Alexander Stein <alexander.stein@systec-electronic.com>
    Tested-by: Tomoya MORINAGA <tomoya-linux@dsn.okisemi.com>
    Signed-off-by: Vinod Koul <vinod.koul@intel.com>

commit 34e32c9cf37e250c797917fd8529504e55023f86
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jul 1 17:30:00 2011 -0700

    6pack,mkiss: fix lock inconsistency
    
    commit 6e4e2f811bade330126d4029c88c831784a7efd9 upstream.
    
    Lockdep found a locking inconsistency in the mkiss_close function:
    
    > kernel: [ INFO: inconsistent lock state ]
    > kernel: 2.6.39.1 #3
    > kernel: ---------------------------------
    > kernel: inconsistent {IN-SOFTIRQ-R} -> {SOFTIRQ-ON-W} usage.
    > kernel: ax25ipd/2813 [HC0[0]:SC0[0]:HE1:SE1] takes:
    > kernel: (disc_data_lock){+++?.-}, at: [<ffffffffa018552b>] mkiss_close+0x1b/0x90 [mkiss]
    > kernel: {IN-SOFTIRQ-R} state was registered at:
    
    The message hints that disc_data_lock is aquired with softirqs disabled,
    but does not itself disable softirqs, which can in rare circumstances
    lead to a deadlock.
    The same problem is present in the 6pack driver, this patch fixes both
    by using write_lock_bh instead of write_lock.
    
    Reported-by: Bernard F6BVP <f6bvp@free.fr>
    Tested-by: Bernard F6BVP <f6bvp@free.fr>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Ralf Baechle<ralf@linux-mips.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 06ce414d4a0811a979a82cddc3e429e6bd704c98
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jul 1 17:30:00 2011 -0700

    6pack,mkiss: fix lock inconsistency
    
    commit 6e4e2f811bade330126d4029c88c831784a7efd9 upstream.
    
    Lockdep found a locking inconsistency in the mkiss_close function:
    
    > kernel: [ INFO: inconsistent lock state ]
    > kernel: 2.6.39.1 #3
    > kernel: ---------------------------------
    > kernel: inconsistent {IN-SOFTIRQ-R} -> {SOFTIRQ-ON-W} usage.
    > kernel: ax25ipd/2813 [HC0[0]:SC0[0]:HE1:SE1] takes:
    > kernel: (disc_data_lock){+++?.-}, at: [<ffffffffa018552b>] mkiss_close+0x1b/0x90 [mkiss]
    > kernel: {IN-SOFTIRQ-R} state was registered at:
    
    The message hints that disc_data_lock is aquired with softirqs disabled,
    but does not itself disable softirqs, which can in rare circumstances
    lead to a deadlock.
    The same problem is present in the 6pack driver, this patch fixes both
    by using write_lock_bh instead of write_lock.
    
    Reported-by: Bernard F6BVP <f6bvp@free.fr>
    Tested-by: Bernard F6BVP <f6bvp@free.fr>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Ralf Baechle<ralf@linux-mips.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 1f224f1fcc301d291e0ef607b294c5dc3a2b3a5b
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jul 1 17:30:00 2011 -0700

    6pack,mkiss: fix lock inconsistency
    
    commit 6e4e2f811bade330126d4029c88c831784a7efd9 upstream.
    
    Lockdep found a locking inconsistency in the mkiss_close function:
    
    > kernel: [ INFO: inconsistent lock state ]
    > kernel: 2.6.39.1 #3
    > kernel: ---------------------------------
    > kernel: inconsistent {IN-SOFTIRQ-R} -> {SOFTIRQ-ON-W} usage.
    > kernel: ax25ipd/2813 [HC0[0]:SC0[0]:HE1:SE1] takes:
    > kernel: (disc_data_lock){+++?.-}, at: [<ffffffffa018552b>] mkiss_close+0x1b/0x90 [mkiss]
    > kernel: {IN-SOFTIRQ-R} state was registered at:
    
    The message hints that disc_data_lock is aquired with softirqs disabled,
    but does not itself disable softirqs, which can in rare circumstances
    lead to a deadlock.
    The same problem is present in the 6pack driver, this patch fixes both
    by using write_lock_bh instead of write_lock.
    
    Reported-by: Bernard F6BVP <f6bvp@free.fr>
    Tested-by: Bernard F6BVP <f6bvp@free.fr>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Ralf Baechle<ralf@linux-mips.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 6e4e2f811bade330126d4029c88c831784a7efd9
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jul 1 17:30:00 2011 -0700

    6pack,mkiss: fix lock inconsistency
    
    Lockdep found a locking inconsistency in the mkiss_close function:
    
    > kernel: [ INFO: inconsistent lock state ]
    > kernel: 2.6.39.1 #3
    > kernel: ---------------------------------
    > kernel: inconsistent {IN-SOFTIRQ-R} -> {SOFTIRQ-ON-W} usage.
    > kernel: ax25ipd/2813 [HC0[0]:SC0[0]:HE1:SE1] takes:
    > kernel: (disc_data_lock){+++?.-}, at: [<ffffffffa018552b>] mkiss_close+0x1b/0x90 [mkiss]
    > kernel: {IN-SOFTIRQ-R} state was registered at:
    
    The message hints that disc_data_lock is aquired with softirqs disabled,
    but does not itself disable softirqs, which can in rare circumstances
    lead to a deadlock.
    The same problem is present in the 6pack driver, this patch fixes both
    by using write_lock_bh instead of write_lock.
    
    Reported-by: Bernard F6BVP <f6bvp@free.fr>
    Tested-by: Bernard F6BVP <f6bvp@free.fr>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Ralf Baechle<ralf@linux-mips.org>
    Cc: stable@kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 83c67571b372c4a40023a84e183fdb7fa4e89e48
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Wed Apr 20 10:43:18 2011 +0200

    TTY: tty_io, annotate locking functions
    
    tty_write_lock and tty_write_unlock contain imbalanced locking. But
    this is intentional, so mark them appropriately by
    __acquires/__releases.
    
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 98d4499fbe16dd02e3846b2a05c07490e8aeeb33
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Sep 30 15:15:38 2010 -0700

    reiserfs: fix unwanted reiserfs lock recursion
    
    commit 9d8117e72bf453dd9d85e0cd322ce4a0f8bccbc0 upstream.
    
    Prevent from recursively locking the reiserfs lock in reiserfs_unpack()
    because we may call journal_begin() that requires the lock to be taken
    only once, otherwise it won't be able to release the lock while taking
    other mutexes, ending up in inverted dependencies between the journal
    mutex and the reiserfs lock for example.
    
    This fixes:
    
      =======================================================
      [ INFO: possible circular locking dependency detected ]
      2.6.35.4.4a #3
      -------------------------------------------------------
      lilo/1620 is trying to acquire lock:
       (&journal->j_mutex){+.+...}, at: [<d0325bff>] do_journal_begin_r+0x7f/0x340 [reiserfs]
    
      but task is already holding lock:
       (&REISERFS_SB(s)->lock){+.+.+.}, at: [<d032a278>] reiserfs_write_lock+0x28/0x40 [reiserfs]
    
      which lock already depends on the new lock.
    
      the existing dependency chain (in reverse order) is:
    
      -> #1 (&REISERFS_SB(s)->lock){+.+.+.}:
             [<c10562b7>] lock_acquire+0x67/0x80
             [<c12facad>] __mutex_lock_common+0x4d/0x410
             [<c12fb0c8>] mutex_lock_nested+0x18/0x20
             [<d032a278>] reiserfs_write_lock+0x28/0x40 [reiserfs]
             [<d0325c06>] do_journal_begin_r+0x86/0x340 [reiserfs]
             [<d0325f77>] journal_begin+0x77/0x140 [reiserfs]
             [<d0315be4>] reiserfs_remount+0x224/0x530 [reiserfs]
             [<c10b6a20>] do_remount_sb+0x60/0x110
             [<c10cee25>] do_mount+0x625/0x790
             [<c10cf014>] sys_mount+0x84/0xb0
             [<c12fca3d>] syscall_call+0x7/0xb
    
      -> #0 (&journal->j_mutex){+.+...}:
             [<c10560f6>] __lock_acquire+0x1026/0x1180
             [<c10562b7>] lock_acquire+0x67/0x80
             [<c12facad>] __mutex_lock_common+0x4d/0x410
             [<c12fb0c8>] mutex_lock_nested+0x18/0x20
             [<d0325bff>] do_journal_begin_r+0x7f/0x340 [reiserfs]
             [<d0325f77>] journal_begin+0x77/0x140 [reiserfs]
             [<d0326271>] reiserfs_persistent_transaction+0x41/0x90 [reiserfs]
             [<d030d06c>] reiserfs_get_block+0x22c/0x1530 [reiserfs]
             [<c10db9db>] __block_prepare_write+0x1bb/0x3a0
             [<c10dbbe6>] block_prepare_write+0x26/0x40
             [<d030b738>] reiserfs_prepare_write+0x88/0x170 [reiserfs]
             [<d03294d6>] reiserfs_unpack+0xe6/0x120 [reiserfs]
             [<d0329782>] reiserfs_ioctl+0x272/0x320 [reiserfs]
             [<c10c3188>] vfs_ioctl+0x28/0xa0
             [<c10c3bbd>] do_vfs_ioctl+0x32d/0x5c0
             [<c10c3eb3>] sys_ioctl+0x63/0x70
             [<c12fca3d>] syscall_call+0x7/0xb
    
      other info that might help us debug this:
    
      2 locks held by lilo/1620:
       #0:  (&sb->s_type->i_mutex_key#8){+.+.+.}, at: [<d032945a>] reiserfs_unpack+0x6a/0x120 [reiserfs]
       #1:  (&REISERFS_SB(s)->lock){+.+.+.}, at: [<d032a278>] reiserfs_write_lock+0x28/0x40 [reiserfs]
    
      stack backtrace:
      Pid: 1620, comm: lilo Not tainted 2.6.35.4.4a #3
      Call Trace:
       [<c10560f6>] __lock_acquire+0x1026/0x1180
       [<c10562b7>] lock_acquire+0x67/0x80
       [<c12facad>] __mutex_lock_common+0x4d/0x410
       [<c12fb0c8>] mutex_lock_nested+0x18/0x20
       [<d0325bff>] do_journal_begin_r+0x7f/0x340 [reiserfs]
       [<d0325f77>] journal_begin+0x77/0x140 [reiserfs]
       [<d0326271>] reiserfs_persistent_transaction+0x41/0x90 [reiserfs]
       [<d030d06c>] reiserfs_get_block+0x22c/0x1530 [reiserfs]
       [<c10db9db>] __block_prepare_write+0x1bb/0x3a0
       [<c10dbbe6>] block_prepare_write+0x26/0x40
       [<d030b738>] reiserfs_prepare_write+0x88/0x170 [reiserfs]
       [<d03294d6>] reiserfs_unpack+0xe6/0x120 [reiserfs]
       [<d0329782>] reiserfs_ioctl+0x272/0x320 [reiserfs]
       [<c10c3188>] vfs_ioctl+0x28/0xa0
       [<c10c3bbd>] do_vfs_ioctl+0x32d/0x5c0
       [<c10c3eb3>] sys_ioctl+0x63/0x70
       [<c12fca3d>] syscall_call+0x7/0xb
    
    Reported-by: Jarek Poplawski <jarkao2@gmail.com>
    Tested-by: Jarek Poplawski <jarkao2@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 34f63f1b1791a97ab6d90de421c4091341a02cb4
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Sep 30 15:15:37 2010 -0700

    reiserfs: fix dependency inversion between inode and reiserfs mutexes
    
    commit 3f259d092c7a2fdf217823e8f1838530adb0cdb0 upstream.
    
    The reiserfs mutex already depends on the inode mutex, so we can't lock
    the inode mutex in reiserfs_unpack() without using the safe locking API,
    because reiserfs_unpack() is always called with the reiserfs mutex locked.
    
    This fixes:
    
      =======================================================
      [ INFO: possible circular locking dependency detected ]
      2.6.35c #13
      -------------------------------------------------------
      lilo/1606 is trying to acquire lock:
       (&sb->s_type->i_mutex_key#8){+.+.+.}, at: [<d0329450>] reiserfs_unpack+0x60/0x110 [reiserfs]
    
      but task is already holding lock:
       (&REISERFS_SB(s)->lock){+.+.+.}, at: [<d032a268>] reiserfs_write_lock+0x28/0x40 [reiserfs]
    
      which lock already depends on the new lock.
    
      the existing dependency chain (in reverse order) is:
    
      -> #1 (&REISERFS_SB(s)->lock){+.+.+.}:
             [<c1056347>] lock_acquire+0x67/0x80
             [<c12f083d>] __mutex_lock_common+0x4d/0x410
             [<c12f0c58>] mutex_lock_nested+0x18/0x20
             [<d032a268>] reiserfs_write_lock+0x28/0x40 [reiserfs]
             [<d0329e9a>] reiserfs_lookup_privroot+0x2a/0x90 [reiserfs]
             [<d0316b81>] reiserfs_fill_super+0x941/0xe60 [reiserfs]
             [<c10b7d17>] get_sb_bdev+0x117/0x170
             [<d0313e21>] get_super_block+0x21/0x30 [reiserfs]
             [<c10b74ba>] vfs_kern_mount+0x6a/0x1b0
             [<c10b7659>] do_kern_mount+0x39/0xe0
             [<c10cebe0>] do_mount+0x340/0x790
             [<c10cf0b4>] sys_mount+0x84/0xb0
             [<c12f25cd>] syscall_call+0x7/0xb
    
      -> #0 (&sb->s_type->i_mutex_key#8){+.+.+.}:
             [<c1056186>] __lock_acquire+0x1026/0x1180
             [<c1056347>] lock_acquire+0x67/0x80
             [<c12f083d>] __mutex_lock_common+0x4d/0x410
             [<c12f0c58>] mutex_lock_nested+0x18/0x20
             [<d0329450>] reiserfs_unpack+0x60/0x110 [reiserfs]
             [<d0329772>] reiserfs_ioctl+0x272/0x320 [reiserfs]
             [<c10c3228>] vfs_ioctl+0x28/0xa0
             [<c10c3c5d>] do_vfs_ioctl+0x32d/0x5c0
             [<c10c3f53>] sys_ioctl+0x63/0x70
             [<c12f25cd>] syscall_call+0x7/0xb
    
      other info that might help us debug this:
    
      1 lock held by lilo/1606:
       #0:  (&REISERFS_SB(s)->lock){+.+.+.}, at: [<d032a268>] reiserfs_write_lock+0x28/0x40 [reiserfs]
    
      stack backtrace:
      Pid: 1606, comm: lilo Not tainted 2.6.35c #13
      Call Trace:
       [<c1056186>] __lock_acquire+0x1026/0x1180
       [<c1056347>] lock_acquire+0x67/0x80
       [<c12f083d>] __mutex_lock_common+0x4d/0x410
       [<c12f0c58>] mutex_lock_nested+0x18/0x20
       [<d0329450>] reiserfs_unpack+0x60/0x110 [reiserfs]
       [<d0329772>] reiserfs_ioctl+0x272/0x320 [reiserfs]
       [<c10c3228>] vfs_ioctl+0x28/0xa0
       [<c10c3c5d>] do_vfs_ioctl+0x32d/0x5c0
       [<c10c3f53>] sys_ioctl+0x63/0x70
       [<c12f25cd>] syscall_call+0x7/0xb
    
    Reported-by: Jarek Poplawski <jarkao2@gmail.com>
    Tested-by: Jarek Poplawski <jarkao2@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 038f8c110eace38d7598e271835ae96ad04a3a26
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Fri Feb 4 10:49:11 2011 +0100

    KVM: x86: Convert tsc_write_lock to raw_spinlock
    
    Code under this lock requires non-preemptibility. Ensure this also over
    -rt by converting it to raw spinlock.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

commit 5ecce9b3a94f2faa4ee0528662960ae9308aff37
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Sep 30 15:15:38 2010 -0700

    reiserfs: fix unwanted reiserfs lock recursion
    
    commit 9d8117e72bf453dd9d85e0cd322ce4a0f8bccbc0 upstream.
    
    Prevent from recursively locking the reiserfs lock in reiserfs_unpack()
    because we may call journal_begin() that requires the lock to be taken
    only once, otherwise it won't be able to release the lock while taking
    other mutexes, ending up in inverted dependencies between the journal
    mutex and the reiserfs lock for example.
    
    This fixes:
    
      =======================================================
      [ INFO: possible circular locking dependency detected ]
      2.6.35.4.4a #3
      -------------------------------------------------------
      lilo/1620 is trying to acquire lock:
       (&journal->j_mutex){+.+...}, at: [<d0325bff>] do_journal_begin_r+0x7f/0x340 [reiserfs]
    
      but task is already holding lock:
       (&REISERFS_SB(s)->lock){+.+.+.}, at: [<d032a278>] reiserfs_write_lock+0x28/0x40 [reiserfs]
    
      which lock already depends on the new lock.
    
      the existing dependency chain (in reverse order) is:
    
      -> #1 (&REISERFS_SB(s)->lock){+.+.+.}:
             [<c10562b7>] lock_acquire+0x67/0x80
             [<c12facad>] __mutex_lock_common+0x4d/0x410
             [<c12fb0c8>] mutex_lock_nested+0x18/0x20
             [<d032a278>] reiserfs_write_lock+0x28/0x40 [reiserfs]
             [<d0325c06>] do_journal_begin_r+0x86/0x340 [reiserfs]
             [<d0325f77>] journal_begin+0x77/0x140 [reiserfs]
             [<d0315be4>] reiserfs_remount+0x224/0x530 [reiserfs]
             [<c10b6a20>] do_remount_sb+0x60/0x110
             [<c10cee25>] do_mount+0x625/0x790
             [<c10cf014>] sys_mount+0x84/0xb0
             [<c12fca3d>] syscall_call+0x7/0xb
    
      -> #0 (&journal->j_mutex){+.+...}:
             [<c10560f6>] __lock_acquire+0x1026/0x1180
             [<c10562b7>] lock_acquire+0x67/0x80
             [<c12facad>] __mutex_lock_common+0x4d/0x410
             [<c12fb0c8>] mutex_lock_nested+0x18/0x20
             [<d0325bff>] do_journal_begin_r+0x7f/0x340 [reiserfs]
             [<d0325f77>] journal_begin+0x77/0x140 [reiserfs]
             [<d0326271>] reiserfs_persistent_transaction+0x41/0x90 [reiserfs]
             [<d030d06c>] reiserfs_get_block+0x22c/0x1530 [reiserfs]
             [<c10db9db>] __block_prepare_write+0x1bb/0x3a0
             [<c10dbbe6>] block_prepare_write+0x26/0x40
             [<d030b738>] reiserfs_prepare_write+0x88/0x170 [reiserfs]
             [<d03294d6>] reiserfs_unpack+0xe6/0x120 [reiserfs]
             [<d0329782>] reiserfs_ioctl+0x272/0x320 [reiserfs]
             [<c10c3188>] vfs_ioctl+0x28/0xa0
             [<c10c3bbd>] do_vfs_ioctl+0x32d/0x5c0
             [<c10c3eb3>] sys_ioctl+0x63/0x70
             [<c12fca3d>] syscall_call+0x7/0xb
    
      other info that might help us debug this:
    
      2 locks held by lilo/1620:
       #0:  (&sb->s_type->i_mutex_key#8){+.+.+.}, at: [<d032945a>] reiserfs_unpack+0x6a/0x120 [reiserfs]
       #1:  (&REISERFS_SB(s)->lock){+.+.+.}, at: [<d032a278>] reiserfs_write_lock+0x28/0x40 [reiserfs]
    
      stack backtrace:
      Pid: 1620, comm: lilo Not tainted 2.6.35.4.4a #3
      Call Trace:
       [<c10560f6>] __lock_acquire+0x1026/0x1180
       [<c10562b7>] lock_acquire+0x67/0x80
       [<c12facad>] __mutex_lock_common+0x4d/0x410
       [<c12fb0c8>] mutex_lock_nested+0x18/0x20
       [<d0325bff>] do_journal_begin_r+0x7f/0x340 [reiserfs]
       [<d0325f77>] journal_begin+0x77/0x140 [reiserfs]
       [<d0326271>] reiserfs_persistent_transaction+0x41/0x90 [reiserfs]
       [<d030d06c>] reiserfs_get_block+0x22c/0x1530 [reiserfs]
       [<c10db9db>] __block_prepare_write+0x1bb/0x3a0
       [<c10dbbe6>] block_prepare_write+0x26/0x40
       [<d030b738>] reiserfs_prepare_write+0x88/0x170 [reiserfs]
       [<d03294d6>] reiserfs_unpack+0xe6/0x120 [reiserfs]
       [<d0329782>] reiserfs_ioctl+0x272/0x320 [reiserfs]
       [<c10c3188>] vfs_ioctl+0x28/0xa0
       [<c10c3bbd>] do_vfs_ioctl+0x32d/0x5c0
       [<c10c3eb3>] sys_ioctl+0x63/0x70
       [<c12fca3d>] syscall_call+0x7/0xb
    
    Reported-by: Jarek Poplawski <jarkao2@gmail.com>
    Tested-by: Jarek Poplawski <jarkao2@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit cd1bbdfed8ff5047d96ed27a3ee7b881069daf03
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Sep 30 15:15:37 2010 -0700

    reiserfs: fix dependency inversion between inode and reiserfs mutexes
    
    commit 3f259d092c7a2fdf217823e8f1838530adb0cdb0 upstream.
    
    The reiserfs mutex already depends on the inode mutex, so we can't lock
    the inode mutex in reiserfs_unpack() without using the safe locking API,
    because reiserfs_unpack() is always called with the reiserfs mutex locked.
    
    This fixes:
    
      =======================================================
      [ INFO: possible circular locking dependency detected ]
      2.6.35c #13
      -------------------------------------------------------
      lilo/1606 is trying to acquire lock:
       (&sb->s_type->i_mutex_key#8){+.+.+.}, at: [<d0329450>] reiserfs_unpack+0x60/0x110 [reiserfs]
    
      but task is already holding lock:
       (&REISERFS_SB(s)->lock){+.+.+.}, at: [<d032a268>] reiserfs_write_lock+0x28/0x40 [reiserfs]
    
      which lock already depends on the new lock.
    
      the existing dependency chain (in reverse order) is:
    
      -> #1 (&REISERFS_SB(s)->lock){+.+.+.}:
             [<c1056347>] lock_acquire+0x67/0x80
             [<c12f083d>] __mutex_lock_common+0x4d/0x410
             [<c12f0c58>] mutex_lock_nested+0x18/0x20
             [<d032a268>] reiserfs_write_lock+0x28/0x40 [reiserfs]
             [<d0329e9a>] reiserfs_lookup_privroot+0x2a/0x90 [reiserfs]
             [<d0316b81>] reiserfs_fill_super+0x941/0xe60 [reiserfs]
             [<c10b7d17>] get_sb_bdev+0x117/0x170
             [<d0313e21>] get_super_block+0x21/0x30 [reiserfs]
             [<c10b74ba>] vfs_kern_mount+0x6a/0x1b0
             [<c10b7659>] do_kern_mount+0x39/0xe0
             [<c10cebe0>] do_mount+0x340/0x790
             [<c10cf0b4>] sys_mount+0x84/0xb0
             [<c12f25cd>] syscall_call+0x7/0xb
    
      -> #0 (&sb->s_type->i_mutex_key#8){+.+.+.}:
             [<c1056186>] __lock_acquire+0x1026/0x1180
             [<c1056347>] lock_acquire+0x67/0x80
             [<c12f083d>] __mutex_lock_common+0x4d/0x410
             [<c12f0c58>] mutex_lock_nested+0x18/0x20
             [<d0329450>] reiserfs_unpack+0x60/0x110 [reiserfs]
             [<d0329772>] reiserfs_ioctl+0x272/0x320 [reiserfs]
             [<c10c3228>] vfs_ioctl+0x28/0xa0
             [<c10c3c5d>] do_vfs_ioctl+0x32d/0x5c0
             [<c10c3f53>] sys_ioctl+0x63/0x70
             [<c12f25cd>] syscall_call+0x7/0xb
    
      other info that might help us debug this:
    
      1 lock held by lilo/1606:
       #0:  (&REISERFS_SB(s)->lock){+.+.+.}, at: [<d032a268>] reiserfs_write_lock+0x28/0x40 [reiserfs]
    
      stack backtrace:
      Pid: 1606, comm: lilo Not tainted 2.6.35c #13
      Call Trace:
       [<c1056186>] __lock_acquire+0x1026/0x1180
       [<c1056347>] lock_acquire+0x67/0x80
       [<c12f083d>] __mutex_lock_common+0x4d/0x410
       [<c12f0c58>] mutex_lock_nested+0x18/0x20
       [<d0329450>] reiserfs_unpack+0x60/0x110 [reiserfs]
       [<d0329772>] reiserfs_ioctl+0x272/0x320 [reiserfs]
       [<c10c3228>] vfs_ioctl+0x28/0xa0
       [<c10c3c5d>] do_vfs_ioctl+0x32d/0x5c0
       [<c10c3f53>] sys_ioctl+0x63/0x70
       [<c12f25cd>] syscall_call+0x7/0xb
    
    Reported-by: Jarek Poplawski <jarkao2@gmail.com>
    Tested-by: Jarek Poplawski <jarkao2@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit a71fb88145a03678fef3796930993e390db68a15
Author: Jarek Poplawski <jarkao2@gmail.com>
Date:   Wed Oct 27 07:08:22 2010 +0000

    bonding: Fix lockdep warning after bond_vlan_rx_register()
    
    Fix lockdep warning:
    [   52.991402] ======================================================
    [   52.991511] [ INFO: SOFTIRQ-safe -> SOFTIRQ-unsafe lock order detected ]
    [   52.991569] 2.6.36-04573-g4b60626-dirty #65
    [   52.991622] ------------------------------------------------------
    [   52.991696] ip/4842 [HC0[0]:SC0[4]:HE1:SE0] is trying to acquire:
    [   52.991758]  (&bond->lock){++++..}, at: [<efe4d300>] bond_set_multicast_list+0x60/0x2c0 [bonding]
    [   52.991966]
    [   52.991967] and this task is already holding:
    [   52.992008]  (&bonding_netdev_addr_lock_key){+.....}, at: [<c04e5530>] dev_mc_sync+0x50/0xa0
    [   52.992008] which would create a new lock dependency:
    [   52.992008]  (&bonding_netdev_addr_lock_key){+.....} -> (&bond->lock){++++..}
    [   52.992008]
    [   52.992008] but this new dependency connects a SOFTIRQ-irq-safe lock:
    [   52.992008]  (&(&mc->mca_lock)->rlock){+.-...}
    [   52.992008] ... which became SOFTIRQ-irq-safe at:
    [   52.992008]   [<c0272beb>] __lock_acquire+0x96b/0x1960
    [   52.992008]   [<c027415e>] lock_acquire+0x7e/0xf0
    [   52.992008]   [<c05f356d>] _raw_spin_lock_bh+0x3d/0x50
    [   52.992008]   [<c0584e40>] mld_ifc_timer_expire+0xf0/0x280
    [   52.992008]   [<c024cee6>] run_timer_softirq+0x146/0x310
    [   52.992008]   [<c024591d>] __do_softirq+0xad/0x1c0
    [   52.992008]
    [   52.992008] to a SOFTIRQ-irq-unsafe lock:
    [   52.992008]  (&bond->lock){++++..}
    [   52.992008] ... which became SOFTIRQ-irq-unsafe at:
    [   52.992008] ...  [<c0272c3b>] __lock_acquire+0x9bb/0x1960
    [   52.992008]   [<c027415e>] lock_acquire+0x7e/0xf0
    [   52.992008]   [<c05f36b8>] _raw_write_lock+0x38/0x50
    [   52.992008]   [<efe4cbe4>] bond_vlan_rx_register+0x24/0x70 [bonding]
    [   52.992008]   [<c0598010>] register_vlan_dev+0xc0/0x280
    [   52.992008]   [<c0599f3a>] vlan_newlink+0xaa/0xd0
    [   52.992008]   [<c04ed4b4>] rtnl_newlink+0x404/0x490
    [   52.992008]   [<c04ece35>] rtnetlink_rcv_msg+0x1e5/0x220
    [   52.992008]   [<c050424e>] netlink_rcv_skb+0x8e/0xb0
    [   52.992008]   [<c04ecbac>] rtnetlink_rcv+0x1c/0x30
    [   52.992008]   [<c0503bfb>] netlink_unicast+0x24b/0x290
    [   52.992008]   [<c0503e37>] netlink_sendmsg+0x1f7/0x310
    [   52.992008]   [<c04cd41c>] sock_sendmsg+0xac/0xe0
    [   52.992008]   [<c04ceb80>] sys_sendmsg+0x130/0x230
    [   52.992008]   [<c04cf04e>] sys_socketcall+0xde/0x280
    [   52.992008]   [<c0202d10>] sysenter_do_call+0x12/0x36
    [   52.992008]
    [   52.992008] other info that might help us debug this:
    ...
    [ Full info at netdev: Wed, 27 Oct 2010 12:24:30 +0200
      Subject: [BUG net-2.6 vlan/bonding] lockdep splats ]
    
    Use BH variant of write_lock(&bond->lock) (as elsewhere in bond_main)
    to prevent this dependency.
    
    Fixes commit f35188faa0fbabefac476536994f4b6f3677380f [v2.6.36]
    
    Reported-by: Eric Dumazet <eric.dumazet@gmail.com>
    Tested-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: Jarek Poplawski <jarkao2@gmail.com>
    Acked-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Jay Vosburgh <fubar@us.ibm.com>

commit 9d8117e72bf453dd9d85e0cd322ce4a0f8bccbc0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Sep 30 15:15:38 2010 -0700

    reiserfs: fix unwanted reiserfs lock recursion
    
    Prevent from recursively locking the reiserfs lock in reiserfs_unpack()
    because we may call journal_begin() that requires the lock to be taken
    only once, otherwise it won't be able to release the lock while taking
    other mutexes, ending up in inverted dependencies between the journal
    mutex and the reiserfs lock for example.
    
    This fixes:
    
      =======================================================
      [ INFO: possible circular locking dependency detected ]
      2.6.35.4.4a #3
      -------------------------------------------------------
      lilo/1620 is trying to acquire lock:
       (&journal->j_mutex){+.+...}, at: [<d0325bff>] do_journal_begin_r+0x7f/0x340 [reiserfs]
    
      but task is already holding lock:
       (&REISERFS_SB(s)->lock){+.+.+.}, at: [<d032a278>] reiserfs_write_lock+0x28/0x40 [reiserfs]
    
      which lock already depends on the new lock.
    
      the existing dependency chain (in reverse order) is:
    
      -> #1 (&REISERFS_SB(s)->lock){+.+.+.}:
             [<c10562b7>] lock_acquire+0x67/0x80
             [<c12facad>] __mutex_lock_common+0x4d/0x410
             [<c12fb0c8>] mutex_lock_nested+0x18/0x20
             [<d032a278>] reiserfs_write_lock+0x28/0x40 [reiserfs]
             [<d0325c06>] do_journal_begin_r+0x86/0x340 [reiserfs]
             [<d0325f77>] journal_begin+0x77/0x140 [reiserfs]
             [<d0315be4>] reiserfs_remount+0x224/0x530 [reiserfs]
             [<c10b6a20>] do_remount_sb+0x60/0x110
             [<c10cee25>] do_mount+0x625/0x790
             [<c10cf014>] sys_mount+0x84/0xb0
             [<c12fca3d>] syscall_call+0x7/0xb
    
      -> #0 (&journal->j_mutex){+.+...}:
             [<c10560f6>] __lock_acquire+0x1026/0x1180
             [<c10562b7>] lock_acquire+0x67/0x80
             [<c12facad>] __mutex_lock_common+0x4d/0x410
             [<c12fb0c8>] mutex_lock_nested+0x18/0x20
             [<d0325bff>] do_journal_begin_r+0x7f/0x340 [reiserfs]
             [<d0325f77>] journal_begin+0x77/0x140 [reiserfs]
             [<d0326271>] reiserfs_persistent_transaction+0x41/0x90 [reiserfs]
             [<d030d06c>] reiserfs_get_block+0x22c/0x1530 [reiserfs]
             [<c10db9db>] __block_prepare_write+0x1bb/0x3a0
             [<c10dbbe6>] block_prepare_write+0x26/0x40
             [<d030b738>] reiserfs_prepare_write+0x88/0x170 [reiserfs]
             [<d03294d6>] reiserfs_unpack+0xe6/0x120 [reiserfs]
             [<d0329782>] reiserfs_ioctl+0x272/0x320 [reiserfs]
             [<c10c3188>] vfs_ioctl+0x28/0xa0
             [<c10c3bbd>] do_vfs_ioctl+0x32d/0x5c0
             [<c10c3eb3>] sys_ioctl+0x63/0x70
             [<c12fca3d>] syscall_call+0x7/0xb
    
      other info that might help us debug this:
    
      2 locks held by lilo/1620:
       #0:  (&sb->s_type->i_mutex_key#8){+.+.+.}, at: [<d032945a>] reiserfs_unpack+0x6a/0x120 [reiserfs]
       #1:  (&REISERFS_SB(s)->lock){+.+.+.}, at: [<d032a278>] reiserfs_write_lock+0x28/0x40 [reiserfs]
    
      stack backtrace:
      Pid: 1620, comm: lilo Not tainted 2.6.35.4.4a #3
      Call Trace:
       [<c10560f6>] __lock_acquire+0x1026/0x1180
       [<c10562b7>] lock_acquire+0x67/0x80
       [<c12facad>] __mutex_lock_common+0x4d/0x410
       [<c12fb0c8>] mutex_lock_nested+0x18/0x20
       [<d0325bff>] do_journal_begin_r+0x7f/0x340 [reiserfs]
       [<d0325f77>] journal_begin+0x77/0x140 [reiserfs]
       [<d0326271>] reiserfs_persistent_transaction+0x41/0x90 [reiserfs]
       [<d030d06c>] reiserfs_get_block+0x22c/0x1530 [reiserfs]
       [<c10db9db>] __block_prepare_write+0x1bb/0x3a0
       [<c10dbbe6>] block_prepare_write+0x26/0x40
       [<d030b738>] reiserfs_prepare_write+0x88/0x170 [reiserfs]
       [<d03294d6>] reiserfs_unpack+0xe6/0x120 [reiserfs]
       [<d0329782>] reiserfs_ioctl+0x272/0x320 [reiserfs]
       [<c10c3188>] vfs_ioctl+0x28/0xa0
       [<c10c3bbd>] do_vfs_ioctl+0x32d/0x5c0
       [<c10c3eb3>] sys_ioctl+0x63/0x70
       [<c12fca3d>] syscall_call+0x7/0xb
    
    Reported-by: Jarek Poplawski <jarkao2@gmail.com>
    Tested-by: Jarek Poplawski <jarkao2@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: All since 2.6.32 <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 3f259d092c7a2fdf217823e8f1838530adb0cdb0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Sep 30 15:15:37 2010 -0700

    reiserfs: fix dependency inversion between inode and reiserfs mutexes
    
    The reiserfs mutex already depends on the inode mutex, so we can't lock
    the inode mutex in reiserfs_unpack() without using the safe locking API,
    because reiserfs_unpack() is always called with the reiserfs mutex locked.
    
    This fixes:
    
      =======================================================
      [ INFO: possible circular locking dependency detected ]
      2.6.35c #13
      -------------------------------------------------------
      lilo/1606 is trying to acquire lock:
       (&sb->s_type->i_mutex_key#8){+.+.+.}, at: [<d0329450>] reiserfs_unpack+0x60/0x110 [reiserfs]
    
      but task is already holding lock:
       (&REISERFS_SB(s)->lock){+.+.+.}, at: [<d032a268>] reiserfs_write_lock+0x28/0x40 [reiserfs]
    
      which lock already depends on the new lock.
    
      the existing dependency chain (in reverse order) is:
    
      -> #1 (&REISERFS_SB(s)->lock){+.+.+.}:
             [<c1056347>] lock_acquire+0x67/0x80
             [<c12f083d>] __mutex_lock_common+0x4d/0x410
             [<c12f0c58>] mutex_lock_nested+0x18/0x20
             [<d032a268>] reiserfs_write_lock+0x28/0x40 [reiserfs]
             [<d0329e9a>] reiserfs_lookup_privroot+0x2a/0x90 [reiserfs]
             [<d0316b81>] reiserfs_fill_super+0x941/0xe60 [reiserfs]
             [<c10b7d17>] get_sb_bdev+0x117/0x170
             [<d0313e21>] get_super_block+0x21/0x30 [reiserfs]
             [<c10b74ba>] vfs_kern_mount+0x6a/0x1b0
             [<c10b7659>] do_kern_mount+0x39/0xe0
             [<c10cebe0>] do_mount+0x340/0x790
             [<c10cf0b4>] sys_mount+0x84/0xb0
             [<c12f25cd>] syscall_call+0x7/0xb
    
      -> #0 (&sb->s_type->i_mutex_key#8){+.+.+.}:
             [<c1056186>] __lock_acquire+0x1026/0x1180
             [<c1056347>] lock_acquire+0x67/0x80
             [<c12f083d>] __mutex_lock_common+0x4d/0x410
             [<c12f0c58>] mutex_lock_nested+0x18/0x20
             [<d0329450>] reiserfs_unpack+0x60/0x110 [reiserfs]
             [<d0329772>] reiserfs_ioctl+0x272/0x320 [reiserfs]
             [<c10c3228>] vfs_ioctl+0x28/0xa0
             [<c10c3c5d>] do_vfs_ioctl+0x32d/0x5c0
             [<c10c3f53>] sys_ioctl+0x63/0x70
             [<c12f25cd>] syscall_call+0x7/0xb
    
      other info that might help us debug this:
    
      1 lock held by lilo/1606:
       #0:  (&REISERFS_SB(s)->lock){+.+.+.}, at: [<d032a268>] reiserfs_write_lock+0x28/0x40 [reiserfs]
    
      stack backtrace:
      Pid: 1606, comm: lilo Not tainted 2.6.35c #13
      Call Trace:
       [<c1056186>] __lock_acquire+0x1026/0x1180
       [<c1056347>] lock_acquire+0x67/0x80
       [<c12f083d>] __mutex_lock_common+0x4d/0x410
       [<c12f0c58>] mutex_lock_nested+0x18/0x20
       [<d0329450>] reiserfs_unpack+0x60/0x110 [reiserfs]
       [<d0329772>] reiserfs_ioctl+0x272/0x320 [reiserfs]
       [<c10c3228>] vfs_ioctl+0x28/0xa0
       [<c10c3c5d>] do_vfs_ioctl+0x32d/0x5c0
       [<c10c3f53>] sys_ioctl+0x63/0x70
       [<c12f25cd>] syscall_call+0x7/0xb
    
    Reported-by: Jarek Poplawski <jarkao2@gmail.com>
    Tested-by: Jarek Poplawski <jarkao2@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: <stable@kernel.org>         [2.6.32 and later]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit f064af1e500a2bf4607706f0f458163bdb2a6ea5
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed Sep 22 12:43:39 2010 +0000

    net: fix a lockdep splat
    
    We have for each socket :
    
    One spinlock (sk_slock.slock)
    One rwlock (sk_callback_lock)
    
    Possible scenarios are :
    
    (A) (this is used in net/sunrpc/xprtsock.c)
    read_lock(&sk->sk_callback_lock) (without blocking BH)
    <BH>
    spin_lock(&sk->sk_slock.slock);
    ...
    read_lock(&sk->sk_callback_lock);
    ...
    
    (B)
    write_lock_bh(&sk->sk_callback_lock)
    stuff
    write_unlock_bh(&sk->sk_callback_lock)
    
    (C)
    spin_lock_bh(&sk->sk_slock)
    ...
    write_lock_bh(&sk->sk_callback_lock)
    stuff
    write_unlock_bh(&sk->sk_callback_lock)
    spin_unlock_bh(&sk->sk_slock)
    
    This (C) case conflicts with (A) :
    
    CPU1 [A]                         CPU2 [C]
    read_lock(callback_lock)
    <BH>                             spin_lock_bh(slock)
    <wait to spin_lock(slock)>
                                     <wait to write_lock_bh(callback_lock)>
    
    We have one problematic (C) use case in inet_csk_listen_stop() :
    
    local_bh_disable();
    bh_lock_sock(child); // spin_lock_bh(&sk->sk_slock)
    WARN_ON(sock_owned_by_user(child));
    ...
    sock_orphan(child); // write_lock_bh(&sk->sk_callback_lock)
    
    lockdep is not happy with this, as reported by Tetsuo Handa
    
    It seems only way to deal with this is to use read_lock_bh(callbacklock)
    everywhere.
    
    Thanks to Jarek for pointing a bug in my first attempt and suggesting
    this solution.
    
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Tested-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    CC: Jarek Poplawski <jarkao2@gmail.com>
    Tested-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 3a3a5ddb7a0f43c3dd0f98673f3d930a456725f8
Author: NeilBrown <neilb@suse.de>
Date:   Mon Aug 16 18:09:31 2010 +1000

    Update recovery_offset even when external metadata is used.
    
    The update of ->recovery_offset in sync_sbs is appropriate even then external
    metadata is in use.  However sync_sbs is only called when native
    metadata is used.
    
    So move that update in to the top of md_update_sb (which is the only
    caller of sync_sbs) before the test on ->external.
    
    This moves the update out of ->write_lock protection, but those fields
    only need ->reconfig_mutex protection which they still have.
    
    Also move the test on ->persistent up to where ->external is set as
    for metadata update purposes they are the same.
    
    Clear MD_CHANGE_DEVS and MD_CHANGE_CLEAN as they can only be confusing
    if ->external is set or ->persistent isn't.
    
    Finally move the update of ->utime down as it is only relevent (like
    the ->events update) for native metadata.
    
    Signed-off-by: NeilBrown <neilb@suse.de>
    Reported-by: "Kwolek, Adam" <adam.kwolek@intel.com>

commit f38a529fddcb2dd5eb11d7821b4015a7c8dd3b50
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Jul 30 11:46:42 2010 -0700

    x86, mtrr: Use stop machine context to rendezvous all the cpu's
    
    commit 68f202e4e87cfab4439568bf397fcc5c7cf8d729 upstream.
    
    Use the stop machine context rather than IPI's to rendezvous all the cpus for
    MTRR initialization that happens during cpu bringup or for MTRR modifications
    during runtime.
    
    This avoids deadlock scenario (reported by Prarit) like:
    
    cpu A holds a read_lock (tasklist_lock for example) with irqs enabled
    cpu B waits for the same lock with irqs disabled using write_lock_irq
    cpu C doing set_mtrr() (during AP bringup for example), which will try to
    rendezvous all the cpus using IPI's
    
    This will result in C and A come to the rendezvous point and waiting
    for B. B is stuck forever waiting for the lock and thus not
    reaching the rendezvous point.
    
    Using stop cpu (run in the process context of per cpu based keventd) to do
    this rendezvous, avoids this deadlock scenario.
    
    Also make sure all the cpu's are in the rendezvous handler before we proceed
    with the local_irq_save() on each cpu. This lock step disabling irqs on all
    the cpus will avoid other deadlock scenarios (for example involving
    with the blocking smp_call_function's etc).
    
       [ This problem is very old. Marking -stable only for 2.6.35 as the
         stop_one_cpu_nowait() API is present only in 2.6.35. Any older
         kernel interested in this fix need to do some more work in backporting
         this patch. ]
    
    Reported-by: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1280515602.2682.10.camel@sbsiddha-MOBL3.sc.intel.com>
    Acked-by: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 68f202e4e87cfab4439568bf397fcc5c7cf8d729
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Jul 30 11:46:42 2010 -0700

    x86, mtrr: Use stop machine context to rendezvous all the cpu's
    
    Use the stop machine context rather than IPI's to rendezvous all the cpus for
    MTRR initialization that happens during cpu bringup or for MTRR modifications
    during runtime.
    
    This avoids deadlock scenario (reported by Prarit) like:
    
    cpu A holds a read_lock (tasklist_lock for example) with irqs enabled
    cpu B waits for the same lock with irqs disabled using write_lock_irq
    cpu C doing set_mtrr() (during AP bringup for example), which will try to
    rendezvous all the cpus using IPI's
    
    This will result in C and A come to the rendezvous point and waiting
    for B. B is stuck forever waiting for the lock and thus not
    reaching the rendezvous point.
    
    Using stop cpu (run in the process context of per cpu based keventd) to do
    this rendezvous, avoids this deadlock scenario.
    
    Also make sure all the cpu's are in the rendezvous handler before we proceed
    with the local_irq_save() on each cpu. This lock step disabling irqs on all
    the cpus will avoid other deadlock scenarios (for example involving
    with the blocking smp_call_function's etc).
    
       [ This problem is very old. Marking -stable only for 2.6.35 as the
         stop_one_cpu_nowait() API is present only in 2.6.35. Any older
         kernel interested in this fix need to do some more work in backporting
         this patch. ]
    
    Reported-by: Prarit Bhargava <prarit@redhat.com>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1280515602.2682.10.camel@sbsiddha-MOBL3.sc.intel.com>
    Acked-by: Prarit Bhargava <prarit@redhat.com>
    Cc: stable@kernel.org   [2.6.35]
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit d8b6569a31e9dfa66fc85afe79dc9b3aed48a1c7
Author: Toshiyuki Okajima <toshi.okajima@jp.fujitsu.com>
Date:   Fri Apr 30 14:32:13 2010 +0100

    KEYS: find_keyring_by_name() can gain access to a freed keyring
    
    commit cea7daa3589d6b550546a8c8963599f7c1a3ae5c upstream.
    
    find_keyring_by_name() can gain access to a keyring that has had its reference
    count reduced to zero, and is thus ready to be freed.  This then allows the
    dead keyring to be brought back into use whilst it is being destroyed.
    
    The following timeline illustrates the process:
    
    |(cleaner)                           (user)
    |
    | free_user(user)                    sys_keyctl()
    |  |                                  |
    |  key_put(user->session_keyring)     keyctl_get_keyring_ID()
    |  ||   //=> keyring->usage = 0        |
    |  |schedule_work(&key_cleanup_task)   lookup_user_key()
    |  ||                                   |
    |  kmem_cache_free(,user)               |
    |  .                                    |[KEY_SPEC_USER_KEYRING]
    |  .                                    install_user_keyrings()
    |  .                                    ||
    | key_cleanup() [<= worker_thread()]    ||
    |  |                                    ||
    |  [spin_lock(&key_serial_lock)]        |[mutex_lock(&key_user_keyr..mutex)]
    |  |                                    ||
    |  atomic_read() == 0                   ||
    |  |{ rb_ease(&key->serial_node,) }     ||
    |  |                                    ||
    |  [spin_unlock(&key_serial_lock)]      |find_keyring_by_name()
    |  |                                    |||
    |  keyring_destroy(keyring)             ||[read_lock(&keyring_name_lock)]
    |  ||                                   |||
    |  |[write_lock(&keyring_name_lock)]    ||atomic_inc(&keyring->usage)
    |  |.                                   ||| *** GET freeing keyring ***
    |  |.                                   ||[read_unlock(&keyring_name_lock)]
    |  ||                                   ||
    |  |list_del()                          |[mutex_unlock(&key_user_k..mutex)]
    |  ||                                   |
    |  |[write_unlock(&keyring_name_lock)]  ** INVALID keyring is returned **
    |  |                                    .
    |  kmem_cache_free(,keyring)            .
    |                                       .
    |                                       atomic_dec(&keyring->usage)
    v                                         *** DESTROYED ***
    TIME
    
    If CONFIG_SLUB_DEBUG=y then we may see the following message generated:
    
            =============================================================================
            BUG key_jar: Poison overwritten
            -----------------------------------------------------------------------------
    
            INFO: 0xffff880197a7e200-0xffff880197a7e200. First byte 0x6a instead of 0x6b
            INFO: Allocated in key_alloc+0x10b/0x35f age=25 cpu=1 pid=5086
            INFO: Freed in key_cleanup+0xd0/0xd5 age=12 cpu=1 pid=10
            INFO: Slab 0xffffea000592cb90 objects=16 used=2 fp=0xffff880197a7e200 flags=0x200000000000c3
            INFO: Object 0xffff880197a7e200 @offset=512 fp=0xffff880197a7e300
    
            Bytes b4 0xffff880197a7e1f0:  5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ
              Object 0xffff880197a7e200:  6a 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b jkkkkkkkkkkkkkkk
    
    Alternatively, we may see a system panic happen, such as:
    
            BUG: unable to handle kernel NULL pointer dereference at 0000000000000001
            IP: [<ffffffff810e61a3>] kmem_cache_alloc+0x5b/0xe9
            PGD 6b2b4067 PUD 6a80d067 PMD 0
            Oops: 0000 [#1] SMP
            last sysfs file: /sys/kernel/kexec_crash_loaded
            CPU 1
            ...
            Pid: 31245, comm: su Not tainted 2.6.34-rc5-nofixed-nodebug #2 D2089/PRIMERGY
            RIP: 0010:[<ffffffff810e61a3>]  [<ffffffff810e61a3>] kmem_cache_alloc+0x5b/0xe9
            RSP: 0018:ffff88006af3bd98  EFLAGS: 00010002
            RAX: 0000000000000000 RBX: 0000000000000001 RCX: ffff88007d19900b
            RDX: 0000000100000000 RSI: 00000000000080d0 RDI: ffffffff81828430
            RBP: ffffffff81828430 R08: ffff88000a293750 R09: 0000000000000000
            R10: 0000000000000001 R11: 0000000000100000 R12: 00000000000080d0
            R13: 00000000000080d0 R14: 0000000000000296 R15: ffffffff810f20ce
            FS:  00007f97116bc700(0000) GS:ffff88000a280000(0000) knlGS:0000000000000000
            CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
            CR2: 0000000000000001 CR3: 000000006a91c000 CR4: 00000000000006e0
            DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
            DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
            Process su (pid: 31245, threadinfo ffff88006af3a000, task ffff8800374414c0)
            Stack:
             0000000512e0958e 0000000000008000 ffff880037f8d180 0000000000000001
             0000000000000000 0000000000008001 ffff88007d199000 ffffffff810f20ce
             0000000000008000 ffff88006af3be48 0000000000000024 ffffffff810face3
            Call Trace:
             [<ffffffff810f20ce>] ? get_empty_filp+0x70/0x12f
             [<ffffffff810face3>] ? do_filp_open+0x145/0x590
             [<ffffffff810ce208>] ? tlb_finish_mmu+0x2a/0x33
             [<ffffffff810ce43c>] ? unmap_region+0xd3/0xe2
             [<ffffffff810e4393>] ? virt_to_head_page+0x9/0x2d
             [<ffffffff81103916>] ? alloc_fd+0x69/0x10e
             [<ffffffff810ef4ed>] ? do_sys_open+0x56/0xfc
             [<ffffffff81008a02>] ? system_call_fastpath+0x16/0x1b
            Code: 0f 1f 44 00 00 49 89 c6 fa 66 0f 1f 44 00 00 65 4c 8b 04 25 60 e8 00 00 48 8b 45 00 49 01 c0 49 8b 18 48 85 db 74 0d 48 63 45 18 <48> 8b 04 03 49 89 00 eb 14 4c 89 f9 83 ca ff 44 89 e6 48 89 ef
            RIP  [<ffffffff810e61a3>] kmem_cache_alloc+0x5b/0xe9
    
    This problem is that find_keyring_by_name does not confirm that the keyring is
    valid before accepting it.
    
    Skipping keyrings that have been reduced to a zero count seems the way to go.
    To this end, use atomic_inc_not_zero() to increment the usage count and skip
    the candidate keyring if that returns false.
    
    The following script _may_ cause the bug to happen, but there's no guarantee
    as the window of opportunity is small:
    
            #!/bin/sh
            LOOP=100000
            USER=dummy_user
            /bin/su -c "exit;" $USER || { /usr/sbin/adduser -m $USER; add=1; }
            for ((i=0; i<LOOP; i++))
            do
                    /bin/su -c "echo '$i' > /dev/null" $USER
            done
            (( add == 1 )) && /usr/sbin/userdel -r $USER
            exit
    
    Note that the nominated user must not be in use.
    
    An alternative way of testing this may be:
    
            for ((i=0; i<100000; i++))
            do
                    keyctl session foo /bin/true || break
            done >&/dev/null
    
    as that uses a keyring named "foo" rather than relying on the user and
    user-session named keyrings.
    
    Reported-by: Toshiyuki Okajima <toshi.okajima@jp.fujitsu.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Toshiyuki Okajima <toshi.okajima@jp.fujitsu.com>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>
    Cc: Ben Hutchings <ben@decadent.org.uk>
    Cc: Chuck Ebbert <cebbert@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 48b97a01ba4d411047dad9abce933301699cdf25
Author: Toshiyuki Okajima <toshi.okajima@jp.fujitsu.com>
Date:   Fri Apr 30 14:32:13 2010 +0100

    KEYS: find_keyring_by_name() can gain access to a freed keyring
    
    commit cea7daa3589d6b550546a8c8963599f7c1a3ae5c upstream.
    
    find_keyring_by_name() can gain access to a keyring that has had its reference
    count reduced to zero, and is thus ready to be freed.  This then allows the
    dead keyring to be brought back into use whilst it is being destroyed.
    
    The following timeline illustrates the process:
    
    |(cleaner)                           (user)
    |
    | free_user(user)                    sys_keyctl()
    |  |                                  |
    |  key_put(user->session_keyring)     keyctl_get_keyring_ID()
    |  ||   //=> keyring->usage = 0        |
    |  |schedule_work(&key_cleanup_task)   lookup_user_key()
    |  ||                                   |
    |  kmem_cache_free(,user)               |
    |  .                                    |[KEY_SPEC_USER_KEYRING]
    |  .                                    install_user_keyrings()
    |  .                                    ||
    | key_cleanup() [<= worker_thread()]    ||
    |  |                                    ||
    |  [spin_lock(&key_serial_lock)]        |[mutex_lock(&key_user_keyr..mutex)]
    |  |                                    ||
    |  atomic_read() == 0                   ||
    |  |{ rb_ease(&key->serial_node,) }     ||
    |  |                                    ||
    |  [spin_unlock(&key_serial_lock)]      |find_keyring_by_name()
    |  |                                    |||
    |  keyring_destroy(keyring)             ||[read_lock(&keyring_name_lock)]
    |  ||                                   |||
    |  |[write_lock(&keyring_name_lock)]    ||atomic_inc(&keyring->usage)
    |  |.                                   ||| *** GET freeing keyring ***
    |  |.                                   ||[read_unlock(&keyring_name_lock)]
    |  ||                                   ||
    |  |list_del()                          |[mutex_unlock(&key_user_k..mutex)]
    |  ||                                   |
    |  |[write_unlock(&keyring_name_lock)]  ** INVALID keyring is returned **
    |  |                                    .
    |  kmem_cache_free(,keyring)            .
    |                                       .
    |                                       atomic_dec(&keyring->usage)
    v                                         *** DESTROYED ***
    TIME
    
    If CONFIG_SLUB_DEBUG=y then we may see the following message generated:
    
            =============================================================================
            BUG key_jar: Poison overwritten
            -----------------------------------------------------------------------------
    
            INFO: 0xffff880197a7e200-0xffff880197a7e200. First byte 0x6a instead of 0x6b
            INFO: Allocated in key_alloc+0x10b/0x35f age=25 cpu=1 pid=5086
            INFO: Freed in key_cleanup+0xd0/0xd5 age=12 cpu=1 pid=10
            INFO: Slab 0xffffea000592cb90 objects=16 used=2 fp=0xffff880197a7e200 flags=0x200000000000c3
            INFO: Object 0xffff880197a7e200 @offset=512 fp=0xffff880197a7e300
    
            Bytes b4 0xffff880197a7e1f0:  5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ
              Object 0xffff880197a7e200:  6a 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b jkkkkkkkkkkkkkkk
    
    Alternatively, we may see a system panic happen, such as:
    
            BUG: unable to handle kernel NULL pointer dereference at 0000000000000001
            IP: [<ffffffff810e61a3>] kmem_cache_alloc+0x5b/0xe9
            PGD 6b2b4067 PUD 6a80d067 PMD 0
            Oops: 0000 [#1] SMP
            last sysfs file: /sys/kernel/kexec_crash_loaded
            CPU 1
            ...
            Pid: 31245, comm: su Not tainted 2.6.34-rc5-nofixed-nodebug #2 D2089/PRIMERGY
            RIP: 0010:[<ffffffff810e61a3>]  [<ffffffff810e61a3>] kmem_cache_alloc+0x5b/0xe9
            RSP: 0018:ffff88006af3bd98  EFLAGS: 00010002
            RAX: 0000000000000000 RBX: 0000000000000001 RCX: ffff88007d19900b
            RDX: 0000000100000000 RSI: 00000000000080d0 RDI: ffffffff81828430
            RBP: ffffffff81828430 R08: ffff88000a293750 R09: 0000000000000000
            R10: 0000000000000001 R11: 0000000000100000 R12: 00000000000080d0
            R13: 00000000000080d0 R14: 0000000000000296 R15: ffffffff810f20ce
            FS:  00007f97116bc700(0000) GS:ffff88000a280000(0000) knlGS:0000000000000000
            CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
            CR2: 0000000000000001 CR3: 000000006a91c000 CR4: 00000000000006e0
            DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
            DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
            Process su (pid: 31245, threadinfo ffff88006af3a000, task ffff8800374414c0)
            Stack:
             0000000512e0958e 0000000000008000 ffff880037f8d180 0000000000000001
             0000000000000000 0000000000008001 ffff88007d199000 ffffffff810f20ce
             0000000000008000 ffff88006af3be48 0000000000000024 ffffffff810face3
            Call Trace:
             [<ffffffff810f20ce>] ? get_empty_filp+0x70/0x12f
             [<ffffffff810face3>] ? do_filp_open+0x145/0x590
             [<ffffffff810ce208>] ? tlb_finish_mmu+0x2a/0x33
             [<ffffffff810ce43c>] ? unmap_region+0xd3/0xe2
             [<ffffffff810e4393>] ? virt_to_head_page+0x9/0x2d
             [<ffffffff81103916>] ? alloc_fd+0x69/0x10e
             [<ffffffff810ef4ed>] ? do_sys_open+0x56/0xfc
             [<ffffffff81008a02>] ? system_call_fastpath+0x16/0x1b
            Code: 0f 1f 44 00 00 49 89 c6 fa 66 0f 1f 44 00 00 65 4c 8b 04 25 60 e8 00 00 48 8b 45 00 49 01 c0 49 8b 18 48 85 db 74 0d 48 63 45 18 <48> 8b 04 03 49 89 00 eb 14 4c 89 f9 83 ca ff 44 89 e6 48 89 ef
            RIP  [<ffffffff810e61a3>] kmem_cache_alloc+0x5b/0xe9
    
    This problem is that find_keyring_by_name does not confirm that the keyring is
    valid before accepting it.
    
    Skipping keyrings that have been reduced to a zero count seems the way to go.
    To this end, use atomic_inc_not_zero() to increment the usage count and skip
    the candidate keyring if that returns false.
    
    The following script _may_ cause the bug to happen, but there's no guarantee
    as the window of opportunity is small:
    
            #!/bin/sh
            LOOP=100000
            USER=dummy_user
            /bin/su -c "exit;" $USER || { /usr/sbin/adduser -m $USER; add=1; }
            for ((i=0; i<LOOP; i++))
            do
                    /bin/su -c "echo '$i' > /dev/null" $USER
            done
            (( add == 1 )) && /usr/sbin/userdel -r $USER
            exit
    
    Note that the nominated user must not be in use.
    
    An alternative way of testing this may be:
    
            for ((i=0; i<100000; i++))
            do
                    keyctl session foo /bin/true || break
            done >&/dev/null
    
    as that uses a keyring named "foo" rather than relying on the user and
    user-session named keyrings.
    
    Reported-by: Toshiyuki Okajima <toshi.okajima@jp.fujitsu.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Toshiyuki Okajima <toshi.okajima@jp.fujitsu.com>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>
    Cc: Ben Hutchings <ben@decadent.org.uk>
    Cc: Chuck Ebbert <cebbert@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 8e3c5b14d0aa33e347569e53f42f874a83f426c5
Author: Toshiyuki Okajima <toshi.okajima@jp.fujitsu.com>
Date:   Fri Apr 30 14:32:13 2010 +0100

    KEYS: find_keyring_by_name() can gain access to a freed keyring
    
    commit cea7daa3589d6b550546a8c8963599f7c1a3ae5c upstream.
    
    find_keyring_by_name() can gain access to a keyring that has had its reference
    count reduced to zero, and is thus ready to be freed.  This then allows the
    dead keyring to be brought back into use whilst it is being destroyed.
    
    The following timeline illustrates the process:
    
    |(cleaner)                           (user)
    |
    | free_user(user)                    sys_keyctl()
    |  |                                  |
    |  key_put(user->session_keyring)     keyctl_get_keyring_ID()
    |  ||   //=> keyring->usage = 0        |
    |  |schedule_work(&key_cleanup_task)   lookup_user_key()
    |  ||                                   |
    |  kmem_cache_free(,user)               |
    |  .                                    |[KEY_SPEC_USER_KEYRING]
    |  .                                    install_user_keyrings()
    |  .                                    ||
    | key_cleanup() [<= worker_thread()]    ||
    |  |                                    ||
    |  [spin_lock(&key_serial_lock)]        |[mutex_lock(&key_user_keyr..mutex)]
    |  |                                    ||
    |  atomic_read() == 0                   ||
    |  |{ rb_ease(&key->serial_node,) }     ||
    |  |                                    ||
    |  [spin_unlock(&key_serial_lock)]      |find_keyring_by_name()
    |  |                                    |||
    |  keyring_destroy(keyring)             ||[read_lock(&keyring_name_lock)]
    |  ||                                   |||
    |  |[write_lock(&keyring_name_lock)]    ||atomic_inc(&keyring->usage)
    |  |.                                   ||| *** GET freeing keyring ***
    |  |.                                   ||[read_unlock(&keyring_name_lock)]
    |  ||                                   ||
    |  |list_del()                          |[mutex_unlock(&key_user_k..mutex)]
    |  ||                                   |
    |  |[write_unlock(&keyring_name_lock)]  ** INVALID keyring is returned **
    |  |                                    .
    |  kmem_cache_free(,keyring)            .
    |                                       .
    |                                       atomic_dec(&keyring->usage)
    v                                         *** DESTROYED ***
    TIME
    
    If CONFIG_SLUB_DEBUG=y then we may see the following message generated:
    
            =============================================================================
            BUG key_jar: Poison overwritten
            -----------------------------------------------------------------------------
    
            INFO: 0xffff880197a7e200-0xffff880197a7e200. First byte 0x6a instead of 0x6b
            INFO: Allocated in key_alloc+0x10b/0x35f age=25 cpu=1 pid=5086
            INFO: Freed in key_cleanup+0xd0/0xd5 age=12 cpu=1 pid=10
            INFO: Slab 0xffffea000592cb90 objects=16 used=2 fp=0xffff880197a7e200 flags=0x200000000000c3
            INFO: Object 0xffff880197a7e200 @offset=512 fp=0xffff880197a7e300
    
            Bytes b4 0xffff880197a7e1f0:  5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ
              Object 0xffff880197a7e200:  6a 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b jkkkkkkkkkkkkkkk
    
    Alternatively, we may see a system panic happen, such as:
    
            BUG: unable to handle kernel NULL pointer dereference at 0000000000000001
            IP: [<ffffffff810e61a3>] kmem_cache_alloc+0x5b/0xe9
            PGD 6b2b4067 PUD 6a80d067 PMD 0
            Oops: 0000 [#1] SMP
            last sysfs file: /sys/kernel/kexec_crash_loaded
            CPU 1
            ...
            Pid: 31245, comm: su Not tainted 2.6.34-rc5-nofixed-nodebug #2 D2089/PRIMERGY
            RIP: 0010:[<ffffffff810e61a3>]  [<ffffffff810e61a3>] kmem_cache_alloc+0x5b/0xe9
            RSP: 0018:ffff88006af3bd98  EFLAGS: 00010002
            RAX: 0000000000000000 RBX: 0000000000000001 RCX: ffff88007d19900b
            RDX: 0000000100000000 RSI: 00000000000080d0 RDI: ffffffff81828430
            RBP: ffffffff81828430 R08: ffff88000a293750 R09: 0000000000000000
            R10: 0000000000000001 R11: 0000000000100000 R12: 00000000000080d0
            R13: 00000000000080d0 R14: 0000000000000296 R15: ffffffff810f20ce
            FS:  00007f97116bc700(0000) GS:ffff88000a280000(0000) knlGS:0000000000000000
            CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
            CR2: 0000000000000001 CR3: 000000006a91c000 CR4: 00000000000006e0
            DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
            DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
            Process su (pid: 31245, threadinfo ffff88006af3a000, task ffff8800374414c0)
            Stack:
             0000000512e0958e 0000000000008000 ffff880037f8d180 0000000000000001
             0000000000000000 0000000000008001 ffff88007d199000 ffffffff810f20ce
             0000000000008000 ffff88006af3be48 0000000000000024 ffffffff810face3
            Call Trace:
             [<ffffffff810f20ce>] ? get_empty_filp+0x70/0x12f
             [<ffffffff810face3>] ? do_filp_open+0x145/0x590
             [<ffffffff810ce208>] ? tlb_finish_mmu+0x2a/0x33
             [<ffffffff810ce43c>] ? unmap_region+0xd3/0xe2
             [<ffffffff810e4393>] ? virt_to_head_page+0x9/0x2d
             [<ffffffff81103916>] ? alloc_fd+0x69/0x10e
             [<ffffffff810ef4ed>] ? do_sys_open+0x56/0xfc
             [<ffffffff81008a02>] ? system_call_fastpath+0x16/0x1b
            Code: 0f 1f 44 00 00 49 89 c6 fa 66 0f 1f 44 00 00 65 4c 8b 04 25 60 e8 00 00 48 8b 45 00 49 01 c0 49 8b 18 48 85 db 74 0d 48 63 45 18 <48> 8b 04 03 49 89 00 eb 14 4c 89 f9 83 ca ff 44 89 e6 48 89 ef
            RIP  [<ffffffff810e61a3>] kmem_cache_alloc+0x5b/0xe9
    
    This problem is that find_keyring_by_name does not confirm that the keyring is
    valid before accepting it.
    
    Skipping keyrings that have been reduced to a zero count seems the way to go.
    To this end, use atomic_inc_not_zero() to increment the usage count and skip
    the candidate keyring if that returns false.
    
    The following script _may_ cause the bug to happen, but there's no guarantee
    as the window of opportunity is small:
    
            #!/bin/sh
            LOOP=100000
            USER=dummy_user
            /bin/su -c "exit;" $USER || { /usr/sbin/adduser -m $USER; add=1; }
            for ((i=0; i<LOOP; i++))
            do
                    /bin/su -c "echo '$i' > /dev/null" $USER
            done
            (( add == 1 )) && /usr/sbin/userdel -r $USER
            exit
    
    Note that the nominated user must not be in use.
    
    An alternative way of testing this may be:
    
            for ((i=0; i<100000; i++))
            do
                    keyctl session foo /bin/true || break
            done >&/dev/null
    
    as that uses a keyring named "foo" rather than relying on the user and
    user-session named keyrings.
    
    Reported-by: Toshiyuki Okajima <toshi.okajima@jp.fujitsu.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Toshiyuki Okajima <toshi.okajima@jp.fujitsu.com>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>
    Cc: Ben Hutchings <ben@decadent.org.uk>
    Cc: Chuck Ebbert <cebbert@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit cea7daa3589d6b550546a8c8963599f7c1a3ae5c
Author: Toshiyuki Okajima <toshi.okajima@jp.fujitsu.com>
Date:   Fri Apr 30 14:32:13 2010 +0100

    KEYS: find_keyring_by_name() can gain access to a freed keyring
    
    find_keyring_by_name() can gain access to a keyring that has had its reference
    count reduced to zero, and is thus ready to be freed.  This then allows the
    dead keyring to be brought back into use whilst it is being destroyed.
    
    The following timeline illustrates the process:
    
    |(cleaner)                           (user)
    |
    | free_user(user)                    sys_keyctl()
    |  |                                  |
    |  key_put(user->session_keyring)     keyctl_get_keyring_ID()
    |  ||   //=> keyring->usage = 0        |
    |  |schedule_work(&key_cleanup_task)   lookup_user_key()
    |  ||                                   |
    |  kmem_cache_free(,user)               |
    |  .                                    |[KEY_SPEC_USER_KEYRING]
    |  .                                    install_user_keyrings()
    |  .                                    ||
    | key_cleanup() [<= worker_thread()]    ||
    |  |                                    ||
    |  [spin_lock(&key_serial_lock)]        |[mutex_lock(&key_user_keyr..mutex)]
    |  |                                    ||
    |  atomic_read() == 0                   ||
    |  |{ rb_ease(&key->serial_node,) }     ||
    |  |                                    ||
    |  [spin_unlock(&key_serial_lock)]      |find_keyring_by_name()
    |  |                                    |||
    |  keyring_destroy(keyring)             ||[read_lock(&keyring_name_lock)]
    |  ||                                   |||
    |  |[write_lock(&keyring_name_lock)]    ||atomic_inc(&keyring->usage)
    |  |.                                   ||| *** GET freeing keyring ***
    |  |.                                   ||[read_unlock(&keyring_name_lock)]
    |  ||                                   ||
    |  |list_del()                          |[mutex_unlock(&key_user_k..mutex)]
    |  ||                                   |
    |  |[write_unlock(&keyring_name_lock)]  ** INVALID keyring is returned **
    |  |                                    .
    |  kmem_cache_free(,keyring)            .
    |                                       .
    |                                       atomic_dec(&keyring->usage)
    v                                         *** DESTROYED ***
    TIME
    
    If CONFIG_SLUB_DEBUG=y then we may see the following message generated:
    
            =============================================================================
            BUG key_jar: Poison overwritten
            -----------------------------------------------------------------------------
    
            INFO: 0xffff880197a7e200-0xffff880197a7e200. First byte 0x6a instead of 0x6b
            INFO: Allocated in key_alloc+0x10b/0x35f age=25 cpu=1 pid=5086
            INFO: Freed in key_cleanup+0xd0/0xd5 age=12 cpu=1 pid=10
            INFO: Slab 0xffffea000592cb90 objects=16 used=2 fp=0xffff880197a7e200 flags=0x200000000000c3
            INFO: Object 0xffff880197a7e200 @offset=512 fp=0xffff880197a7e300
    
            Bytes b4 0xffff880197a7e1f0:  5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ
              Object 0xffff880197a7e200:  6a 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b jkkkkkkkkkkkkkkk
    
    Alternatively, we may see a system panic happen, such as:
    
            BUG: unable to handle kernel NULL pointer dereference at 0000000000000001
            IP: [<ffffffff810e61a3>] kmem_cache_alloc+0x5b/0xe9
            PGD 6b2b4067 PUD 6a80d067 PMD 0
            Oops: 0000 [#1] SMP
            last sysfs file: /sys/kernel/kexec_crash_loaded
            CPU 1
            ...
            Pid: 31245, comm: su Not tainted 2.6.34-rc5-nofixed-nodebug #2 D2089/PRIMERGY
            RIP: 0010:[<ffffffff810e61a3>]  [<ffffffff810e61a3>] kmem_cache_alloc+0x5b/0xe9
            RSP: 0018:ffff88006af3bd98  EFLAGS: 00010002
            RAX: 0000000000000000 RBX: 0000000000000001 RCX: ffff88007d19900b
            RDX: 0000000100000000 RSI: 00000000000080d0 RDI: ffffffff81828430
            RBP: ffffffff81828430 R08: ffff88000a293750 R09: 0000000000000000
            R10: 0000000000000001 R11: 0000000000100000 R12: 00000000000080d0
            R13: 00000000000080d0 R14: 0000000000000296 R15: ffffffff810f20ce
            FS:  00007f97116bc700(0000) GS:ffff88000a280000(0000) knlGS:0000000000000000
            CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
            CR2: 0000000000000001 CR3: 000000006a91c000 CR4: 00000000000006e0
            DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
            DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
            Process su (pid: 31245, threadinfo ffff88006af3a000, task ffff8800374414c0)
            Stack:
             0000000512e0958e 0000000000008000 ffff880037f8d180 0000000000000001
             0000000000000000 0000000000008001 ffff88007d199000 ffffffff810f20ce
             0000000000008000 ffff88006af3be48 0000000000000024 ffffffff810face3
            Call Trace:
             [<ffffffff810f20ce>] ? get_empty_filp+0x70/0x12f
             [<ffffffff810face3>] ? do_filp_open+0x145/0x590
             [<ffffffff810ce208>] ? tlb_finish_mmu+0x2a/0x33
             [<ffffffff810ce43c>] ? unmap_region+0xd3/0xe2
             [<ffffffff810e4393>] ? virt_to_head_page+0x9/0x2d
             [<ffffffff81103916>] ? alloc_fd+0x69/0x10e
             [<ffffffff810ef4ed>] ? do_sys_open+0x56/0xfc
             [<ffffffff81008a02>] ? system_call_fastpath+0x16/0x1b
            Code: 0f 1f 44 00 00 49 89 c6 fa 66 0f 1f 44 00 00 65 4c 8b 04 25 60 e8 00 00 48 8b 45 00 49 01 c0 49 8b 18 48 85 db 74 0d 48 63 45 18 <48> 8b 04 03 49 89 00 eb 14 4c 89 f9 83 ca ff 44 89 e6 48 89 ef
            RIP  [<ffffffff810e61a3>] kmem_cache_alloc+0x5b/0xe9
    
    This problem is that find_keyring_by_name does not confirm that the keyring is
    valid before accepting it.
    
    Skipping keyrings that have been reduced to a zero count seems the way to go.
    To this end, use atomic_inc_not_zero() to increment the usage count and skip
    the candidate keyring if that returns false.
    
    The following script _may_ cause the bug to happen, but there's no guarantee
    as the window of opportunity is small:
    
            #!/bin/sh
            LOOP=100000
            USER=dummy_user
            /bin/su -c "exit;" $USER || { /usr/sbin/adduser -m $USER; add=1; }
            for ((i=0; i<LOOP; i++))
            do
                    /bin/su -c "echo '$i' > /dev/null" $USER
            done
            (( add == 1 )) && /usr/sbin/userdel -r $USER
            exit
    
    Note that the nominated user must not be in use.
    
    An alternative way of testing this may be:
    
            for ((i=0; i<100000; i++))
            do
                    keyctl session foo /bin/true || break
            done >&/dev/null
    
    as that uses a keyring named "foo" rather than relying on the user and
    user-session named keyrings.
    
    Reported-by: Toshiyuki Okajima <toshi.okajima@jp.fujitsu.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Toshiyuki Okajima <toshi.okajima@jp.fujitsu.com>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>

commit f8e1d0803d2db9ef0116941a4ce46069a2009ea6
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Tue Apr 13 00:29:27 2010 +0800

    ath9k-htc: fix lockdep warning and kernel warning after unplugging ar9271 usb device
    
    This patch fixes two warnings below after unplugging ar9271 usb device:
            -one is a kernel warning[1]
            -another is a lockdep warning[2]
    
    The root reason is that __skb_queue_purge can't be executed in hardirq
    context, so the patch forks ath9k_skb_queue_purge(ath9k version of _skb_queue_purge),
    which frees skb with dev_kfree_skb_any which can be run in hardirq
    context safely, then prevent the lockdep warning and kernel warning after
    unplugging ar9271 usb device.
    
    [1] kernel warning
    [  602.894005] ------------[ cut here ]------------
    [  602.894005] WARNING: at net/core/skbuff.c:398 skb_release_head_state+0x71/0x87()
    [  602.894005] Hardware name: 6475EK2
    [  602.894005] Modules linked in: ath9k_htc ath9k ath9k_common ath9k_hw ath bridge stp llc sunrpc ipv6 cpufreq_ondemand acpi_cpufreq freq_table kvm_intel kvm arc4 ecb mac80211 snd_hda_codec_conexant snd_hda_intel snd_hda_codec snd_hwdep thinkpad_acpi snd_pcm snd_timer hwmon iTCO_wdt snd e1000e pcspkr i2c_i801 usbhid iTCO_vendor_support wmi cfg80211 yenta_socket rsrc_nonstatic pata_acpi snd_page_alloc soundcore uhci_hcd ohci_hcd ehci_hcd usbcore i915 drm_kms_helper drm i2c_algo_bit i2c_core video output [last unloaded: ath]
    [  602.894005] Pid: 2506, comm: ping Tainted: G        W  2.6.34-rc3-wl #20
    [  602.894005] Call Trace:
    [  602.894005]  <IRQ>  [<ffffffff8104a41c>] warn_slowpath_common+0x7c/0x94
    [  602.894005]  [<ffffffffa022f398>] ? __skb_queue_purge+0x43/0x4a [ath9k_htc]
    [  602.894005]  [<ffffffff8104a448>] warn_slowpath_null+0x14/0x16
    [  602.894005]  [<ffffffff813269c1>] skb_release_head_state+0x71/0x87
    [  602.894005]  [<ffffffff8132829a>] __kfree_skb+0x16/0x81
    [  602.894005]  [<ffffffff813283b2>] kfree_skb+0x7e/0x86
    [  602.894005]  [<ffffffffa022f398>] __skb_queue_purge+0x43/0x4a [ath9k_htc]
    [  602.894005]  [<ffffffffa022f560>] __hif_usb_tx+0x1c1/0x21b [ath9k_htc]
    [  602.894005]  [<ffffffffa022f73c>] hif_usb_tx_cb+0x12f/0x154 [ath9k_htc]
    [  602.894005]  [<ffffffffa00d2fbe>] usb_hcd_giveback_urb+0x91/0xc5 [usbcore]
    [  602.894005]  [<ffffffffa00f6c34>] ehci_urb_done+0x7a/0x8b [ehci_hcd]
    [  602.894005]  [<ffffffffa00f6f33>] qh_completions+0x2ee/0x376 [ehci_hcd]
    [  602.894005]  [<ffffffffa00f8ba5>] ehci_work+0x95/0x76e [ehci_hcd]
    [  602.894005]  [<ffffffffa00fa5ae>] ? ehci_irq+0x2f/0x1d4 [ehci_hcd]
    [  602.894005]  [<ffffffffa00fa725>] ehci_irq+0x1a6/0x1d4 [ehci_hcd]
    [  602.894005]  [<ffffffff810a6d18>] ? __rcu_process_callbacks+0x7a/0x2df
    [  602.894005]  [<ffffffff810a47a4>] ? handle_fasteoi_irq+0x22/0xd2
    [  602.894005]  [<ffffffffa00d268d>] usb_hcd_irq+0x4a/0xa7 [usbcore]
    [  602.894005]  [<ffffffff810a2853>] handle_IRQ_event+0x77/0x14f
    [  602.894005]  [<ffffffff813285ce>] ? skb_release_data+0xc9/0xce
    [  602.894005]  [<ffffffff810a4814>] handle_fasteoi_irq+0x92/0xd2
    [  602.894005]  [<ffffffff8100c4fb>] handle_irq+0x88/0x91
    [  602.894005]  [<ffffffff8100baed>] do_IRQ+0x63/0xc9
    [  602.894005]  [<ffffffff81354245>] ? ip_flush_pending_frames+0x4d/0x5c
    [  602.894005]  [<ffffffff813ba993>] ret_from_intr+0x0/0x16
    [  602.894005]  <EOI>  [<ffffffff811095fe>] ? __delete_object+0x5a/0xb1
    [  602.894005]  [<ffffffff813ba5f5>] ? _raw_write_unlock_irqrestore+0x47/0x7e
    [  602.894005]  [<ffffffff813ba5fa>] ? _raw_write_unlock_irqrestore+0x4c/0x7e
    [  602.894005]  [<ffffffff811095fe>] __delete_object+0x5a/0xb1
    [  602.894005]  [<ffffffff81109814>] delete_object_full+0x25/0x31
    [  602.894005]  [<ffffffff813a60c0>] kmemleak_free+0x26/0x45
    [  602.894005]  [<ffffffff810ff517>] kfree+0xaa/0x149
    [  602.894005]  [<ffffffff81323fb7>] ? sock_def_write_space+0x84/0x89
    [  602.894005]  [<ffffffff81354245>] ? ip_flush_pending_frames+0x4d/0x5c
    [  602.894005]  [<ffffffff813285ce>] skb_release_data+0xc9/0xce
    [  602.894005]  [<ffffffff813282a2>] __kfree_skb+0x1e/0x81
    [  602.894005]  [<ffffffff813283b2>] kfree_skb+0x7e/0x86
    [  602.894005]  [<ffffffff81354245>] ip_flush_pending_frames+0x4d/0x5c
    [  602.894005]  [<ffffffff81370c1f>] raw_sendmsg+0x653/0x709
    [  602.894005]  [<ffffffff81379e31>] inet_sendmsg+0x54/0x5d
    [  602.894005]  [<ffffffff813207a2>] ? sock_recvmsg+0xc6/0xdf
    [  602.894005]  [<ffffffff813208c1>] sock_sendmsg+0xc0/0xd9
    [  602.894005]  [<ffffffff810e13b4>] ? might_fault+0x68/0xb8
    [  602.894005]  [<ffffffff810e13fd>] ? might_fault+0xb1/0xb8
    [  602.894005]  [<ffffffff8132a1c3>] ? copy_from_user+0x2f/0x31
    [  602.894005]  [<ffffffff8132a5b3>] ? verify_iovec+0x54/0x91
    [  602.894005]  [<ffffffff81320d41>] sys_sendmsg+0x1da/0x241
    [  602.894005]  [<ffffffff8103d327>] ? finish_task_switch+0x0/0xc9
    [  602.894005]  [<ffffffff8103d327>] ? finish_task_switch+0x0/0xc9
    [  602.894005]  [<ffffffff8107642e>] ? trace_hardirqs_on_caller+0x16/0x150
    [  602.894005]  [<ffffffff813ba27d>] ? _raw_spin_unlock_irq+0x56/0x63
    [  602.894005]  [<ffffffff8103d3cb>] ? finish_task_switch+0xa4/0xc9
    [  602.894005]  [<ffffffff8103d327>] ? finish_task_switch+0x0/0xc9
    [  602.894005]  [<ffffffff810357fe>] ? need_resched+0x23/0x2d
    [  602.894005]  [<ffffffff8107642e>] ? trace_hardirqs_on_caller+0x16/0x150
    [  602.894005]  [<ffffffff813b9750>] ? trace_hardirqs_on_thunk+0x3a/0x3f
    [  602.894005]  [<ffffffff81009c02>] system_call_fastpath+0x16/0x1b
    [  602.894005] ---[ end trace 91ba2d8dc7826839 ]---
    
    [2] lockdep warning
    [  169.363215] ======================================================
    [  169.365390] [ INFO: HARDIRQ-safe -> HARDIRQ-unsafe lock order detected ]
    [  169.366334] 2.6.34-rc3-wl #20
    [  169.366872] ------------------------------------------------------
    [  169.366872] khubd/78 [HC0[0]:SC0[0]:HE0:SE1] is trying to acquire:
    [  169.366872]  (clock-AF_INET){++.?..}, at: [<ffffffff81323f51>] sock_def_write_space+0x1e/0x89
    [  169.366872]
    [  169.366872] and this task is already holding:
    [  169.366872]  (&(&hif_dev->tx.tx_lock)->rlock){-.-...}, at: [<ffffffffa03715b0>] hif_usb_stop+0x24/0x53 [ath9k_htc]
    [  169.366872] which would create a new lock dependency:
    [  169.366872]  (&(&hif_dev->tx.tx_lock)->rlock){-.-...} -> (clock-AF_INET){++.?..}
    [  169.366872]
    [  169.366872] but this new dependency connects a HARDIRQ-irq-safe lock:
    [  169.366872]  (&(&hif_dev->tx.tx_lock)->rlock){-.-...}
    [  169.366872] ... which became HARDIRQ-irq-safe at:
    [  169.366872]   [<ffffffff810772d5>] __lock_acquire+0x2c6/0xd2b
    [  169.366872]   [<ffffffff8107866d>] lock_acquire+0xec/0x119
    [  169.366872]   [<ffffffff813b99bb>] _raw_spin_lock+0x40/0x73
    [  169.366872]   [<ffffffffa037163d>] hif_usb_tx_cb+0x5e/0x154 [ath9k_htc]
    [  169.366872]   [<ffffffffa00d2fbe>] usb_hcd_giveback_urb+0x91/0xc5 [usbcore]
    [  169.366872]   [<ffffffffa00f6c34>] ehci_urb_done+0x7a/0x8b [ehci_hcd]
    [  169.366872]   [<ffffffffa00f6f33>] qh_completions+0x2ee/0x376 [ehci_hcd]
    [  169.366872]   [<ffffffffa00f8ba5>] ehci_work+0x95/0x76e [ehci_hcd]
    [  169.366872]   [<ffffffffa00fa725>] ehci_irq+0x1a6/0x1d4 [ehci_hcd]
    [  169.366872]   [<ffffffffa00d268d>] usb_hcd_irq+0x4a/0xa7 [usbcore]
    [  169.366872]   [<ffffffff810a2853>] handle_IRQ_event+0x77/0x14f
    [  169.366872]   [<ffffffff810a4814>] handle_fasteoi_irq+0x92/0xd2
    [  169.366872]   [<ffffffff8100c4fb>] handle_irq+0x88/0x91
    [  169.366872]   [<ffffffff8100baed>] do_IRQ+0x63/0xc9
    [  169.366872]   [<ffffffff813ba993>] ret_from_intr+0x0/0x16
    [  169.366872]   [<ffffffff8130f6ee>] cpuidle_idle_call+0xa7/0x115
    [  169.366872]   [<ffffffff81008c4f>] cpu_idle+0x68/0xc4
    [  169.366872]   [<ffffffff813a41e0>] rest_init+0x104/0x10b
    [  169.366872]   [<ffffffff81899db3>] start_kernel+0x3f1/0x3fc
    [  169.366872]   [<ffffffff818992c8>] x86_64_start_reservations+0xb3/0xb7
    [  169.366872]   [<ffffffff818993c4>] x86_64_start_kernel+0xf8/0x107
    [  169.366872]
    [  169.366872] to a HARDIRQ-irq-unsafe lock:
    [  169.366872]  (clock-AF_INET){++.?..}
    [  169.366872] ... which became HARDIRQ-irq-unsafe at:
    [  169.366872] ...  [<ffffffff81077349>] __lock_acquire+0x33a/0xd2b
    [  169.366872]   [<ffffffff8107866d>] lock_acquire+0xec/0x119
    [  169.366872]   [<ffffffff813b9d07>] _raw_write_lock_bh+0x45/0x7a
    [  169.366872]   [<ffffffff8135cf14>] tcp_close+0x165/0x34d
    [  169.366872]   [<ffffffff8137aced>] inet_release+0x55/0x5c
    [  169.366872]   [<ffffffff81321350>] sock_release+0x1f/0x6e
    [  169.366872]   [<ffffffff813213c6>] sock_close+0x27/0x2b
    [  169.366872]   [<ffffffff8110dd45>] __fput+0x125/0x1ca
    [  169.366872]   [<ffffffff8110de04>] fput+0x1a/0x1c
    [  169.366872]   [<ffffffff8110adc9>] filp_close+0x68/0x72
    [  169.366872]   [<ffffffff8110ae80>] sys_close+0xad/0xe7
    [  169.366872]   [<ffffffff81009c02>] system_call_fastpath+0x16/0x1b
    
    (Trimmed at the "other info that might help us debug this" line in
    the interest of brevity... -- JWL)
    
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Acked-by: Sujith <Sujith.Manoharan@atheros.com>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>

commit c89094b689c85b9e6c2fcf97e0c7d81620e46f79
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 7 10:11:23 2010 -0800

    Fix race in tty_fasync() properly
    
    commit 80e1e823989ec44d8e35bdfddadbddcffec90424 upstream.
    
    This reverts commit 703625118069 ("tty: fix race in tty_fasync") and
    commit b04da8bfdfbb ("fnctl: f_modown should call write_lock_irqsave/
    restore") that tried to fix up some of the fallout but was incomplete.
    
    It turns out that we really cannot hold 'tty->ctrl_lock' over calling
    __f_setown, because not only did that cause problems with interrupt
    disables (which the second commit fixed), it also causes a potential
    ABBA deadlock due to lock ordering.
    
    Thanks to Tetsuo Handa for following up on the issue, and running
    lockdep to show the problem.  It goes roughly like this:
    
     - f_getown gets filp->f_owner.lock for reading without interrupts
       disabled, so an interrupt that happens while that lock is held can
       cause a lockdep chain from f_owner.lock -> sighand->siglock.
    
     - at the same time, the tty->ctrl_lock -> f_owner.lock chain that
       commit 703625118069 introduced, together with the pre-existing
       sighand->siglock -> tty->ctrl_lock chain means that we have a lock
       dependency the other way too.
    
    So instead of extending tty->ctrl_lock over the whole __f_setown() call,
    we now just take a reference to the 'pid' structure while holding the
    lock, and then release it after having done the __f_setown.  That still
    guarantees that 'struct pid' won't go away from under us, which is all
    we really ever needed.
    
    Reported-and-tested-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Acked-by: Greg Kroah-Hartman <gregkh@suse.de>
    Acked-by: Américo Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 51d5fe4109bff13686634a408988547e2d1b3add
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Tue Jan 26 15:04:02 2010 -0800

    fnctl: f_modown should call write_lock_irqsave/restore
    
    commit b04da8bfdfbbd79544cab2fadfdc12e87eb01600 upstream.
    
    Commit 703625118069f9f8960d356676662d3db5a9d116 exposed that f_modown()
    should call write_lock_irqsave instead of just write_lock_irq so that
    because a caller could have a spinlock held and it would not be good to
    renable interrupts.
    
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Tavis Ormandy <taviso@google.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 24fce8f6a79db04afef0c6118f6ecdcfec12ffc4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 7 10:11:23 2010 -0800

    Fix race in tty_fasync() properly
    
    commit 80e1e823989ec44d8e35bdfddadbddcffec90424 upstream.
    
    This reverts commit 703625118069 ("tty: fix race in tty_fasync") and
    commit b04da8bfdfbb ("fnctl: f_modown should call write_lock_irqsave/
    restore") that tried to fix up some of the fallout but was incomplete.
    
    It turns out that we really cannot hold 'tty->ctrl_lock' over calling
    __f_setown, because not only did that cause problems with interrupt
    disables (which the second commit fixed), it also causes a potential
    ABBA deadlock due to lock ordering.
    
    Thanks to Tetsuo Handa for following up on the issue, and running
    lockdep to show the problem.  It goes roughly like this:
    
     - f_getown gets filp->f_owner.lock for reading without interrupts
       disabled, so an interrupt that happens while that lock is held can
       cause a lockdep chain from f_owner.lock -> sighand->siglock.
    
     - at the same time, the tty->ctrl_lock -> f_owner.lock chain that
       commit 703625118069 introduced, together with the pre-existing
       sighand->siglock -> tty->ctrl_lock chain means that we have a lock
       dependency the other way too.
    
    So instead of extending tty->ctrl_lock over the whole __f_setown() call,
    we now just take a reference to the 'pid' structure while holding the
    lock, and then release it after having done the __f_setown.  That still
    guarantees that 'struct pid' won't go away from under us, which is all
    we really ever needed.
    
    Reported-and-tested-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Acked-by: Greg Kroah-Hartman <gregkh@suse.de>
    Acked-by: Américo Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit a2f46ee1ba5ee249ce2ca1ee7a7a0ac46529fb4f
Author: Neil Horman <nhorman@tuxdriver.com>
Date:   Tue Mar 16 08:14:33 2010 +0000

    tipc: fix lockdep warning on address assignment
    
    So in the forward porting of various tipc packages, I was constantly
    getting this lockdep warning everytime I used tipc-config to set a network
    address for the protocol:
    
    [ INFO: possible circular locking dependency detected ]
    2.6.33 #1
    tipc-config/1326 is trying to acquire lock:
    (ref_table_lock){+.-...}, at: [<ffffffffa0315148>] tipc_ref_discard+0x53/0xd4 [tipc]
    
    but task is already holding lock:
    (&(&entry->lock)->rlock#2){+.-...}, at: [<ffffffffa03150d5>] tipc_ref_lock+0x43/0x63 [tipc]
    
    which lock already depends on the new lock.
    
    the existing dependency chain (in reverse order) is:
    
    -> #1 (&(&entry->lock)->rlock#2){+.-...}:
    [<ffffffff8107b508>] __lock_acquire+0xb67/0xd0f
    [<ffffffff8107b78c>] lock_acquire+0xdc/0x102
    [<ffffffff8145471e>] _raw_spin_lock_bh+0x3b/0x6e
    [<ffffffffa03152b1>] tipc_ref_acquire+0xe8/0x11b [tipc]
    [<ffffffffa031433f>] tipc_createport_raw+0x78/0x1b9 [tipc]
    [<ffffffffa031450b>] tipc_createport+0x8b/0x125 [tipc]
    [<ffffffffa030f221>] tipc_subscr_start+0xce/0x126 [tipc]
    [<ffffffffa0308fb2>] process_signal_queue+0x47/0x7d [tipc]
    [<ffffffff81053e0c>] tasklet_action+0x8c/0xf4
    [<ffffffff81054bd8>] __do_softirq+0xf8/0x1cd
    [<ffffffff8100aadc>] call_softirq+0x1c/0x30
    [<ffffffff810549f4>] _local_bh_enable_ip+0xb8/0xd7
    [<ffffffff81054a21>] local_bh_enable_ip+0xe/0x10
    [<ffffffff81454d31>] _raw_spin_unlock_bh+0x34/0x39
    [<ffffffffa0308eb8>] spin_unlock_bh.clone.0+0x15/0x17 [tipc]
    [<ffffffffa0308f47>] tipc_k_signal+0x8d/0xb1 [tipc]
    [<ffffffffa0308dd9>] tipc_core_start+0x8a/0xad [tipc]
    [<ffffffffa01b1087>] 0xffffffffa01b1087
    [<ffffffff8100207d>] do_one_initcall+0x72/0x18a
    [<ffffffff810872fb>] sys_init_module+0xd8/0x23a
    [<ffffffff81009b42>] system_call_fastpath+0x16/0x1b
    
    -> #0 (ref_table_lock){+.-...}:
    [<ffffffff8107b3b2>] __lock_acquire+0xa11/0xd0f
    [<ffffffff8107b78c>] lock_acquire+0xdc/0x102
    [<ffffffff81454836>] _raw_write_lock_bh+0x3b/0x6e
    [<ffffffffa0315148>] tipc_ref_discard+0x53/0xd4 [tipc]
    [<ffffffffa03141ee>] tipc_deleteport+0x40/0x119 [tipc]
    [<ffffffffa0316e35>] release+0xeb/0x137 [tipc]
    [<ffffffff8139dbf4>] sock_release+0x1f/0x6f
    [<ffffffff8139dc6b>] sock_close+0x27/0x2b
    [<ffffffff811116f6>] __fput+0x12a/0x1df
    [<ffffffff811117c5>] fput+0x1a/0x1c
    [<ffffffff8110e49b>] filp_close+0x68/0x72
    [<ffffffff8110e552>] sys_close+0xad/0xe7
    [<ffffffff81009b42>] system_call_fastpath+0x16/0x1b
    
    Finally decided I should fix this.  Its a straightforward inversion,
    tipc_ref_acquire takes two locks in this order:
    ref_table_lock
    entry->lock
    
    while tipc_deleteport takes them in this order:
    entry->lock (via tipc_port_lock())
    ref_table_lock (via tipc_ref_discard())
    
    when the same entry is referenced, we get the above warning.  The fix is equally
    straightforward.  Theres no real relation between the entry->lock and the
    ref_table_lock (they just are needed at the same time), so move the entry->lock
    aquisition in tipc_ref_acquire down, after we unlock ref_table_lock (this is
    safe since the ref_table_lock guards changes to the reference table, and we've
    already claimed a slot there.  I've tested the below fix and confirmed that it
    clears up the lockdep issue
    
    Signed-off-by: Neil Horman <nhorman@tuxdriver.com>
    CC: Allan Stephens <allan.stephens@windriver.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 3520341ce308afaf676c961e8432ff5c87d3121b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 7 10:11:23 2010 -0800

    Fix race in tty_fasync() properly
    
    commit 80e1e823989ec44d8e35bdfddadbddcffec90424 upstream.
    
    This reverts commit 703625118069 ("tty: fix race in tty_fasync") and
    commit b04da8bfdfbb ("fnctl: f_modown should call write_lock_irqsave/
    restore") that tried to fix up some of the fallout but was incomplete.
    
    It turns out that we really cannot hold 'tty->ctrl_lock' over calling
    __f_setown, because not only did that cause problems with interrupt
    disables (which the second commit fixed), it also causes a potential
    ABBA deadlock due to lock ordering.
    
    Thanks to Tetsuo Handa for following up on the issue, and running
    lockdep to show the problem.  It goes roughly like this:
    
     - f_getown gets filp->f_owner.lock for reading without interrupts
       disabled, so an interrupt that happens while that lock is held can
       cause a lockdep chain from f_owner.lock -> sighand->siglock.
    
     - at the same time, the tty->ctrl_lock -> f_owner.lock chain that
       commit 703625118069 introduced, together with the pre-existing
       sighand->siglock -> tty->ctrl_lock chain means that we have a lock
       dependency the other way too.
    
    So instead of extending tty->ctrl_lock over the whole __f_setown() call,
    we now just take a reference to the 'pid' structure while holding the
    lock, and then release it after having done the __f_setown.  That still
    guarantees that 'struct pid' won't go away from under us, which is all
    we really ever needed.
    
    Reported-and-tested-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Acked-by: Greg Kroah-Hartman <gregkh@suse.de>
    Acked-by: Américo Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 80e1e823989ec44d8e35bdfddadbddcffec90424
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 7 10:11:23 2010 -0800

    Fix race in tty_fasync() properly
    
    This reverts commit 703625118069 ("tty: fix race in tty_fasync") and
    commit b04da8bfdfbb ("fnctl: f_modown should call write_lock_irqsave/
    restore") that tried to fix up some of the fallout but was incomplete.
    
    It turns out that we really cannot hold 'tty->ctrl_lock' over calling
    __f_setown, because not only did that cause problems with interrupt
    disables (which the second commit fixed), it also causes a potential
    ABBA deadlock due to lock ordering.
    
    Thanks to Tetsuo Handa for following up on the issue, and running
    lockdep to show the problem.  It goes roughly like this:
    
     - f_getown gets filp->f_owner.lock for reading without interrupts
       disabled, so an interrupt that happens while that lock is held can
       cause a lockdep chain from f_owner.lock -> sighand->siglock.
    
     - at the same time, the tty->ctrl_lock -> f_owner.lock chain that
       commit 703625118069 introduced, together with the pre-existing
       sighand->siglock -> tty->ctrl_lock chain means that we have a lock
       dependency the other way too.
    
    So instead of extending tty->ctrl_lock over the whole __f_setown() call,
    we now just take a reference to the 'pid' structure while holding the
    lock, and then release it after having done the __f_setown.  That still
    guarantees that 'struct pid' won't go away from under us, which is all
    we really ever needed.
    
    Reported-and-tested-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Acked-by: Greg Kroah-Hartman <gregkh@suse.de>
    Acked-by: Américo Wang <xiyou.wangcong@gmail.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ad9888834c693ec18a9c218409fa3c21a5e30c17
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Tue Jan 26 15:04:02 2010 -0800

    fnctl: f_modown should call write_lock_irqsave/restore
    
    commit b04da8bfdfbbd79544cab2fadfdc12e87eb01600 upstream.
    
    Commit 703625118069f9f8960d356676662d3db5a9d116 exposed that f_modown()
    should call write_lock_irqsave instead of just write_lock_irq so that
    because a caller could have a spinlock held and it would not be good to
    renable interrupts.
    
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Tavis Ormandy <taviso@google.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 0a1c275a44db55b2624652b04d4ed9430e82957b
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Tue Jan 26 15:04:02 2010 -0800

    fnctl: f_modown should call write_lock_irqsave/restore
    
    commit b04da8bfdfbbd79544cab2fadfdc12e87eb01600 upstream.
    
    Commit 703625118069f9f8960d356676662d3db5a9d116 exposed that f_modown()
    should call write_lock_irqsave instead of just write_lock_irq so that
    because a caller could have a spinlock held and it would not be good to
    renable interrupts.
    
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Tavis Ormandy <taviso@google.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit bbec919150037b8a2e58e32d3ba642ba3b6582a5
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jan 28 13:43:50 2010 +0100

    reiserfs: Fix vmalloc call under reiserfs lock
    
    Vmalloc is called to allocate journal->j_cnode_free_list but
    we hold the reiserfs lock at this time, which raises a
    {RECLAIM_FS-ON-W} -> {IN-RECLAIM_FS-W} lock inversion.
    
    Just drop the reiserfs lock at this time, as it's not even
    needed but kept for paranoid reasons.
    
    This fixes:
    
    [ INFO: inconsistent lock state ]
    2.6.33-rc5 #1
    ---------------------------------
    inconsistent {RECLAIM_FS-ON-W} -> {IN-RECLAIM_FS-W} usage.
    kswapd0/313 [HC0[0]:SC0[0]:HE1:SE1] takes:
     (&REISERFS_SB(s)->lock){+.+.?.}, at: [<c11118c8>]
    reiserfs_write_lock_once+0x28/0x50
    {RECLAIM_FS-ON-W} state was registered at:
      [<c104ee32>] mark_held_locks+0x62/0x90
      [<c104eefa>] lockdep_trace_alloc+0x9a/0xc0
      [<c108f7b6>] kmem_cache_alloc+0x26/0xf0
      [<c108621c>] __get_vm_area_node+0x6c/0xf0
      [<c108690e>] __vmalloc_node+0x7e/0xa0
      [<c1086aab>] vmalloc+0x2b/0x30
      [<c110e1fb>] journal_init+0x6cb/0xa10
      [<c10f90a2>] reiserfs_fill_super+0x342/0xb80
      [<c1095665>] get_sb_bdev+0x145/0x180
      [<c10f68e1>] get_super_block+0x21/0x30
      [<c1094520>] vfs_kern_mount+0x40/0xd0
      [<c1094609>] do_kern_mount+0x39/0xd0
      [<c10aaa97>] do_mount+0x2c7/0x6d0
      [<c10aaf06>] sys_mount+0x66/0xa0
      [<c16198a7>] mount_block_root+0xc4/0x245
      [<c1619a81>] mount_root+0x59/0x5f
      [<c1619b98>] prepare_namespace+0x111/0x14b
      [<c1619269>] kernel_init+0xcf/0xdb
      [<c100303a>] kernel_thread_helper+0x6/0x1c
    irq event stamp: 63236801
    hardirqs last  enabled at (63236801): [<c134e7fa>]
    __mutex_unlock_slowpath+0x9a/0x120
    hardirqs last disabled at (63236800): [<c134e799>]
    __mutex_unlock_slowpath+0x39/0x120
    softirqs last  enabled at (63218800): [<c102f451>] __do_softirq+0xc1/0x110
    softirqs last disabled at (63218789): [<c102f4ed>] do_softirq+0x4d/0x60
    
    other info that might help us debug this:
    2 locks held by kswapd0/313:
     #0:  (shrinker_rwsem){++++..}, at: [<c1074bb4>] shrink_slab+0x24/0x170
     #1:  (&type->s_umount_key#19){++++..}, at: [<c10a2edd>]
    shrink_dcache_memory+0xfd/0x1a0
    
    stack backtrace:
    Pid: 313, comm: kswapd0 Not tainted 2.6.33-rc5 #1
    Call Trace:
     [<c134db2c>] ? printk+0x18/0x1c
     [<c104e7ef>] print_usage_bug+0x15f/0x1a0
     [<c104ebcf>] mark_lock+0x39f/0x5a0
     [<c104d66b>] ? trace_hardirqs_off+0xb/0x10
     [<c1052c50>] ? check_usage_forwards+0x0/0xf0
     [<c1050c24>] __lock_acquire+0x214/0xa70
     [<c10438c5>] ? sched_clock_cpu+0x95/0x110
     [<c10514fa>] lock_acquire+0x7a/0xa0
     [<c11118c8>] ? reiserfs_write_lock_once+0x28/0x50
     [<c134f03f>] mutex_lock_nested+0x5f/0x2b0
     [<c11118c8>] ? reiserfs_write_lock_once+0x28/0x50
     [<c11118c8>] ? reiserfs_write_lock_once+0x28/0x50
     [<c11118c8>] reiserfs_write_lock_once+0x28/0x50
     [<c10f05b0>] reiserfs_delete_inode+0x50/0x140
     [<c10a653f>] ? generic_delete_inode+0x5f/0x150
     [<c10f0560>] ? reiserfs_delete_inode+0x0/0x140
     [<c10a657c>] generic_delete_inode+0x9c/0x150
     [<c10a666d>] generic_drop_inode+0x3d/0x60
     [<c10a5597>] iput+0x47/0x50
     [<c10a2a4f>] dentry_iput+0x6f/0xf0
     [<c10a2af4>] d_kill+0x24/0x50
     [<c10a2d3d>] __shrink_dcache_sb+0x21d/0x2b0
     [<c10a2f0f>] shrink_dcache_memory+0x12f/0x1a0
     [<c1074c9e>] shrink_slab+0x10e/0x170
     [<c1075177>] kswapd+0x477/0x6a0
     [<c1072d10>] ? isolate_pages_global+0x0/0x1b0
     [<c103e160>] ? autoremove_wake_function+0x0/0x40
     [<c1074d00>] ? kswapd+0x0/0x6a0
     [<c103de6c>] kthread+0x6c/0x80
     [<c103de00>] ? kthread+0x0/0x80
     [<c100303a>] kernel_thread_helper+0x6/0x1c
    
    Reported-by: Alexander Beregalov <a.beregalov@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Christian Kujau <lists@nerdbynature.de>
    Cc: Chris Mason <chris.mason@oracle.com>

commit b04da8bfdfbbd79544cab2fadfdc12e87eb01600
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Tue Jan 26 15:04:02 2010 -0800

    fnctl: f_modown should call write_lock_irqsave/restore
    
    Commit 703625118069f9f8960d356676662d3db5a9d116 exposed that f_modown()
    should call write_lock_irqsave instead of just write_lock_irq so that
    because a caller could have a spinlock held and it would not be good to
    renable interrupts.
    
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Tavis Ormandy <taviso@google.com>
    Cc: stable <stable@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 108d3943c021f0b66e860ba98ded40b82b677bd7
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 5 00:15:38 2010 +0100

    reiserfs: Relax the lock before truncating pages
    
    While truncating a file, reiserfs_setattr() calls inode_setattr()
    that will truncate the mapping for the given inode, but for that
    it needs the pages locks.
    
    In order to release these, the owners need the reiserfs lock to
    complete their jobs. But they can't, as we don't release it before
    calling inode_setattr().
    
    We need to do that to fix the following softlockups:
    
    INFO: task flush-8:0:2149 blocked for more than 120 seconds.
    "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    flush-8:0     D f51af998     0  2149      2 0x00000000
     f51af9ac 00000092 00000002 f51af998 c2803304 00000000 c1894ad0 010f3000
     f51af9cc c1462604 c189ef80 f51af974 c1710304 f715b450 f715b5ec c2807c40
     00000000 0005bb00 c2803320 c102c55b c1710304 c2807c50 c2803304 00000246
    Call Trace:
     [<c1462604>] ? schedule+0x434/0xb20
     [<c102c55b>] ? resched_task+0x4b/0x70
     [<c106fa22>] ? mark_held_locks+0x62/0x80
     [<c146414d>] ? mutex_lock_nested+0x1fd/0x350
     [<c14640b9>] mutex_lock_nested+0x169/0x350
     [<c1178cde>] ? reiserfs_write_lock+0x2e/0x40
     [<c1178cde>] reiserfs_write_lock+0x2e/0x40
     [<c11719a2>] do_journal_end+0xc2/0xe70
     [<c1172912>] journal_end+0xb2/0x120
     [<c11686b3>] ? pathrelse+0x33/0xb0
     [<c11729e4>] reiserfs_end_persistent_transaction+0x64/0x70
     [<c1153caa>] reiserfs_get_block+0x12ba/0x15f0
     [<c106fa22>] ? mark_held_locks+0x62/0x80
     [<c1154b24>] reiserfs_writepage+0xa74/0xe80
     [<c1465a27>] ? _raw_spin_unlock_irq+0x27/0x50
     [<c11f3d25>] ? radix_tree_gang_lookup_tag_slot+0x95/0xc0
     [<c10b5377>] ? find_get_pages_tag+0x127/0x1a0
     [<c106fa22>] ? mark_held_locks+0x62/0x80
     [<c106fcd4>] ? trace_hardirqs_on_caller+0x124/0x170
     [<c10bc1e0>] __writepage+0x10/0x40
     [<c10bc9ab>] write_cache_pages+0x16b/0x320
     [<c10bc1d0>] ? __writepage+0x0/0x40
     [<c10bcb88>] generic_writepages+0x28/0x40
     [<c10bcbd5>] do_writepages+0x35/0x40
     [<c11059f7>] writeback_single_inode+0xc7/0x330
     [<c11067b2>] writeback_inodes_wb+0x2c2/0x490
     [<c1106a86>] wb_writeback+0x106/0x1b0
     [<c1106cf6>] wb_do_writeback+0x106/0x1e0
     [<c1106c18>] ? wb_do_writeback+0x28/0x1e0
     [<c1106e0a>] bdi_writeback_task+0x3a/0xb0
     [<c10cbb13>] bdi_start_fn+0x63/0xc0
     [<c10cbab0>] ? bdi_start_fn+0x0/0xc0
     [<c105d1f4>] kthread+0x74/0x80
     [<c105d180>] ? kthread+0x0/0x80
     [<c100327a>] kernel_thread_helper+0x6/0x10
    3 locks held by flush-8:0/2149:
     #0:  (&type->s_umount_key#30){+++++.}, at: [<c110676f>] writeback_inodes_wb+0x27f/0x490
     #1:  (&journal->j_mutex){+.+...}, at: [<c117199a>] do_journal_end+0xba/0xe70
     #2:  (&REISERFS_SB(s)->lock){+.+.+.}, at: [<c1178cde>] reiserfs_write_lock+0x2e/0x40
    INFO: task fstest:3813 blocked for more than 120 seconds.
    "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    fstest        D 00000002     0  3813   3812 0x00000000
     f5103c94 00000082 f5103c40 00000002 f5ad5450 00000007 f5103c28 011f3000
     00000006 f5ad5450 c10bb005 00000480 c1710304 f5ad5450 f5ad55ec c2907c40
     00000001 f5ad5450 f5103c74 00000046 00000002 f5ad5450 00000007 f5103c6c
    Call Trace:
     [<c10bb005>] ? free_hot_cold_page+0x1d5/0x280
     [<c1462d64>] io_schedule+0x74/0xc0
     [<c10b5a45>] sync_page+0x35/0x60
     [<c146325a>] __wait_on_bit_lock+0x4a/0x90
     [<c10b5a10>] ? sync_page+0x0/0x60
     [<c10b59e5>] __lock_page+0x85/0x90
     [<c105d660>] ? wake_bit_function+0x0/0x60
     [<c10bf654>] truncate_inode_pages_range+0x1e4/0x2d0
     [<c10bf75f>] truncate_inode_pages+0x1f/0x30
     [<c10bf7cf>] truncate_pagecache+0x5f/0xa0
     [<c10bf86a>] vmtruncate+0x5a/0x70
     [<c10fdb7d>] inode_setattr+0x5d/0x190
     [<c1150117>] reiserfs_setattr+0x1f7/0x2f0
     [<c1464569>] ? down_write+0x49/0x70
     [<c10fde01>] notify_change+0x151/0x330
     [<c10e6f3d>] do_truncate+0x6d/0xa0
     [<c10f4ce2>] do_filp_open+0x9a2/0xcf0
     [<c1465aec>] ? _raw_spin_unlock+0x2c/0x50
     [<c10fec50>] ? alloc_fd+0xe0/0x100
     [<c10e602d>] do_sys_open+0x6d/0x130
     [<c1002cfb>] ? sysenter_exit+0xf/0x16
     [<c10e615e>] sys_open+0x2e/0x40
     [<c1002ccc>] sysenter_do_call+0x12/0x32
    3 locks held by fstest/3813:
     #0:  (&sb->s_type->i_mutex_key#4){+.+.+.}, at: [<c10e6f33>] do_truncate+0x63/0xa0
     #1:  (&sb->s_type->i_alloc_sem_key#3){+.+.+.}, at: [<c10fdf07>] notify_change+0x257/0x330
     #2:  (&REISERFS_SB(s)->lock){+.+.+.}, at: [<c1178c8e>] reiserfs_write_lock_once+0x2e/0x50
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Christian Kujau <lists@nerdbynature.de>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>

commit 5fe1533fda8ae005541bd418a7a8bc4fa0cda522
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Jan 4 22:04:01 2010 +0100

    reiserfs: Fix recursive lock on lchown
    
    On chown, reiserfs will call reiserfs_setattr() to change the owner
    of the given inode, but it may also recursively call
    reiserfs_setattr() to propagate the owner change to the private xattr
    files for this inode.
    
    Hence, the reiserfs lock may be acquired twice which is not wanted
    as reiserfs_setattr() calls journal_begin() that is going to try to
    relax the lock in order to safely acquire the journal mutex.
    
    Using reiserfs_write_lock_once() from reiserfs_setattr() solves
    the problem.
    
    This fixes the following warning, that precedes a lockdep report.
    
    WARNING: at fs/reiserfs/lock.c:95 reiserfs_lock_check_recursive+0x3f/0x50()
    Hardware name: MS-7418
    Unwanted recursive reiserfs lock!
    Pid: 4189, comm: fsstress Not tainted 2.6.33-rc2-tip-atom+ #195
    Call Trace:
     [<c1178bff>] ? reiserfs_lock_check_recursive+0x3f/0x50
     [<c1178bff>] ? reiserfs_lock_check_recursive+0x3f/0x50
     [<c103f7ac>] warn_slowpath_common+0x6c/0xc0
     [<c1178bff>] ? reiserfs_lock_check_recursive+0x3f/0x50
     [<c103f84b>] warn_slowpath_fmt+0x2b/0x30
     [<c1178bff>] reiserfs_lock_check_recursive+0x3f/0x50
     [<c1172ae3>] do_journal_begin_r+0x83/0x350
     [<c1172f2d>] journal_begin+0x7d/0x140
     [<c106509a>] ? in_group_p+0x2a/0x30
     [<c10fda71>] ? inode_change_ok+0x91/0x140
     [<c115007d>] reiserfs_setattr+0x15d/0x2e0
     [<c10f9bf3>] ? dput+0xe3/0x140
     [<c1465adc>] ? _raw_spin_unlock+0x2c/0x50
     [<c117831d>] chown_one_xattr+0xd/0x10
     [<c11780a3>] reiserfs_for_each_xattr+0x113/0x2c0
     [<c1178310>] ? chown_one_xattr+0x0/0x10
     [<c14641e9>] ? mutex_lock_nested+0x2a9/0x350
     [<c117826f>] reiserfs_chown_xattrs+0x1f/0x60
     [<c106509a>] ? in_group_p+0x2a/0x30
     [<c10fda71>] ? inode_change_ok+0x91/0x140
     [<c1150046>] reiserfs_setattr+0x126/0x2e0
     [<c1177c20>] ? reiserfs_getxattr+0x0/0x90
     [<c11b0d57>] ? cap_inode_need_killpriv+0x37/0x50
     [<c10fde01>] notify_change+0x151/0x330
     [<c10e659f>] chown_common+0x6f/0x90
     [<c10e67bd>] sys_lchown+0x6d/0x80
     [<c1002ccc>] sysenter_do_call+0x12/0x32
    ---[ end trace 7c2b77224c1442fc ]---
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Christian Kujau <lists@nerdbynature.de>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>

commit 846f99749ab68bbc7f75c74fec305de675b1a1bf
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Jan 2 13:37:12 2010 -0800

    sysfs: Add lockdep annotations for the sysfs active reference
    
    Holding locks over device_del -> kobject_del -> sysfs_deactivate can
    cause deadlocks if those same locks are grabbed in sysfs show or store
    methods.
    
    The I model s_active count + completion as a sleeping read/write lock.
    I describe to lockdep sysfs_get_active as a read_trylock,
    sysfs_put_active as a read_unlock, and sysfs_deactivate as a
    write_lock and write_unlock pair.  This seems to capture the essence
    for purposes of finding deadlocks, and in my testing gives finds real
    issues and ignores non-issues.
    
    This brings us back to holding locks over kobject_del is a problem
    that ideally we should find a way of addressing, but at least lockdep
    can tell us about the problems instead of requiring developers to debug
    rare strange system deadlocks, that happen when sysfs files are removed
    while being written to.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 8b513f56d4e117f11cf0760abcc030eedefc45c3
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 30 07:28:58 2009 +0100

    reiserfs: Safely acquire i_mutex from reiserfs_for_each_xattr
    
    Relax the reiserfs lock before taking the inode mutex from
    reiserfs_for_each_xattr() to avoid the usual bad dependencies:
    
    =======================================================
    [ INFO: possible circular locking dependency detected ]
    2.6.32-atom #179
    -------------------------------------------------------
    rm/3242 is trying to acquire lock:
     (&sb->s_type->i_mutex_key#4/3){+.+.+.}, at: [<c11428ef>] reiserfs_for_each_xattr+0x23f/0x290
    
    but task is already holding lock:
     (&REISERFS_SB(s)->lock){+.+.+.}, at: [<c1143389>] reiserfs_write_lock+0x29/0x40
    
    which lock already depends on the new lock.
    
    the existing dependency chain (in reverse order) is:
    
    -> #1 (&REISERFS_SB(s)->lock){+.+.+.}:
           [<c105ea7f>] __lock_acquire+0x11ff/0x19e0
           [<c105f2c8>] lock_acquire+0x68/0x90
           [<c1401aab>] mutex_lock_nested+0x5b/0x340
           [<c1143339>] reiserfs_write_lock_once+0x29/0x50
           [<c1117022>] reiserfs_lookup+0x62/0x140
           [<c10bd85f>] __lookup_hash+0xef/0x110
           [<c10bf21d>] lookup_one_len+0x8d/0xc0
           [<c1141e3a>] open_xa_dir+0xea/0x1b0
           [<c1142720>] reiserfs_for_each_xattr+0x70/0x290
           [<c11429ba>] reiserfs_delete_xattrs+0x1a/0x60
           [<c111ea2f>] reiserfs_delete_inode+0x9f/0x150
           [<c10c9c32>] generic_delete_inode+0xa2/0x170
           [<c10c9d4f>] generic_drop_inode+0x4f/0x70
           [<c10c8b07>] iput+0x47/0x50
           [<c10c0965>] do_unlinkat+0xd5/0x160
           [<c10c0b13>] sys_unlinkat+0x23/0x40
           [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    -> #0 (&sb->s_type->i_mutex_key#4/3){+.+.+.}:
           [<c105f176>] __lock_acquire+0x18f6/0x19e0
           [<c105f2c8>] lock_acquire+0x68/0x90
           [<c1401aab>] mutex_lock_nested+0x5b/0x340
           [<c11428ef>] reiserfs_for_each_xattr+0x23f/0x290
           [<c11429ba>] reiserfs_delete_xattrs+0x1a/0x60
           [<c111ea2f>] reiserfs_delete_inode+0x9f/0x150
           [<c10c9c32>] generic_delete_inode+0xa2/0x170
           [<c10c9d4f>] generic_drop_inode+0x4f/0x70
           [<c10c8b07>] iput+0x47/0x50
           [<c10c0965>] do_unlinkat+0xd5/0x160
           [<c10c0b13>] sys_unlinkat+0x23/0x40
           [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    other info that might help us debug this:
    
    1 lock held by rm/3242:
     #0:  (&REISERFS_SB(s)->lock){+.+.+.}, at: [<c1143389>] reiserfs_write_lock+0x29/0x40
    
    stack backtrace:
    Pid: 3242, comm: rm Not tainted 2.6.32-atom #179
    Call Trace:
     [<c13ffa13>] ? printk+0x18/0x1a
     [<c105d33a>] print_circular_bug+0xca/0xd0
     [<c105f176>] __lock_acquire+0x18f6/0x19e0
     [<c105c932>] ? mark_held_locks+0x62/0x80
     [<c105cc3b>] ? trace_hardirqs_on+0xb/0x10
     [<c1401098>] ? mutex_unlock+0x8/0x10
     [<c105f2c8>] lock_acquire+0x68/0x90
     [<c11428ef>] ? reiserfs_for_each_xattr+0x23f/0x290
     [<c11428ef>] ? reiserfs_for_each_xattr+0x23f/0x290
     [<c1401aab>] mutex_lock_nested+0x5b/0x340
     [<c11428ef>] ? reiserfs_for_each_xattr+0x23f/0x290
     [<c11428ef>] reiserfs_for_each_xattr+0x23f/0x290
     [<c1143180>] ? delete_one_xattr+0x0/0x100
     [<c11429ba>] reiserfs_delete_xattrs+0x1a/0x60
     [<c1143339>] ? reiserfs_write_lock_once+0x29/0x50
     [<c111ea2f>] reiserfs_delete_inode+0x9f/0x150
     [<c11b0d4f>] ? _atomic_dec_and_lock+0x4f/0x70
     [<c111e990>] ? reiserfs_delete_inode+0x0/0x150
     [<c10c9c32>] generic_delete_inode+0xa2/0x170
     [<c10c9d4f>] generic_drop_inode+0x4f/0x70
     [<c10c8b07>] iput+0x47/0x50
     [<c10c0965>] do_unlinkat+0xd5/0x160
     [<c1401098>] ? mutex_unlock+0x8/0x10
     [<c10c3e0d>] ? vfs_readdir+0x7d/0xb0
     [<c10c3af0>] ? filldir64+0x0/0xf0
     [<c1002ef3>] ? sysenter_exit+0xf/0x16
     [<c105cbe4>] ? trace_hardirqs_on_caller+0x124/0x170
     [<c10c0b13>] sys_unlinkat+0x23/0x40
     [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Tested-by: Christian Kujau <lists@nerdbynature.de>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>

commit 4dd859697f836cf62c8de08bd9a9f4b4f4beaa91
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 30 07:26:28 2009 +0100

    reiserfs: Fix journal mutex <-> inode mutex lock inversion
    
    We need to relax the reiserfs lock before locking the inode mutex
    from xattr_unlink(), otherwise we'll face the usual bad dependencies:
    
    =======================================================
    [ INFO: possible circular locking dependency detected ]
    2.6.32-atom #178
    -------------------------------------------------------
    rm/3202 is trying to acquire lock:
     (&journal->j_mutex){+.+...}, at: [<c113c234>] do_journal_begin_r+0x94/0x360
    
    but task is already holding lock:
     (&sb->s_type->i_mutex_key#4/2){+.+...}, at: [<c1142a67>] xattr_unlink+0x57/0xb0
    
    which lock already depends on the new lock.
    
    the existing dependency chain (in reverse order) is:
    
    -> #2 (&sb->s_type->i_mutex_key#4/2){+.+...}:
           [<c105ea7f>] __lock_acquire+0x11ff/0x19e0
           [<c105f2c8>] lock_acquire+0x68/0x90
           [<c1401a7b>] mutex_lock_nested+0x5b/0x340
           [<c1142a67>] xattr_unlink+0x57/0xb0
           [<c1143179>] delete_one_xattr+0x29/0x100
           [<c11427bb>] reiserfs_for_each_xattr+0x10b/0x290
           [<c11429ba>] reiserfs_delete_xattrs+0x1a/0x60
           [<c111ea2f>] reiserfs_delete_inode+0x9f/0x150
           [<c10c9c32>] generic_delete_inode+0xa2/0x170
           [<c10c9d4f>] generic_drop_inode+0x4f/0x70
           [<c10c8b07>] iput+0x47/0x50
           [<c10c0965>] do_unlinkat+0xd5/0x160
           [<c10c0b13>] sys_unlinkat+0x23/0x40
           [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    -> #1 (&REISERFS_SB(s)->lock){+.+.+.}:
           [<c105ea7f>] __lock_acquire+0x11ff/0x19e0
           [<c105f2c8>] lock_acquire+0x68/0x90
           [<c1401a7b>] mutex_lock_nested+0x5b/0x340
           [<c1143359>] reiserfs_write_lock+0x29/0x40
           [<c113c23c>] do_journal_begin_r+0x9c/0x360
           [<c113c680>] journal_begin+0x80/0x130
           [<c1127363>] reiserfs_remount+0x223/0x4e0
           [<c10b6dd6>] do_remount_sb+0xa6/0x140
           [<c10ce6a0>] do_mount+0x560/0x750
           [<c10ce914>] sys_mount+0x84/0xb0
           [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    -> #0 (&journal->j_mutex){+.+...}:
           [<c105f176>] __lock_acquire+0x18f6/0x19e0
           [<c105f2c8>] lock_acquire+0x68/0x90
           [<c1401a7b>] mutex_lock_nested+0x5b/0x340
           [<c113c234>] do_journal_begin_r+0x94/0x360
           [<c113c680>] journal_begin+0x80/0x130
           [<c1116d63>] reiserfs_unlink+0x83/0x2e0
           [<c1142a74>] xattr_unlink+0x64/0xb0
           [<c1143179>] delete_one_xattr+0x29/0x100
           [<c11427bb>] reiserfs_for_each_xattr+0x10b/0x290
           [<c11429ba>] reiserfs_delete_xattrs+0x1a/0x60
           [<c111ea2f>] reiserfs_delete_inode+0x9f/0x150
           [<c10c9c32>] generic_delete_inode+0xa2/0x170
           [<c10c9d4f>] generic_drop_inode+0x4f/0x70
           [<c10c8b07>] iput+0x47/0x50
           [<c10c0965>] do_unlinkat+0xd5/0x160
           [<c10c0b13>] sys_unlinkat+0x23/0x40
           [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    other info that might help us debug this:
    
    2 locks held by rm/3202:
     #0:  (&sb->s_type->i_mutex_key#4/3){+.+.+.}, at: [<c114274b>] reiserfs_for_each_xattr+0x9b/0x290
     #1:  (&sb->s_type->i_mutex_key#4/2){+.+...}, at: [<c1142a67>] xattr_unlink+0x57/0xb0
    
    stack backtrace:
    Pid: 3202, comm: rm Not tainted 2.6.32-atom #178
    Call Trace:
     [<c13ff9e3>] ? printk+0x18/0x1a
     [<c105d33a>] print_circular_bug+0xca/0xd0
     [<c105f176>] __lock_acquire+0x18f6/0x19e0
     [<c1142a67>] ? xattr_unlink+0x57/0xb0
     [<c105f2c8>] lock_acquire+0x68/0x90
     [<c113c234>] ? do_journal_begin_r+0x94/0x360
     [<c113c234>] ? do_journal_begin_r+0x94/0x360
     [<c1401a7b>] mutex_lock_nested+0x5b/0x340
     [<c113c234>] ? do_journal_begin_r+0x94/0x360
     [<c113c234>] do_journal_begin_r+0x94/0x360
     [<c10411b6>] ? run_timer_softirq+0x1a6/0x220
     [<c103cb00>] ? __do_softirq+0x50/0x140
     [<c113c680>] journal_begin+0x80/0x130
     [<c103cba2>] ? __do_softirq+0xf2/0x140
     [<c104f72f>] ? hrtimer_interrupt+0xdf/0x220
     [<c1116d63>] reiserfs_unlink+0x83/0x2e0
     [<c105c932>] ? mark_held_locks+0x62/0x80
     [<c11b8d08>] ? trace_hardirqs_on_thunk+0xc/0x10
     [<c1002fd8>] ? restore_all_notrace+0x0/0x18
     [<c1142a67>] ? xattr_unlink+0x57/0xb0
     [<c1142a74>] xattr_unlink+0x64/0xb0
     [<c1143179>] delete_one_xattr+0x29/0x100
     [<c11427bb>] reiserfs_for_each_xattr+0x10b/0x290
     [<c1143150>] ? delete_one_xattr+0x0/0x100
     [<c1401cb9>] ? mutex_lock_nested+0x299/0x340
     [<c11429ba>] reiserfs_delete_xattrs+0x1a/0x60
     [<c1143309>] ? reiserfs_write_lock_once+0x29/0x50
     [<c111ea2f>] reiserfs_delete_inode+0x9f/0x150
     [<c11b0d1f>] ? _atomic_dec_and_lock+0x4f/0x70
     [<c111e990>] ? reiserfs_delete_inode+0x0/0x150
     [<c10c9c32>] generic_delete_inode+0xa2/0x170
     [<c10c9d4f>] generic_drop_inode+0x4f/0x70
     [<c10c8b07>] iput+0x47/0x50
     [<c10c0965>] do_unlinkat+0xd5/0x160
     [<c1401068>] ? mutex_unlock+0x8/0x10
     [<c10c3e0d>] ? vfs_readdir+0x7d/0xb0
     [<c10c3af0>] ? filldir64+0x0/0xf0
     [<c1002ef3>] ? sysenter_exit+0xf/0x16
     [<c105cbe4>] ? trace_hardirqs_on_caller+0x124/0x170
     [<c10c0b13>] sys_unlinkat+0x23/0x40
     [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Tested-by: Christian Kujau <lists@nerdbynature.de>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>

commit c674905ca74ad0ae5b048afb1ef68663a0d7e987
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 30 07:12:03 2009 +0100

    reiserfs: Fix unwanted recursive reiserfs lock in reiserfs_unlink()
    
    reiserfs_unlink() may or may not be called under the reiserfs
    lock.
    But it also takes the reiserfs lock and can then acquire it
    recursively which leads to do_journal_begin_r() that fails to
    relax the reiserfs lock before grabbing the journal mutex,
    creating an unexpected lock inversion.
    
    We need to ensure reiserfs_unlink() won't get the reiserfs lock
    recursively using reiserfs_write_lock_once().
    
    This fixes the following warning that precedes a lock inversion
    report (reiserfs lock <-> journal mutex).
    
    ------------[ cut here ]------------
    WARNING: at fs/reiserfs/lock.c:95 reiserfs_lock_check_recursive+0x3a/0x50()
    Hardware name: MS-7418
    Unwanted recursive reiserfs lock!
    Pid: 3208, comm: dbench Not tainted 2.6.32-atom #177
    Call Trace:
     [<c114327a>] ? reiserfs_lock_check_recursive+0x3a/0x50
     [<c114327a>] ? reiserfs_lock_check_recursive+0x3a/0x50
     [<c10373a7>] warn_slowpath_common+0x67/0xc0
     [<c114327a>] ? reiserfs_lock_check_recursive+0x3a/0x50
     [<c1037446>] warn_slowpath_fmt+0x26/0x30
     [<c114327a>] reiserfs_lock_check_recursive+0x3a/0x50
     [<c113c213>] do_journal_begin_r+0x83/0x360
     [<c105eb16>] ? __lock_acquire+0x1296/0x19e0
     [<c1142a57>] ? xattr_unlink+0x57/0xb0
     [<c113c670>] journal_begin+0x80/0x130
     [<c1116d5d>] reiserfs_unlink+0x7d/0x2d0
     [<c1142a57>] ? xattr_unlink+0x57/0xb0
     [<c1142a57>] ? xattr_unlink+0x57/0xb0
     [<c1142a57>] ? xattr_unlink+0x57/0xb0
     [<c1142a64>] xattr_unlink+0x64/0xb0
     [<c1143169>] delete_one_xattr+0x29/0x100
     [<c11427ab>] reiserfs_for_each_xattr+0x10b/0x290
     [<c1143140>] ? delete_one_xattr+0x0/0x100
     [<c1401ca9>] ? mutex_lock_nested+0x299/0x340
     [<c11429aa>] reiserfs_delete_xattrs+0x1a/0x60
     [<c11432f9>] ? reiserfs_write_lock_once+0x29/0x50
     [<c111ea1f>] reiserfs_delete_inode+0x9f/0x150
     [<c11b0d0f>] ? _atomic_dec_and_lock+0x4f/0x70
     [<c111e980>] ? reiserfs_delete_inode+0x0/0x150
     [<c10c9c32>] generic_delete_inode+0xa2/0x170
     [<c10c9d4f>] generic_drop_inode+0x4f/0x70
     [<c10c8b07>] iput+0x47/0x50
     [<c10c0965>] do_unlinkat+0xd5/0x160
     [<c10505c6>] ? up_read+0x16/0x30
     [<c1022ab7>] ? do_page_fault+0x187/0x330
     [<c1002fd8>] ? restore_all_notrace+0x0/0x18
     [<c1022930>] ? do_page_fault+0x0/0x330
     [<c105cbe4>] ? trace_hardirqs_on_caller+0x124/0x170
     [<c10c0a00>] sys_unlink+0x10/0x20
     [<c1002ec4>] sysenter_do_call+0x12/0x32
    ---[ end trace 2e35d71a6cc69d0c ]---
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Tested-by: Christian Kujau <lists@nerdbynature.de>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>

commit 3f14fea6bbd3444dd46a2af3a2e219e792616645
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 30 07:03:53 2009 +0100

    reiserfs: Relax lock before open xattr dir in reiserfs_xattr_set_handle()
    
    We call xattr_lookup() from reiserfs_xattr_get(). We then hold
    the reiserfs lock when we grab the i_mutex. But later, we may
    relax the reiserfs lock, creating dependency inversion between
    both locks.
    
    The lookups and creation jobs ar already protected by the
    inode mutex, so we can safely relax the reiserfs lock, dropping
    the unwanted reiserfs lock -> i_mutex dependency, as shown
    in the following lockdep report:
    
    =======================================================
    [ INFO: possible circular locking dependency detected ]
    2.6.32-atom #173
    -------------------------------------------------------
    cp/3204 is trying to acquire lock:
     (&REISERFS_SB(s)->lock){+.+.+.}, at: [<c11432b9>] reiserfs_write_lock_once+0x29/0x50
    
    but task is already holding lock:
     (&sb->s_type->i_mutex_key#4/3){+.+.+.}, at: [<c1141e18>] open_xa_dir+0xd8/0x1b0
    
    which lock already depends on the new lock.
    
    the existing dependency chain (in reverse order) is:
    
    -> #1 (&sb->s_type->i_mutex_key#4/3){+.+.+.}:
           [<c105ea7f>] __lock_acquire+0x11ff/0x19e0
           [<c105f2c8>] lock_acquire+0x68/0x90
           [<c1401a2b>] mutex_lock_nested+0x5b/0x340
           [<c1141d83>] open_xa_dir+0x43/0x1b0
           [<c1142722>] reiserfs_for_each_xattr+0x62/0x260
           [<c114299a>] reiserfs_delete_xattrs+0x1a/0x60
           [<c111ea1f>] reiserfs_delete_inode+0x9f/0x150
           [<c10c9c32>] generic_delete_inode+0xa2/0x170
           [<c10c9d4f>] generic_drop_inode+0x4f/0x70
           [<c10c8b07>] iput+0x47/0x50
           [<c10c0965>] do_unlinkat+0xd5/0x160
           [<c10c0a00>] sys_unlink+0x10/0x20
           [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    -> #0 (&REISERFS_SB(s)->lock){+.+.+.}:
           [<c105f176>] __lock_acquire+0x18f6/0x19e0
           [<c105f2c8>] lock_acquire+0x68/0x90
           [<c1401a2b>] mutex_lock_nested+0x5b/0x340
           [<c11432b9>] reiserfs_write_lock_once+0x29/0x50
           [<c1117012>] reiserfs_lookup+0x62/0x140
           [<c10bd85f>] __lookup_hash+0xef/0x110
           [<c10bf21d>] lookup_one_len+0x8d/0xc0
           [<c1141e2a>] open_xa_dir+0xea/0x1b0
           [<c1141fe5>] xattr_lookup+0x15/0x160
           [<c1142476>] reiserfs_xattr_get+0x56/0x2a0
           [<c1144042>] reiserfs_get_acl+0xa2/0x360
           [<c114461a>] reiserfs_cache_default_acl+0x3a/0x160
           [<c111789c>] reiserfs_mkdir+0x6c/0x2c0
           [<c10bea96>] vfs_mkdir+0xd6/0x180
           [<c10c0c10>] sys_mkdirat+0xc0/0xd0
           [<c10c0c40>] sys_mkdir+0x20/0x30
           [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    other info that might help us debug this:
    
    2 locks held by cp/3204:
     #0:  (&sb->s_type->i_mutex_key#4/1){+.+.+.}, at: [<c10bd8d6>] lookup_create+0x26/0xa0
     #1:  (&sb->s_type->i_mutex_key#4/3){+.+.+.}, at: [<c1141e18>] open_xa_dir+0xd8/0x1b0
    
    stack backtrace:
    Pid: 3204, comm: cp Not tainted 2.6.32-atom #173
    Call Trace:
     [<c13ff993>] ? printk+0x18/0x1a
     [<c105d33a>] print_circular_bug+0xca/0xd0
     [<c105f176>] __lock_acquire+0x18f6/0x19e0
     [<c105d3aa>] ? check_usage+0x6a/0x460
     [<c105f2c8>] lock_acquire+0x68/0x90
     [<c11432b9>] ? reiserfs_write_lock_once+0x29/0x50
     [<c11432b9>] ? reiserfs_write_lock_once+0x29/0x50
     [<c1401a2b>] mutex_lock_nested+0x5b/0x340
     [<c11432b9>] ? reiserfs_write_lock_once+0x29/0x50
     [<c11432b9>] reiserfs_write_lock_once+0x29/0x50
     [<c1117012>] reiserfs_lookup+0x62/0x140
     [<c105ccca>] ? debug_check_no_locks_freed+0x8a/0x140
     [<c105cbe4>] ? trace_hardirqs_on_caller+0x124/0x170
     [<c10bd85f>] __lookup_hash+0xef/0x110
     [<c10bf21d>] lookup_one_len+0x8d/0xc0
     [<c1141e2a>] open_xa_dir+0xea/0x1b0
     [<c1141fe5>] xattr_lookup+0x15/0x160
     [<c1142476>] reiserfs_xattr_get+0x56/0x2a0
     [<c1144042>] reiserfs_get_acl+0xa2/0x360
     [<c10ca2e7>] ? new_inode+0x27/0xa0
     [<c114461a>] reiserfs_cache_default_acl+0x3a/0x160
     [<c1402eb7>] ? _spin_unlock+0x27/0x40
     [<c111789c>] reiserfs_mkdir+0x6c/0x2c0
     [<c10c7cb8>] ? __d_lookup+0x108/0x190
     [<c105c932>] ? mark_held_locks+0x62/0x80
     [<c1401c8d>] ? mutex_lock_nested+0x2bd/0x340
     [<c10bd17a>] ? generic_permission+0x1a/0xa0
     [<c11788fe>] ? security_inode_permission+0x1e/0x20
     [<c10bea96>] vfs_mkdir+0xd6/0x180
     [<c10c0c10>] sys_mkdirat+0xc0/0xd0
     [<c10505c6>] ? up_read+0x16/0x30
     [<c1002fd8>] ? restore_all_notrace+0x0/0x18
     [<c10c0c40>] sys_mkdir+0x20/0x30
     [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Tested-by: Christian Kujau <lists@nerdbynature.de>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>

commit 0523676d3f3aa7edeea63cc3a1bc4dc612380a26
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 30 05:56:08 2009 +0100

    reiserfs: Relax reiserfs lock while freeing the journal
    
    Keeping the reiserfs lock while freeing the journal on
    umount path triggers a lock inversion between bdev->bd_mutex
    and the reiserfs lock.
    
    We don't need the reiserfs lock at this stage. The filesystem
    is not usable anymore, and there are no more pending commits,
    everything got flushed (even this operation was done in parallel
    and didn't required the reiserfs lock from the current process).
    
    This fixes the following lockdep report:
    
    =======================================================
    [ INFO: possible circular locking dependency detected ]
    2.6.32-atom #172
    -------------------------------------------------------
    umount/3904 is trying to acquire lock:
     (&bdev->bd_mutex){+.+.+.}, at: [<c10de2c2>] __blkdev_put+0x22/0x160
    
    but task is already holding lock:
     (&REISERFS_SB(s)->lock){+.+.+.}, at: [<c1143279>] reiserfs_write_lock+0x29/0x40
    
    which lock already depends on the new lock.
    
    the existing dependency chain (in reverse order) is:
    
    -> #3 (&REISERFS_SB(s)->lock){+.+.+.}:
           [<c105ea7f>] __lock_acquire+0x11ff/0x19e0
           [<c105f2c8>] lock_acquire+0x68/0x90
           [<c140199b>] mutex_lock_nested+0x5b/0x340
           [<c1143229>] reiserfs_write_lock_once+0x29/0x50
           [<c111c485>] reiserfs_get_block+0x85/0x1620
           [<c10e1040>] do_mpage_readpage+0x1f0/0x6d0
           [<c10e1640>] mpage_readpages+0xc0/0x100
           [<c1119b89>] reiserfs_readpages+0x19/0x20
           [<c108f1ec>] __do_page_cache_readahead+0x1bc/0x260
           [<c108f2b8>] ra_submit+0x28/0x40
           [<c1087e3e>] filemap_fault+0x40e/0x420
           [<c109b5fd>] __do_fault+0x3d/0x430
           [<c109d47e>] handle_mm_fault+0x12e/0x790
           [<c1022a65>] do_page_fault+0x135/0x330
           [<c1403663>] error_code+0x6b/0x70
           [<c10ef9ca>] load_elf_binary+0x82a/0x1a10
           [<c10ba130>] search_binary_handler+0x90/0x1d0
           [<c10bb70f>] do_execve+0x1df/0x250
           [<c1001746>] sys_execve+0x46/0x70
           [<c1002fa5>] syscall_call+0x7/0xb
    
    -> #2 (&mm->mmap_sem){++++++}:
           [<c105ea7f>] __lock_acquire+0x11ff/0x19e0
           [<c105f2c8>] lock_acquire+0x68/0x90
           [<c109b1ab>] might_fault+0x8b/0xb0
           [<c11b8f52>] copy_to_user+0x32/0x70
           [<c10c3b94>] filldir64+0xa4/0xf0
           [<c1109116>] sysfs_readdir+0x116/0x210
           [<c10c3e1d>] vfs_readdir+0x8d/0xb0
           [<c10c3ea9>] sys_getdents64+0x69/0xb0
           [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    -> #1 (sysfs_mutex){+.+.+.}:
           [<c105ea7f>] __lock_acquire+0x11ff/0x19e0
           [<c105f2c8>] lock_acquire+0x68/0x90
           [<c140199b>] mutex_lock_nested+0x5b/0x340
           [<c110951c>] sysfs_addrm_start+0x2c/0xb0
           [<c1109aa0>] create_dir+0x40/0x90
           [<c1109b1b>] sysfs_create_dir+0x2b/0x50
           [<c11b2352>] kobject_add_internal+0xc2/0x1b0
           [<c11b2531>] kobject_add_varg+0x31/0x50
           [<c11b25ac>] kobject_add+0x2c/0x60
           [<c1258294>] device_add+0x94/0x560
           [<c11036ea>] add_partition+0x18a/0x2a0
           [<c110418a>] rescan_partitions+0x33a/0x450
           [<c10de5bf>] __blkdev_get+0x12f/0x2d0
           [<c10de76a>] blkdev_get+0xa/0x10
           [<c11034b8>] register_disk+0x108/0x130
           [<c11a87a9>] add_disk+0xd9/0x130
           [<c12998e5>] sd_probe_async+0x105/0x1d0
           [<c10528af>] async_thread+0xcf/0x230
           [<c104bfd4>] kthread+0x74/0x80
           [<c1003aab>] kernel_thread_helper+0x7/0x3c
    
    -> #0 (&bdev->bd_mutex){+.+.+.}:
           [<c105f176>] __lock_acquire+0x18f6/0x19e0
           [<c105f2c8>] lock_acquire+0x68/0x90
           [<c140199b>] mutex_lock_nested+0x5b/0x340
           [<c10de2c2>] __blkdev_put+0x22/0x160
           [<c10de40a>] blkdev_put+0xa/0x10
           [<c113ce22>] free_journal_ram+0xd2/0x130
           [<c113ea18>] do_journal_release+0x98/0x190
           [<c113eb2a>] journal_release+0xa/0x10
           [<c1128eb6>] reiserfs_put_super+0x36/0x130
           [<c10b776f>] generic_shutdown_super+0x4f/0xe0
           [<c10b7825>] kill_block_super+0x25/0x40
           [<c11255df>] reiserfs_kill_sb+0x7f/0x90
           [<c10b7f4a>] deactivate_super+0x7a/0x90
           [<c10cccd8>] mntput_no_expire+0x98/0xd0
           [<c10ccfcc>] sys_umount+0x4c/0x310
           [<c10cd2a9>] sys_oldumount+0x19/0x20
           [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    other info that might help us debug this:
    
    2 locks held by umount/3904:
     #0:  (&type->s_umount_key#30){+++++.}, at: [<c10b7f45>] deactivate_super+0x75/0x90
     #1:  (&REISERFS_SB(s)->lock){+.+.+.}, at: [<c1143279>] reiserfs_write_lock+0x29/0x40
    
    stack backtrace:
    Pid: 3904, comm: umount Not tainted 2.6.32-atom #172
    Call Trace:
     [<c13ff903>] ? printk+0x18/0x1a
     [<c105d33a>] print_circular_bug+0xca/0xd0
     [<c105f176>] __lock_acquire+0x18f6/0x19e0
     [<c108b66f>] ? free_pcppages_bulk+0x1f/0x250
     [<c105f2c8>] lock_acquire+0x68/0x90
     [<c10de2c2>] ? __blkdev_put+0x22/0x160
     [<c10de2c2>] ? __blkdev_put+0x22/0x160
     [<c140199b>] mutex_lock_nested+0x5b/0x340
     [<c10de2c2>] ? __blkdev_put+0x22/0x160
     [<c105c932>] ? mark_held_locks+0x62/0x80
     [<c10afe12>] ? kfree+0x92/0xd0
     [<c10de2c2>] __blkdev_put+0x22/0x160
     [<c105cc3b>] ? trace_hardirqs_on+0xb/0x10
     [<c10de40a>] blkdev_put+0xa/0x10
     [<c113ce22>] free_journal_ram+0xd2/0x130
     [<c113ea18>] do_journal_release+0x98/0x190
     [<c113eb2a>] journal_release+0xa/0x10
     [<c1128eb6>] reiserfs_put_super+0x36/0x130
     [<c1050596>] ? up_write+0x16/0x30
     [<c10b776f>] generic_shutdown_super+0x4f/0xe0
     [<c10b7825>] kill_block_super+0x25/0x40
     [<c10f41e0>] ? vfs_quota_off+0x0/0x20
     [<c11255df>] reiserfs_kill_sb+0x7f/0x90
     [<c10b7f4a>] deactivate_super+0x7a/0x90
     [<c10cccd8>] mntput_no_expire+0x98/0xd0
     [<c10ccfcc>] sys_umount+0x4c/0x310
     [<c10cd2a9>] sys_oldumount+0x19/0x20
     [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>

commit 27026a05bb805866a3b9068dda8153b72cb942f4
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 30 05:06:21 2009 +0100

    reiserfs: Fix reiserfs lock <-> i_mutex dependency inversion on xattr
    
    While deleting the xattrs of an inode, we hold the reiserfs lock
    and grab the inode->i_mutex of the targeted inode and the root
    private xattr directory.
    
    Later on, we may relax the reiserfs lock for various reasons, this
    creates inverted dependencies.
    
    We can remove the reiserfs lock -> i_mutex dependency by relaxing
    the former before calling open_xa_dir(). This is fine because the
    lookup and creation of xattr private directories done in
    open_xa_dir() are covered by the targeted inode mutexes. And deeper
    operations in the tree are still done under the write lock.
    
    This fixes the following lockdep report:
    
    =======================================================
    [ INFO: possible circular locking dependency detected ]
    2.6.32-atom #173
    -------------------------------------------------------
    cp/3204 is trying to acquire lock:
     (&REISERFS_SB(s)->lock){+.+.+.}, at: [<c11432b9>] reiserfs_write_lock_once+0x29/0x50
    
    but task is already holding lock:
     (&sb->s_type->i_mutex_key#4/3){+.+.+.}, at: [<c1141e18>] open_xa_dir+0xd8/0x1b0
    
    which lock already depends on the new lock.
    
    the existing dependency chain (in reverse order) is:
    
    -> #1 (&sb->s_type->i_mutex_key#4/3){+.+.+.}:
           [<c105ea7f>] __lock_acquire+0x11ff/0x19e0
           [<c105f2c8>] lock_acquire+0x68/0x90
           [<c1401a2b>] mutex_lock_nested+0x5b/0x340
           [<c1141d83>] open_xa_dir+0x43/0x1b0
           [<c1142722>] reiserfs_for_each_xattr+0x62/0x260
           [<c114299a>] reiserfs_delete_xattrs+0x1a/0x60
           [<c111ea1f>] reiserfs_delete_inode+0x9f/0x150
           [<c10c9c32>] generic_delete_inode+0xa2/0x170
           [<c10c9d4f>] generic_drop_inode+0x4f/0x70
           [<c10c8b07>] iput+0x47/0x50
           [<c10c0965>] do_unlinkat+0xd5/0x160
           [<c10c0a00>] sys_unlink+0x10/0x20
           [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    -> #0 (&REISERFS_SB(s)->lock){+.+.+.}:
           [<c105f176>] __lock_acquire+0x18f6/0x19e0
           [<c105f2c8>] lock_acquire+0x68/0x90
           [<c1401a2b>] mutex_lock_nested+0x5b/0x340
           [<c11432b9>] reiserfs_write_lock_once+0x29/0x50
           [<c1117012>] reiserfs_lookup+0x62/0x140
           [<c10bd85f>] __lookup_hash+0xef/0x110
           [<c10bf21d>] lookup_one_len+0x8d/0xc0
           [<c1141e2a>] open_xa_dir+0xea/0x1b0
           [<c1141fe5>] xattr_lookup+0x15/0x160
           [<c1142476>] reiserfs_xattr_get+0x56/0x2a0
           [<c1144042>] reiserfs_get_acl+0xa2/0x360
           [<c114461a>] reiserfs_cache_default_acl+0x3a/0x160
           [<c111789c>] reiserfs_mkdir+0x6c/0x2c0
           [<c10bea96>] vfs_mkdir+0xd6/0x180
           [<c10c0c10>] sys_mkdirat+0xc0/0xd0
           [<c10c0c40>] sys_mkdir+0x20/0x30
           [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    other info that might help us debug this:
    
    2 locks held by cp/3204:
     #0:  (&sb->s_type->i_mutex_key#4/1){+.+.+.}, at: [<c10bd8d6>] lookup_create+0x26/0xa0
     #1:  (&sb->s_type->i_mutex_key#4/3){+.+.+.}, at: [<c1141e18>] open_xa_dir+0xd8/0x1b0
    
    stack backtrace:
    Pid: 3204, comm: cp Not tainted 2.6.32-atom #173
    Call Trace:
     [<c13ff993>] ? printk+0x18/0x1a
     [<c105d33a>] print_circular_bug+0xca/0xd0
     [<c105f176>] __lock_acquire+0x18f6/0x19e0
     [<c105d3aa>] ? check_usage+0x6a/0x460
     [<c105f2c8>] lock_acquire+0x68/0x90
     [<c11432b9>] ? reiserfs_write_lock_once+0x29/0x50
     [<c11432b9>] ? reiserfs_write_lock_once+0x29/0x50
     [<c1401a2b>] mutex_lock_nested+0x5b/0x340
     [<c11432b9>] ? reiserfs_write_lock_once+0x29/0x50
     [<c11432b9>] reiserfs_write_lock_once+0x29/0x50
     [<c1117012>] reiserfs_lookup+0x62/0x140
     [<c105ccca>] ? debug_check_no_locks_freed+0x8a/0x140
     [<c105cbe4>] ? trace_hardirqs_on_caller+0x124/0x170
     [<c10bd85f>] __lookup_hash+0xef/0x110
     [<c10bf21d>] lookup_one_len+0x8d/0xc0
     [<c1141e2a>] open_xa_dir+0xea/0x1b0
     [<c1141fe5>] xattr_lookup+0x15/0x160
     [<c1142476>] reiserfs_xattr_get+0x56/0x2a0
     [<c1144042>] reiserfs_get_acl+0xa2/0x360
     [<c10ca2e7>] ? new_inode+0x27/0xa0
     [<c114461a>] reiserfs_cache_default_acl+0x3a/0x160
     [<c1402eb7>] ? _spin_unlock+0x27/0x40
     [<c111789c>] reiserfs_mkdir+0x6c/0x2c0
     [<c10c7cb8>] ? __d_lookup+0x108/0x190
     [<c105c932>] ? mark_held_locks+0x62/0x80
     [<c1401c8d>] ? mutex_lock_nested+0x2bd/0x340
     [<c10bd17a>] ? generic_permission+0x1a/0xa0
     [<c11788fe>] ? security_inode_permission+0x1e/0x20
     [<c10bea96>] vfs_mkdir+0xd6/0x180
     [<c10c0c10>] sys_mkdirat+0xc0/0xd0
     [<c10505c6>] ? up_read+0x16/0x30
     [<c1002fd8>] ? restore_all_notrace+0x0/0x18
     [<c10c0c40>] sys_mkdir+0x20/0x30
     [<c1002ec4>] sysenter_do_call+0x12/0x32
    
    v2: Don't drop reiserfs_mutex_lock_nested_safe() as we'll still
        need it later
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Tested-by: Christian Kujau <lists@nerdbynature.de>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>

commit 0719d3434747889b314a1e8add776418c4148bcf
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 30 00:39:22 2009 +0100

    reiserfs: Fix reiserfs lock <-> i_xattr_sem dependency inversion
    
    i_xattr_sem depends on the reiserfs lock. But after we grab
    i_xattr_sem, we may relax/relock the reiserfs lock while waiting
    on a freezed filesystem, creating a dependency inversion between
    the two locks.
    
    In order to avoid the i_xattr_sem -> reiserfs lock dependency, let's
    create a reiserfs_down_read_safe() that acts like
    reiserfs_mutex_lock_safe(): relax the reiserfs lock while grabbing
    another lock to avoid undesired dependencies induced by the
    heivyweight reiserfs lock.
    
    This fixes the following warning:
    
    [  990.005931] =======================================================
    [  990.012373] [ INFO: possible circular locking dependency detected ]
    [  990.013233] 2.6.33-rc1 #1
    [  990.013233] -------------------------------------------------------
    [  990.013233] dbench/1891 is trying to acquire lock:
    [  990.013233]  (&REISERFS_SB(s)->lock){+.+.+.}, at: [<ffffffff81159505>] reiserfs_write_lock+0x35/0x50
    [  990.013233]
    [  990.013233] but task is already holding lock:
    [  990.013233]  (&REISERFS_I(inode)->i_xattr_sem){+.+.+.}, at: [<ffffffff8115899a>] reiserfs_xattr_set_handle+0x8a/0x470
    [  990.013233]
    [  990.013233] which lock already depends on the new lock.
    [  990.013233]
    [  990.013233]
    [  990.013233] the existing dependency chain (in reverse order) is:
    [  990.013233]
    [  990.013233] -> #1 (&REISERFS_I(inode)->i_xattr_sem){+.+.+.}:
    [  990.013233]        [<ffffffff81063afc>] __lock_acquire+0xf9c/0x1560
    [  990.013233]        [<ffffffff8106414f>] lock_acquire+0x8f/0xb0
    [  990.013233]        [<ffffffff814ac194>] down_write+0x44/0x80
    [  990.013233]        [<ffffffff8115899a>] reiserfs_xattr_set_handle+0x8a/0x470
    [  990.013233]        [<ffffffff81158e30>] reiserfs_xattr_set+0xb0/0x150
    [  990.013233]        [<ffffffff8115a6aa>] user_set+0x8a/0x90
    [  990.013233]        [<ffffffff8115901a>] reiserfs_setxattr+0xaa/0xb0
    [  990.013233]        [<ffffffff810e2596>] __vfs_setxattr_noperm+0x36/0xa0
    [  990.013233]        [<ffffffff810e26bc>] vfs_setxattr+0xbc/0xc0
    [  990.013233]        [<ffffffff810e2780>] setxattr+0xc0/0x150
    [  990.013233]        [<ffffffff810e289d>] sys_fsetxattr+0x8d/0xa0
    [  990.013233]        [<ffffffff81002dab>] system_call_fastpath+0x16/0x1b
    [  990.013233]
    [  990.013233] -> #0 (&REISERFS_SB(s)->lock){+.+.+.}:
    [  990.013233]        [<ffffffff81063e30>] __lock_acquire+0x12d0/0x1560
    [  990.013233]        [<ffffffff8106414f>] lock_acquire+0x8f/0xb0
    [  990.013233]        [<ffffffff814aba77>] __mutex_lock_common+0x47/0x3b0
    [  990.013233]        [<ffffffff814abebe>] mutex_lock_nested+0x3e/0x50
    [  990.013233]        [<ffffffff81159505>] reiserfs_write_lock+0x35/0x50
    [  990.013233]        [<ffffffff811340e5>] reiserfs_prepare_write+0x45/0x180
    [  990.013233]        [<ffffffff81158bb6>] reiserfs_xattr_set_handle+0x2a6/0x470
    [  990.013233]        [<ffffffff81158e30>] reiserfs_xattr_set+0xb0/0x150
    [  990.013233]        [<ffffffff8115a6aa>] user_set+0x8a/0x90
    [  990.013233]        [<ffffffff8115901a>] reiserfs_setxattr+0xaa/0xb0
    [  990.013233]        [<ffffffff810e2596>] __vfs_setxattr_noperm+0x36/0xa0
    [  990.013233]        [<ffffffff810e26bc>] vfs_setxattr+0xbc/0xc0
    [  990.013233]        [<ffffffff810e2780>] setxattr+0xc0/0x150
    [  990.013233]        [<ffffffff810e289d>] sys_fsetxattr+0x8d/0xa0
    [  990.013233]        [<ffffffff81002dab>] system_call_fastpath+0x16/0x1b
    [  990.013233]
    [  990.013233] other info that might help us debug this:
    [  990.013233]
    [  990.013233] 2 locks held by dbench/1891:
    [  990.013233]  #0:  (&sb->s_type->i_mutex_key#12){+.+.+.}, at: [<ffffffff810e2678>] vfs_setxattr+0x78/0xc0
    [  990.013233]  #1:  (&REISERFS_I(inode)->i_xattr_sem){+.+.+.}, at: [<ffffffff8115899a>] reiserfs_xattr_set_handle+0x8a/0x470
    [  990.013233]
    [  990.013233] stack backtrace:
    [  990.013233] Pid: 1891, comm: dbench Not tainted 2.6.33-rc1 #1
    [  990.013233] Call Trace:
    [  990.013233]  [<ffffffff81061639>] print_circular_bug+0xe9/0xf0
    [  990.013233]  [<ffffffff81063e30>] __lock_acquire+0x12d0/0x1560
    [  990.013233]  [<ffffffff8115899a>] ? reiserfs_xattr_set_handle+0x8a/0x470
    [  990.013233]  [<ffffffff8106414f>] lock_acquire+0x8f/0xb0
    [  990.013233]  [<ffffffff81159505>] ? reiserfs_write_lock+0x35/0x50
    [  990.013233]  [<ffffffff8115899a>] ? reiserfs_xattr_set_handle+0x8a/0x470
    [  990.013233]  [<ffffffff814aba77>] __mutex_lock_common+0x47/0x3b0
    [  990.013233]  [<ffffffff81159505>] ? reiserfs_write_lock+0x35/0x50
    [  990.013233]  [<ffffffff81159505>] ? reiserfs_write_lock+0x35/0x50
    [  990.013233]  [<ffffffff81062592>] ? mark_held_locks+0x72/0xa0
    [  990.013233]  [<ffffffff814ab81d>] ? __mutex_unlock_slowpath+0xbd/0x140
    [  990.013233]  [<ffffffff810628ad>] ? trace_hardirqs_on_caller+0x14d/0x1a0
    [  990.013233]  [<ffffffff814abebe>] mutex_lock_nested+0x3e/0x50
    [  990.013233]  [<ffffffff81159505>] reiserfs_write_lock+0x35/0x50
    [  990.013233]  [<ffffffff811340e5>] reiserfs_prepare_write+0x45/0x180
    [  990.013233]  [<ffffffff81158bb6>] reiserfs_xattr_set_handle+0x2a6/0x470
    [  990.013233]  [<ffffffff81158e30>] reiserfs_xattr_set+0xb0/0x150
    [  990.013233]  [<ffffffff814abcb4>] ? __mutex_lock_common+0x284/0x3b0
    [  990.013233]  [<ffffffff8115a6aa>] user_set+0x8a/0x90
    [  990.013233]  [<ffffffff8115901a>] reiserfs_setxattr+0xaa/0xb0
    [  990.013233]  [<ffffffff810e2596>] __vfs_setxattr_noperm+0x36/0xa0
    [  990.013233]  [<ffffffff810e26bc>] vfs_setxattr+0xbc/0xc0
    [  990.013233]  [<ffffffff810e2780>] setxattr+0xc0/0x150
    [  990.013233]  [<ffffffff81056018>] ? sched_clock_cpu+0xb8/0x100
    [  990.013233]  [<ffffffff8105eded>] ? trace_hardirqs_off+0xd/0x10
    [  990.013233]  [<ffffffff810560a3>] ? cpu_clock+0x43/0x50
    [  990.013233]  [<ffffffff810c6820>] ? fget+0xb0/0x110
    [  990.013233]  [<ffffffff810c6770>] ? fget+0x0/0x110
    [  990.013233]  [<ffffffff81002ddc>] ? sysret_check+0x27/0x62
    [  990.013233]  [<ffffffff810e289d>] sys_fsetxattr+0x8d/0xa0
    [  990.013233]  [<ffffffff81002dab>] system_call_fastpath+0x16/0x1b
    
    Reported-and-tested-by: Christian Kujau <lists@nerdbynature.de>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>

commit 47376ceba54600cec4dd9e7c4fe8b98e4269633a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 16 23:25:50 2009 +0100

    reiserfs: Fix reiserfs lock <-> inode mutex dependency inversion
    
    The reiserfs lock -> inode mutex dependency gets inverted when we
    relax the lock while walking to the tree.
    
    To fix this, use a specialized version of reiserfs_mutex_lock_safe
    that takes care of mutex subclasses. Then we can grab the inode
    mutex with I_MUTEX_XATTR subclass without any reiserfs lock
    dependency.
    
    This fixes the following report:
    
    [ INFO: possible circular locking dependency detected ]
    2.6.32-06793-gf405425-dirty #2
    -------------------------------------------------------
    mv/18566 is trying to acquire lock:
     (&REISERFS_SB(s)->lock){+.+.+.}, at: [<c1110708>] reiserfs_write_lock+0x28=
    /0x40
    
    but task is already holding lock:
     (&sb->s_type->i_mutex_key#5/3){+.+.+.}, at: [<c111033c>]
    reiserfs_for_each_xattr+0x10c/0x380
    
    which lock already depends on the new lock.
    
    the existing dependency chain (in reverse order) is:
    
    -> #1 (&sb->s_type->i_mutex_key#5/3){+.+.+.}:
           [<c104f723>] validate_chain+0xa23/0xf70
           [<c1050155>] __lock_acquire+0x4e5/0xa70
           [<c105075a>] lock_acquire+0x7a/0xa0
           [<c134c76f>] mutex_lock_nested+0x5f/0x2b0
           [<c11102b4>] reiserfs_for_each_xattr+0x84/0x380
           [<c1110615>] reiserfs_delete_xattrs+0x15/0x50
           [<c10ef57f>] reiserfs_delete_inode+0x8f/0x140
           [<c10a565c>] generic_delete_inode+0x9c/0x150
           [<c10a574d>] generic_drop_inode+0x3d/0x60
           [<c10a4667>] iput+0x47/0x50
           [<c109cc0b>] do_unlinkat+0xdb/0x160
           [<c109cca0>] sys_unlink+0x10/0x20
           [<c1002c50>] sysenter_do_call+0x12/0x36
    
    -> #0 (&REISERFS_SB(s)->lock){+.+.+.}:
           [<c104fc68>] validate_chain+0xf68/0xf70
           [<c1050155>] __lock_acquire+0x4e5/0xa70
           [<c105075a>] lock_acquire+0x7a/0xa0
           [<c134c76f>] mutex_lock_nested+0x5f/0x2b0
           [<c1110708>] reiserfs_write_lock+0x28/0x40
           [<c1103d6b>] search_by_key+0x1f7b/0x21b0
           [<c10e73ef>] search_by_entry_key+0x1f/0x3b0
           [<c10e77f7>] reiserfs_find_entry+0x77/0x400
           [<c10e81e5>] reiserfs_lookup+0x85/0x130
           [<c109a144>] __lookup_hash+0xb4/0x110
           [<c109b763>] lookup_one_len+0xb3/0x100
           [<c1110350>] reiserfs_for_each_xattr+0x120/0x380
           [<c1110615>] reiserfs_delete_xattrs+0x15/0x50
           [<c10ef57f>] reiserfs_delete_inode+0x8f/0x140
           [<c10a565c>] generic_delete_inode+0x9c/0x150
           [<c10a574d>] generic_drop_inode+0x3d/0x60
           [<c10a4667>] iput+0x47/0x50
           [<c10a1c4f>] dentry_iput+0x6f/0xf0
           [<c10a1d74>] d_kill+0x24/0x50
           [<c10a396b>] dput+0x5b/0x120
           [<c109ca89>] sys_renameat+0x1b9/0x230
           [<c109cb28>] sys_rename+0x28/0x30
           [<c1002c50>] sysenter_do_call+0x12/0x36
    
    other info that might help us debug this:
    
    2 locks held by mv/18566:
     #0:  (&sb->s_type->i_mutex_key#5/1){+.+.+.}, at: [<c109b6ac>]
    lock_rename+0xcc/0xd0
     #1:  (&sb->s_type->i_mutex_key#5/3){+.+.+.}, at: [<c111033c>]
    reiserfs_for_each_xattr+0x10c/0x380
    
    stack backtrace:
    Pid: 18566, comm: mv Tainted: G         C 2.6.32-06793-gf405425-dirty #2
    Call Trace:
     [<c134b252>] ? printk+0x18/0x1e
     [<c104e790>] print_circular_bug+0xc0/0xd0
     [<c104fc68>] validate_chain+0xf68/0xf70
     [<c104c8cb>] ? trace_hardirqs_off+0xb/0x10
     [<c1050155>] __lock_acquire+0x4e5/0xa70
     [<c105075a>] lock_acquire+0x7a/0xa0
     [<c1110708>] ? reiserfs_write_lock+0x28/0x40
     [<c134c76f>] mutex_lock_nested+0x5f/0x2b0
     [<c1110708>] ? reiserfs_write_lock+0x28/0x40
     [<c1110708>] ? reiserfs_write_lock+0x28/0x40
     [<c134b60a>] ? schedule+0x27a/0x440
     [<c1110708>] reiserfs_write_lock+0x28/0x40
     [<c1103d6b>] search_by_key+0x1f7b/0x21b0
     [<c1050176>] ? __lock_acquire+0x506/0xa70
     [<c1051267>] ? lock_release_non_nested+0x1e7/0x340
     [<c1110708>] ? reiserfs_write_lock+0x28/0x40
     [<c104e354>] ? trace_hardirqs_on_caller+0x124/0x170
     [<c104e3ab>] ? trace_hardirqs_on+0xb/0x10
     [<c1042a55>] ? T.316+0x15/0x1a0
     [<c1042d2d>] ? sched_clock_cpu+0x9d/0x100
     [<c10e73ef>] search_by_entry_key+0x1f/0x3b0
     [<c134bf2a>] ? __mutex_unlock_slowpath+0x9a/0x120
     [<c104e354>] ? trace_hardirqs_on_caller+0x124/0x170
     [<c10e77f7>] reiserfs_find_entry+0x77/0x400
     [<c10e81e5>] reiserfs_lookup+0x85/0x130
     [<c1042d2d>] ? sched_clock_cpu+0x9d/0x100
     [<c109a144>] __lookup_hash+0xb4/0x110
     [<c109b763>] lookup_one_len+0xb3/0x100
     [<c1110350>] reiserfs_for_each_xattr+0x120/0x380
     [<c110ffe0>] ? delete_one_xattr+0x0/0x1c0
     [<c1003342>] ? math_error+0x22/0x150
     [<c1110708>] ? reiserfs_write_lock+0x28/0x40
     [<c1110615>] reiserfs_delete_xattrs+0x15/0x50
     [<c1110708>] ? reiserfs_write_lock+0x28/0x40
     [<c10ef57f>] reiserfs_delete_inode+0x8f/0x140
     [<c10a561f>] ? generic_delete_inode+0x5f/0x150
     [<c10ef4f0>] ? reiserfs_delete_inode+0x0/0x140
     [<c10a565c>] generic_delete_inode+0x9c/0x150
     [<c10a574d>] generic_drop_inode+0x3d/0x60
     [<c10a4667>] iput+0x47/0x50
     [<c10a1c4f>] dentry_iput+0x6f/0xf0
     [<c10a1d74>] d_kill+0x24/0x50
     [<c10a396b>] dput+0x5b/0x120
     [<c109ca89>] sys_renameat+0x1b9/0x230
     [<c1042d2d>] ? sched_clock_cpu+0x9d/0x100
     [<c104c8cb>] ? trace_hardirqs_off+0xb/0x10
     [<c1042dde>] ? cpu_clock+0x4e/0x60
     [<c1350825>] ? do_page_fault+0x155/0x370
     [<c1041816>] ? up_read+0x16/0x30
     [<c1350825>] ? do_page_fault+0x155/0x370
     [<c109cb28>] sys_rename+0x28/0x30
     [<c1002c50>] sysenter_do_call+0x12/0x36
    
    Reported-by: Alexander Beregalov <a.beregalov@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>

commit cb1c2e51c5a72f093b5af384b11d2f1c2abd6c13
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Dec 13 23:32:06 2009 +0100

    reiserfs: Fix reiserfs lock and journal lock inversion dependency
    
    When we were using the bkl, we didn't care about dependencies against
    other locks, but the mutex conversion created new ones, which is why
    we have reiserfs_mutex_lock_safe(), which unlocks the reiserfs lock
    before acquiring another mutex.
    
    But this trick actually fails if we have acquired the reiserfs lock
    recursively, as we try to unlock it to acquire the new mutex without
    inverted dependency, but we eventually only decrease its depth.
    
    This happens in the case of a nested inode creation/deletion.
    Say we have no space left on the device, we create an inode
    and tak the lock but fail to create its entry, then we release the
    inode using iput(), which calls reiserfs_delete_inode() that takes
    the reiserfs lock recursively. The path eventually ends up in
    journal_begin() where we try to take the journal safely but we
    fail because of the reiserfs lock recursion:
    
    [ INFO: possible circular locking dependency detected ]
    2.6.32-06486-g053fe57 #2
    -------------------------------------------------------
    vi/23454 is trying to acquire lock:
     (&journal->j_mutex){+.+...}, at: [<c110dac4>] do_journal_begin_r+0x64/0x2f0
    
    but task is already holding lock:
     (&REISERFS_SB(s)->lock){+.+.+.}, at: [<c11106a8>] reiserfs_write_lock+0x28/0x40
    
    which lock already depends on the new lock.
    
    the existing dependency chain (in reverse order) is:
    
    -> #1 (&REISERFS_SB(s)->lock){+.+.+.}:
           [<c104f8f3>] validate_chain+0xa23/0xf70
           [<c1050325>] __lock_acquire+0x4e5/0xa70
           [<c105092a>] lock_acquire+0x7a/0xa0
           [<c134c78f>] mutex_lock_nested+0x5f/0x2b0
           [<c11106a8>] reiserfs_write_lock+0x28/0x40
           [<c110dacb>] do_journal_begin_r+0x6b/0x2f0
           [<c110ddcf>] journal_begin+0x7f/0x120
           [<c10f76c2>] reiserfs_remount+0x212/0x4d0
           [<c1093997>] do_remount_sb+0x67/0x140
           [<c10a9ca6>] do_mount+0x436/0x6b0
           [<c10a9f86>] sys_mount+0x66/0xa0
           [<c1002c50>] sysenter_do_call+0x12/0x36
    
    -> #0 (&journal->j_mutex){+.+...}:
           [<c104fe38>] validate_chain+0xf68/0xf70
           [<c1050325>] __lock_acquire+0x4e5/0xa70
           [<c105092a>] lock_acquire+0x7a/0xa0
           [<c134c78f>] mutex_lock_nested+0x5f/0x2b0
           [<c110dac4>] do_journal_begin_r+0x64/0x2f0
           [<c110ddcf>] journal_begin+0x7f/0x120
           [<c10ef52f>] reiserfs_delete_inode+0x9f/0x140
           [<c10a55fc>] generic_delete_inode+0x9c/0x150
           [<c10a56ed>] generic_drop_inode+0x3d/0x60
           [<c10a4607>] iput+0x47/0x50
           [<c10e915c>] reiserfs_create+0x16c/0x1c0
           [<c109a9c1>] vfs_create+0xc1/0x130
           [<c109dbec>] do_filp_open+0x81c/0x920
           [<c109004f>] do_sys_open+0x4f/0x110
           [<c1090179>] sys_open+0x29/0x40
           [<c1002c50>] sysenter_do_call+0x12/0x36
    
    other info that might help us debug this:
    
    2 locks held by vi/23454:
     #0:  (&sb->s_type->i_mutex_key#5){+.+.+.}, at: [<c109d64e>]
    do_filp_open+0x27e/0x920
     #1:  (&REISERFS_SB(s)->lock){+.+.+.}, at: [<c11106a8>]
    reiserfs_write_lock+0x28/0x40
    
    stack backtrace:
    Pid: 23454, comm: vi Not tainted 2.6.32-06486-g053fe57 #2
    Call Trace:
     [<c134b202>] ? printk+0x18/0x1e
     [<c104e960>] print_circular_bug+0xc0/0xd0
     [<c104fe38>] validate_chain+0xf68/0xf70
     [<c104ca9b>] ? trace_hardirqs_off+0xb/0x10
     [<c1050325>] __lock_acquire+0x4e5/0xa70
     [<c105092a>] lock_acquire+0x7a/0xa0
     [<c110dac4>] ? do_journal_begin_r+0x64/0x2f0
     [<c134c78f>] mutex_lock_nested+0x5f/0x2b0
     [<c110dac4>] ? do_journal_begin_r+0x64/0x2f0
     [<c110dac4>] ? do_journal_begin_r+0x64/0x2f0
     [<c110ff80>] ? delete_one_xattr+0x0/0x1c0
     [<c110dac4>] do_journal_begin_r+0x64/0x2f0
     [<c110ddcf>] journal_begin+0x7f/0x120
     [<c11105b5>] ? reiserfs_delete_xattrs+0x15/0x50
     [<c10ef52f>] reiserfs_delete_inode+0x9f/0x140
     [<c10a55bf>] ? generic_delete_inode+0x5f/0x150
     [<c10ef490>] ? reiserfs_delete_inode+0x0/0x140
     [<c10a55fc>] generic_delete_inode+0x9c/0x150
     [<c10a56ed>] generic_drop_inode+0x3d/0x60
     [<c10a4607>] iput+0x47/0x50
     [<c10e915c>] reiserfs_create+0x16c/0x1c0
     [<c1099a5d>] ? inode_permission+0x7d/0xa0
     [<c109a9c1>] vfs_create+0xc1/0x130
     [<c10e8ff0>] ? reiserfs_create+0x0/0x1c0
     [<c109dbec>] do_filp_open+0x81c/0x920
     [<c104ca9b>] ? trace_hardirqs_off+0xb/0x10
     [<c134dc0d>] ? _spin_unlock+0x1d/0x20
     [<c10a6eea>] ? alloc_fd+0xba/0xf0
     [<c109004f>] do_sys_open+0x4f/0x110
     [<c1090179>] sys_open+0x29/0x40
     [<c1002c50>] sysenter_do_call+0x12/0x36
    
    To fix this, use reiserfs_lock_once() from reiserfs_delete_inode()
    which prevents from adding reiserfs lock recursion.
    
    Reported-by: Alexander Beregalov <a.beregalov@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>

commit 500f5a0bf5f0624dae34307010e240ec090e4cde
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Dec 13 22:48:54 2009 +0100

    reiserfs: Fix possible recursive lock
    
    While allocating the bitmap using vmalloc, we hold the reiserfs lock,
    which makes lockdep later reporting a possible deadlock as we may
    swap out pages to allocate memory and then take the reiserfs lock
    recursively:
    
    inconsistent {RECLAIM_FS-ON-W} -> {IN-RECLAIM_FS-W} usage.
    kswapd0/312 [HC0[0]:SC0[0]:HE1:SE1] takes:
     (&REISERFS_SB(s)->lock){+.+.?.}, at: [<c11108a8>] reiserfs_write_lock+0x28/0x40
    {RECLAIM_FS-ON-W} state was registered at:
      [<c104e1c2>] mark_held_locks+0x62/0x90
      [<c104e28a>] lockdep_trace_alloc+0x9a/0xc0
      [<c108e396>] kmem_cache_alloc+0x26/0xf0
      [<c10850ec>] __get_vm_area_node+0x6c/0xf0
      [<c10857de>] __vmalloc_node+0x7e/0xa0
      [<c108597b>] vmalloc+0x2b/0x30
      [<c10e00b9>] reiserfs_init_bitmap_cache+0x39/0x70
      [<c10f8178>] reiserfs_fill_super+0x2e8/0xb90
      [<c1094345>] get_sb_bdev+0x145/0x180
      [<c10f5a11>] get_super_block+0x21/0x30
      [<c10931f0>] vfs_kern_mount+0x40/0xd0
      [<c10932d9>] do_kern_mount+0x39/0xd0
      [<c10a9857>] do_mount+0x2c7/0x6b0
      [<c10a9ca6>] sys_mount+0x66/0xa0
      [<c161589b>] mount_block_root+0xc4/0x245
      [<c1615a75>] mount_root+0x59/0x5f
      [<c1615b8c>] prepare_namespace+0x111/0x14b
      [<c1615269>] kernel_init+0xcf/0xdb
      [<c10031fb>] kernel_thread_helper+0x7/0x1c
    
    This is actually fine for two reasons: we call vmalloc at mount time
    then it's not in the swapping out path. Also the reiserfs lock can be
    acquired recursively, but since its implementation depends on a mutex,
    it's hard and not necessary worth it to teach that to lockdep.
    
    The lock is useless at mount time anyway, at least until we replay the
    journal. But let's remove it from this path later as this needs
    more thinking and is a sensible change.
    
    For now we can just relax the lock around vmalloc,
    
    Reported-by: Alexander Beregalov <a.beregalov@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>

commit 3d76c082907e8f83c5d5c4572f38d53ad8f00c4b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Sep 28 07:46:32 2009 -0700

    rcu: Clean up code based on review feedback from Josh Triplett, part 3
    
    Whitespace fixes, updated comments, and trivial code movement.
    
    o       Fix whitespace error in RCU_HEAD_INIT()
    
    o       Move "So where is rcu_write_lock()" comment so that it does
            not come between the rcu_read_unlock() header comment and
            the rcu_read_unlock() definition.
    
    o       Move the module_param statements for blimit, qhimark, and
            qlowmark to immediately follow the corresponding
            definitions.
    
    o       In __rcu_offline_cpu(), move the assignment to rdp_me
            inside the "if" statement, given that rdp_me is not used
            outside of that "if" statement.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <12541491931164-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 7e94277050e31aa4204060f03953bba72598cf7d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Aug 25 03:38:12 2009 +0200

    kill-the-bkl/reiserfs: fix recursive reiserfs write lock in reiserfs_commit_write()
    
    reiserfs_commit_write() is always called with the write lock held.
    Thus the current calls to reiserfs_write_lock() in this function are
    acquiring the lock recursively.
    We can safely drop them.
    
    This also solves further assumptions for this lock to be really
    released while calling reiserfs_write_unlock().
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Cc: Laurent Riffard <laurent.riffard@free.fr>

commit c72e05756b900b3be24cd73a16de52bab80984c0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat May 16 18:12:08 2009 +0200

    kill-the-bkl/reiserfs: acquire the inode mutex safely
    
    While searching a pathname, an inode mutex can be acquired
    in do_lookup() which calls reiserfs_lookup() which in turn
    acquires the write lock.
    
    On the other side reiserfs_fill_super() can acquire the write_lock
    and then call reiserfs_lookup_privroot() which can acquire an
    inode mutex (the root of the mount point).
    
    So we theoretically risk an AB - BA lock inversion that could lead
    to a deadlock.
    
    As for other lock dependencies found since the bkl to mutex
    conversion, the fix is to use reiserfs_mutex_lock_safe() which
    drops the lock dependency to the write lock.
    
    [ Impact: fix a possible deadlock with reiserfs ]
    
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

commit c63e3c0b2498adec921b06c670d12c8c74b85538
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri May 8 20:01:09 2009 +0200

    kill-the-bkl/reiserfs: use mutex_lock in reiserfs_mutex_lock_safe
    
    reiserfs_mutex_lock_safe() is a hack to avoid any dependency between
    an internal reiserfs mutex and the write lock, it has been proposed
    to follow the old bkl logic.
    
    The code does the following:
    
    while (!mutex_trylock(m)) {
            reiserfs_write_unlock(s);
            schedule();
            reiserfs_write_lock(s);
    }
    
    It then imitate the implicit behaviour of the lock when it was
    a Bkl and hadn't such dependency:
    
    mutex_lock(m) {
            if (fastpath)
                    let's go
            else {
                    wait_for_mutex() {
                            schedule() {
                                    unlock_kernel()
                                    reacquire_lock_kernel()
                            }
                    }
            }
    }
    
    The problem is that by using such explicit schedule(), we don't
    benefit of the adaptive mutex spinning on owner.
    
    The logic in use now is:
    
    reiserfs_write_unlock(s);
    mutex_lock(m); // -> possible adaptive spinning
    reiserfs_write_lock(s);
    
    [ Impact: restore the use of adaptive spinning mutexes in reiserfs ]
    
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

commit 26931309a47747fd31b2ef029c29d47794c2d93d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 7 23:48:44 2009 +0200

    kill-the-bkl/reiserfs: lock only once on reiserfs_get_block()
    
    reiserfs_get_block() is one of these sites where the write lock might
    be acquired recursively.
    
    It's a particular problem because this function is called very often.
    It's a hot spot which needs to reschedule() periodically while converting
    direct items to indirect ones because it can take some time.
    
    Then if we are applying the write lock release/reacquire pattern on
    schedule() here, it may not produce the desired effect since we may have
    locked in more than one depth.
    
    The solution is to use reiserfs_write_lock_once() which won't try
    to reacquire the lock recursively. Then the lock will be *really*
    released before schedule().
    
    Also, we only release the lock if TIF_NEED_RESCHED is set to not
    create wasteful numerous contentions.
    
    [ Impact: fix a too long holded lock case in reiserfs_get_block() ]
    
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

commit dc8f6d8936eb244eea452af689df5ee19e635206
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Apr 14 05:34:25 2009 +0200

    kill-the-BKL/reiserfs: only acquire the write lock once in reiserfs_dirty_inode
    
    Impact: fix a deadlock
    
    reiserfs_dirty_inode() is the super_operations::dirty_inode() callback
    of reiserfs. It can be called from different contexts where the write
    lock can be already held.
    
    But this function also grab the write lock (possibly recursively).
    Subsequent release of the lock before sleep will actually not release
    the lock if the caller of mark_inode_dirty() (which in turn calls
    reiserfs_dirty_inode()) already owns the lock.
    
    A typical case:
    
    reiserfs_write_end() {
            acquire_write_lock()
            mark_inode_dirty() {
                    reiserfs_dirty_inode() {
                            reacquire_write_lock() {
                                    journal_begin() {
                                            do_journal_begin_r() {
                                                    /*
                                                     * fail to release, still
                                                     * one depth of lock
                                                     */
                                                    release_write_lock()
                                                    reiserfs_wait_on_write_block() {
                                                            wait_event()
    
    The event is usually provided by something which needs the write lock but
    it hasn't been released.
    
    We use reiserfs_write_lock_once() here to ensure we only grab the
    write lock in one level.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@texware.it>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    LKML-Reference: <1239680065-25013-4-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 22c963addcf426bef97a43f6e601f985f8082ed5
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Apr 14 05:34:24 2009 +0200

    kill-the-BKL/reiserfs: lock only once in reiserfs_truncate_file
    
    Impact: fix a deadlock
    
    reiserfs_truncate_file() can be called from multiple context where
    the write lock can be already hold or not.
    
    This function also acquire (possibly recursively) the write
    lock. Subsequent releases before sleeping will not actually release
    the lock because we may be in more than one lock depth degree.
    
    A typical case is:
    
    reiserfs_file_release {
            acquire_the_lock()
            reiserfs_truncate_file()
                    reacquire_the_lock()
                    journal_begin() {
                            do_journal_begin_r() {
                                    reiserfs_wait_on_write_block() {
                                            /*
                                             * Not released because still one
                                             * depth owned
                                             */
                                            release_lock()
                                            wait_for_event()
    
    At this stage the event never happen because the one which provides
    it needs the write lock.
    
    We use reiserfs_write_lock_once() here to ensure that we don't acquire the
    write lock recursively.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@texware.it>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    LKML-Reference: <1239680065-25013-3-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit daf88c898312a22b5385655bc6e0b064eaa2efba
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Apr 14 05:34:23 2009 +0200

    kill-the-BKL/reiserfs: provide a tool to lock only once the write lock
    
    Sometimes we don't want to recursively hold the per superblock write
    lock because we want to be sure it is actually released when we come
    to sleep.
    
    This patch introduces the necessary tools for that.
    
    reiserfs_write_lock_once() does the same job than reiserfs_write_lock()
    except that it won't try to acquire recursively the lock if the current
    task already owns it. Also the lock_depth before the call of this function
    is returned.
    
    reiserfs_write_unlock_once() unlock only if reiserfs_write_lock_once()
    returned a depth equal to -1, ie: only if it actually locked.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@texware.it>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Alexander Beregalov <a.beregalov@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    LKML-Reference: <1239680065-25013-2-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a412f9efdd6424bf4bf28c8e8c92060b5e975482
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Apr 14 00:10:35 2009 +0200

    reiserfs, kill-the-BKL: fix unsafe j_flush_mutex lock
    
    Impact: fix a deadlock
    
    The j_flush_mutex is acquired safely in journal.c:
    if we can't take it, we free the reiserfs per superblock lock
    and wait a bit.
    
    But we have a remaining place in kupdate_transactions() where
    j_flush_mutex is still acquired traditionnaly. Thus the following
    scenario (warned by lockdep) can happen:
    
    A                                               B
    
    mutex_lock(&write_lock)                 mutex_lock(&write_lock)
            mutex_lock(&j_flush_mutex)      mutex_lock(&j_flush_mutex) //block
            mutex_unlock(&write_lock)
            sleep...
            mutex_lock(&write_lock) //deadlock
    
    Fix this by using reiserfs_mutex_lock_safe() in kupdate_transactions().
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@texware.it>
    Cc: Jeff Mahoney <jeffm@suse.com>
    LKML-Reference: <1239660635-12940-1-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 8ebc423238341b52912c7295b045a32477b33f09
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Apr 7 04:19:49 2009 +0200

    reiserfs: kill-the-BKL
    
    This patch is an attempt to remove the Bkl based locking scheme from
    reiserfs and is intended.
    
    It is a bit inspired from an old attempt by Peter Zijlstra:
    
       http://lkml.indiana.edu/hypermail/linux/kernel/0704.2/2174.html
    
    The bkl is heavily used in this filesystem to prevent from
    concurrent write accesses on the filesystem.
    
    Reiserfs makes a deep use of the specific properties of the Bkl:
    
    - It can be acqquired recursively by a same task
    - It is released on the schedule() calls and reacquired when schedule() returns
    
    The two properties above are a roadmap for the reiserfs write locking so it's
    very hard to simply replace it with a common mutex.
    
    - We need a recursive-able locking unless we want to restructure several blocks
      of the code.
    - We need to identify the sites where the bkl was implictly relaxed
      (schedule, wait, sync, etc...) so that we can in turn release and
      reacquire our new lock explicitly.
      Such implicit releases of the lock are often required to let other
      resources producer/consumer do their job or we can suffer unexpected
      starvations or deadlocks.
    
    So the new lock that replaces the bkl here is a per superblock mutex with a
    specific property: it can be acquired recursively by a same task, like the
    bkl.
    
    For such purpose, we integrate a lock owner and a lock depth field on the
    superblock information structure.
    
    The first axis on this patch is to turn reiserfs_write_(un)lock() function
    into a wrapper to manage this mutex. Also some explicit calls to
    lock_kernel() have been converted to reiserfs_write_lock() helpers.
    
    The second axis is to find the important blocking sites (schedule...(),
    wait_on_buffer(), sync_dirty_buffer(), etc...) and then apply an explicit
    release of the write lock on these locations before blocking. Then we can
    safely wait for those who can give us resources or those who need some.
    Typically this is a fight between the current writer, the reiserfs workqueue
    (aka the async commiter) and the pdflush threads.
    
    The third axis is a consequence of the second. The write lock is usually
    on top of a lock dependency chain which can include the journal lock, the
    flush lock or the commit lock. So it's dangerous to release and trying to
    reacquire the write lock while we still hold other locks.
    
    This is fine with the bkl:
    
          T1                       T2
    
    lock_kernel()
        mutex_lock(A)
        unlock_kernel()
        // do something
                                lock_kernel()
                                    mutex_lock(A) -> already locked by T1
                                    schedule() (and then unlock_kernel())
        lock_kernel()
        mutex_unlock(A)
        ....
    
    This is not fine with a mutex:
    
          T1                       T2
    
    mutex_lock(write)
        mutex_lock(A)
        mutex_unlock(write)
        // do something
                               mutex_lock(write)
                                  mutex_lock(A) -> already locked by T1
                                  schedule()
    
        mutex_lock(write) -> already locked by T2
        deadlock
    
    The solution in this patch is to provide a helper which releases the write
    lock and sleep a bit if we can't lock a mutex that depend on it. It's another
    simulation of the bkl behaviour.
    
    The last axis is to locate the fs callbacks that are called with the bkl held,
    according to Documentation/filesystem/Locking.
    
    Those are:
    
    - reiserfs_remount
    - reiserfs_fill_super
    - reiserfs_put_super
    
    Reiserfs didn't need to explicitly lock because of the context of these callbacks.
    But now we must take care of that with the new locking.
    
    After this patch, reiserfs suffers from a slight performance regression (for now).
    On UP, a high volume write with dd reports an average of 27 MB/s instead
    of 30 MB/s without the patch applied.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Cc: Jeff Mahoney <jeffm@suse.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Bron Gondwana <brong@fastmail.fm>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    LKML-Reference: <1239070789-13354-1-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 5fbba42ae4b30f9eb2a4fd87d4599e3c4cde2137
Author: Zygo Blaxell <zygo.blaxell@xandros.com>
Date:   Tue Jun 16 15:33:57 2009 -0700

    lib/genalloc.c: remove unmatched write_lock() in gen_pool_destroy
    
    commit 8e8a2dea0ca91fe2cb7de7ea212124cfe8c82c35 upstream.
    
    There is a call to write_lock() in gen_pool_destroy which is not balanced
    by any corresponding write_unlock().  This causes problems with preemption
    because the preemption-disable counter is incremented in the write_lock()
    call, but never decremented by any call to write_unlock().  This bug is
    gen_pool_destroy, and one of them is non-x86 arch-specific code.
    
    Signed-off-by: Zygo Blaxell <zygo.blaxell@xandros.com>
    Cc: Jiri Kosina <trivial@kernel.org>
    Cc: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 8e8a2dea0ca91fe2cb7de7ea212124cfe8c82c35
Author: Zygo Blaxell <zygo.blaxell@xandros.com>
Date:   Tue Jun 16 15:33:57 2009 -0700

    lib/genalloc.c: remove unmatched write_lock() in gen_pool_destroy
    
    There is a call to write_lock() in gen_pool_destroy which is not balanced
    by any corresponding write_unlock().  This causes problems with preemption
    because the preemption-disable counter is incremented in the write_lock()
    call, but never decremented by any call to write_unlock().  This bug is
    gen_pool_destroy, and one of them is non-x86 arch-specific code.
    
    Signed-off-by: Zygo Blaxell <zygo.blaxell@xandros.com>
    Cc: Jiri Kosina <trivial@kernel.org>
    Cc: Steve Wise <swise@opengridcomputing.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ebc1ac164560a241d9bf1b7519062910c3f90a01
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 11 23:35:03 2009 +0200

    ->write_super lock_super pushdown
    
    Push down lock_super into ->write_super instances and remove it from the
    caller.
    
    Following filesystem don't need ->s_lock in ->write_super and are skipped:
    
     * bfs, nilfs2 - no other uses of s_lock and have internal locks in
            ->write_super
     * ext2 - uses BKL in ext2_write_super and has internal calls without s_lock
     * reiserfs - no other uses of s_lock as has reiserfs_write_lock (BKL) in
            ->write_super
     * xfs - no other uses of s_lock and uses internal lock (buffer lock on
            superblock buffer) to serialize ->write_super.  Also xfs_fs_write_super
            is superflous and will go away in the next merge window
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 4cc341cacb9288d4678e1b899af0a5ac66dbe2f2
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Apr 24 01:02:45 2009 +0200

    check_unsafe_exec: s/lock_task_sighand/rcu_read_lock/
    
    commit 437f7fdb607f32b737e4da9f14bebcfdac2c90c3 upstream.
    
    write_lock(&current->fs->lock) guarantees we can't wrongly miss
    LSM_UNSAFE_SHARE, this is what we care about. Use rcu_read_lock()
    instead of ->siglock to iterate over the sub-threads. We must see
    all CLONE_THREAD|CLONE_FS threads which didn't pass exit_fs(), it
    takes fs->lock too.
    
    With or without this patch we can miss the freshly cloned thread
    and set LSM_UNSAFE_SHARE, we don't care.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    [ Fixed lock/unlock typo  - Hugh ]
    Acked-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit d093166dbab62418c468481891cdfbc70e3c73f9
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Mar 30 07:20:30 2009 -0400

    New locking/refcounting for fs_struct
    
    commit 498052bba55ecaff58db6a1436b0e25bfd75a7ff upstream.
    
    * all changes of current->fs are done under task_lock and write_lock of
      old fs->lock
    * refcount is not atomic anymore (same protection)
    * its decrements are done when removing reference from current; at the
      same time we decide whether to free it.
    * put_fs_struct() is gone
    * new field - ->in_exec.  Set by check_unsafe_exec() if we are trying to do
      execve() and only subthreads share fs_struct.  Cleared when finishing exec
      (success and failure alike).  Makes CLONE_FS fail with -EAGAIN if set.
    * check_unsafe_exec() may fail with -EAGAIN if another execve() from subthread
      is in progress.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 437f7fdb607f32b737e4da9f14bebcfdac2c90c3
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Apr 24 01:02:45 2009 +0200

    check_unsafe_exec: s/lock_task_sighand/rcu_read_lock/
    
    write_lock(&current->fs->lock) guarantees we can't wrongly miss
    LSM_UNSAFE_SHARE, this is what we care about. Use rcu_read_lock()
    instead of ->siglock to iterate over the sub-threads. We must see
    all CLONE_THREAD|CLONE_FS threads which didn't pass exit_fs(), it
    takes fs->lock too.
    
    With or without this patch we can miss the freshly cloned thread
    and set LSM_UNSAFE_SHARE, we don't care.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    [ Fixed lock/unlock typo  - Hugh ]
    Acked-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 2d09cde985702503970d7cc18d762fae17e1cf88
Author: Robin Holt <holt@sgi.com>
Date:   Thu Apr 2 16:59:47 2009 -0700

    ia64: implement interrupt-enabling rwlocks
    
    Implement __raw_read_lock_flags and __raw_write_lock_flags for the ia64
    architecture.
    
    [kosaki.motohiro@jp.fujitsu.com: typo fix]
    Signed-off-by: Petr Tesarik <ptesarik@suse.cz>
    Signed-off-by: Robin Holt <holt@sgi.com>
    Cc: <linux-arch@vger.kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit f5f7eac41db827a47b2163330eecd7bb55ae9f12
Author: Robin Holt <holt@sgi.com>
Date:   Thu Apr 2 16:59:46 2009 -0700

    Allow rwlocks to re-enable interrupts
    
    Pass the original flags to rwlock arch-code, so that it can re-enable
    interrupts if implemented for that architecture.
    
    Initially, make __raw_read_lock_flags and __raw_write_lock_flags stubs
    which just do the same thing as non-flags variants.
    
    Signed-off-by: Petr Tesarik <ptesarik@suse.cz>
    Signed-off-by: Robin Holt <holt@sgi.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: <linux-arch@vger.kernel.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 498052bba55ecaff58db6a1436b0e25bfd75a7ff
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Mar 30 07:20:30 2009 -0400

    New locking/refcounting for fs_struct
    
    * all changes of current->fs are done under task_lock and write_lock of
      old fs->lock
    * refcount is not atomic anymore (same protection)
    * its decrements are done when removing reference from current; at the
      same time we decide whether to free it.
    * put_fs_struct() is gone
    * new field - ->in_exec.  Set by check_unsafe_exec() if we are trying to do
      execve() and only subthreads share fs_struct.  Cleared when finishing exec
      (success and failure alike).  Makes CLONE_FS fail with -EAGAIN if set.
    * check_unsafe_exec() may fail with -EAGAIN if another execve() from subthread
      is in progress.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 7534432dcc3c654a8671b6b0cdffd1dbdbc73074
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Jan 7 18:07:40 2009 -0800

    cgroups: remove rcu_read_lock() in cgroupstats_build()
    
    cgroup_iter_* do not need rcu_read_lock().
    
    In cgroup_enable_task_cg_lists(), do_each_thread() and while_each_thread()
    are protected by RCU, it's OK, for write_lock(&css_set_lock) implies
    rcu_read_lock() in non-RT kernel.
    
    If we need explicit rcu_read_lock(), we should add rcu_read_lock() in
    cgroup_enable_task_cg_lists(), not cgroup_iter_*.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Acked-by: Paul Menage <menage@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 957ad0166e9f76a8561dafa5e14ef5bd3f5e9a3b
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Fri Nov 21 01:30:36 2008 +0100

    sched: update comment for move_task_off_dead_cpu
    
    Impact: cleanup
    
    This commit:
    
    commit f7b4cddcc5aca03e80e357360c9424dfba5056c2
    Author: Oleg Nesterov <oleg@tv-sign.ru>
    Date:   Tue Oct 16 23:30:56 2007 -0700
    
        do CPU_DEAD migrating under read_lock(tasklist) instead of write_lock_irq(ta
    
        Currently move_task_off_dead_cpu() is called under
        write_lock_irq(tasklist).  This means it can't use task_lock() which is
        needed to improve migrating to take task's ->cpuset into account.
    
        Change the code to call move_task_off_dead_cpu() with irqs enabled, and
        change migrate_live_tasks() to use read_lock(tasklist).
    
    ...forgot to update the comment in front of move_task_off_dead_cpu.
    
    Reference: http://lkml.org/lkml/2008/6/23/135
    
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 9db66bdcc83749affe61c61eb8ff3cf08f42afec
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Thu Nov 20 20:39:09 2008 -0800

    net: convert TCP/DCCP ehash rwlocks to spinlocks
    
    Now TCP & DCCP use RCU lookups, we can convert ehash rwlocks to spinlocks.
    
    /proc/net/tcp and other seq_file 'readers' can safely be converted to 'writers'.
    
    This should speedup writers, since spin_lock()/spin_unlock()
    only use one atomic operation instead of two for write_lock()/write_unlock()
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 3ab4c4da271c5628b1320d8ee2232db297c901b6
Author: Brandon Philips <brandon@ifup.org>
Date:   Thu Nov 6 11:19:11 2008 -0800

    USB: cdc-acm.c: fix recursive lock in acm_start_wb error path
    
    commit ad0b65efd12d020b046cde8d6f474e37bb98dd73 upstream.
    
    Fixes an obvious bug in cdc-acm by avoiding a recursive lock on
    acm_start_wb()'s error path. Should apply towards 2.6.27 stable and
    2.6.28.
    
    =============================================
    [ INFO: possible recursive locking detected ]
    2.6.27-2-pae #109
    ---------------------------------------------
    python/31449 is trying to acquire lock:
     (&acm->write_lock){++..}, at: [<f89a0348>] acm_start_wb+0x5c/0x7b [cdc_acm]
    
    but task is already holding lock:
     (&acm->write_lock){++..}, at: [<f89a04fb>] acm_tty_write+0xe1/0x167 [cdc_acm]
    
    other info that might help us debug this:
    2 locks held by python/31449:
     #0:  (&tty->atomic_write_lock){--..}, at: [<c0260fae>] tty_write_lock+0x14/0x3b
     #1:  (&acm->write_lock){++..}, at: [<f89a04fb>] acm_tty_write+0xe1/0x167 [cdc_acm]
    
    stack backtrace:
    Pid: 31449, comm: python Not tainted 2.6.27-2-pae #109
     [<c030f42f>] ? printk+0xf/0x18
     [<c0149f33>] __lock_acquire+0xc7b/0x1316
     [<c014a63e>] lock_acquire+0x70/0x97
     [<f89a0348>] ? acm_start_wb+0x5c/0x7b [cdc_acm]
     [<c0312109>] _spin_lock_irqsave+0x37/0x47
     [<f89a0348>] ? acm_start_wb+0x5c/0x7b [cdc_acm]
     [<f89a0348>] acm_start_wb+0x5c/0x7b [cdc_acm]
     [<f89a055d>] acm_tty_write+0x143/0x167 [cdc_acm]
     [<c0262a98>] write_chan+0x1cd/0x297
     [<c012527e>] ? default_wake_function+0x0/0xd
     [<c026111e>] tty_write+0x149/0x1b9
     [<c02628cb>] ? write_chan+0x0/0x297
     [<c01912c5>] ? rw_verify_area+0x76/0x98
     [<c0260fd5>] ? tty_write+0x0/0x1b9
     [<c01919ba>] vfs_write+0x8c/0x136
     [<c0191afd>] sys_write+0x3b/0x60
     [<c0103beb>] sysenter_do_call+0x12/0x3f
     =======================
    
    Signed-off-by: Brandon Philips <bphilips@suse.de>
    Cc: Oliver Neukum <oliver@neukum.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit ad0b65efd12d020b046cde8d6f474e37bb98dd73
Author: Brandon Philips <brandon@ifup.org>
Date:   Thu Nov 6 11:19:11 2008 -0800

    USB: cdc-acm.c: fix recursive lock in acm_start_wb error path
    
    Fixes an obvious bug in cdc-acm by avoiding a recursive lock on
    acm_start_wb()'s error path. Should apply towards 2.6.27 stable and
    2.6.28.
    
    =============================================
    [ INFO: possible recursive locking detected ]
    2.6.27-2-pae #109
    ---------------------------------------------
    python/31449 is trying to acquire lock:
     (&acm->write_lock){++..}, at: [<f89a0348>] acm_start_wb+0x5c/0x7b [cdc_acm]
    
    but task is already holding lock:
     (&acm->write_lock){++..}, at: [<f89a04fb>] acm_tty_write+0xe1/0x167 [cdc_acm]
    
    other info that might help us debug this:
    2 locks held by python/31449:
     #0:  (&tty->atomic_write_lock){--..}, at: [<c0260fae>] tty_write_lock+0x14/0x3b
     #1:  (&acm->write_lock){++..}, at: [<f89a04fb>] acm_tty_write+0xe1/0x167 [cdc_acm]
    
    stack backtrace:
    Pid: 31449, comm: python Not tainted 2.6.27-2-pae #109
     [<c030f42f>] ? printk+0xf/0x18
     [<c0149f33>] __lock_acquire+0xc7b/0x1316
     [<c014a63e>] lock_acquire+0x70/0x97
     [<f89a0348>] ? acm_start_wb+0x5c/0x7b [cdc_acm]
     [<c0312109>] _spin_lock_irqsave+0x37/0x47
     [<f89a0348>] ? acm_start_wb+0x5c/0x7b [cdc_acm]
     [<f89a0348>] acm_start_wb+0x5c/0x7b [cdc_acm]
     [<f89a055d>] acm_tty_write+0x143/0x167 [cdc_acm]
     [<c0262a98>] write_chan+0x1cd/0x297
     [<c012527e>] ? default_wake_function+0x0/0xd
     [<c026111e>] tty_write+0x149/0x1b9
     [<c02628cb>] ? write_chan+0x0/0x297
     [<c01912c5>] ? rw_verify_area+0x76/0x98
     [<c0260fd5>] ? tty_write+0x0/0x1b9
     [<c01919ba>] vfs_write+0x8c/0x136
     [<c0191afd>] sys_write+0x3b/0x60
     [<c0103beb>] sysenter_do_call+0x12/0x3f
     =======================
    
    Signed-off-by: Brandon Philips <bphilips@suse.de>
    Cc: Oliver Neukum <oliver@neukum.org>
    Cc: stable <stable@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit db64fe02258f1507e13fe5212a989922323685ce
Author: Nick Piggin <npiggin@suse.de>
Date:   Sat Oct 18 20:27:03 2008 -0700

    mm: rewrite vmap layer
    
    Rewrite the vmap allocator to use rbtrees and lazy tlb flushing, and
    provide a fast, scalable percpu frontend for small vmaps (requires a
    slightly different API, though).
    
    The biggest problem with vmap is actually vunmap.  Presently this requires
    a global kernel TLB flush, which on most architectures is a broadcast IPI
    to all CPUs to flush the cache.  This is all done under a global lock.  As
    the number of CPUs increases, so will the number of vunmaps a scaled
    workload will want to perform, and so will the cost of a global TLB flush.
     This gives terrible quadratic scalability characteristics.
    
    Another problem is that the entire vmap subsystem works under a single
    lock.  It is a rwlock, but it is actually taken for write in all the fast
    paths, and the read locking would likely never be run concurrently anyway,
    so it's just pointless.
    
    This is a rewrite of vmap subsystem to solve those problems.  The existing
    vmalloc API is implemented on top of the rewritten subsystem.
    
    The TLB flushing problem is solved by using lazy TLB unmapping.  vmap
    addresses do not have to be flushed immediately when they are vunmapped,
    because the kernel will not reuse them again (would be a use-after-free)
    until they are reallocated.  So the addresses aren't allocated again until
    a subsequent TLB flush.  A single TLB flush then can flush multiple
    vunmaps from each CPU.
    
    XEN and PAT and such do not like deferred TLB flushing because they can't
    always handle multiple aliasing virtual addresses to a physical address.
    They now call vm_unmap_aliases() in order to flush any deferred mappings.
    That call is very expensive (well, actually not a lot more expensive than
    a single vunmap under the old scheme), however it should be OK if not
    called too often.
    
    The virtual memory extent information is stored in an rbtree rather than a
    linked list to improve the algorithmic scalability.
    
    There is a per-CPU allocator for small vmaps, which amortizes or avoids
    global locking.
    
    To use the per-CPU interface, the vm_map_ram / vm_unmap_ram interfaces
    must be used in place of vmap and vunmap.  Vmalloc does not use these
    interfaces at the moment, so it will not be quite so scalable (although it
    will use lazy TLB flushing).
    
    As a quick test of performance, I ran a test that loops in the kernel,
    linearly mapping then touching then unmapping 4 pages.  Different numbers
    of tests were run in parallel on an 4 core, 2 socket opteron.  Results are
    in nanoseconds per map+touch+unmap.
    
    threads           vanilla         vmap rewrite
    1                 14700           2900
    2                 33600           3000
    4                 49500           2800
    8                 70631           2900
    
    So with a 8 cores, the rewritten version is already 25x faster.
    
    In a slightly more realistic test (although with an older and less
    scalable version of the patch), I ripped the not-very-good vunmap batching
    code out of XFS, and implemented the large buffer mapping with vm_map_ram
    and vm_unmap_ram...  along with a couple of other tricks, I was able to
    speed up a large directory workload by 20x on a 64 CPU system.  I believe
    vmap/vunmap is actually sped up a lot more than 20x on such a system, but
    I'm running into other locks now.  vmap is pretty well blown off the
    profiles.
    
    Before:
    1352059 total                                      0.1401
    798784 _write_lock                              8320.6667 <- vmlist_lock
    529313 default_idle                             1181.5022
     15242 smp_call_function                         15.8771  <- vmap tlb flushing
      2472 __get_vm_area_node                         1.9312  <- vmap
      1762 remove_vm_area                             4.5885  <- vunmap
       316 map_vm_area                                0.2297  <- vmap
       312 kfree                                      0.1950
       300 _spin_lock                                 3.1250
       252 sn_send_IPI_phys                           0.4375  <- tlb flushing
       238 vmap                                       0.8264  <- vmap
       216 find_lock_page                             0.5192
       196 find_next_bit                              0.3603
       136 sn2_send_IPI                               0.2024
       130 pio_phys_write_mmr                         2.0312
       118 unmap_kernel_range                         0.1229
    
    After:
     78406 total                                      0.0081
     40053 default_idle                              89.4040
     33576 ia64_spinlock_contention                 349.7500
      1650 _spin_lock                                17.1875
       319 __reg_op                                   0.5538
       281 _atomic_dec_and_lock                       1.0977
       153 mutex_unlock                               1.5938
       123 iget_locked                                0.1671
       117 xfs_dir_lookup                             0.1662
       117 dput                                       0.1406
       114 xfs_iget_core                              0.0268
        92 xfs_da_hashname                            0.1917
        75 d_alloc                                    0.0670
        68 vmap_page_range                            0.0462 <- vmap
        58 kmem_cache_alloc                           0.0604
        57 memset                                     0.0540
        52 rb_next                                    0.1625
        50 __copy_user                                0.0208
        49 bitmap_find_free_region                    0.2188 <- vmap
        46 ia64_sn_udelay                             0.1106
        45 find_inode_fast                            0.1406
        42 memcmp                                     0.2188
        42 finish_task_switch                         0.1094
        42 __d_lookup                                 0.0410
        40 radix_tree_lookup_slot                     0.1250
        37 _spin_unlock_irqrestore                    0.3854
        36 xfs_bmapi                                  0.0050
        36 kmem_cache_free                            0.0256
        35 xfs_vn_getattr                             0.0322
        34 radix_tree_lookup                          0.1062
        33 __link_path_walk                           0.0035
        31 xfs_da_do_buf                              0.0091
        30 _xfs_buf_find                              0.0204
        28 find_get_page                              0.0875
        27 xfs_iread                                  0.0241
        27 __strncpy_from_user                        0.2812
        26 _xfs_buf_initialize                        0.0406
        24 _xfs_buf_lookup_pages                      0.0179
        24 vunmap_page_range                          0.0250 <- vunmap
        23 find_lock_page                             0.0799
        22 vm_map_ram                                 0.0087 <- vmap
        20 kfree                                      0.0125
        19 put_page                                   0.0330
        18 __kmalloc                                  0.0176
        17 xfs_da_node_lookup_int                     0.0086
        17 _read_lock                                 0.0885
        17 page_waitqueue                             0.0664
    
    vmap has gone from being the top 5 on the profiles and flushing the crap
    out of all TLBs, to using less than 1% of kernel time.
    
    [akpm@linux-foundation.org: cleanups, section fix]
    [akpm@linux-foundation.org: fix build on alpha]
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Krzysztof Helt <krzysztof.h1@poczta.fm>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b1e766137fe2462fd110e2930f74ef5636adb436
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Oct 10 21:33:17 2008 +0200

    [S390] cio: fix cio_tpi.
    
    In cio_tpi only disable bottom halves when not in interrupt context.
    Otherwise a WARN_ON gets triggered. Besides that, when we are in
    interrupt context bottom halves are disabled anyway.
    Fixes this one:
    
    Badness at kernel/softirq.c:77
    Modules linked in:
    CPU: 2 Not tainted 2.6.26 #4
    Process swapper (pid: 0, task: 000000003fe83db0, ksp: 000000003fea7d28)
    Krnl PSW : 0404c00180000000 0000000000053f4e (__local_bh_disable+0xbe/0xcc)
               R:0 T:1 IO:0 EX:0 Key:0 M:1 W:0 P:0 AS:3 CC:0 PM:0 EA:3
    Krnl GPRS: 0000000000008ee0 00000000005f95e0 0000000000000000 0000000000000001
               000000000020be92 0000000000000000 0000000000000210 00000000005d36c0
               000000003fb5f4d8 0000000000000000 000000000020bed0 000000003fb5f3c8
               00000000009be920 0000000000364898 000000003fb5f408 000000003fb5f3c8
    Krnl Code: 0000000000053f42: bf2f1000           icm     %r2,15,0(%r1)
               0000000000053f46: a774ffc5           brc     7,53ed0
               0000000000053f4a: a7f40001           brc     15,53f4c
              >0000000000053f4e: a7280001           lhi     %r2,1
               0000000000053f52: 50201000           st      %r2,0(%r1)
               0000000000053f56: a7f4ffbd           brc     15,53ed0
               0000000000053f5a: 0707               bcr     0,%r7
               0000000000053f5c: a7f13fc0           tmll    %r15,16320
    Call Trace:
    ([<0000000000000210>] 0x210)
     [<0000000000053f86>] local_bh_disable+0x2a/0x38
     [<000000000020bed0>] wait_cons_dev+0xd4/0x154
     [<0000000000247cb2>] raw3215_make_room+0x6a/0x1a8
     [<000000000024861a>] raw3215_write+0x86/0x28c
     [<00000000002488a0>] con3215_write+0x80/0x110
     [<000000000004c3e0>] __call_console_drivers+0xc8/0xe4
     [<000000000004c47e>] _call_console_drivers+0x82/0xc4
     [<000000000004c744>] release_console_sem+0x218/0x2c0
     [<000000000004cf64>] vprintk+0x3c0/0x504
     [<0000000000354a4a>] printk+0x52/0x64
     [<0000000000088004>] __print_symbol+0x40/0x50
     [<0000000000071dbc>] print_stack_trace+0x78/0xac
     [<0000000000079e78>] print_lock_dependencies+0x148/0x208
     [<000000000007a050>] print_irq_inversion_bug+0x118/0x15c
     [<000000000007a106>] check_usage_forwards+0x72/0x84
     [<000000000007a36e>] mark_lock+0x1d2/0x594
     [<000000000007baca>] __lock_acquire+0x886/0xf48
     [<000000000007c234>] lock_acquire+0xa8/0xe0
     [<0000000000350316>] _write_lock+0x56/0x98
     [<000000000026cd92>] zfcp_erp_adapter_reopen+0x4e/0x8c
     [<000000000026f1e8>] zfcp_qdio_int_resp+0x2e4/0x2f4
     [<00000000002210f4>] qdio_int_handler+0x274/0x888
     [<00000000002177b6>] ccw_device_call_handler+0x6e/0xd8
     [<0000000000215336>] ccw_device_irq+0xd6/0x160
     [<0000000000212f88>] io_subchannel_irq+0x8c/0x118
     [<000000000020c120>] do_IRQ+0x1d0/0x1fc
     [<00000000000270b2>] io_return+0x0/0x8
     [<000000000001c8a4>] cpu_idle+0x178/0x21c
    ([<000000000001c884>] cpu_idle+0x158/0x21c)
     [<00000000003483a2>] start_secondary+0xb6/0xc8
    INFO: lockdep is turned off.
    Last Breaking-Event-Address:
     [<0000000000053f4a>] __local_bh_disable+0xba/0xcc
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit e8aed68614c81f24d8c4cbcb4923f848ece846e1
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Sep 10 11:01:07 2008 +0800

    doc/RCU: fix pseudocode in rcuref.txt
    
    atomic_inc_not_zero(v) return 0 if *v = 0.
    use spin_lock instead of write_lock for update lock.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 52fd8ca6ad4124c15952ded35cfcf6adbd7ae8d4
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Wed Jul 30 09:29:06 2008 -0700

    IB/ipath: Use unsigned long for irq flags
    
    A few functions in the ipath driver incorrectly use unsigned int to
    hold irq flags for spin_lock_irqsave().
    
    This patch was generated using the Coccinelle framework with the
    following semantic patch:
    
    The semantic patch I used was this:
    
    @@
    expression lock;
    identifier flags;
    expression subclass;
    @@
    
    - unsigned int flags;
    + unsigned long flags;
    
    ...
    
    <+...
    
    (
     spin_lock_irqsave(lock, flags)
    |
     _spin_lock_irqsave(lock)
    |
     spin_unlock_irqrestore(lock, flags)
    |
     _spin_unlock_irqrestore(lock, flags)
    |
     read_lock_irqsave(lock, flags)
    |
     _read_lock_irqsave(lock)
    |
     read_unlock_irqrestore(lock, flags)
    |
     _read_unlock_irqrestore(lock, flags)
    |
     write_lock_irqsave(lock, flags)
    |
     _write_lock_irqsave(lock)
    |
     write_unlock_irqrestore(lock, flags)
    |
     _write_unlock_irqrestore(lock, flags)
    |
     spin_lock_irqsave_nested(lock, flags, subclass)
    |
     _spin_lock_irqsave_nested(lock, subclass)
    |
     spin_unlock_irqrestore(lock, flags)
    |
     _spin_unlock_irqrestore(lock, flags)
    |
     _raw_spin_lock_flags(lock, flags)
    |
     __raw_spin_lock_flags(lock, flags)
    )
    
    ...+>
    
    Cc: Ralph Campbell <ralph.campbell@qlogic.com>
    Cc: Julia Lawall <julia@diku.dk>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

commit 77bbadd5ea893f364a0d1879723037678a03725c
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Tue Jul 29 13:31:47 2008 +0200

    PS3: gelic: use unsigned long for irqflags
    
    The semantic patch I used was this:
    
    @@
    expression lock;
    identifier flags;
    expression subclass;
    @@
    
    - unsigned int flags;
    + unsigned long flags;
    
    ...
    
    <+...
    
    (
     spin_lock_irqsave(lock, flags)
    |
     _spin_lock_irqsave(lock)
    |
     spin_unlock_irqrestore(lock, flags)
    |
     _spin_unlock_irqrestore(lock, flags)
    |
     read_lock_irqsave(lock, flags)
    |
     _read_lock_irqsave(lock)
    |
     read_unlock_irqrestore(lock, flags)
    |
     _read_unlock_irqrestore(lock, flags)
    |
     write_lock_irqsave(lock, flags)
    |
     _write_lock_irqsave(lock)
    |
     write_unlock_irqrestore(lock, flags)
    |
     _write_unlock_irqrestore(lock, flags)
    |
     spin_lock_irqsave_nested(lock, flags, subclass)
    |
     _spin_lock_irqsave_nested(lock, subclass)
    |
     spin_unlock_irqrestore(lock, flags)
    |
     _spin_unlock_irqrestore(lock, flags)
    |
     _raw_spin_lock_flags(lock, flags)
    |
     __raw_spin_lock_flags(lock, flags)
    )
    
    ...+>
    
    This patch was generated using the Coccinelle framework.
    
    Cc: Masakazu Mokuno <mokuno@sm.sony.co.jp>
    Cc: Julia Lawall <julia@diku.dk>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>

commit 992860e991f2015fb8c8df65aa32afa0dcbb4430
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 14 10:28:38 2008 +0200

    lockdep: fix ftrace irq tracing false positive
    
    fix this false positive:
    
    [    0.020000] ------------[ cut here ]------------
    [    0.020000] WARNING: at kernel/lockdep.c:2718 check_flags+0x14a/0x170()
    [    0.020000] Modules linked in:
    [    0.020000] Pid: 0, comm: swapper Not tainted 2.6.26-tip-00343-gd7e5521-dirty #14486
    [    0.020000]  [<c01312e4>] warn_on_slowpath+0x54/0x80
    [    0.020000]  [<c067e451>] ? _spin_unlock_irqrestore+0x61/0x70
    [    0.020000]  [<c0131bb1>] ? release_console_sem+0x201/0x210
    [    0.020000]  [<c0143d65>] ? __kernel_text_address+0x35/0x40
    [    0.020000]  [<c010562e>] ? dump_trace+0x5e/0x140
    [    0.020000]  [<c01518b5>] ? __lock_acquire+0x245/0x820
    [    0.020000]  [<c015063a>] check_flags+0x14a/0x170
    [    0.020000]  [<c0151ed8>] ? lock_acquire+0x48/0xc0
    [    0.020000]  [<c0151ee1>] lock_acquire+0x51/0xc0
    [    0.020000]  [<c014a16c>] ? down+0x2c/0x40
    [    0.020000]  [<c010a609>] ? sched_clock+0x9/0x10
    [    0.020000]  [<c067e7b2>] _write_lock+0x32/0x60
    [    0.020000]  [<c013797f>] ? request_resource+0x1f/0xb0
    [    0.020000]  [<c013797f>] request_resource+0x1f/0xb0
    [    0.020000]  [<c02f89ad>] vgacon_startup+0x2bd/0x3e0
    [    0.020000]  [<c094d62a>] con_init+0x19/0x22f
    [    0.020000]  [<c0330c7c>] ? tty_register_ldisc+0x5c/0x70
    [    0.020000]  [<c094cf49>] console_init+0x20/0x2e
    [    0.020000]  [<c092a969>] start_kernel+0x20c/0x379
    [    0.020000]  [<c092a516>] ? unknown_bootoption+0x0/0x1f6
    [    0.020000]  [<c092a099>] __init_begin+0x99/0xa1
    [    0.020000]  =======================
    [    0.020000] ---[ end trace 4eaa2a86a8e2da22 ]---
    [    0.020000] possible reason: unannotated irqs-on.
    [    0.020000] irq event stamp: 0
    
    which occurs if CONFIG_TRACE_IRQFLAGS=y, CONFIG_DEBUG_LOCKDEP=y,
    but CONFIG_PROVE_LOCKING is disabled.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 8566dc3fca470454461b161677a5ae3bb3a8c1b8
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed May 7 19:51:51 2008 +0400

    mac80211: Fix sleeping allocation under lock in mesh_path_node_copy.
    
    The mesh_path_node_copy() can be called like this:
    mesh_path_add
     `- write_lock(&pathtbl_resize_lock); /* ! */
     `- mesh_table_grow
         `- ->copy_node
               `- mesh_path_node_copy
    
    thus, the GFP_KERNEL is not suitable here.
    
    The acceptable fix, I suppose, is make this allocation GPF_ATOMIC -
    the mpath_node being allocated is 4 pointers, i.e. this allocation
    is small enough to survive even under a moderate memory pressure.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>

commit 047f7617eba3653ff3bcfbe902986903fff2ed3b
Author: Bernard Pidoux <f6bvp@amsat.org>
Date:   Sun Apr 20 15:58:07 2008 -0700

    [ROSE]: Fix soft lockup wrt. rose_node_list_lock
    
    [ INFO: possible recursive locking detected ]
    2.6.25 #3
    ---------------------------------------------
    ax25ipd/3811 is trying to acquire lock:
      (rose_node_list_lock){-+..}, at: [<f8d31f1a>] rose_get_neigh+0x1a/0xa0
    [rose]
    
    but task is already holding lock:
      (rose_node_list_lock){-+..}, at: [<f8d31fed>]
    rose_route_frame+0x4d/0x620 [rose]
    
    other info that might help us debug this:
    6 locks held by ax25ipd/3811:
      #0:  (&tty->atomic_write_lock){--..}, at: [<c0259a1c>]
    tty_write_lock+0x1c/0x50
      #1:  (rcu_read_lock){..--}, at: [<c02aea36>] net_rx_action+0x96/0x230
      #2:  (rcu_read_lock){..--}, at: [<c02ac5c0>] netif_receive_skb+0x100/0x2f0
      #3:  (rose_node_list_lock){-+..}, at: [<f8d31fed>]
    rose_route_frame+0x4d/0x620 [rose]
      #4:  (rose_neigh_list_lock){-+..}, at: [<f8d31ff7>]
    rose_route_frame+0x57/0x620 [rose]
      #5:  (rose_route_list_lock){-+..}, at: [<f8d32001>]
    rose_route_frame+0x61/0x620 [rose]
    
    stack backtrace:
    Pid: 3811, comm: ax25ipd Not tainted 2.6.25 #3
      [<c0147e27>] print_deadlock_bug+0xc7/0xd0
      [<c0147eca>] check_deadlock+0x9a/0xb0
      [<c0149cd2>] validate_chain+0x1e2/0x310
      [<c0149b95>] ? validate_chain+0xa5/0x310
      [<c010a7d8>] ? native_sched_clock+0x88/0xc0
      [<c0149fa1>] __lock_acquire+0x1a1/0x750
      [<c014a5d1>] lock_acquire+0x81/0xa0
      [<f8d31f1a>] ? rose_get_neigh+0x1a/0xa0 [rose]
      [<c03201a3>] _spin_lock_bh+0x33/0x60
      [<f8d31f1a>] ? rose_get_neigh+0x1a/0xa0 [rose]
      [<f8d31f1a>] rose_get_neigh+0x1a/0xa0 [rose]
      [<f8d32404>] rose_route_frame+0x464/0x620 [rose]
      [<c031ffdd>] ? _read_unlock+0x1d/0x20
      [<f8d31fa0>] ? rose_route_frame+0x0/0x620 [rose]
      [<f8d1c396>] ax25_rx_iframe+0x66/0x3b0 [ax25]
      [<f8d1f42f>] ? ax25_start_t3timer+0x1f/0x40 [ax25]
      [<f8d1e65b>] ax25_std_frame_in+0x7fb/0x890 [ax25]
      [<c0320005>] ? _spin_unlock_bh+0x25/0x30
      [<f8d1bdf6>] ax25_kiss_rcv+0x2c6/0x800 [ax25]
      [<c02a4769>] ? sock_def_readable+0x59/0x80
      [<c014a8a7>] ? __lock_release+0x47/0x70
      [<c02a4769>] ? sock_def_readable+0x59/0x80
      [<c031ffdd>] ? _read_unlock+0x1d/0x20
      [<c02a4769>] ? sock_def_readable+0x59/0x80
      [<c02a4d3a>] ? sock_queue_rcv_skb+0x13a/0x1d0
      [<c02a4c45>] ? sock_queue_rcv_skb+0x45/0x1d0
      [<f8d1bb30>] ? ax25_kiss_rcv+0x0/0x800 [ax25]
      [<c02ac715>] netif_receive_skb+0x255/0x2f0
      [<c02ac5c0>] ? netif_receive_skb+0x100/0x2f0
      [<c02af05c>] process_backlog+0x7c/0xf0
      [<c02aeb0c>] net_rx_action+0x16c/0x230
      [<c02aea36>] ? net_rx_action+0x96/0x230
      [<c012bd53>] __do_softirq+0x93/0x120
      [<f8d2a68a>] ? mkiss_receive_buf+0x33a/0x3f0 [mkiss]
      [<c012be37>] do_softirq+0x57/0x60
      [<c012c265>] local_bh_enable_ip+0xa5/0xe0
      [<c0320005>] _spin_unlock_bh+0x25/0x30
      [<f8d2a68a>] mkiss_receive_buf+0x33a/0x3f0 [mkiss]
      [<c025ea37>] pty_write+0x47/0x60
      [<c025c620>] write_chan+0x1b0/0x220
      [<c0259a1c>] ? tty_write_lock+0x1c/0x50
      [<c011fec0>] ? default_wake_function+0x0/0x10
      [<c0259bea>] tty_write+0x12a/0x1c0
      [<c025c470>] ? write_chan+0x0/0x220
      [<c018bbc6>] vfs_write+0x96/0x130
      [<c0259ac0>] ? tty_write+0x0/0x1c0
      [<c018c24d>] sys_write+0x3d/0x70
      [<c0104d1e>] sysenter_past_esp+0x5f/0xa5
      =======================
    BUG: soft lockup - CPU#0 stuck for 61s! [ax25ipd:3811]
    
    Pid: 3811, comm: ax25ipd Not tainted (2.6.25 #3)
    EIP: 0060:[<c010a9db>] EFLAGS: 00000246 CPU: 0
    EIP is at native_read_tsc+0xb/0x20
    EAX: b404aa2c EBX: b404a9c9 ECX: 017f1000 EDX: 0000076b
    ESI: 00000001 EDI: 00000000 EBP: ecc83afc ESP: ecc83afc
      DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
    CR0: 8005003b CR2: b7f5f000 CR3: 2cd8e000 CR4: 000006f0
    DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
    DR6: ffff0ff0 DR7: 00000400
      [<c0204937>] delay_tsc+0x17/0x30
      [<c02048e9>] __delay+0x9/0x10
      [<c02127f6>] __spin_lock_debug+0x76/0xf0
      [<c0212618>] ? spin_bug+0x18/0x100
      [<c0147923>] ? __lock_contended+0xa3/0x110
      [<c0212998>] _raw_spin_lock+0x68/0x90
      [<c03201bf>] _spin_lock_bh+0x4f/0x60
      [<f8d31f1a>] ? rose_get_neigh+0x1a/0xa0 [rose]
      [<f8d31f1a>] rose_get_neigh+0x1a/0xa0 [rose]
      [<f8d32404>] rose_route_frame+0x464/0x620 [rose]
      [<c031ffdd>] ? _read_unlock+0x1d/0x20
      [<f8d31fa0>] ? rose_route_frame+0x0/0x620 [rose]
      [<f8d1c396>] ax25_rx_iframe+0x66/0x3b0 [ax25]
      [<f8d1f42f>] ? ax25_start_t3timer+0x1f/0x40 [ax25]
      [<f8d1e65b>] ax25_std_frame_in+0x7fb/0x890 [ax25]
      [<c0320005>] ? _spin_unlock_bh+0x25/0x30
      [<f8d1bdf6>] ax25_kiss_rcv+0x2c6/0x800 [ax25]
      [<c02a4769>] ? sock_def_readable+0x59/0x80
      [<c014a8a7>] ? __lock_release+0x47/0x70
      [<c02a4769>] ? sock_def_readable+0x59/0x80
      [<c031ffdd>] ? _read_unlock+0x1d/0x20
      [<c02a4769>] ? sock_def_readable+0x59/0x80
      [<c02a4d3a>] ? sock_queue_rcv_skb+0x13a/0x1d0
      [<c02a4c45>] ? sock_queue_rcv_skb+0x45/0x1d0
      [<f8d1bb30>] ? ax25_kiss_rcv+0x0/0x800 [ax25]
      [<c02ac715>] netif_receive_skb+0x255/0x2f0
      [<c02ac5c0>] ? netif_receive_skb+0x100/0x2f0
      [<c02af05c>] process_backlog+0x7c/0xf0
      [<c02aeb0c>] net_rx_action+0x16c/0x230
      [<c02aea36>] ? net_rx_action+0x96/0x230
      [<c012bd53>] __do_softirq+0x93/0x120
      [<f8d2a68a>] ? mkiss_receive_buf+0x33a/0x3f0 [mkiss]
      [<c012be37>] do_softirq+0x57/0x60
      [<c012c265>] local_bh_enable_ip+0xa5/0xe0
      [<c0320005>] _spin_unlock_bh+0x25/0x30
      [<f8d2a68a>] mkiss_receive_buf+0x33a/0x3f0 [mkiss]
      [<c025ea37>] pty_write+0x47/0x60
      [<c025c620>] write_chan+0x1b0/0x220
      [<c0259a1c>] ? tty_write_lock+0x1c/0x50
      [<c011fec0>] ? default_wake_function+0x0/0x10
      [<c0259bea>] tty_write+0x12a/0x1c0
      [<c025c470>] ? write_chan+0x0/0x220
      [<c018bbc6>] vfs_write+0x96/0x130
      [<c0259ac0>] ? tty_write+0x0/0x1c0
      [<c018c24d>] sys_write+0x3d/0x70
      [<c0104d1e>] sysenter_past_esp+0x5f/0xa5
      =======================
    
    Since rose_route_frame() does not use rose_node_list we can safely
    remove rose_node_list_lock spin lock here and let it be free for
    rose_get_neigh().
    
    Signed-off-by: Bernard Pidoux <f6bvp@amsat.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 5998533a1a9aa7480329cca82bab577eec396679
Author: James Chapman <jchapman@katalix.com>
Date:   Sun Apr 6 23:41:18 2008 -0700

    PPPOL2TP: Make locking calls softirq-safe
    
    Upstream commit: cf3752e2d203bbbfc88d29e362e6938cef4339b3
    
    Fix locking issues in the pppol2tp driver which can cause a kernel
    crash on SMP boxes. There were two problems:-
    
    1. The driver was violating read_lock() and write_lock() scheduling
       rules because it wasn't using softirq-safe locks in softirq
       contexts. So we now consistently use the _bh variants of the lock
       functions.
    
    2. The driver was calling sk_dst_get() in pppol2tp_xmit() which was
       taking sk_dst_lock in softirq context. We now call __sk_dst_get().
    
    Signed-off-by: James Chapman <jchapman@katalix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Chris Wright <chrisw@sous-sol.org>

commit 4dee959723e2bf3a0f9343a46841cd2f0029d424
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Mon Apr 14 00:44:52 2008 -0700

    [NETFILTER]: ipt_CLUSTERIP: fix race between clusterip_config_find_get and _entry_put
    
    Consider we are putting a clusterip_config entry with the "entries"
    count == 1, and on the other CPU there's a clusterip_config_find_get
    in progress:
    
    CPU1:                                                   CPU2:
    clusterip_config_entry_put:                             clusterip_config_find_get:
    if (atomic_dec_and_test(&c->entries)) {
            /* true */
                                                            read_lock_bh(&clusterip_lock);
                                                            c = __clusterip_config_find(clusterip);
                                                            /* found - it's still in list */
                                                            ...
                                                            atomic_inc(&c->entries);
                                                            read_unlock_bh(&clusterip_lock);
    
            write_lock_bh(&clusterip_lock);
            list_del(&c->list);
            write_unlock_bh(&clusterip_lock);
            ...
            dev_put(c->dev);
    
    Oops! We have an entry returned by the clusterip_config_find_get,
    which is a) not in list b) has a stale dev pointer.
    
    The problems will happen when the CPU2 will release the entry - it
    will remove it from the list for the 2nd time, thus spoiling it, and
    will put a stale dev pointer.
    
    The fix is to make atomic_dec_and_test under the clusterip_lock.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Patrick McHardy <kaber@trash.net>

commit cf3752e2d203bbbfc88d29e362e6938cef4339b3
Author: James Chapman <jchapman@katalix.com>
Date:   Wed Mar 5 18:39:08 2008 -0800

    [PPPOL2TP]: Make locking calls softirq-safe
    
    Fix locking issues in the pppol2tp driver which can cause a kernel
    crash on SMP boxes. There were two problems:-
    
    1. The driver was violating read_lock() and write_lock() scheduling
       rules because it wasn't using softirq-safe locks in softirq
       contexts. So we now consistently use the _bh variants of the lock
       functions.
    
    2. The driver was calling sk_dst_get() in pppol2tp_xmit() which was
       taking sk_dst_lock in softirq context. We now call __sk_dst_get().
    
    Signed-off-by: James Chapman <jchapman@katalix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 4de211f1a279275c6c67d6e9b6b25513e46b0bb9
Author: Jarek Poplawski <jarkao2@gmail.com>
Date:   Mon Feb 11 21:26:43 2008 -0800

    [AX25] ax25_route: make ax25_route_lock BH safe
    
    > =================================
    > [ INFO: inconsistent lock state ]
    > 2.6.24-dg8ngn-p02 #1
    > ---------------------------------
    > inconsistent {softirq-on-W} -> {in-softirq-R} usage.
    > linuxnet/3046 [HC0[0]:SC1[2]:HE1:SE0] takes:
    >  (ax25_route_lock){--.+}, at: [<f8a0cfb7>] ax25_get_route+0x18/0xb7 [ax25]
    > {softirq-on-W} state was registered at:
    ...
    
    This lockdep report shows that ax25_route_lock is taken for reading in
    softirq context, and for writing in process context with BHs enabled.
    So, to make this safe, all write_locks in ax25_route.c are changed to
    _bh versions.
    
    Reported-by: Jann Traschewski <jann@gmx.de>,
    Signed-off-by: Jarek Poplawski <jarkao2@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit ee7c82da830ea860b1f9274f1f0cdf99f206e7c2
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:01 2008 -0800

    wait_task_stopped: simplify and fix races with SIGCONT/SIGKILL/untrace
    
    wait_task_stopped() has multiple races with SIGCONT/SIGKILL.  tasklist_lock
    does not pin the child in TASK_TRACED/TASK_STOPPED stated, almost all info
    reported (including exit_code) may be wrong.
    
    In fact, the code under write_lock_irq(tasklist_lock) is not safe.  The child
    may be PTRACE_DETACH'ed at this time by another subthread, in that case it is
    possible we are no longer its ->parent.
    
    Change wait_task_stopped() to take ->siglock before inspecting the task.  This
    guarantees that the child can't resume and (for example) clear its
    ->exit_code, so we don't need to use xchg(&p->exit_code) and re-check.  The
    only exception is ptrace_stop() which changes ->state and ->exit_code without
    ->siglock held during abort.  But this can only happen if both the tracer and
    the tracee are dying (coredump is in progress), we don't care.
    
    With this patch wait_task_stopped() doesn't move the child to the end of
    the ->parent list on success.  This optimization could be restored, but
    in that case we have to take write_lock(tasklist) and do some nasty
    checks.
    
    Also change the do_wait() since we don't return EAGAIN any longer.
    
    [akpm@linux-foundation.org: fix up after Willy renamed everything]
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 02502f6224ba7735182a83e2a6b4cd5e69278d6d
Author: Patrick McHardy <kaber@trash.net>
Date:   Thu Jan 31 04:43:06 2008 -0800

    [NETFILTER]: nf_nat: switch rwlock to spinlock
    
    Since we're using RCU, all users of nf_nat_lock take a write_lock.
    Switch it to a spinlock.
    
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit f8ba1affa18398610e765736153fff614309ccc8
Author: Patrick McHardy <kaber@trash.net>
Date:   Thu Jan 31 04:38:58 2008 -0800

    [NETFILTER]: nf_conntrack: switch rwlock to spinlock
    
    With the RCU conversion only write_lock usages of nf_conntrack_lock are
    left (except one read_lock that should actually use write_lock in the
    H.323 helper). Switch to a spinlock.
    
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c52fbb410b2662a7bbc5cbe5969d73c733151498
Author: Patrick McHardy <kaber@trash.net>
Date:   Thu Jan 31 04:37:36 2008 -0800

    [NETFILTER]: nf_conntrack_core: avoid taking nf_conntrack_lock in nf_conntrack_alter_reply
    
    The conntrack is unconfirmed, so we have an exclusive reference, which
    means that the write_lock is definitely unneeded. A read_lock used to
    be needed for the helper lookup, but since we're using RCU for helpers
    now rcu_read_lock is enough.
    
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 86e8dfc5603ed76917eed0a9dd9e85a1e1a8b162
Author: Martin Peschke <mp3@de.ibm.com>
Date:   Thu Nov 15 13:57:17 2007 +0100

    [SCSI] zfcp: fix cleanup of dismissed error recovery actions
    
    Calling zfcp_erp_strategy_check_action() after zfcp_erp_action_to_running()
    in zfcp_erp_strategy() might cause an unbalanced up() for erp_ready_sem,
    which makes the zfcp recovery fail somewhere along the way:
    
    erp thread processing erp_action:
    |
    |       someone waking up erp thread for erp_action
    |       |
    |       |               someone else dismissing erp_action:
    |       |               |
    V       V               V
    
            write_lock_irqsave(&adapter->erp_lock, flags);
            ...
            if (zfcp_erp_action_exists(erp_action) == ZFCP_ERP_ACTION_RUNNING) {
                    zfcp_erp_action_to_ready(erp_action);
                    up(&adapter->erp_ready_sem);    /* first up() for erp_action */
            }
            write_unlock_irqrestore(&adapter->erp_lock, flags);
    
    write_lock_irqsave(&adapter->erp_lock, flags);
    ...
    zfcp_erp_action_to_running(erp_action);
    write_unlock_restore(&adapter->erp_lock, flags);
    /* processing erp_action */
    
                            write_lock_irqsave(&adapter->erp_lock, flags);
                            ...
                            erp_action->status |= ZFCP_STATUS_ERP_DISMISSED;
                            if (zfcp_erp_action_exists(erp_action) ==
                                                    ZFCP_ERP_ACTION_RUNNING) {
                                    zfcp_erp_action_to_ready(erp_action);
                                    up(&adapter->erp_ready_sem);
                                    /* second, unbalanced up() for erp_action */
                            }
                            ...
                            write_unlock_restore(&adapter->erp_lock, flags);
    
    write_lock_irqsave(&adapter->erp_lock, flags);
    if (erp_action->status & ZFCP_STATUS_ERP_DISMISSED) {
            zfcp_erp_action_dequeue(erp_action);
            retval = ZFCP_ERP_DISMISSED;
    }
    ...
    write_unlock_restore(&adapter->erp_lock, flags);
    down(&adapter->erp_ready_sem);
    /* this down() is meant to balance the first up() */
    
    The erp thread must not dismiss an erp_action after moving that action to
    erp_running_head. Instead it should just go through the down() operation,
    which balances the first up(), and run through zfcp_erp_strategy one more
    time for the second up(), which eventually cleans up erp_action. Which
    is similar to the normal processing of an event for erp_action doing
    something asynchronously (e.g. waiting for the completion of an fsf_req).
    
    This only works if we make sure that a dismissed erp_action is passed to
    zfcp_erp_strategy() prior to the other action, which caused actions to be
    dismissed. Therefore the patch implements this rule: running actions go to
    the head of the ready list; new actions go to the tail of the ready list;
    the erp thread picks actions to be processed from the ready list's head.
    
    Signed-off-by: Martin Peschke <mp3@de.ibm.com>
    Acked-by: Swen Schillig <swen@vnet.ibm.com>
    Signed-off-by: James Bottomley <James.Bottomley@HansenPartnership.com>

commit 2ba6064c00a38885e8997059908f9aed5299e196
Author: Ranko Zivojnovic <ranko@spidernet.net>
Date:   Fri Nov 2 22:51:48 2007 +0100

    [NET]: gen_estimator deadlock fix
    
    -Fixes ABBA deadlock noted by Patrick McHardy <kaber@trash.net>:
    
    > There is at least one ABBA deadlock, est_timer() does:
    > read_lock(&est_lock)
    > spin_lock(e->stats_lock) (which is dev->queue_lock)
    >
    > and qdisc_destroy calls htb_destroy under dev->queue_lock, which
    > calls htb_destroy_class, then gen_kill_estimator and this
    > write_locks est_lock.
    
    To fix the ABBA deadlock the rate estimators are now kept on an rcu list.
    
    -The est_lock changes the use from protecting the list to protecting
    the update to the 'bstat' pointer in order to avoid NULL dereferencing.
    
    -The 'interval' member of the gen_estimator structure removed as it is
    not needed.
    
    Signed-off-by: Ranko Zivojnovic <ranko@spidernet.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Adrian Bunk <bunk@kernel.org>

commit 885be03b069131d242506f0f717d38659b2bdb6c
Author: Robin Getz <robin.getz@analog.com>
Date:   Mon Oct 29 17:20:41 2007 +0800

    Blackfin arch: fix bug: kernel prints out error message twice
    
    This fixes two things:
     - stop calling write_lock_irq/write_unlock_irq which can turn modify
       irq levels
     - don't calling mmput when handing exceptions - since this might_sleep,
       which does a rti, and leaves us in kernel space (irq15, rather
       than irq5).
    
    Signed-off-by: Robin Getz <robin.getz@analog.com>
    Signed-off-by: Bryan Wu <bryan.wu@analog.com>

commit f7b4cddcc5aca03e80e357360c9424dfba5056c2
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Oct 16 23:30:56 2007 -0700

    do CPU_DEAD migrating under read_lock(tasklist) instead of write_lock_irq(tasklist)
    
    Currently move_task_off_dead_cpu() is called under
    write_lock_irq(tasklist).  This means it can't use task_lock() which is
    needed to improve migrating to take task's ->cpuset into account.
    
    Change the code to call move_task_off_dead_cpu() with irqs enabled, and
    change migrate_live_tasks() to use read_lock(tasklist).
    
    This all is a preparation for the futher changes proposed by Cliff Wickman, see
            http://marc.info/?t=117327786100003
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Cliff Wickman <cpw@sgi.com>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 715015e8da37c4d13e234def054bcbea116297e9
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Oct 16 23:27:00 2007 -0700

    wait_task_stopped/continued: remove unneeded p->signal != NULL check
    
    The child was found on ->children list under tasklist_lock, it must have a
    valid ->signal. __exit_signal() both removes the task from parent->children
    and clears ->signal "atomically" under write_lock(tasklist).
    
    Remove unneeded checks.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 442a10cf9e1c350b4de4dd6f22c72618a0b13d7f
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Oct 16 23:26:59 2007 -0700

    wait_task_zombie: don't fight with non-existing race with a dying ptracee
    
    The "p->exit_signal == -1 && p->ptrace == 0" check and the comment are
    bogus.  We already did exactly the same check in eligible_child(), we did
    not drop tasklist_lock since then, and both variables need
    write_lock(tasklist) to be changed.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 7eb95156d9dce2f59794264db336ce007d71638b
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Mon Oct 15 02:31:52 2007 -0700

    [INET]: Collect frag queues management objects together
    
    There are some objects that are common in all the places
    which are used to keep track of frag queues, they are:
    
     * hash table
     * LRU list
     * rw lock
     * rnd number for hash function
     * the number of queues
     * the amount of memory occupied by queues
     * secret timer
    
    Move all this stuff into one structure (struct inet_frags)
    to make it possible use them uniformly in the future. Like
    with the previous patch this mostly consists of hunks like
    
    -    write_lock(&ipfrag_lock);
    +    write_lock(&ip4_frags.lock);
    
    To address the issue with exporting the number of queues and
    the amount of memory occupied by queues outside the .c file
    they are declared in, I introduce a couple of helpers.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 90f1c19a9fd2a943adc69d2b9b8c83bcc4bba6f9
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Sep 10 20:02:27 2007 -0400

    Btrfs: [PATCH] extent_map: fix locking for bio completion
    
    The bio completion handlers can be run in any context, e.g. when using
    the old ide driver they run in hardirq context with irqs disabled so
    lockdep rightfully warns about using write_lock_irq useage in these
    handlers.
    
    This patch switches clear_extent_bit and set_extent_bit to
    write_lock_irqsave to fix this problem.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

commit 66f2a2e33796cb356ae01887db245a3ccb4d6692
Author: Ranko Zivojnovic <ranko@spidernet.net>
Date:   Wed Jul 18 02:49:48 2007 -0700

    [PATCH] gen estimator deadlock fix
    
    [NET]: gen_estimator deadlock fix
    
    -Fixes ABBA deadlock noted by Patrick McHardy <kaber@trash.net>:
    
    > There is at least one ABBA deadlock, est_timer() does:
    > read_lock(&est_lock)
    > spin_lock(e->stats_lock) (which is dev->queue_lock)
    >
    > and qdisc_destroy calls htb_destroy under dev->queue_lock, which
    > calls htb_destroy_class, then gen_kill_estimator and this
    > write_locks est_lock.
    
    To fix the ABBA deadlock the rate estimators are now kept on an rcu list.
    
    -The est_lock changes the use from protecting the list to protecting
    the update to the 'bstat' pointer in order to avoid NULL dereferencing.
    
    -The 'interval' member of the gen_estimator structure removed as it is
    not needed.
    
    Signed-off-by: Ranko Zivojnovic <ranko@spidernet.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>
    Signed-off-by: Willy Tarreau <w@1wt.eu>

commit 8a1c1646795c03edc0c4f18d3ad97e18e56f888c
Author: Ranko Zivojnovic <ranko@spidernet.net>
Date:   Wed Jul 18 02:49:48 2007 -0700

    gen estimator deadlock fix
    
    [NET]: gen_estimator deadlock fix
    
    -Fixes ABBA deadlock noted by Patrick McHardy <kaber@trash.net>:
    
    > There is at least one ABBA deadlock, est_timer() does:
    > read_lock(&est_lock)
    > spin_lock(e->stats_lock) (which is dev->queue_lock)
    >
    > and qdisc_destroy calls htb_destroy under dev->queue_lock, which
    > calls htb_destroy_class, then gen_kill_estimator and this
    > write_locks est_lock.
    
    To fix the ABBA deadlock the rate estimators are now kept on an rcu list.
    
    -The est_lock changes the use from protecting the list to protecting
    the update to the 'bstat' pointer in order to avoid NULL dereferencing.
    
    -The 'interval' member of the gen_estimator structure removed as it is
    not needed.
    
    Signed-off-by: Ranko Zivojnovic <ranko@spidernet.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 0929c2dd83317813425b937fbc0041013b8685ff
Author: Ranko Zivojnovic <ranko@spidernet.net>
Date:   Mon Jul 16 18:28:32 2007 -0700

    [NET]: gen_estimator deadlock fix
    
    -Fixes ABBA deadlock noted by Patrick McHardy <kaber@trash.net>:
    
    > There is at least one ABBA deadlock, est_timer() does:
    > read_lock(&est_lock)
    > spin_lock(e->stats_lock) (which is dev->queue_lock)
    >
    > and qdisc_destroy calls htb_destroy under dev->queue_lock, which
    > calls htb_destroy_class, then gen_kill_estimator and this
    > write_locks est_lock.
    
    To fix the ABBA deadlock the rate estimators are now kept on an rcu list.
    
    -The est_lock changes the use from protecting the list to protecting
    the update to the 'bstat' pointer in order to avoid NULL dereferencing.
    
    -The 'interval' member of the gen_estimator structure removed as it is
    not needed.
    
    Signed-off-by: Ranko Zivojnovic <ranko@spidernet.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c2aef333c98b41eeb0f0d55b7faa7d4625a6160b
Author: Paul Menage <menage@google.com>
Date:   Sun Jul 15 23:40:11 2007 -0700

    Reduce cpuset.c write_lock_irq() to read_lock()
    
    cpuset.c:update_nodemask() uses a write_lock_irq() on tasklist_lock to
    block concurrent forks; a read_lock() suffices and is less intrusive.
    
    Signed-off-by: Paul Menage<menage@google.com>
    Acked-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit add3f2fa7a6cec16d35a95b9078e1ecc80a6f332
Author: Ursula Braun <braunu@de.ibm.com>
Date:   Wed Jun 20 12:58:02 2007 +0200

    s390: avoid inconsistent lock state in qeth
    
    ipv6_regen_rndid in net/ipv6/addrconf.c makes use of "write_lock_bh"
    for its inet6_dev->lock. It may run in softirq-context.
    qeth makes use of "read_lock" for the same inet6_dev->lock.
    To avoid a potential deadlock situation, qeth should make use of
    "read_lock_bh" for its usages of inet6_dev->lock.
    
    Signed-off-by: Ursula Braun <braunu@de.ibm.com>
    Signed-off-by: Frank Pavlic <fpavlic@de.ibm.com>
    Signed-off-by: Jeff Garzik <jeff@garzik.org>

commit ee527cd3a20c2aeaac17d939e5d011f7a76d69f5
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Tue May 8 00:25:08 2007 -0700

    Use stop_machine_run in the Intel RNG driver
    
    Replace call_smp_function with stop_machine_run in the Intel RNG driver.
    
    CPU A has done read_lock(&lock)
    CPU B has done write_lock_irq(&lock) and is waiting for A to release the lock.
    
    A third CPU calls call_smp_function and issues the IPI.  CPU A takes CPU
    C's IPI.  CPU B is waiting with interrupts disabled and does not see the
    IPI.  CPU C is stuck waiting for CPU B to respond to the IPI.
    
    Deadlock.
    
    The solution is to use stop_machine_run instead of call_smp_function
    (call_smp_function should not be called in situations where the CPUs may be
    suspended).
    
    [haruo.tomita@toshiba.co.jp: fix a typo in mod_init()]
    [haruo.tomita@toshiba.co.jp: fix memory leak]
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Cc: Jan Beulich <jbeulich@novell.com>
    Cc: "Tomita, Haruo" <haruo.tomita@toshiba.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 9575499dfebc0f0fbbf122223f02e9e92630661d
Author: Helge Deller <deller@gmx.de>
Date:   Fri Mar 16 00:59:29 2007 -0400

    Input: HIL - fix rwlock recursion bug
    
    The following bug happens when insmoding hp_sdc_mlc.ko:
    
        HP SDC MLC: Registering the System Domain Controller's HIL MLC.
        BUG: rwlock recursion on CPU#0, hotplug/1814, 00854734
        Backtrace:
         [<10267560>] _raw_write_lock+0x50/0x88
         [<10104008>] _write_lock_irqsave+0x14/0x24
         [<008537d4>] hp_sdc_mlc_out+0x38/0x25c [hp_sdc_mlc]
         [<0084ebd8>] hilse_donode+0x308/0x470 [hil_mlc]
         [<0084ed80>] hil_mlcs_process+0x40/0x6c [hil_mlc]
         [<10130f80>] tasklet_action+0x78/0xb8
         [<10130cec>] __do_softirq+0x60/0xcc
         [<1010428c>] __lock_text_end+0x38/0x48
         [<10108348>] do_cpu_irq_mask+0xf0/0x11c
         [<1010b068>] intr_return+0x0/0xc
    
    Signed-off-by: Helge Deller <deller@gmx.de>
    Signed-off-by: Dmitry Torokhov <dtor@mail.ru>

commit 83d285a27720a4927ad1ca8e12b035ddcf1b5e38
Author: Patrick McHardy <kaber@trash.net>
Date:   Thu Jan 4 00:38:10 2007 +0100

    NET_SCHED: Fix fallout from dev->qdisc RCU change
    
    The move of qdisc destruction to a rcu callback broke locking in the
    entire qdisc layer by invalidating previously valid assumptions about
    the context in which changes to the qdisc tree occur.
    
    The two assumptions were:
    
    - since changes only happen in process context, read_lock doesn't need
      bottem half protection. Now invalid since destruction of inner qdiscs,
      classifiers, actions and estimators happens in the RCU callback unless
      they're manually deleted, resulting in dead-locks when read_lock in
      process context is interrupted by write_lock_bh in bottem half context.
    
    - since changes only happen under the RTNL, no additional locking is
      necessary for data not used during packet processing (f.e. u32_list).
      Again, since destruction now happens in the RCU callback, this assumption
      is not valid anymore, causing races while using this data, which can
      result in corruption or use-after-free.
    
    Instead of "fixing" this by disabling bottem halfs everywhere and adding
    new locks/refcounting, this patch makes these assumptions valid again by
    moving destruction back to process context. Since only the dev->qdisc
    pointer is protected by RCU, but ->enqueue and the qdisc tree are still
    protected by dev->qdisc_lock, destruction of the tree can be performed
    immediately and only the final free needs to happen in the rcu callback
    to make sure dev_queue_xmit doesn't access already freed memory.
    
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

commit 2a9d7dbf7566a36221b4066e57f4c3212bff1ef0
Author: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
Date:   Thu Dec 14 22:28:51 2006 +0100

    [IPV4]: severe locking bug in fib_semantics.c
    
    Found in 2.4 by Yixin Pan <yxpan@hotmail.com>.
    
    > When I read fib_semantics.c of Linux-2.4.32, write_lock(&fib_info_lock) =
    > is used in fib_release_info() instead of write_lock_bh(&fib_info_lock).  =
    > Is the following case possible: a BH interrupts fib_release_info() while =
    > holding the write lock, and calls ip_check_fib_default() which calls =
    > read_lock(&fib_info_lock), and spin forever.
    
    Signed-off-by: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

commit de6c0ccfa9ab24a9104c8791030e8ebecb1e2c5a
Author: Andrew Morton <akpm@osdl.org>
Date:   Mon Nov 20 00:11:42 2006 +0100

    disable debugging version of write_lock()
    
    We've confirmed that the debug version of write_lock() can get stuck for long
    enough to cause NMI watchdog timeouts and hence a crash.
    
    We don't know why, yet.   Disable it for now.
    
    Also disable the similar read_lock() code.  Just in case.
    
    Thanks to Dave Olson <olson@unixfolk.com> for reporting and testing.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

commit 5f804c752ba4e4e85457bf0e4da9f8521720328b
Author: Patrick McHardy <kaber@trash.net>
Date:   Mon Oct 9 21:16:11 2006 -0700

    NET_SCHED: Fix fallout from dev->qdisc RCU change
    
    The move of qdisc destruction to a rcu callback broke locking in the
    entire qdisc layer by invalidating previously valid assumptions about
    the context in which changes to the qdisc tree occur.
    
    The two assumptions were:
    
    - since changes only happen in process context, read_lock doesn't need
      bottem half protection. Now invalid since destruction of inner qdiscs,
      classifiers, actions and estimators happens in the RCU callback unless
      they're manually deleted, resulting in dead-locks when read_lock in
      process context is interrupted by write_lock_bh in bottem half context.
    
    - since changes only happen under the RTNL, no additional locking is
      necessary for data not used during packet processing (f.e. u32_list).
      Again, since destruction now happens in the RCU callback, this assumption
      is not valid anymore, causing races while using this data, which can
      result in corruption or use-after-free.
    
    Instead of "fixing" this by disabling bottem halfs everywhere and adding
    new locks/refcounting, this patch makes these assumptions valid again by
    moving destruction back to process context. Since only the dev->qdisc
    pointer is protected by RCU, but ->enqueue and the qdisc tree are still
    protected by dev->qdisc_lock, destruction of the tree can be performed
    immediately and only the final free needs to happen in the rcu callback
    to make sure dev_queue_xmit doesn't access already freed memory.
    
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 5f412b24240d92212e50ebbaff2dff20c9e6f3d0
Author: Alan Cox <alan@lxorguk.ukuu.org.uk>
Date:   Fri Sep 29 02:01:40 2006 -0700

    [PATCH] Fix locking for tty drivers when doing urgent characters
    
    If you send a priority character (as is done for flow control) then the tty
    driver can either have its own method for "jumping the queue" or the characrer
    can be queued normally.  In the latter case we call the write method but
    without the atomic_write_lock taken elsewhere.
    
    Make this consistent.  Note that the send_xchar method if implemented remains
    outside of the lock as it can jump ahead of a current write so must not be
    locked out by it.
    
    Signed-off-by: Alan Cox <alan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 85670cc1faa2e1472e4a423cbf0b5e3d55c5ba88
Author: Patrick McHardy <kaber@trash.net>
Date:   Wed Sep 27 16:45:45 2006 -0700

    [NET_SCHED]: Fix fallout from dev->qdisc RCU change
    
    The move of qdisc destruction to a rcu callback broke locking in the
    entire qdisc layer by invalidating previously valid assumptions about
    the context in which changes to the qdisc tree occur.
    
    The two assumptions were:
    
    - since changes only happen in process context, read_lock doesn't need
      bottem half protection. Now invalid since destruction of inner qdiscs,
      classifiers, actions and estimators happens in the RCU callback unless
      they're manually deleted, resulting in dead-locks when read_lock in
      process context is interrupted by write_lock_bh in bottem half context.
    
    - since changes only happen under the RTNL, no additional locking is
      necessary for data not used during packet processing (f.e. u32_list).
      Again, since destruction now happens in the RCU callback, this assumption
      is not valid anymore, causing races while using this data, which can
      result in corruption or use-after-free.
    
    Instead of "fixing" this by disabling bottem halfs everywhere and adding
    new locks/refcounting, this patch makes these assumptions valid again by
    moving destruction back to process context. Since only the dev->qdisc
    pointer is protected by RCU, but ->enqueue and the qdisc tree are still
    protected by dev->qdisc_lock, destruction of the tree can be performed
    immediately and only the final free needs to happen in the rcu callback
    to make sure dev_queue_xmit doesn't access already freed memory.
    
    Signed-off-by: Patrick McHardy <kaber@trash.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 24264434603cc102d71fb2a1b3b7e282a781f449
Author: Steven Whitehouse <swhiteho@redhat.com>
Date:   Mon Sep 11 21:40:30 2006 -0400

    [GFS2] Rewrite of examine_bucket()
    
    The existing implementation of this function in glock.c was not
    very efficient as it relied upon keeping a cursor element upon the
    hash chain in question and moving it along. This new version improves
    upon this by using the current element as a cursor. This is possible
    since we only look at the "next" element in the list after we've
    taken the read_lock() subsequent to calling the examiner function.
    Obviously we have to eventually drop the ref count that we are then
    left with and we cannot do that while holding the read_lock, so we
    do that next time we drop the lock. That means either just before
    we examine another glock, or when the loop has terminated.
    
    The new implementation has several advantages: it uses only a
    read_lock() rather than a write_lock(), so it can run simnultaneously
    with other code, it doesn't need a "plug" element, so that it removes
    a test not only from this list iterator, but from all the other glock
    list iterators too. So it makes things faster and smaller.
    
    Signed-off-by: Steven Whitehouse <swhiteho@redhat.com>

commit 22db37ec5fd51b0c77b1dd5751b1cdc2672c08d6
Author: Chris Wright <chrisw@sous-sol.org>
Date:   Thu Aug 31 00:53:22 2006 -0700

    [PATCH] i386: rwlock.h fix smp alternatives fix
    
    Commit 8c74932779fc6f61b4c30145863a17125c1a296c ("i386: Remove
    alternative_smp") did not actually compile on x86 with CONFIG_SMP.
    
    This fixes the __build_read/write_lock helpers.  I've boot tested on
    SMP.
    
    [ Andi: "Oops, I think that was a quilt unrefreshed patch.  Sorry.  I
      fixed those before testing, but then still send out the old patch." ]
    
    Signed-off-by: Chris Wright <chrisw@sous-sol.org>
    Cc: Gerd Hoffmann <kraxel@suse.de>
    Acked-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 67ce628f410668c5ddee2f4c7424a1bd8a8fc64b
Author: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
Date:   Thu Aug 17 22:57:22 2006 -0700

    Fix ipv4 routing locking bug
    
    [IPV4]: severe locking bug in fib_semantics.c
    
    Found in 2.4 by Yixin Pan <yxpan@hotmail.com>.
    
    > When I read fib_semantics.c of Linux-2.4.32, write_lock(&fib_info_lock) =
    > is used in fib_release_info() instead of write_lock_bh(&fib_info_lock).  =
    > Is the following case possible: a BH interrupts fib_release_info() while =
    > holding the write lock, and calls ip_check_fib_default() which calls =
    > read_lock(&fib_info_lock), and spin forever.
    
    Signed-off-by: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit a80b26d5597e62094c165c26e77fde4d4ab2e37e
Author: Andrew Morton <akpm@osdl.org>
Date:   Sat Aug 5 12:13:47 2006 -0700

    disable debugging version of write_lock()
    
    We've confirmed that the debug version of write_lock() can get stuck for long
    enough to cause NMI watchdog timeouts and hence a crash.
    
    We don't know why, yet.   Disable it for now.
    
    Also disable the similar read_lock() code.  Just in case.
    
    Thanks to Dave Olson <olson@unixfolk.com> for reporting and testing.
    
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 6e8fcbf64024f9056ba122abbb66554aa76bae5d
Author: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
Date:   Thu Aug 17 16:44:46 2006 -0700

    [IPV4]: severe locking bug in fib_semantics.c
    
    Found in 2.4 by Yixin Pan <yxpan@hotmail.com>.
    
    > When I read fib_semantics.c of Linux-2.4.32, write_lock(&fib_info_lock) =
    > is used in fib_release_info() instead of write_lock_bh(&fib_info_lock).  =
    > Is the following case possible: a BH interrupts fib_release_info() while =
    > holding the write lock, and calls ip_check_fib_default() which calls =
    > read_lock(&fib_info_lock), and spin forever.
    
    Signed-off-by: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 72f0b4e2133ba1d65147d06016c0b6d2202235ca
Author: Andrew Morton <akpm@osdl.org>
Date:   Sat Aug 5 12:13:47 2006 -0700

    [PATCH] disable debugging version of write_lock()
    
    We've confirmed that the debug version of write_lock() can get stuck for long
    enough to cause NMI watchdog timeouts and hence a crash.
    
    We don't know why, yet.   Disable it for now.
    
    Also disable the similar read_lock() code.  Just in case.
    
    Thanks to Dave Olson <olson@unixfolk.com> for reporting and testing.
    
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 9f09c548e185b63a3567498b96783f897b5f0399
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Jul 3 17:32:20 2006 -0700

    [PATCH] zfcp: fix incorrect usage of erp_lock
    
      =================================
      [ INFO: inconsistent lock state ]
      ---------------------------------
      inconsistent {hardirq-on-W} -> {in-hardirq-W} usage.
      swapper/0 [HC1[1]:SC0[0]:HE0:SE1] takes:
       (&adapter->erp_lock){+-..}, at: [<000000000026c7f8>] zfcp_erp_async_handler+0x3c/0x70
      {hardirq-on-W} state was registered at:
        [<000000000005f33e>] __lock_acquire+0x30a/0xed0
        [<00000000000604ae>] lock_acquire+0x9a/0xc8
        [<000000000035a7ae>] _write_lock+0x4e/0x68
        [<000000000026d822>] zfcp_erp_adapter_strategy_generic+0x286/0xd94
        [<000000000026fd72>] zfcp_erp_strategy_do_action+0x91e/0x1a94
        [<0000000000271a3a>] zfcp_erp_thread+0x21a/0x1568
        [<0000000000019096>] kernel_thread_starter+0x6/0xc
        [<0000000000019090>] kernel_thread_starter+0x0/0xc
      irq event stamp: 12078
      hardirqs last  enabled at (12077): [<0000000000019416>] cpu_idle+0x206/0x250
      hardirqs last disabled at (12078): [<0000000000020458>] io_no_vtime+0xc/0x1c
      softirqs last  enabled at (12072): [<0000000000040b62>] __do_softirq+0x13a/0x180
      softirqs last disabled at (12059): [<000000000001fd58>] do_softirq+0xec/0xf0
    
      other info that might help us debug this:
      no locks held by swapper/0.
    
      stack backtrace:
      00000000012bb648 0000000000000002 0000000000000000 00000000012bb758
             00000000012bb6c0 0000000000399122 0000000000399122 0000000000016b0a
             0000000000000000 0000000000000001 0000000000000000 00000000004660e8
             0000000000000000 000000000000000d 00000000012bb6b8 00000000012bb730
             0000000000368b90 0000000000016b0a 00000000012bb6b8 00000000012bb708
      Call Trace:
      ([<0000000000016a26>] show_trace+0x76/0xdc)
       [<0000000000016b2c>] show_stack+0xa0/0xd0
       [<0000000000016b8a>] dump_stack+0x2e/0x3c
       [<000000000005e3da>] print_usage_bug+0x27e/0x290
       [<000000000005e934>] mark_lock+0x548/0x6c0
       [<000000000005fb0c>] __lock_acquire+0xad8/0xed0
       [<00000000000604ae>] lock_acquire+0x9a/0xc8
       [<000000000035a662>] _write_lock_irqsave+0x62/0x80
       [<000000000026c7f8>] zfcp_erp_async_handler+0x3c/0x70
       [<0000000000279178>] zfcp_fsf_req_dispatch+0xd8/0x1fa8
       [<000000000027e538>] zfcp_fsf_req_complete+0x104/0xe4c
       [<0000000000274534>] zfcp_qdio_reqid_check+0xf4/0x178
       [<000000000027469e>] zfcp_qdio_response_handler+0xe6/0x430
       [<0000000000219dd4>] tiqdio_thinint_handler+0xd20/0x213c
       [<000000000020229a>] do_adapter_IO+0xb2/0xc0
       [<0000000000206f32>] do_IRQ+0x136/0x16c
       [<0000000000020462>] io_no_vtime+0x16/0x1c
       [<0000000000019432>] cpu_idle+0x222/0x250
      ([<0000000000019416>] cpu_idle+0x206/0x250)
       [<000000000001405a>] rest_init+0x5a/0x68
       [<0000000000536998>] start_kernel+0x39c/0x3dc
       [<0000000000013046>] _stext+0x46/0x1000
    
    Fix incorrect usage of erp_lock. Using the write_lock() variant is wrong,
    since this might lead to deadlocks.
    
    Acked-by: Andreas Herrmann <aherrman@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 8dff7c29707b7514043539f5ab5e0a6eb7bd9dcd
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Apr 28 15:23:59 2006 -0700

    [XFRM]: fix softirq-unsafe xfrm typemap->lock use
    
    xfrm typemap->lock may be used in softirq context, so all write_lock()
    uses must be softirq-safe.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 1434261c07bcebd5ef8b8a18f919fdee533b84e0
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:10:59 2006 -0800

    [PATCH] simplify exec from init's subthread
    
    I think it is enough to take tasklist_lock for reading while changing
    child_reaper:
    
            Reparenting needs write_lock(tasklist_lock)
    
            Only one thread in a thread group can do exec()
    
            sighand->siglock garantees that get_signal_to_deliver()
            will not see a stale value of child_reaper.
    
    This means that we can change child_reaper earlier, without calling
    zap_other_threads() twice.
    
    "child_reaper = current" is a NOOP when init does exec from main thread, we
    don't care.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 6f4b6ec1cffcbb12cc47244381496d59b6a5a790
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Thu Feb 2 17:01:13 2006 -0800

    [IPV6]: Fix illegal dst locking in softirq context.
    
    On Tue, Jan 31, 2006 at 10:24:32PM +0100, Ingo Molnar wrote:
    >
    >  [<c04de9e8>] _write_lock+0x8/0x10
    >  [<c0499015>] inet6_destroy_sock+0x25/0x100
    >  [<c04b8672>] tcp_v6_destroy_sock+0x12/0x20
    >  [<c046bbda>] inet_csk_destroy_sock+0x4a/0x150
    >  [<c047625c>] tcp_rcv_state_process+0xd4c/0xdd0
    >  [<c047d8e9>] tcp_v4_do_rcv+0xa9/0x340
    >  [<c047eabb>] tcp_v4_rcv+0x8eb/0x9d0
    
    OK this is definitely broken.  We should never touch the dst lock in
    softirq context.  Since inet6_destroy_sock may be called from that
    context due to the asynchronous nature of sockets, we can't take the
    lock there.
    
    In fact this sk_dst_reset is totally redundant since all IPv6 sockets
    use inet_sock_destruct as their socket destructor which always cleans
    up the dst anyway.  So the solution is to simply remove the call.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e5c34a57c8b3a94b8d2b329936f8b1cbcc765307
Author: Ben Collins <ben.collins@ubuntu.com>
Date:   Fri Dec 23 09:10:03 2005 -0500

    [PATCH] Fix typo in x86_64 __build_write_lock_const assembly
    
    Based on __build_read_lock_const, this looked like a bug.
    
    [ Indeed. Maybe nobody uses this version? Worth fixing up anyway ]
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit c13cf856cbe16aec3007604dc013cbf3a16c6686
Author: Andrew Morton <akpm@osdl.org>
Date:   Mon Nov 28 13:43:48 2005 -0800

    [PATCH] fork.c: proc_fork_connector() called under write_lock()
    
    Don't do that - it does GFP_KERNEL allocations, for a start.
    
    (Reported by Guillaume Thouvenin <guillaume.thouvenin@bull.net>)
    
    Acked-by: Matt Helsley <matthltc@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit eb92f4ef320b738e41ad43476a5d05c8a20d5cc7
Author: Rik Van Riel <riel@redhat.com>
Date:   Sat Oct 29 18:15:44 2005 -0700

    [PATCH] add sem_is_read/write_locked()
    
    Add sem_is_read/write_locked functions to the read/write semaphores, along the
    same lines of the *_is_locked spinlock functions.  The swap token tuning patch
    uses sem_is_read_locked; sem_is_write_locked is added for completeness.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 47d6b08334a43fafa61a587f721fa21ef65d81be
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Mon Oct 17 18:49:42 2005 +0400

    [PATCH] posix-timers: fix task accounting
    
    Make sure we release the task struct properly when releasing pending
    timers.
    
    release_task() does write_lock_irq(&tasklist_lock), so it can't race
    with run_posix_cpu_timers() on any cpu.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 99e56fc6ee51c66c0d248c6dea7a45d70e15604e
Author: Alexander Nyberg <alexn@telia.com>
Date:   Wed Sep 14 18:54:06 2005 +0200

    [PATCH] Fix fs/exec.c:788 (de_thread()) BUG_ON
    
    It turns out that the BUG_ON() in fs/exec.c: de_thread() is unreliable
    and can trigger due to the test itself being racy.
    
    de_thread() does
            while (atomic_read(&sig->count) > count) {
            }
            .....
            .....
            BUG_ON(!thread_group_empty(current));
    
    but release_task does
            write_lock_irq(&tasklist_lock)
            __exit_signal
                    (this is where atomic_dec(&sig->count) is run)
            __exit_sighand
            __unhash_process
                    takes write lock on tasklist_lock
                    remove itself out of PIDTYPE_TGID list
            write_unlock_irq(&tasklist_lock)
    
    so there's a clear (although small) window between the
    atomic_dec(&sig->count) and the actual PIDTYPE_TGID unhashing of the
    thread.
    
    And actually there is no need for all threads to have exited at this
    point, so we simply kill the BUG_ON.
    
    Big thanks to Marc Lehmann who provided the test-case.
    
    Fixes Bug 5170 (http://bugme.osdl.org/show_bug.cgi?id=5170)
    
    Signed-off-by: Alexander Nyberg <alexn@telia.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Andrew Morton <akpm@osdl.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
    Signed-off-by: Chris Wright <chrisw@osdl.org>

commit fb085cf1d4294824571815d487daccc0609543f0
Author: Alexander Nyberg <alexn@telia.com>
Date:   Wed Sep 14 18:54:06 2005 +0200

    [PATCH] Fix fs/exec.c:788 (de_thread()) BUG_ON
    
    It turns out that the BUG_ON() in fs/exec.c: de_thread() is unreliable
    and can trigger due to the test itself being racy.
    
    de_thread() does
            while (atomic_read(&sig->count) > count) {
            }
            .....
            .....
            BUG_ON(!thread_group_empty(current));
    
    but release_task does
            write_lock_irq(&tasklist_lock)
            __exit_signal
                    (this is where atomic_dec(&sig->count) is run)
            __exit_sighand
            __unhash_process
                    takes write lock on tasklist_lock
                    remove itself out of PIDTYPE_TGID list
            write_unlock_irq(&tasklist_lock)
    
    so there's a clear (although small) window between the
    atomic_dec(&sig->count) and the actual PIDTYPE_TGID unhashing of the
    thread.
    
    And actually there is no need for all threads to have exited at this
    point, so we simply kill the BUG_ON.
    
    Big thanks to Marc Lehmann who provided the test-case.
    
    Fixes Bug 5170 (http://bugme.osdl.org/show_bug.cgi?id=5170)
    
    Signed-off-by: Alexander Nyberg <alexn@telia.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Andrew Morton <akpm@osdl.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 0f36163d3abefbda1b21a330b3fdf3c2dc076d94
Author: Thomas Sailer <sailer@ife.ee.ethz.ch>
Date:   Fri Sep 9 10:43:50 2005 +0200

    [PATCH] usb: fix uss720 schedule with interrupts off
    
    This patch fixes the long standing schedule with interrupts off problem
    of the uss720 driver. The problem is caused by the parport layer calling
    the save and restore methods within a write_lock_irqsave guarded region.
    The fix is to issue the control transaction requests required by save
    and restore asynchronously.
    
    Signed-off-by: Thomas Sailer, <sailer@ife.ee.ethz.ch>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit f521089158cd48a81b4d72e8e39da006dd79779b
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Aug 5 08:02:00 2005 -0700

    [IA64] Spinlock optimizations
    
    1. Nontemporal store for spin unlock.
    
    A nontemporal store will not update the LRU setting for the cacheline. The
    cacheline with the lock may therefore be evicted faster from the cpu
    caches. Doing so may be useful since it increases the chance that the
    exclusive cache line has been evicted when another cpu is trying to
    acquire the lock.
    
    The time between dropping and reacquiring a lock on the same cpu is
    typically very small so the danger of the cacheline being
    evicted is negligible.
    
    2. Avoid semaphore operation in write_unlock and use nontemporal store
    
    write_lock uses a cmpxchg like the regular spin_lock but write_unlock uses
    clear_bit which requires a load and then a loop over a cmpxchg. The
    following patch makes write_unlock simply use a nontemporal store to clear
    the highest 8 bits. We will then still have the lower 3 bytes (24 bits)
    left to count the readers.
    
    Doing the byte store will reduce the number of possible readers from 2^31
    to 2^24 = 16 million.
    
    These patches were discussed already:
    
    http://marc.theaimsgroup.com/?t=111472054400001&r=1&w=2
    http://marc.theaimsgroup.com/?l=linux-ia64&m=111401837707849&w=2
    
    The nontemporal stores will only work using GCC. If a compiler is used
    that does not support inline asm then fallback C code is used. This
    will preserve the byte store but not be able to do the nontemporal stores.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>

commit fb3d89498d268c8dedc1ab5b15fa64f536564577
Author: Neil Horman <nhorman@redhat.com>
Date:   Tue Jun 28 15:40:02 2005 -0700

    [IPVS]: Close race conditions on ip_vs_conn_tab list modification
    
    In an smp system, it is possible for an connection timer to expire, calling
    ip_vs_conn_expire while the connection table is being flushed, before
    ct_write_lock_bh is acquired.
    
    Since the list iterator loop in ip_vs_con_flush releases and re-acquires the
    spinlock (even though it doesn't re-enable softirqs), it is possible for the
    expiration function to modify the connection list, while it is being traversed
    in ip_vs_conn_flush.
    
    The result is that the next pointer gets set to NULL, and subsequently
    dereferenced, resulting in an oops.
    
    Signed-off-by: Neil Horman <nhorman@redhat.com>
    Acked-by: JulianAnastasov
    Signed-off-by: David S. Miller <davem@davemloft.net>
