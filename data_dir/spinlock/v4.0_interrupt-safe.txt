commit 3f6c02fa712bd453871877fe1d1969625617471e
Author: Marek Vasut <marex@denx.de>
Date:   Fri Jan 20 17:03:32 2023 +0100

    serial: stm32: Merge hard IRQ and threaded IRQ handling into single IRQ handler
    
    Requesting an interrupt with IRQF_ONESHOT will run the primary handler
    in the hard-IRQ context even in the force-threaded mode. The
    force-threaded mode is used by PREEMPT_RT in order to avoid acquiring
    sleeping locks (spinlock_t) in hard-IRQ context. This combination
    makes it impossible and leads to "sleeping while atomic" warnings.
    
    Use one interrupt handler for both handlers (primary and secondary)
    and drop the IRQF_ONESHOT flag which is not needed.
    
    Fixes: e359b4411c283 ("serial: stm32: fix threaded interrupt handling")
    Reviewed-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Tested-by: Valentin Caron <valentin.caron@foss.st.com> # V3
    Signed-off-by: Marek Vasut <marex@denx.de>
    Cc: stable@vger.kernel.org
    Link: https://lore.kernel.org/r/20230120160332.57930-1-marex@denx.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2b5412ca6fd33523a138131abdd8ddf873af913e
Author: Marek Vasut <marex@denx.de>
Date:   Thu Jan 12 19:04:17 2023 +0100

    serial: stm32: Merge hard IRQ and threaded IRQ handling into single IRQ handler
    
    commit f24771b62a83239f0dce816bddf0f6807f436235 upstream.
    
    Requesting an interrupt with IRQF_ONESHOT will run the primary handler
    in the hard-IRQ context even in the force-threaded mode. The
    force-threaded mode is used by PREEMPT_RT in order to avoid acquiring
    sleeping locks (spinlock_t) in hard-IRQ context. This combination
    makes it impossible and leads to "sleeping while atomic" warnings.
    
    Use one interrupt handler for both handlers (primary and secondary)
    and drop the IRQF_ONESHOT flag which is not needed.
    
    Fixes: e359b4411c283 ("serial: stm32: fix threaded interrupt handling")
    Reviewed-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Tested-by: Valentin Caron <valentin.caron@foss.st.com> # V3
    Signed-off-by: Marek Vasut <marex@denx.de>
    Cc: stable@vger.kernel.org
    Link: https://lore.kernel.org/r/20230112180417.25595-1-marex@denx.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f24771b62a83239f0dce816bddf0f6807f436235
Author: Marek Vasut <marex@denx.de>
Date:   Thu Jan 12 19:04:17 2023 +0100

    serial: stm32: Merge hard IRQ and threaded IRQ handling into single IRQ handler
    
    Requesting an interrupt with IRQF_ONESHOT will run the primary handler
    in the hard-IRQ context even in the force-threaded mode. The
    force-threaded mode is used by PREEMPT_RT in order to avoid acquiring
    sleeping locks (spinlock_t) in hard-IRQ context. This combination
    makes it impossible and leads to "sleeping while atomic" warnings.
    
    Use one interrupt handler for both handlers (primary and secondary)
    and drop the IRQF_ONESHOT flag which is not needed.
    
    Fixes: e359b4411c283 ("serial: stm32: fix threaded interrupt handling")
    Reviewed-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Tested-by: Valentin Caron <valentin.caron@foss.st.com> # V3
    Signed-off-by: Marek Vasut <marex@denx.de>
    Cc: stable@vger.kernel.org
    Link: https://lore.kernel.org/r/20230112180417.25595-1-marex@denx.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 97e14ae082040b2a65c7cbf7f2a53c240cc805db
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Dec 2 10:02:23 2022 +0000

    rtmutex: Add acquire semantics for rtmutex lock acquisition slow path
    
    commit 1c0908d8e441631f5b8ba433523cf39339ee2ba0 upstream.
    
    Jan Kara reported the following bug triggering on 6.0.5-rt14 running dbench
    on XFS on arm64.
    
     kernel BUG at fs/inode.c:625!
     Internal error: Oops - BUG: 0 [#1] PREEMPT_RT SMP
     CPU: 11 PID: 6611 Comm: dbench Tainted: G            E   6.0.0-rt14-rt+ #1
     pc : clear_inode+0xa0/0xc0
     lr : clear_inode+0x38/0xc0
     Call trace:
      clear_inode+0xa0/0xc0
      evict+0x160/0x180
      iput+0x154/0x240
      do_unlinkat+0x184/0x300
      __arm64_sys_unlinkat+0x48/0xc0
      el0_svc_common.constprop.4+0xe4/0x2c0
      do_el0_svc+0xac/0x100
      el0_svc+0x78/0x200
      el0t_64_sync_handler+0x9c/0xc0
      el0t_64_sync+0x19c/0x1a0
    
    It also affects 6.1-rc7-rt5 and affects a preempt-rt fork of 5.14 so this
    is likely a bug that existed forever and only became visible when ARM
    support was added to preempt-rt. The same problem does not occur on x86-64
    and he also reported that converting sb->s_inode_wblist_lock to
    raw_spinlock_t makes the problem disappear indicating that the RT spinlock
    variant is the problem.
    
    Which in turn means that RT mutexes on ARM64 and any other weakly ordered
    architecture are affected by this independent of RT.
    
    Will Deacon observed:
    
      "I'd be more inclined to be suspicious of the slowpath tbh, as we need to
       make sure that we have acquire semantics on all paths where the lock can
       be taken. Looking at the rtmutex code, this really isn't obvious to me
       -- for example, try_to_take_rt_mutex() appears to be able to return via
       the 'takeit' label without acquire semantics and it looks like we might
       be relying on the caller's subsequent _unlock_ of the wait_lock for
       ordering, but that will give us release semantics which aren't correct."
    
    Sebastian Andrzej Siewior prototyped a fix that does work based on that
    comment but it was a little bit overkill and added some fences that should
    not be necessary.
    
    The lock owner is updated with an IRQ-safe raw spinlock held, but the
    spin_unlock does not provide acquire semantics which are needed when
    acquiring a mutex.
    
    Adds the necessary acquire semantics for lock owner updates in the slow path
    acquisition and the waiter bit logic.
    
    It successfully completed 10 iterations of the dbench workload while the
    vanilla kernel fails on the first iteration.
    
    [ bigeasy@linutronix.de: Initial prototype fix ]
    
    Fixes: 700318d1d7b38 ("locking/rtmutex: Use acquire/release semantics")
    Fixes: 23f78d4a03c5 ("[PATCH] pi-futex: rt mutex core")
    Reported-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: https://lore.kernel.org/r/20221202100223.6mevpbl7i6x5udfd@techsingularity.net
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 693e7e52a89a1de572bfef47ffae3644096df2f2
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Dec 2 10:02:23 2022 +0000

    rtmutex: Add acquire semantics for rtmutex lock acquisition slow path
    
    commit 1c0908d8e441631f5b8ba433523cf39339ee2ba0 upstream.
    
    Jan Kara reported the following bug triggering on 6.0.5-rt14 running dbench
    on XFS on arm64.
    
     kernel BUG at fs/inode.c:625!
     Internal error: Oops - BUG: 0 [#1] PREEMPT_RT SMP
     CPU: 11 PID: 6611 Comm: dbench Tainted: G            E   6.0.0-rt14-rt+ #1
     pc : clear_inode+0xa0/0xc0
     lr : clear_inode+0x38/0xc0
     Call trace:
      clear_inode+0xa0/0xc0
      evict+0x160/0x180
      iput+0x154/0x240
      do_unlinkat+0x184/0x300
      __arm64_sys_unlinkat+0x48/0xc0
      el0_svc_common.constprop.4+0xe4/0x2c0
      do_el0_svc+0xac/0x100
      el0_svc+0x78/0x200
      el0t_64_sync_handler+0x9c/0xc0
      el0t_64_sync+0x19c/0x1a0
    
    It also affects 6.1-rc7-rt5 and affects a preempt-rt fork of 5.14 so this
    is likely a bug that existed forever and only became visible when ARM
    support was added to preempt-rt. The same problem does not occur on x86-64
    and he also reported that converting sb->s_inode_wblist_lock to
    raw_spinlock_t makes the problem disappear indicating that the RT spinlock
    variant is the problem.
    
    Which in turn means that RT mutexes on ARM64 and any other weakly ordered
    architecture are affected by this independent of RT.
    
    Will Deacon observed:
    
      "I'd be more inclined to be suspicious of the slowpath tbh, as we need to
       make sure that we have acquire semantics on all paths where the lock can
       be taken. Looking at the rtmutex code, this really isn't obvious to me
       -- for example, try_to_take_rt_mutex() appears to be able to return via
       the 'takeit' label without acquire semantics and it looks like we might
       be relying on the caller's subsequent _unlock_ of the wait_lock for
       ordering, but that will give us release semantics which aren't correct."
    
    Sebastian Andrzej Siewior prototyped a fix that does work based on that
    comment but it was a little bit overkill and added some fences that should
    not be necessary.
    
    The lock owner is updated with an IRQ-safe raw spinlock held, but the
    spin_unlock does not provide acquire semantics which are needed when
    acquiring a mutex.
    
    Adds the necessary acquire semantics for lock owner updates in the slow path
    acquisition and the waiter bit logic.
    
    It successfully completed 10 iterations of the dbench workload while the
    vanilla kernel fails on the first iteration.
    
    [ bigeasy@linutronix.de: Initial prototype fix ]
    
    Fixes: 700318d1d7b38 ("locking/rtmutex: Use acquire/release semantics")
    Fixes: 23f78d4a03c5 ("[PATCH] pi-futex: rt mutex core")
    Reported-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: https://lore.kernel.org/r/20221202100223.6mevpbl7i6x5udfd@techsingularity.net
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1c0908d8e441631f5b8ba433523cf39339ee2ba0
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Dec 2 10:02:23 2022 +0000

    rtmutex: Add acquire semantics for rtmutex lock acquisition slow path
    
    Jan Kara reported the following bug triggering on 6.0.5-rt14 running dbench
    on XFS on arm64.
    
     kernel BUG at fs/inode.c:625!
     Internal error: Oops - BUG: 0 [#1] PREEMPT_RT SMP
     CPU: 11 PID: 6611 Comm: dbench Tainted: G            E   6.0.0-rt14-rt+ #1
     pc : clear_inode+0xa0/0xc0
     lr : clear_inode+0x38/0xc0
     Call trace:
      clear_inode+0xa0/0xc0
      evict+0x160/0x180
      iput+0x154/0x240
      do_unlinkat+0x184/0x300
      __arm64_sys_unlinkat+0x48/0xc0
      el0_svc_common.constprop.4+0xe4/0x2c0
      do_el0_svc+0xac/0x100
      el0_svc+0x78/0x200
      el0t_64_sync_handler+0x9c/0xc0
      el0t_64_sync+0x19c/0x1a0
    
    It also affects 6.1-rc7-rt5 and affects a preempt-rt fork of 5.14 so this
    is likely a bug that existed forever and only became visible when ARM
    support was added to preempt-rt. The same problem does not occur on x86-64
    and he also reported that converting sb->s_inode_wblist_lock to
    raw_spinlock_t makes the problem disappear indicating that the RT spinlock
    variant is the problem.
    
    Which in turn means that RT mutexes on ARM64 and any other weakly ordered
    architecture are affected by this independent of RT.
    
    Will Deacon observed:
    
      "I'd be more inclined to be suspicious of the slowpath tbh, as we need to
       make sure that we have acquire semantics on all paths where the lock can
       be taken. Looking at the rtmutex code, this really isn't obvious to me
       -- for example, try_to_take_rt_mutex() appears to be able to return via
       the 'takeit' label without acquire semantics and it looks like we might
       be relying on the caller's subsequent _unlock_ of the wait_lock for
       ordering, but that will give us release semantics which aren't correct."
    
    Sebastian Andrzej Siewior prototyped a fix that does work based on that
    comment but it was a little bit overkill and added some fences that should
    not be necessary.
    
    The lock owner is updated with an IRQ-safe raw spinlock held, but the
    spin_unlock does not provide acquire semantics which are needed when
    acquiring a mutex.
    
    Adds the necessary acquire semantics for lock owner updates in the slow path
    acquisition and the waiter bit logic.
    
    It successfully completed 10 iterations of the dbench workload while the
    vanilla kernel fails on the first iteration.
    
    [ bigeasy@linutronix.de: Initial prototype fix ]
    
    Fixes: 700318d1d7b38 ("locking/rtmutex: Use acquire/release semantics")
    Fixes: 23f78d4a03c5 ("[PATCH] pi-futex: rt mutex core")
    Reported-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: https://lore.kernel.org/r/20221202100223.6mevpbl7i6x5udfd@techsingularity.net

commit cfa72993d13302fe958a1a58234a1a8efa72a5b8
Merge: fe8e3f44c587 be66e67f1750
Author: Marc Zyngier <maz@kernel.org>
Date:   Mon Dec 5 14:30:49 2022 +0000

    Merge branch kvm-arm64/pkvm-vcpu-state into kvmarm-master/next
    
    * kvm-arm64/pkvm-vcpu-state: (25 commits)
      : .
      : Large drop of pKVM patches from Will Deacon and co, adding
      : a private vm/vcpu state at EL2, managed independently from
      : the EL1 state. From the cover letter:
      :
      : "This is version six of the pKVM EL2 state series, extending the pKVM
      : hypervisor code so that it can dynamically instantiate and manage VM
      : data structures without the host being able to access them directly.
      : These structures consist of a hyp VM, a set of hyp vCPUs and the stage-2
      : page-table for the MMU. The pages used to hold the hypervisor structures
      : are returned to the host when the VM is destroyed."
      : .
      KVM: arm64: Use the pKVM hyp vCPU structure in handle___kvm_vcpu_run()
      KVM: arm64: Don't unnecessarily map host kernel sections at EL2
      KVM: arm64: Explicitly map 'kvm_vgic_global_state' at EL2
      KVM: arm64: Maintain a copy of 'kvm_arm_vmid_bits' at EL2
      KVM: arm64: Unmap 'kvm_arm_hyp_percpu_base' from the host
      KVM: arm64: Return guest memory from EL2 via dedicated teardown memcache
      KVM: arm64: Instantiate guest stage-2 page-tables at EL2
      KVM: arm64: Consolidate stage-2 initialisation into a single function
      KVM: arm64: Add generic hyp_memcache helpers
      KVM: arm64: Provide I-cache invalidation by virtual address at EL2
      KVM: arm64: Initialise hypervisor copies of host symbols unconditionally
      KVM: arm64: Add per-cpu fixmap infrastructure at EL2
      KVM: arm64: Instantiate pKVM hypervisor VM and vCPU structures from EL1
      KVM: arm64: Add infrastructure to create and track pKVM instances at EL2
      KVM: arm64: Rename 'host_kvm' to 'host_mmu'
      KVM: arm64: Add hyp_spinlock_t static initializer
      KVM: arm64: Include asm/kvm_mmu.h in nvhe/mem_protect.h
      KVM: arm64: Add helpers to pin memory shared with the hypervisor at EL2
      KVM: arm64: Prevent the donation of no-map pages
      KVM: arm64: Implement do_donate() helper for donating memory
      ...
    
    Signed-off-by: Marc Zyngier <maz@kernel.org>

commit 14d3eb66e16a55d279598c8ed7ae1ca85066ff5b
Merge: 4b28ba9eeab4 b539ce9f1a31
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Thu Nov 10 09:48:37 2022 +0100

    Merge branch 'slab/for-6.2/locking' into slab/for-next
    
    A patch from Jiri Kosina that makes SLAB's list_lock a raw_spinlock_t.
    While there are no plans to make SLAB actually compatible with
    PREEMPT_RT or any other future, it makes !PREEMPT_RT lockdep happy.

commit 1c80002e3264552d8b9c0e162e09aa4087403716
Author: Fuad Tabba <tabba@google.com>
Date:   Thu Nov 10 19:02:43 2022 +0000

    KVM: arm64: Add hyp_spinlock_t static initializer
    
    Introduce a static initializer macro for 'hyp_spinlock_t' so that it is
    straightforward to instantiate global locks at EL2. This will be later
    utilised for locking the VM table in the hypervisor.
    
    Reviewed-by: Philippe Mathieu-Daudé <philmd@linaro.org>
    Tested-by: Vincent Donnefort <vdonnefort@google.com>
    Signed-off-by: Fuad Tabba <tabba@google.com>
    Signed-off-by: Will Deacon <will@kernel.org>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20221110190259.26861-11-will@kernel.org

commit 21d65b35169112af9b6f873c8eeab972e60107c2
Author: James Smart <jsmart2021@gmail.com>
Date:   Thu Aug 18 18:17:32 2022 -0700

    scsi: lpfc: Rework MIB Rx Monitor debug info logic
    
    [ Upstream commit bd269188ea94e40ab002cad7b0df8f12b8f0de54 ]
    
    The kernel test robot reported the following sparse warning:
    
    arch/arm64/include/asm/cmpxchg.h:88:1: sparse: sparse: cast truncates
       bits from constant value (369 becomes 69)
    
    On arm64, atomic_xchg only works on 8-bit byte fields.  Thus, the macro
    usage of LPFC_RXMONITOR_TABLE_IN_USE can be unintentionally truncated
    leading to all logic involving the LPFC_RXMONITOR_TABLE_IN_USE macro to not
    work properly.
    
    Replace the Rx Table atomic_t indexing logic with a new
    lpfc_rx_info_monitor structure that holds a circular ring buffer.  For
    locking semantics, a spinlock_t is used.
    
    Link: https://lore.kernel.org/r/20220819011736.14141-4-jsmart2021@gmail.com
    Fixes: 17b27ac59224 ("scsi: lpfc: Add rx monitoring statistics")
    Cc: <stable@vger.kernel.org> # v5.15+
    Co-developed-by: Justin Tee <justin.tee@broadcom.com>
    Signed-off-by: Justin Tee <justin.tee@broadcom.com>
    Signed-off-by: James Smart <jsmart2021@gmail.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit fc08f8438172a036308e0b9a52b67e6f33870100
Author: Waiman Long <longman@redhat.com>
Date:   Thu Sep 22 10:56:22 2022 -0400

    tracing: Disable interrupt or preemption before acquiring arch_spinlock_t
    
    commit c0a581d7126c0bbc96163276f585fd7b4e4d8d0e upstream.
    
    It was found that some tracing functions in kernel/trace/trace.c acquire
    an arch_spinlock_t with preemption and irqs enabled. An example is the
    tracing_saved_cmdlines_size_read() function which intermittently causes
    a "BUG: using smp_processor_id() in preemptible" warning when the LTP
    read_all_proc test is run.
    
    That can be problematic in case preemption happens after acquiring the
    lock. Add the necessary preemption or interrupt disabling code in the
    appropriate places before acquiring an arch_spinlock_t.
    
    The convention here is to disable preemption for trace_cmdline_lock and
    interupt for max_lock.
    
    Link: https://lkml.kernel.org/r/20220922145622.1744826-1-longman@redhat.com
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: stable@vger.kernel.org
    Fixes: a35873a0993b ("tracing: Add conditional snapshot")
    Fixes: 939c7a4f04fc ("tracing: Introduce saved_cmdlines_size file")
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Steven Rostedt (Google) <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 29d0c45cf16e5be1b33561b3d3184dbd56e45586
Author: Waiman Long <longman@redhat.com>
Date:   Thu Sep 22 10:56:22 2022 -0400

    tracing: Disable interrupt or preemption before acquiring arch_spinlock_t
    
    commit c0a581d7126c0bbc96163276f585fd7b4e4d8d0e upstream.
    
    It was found that some tracing functions in kernel/trace/trace.c acquire
    an arch_spinlock_t with preemption and irqs enabled. An example is the
    tracing_saved_cmdlines_size_read() function which intermittently causes
    a "BUG: using smp_processor_id() in preemptible" warning when the LTP
    read_all_proc test is run.
    
    That can be problematic in case preemption happens after acquiring the
    lock. Add the necessary preemption or interrupt disabling code in the
    appropriate places before acquiring an arch_spinlock_t.
    
    The convention here is to disable preemption for trace_cmdline_lock and
    interupt for max_lock.
    
    Link: https://lkml.kernel.org/r/20220922145622.1744826-1-longman@redhat.com
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: stable@vger.kernel.org
    Fixes: a35873a0993b ("tracing: Add conditional snapshot")
    Fixes: 939c7a4f04fc ("tracing: Introduce saved_cmdlines_size file")
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Steven Rostedt (Google) <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 48272aa48d80a1ed877bc4398594def6c8e7920f
Author: Waiman Long <longman@redhat.com>
Date:   Thu Sep 22 10:56:22 2022 -0400

    tracing: Disable interrupt or preemption before acquiring arch_spinlock_t
    
    commit c0a581d7126c0bbc96163276f585fd7b4e4d8d0e upstream.
    
    It was found that some tracing functions in kernel/trace/trace.c acquire
    an arch_spinlock_t with preemption and irqs enabled. An example is the
    tracing_saved_cmdlines_size_read() function which intermittently causes
    a "BUG: using smp_processor_id() in preemptible" warning when the LTP
    read_all_proc test is run.
    
    That can be problematic in case preemption happens after acquiring the
    lock. Add the necessary preemption or interrupt disabling code in the
    appropriate places before acquiring an arch_spinlock_t.
    
    The convention here is to disable preemption for trace_cmdline_lock and
    interupt for max_lock.
    
    Link: https://lkml.kernel.org/r/20220922145622.1744826-1-longman@redhat.com
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: stable@vger.kernel.org
    Fixes: a35873a0993b ("tracing: Add conditional snapshot")
    Fixes: 939c7a4f04fc ("tracing: Introduce saved_cmdlines_size file")
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Steven Rostedt (Google) <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 220d170455aa849ad0c45e5190dfe4698c167502
Author: Waiman Long <longman@redhat.com>
Date:   Thu Sep 22 10:56:22 2022 -0400

    tracing: Disable interrupt or preemption before acquiring arch_spinlock_t
    
    commit c0a581d7126c0bbc96163276f585fd7b4e4d8d0e upstream.
    
    It was found that some tracing functions in kernel/trace/trace.c acquire
    an arch_spinlock_t with preemption and irqs enabled. An example is the
    tracing_saved_cmdlines_size_read() function which intermittently causes
    a "BUG: using smp_processor_id() in preemptible" warning when the LTP
    read_all_proc test is run.
    
    That can be problematic in case preemption happens after acquiring the
    lock. Add the necessary preemption or interrupt disabling code in the
    appropriate places before acquiring an arch_spinlock_t.
    
    The convention here is to disable preemption for trace_cmdline_lock and
    interupt for max_lock.
    
    Link: https://lkml.kernel.org/r/20220922145622.1744826-1-longman@redhat.com
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: stable@vger.kernel.org
    Fixes: a35873a0993b ("tracing: Add conditional snapshot")
    Fixes: 939c7a4f04fc ("tracing: Introduce saved_cmdlines_size file")
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Steven Rostedt (Google) <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 147d397e08a406f5997f8a1c7f747fe546bf8395
Author: James Smart <jsmart2021@gmail.com>
Date:   Thu Aug 18 18:17:32 2022 -0700

    scsi: lpfc: Rework MIB Rx Monitor debug info logic
    
    commit bd269188ea94e40ab002cad7b0df8f12b8f0de54 upstream.
    
    The kernel test robot reported the following sparse warning:
    
    arch/arm64/include/asm/cmpxchg.h:88:1: sparse: sparse: cast truncates
       bits from constant value (369 becomes 69)
    
    On arm64, atomic_xchg only works on 8-bit byte fields.  Thus, the macro
    usage of LPFC_RXMONITOR_TABLE_IN_USE can be unintentionally truncated
    leading to all logic involving the LPFC_RXMONITOR_TABLE_IN_USE macro to not
    work properly.
    
    Replace the Rx Table atomic_t indexing logic with a new
    lpfc_rx_info_monitor structure that holds a circular ring buffer.  For
    locking semantics, a spinlock_t is used.
    
    Link: https://lore.kernel.org/r/20220819011736.14141-4-jsmart2021@gmail.com
    Fixes: 17b27ac59224 ("scsi: lpfc: Add rx monitoring statistics")
    Cc: <stable@vger.kernel.org> # v5.15+
    Co-developed-by: Justin Tee <justin.tee@broadcom.com>
    Signed-off-by: Justin Tee <justin.tee@broadcom.com>
    Signed-off-by: James Smart <jsmart2021@gmail.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit be969931c877a8847ae57b15a0052e3df8b99521
Author: Waiman Long <longman@redhat.com>
Date:   Thu Sep 22 10:56:22 2022 -0400

    tracing: Disable interrupt or preemption before acquiring arch_spinlock_t
    
    commit c0a581d7126c0bbc96163276f585fd7b4e4d8d0e upstream.
    
    It was found that some tracing functions in kernel/trace/trace.c acquire
    an arch_spinlock_t with preemption and irqs enabled. An example is the
    tracing_saved_cmdlines_size_read() function which intermittently causes
    a "BUG: using smp_processor_id() in preemptible" warning when the LTP
    read_all_proc test is run.
    
    That can be problematic in case preemption happens after acquiring the
    lock. Add the necessary preemption or interrupt disabling code in the
    appropriate places before acquiring an arch_spinlock_t.
    
    The convention here is to disable preemption for trace_cmdline_lock and
    interupt for max_lock.
    
    Link: https://lkml.kernel.org/r/20220922145622.1744826-1-longman@redhat.com
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: stable@vger.kernel.org
    Fixes: a35873a0993b ("tracing: Add conditional snapshot")
    Fixes: 939c7a4f04fc ("tracing: Introduce saved_cmdlines_size file")
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Steven Rostedt (Google) <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2c9b5b8326b953f2f48338a7c889e6af457d146f
Author: James Smart <jsmart2021@gmail.com>
Date:   Thu Aug 18 18:17:32 2022 -0700

    scsi: lpfc: Rework MIB Rx Monitor debug info logic
    
    commit bd269188ea94e40ab002cad7b0df8f12b8f0de54 upstream.
    
    The kernel test robot reported the following sparse warning:
    
    arch/arm64/include/asm/cmpxchg.h:88:1: sparse: sparse: cast truncates
       bits from constant value (369 becomes 69)
    
    On arm64, atomic_xchg only works on 8-bit byte fields.  Thus, the macro
    usage of LPFC_RXMONITOR_TABLE_IN_USE can be unintentionally truncated
    leading to all logic involving the LPFC_RXMONITOR_TABLE_IN_USE macro to not
    work properly.
    
    Replace the Rx Table atomic_t indexing logic with a new
    lpfc_rx_info_monitor structure that holds a circular ring buffer.  For
    locking semantics, a spinlock_t is used.
    
    Link: https://lore.kernel.org/r/20220819011736.14141-4-jsmart2021@gmail.com
    Fixes: 17b27ac59224 ("scsi: lpfc: Add rx monitoring statistics")
    Cc: <stable@vger.kernel.org> # v5.15+
    Co-developed-by: Justin Tee <justin.tee@broadcom.com>
    Signed-off-by: Justin Tee <justin.tee@broadcom.com>
    Signed-off-by: James Smart <jsmart2021@gmail.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c7c9c7eb305ab8b4e93e4e4e1b78d8cfcbc26323
Author: Vladimir Oltean <vladimir.oltean@nxp.com>
Date:   Thu Sep 15 13:08:01 2022 +0300

    net/sched: taprio: avoid disabling offload when it was never enabled
    
    [ Upstream commit db46e3a88a09c5cf7e505664d01da7238cd56c92 ]
    
    In an incredibly strange API design decision, qdisc->destroy() gets
    called even if qdisc->init() never succeeded, not exclusively since
    commit 87b60cfacf9f ("net_sched: fix error recovery at qdisc creation"),
    but apparently also earlier (in the case of qdisc_create_dflt()).
    
    The taprio qdisc does not fully acknowledge this when it attempts full
    offload, because it starts off with q->flags = TAPRIO_FLAGS_INVALID in
    taprio_init(), then it replaces q->flags with TCA_TAPRIO_ATTR_FLAGS
    parsed from netlink (in taprio_change(), tail called from taprio_init()).
    
    But in taprio_destroy(), we call taprio_disable_offload(), and this
    determines what to do based on FULL_OFFLOAD_IS_ENABLED(q->flags).
    
    But looking at the implementation of FULL_OFFLOAD_IS_ENABLED()
    (a bitwise check of bit 1 in q->flags), it is invalid to call this macro
    on q->flags when it contains TAPRIO_FLAGS_INVALID, because that is set
    to U32_MAX, and therefore FULL_OFFLOAD_IS_ENABLED() will return true on
    an invalid set of flags.
    
    As a result, it is possible to crash the kernel if user space forces an
    error between setting q->flags = TAPRIO_FLAGS_INVALID, and the calling
    of taprio_enable_offload(). This is because drivers do not expect the
    offload to be disabled when it was never enabled.
    
    The error that we force here is to attach taprio as a non-root qdisc,
    but instead as child of an mqprio root qdisc:
    
    $ tc qdisc add dev swp0 root handle 1: \
            mqprio num_tc 8 map 0 1 2 3 4 5 6 7 \
            queues 1@0 1@1 1@2 1@3 1@4 1@5 1@6 1@7 hw 0
    $ tc qdisc replace dev swp0 parent 1:1 \
            taprio num_tc 8 map 0 1 2 3 4 5 6 7 \
            queues 1@0 1@1 1@2 1@3 1@4 1@5 1@6 1@7 base-time 0 \
            sched-entry S 0x7f 990000 sched-entry S 0x80 100000 \
            flags 0x0 clockid CLOCK_TAI
    Unable to handle kernel paging request at virtual address fffffffffffffff8
    [fffffffffffffff8] pgd=0000000000000000, p4d=0000000000000000
    Internal error: Oops: 96000004 [#1] PREEMPT SMP
    Call trace:
     taprio_dump+0x27c/0x310
     vsc9959_port_setup_tc+0x1f4/0x460
     felix_port_setup_tc+0x24/0x3c
     dsa_slave_setup_tc+0x54/0x27c
     taprio_disable_offload.isra.0+0x58/0xe0
     taprio_destroy+0x80/0x104
     qdisc_create+0x240/0x470
     tc_modify_qdisc+0x1fc/0x6b0
     rtnetlink_rcv_msg+0x12c/0x390
     netlink_rcv_skb+0x5c/0x130
     rtnetlink_rcv+0x1c/0x2c
    
    Fix this by keeping track of the operations we made, and undo the
    offload only if we actually did it.
    
    I've added "bool offloaded" inside a 4 byte hole between "int clockid"
    and "atomic64_t picos_per_byte". Now the first cache line looks like
    below:
    
    $ pahole -C taprio_sched net/sched/sch_taprio.o
    struct taprio_sched {
            struct Qdisc * *           qdiscs;               /*     0     8 */
            struct Qdisc *             root;                 /*     8     8 */
            u32                        flags;                /*    16     4 */
            enum tk_offsets            tk_offset;            /*    20     4 */
            int                        clockid;              /*    24     4 */
            bool                       offloaded;            /*    28     1 */
    
            /* XXX 3 bytes hole, try to pack */
    
            atomic64_t                 picos_per_byte;       /*    32     0 */
    
            /* XXX 8 bytes hole, try to pack */
    
            spinlock_t                 current_entry_lock;   /*    40     0 */
    
            /* XXX 8 bytes hole, try to pack */
    
            struct sched_entry *       current_entry;        /*    48     8 */
            struct sched_gate_list *   oper_sched;           /*    56     8 */
            /* --- cacheline 1 boundary (64 bytes) --- */
    
    Fixes: 9c66d1564676 ("taprio: Add support for hardware offloading")
    Signed-off-by: Vladimir Oltean <vladimir.oltean@nxp.com>
    Reviewed-by: Vinicius Costa Gomes <vinicius.gomes@intel.com>
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f58e43184226e5e9662088ccf1389e424a3a4cbd
Author: Vladimir Oltean <vladimir.oltean@nxp.com>
Date:   Thu Sep 15 13:08:01 2022 +0300

    net/sched: taprio: avoid disabling offload when it was never enabled
    
    [ Upstream commit db46e3a88a09c5cf7e505664d01da7238cd56c92 ]
    
    In an incredibly strange API design decision, qdisc->destroy() gets
    called even if qdisc->init() never succeeded, not exclusively since
    commit 87b60cfacf9f ("net_sched: fix error recovery at qdisc creation"),
    but apparently also earlier (in the case of qdisc_create_dflt()).
    
    The taprio qdisc does not fully acknowledge this when it attempts full
    offload, because it starts off with q->flags = TAPRIO_FLAGS_INVALID in
    taprio_init(), then it replaces q->flags with TCA_TAPRIO_ATTR_FLAGS
    parsed from netlink (in taprio_change(), tail called from taprio_init()).
    
    But in taprio_destroy(), we call taprio_disable_offload(), and this
    determines what to do based on FULL_OFFLOAD_IS_ENABLED(q->flags).
    
    But looking at the implementation of FULL_OFFLOAD_IS_ENABLED()
    (a bitwise check of bit 1 in q->flags), it is invalid to call this macro
    on q->flags when it contains TAPRIO_FLAGS_INVALID, because that is set
    to U32_MAX, and therefore FULL_OFFLOAD_IS_ENABLED() will return true on
    an invalid set of flags.
    
    As a result, it is possible to crash the kernel if user space forces an
    error between setting q->flags = TAPRIO_FLAGS_INVALID, and the calling
    of taprio_enable_offload(). This is because drivers do not expect the
    offload to be disabled when it was never enabled.
    
    The error that we force here is to attach taprio as a non-root qdisc,
    but instead as child of an mqprio root qdisc:
    
    $ tc qdisc add dev swp0 root handle 1: \
            mqprio num_tc 8 map 0 1 2 3 4 5 6 7 \
            queues 1@0 1@1 1@2 1@3 1@4 1@5 1@6 1@7 hw 0
    $ tc qdisc replace dev swp0 parent 1:1 \
            taprio num_tc 8 map 0 1 2 3 4 5 6 7 \
            queues 1@0 1@1 1@2 1@3 1@4 1@5 1@6 1@7 base-time 0 \
            sched-entry S 0x7f 990000 sched-entry S 0x80 100000 \
            flags 0x0 clockid CLOCK_TAI
    Unable to handle kernel paging request at virtual address fffffffffffffff8
    [fffffffffffffff8] pgd=0000000000000000, p4d=0000000000000000
    Internal error: Oops: 96000004 [#1] PREEMPT SMP
    Call trace:
     taprio_dump+0x27c/0x310
     vsc9959_port_setup_tc+0x1f4/0x460
     felix_port_setup_tc+0x24/0x3c
     dsa_slave_setup_tc+0x54/0x27c
     taprio_disable_offload.isra.0+0x58/0xe0
     taprio_destroy+0x80/0x104
     qdisc_create+0x240/0x470
     tc_modify_qdisc+0x1fc/0x6b0
     rtnetlink_rcv_msg+0x12c/0x390
     netlink_rcv_skb+0x5c/0x130
     rtnetlink_rcv+0x1c/0x2c
    
    Fix this by keeping track of the operations we made, and undo the
    offload only if we actually did it.
    
    I've added "bool offloaded" inside a 4 byte hole between "int clockid"
    and "atomic64_t picos_per_byte". Now the first cache line looks like
    below:
    
    $ pahole -C taprio_sched net/sched/sch_taprio.o
    struct taprio_sched {
            struct Qdisc * *           qdiscs;               /*     0     8 */
            struct Qdisc *             root;                 /*     8     8 */
            u32                        flags;                /*    16     4 */
            enum tk_offsets            tk_offset;            /*    20     4 */
            int                        clockid;              /*    24     4 */
            bool                       offloaded;            /*    28     1 */
    
            /* XXX 3 bytes hole, try to pack */
    
            atomic64_t                 picos_per_byte;       /*    32     0 */
    
            /* XXX 8 bytes hole, try to pack */
    
            spinlock_t                 current_entry_lock;   /*    40     0 */
    
            /* XXX 8 bytes hole, try to pack */
    
            struct sched_entry *       current_entry;        /*    48     8 */
            struct sched_gate_list *   oper_sched;           /*    56     8 */
            /* --- cacheline 1 boundary (64 bytes) --- */
    
    Fixes: 9c66d1564676 ("taprio: Add support for hardware offloading")
    Signed-off-by: Vladimir Oltean <vladimir.oltean@nxp.com>
    Reviewed-by: Vinicius Costa Gomes <vinicius.gomes@intel.com>
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 586def6ebed195f3594a4884f7c5334d0e1ad1bb
Author: Vladimir Oltean <vladimir.oltean@nxp.com>
Date:   Thu Sep 15 13:08:01 2022 +0300

    net/sched: taprio: avoid disabling offload when it was never enabled
    
    [ Upstream commit db46e3a88a09c5cf7e505664d01da7238cd56c92 ]
    
    In an incredibly strange API design decision, qdisc->destroy() gets
    called even if qdisc->init() never succeeded, not exclusively since
    commit 87b60cfacf9f ("net_sched: fix error recovery at qdisc creation"),
    but apparently also earlier (in the case of qdisc_create_dflt()).
    
    The taprio qdisc does not fully acknowledge this when it attempts full
    offload, because it starts off with q->flags = TAPRIO_FLAGS_INVALID in
    taprio_init(), then it replaces q->flags with TCA_TAPRIO_ATTR_FLAGS
    parsed from netlink (in taprio_change(), tail called from taprio_init()).
    
    But in taprio_destroy(), we call taprio_disable_offload(), and this
    determines what to do based on FULL_OFFLOAD_IS_ENABLED(q->flags).
    
    But looking at the implementation of FULL_OFFLOAD_IS_ENABLED()
    (a bitwise check of bit 1 in q->flags), it is invalid to call this macro
    on q->flags when it contains TAPRIO_FLAGS_INVALID, because that is set
    to U32_MAX, and therefore FULL_OFFLOAD_IS_ENABLED() will return true on
    an invalid set of flags.
    
    As a result, it is possible to crash the kernel if user space forces an
    error between setting q->flags = TAPRIO_FLAGS_INVALID, and the calling
    of taprio_enable_offload(). This is because drivers do not expect the
    offload to be disabled when it was never enabled.
    
    The error that we force here is to attach taprio as a non-root qdisc,
    but instead as child of an mqprio root qdisc:
    
    $ tc qdisc add dev swp0 root handle 1: \
            mqprio num_tc 8 map 0 1 2 3 4 5 6 7 \
            queues 1@0 1@1 1@2 1@3 1@4 1@5 1@6 1@7 hw 0
    $ tc qdisc replace dev swp0 parent 1:1 \
            taprio num_tc 8 map 0 1 2 3 4 5 6 7 \
            queues 1@0 1@1 1@2 1@3 1@4 1@5 1@6 1@7 base-time 0 \
            sched-entry S 0x7f 990000 sched-entry S 0x80 100000 \
            flags 0x0 clockid CLOCK_TAI
    Unable to handle kernel paging request at virtual address fffffffffffffff8
    [fffffffffffffff8] pgd=0000000000000000, p4d=0000000000000000
    Internal error: Oops: 96000004 [#1] PREEMPT SMP
    Call trace:
     taprio_dump+0x27c/0x310
     vsc9959_port_setup_tc+0x1f4/0x460
     felix_port_setup_tc+0x24/0x3c
     dsa_slave_setup_tc+0x54/0x27c
     taprio_disable_offload.isra.0+0x58/0xe0
     taprio_destroy+0x80/0x104
     qdisc_create+0x240/0x470
     tc_modify_qdisc+0x1fc/0x6b0
     rtnetlink_rcv_msg+0x12c/0x390
     netlink_rcv_skb+0x5c/0x130
     rtnetlink_rcv+0x1c/0x2c
    
    Fix this by keeping track of the operations we made, and undo the
    offload only if we actually did it.
    
    I've added "bool offloaded" inside a 4 byte hole between "int clockid"
    and "atomic64_t picos_per_byte". Now the first cache line looks like
    below:
    
    $ pahole -C taprio_sched net/sched/sch_taprio.o
    struct taprio_sched {
            struct Qdisc * *           qdiscs;               /*     0     8 */
            struct Qdisc *             root;                 /*     8     8 */
            u32                        flags;                /*    16     4 */
            enum tk_offsets            tk_offset;            /*    20     4 */
            int                        clockid;              /*    24     4 */
            bool                       offloaded;            /*    28     1 */
    
            /* XXX 3 bytes hole, try to pack */
    
            atomic64_t                 picos_per_byte;       /*    32     0 */
    
            /* XXX 8 bytes hole, try to pack */
    
            spinlock_t                 current_entry_lock;   /*    40     0 */
    
            /* XXX 8 bytes hole, try to pack */
    
            struct sched_entry *       current_entry;        /*    48     8 */
            struct sched_gate_list *   oper_sched;           /*    56     8 */
            /* --- cacheline 1 boundary (64 bytes) --- */
    
    Fixes: 9c66d1564676 ("taprio: Add support for hardware offloading")
    Signed-off-by: Vladimir Oltean <vladimir.oltean@nxp.com>
    Reviewed-by: Vinicius Costa Gomes <vinicius.gomes@intel.com>
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d12a1eb07003e597077329767c6aa86a7e972c76
Author: Vladimir Oltean <vladimir.oltean@nxp.com>
Date:   Thu Sep 15 13:08:01 2022 +0300

    net/sched: taprio: avoid disabling offload when it was never enabled
    
    [ Upstream commit db46e3a88a09c5cf7e505664d01da7238cd56c92 ]
    
    In an incredibly strange API design decision, qdisc->destroy() gets
    called even if qdisc->init() never succeeded, not exclusively since
    commit 87b60cfacf9f ("net_sched: fix error recovery at qdisc creation"),
    but apparently also earlier (in the case of qdisc_create_dflt()).
    
    The taprio qdisc does not fully acknowledge this when it attempts full
    offload, because it starts off with q->flags = TAPRIO_FLAGS_INVALID in
    taprio_init(), then it replaces q->flags with TCA_TAPRIO_ATTR_FLAGS
    parsed from netlink (in taprio_change(), tail called from taprio_init()).
    
    But in taprio_destroy(), we call taprio_disable_offload(), and this
    determines what to do based on FULL_OFFLOAD_IS_ENABLED(q->flags).
    
    But looking at the implementation of FULL_OFFLOAD_IS_ENABLED()
    (a bitwise check of bit 1 in q->flags), it is invalid to call this macro
    on q->flags when it contains TAPRIO_FLAGS_INVALID, because that is set
    to U32_MAX, and therefore FULL_OFFLOAD_IS_ENABLED() will return true on
    an invalid set of flags.
    
    As a result, it is possible to crash the kernel if user space forces an
    error between setting q->flags = TAPRIO_FLAGS_INVALID, and the calling
    of taprio_enable_offload(). This is because drivers do not expect the
    offload to be disabled when it was never enabled.
    
    The error that we force here is to attach taprio as a non-root qdisc,
    but instead as child of an mqprio root qdisc:
    
    $ tc qdisc add dev swp0 root handle 1: \
            mqprio num_tc 8 map 0 1 2 3 4 5 6 7 \
            queues 1@0 1@1 1@2 1@3 1@4 1@5 1@6 1@7 hw 0
    $ tc qdisc replace dev swp0 parent 1:1 \
            taprio num_tc 8 map 0 1 2 3 4 5 6 7 \
            queues 1@0 1@1 1@2 1@3 1@4 1@5 1@6 1@7 base-time 0 \
            sched-entry S 0x7f 990000 sched-entry S 0x80 100000 \
            flags 0x0 clockid CLOCK_TAI
    Unable to handle kernel paging request at virtual address fffffffffffffff8
    [fffffffffffffff8] pgd=0000000000000000, p4d=0000000000000000
    Internal error: Oops: 96000004 [#1] PREEMPT SMP
    Call trace:
     taprio_dump+0x27c/0x310
     vsc9959_port_setup_tc+0x1f4/0x460
     felix_port_setup_tc+0x24/0x3c
     dsa_slave_setup_tc+0x54/0x27c
     taprio_disable_offload.isra.0+0x58/0xe0
     taprio_destroy+0x80/0x104
     qdisc_create+0x240/0x470
     tc_modify_qdisc+0x1fc/0x6b0
     rtnetlink_rcv_msg+0x12c/0x390
     netlink_rcv_skb+0x5c/0x130
     rtnetlink_rcv+0x1c/0x2c
    
    Fix this by keeping track of the operations we made, and undo the
    offload only if we actually did it.
    
    I've added "bool offloaded" inside a 4 byte hole between "int clockid"
    and "atomic64_t picos_per_byte". Now the first cache line looks like
    below:
    
    $ pahole -C taprio_sched net/sched/sch_taprio.o
    struct taprio_sched {
            struct Qdisc * *           qdiscs;               /*     0     8 */
            struct Qdisc *             root;                 /*     8     8 */
            u32                        flags;                /*    16     4 */
            enum tk_offsets            tk_offset;            /*    20     4 */
            int                        clockid;              /*    24     4 */
            bool                       offloaded;            /*    28     1 */
    
            /* XXX 3 bytes hole, try to pack */
    
            atomic64_t                 picos_per_byte;       /*    32     0 */
    
            /* XXX 8 bytes hole, try to pack */
    
            spinlock_t                 current_entry_lock;   /*    40     0 */
    
            /* XXX 8 bytes hole, try to pack */
    
            struct sched_entry *       current_entry;        /*    48     8 */
            struct sched_gate_list *   oper_sched;           /*    56     8 */
            /* --- cacheline 1 boundary (64 bytes) --- */
    
    Fixes: 9c66d1564676 ("taprio: Add support for hardware offloading")
    Signed-off-by: Vladimir Oltean <vladimir.oltean@nxp.com>
    Reviewed-by: Vinicius Costa Gomes <vinicius.gomes@intel.com>
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit c0a581d7126c0bbc96163276f585fd7b4e4d8d0e
Author: Waiman Long <longman@redhat.com>
Date:   Thu Sep 22 10:56:22 2022 -0400

    tracing: Disable interrupt or preemption before acquiring arch_spinlock_t
    
    It was found that some tracing functions in kernel/trace/trace.c acquire
    an arch_spinlock_t with preemption and irqs enabled. An example is the
    tracing_saved_cmdlines_size_read() function which intermittently causes
    a "BUG: using smp_processor_id() in preemptible" warning when the LTP
    read_all_proc test is run.
    
    That can be problematic in case preemption happens after acquiring the
    lock. Add the necessary preemption or interrupt disabling code in the
    appropriate places before acquiring an arch_spinlock_t.
    
    The convention here is to disable preemption for trace_cmdline_lock and
    interupt for max_lock.
    
    Link: https://lkml.kernel.org/r/20220922145622.1744826-1-longman@redhat.com
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: stable@vger.kernel.org
    Fixes: a35873a0993b ("tracing: Add conditional snapshot")
    Fixes: 939c7a4f04fc ("tracing: Introduce saved_cmdlines_size file")
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Steven Rostedt (Google) <rostedt@goodmis.org>

commit db46e3a88a09c5cf7e505664d01da7238cd56c92
Author: Vladimir Oltean <vladimir.oltean@nxp.com>
Date:   Thu Sep 15 13:08:01 2022 +0300

    net/sched: taprio: avoid disabling offload when it was never enabled
    
    In an incredibly strange API design decision, qdisc->destroy() gets
    called even if qdisc->init() never succeeded, not exclusively since
    commit 87b60cfacf9f ("net_sched: fix error recovery at qdisc creation"),
    but apparently also earlier (in the case of qdisc_create_dflt()).
    
    The taprio qdisc does not fully acknowledge this when it attempts full
    offload, because it starts off with q->flags = TAPRIO_FLAGS_INVALID in
    taprio_init(), then it replaces q->flags with TCA_TAPRIO_ATTR_FLAGS
    parsed from netlink (in taprio_change(), tail called from taprio_init()).
    
    But in taprio_destroy(), we call taprio_disable_offload(), and this
    determines what to do based on FULL_OFFLOAD_IS_ENABLED(q->flags).
    
    But looking at the implementation of FULL_OFFLOAD_IS_ENABLED()
    (a bitwise check of bit 1 in q->flags), it is invalid to call this macro
    on q->flags when it contains TAPRIO_FLAGS_INVALID, because that is set
    to U32_MAX, and therefore FULL_OFFLOAD_IS_ENABLED() will return true on
    an invalid set of flags.
    
    As a result, it is possible to crash the kernel if user space forces an
    error between setting q->flags = TAPRIO_FLAGS_INVALID, and the calling
    of taprio_enable_offload(). This is because drivers do not expect the
    offload to be disabled when it was never enabled.
    
    The error that we force here is to attach taprio as a non-root qdisc,
    but instead as child of an mqprio root qdisc:
    
    $ tc qdisc add dev swp0 root handle 1: \
            mqprio num_tc 8 map 0 1 2 3 4 5 6 7 \
            queues 1@0 1@1 1@2 1@3 1@4 1@5 1@6 1@7 hw 0
    $ tc qdisc replace dev swp0 parent 1:1 \
            taprio num_tc 8 map 0 1 2 3 4 5 6 7 \
            queues 1@0 1@1 1@2 1@3 1@4 1@5 1@6 1@7 base-time 0 \
            sched-entry S 0x7f 990000 sched-entry S 0x80 100000 \
            flags 0x0 clockid CLOCK_TAI
    Unable to handle kernel paging request at virtual address fffffffffffffff8
    [fffffffffffffff8] pgd=0000000000000000, p4d=0000000000000000
    Internal error: Oops: 96000004 [#1] PREEMPT SMP
    Call trace:
     taprio_dump+0x27c/0x310
     vsc9959_port_setup_tc+0x1f4/0x460
     felix_port_setup_tc+0x24/0x3c
     dsa_slave_setup_tc+0x54/0x27c
     taprio_disable_offload.isra.0+0x58/0xe0
     taprio_destroy+0x80/0x104
     qdisc_create+0x240/0x470
     tc_modify_qdisc+0x1fc/0x6b0
     rtnetlink_rcv_msg+0x12c/0x390
     netlink_rcv_skb+0x5c/0x130
     rtnetlink_rcv+0x1c/0x2c
    
    Fix this by keeping track of the operations we made, and undo the
    offload only if we actually did it.
    
    I've added "bool offloaded" inside a 4 byte hole between "int clockid"
    and "atomic64_t picos_per_byte". Now the first cache line looks like
    below:
    
    $ pahole -C taprio_sched net/sched/sch_taprio.o
    struct taprio_sched {
            struct Qdisc * *           qdiscs;               /*     0     8 */
            struct Qdisc *             root;                 /*     8     8 */
            u32                        flags;                /*    16     4 */
            enum tk_offsets            tk_offset;            /*    20     4 */
            int                        clockid;              /*    24     4 */
            bool                       offloaded;            /*    28     1 */
    
            /* XXX 3 bytes hole, try to pack */
    
            atomic64_t                 picos_per_byte;       /*    32     0 */
    
            /* XXX 8 bytes hole, try to pack */
    
            spinlock_t                 current_entry_lock;   /*    40     0 */
    
            /* XXX 8 bytes hole, try to pack */
    
            struct sched_entry *       current_entry;        /*    48     8 */
            struct sched_gate_list *   oper_sched;           /*    56     8 */
            /* --- cacheline 1 boundary (64 bytes) --- */
    
    Fixes: 9c66d1564676 ("taprio: Add support for hardware offloading")
    Signed-off-by: Vladimir Oltean <vladimir.oltean@nxp.com>
    Reviewed-by: Vinicius Costa Gomes <vinicius.gomes@intel.com>
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>

commit bd269188ea94e40ab002cad7b0df8f12b8f0de54
Author: James Smart <jsmart2021@gmail.com>
Date:   Thu Aug 18 18:17:32 2022 -0700

    scsi: lpfc: Rework MIB Rx Monitor debug info logic
    
    The kernel test robot reported the following sparse warning:
    
    arch/arm64/include/asm/cmpxchg.h:88:1: sparse: sparse: cast truncates
       bits from constant value (369 becomes 69)
    
    On arm64, atomic_xchg only works on 8-bit byte fields.  Thus, the macro
    usage of LPFC_RXMONITOR_TABLE_IN_USE can be unintentionally truncated
    leading to all logic involving the LPFC_RXMONITOR_TABLE_IN_USE macro to not
    work properly.
    
    Replace the Rx Table atomic_t indexing logic with a new
    lpfc_rx_info_monitor structure that holds a circular ring buffer.  For
    locking semantics, a spinlock_t is used.
    
    Link: https://lore.kernel.org/r/20220819011736.14141-4-jsmart2021@gmail.com
    Fixes: 17b27ac59224 ("scsi: lpfc: Add rx monitoring statistics")
    Cc: <stable@vger.kernel.org> # v5.15+
    Co-developed-by: Justin Tee <justin.tee@broadcom.com>
    Signed-off-by: Justin Tee <justin.tee@broadcom.com>
    Signed-off-by: James Smart <jsmart2021@gmail.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit 4b23a68f953628eb4e4b7fe1294ebf93d4b8ceee
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Jun 24 13:54:21 2022 +0100

    mm/page_alloc: protect PCP lists with a spinlock
    
    Currently the PCP lists are protected by using local_lock_irqsave to
    prevent migration and IRQ reentrancy but this is inconvenient.  Remote
    draining of the lists is impossible and a workqueue is required and every
    task allocation/free must disable then enable interrupts which is
    expensive.
    
    As preparation for dealing with both of those problems, protect the
    lists with a spinlock.  The IRQ-unsafe version of the lock is used
    because IRQs are already disabled by local_lock_irqsave.  spin_trylock
    is used in combination with local_lock_irqsave() but later will be
    replaced with a spin_trylock_irqsave when the local_lock is removed.
    
    The per_cpu_pages still fits within the same number of cache lines after
    this patch relative to before the series.
    
    struct per_cpu_pages {
            spinlock_t                 lock;                 /*     0     4 */
            int                        count;                /*     4     4 */
            int                        high;                 /*     8     4 */
            int                        batch;                /*    12     4 */
            short int                  free_factor;          /*    16     2 */
            short int                  expire;               /*    18     2 */
    
            /* XXX 4 bytes hole, try to pack */
    
            struct list_head           lists[13];            /*    24   208 */
    
            /* size: 256, cachelines: 4, members: 7 */
            /* sum members: 228, holes: 1, sum holes: 4 */
            /* padding: 24 */
    } __attribute__((__aligned__(64)));
    
    There is overhead in the fast path due to acquiring the spinlock even
    though the spinlock is per-cpu and uncontended in the common case.  Page
    Fault Test (PFT) running on a 1-socket reported the following results on a
    1 socket machine.
    
                                         5.19.0-rc3               5.19.0-rc3
                                            vanilla      mm-pcpspinirq-v5r16
    Hmean     faults/sec-1   869275.7381 (   0.00%)   874597.5167 *   0.61%*
    Hmean     faults/sec-3  2370266.6681 (   0.00%)  2379802.0362 *   0.40%*
    Hmean     faults/sec-5  2701099.7019 (   0.00%)  2664889.7003 *  -1.34%*
    Hmean     faults/sec-7  3517170.9157 (   0.00%)  3491122.8242 *  -0.74%*
    Hmean     faults/sec-8  3965729.6187 (   0.00%)  3939727.0243 *  -0.66%*
    
    There is a small hit in the number of faults per second but given that the
    results are more stable, it's borderline noise.
    
    [akpm@linux-foundation.org: add missing local_unlock_irqrestore() on contention path]
    Link: https://lkml.kernel.org/r/20220624125423.6126-6-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Tested-by: Yu Zhao <yuzhao@google.com>
    Reviewed-by: Nicolas Saenz Julienne <nsaenzju@redhat.com>
    Tested-by: Nicolas Saenz Julienne <nsaenzju@redhat.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit 1f9cc6d2c6076297d7b0daf87870a5c86385418f
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Wed Feb 9 01:56:35 2022 +0100

    random: absorb fast pool into input pool after fast load
    
    commit c30c575db4858f0bbe5e315ff2e529c782f33a1f upstream.
    
    During crng_init == 0, we never credit entropy in add_interrupt_
    randomness(), but instead dump it directly into the primary_crng. That's
    fine, except for the fact that we then wind up throwing away that
    entropy later when we switch to extracting from the input pool and
    xoring into (and later in this series overwriting) the primary_crng key.
    The two other early init sites -- add_hwgenerator_randomness()'s use
    crng_fast_load() and add_device_ randomness()'s use of crng_slow_load()
    -- always additionally give their inputs to the input pool. But not
    add_interrupt_randomness().
    
    This commit fixes that shortcoming by calling mix_pool_bytes() after
    crng_fast_load() in add_interrupt_randomness(). That's partially
    verboten on PREEMPT_RT, where it implies taking spinlock_t from an IRQ
    handler. But this also only happens during early boot and then never
    again after that. Plus it's a trylock so it has the same considerations
    as calling crng_fast_load(), which we're already using.
    
    Cc: Theodore Ts'o <tytso@mit.edu>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Suggested-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5a595c18329ee6fe9c35c2dfa18346b20ff89d7a
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Wed Feb 9 01:56:35 2022 +0100

    random: absorb fast pool into input pool after fast load
    
    commit c30c575db4858f0bbe5e315ff2e529c782f33a1f upstream.
    
    During crng_init == 0, we never credit entropy in add_interrupt_
    randomness(), but instead dump it directly into the primary_crng. That's
    fine, except for the fact that we then wind up throwing away that
    entropy later when we switch to extracting from the input pool and
    xoring into (and later in this series overwriting) the primary_crng key.
    The two other early init sites -- add_hwgenerator_randomness()'s use
    crng_fast_load() and add_device_ randomness()'s use of crng_slow_load()
    -- always additionally give their inputs to the input pool. But not
    add_interrupt_randomness().
    
    This commit fixes that shortcoming by calling mix_pool_bytes() after
    crng_fast_load() in add_interrupt_randomness(). That's partially
    verboten on PREEMPT_RT, where it implies taking spinlock_t from an IRQ
    handler. But this also only happens during early boot and then never
    again after that. Plus it's a trylock so it has the same considerations
    as calling crng_fast_load(), which we're already using.
    
    Cc: Theodore Ts'o <tytso@mit.edu>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Suggested-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a0a276481b4ded8edb4b2b5caade5ee1c1d29aad
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Wed Feb 9 01:56:35 2022 +0100

    random: absorb fast pool into input pool after fast load
    
    commit c30c575db4858f0bbe5e315ff2e529c782f33a1f upstream.
    
    During crng_init == 0, we never credit entropy in add_interrupt_
    randomness(), but instead dump it directly into the primary_crng. That's
    fine, except for the fact that we then wind up throwing away that
    entropy later when we switch to extracting from the input pool and
    xoring into (and later in this series overwriting) the primary_crng key.
    The two other early init sites -- add_hwgenerator_randomness()'s use
    crng_fast_load() and add_device_ randomness()'s use of crng_slow_load()
    -- always additionally give their inputs to the input pool. But not
    add_interrupt_randomness().
    
    This commit fixes that shortcoming by calling mix_pool_bytes() after
    crng_fast_load() in add_interrupt_randomness(). That's partially
    verboten on PREEMPT_RT, where it implies taking spinlock_t from an IRQ
    handler. But this also only happens during early boot and then never
    again after that. Plus it's a trylock so it has the same considerations
    as calling crng_fast_load(), which we're already using.
    
    Cc: Theodore Ts'o <tytso@mit.edu>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Suggested-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 876736acbe0041f40a5ebe20e16aa8e4d481abc6
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sat Apr 20 00:09:51 2019 -0400

    random: add a spinlock_t to struct batched_entropy
    
    [ Upstream commit b7d5dc21072cda7124d13eae2aefb7343ef94197 ]
    
    The per-CPU variable batched_entropy_uXX is protected by get_cpu_var().
    This is just a preempt_disable() which ensures that the variable is only
    from the local CPU. It does not protect against users on the same CPU
    from another context. It is possible that a preemptible context reads
    slot 0 and then an interrupt occurs and the same value is read again.
    
    The above scenario is confirmed by lockdep if we add a spinlock:
    | ================================
    | WARNING: inconsistent lock state
    | 5.1.0-rc3+ #42 Not tainted
    | --------------------------------
    | inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.
    | ksoftirqd/9/56 [HC0[0]:SC1[1]:HE0:SE0] takes:
    | (____ptrval____) (batched_entropy_u32.lock){+.?.}, at: get_random_u32+0x3e/0xe0
    | {SOFTIRQ-ON-W} state was registered at:
    |   _raw_spin_lock+0x2a/0x40
    |   get_random_u32+0x3e/0xe0
    |   new_slab+0x15c/0x7b0
    |   ___slab_alloc+0x492/0x620
    |   __slab_alloc.isra.73+0x53/0xa0
    |   kmem_cache_alloc_node+0xaf/0x2a0
    |   copy_process.part.41+0x1e1/0x2370
    |   _do_fork+0xdb/0x6d0
    |   kernel_thread+0x20/0x30
    |   kthreadd+0x1ba/0x220
    |   ret_from_fork+0x3a/0x50
    …
    | other info that might help us debug this:
    |  Possible unsafe locking scenario:
    |
    |        CPU0
    |        ----
    |   lock(batched_entropy_u32.lock);
    |   <Interrupt>
    |     lock(batched_entropy_u32.lock);
    |
    |  *** DEADLOCK ***
    |
    | stack backtrace:
    | Call Trace:
    …
    |  kmem_cache_alloc_trace+0x20e/0x270
    |  ipmi_alloc_recv_msg+0x16/0x40
    …
    |  __do_softirq+0xec/0x48d
    |  run_ksoftirqd+0x37/0x60
    |  smpboot_thread_fn+0x191/0x290
    |  kthread+0xfe/0x130
    |  ret_from_fork+0x3a/0x50
    
    Add a spinlock_t to the batched_entropy data structure and acquire the
    lock while accessing it. Acquire the lock with disabled interrupts
    because this function may be used from interrupt context.
    
    Remove the batched_entropy_reset_lock lock. Now that we have a lock for
    the data scructure, we can access it from a remote CPU.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Sasha Levin <sashal@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ea9941fd6e26a94c2d0c66c0980188ce011911ac
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Wed Feb 9 01:56:35 2022 +0100

    random: absorb fast pool into input pool after fast load
    
    commit c30c575db4858f0bbe5e315ff2e529c782f33a1f upstream.
    
    During crng_init == 0, we never credit entropy in add_interrupt_
    randomness(), but instead dump it directly into the primary_crng. That's
    fine, except for the fact that we then wind up throwing away that
    entropy later when we switch to extracting from the input pool and
    xoring into (and later in this series overwriting) the primary_crng key.
    The two other early init sites -- add_hwgenerator_randomness()'s use
    crng_fast_load() and add_device_ randomness()'s use of crng_slow_load()
    -- always additionally give their inputs to the input pool. But not
    add_interrupt_randomness().
    
    This commit fixes that shortcoming by calling mix_pool_bytes() after
    crng_fast_load() in add_interrupt_randomness(). That's partially
    verboten on PREEMPT_RT, where it implies taking spinlock_t from an IRQ
    handler. But this also only happens during early boot and then never
    again after that. Plus it's a trylock so it has the same considerations
    as calling crng_fast_load(), which we're already using.
    
    Cc: Theodore Ts'o <tytso@mit.edu>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Suggested-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b6e811383062f88212082714db849127fa95142c
Author: Kuniyuki Iwashima <kuniyu@amazon.com>
Date:   Tue Jun 21 10:19:10 2022 -0700

    af_unix: Define a per-netns hash table.
    
    This commit adds a per netns hash table for AF_UNIX, which size is fixed
    as UNIX_HASH_SIZE for now.
    
    The first implementation defines a per-netns hash table as a single array
    of lock and list:
    
            struct unix_hashbucket {
                    spinlock_t              lock;
                    struct hlist_head       head;
            };
    
            struct netns_unix {
                    struct unix_hashbucket  *hash;
                    ...
            };
    
    But, Eric pointed out memory cost that the structure has holes because of
    sizeof(spinlock_t), which is 4 (or more if LOCKDEP is enabled). [0]  It
    could be expensive on a host with thousands of netns and few AF_UNIX
    sockets.  For this reason, a per-netns hash table uses two dense arrays.
    
            struct unix_table {
                    spinlock_t              *locks;
                    struct hlist_head       *buckets;
            };
    
            struct netns_unix {
                    struct unix_table       table;
                    ...
            };
    
    Note the length of the list has a significant impact rather than lock
    contention, so having shared locks can be an option.  But, per-netns
    locks and lists still perform better than the global locks and per-netns
    lists. [1]
    
    Also, this patch adds a change so that struct netns_unix disappears from
    struct net if CONFIG_UNIX is disabled.
    
    [0]: https://lore.kernel.org/netdev/CANn89iLVxO5aqx16azNU7p7Z-nz5NrnM5QTqOzueVxEnkVTxyg@mail.gmail.com/
    [1]: https://lore.kernel.org/netdev/20220617175215.1769-1-kuniyu@amazon.com/
    
    Signed-off-by: Kuniyuki Iwashima <kuniyu@amazon.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 732872aa2c412457eae31589681d4eba96263e2f
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Wed Feb 9 01:56:35 2022 +0100

    random: absorb fast pool into input pool after fast load
    
    commit c30c575db4858f0bbe5e315ff2e529c782f33a1f upstream.
    
    During crng_init == 0, we never credit entropy in add_interrupt_
    randomness(), but instead dump it directly into the primary_crng. That's
    fine, except for the fact that we then wind up throwing away that
    entropy later when we switch to extracting from the input pool and
    xoring into (and later in this series overwriting) the primary_crng key.
    The two other early init sites -- add_hwgenerator_randomness()'s use
    crng_fast_load() and add_device_ randomness()'s use of crng_slow_load()
    -- always additionally give their inputs to the input pool. But not
    add_interrupt_randomness().
    
    This commit fixes that shortcoming by calling mix_pool_bytes() after
    crng_fast_load() in add_interrupt_randomness(). That's partially
    verboten on PREEMPT_RT, where it implies taking spinlock_t from an IRQ
    handler. But this also only happens during early boot and then never
    again after that. Plus it's a trylock so it has the same considerations
    as calling crng_fast_load(), which we're already using.
    
    Cc: Theodore Ts'o <tytso@mit.edu>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Suggested-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1aab83f9666398b7831293d7c3db8f568b21ba08
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Wed Feb 9 01:56:35 2022 +0100

    random: absorb fast pool into input pool after fast load
    
    commit c30c575db4858f0bbe5e315ff2e529c782f33a1f upstream.
    
    During crng_init == 0, we never credit entropy in add_interrupt_
    randomness(), but instead dump it directly into the primary_crng. That's
    fine, except for the fact that we then wind up throwing away that
    entropy later when we switch to extracting from the input pool and
    xoring into (and later in this series overwriting) the primary_crng key.
    The two other early init sites -- add_hwgenerator_randomness()'s use
    crng_fast_load() and add_device_ randomness()'s use of crng_slow_load()
    -- always additionally give their inputs to the input pool. But not
    add_interrupt_randomness().
    
    This commit fixes that shortcoming by calling mix_pool_bytes() after
    crng_fast_load() in add_interrupt_randomness(). That's partially
    verboten on PREEMPT_RT, where it implies taking spinlock_t from an IRQ
    handler. But this also only happens during early boot and then never
    again after that. Plus it's a trylock so it has the same considerations
    as calling crng_fast_load(), which we're already using.
    
    Cc: Theodore Ts'o <tytso@mit.edu>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Suggested-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8643bf4db178450c7d630e78be9fe4da1a6c208c
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Wed Feb 9 01:56:35 2022 +0100

    random: absorb fast pool into input pool after fast load
    
    commit c30c575db4858f0bbe5e315ff2e529c782f33a1f upstream.
    
    During crng_init == 0, we never credit entropy in add_interrupt_
    randomness(), but instead dump it directly into the primary_crng. That's
    fine, except for the fact that we then wind up throwing away that
    entropy later when we switch to extracting from the input pool and
    xoring into (and later in this series overwriting) the primary_crng key.
    The two other early init sites -- add_hwgenerator_randomness()'s use
    crng_fast_load() and add_device_ randomness()'s use of crng_slow_load()
    -- always additionally give their inputs to the input pool. But not
    add_interrupt_randomness().
    
    This commit fixes that shortcoming by calling mix_pool_bytes() after
    crng_fast_load() in add_interrupt_randomness(). That's partially
    verboten on PREEMPT_RT, where it implies taking spinlock_t from an IRQ
    handler. But this also only happens during early boot and then never
    again after that. Plus it's a trylock so it has the same considerations
    as calling crng_fast_load(), which we're already using.
    
    Cc: Theodore Ts'o <tytso@mit.edu>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Suggested-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 20f8932f979e7377a625f6e711bd4e84ec025d0b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri May 6 12:57:58 2022 +0200

    scsi: bnx2fc: Avoid using get_cpu() in bnx2fc_cmd_alloc()
    
    Using get_cpu() leads to disabling preemption and in this context it is not
    possible to acquire the following spinlock_t on PREEMPT_RT because it
    becomes a sleeping lock.
    
    Commit 0ea5c27583e1 ("[SCSI] bnx2fc: common free list for cleanup
    commands") says that it is using get_cpu() as a fix in case the CPU is
    preempted.  While this might be true, the important part is that it is now
    using the same CPU for locking and unlocking while previously it always
    relied on smp_processor_id().  The date structure itself is protected with
    a lock so it does not rely on CPU-local access.
    
    Replace get_cpu() with raw_smp_processor_id() to obtain the current CPU
    number which is used as an index for the per-CPU resource.
    
    Link: https://lore.kernel.org/r/20220506105758.283887-5-bigeasy@linutronix.de
    Reviewed-by: Davidlohr Bueso <dave@stgolabs.net>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit 3f80492001aa64ac585016050ace8680611c2e20
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu May 12 20:23:08 2022 -0700

    mm/vmalloc: use raw_cpu_ptr() for vmap_block_queue access
    
    The per-CPU resource vmap_block_queue is accessed via get_cpu_var().  That
    macro disables preemption and then loads the pointer from the current CPU.
    
    This doesn't work on PREEMPT_RT because a spinlock_t is later accessed
    within the preempt-disable section.
    
    There is no need to disable preemption while accessing the per-CPU struct
    vmap_block_queue because the list is protected with a spinlock_t.  The
    per-CPU struct is also accessed cross-CPU in purge_fragmented_blocks().
    
    It is possible that by using raw_cpu_ptr() the code migrates to another
    CPU and uses struct from another CPU.  This is fine because the list is
    locked and the locked section is very short.
    
    Use raw_cpu_ptr() to access vmap_block_queue.
    
    Link: https://lkml.kernel.org/r/YnKx3duAB53P7ojN@linutronix.de
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Uladzislau Rezki (Sony) <urezki@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit 945e659dffad2d2e11a105c477d423cb1b5edd95
Author: Radhey Shyam Pandey <radhey.shyam.pandey@xilinx.com>
Date:   Thu Apr 14 18:07:09 2022 +0530

    net: emaclite: Fix coding style
    
    Make coding style changes to fix checkpatch script warnings.
    There is no functional change. Fixes below check and warnings-
    
    CHECK: Blank lines aren't necessary after an open brace '{'
    CHECK: spinlock_t definition without comment
    CHECK: Please don't use multiple blank lines
    WARNING: Prefer 'unsigned int' to bare use of 'unsigned'
    CHECK: braces {} should be used on all arms of this statement
    CHECK: Unbalanced braces around else statement
    CHECK: Alignment should match open parenthesis
    WARNING: Missing a blank line after declarations
    
    Signed-off-by: Radhey Shyam Pandey <radhey.shyam.pandey@xilinx.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 7e7a4f32b4efb8bad2df73e87ffa914b85cbc966
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Tue Feb 16 15:04:34 2021 +0000

    rcu: Kill rnp->ofl_seq and use only rcu_state.ofl_lock for exclusion
    
    [ Upstream commit 82980b1622d97017053c6792382469d7dc26a486 ]
    
    If we allow architectures to bring APs online in parallel, then we end
    up requiring rcu_cpu_starting() to be reentrant. But currently, the
    manipulation of rnp->ofl_seq is not thread-safe.
    
    However, rnp->ofl_seq is also fairly much pointless anyway since both
    rcu_cpu_starting() and rcu_report_dead() hold rcu_state.ofl_lock for
    fairly much the whole time that rnp->ofl_seq is set to an odd number
    to indicate that an operation is in progress.
    
    So drop rnp->ofl_seq completely, and use only rcu_state.ofl_lock.
    
    This has a couple of minor complexities: lockdep will complain when we
    take rcu_state.ofl_lock, and currently accepts the 'excuse' of having
    an odd value in rnp->ofl_seq. So switch it to an arch_spinlock_t to
    avoid that false positive complaint. Since we're killing rnp->ofl_seq
    of course that 'excuse' has to be changed too, so make it check for
    arch_spin_is_locked(rcu_state.ofl_lock).
    
    There's no arch_spin_lock_irqsave() so we have to manually save and
    restore local interrupts around the locking.
    
    At Paul's request based on Neeraj's analysis, make rcu_gp_init not just
    wait but *exclude* any CPU online/offline activity, which was fairly
    much true already by virtue of it holding rcu_state.ofl_lock.
    
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 1680a05ff9e93cdc932d9b7c684f215c3167ce42
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Tue Feb 16 15:04:34 2021 +0000

    rcu: Kill rnp->ofl_seq and use only rcu_state.ofl_lock for exclusion
    
    [ Upstream commit 82980b1622d97017053c6792382469d7dc26a486 ]
    
    If we allow architectures to bring APs online in parallel, then we end
    up requiring rcu_cpu_starting() to be reentrant. But currently, the
    manipulation of rnp->ofl_seq is not thread-safe.
    
    However, rnp->ofl_seq is also fairly much pointless anyway since both
    rcu_cpu_starting() and rcu_report_dead() hold rcu_state.ofl_lock for
    fairly much the whole time that rnp->ofl_seq is set to an odd number
    to indicate that an operation is in progress.
    
    So drop rnp->ofl_seq completely, and use only rcu_state.ofl_lock.
    
    This has a couple of minor complexities: lockdep will complain when we
    take rcu_state.ofl_lock, and currently accepts the 'excuse' of having
    an odd value in rnp->ofl_seq. So switch it to an arch_spinlock_t to
    avoid that false positive complaint. Since we're killing rnp->ofl_seq
    of course that 'excuse' has to be changed too, so make it check for
    arch_spin_is_locked(rcu_state.ofl_lock).
    
    There's no arch_spin_lock_irqsave() so we have to manually save and
    restore local interrupts around the locking.
    
    At Paul's request based on Neeraj's analysis, make rcu_gp_init not just
    wait but *exclude* any CPU online/offline activity, which was fairly
    much true already by virtue of it holding rcu_state.ofl_lock.
    
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit e425ac99b1573692fc4bb5bda1040caccb127490
Author: Alexander Aring <aahringo@redhat.com>
Date:   Thu Apr 7 10:45:42 2022 -0400

    fs: dlm: cast resource pointer to uintptr_t
    
    This patch fixes the following warning when doing a 32 bit kernel build
    when pointers are 4 byte long:
    
    In file included from ./include/linux/byteorder/little_endian.h:5,
                     from ./arch/x86/include/uapi/asm/byteorder.h:5,
                     from ./include/asm-generic/qrwlock_types.h:6,
                     from ./arch/x86/include/asm/spinlock_types.h:7,
                     from ./include/linux/spinlock_types_raw.h:7,
                     from ./include/linux/ratelimit_types.h:7,
                     from ./include/linux/printk.h:10,
                     from ./include/asm-generic/bug.h:22,
                     from ./arch/x86/include/asm/bug.h:87,
                     from ./include/linux/bug.h:5,
                     from ./include/linux/mmdebug.h:5,
                     from ./include/linux/gfp.h:5,
                     from ./include/linux/slab.h:15,
                     from fs/dlm/dlm_internal.h:19,
                     from fs/dlm/rcom.c:12:
    fs/dlm/rcom.c: In function ‘dlm_send_rcom_lock’:
    ./include/uapi/linux/byteorder/little_endian.h:32:43: warning: cast from pointer to integer of different size [-Wpointer-to-int-cast]
     #define __cpu_to_le64(x) ((__force __le64)(__u64)(x))
                                               ^
    ./include/linux/byteorder/generic.h:86:21: note: in expansion of macro ‘__cpu_to_le64’
     #define cpu_to_le64 __cpu_to_le64
                         ^~~~~~~~~~~~~
    fs/dlm/rcom.c:457:14: note: in expansion of macro ‘cpu_to_le64’
      rc->rc_id = cpu_to_le64(r);
    
    The rc_id value in dlm rcom is handled as u64. The rcom implementation
    uses for an unique number generation the pointer value of the used
    dlm_rsb instance. However if the pointer value is 4 bytes long
    -Wpointer-to-int-cast will print a warning. We get rid of that warning
    to cast the pointer to uintptr_t which is either 4 or 8 bytes. There
    might be a very unlikely case where this number isn't unique anymore if
    using dlm in a mixed cluster of nodes and sizeof(uintptr_t) returns 4 and
    8.
    
    However this problem was already been there and this patch should get
    rid of the warning.
    
    Fixes: 2f9dbeda8dc0 ("dlm: use __le types for rcom messages")
    Reported-by: kernel test robot <lkp@intel.com>
    Signed-off-by: Alexander Aring <aahringo@redhat.com>
    Signed-off-by: David Teigland <teigland@redhat.com>

commit b4a5ea09b29371c2e6a10783faa3593428404343
Merge: b8321ed4a40c 022bb490c797
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 31 12:10:42 2022 -0700

    Merge tag 'docs-5.18-2' of git://git.lwn.net/linux
    
    Pull more documentation updates from Jonathan Corbet:
     "Some late-arriving documentation improvements.
    
      This is mostly build-system fixes from Mauro and Akira; I also took
      the liberty of dropping in my 'messy diffstat' document"
    
    * tag 'docs-5.18-2' of git://git.lwn.net/linux:
      docs: Add a document on how to fix a messy diffstat
      docs: sphinx/requirements: Limit jinja2<3.1
      Documentation: kunit: Fix cross-referencing warnings
      scripts/kernel-doc: change the line number meta info
      scripts/get_abi: change the file/line number meta info
      docs: kernel_include.py: add sphinx build dependencies
      docs: kernel_abi.py: add sphinx build dependencies
      docs: kernel_feat.py: add build dependencies
      scripts/get_feat.pl: allow output the parsed file names
      docs: kfigure.py: Don't warn of missing PDF converter in 'make htmldocs'
      Documentation: Fix duplicate statement about raw_spinlock_t type

commit 8d6451b9a51b555be2c9a6c326a980b2de00741a
Author: Guilherme G. Piccoli <gpiccoli@igalia.com>
Date:   Mon Mar 21 11:41:33 2022 -0300

    Documentation: Fix duplicate statement about raw_spinlock_t type
    
    Unless it was duplicate on purpose, to emphasize that a raw_spinlock_t
    is always a spinning lock regardless of PREEMPT_RT or kernel config,
    it's a bit odd that this text is duplicate. So, this patch just clean
    it up, keeping the consistency with the other sections of the text.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 919e9e6395cf ("Documentation: Add lock ordering and nesting documentation")
    Signed-off-by: Guilherme G. Piccoli <gpiccoli@igalia.com>
    Link: https://lore.kernel.org/r/20220321144133.49804-1-gpiccoli@igalia.com
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>

commit be3e67b54b437123e6144da31cf312ddcaa5aef2
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Mar 22 14:40:41 2022 -0700

    mm/memcg: protect per-CPU counter by disabling preemption on PREEMPT_RT where needed.
    
    The per-CPU counter are modified with the non-atomic modifier.  The
    consistency is ensured by disabling interrupts for the update.  On non
    PREEMPT_RT configuration this works because acquiring a spinlock_t typed
    lock with the _irq() suffix disables interrupts.  On PREEMPT_RT
    configurations the RMW operation can be interrupted.
    
    Another problem is that mem_cgroup_swapout() expects to be invoked with
    disabled interrupts because the caller has to acquire a spinlock_t which
    is acquired with disabled interrupts.  Since spinlock_t never disables
    interrupts on PREEMPT_RT the interrupts are never disabled at this
    point.
    
    The code is never called from in_irq() context on PREEMPT_RT therefore
    disabling preemption during the update is sufficient on PREEMPT_RT.  The
    sections which explicitly disable interrupts can remain on PREEMPT_RT
    because the sections remain short and they don't involve sleeping locks
    (memcg_check_events() is doing nothing on PREEMPT_RT).
    
    Disable preemption during update of the per-CPU variables which do not
    explicitly disable interrupts.
    
    Link: https://lkml.kernel.org/r/20220226204144.1008339-4-bigeasy@linutronix.de
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Roman Gushchin <guro@fb.com>
    Reviewed-by: Shakeel Butt <shakeelb@google.com
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: kernel test robot <oliver.sang@intel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Michal Koutný <mkoutny@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Waiman Long <longman@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b9fae6a47b8bcb397e6a482095431f6ba9648211
Author: Randy Dunlap <rdunlap@infradead.org>
Date:   Sat Feb 26 13:37:03 2022 -0800

    x86/PCI: Add #includes to asm/pci_x86.h
    
    <asm/pci_x86.h> uses raw_spinlock_t, __init, and EINVAL; #include the
    appropriate files to prevent build errors.
    
      ../arch/x86/include/asm/pci_x86.h:105:8: error: unknown type name ‘raw_spinlock_t’
      ../arch/x86/include/asm/pci_x86.h:141:20: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘dmi_check_pciprobe’
      ../arch/x86/include/asm/pci_x86.h:150:10: error: ‘EINVAL’ undeclared (first use in this function)
    
    Link: https://lore.kernel.org/r/20220226213703.24041-1-rdunlap@infradead.org
    Signed-off-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Mark Brown <broonie@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>

commit bf9ad37dc8a30cce22ae95d6c2ca6abf8731d305
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Jul 14 14:26:34 2015 +0200

    signal, x86: Delay calling signals in atomic on RT enabled kernels
    
    On x86_64 we must disable preemption before we enable interrupts
    for stack faults, int3 and debugging, because the current task is using
    a per CPU debug stack defined by the IST. If we schedule out, another task
    can come in and use the same stack and cause the stack to be corrupted
    and crash the kernel on return.
    
    When CONFIG_PREEMPT_RT is enabled, spinlock_t locks become sleeping, and
    one of these is the spin lock used in signal handling.
    
    Some of the debug code (int3) causes do_trap() to send a signal.
    This function calls a spinlock_t lock that has been converted to a
    sleeping lock. If this happens, the above issues with the corrupted
    stack is possible.
    
    Instead of calling the signal right away, for PREEMPT_RT and x86,
    the signal information is stored on the stacks task_struct and
    TIF_NOTIFY_RESUME is set. Then on exit of the trap, the signal resume
    code will send the signal when preemption is enabled.
    
    [ rostedt: Switched from #ifdef CONFIG_PREEMPT_RT to
      ARCH_RT_DELAYS_SIGNAL_SEND and added comments to the code. ]
    [bigeasy: Add on 32bit as per Yang Shi, minor rewording. ]
    [ tglx: Use a config option ]
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/Ygq5aBB/qMQw6aP5@linutronix.de

commit efae250a1a12196523f6fdbac3c5f7a440b5edce
Author: Philipp Hortmann <philipp.g.hortmann@gmail.com>
Date:   Mon Feb 21 22:25:05 2022 +0100

    staging: vt6656: Add comment for locks
    
    This patch fixes the checkpatch.pl warnings like:
    - CHECK: spinlock_t definition without comment
    
    Reviewed-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Philipp Hortmann <philipp.g.hortmann@gmail.com>
    Link: https://lore.kernel.org/r/c6a5ef8e7704b488c54145b09ac44bd4880c13b4.1645477326.git.philipp.g.hortmann@gmail.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c30c575db4858f0bbe5e315ff2e529c782f33a1f
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Wed Feb 9 01:56:35 2022 +0100

    random: absorb fast pool into input pool after fast load
    
    During crng_init == 0, we never credit entropy in add_interrupt_
    randomness(), but instead dump it directly into the primary_crng. That's
    fine, except for the fact that we then wind up throwing away that
    entropy later when we switch to extracting from the input pool and
    xoring into (and later in this series overwriting) the primary_crng key.
    The two other early init sites -- add_hwgenerator_randomness()'s use
    crng_fast_load() and add_device_ randomness()'s use of crng_slow_load()
    -- always additionally give their inputs to the input pool. But not
    add_interrupt_randomness().
    
    This commit fixes that shortcoming by calling mix_pool_bytes() after
    crng_fast_load() in add_interrupt_randomness(). That's partially
    verboten on PREEMPT_RT, where it implies taking spinlock_t from an IRQ
    handler. But this also only happens during early boot and then never
    again after that. Plus it's a trylock so it has the same considerations
    as calling crng_fast_load(), which we're already using.
    
    Cc: Theodore Ts'o <tytso@mit.edu>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Reviewed-by: Eric Biggers <ebiggers@google.com>
    Suggested-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>

commit e722db8de6e6932267457ace2657a19015f3db4a
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sat Feb 12 00:38:39 2022 +0100

    net: dev: Make rps_lock() disable interrupts.
    
    Disabling interrupts and in the RPS case locking input_pkt_queue is
    split into local_irq_disable() and optional spin_lock().
    
    This breaks on PREEMPT_RT because the spinlock_t typed lock can not be
    acquired with disabled interrupts.
    The sections in which the lock is acquired is usually short in a sense that it
    is not causing long und unbounded latiencies. One exception is the
    skb_flow_limit() invocation which may invoke a BPF program (and may
    require sleeping locks).
    
    By moving local_irq_disable() + spin_lock() into rps_lock(), we can keep
    interrupts disabled on !PREEMPT_RT and enabled on PREEMPT_RT kernels.
    Without RPS on a PREEMPT_RT kernel, the needed synchronisation happens
    as part of local_bh_disable() on the local CPU.
    ____napi_schedule() is only invoked if sd is from the local CPU. Replace
    it with __napi_schedule_irqoff() which already disables interrupts on
    PREEMPT_RT as needed. Move this call to rps_ipi_queued() and rename the
    function to napi_schedule_rps as suggested by Jakub.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 82980b1622d97017053c6792382469d7dc26a486
Author: David Woodhouse <dwmw@amazon.co.uk>
Date:   Tue Feb 16 15:04:34 2021 +0000

    rcu: Kill rnp->ofl_seq and use only rcu_state.ofl_lock for exclusion
    
    If we allow architectures to bring APs online in parallel, then we end
    up requiring rcu_cpu_starting() to be reentrant. But currently, the
    manipulation of rnp->ofl_seq is not thread-safe.
    
    However, rnp->ofl_seq is also fairly much pointless anyway since both
    rcu_cpu_starting() and rcu_report_dead() hold rcu_state.ofl_lock for
    fairly much the whole time that rnp->ofl_seq is set to an odd number
    to indicate that an operation is in progress.
    
    So drop rnp->ofl_seq completely, and use only rcu_state.ofl_lock.
    
    This has a couple of minor complexities: lockdep will complain when we
    take rcu_state.ofl_lock, and currently accepts the 'excuse' of having
    an odd value in rnp->ofl_seq. So switch it to an arch_spinlock_t to
    avoid that false positive complaint. Since we're killing rnp->ofl_seq
    of course that 'excuse' has to be changed too, so make it check for
    arch_spin_is_locked(rcu_state.ofl_lock).
    
    There's no arch_spin_lock_irqsave() so we have to manually save and
    restore local interrupts around the locking.
    
    At Paul's request based on Neeraj's analysis, make rcu_gp_init not just
    wait but *exclude* any CPU online/offline activity, which was fairly
    much true already by virtue of it holding rcu_state.ofl_lock.
    
    Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit 1170233f063de7eb9020344338966fd30afea7b6
Author: Iwona Winiarska <iwona.winiarska@intel.com>
Date:   Sat Dec 4 18:10:27 2021 +0100

    gpio: aspeed-sgpio: Convert aspeed_sgpio.lock to raw_spinlock
    
    [ Upstream commit ab39d6988dd53f354130438d8afa5596a2440fed ]
    
    The gpio-aspeed-sgpio driver implements an irq_chip which need to be
    invoked from hardirq context. Since spin_lock() can sleep with
    PREEMPT_RT, it is no longer legal to invoke it while interrupts are
    disabled.
    This also causes lockdep to complain about:
    [   25.919465] [ BUG: Invalid wait context ]
    because aspeed_sgpio.lock (spin_lock_t) is taken under irq_desc.lock
    (raw_spinlock_t).
    Let's use of raw_spinlock_t instead of spinlock_t.
    
    Signed-off-by: Iwona Winiarska <iwona.winiarska@intel.com>
    Signed-off-by: Bartosz Golaszewski <brgl@bgdev.pl>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 102dd8b25de70d91db4736b3e11b9a44308babd2
Author: Iwona Winiarska <iwona.winiarska@intel.com>
Date:   Sat Dec 4 18:10:26 2021 +0100

    gpio: aspeed: Convert aspeed_gpio.lock to raw_spinlock
    
    [ Upstream commit 61a7904b6ace99b1bde0d0e867fa3097f5c8cee2 ]
    
    The gpio-aspeed driver implements an irq_chip which need to be invoked
    from hardirq context. Since spin_lock() can sleep with PREEMPT_RT, it is
    no longer legal to invoke it while interrupts are disabled.
    This also causes lockdep to complain about:
    [    0.649797] [ BUG: Invalid wait context ]
    because aspeed_gpio.lock (spin_lock_t) is taken under irq_desc.lock
    (raw_spinlock_t).
    Let's use of raw_spinlock_t instead of spinlock_t.
    
    Signed-off-by: Iwona Winiarska <iwona.winiarska@intel.com>
    Signed-off-by: Bartosz Golaszewski <brgl@bgdev.pl>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 8bb1290e90504203c2909113e1af1828948b1b7a
Author: Iwona Winiarska <iwona.winiarska@intel.com>
Date:   Sat Dec 4 18:10:27 2021 +0100

    gpio: aspeed-sgpio: Convert aspeed_sgpio.lock to raw_spinlock
    
    [ Upstream commit ab39d6988dd53f354130438d8afa5596a2440fed ]
    
    The gpio-aspeed-sgpio driver implements an irq_chip which need to be
    invoked from hardirq context. Since spin_lock() can sleep with
    PREEMPT_RT, it is no longer legal to invoke it while interrupts are
    disabled.
    This also causes lockdep to complain about:
    [   25.919465] [ BUG: Invalid wait context ]
    because aspeed_sgpio.lock (spin_lock_t) is taken under irq_desc.lock
    (raw_spinlock_t).
    Let's use of raw_spinlock_t instead of spinlock_t.
    
    Signed-off-by: Iwona Winiarska <iwona.winiarska@intel.com>
    Signed-off-by: Bartosz Golaszewski <brgl@bgdev.pl>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d9332eab23381364b70e13eb17e7bf0a709504c5
Author: Iwona Winiarska <iwona.winiarska@intel.com>
Date:   Sat Dec 4 18:10:26 2021 +0100

    gpio: aspeed: Convert aspeed_gpio.lock to raw_spinlock
    
    [ Upstream commit 61a7904b6ace99b1bde0d0e867fa3097f5c8cee2 ]
    
    The gpio-aspeed driver implements an irq_chip which need to be invoked
    from hardirq context. Since spin_lock() can sleep with PREEMPT_RT, it is
    no longer legal to invoke it while interrupts are disabled.
    This also causes lockdep to complain about:
    [    0.649797] [ BUG: Invalid wait context ]
    because aspeed_gpio.lock (spin_lock_t) is taken under irq_desc.lock
    (raw_spinlock_t).
    Let's use of raw_spinlock_t instead of spinlock_t.
    
    Signed-off-by: Iwona Winiarska <iwona.winiarska@intel.com>
    Signed-off-by: Bartosz Golaszewski <brgl@bgdev.pl>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 07ecabf15ad3bfcaaa7e3ee6d660cfce8018dc2a
Author: Iwona Winiarska <iwona.winiarska@intel.com>
Date:   Sat Dec 4 18:10:26 2021 +0100

    gpio: aspeed: Convert aspeed_gpio.lock to raw_spinlock
    
    [ Upstream commit 61a7904b6ace99b1bde0d0e867fa3097f5c8cee2 ]
    
    The gpio-aspeed driver implements an irq_chip which need to be invoked
    from hardirq context. Since spin_lock() can sleep with PREEMPT_RT, it is
    no longer legal to invoke it while interrupts are disabled.
    This also causes lockdep to complain about:
    [    0.649797] [ BUG: Invalid wait context ]
    because aspeed_gpio.lock (spin_lock_t) is taken under irq_desc.lock
    (raw_spinlock_t).
    Let's use of raw_spinlock_t instead of spinlock_t.
    
    Signed-off-by: Iwona Winiarska <iwona.winiarska@intel.com>
    Signed-off-by: Bartosz Golaszewski <brgl@bgdev.pl>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2481ee0ce59cae61dcad426e659b0d24e10abe5e
Author: Iwona Winiarska <iwona.winiarska@intel.com>
Date:   Sat Dec 4 18:10:26 2021 +0100

    gpio: aspeed: Convert aspeed_gpio.lock to raw_spinlock
    
    [ Upstream commit 61a7904b6ace99b1bde0d0e867fa3097f5c8cee2 ]
    
    The gpio-aspeed driver implements an irq_chip which need to be invoked
    from hardirq context. Since spin_lock() can sleep with PREEMPT_RT, it is
    no longer legal to invoke it while interrupts are disabled.
    This also causes lockdep to complain about:
    [    0.649797] [ BUG: Invalid wait context ]
    because aspeed_gpio.lock (spin_lock_t) is taken under irq_desc.lock
    (raw_spinlock_t).
    Let's use of raw_spinlock_t instead of spinlock_t.
    
    Signed-off-by: Iwona Winiarska <iwona.winiarska@intel.com>
    Signed-off-by: Bartosz Golaszewski <brgl@bgdev.pl>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit a37265995c867a4e413761d846cef0445b08d6d5
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Fri Jan 21 22:14:17 2022 -0800

    zsmalloc: replace get_cpu_var with local_lock
    
    The usage of get_cpu_var() in zs_map_object() is problematic because it
    disables preemption and makes it impossible to acquire any sleeping lock
    on PREEMPT_RT such as a spinlock_t.
    
    Replace the get_cpu_var() usage with a local_lock_t which is embedded
    struct mapping_area.  It ensures that the access the struct is
    synchronized against all users on the same CPU.
    
    [minchan: remove the bit_spin_lock part and change the title]
    
    Link: https://lkml.kernel.org/r/20211115185909.3949505-10-minchan@kernel.org
    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Tested-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Sergey Senozhatsky <senozhatsky@chromium.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit daadb3bd0e8d3e317e36bc2c1542e86c528665e5
Merge: 6ae71436cda7 f16cc980d649
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 11 17:24:45 2022 -0800

    Merge tag 'locking_core_for_v5.17_rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Borislav Petkov:
     "Lots of cleanups and preparation. Highlights:
    
       - futex: Cleanup and remove runtime futex_cmpxchg detection
    
       - rtmutex: Some fixes for the PREEMPT_RT locking infrastructure
    
       - kcsan: Share owner_on_cpu() between mutex,rtmutex and rwsem and
         annotate the racy owner->on_cpu access *once*.
    
       - atomic64: Dead-Code-Elemination"
    
    [ Description above by Peter Zijlstra ]
    
    * tag 'locking_core_for_v5.17_rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/atomic: atomic64: Remove unusable atomic ops
      futex: Fix additional regressions
      locking: Allow to include asm/spinlock_types.h from linux/spinlock_types_raw.h
      x86/mm: Include spinlock_t definition in pgtable.
      locking: Mark racy reads of owner->on_cpu
      locking: Make owner_on_cpu() into <linux/sched.h>
      lockdep/selftests: Adapt ww-tests for PREEMPT_RT
      lockdep/selftests: Skip the softirq related tests on PREEMPT_RT
      lockdep/selftests: Unbalanced migrate_disable() & rcu_read_lock().
      lockdep/selftests: Avoid using local_lock_{acquire|release}().
      lockdep: Remove softirq accounting on PREEMPT_RT.
      locking/rtmutex: Add rt_mutex_lock_nest_lock() and rt_mutex_lock_killable().
      locking/rtmutex: Squash self-deadlock check for ww_rt_mutex.
      locking: Remove rt_rwlock_is_contended().
      sched: Trigger warning if ->migration_disabled counter underflows.
      futex: Fix sparc32/m68k/nds32 build regression
      futex: Remove futex_cmpxchg detection
      futex: Ensure futex_atomic_cmpxchg_inatomic() is present
      kernel/locking: Use a pointer in ww_mutex_trylock().

commit 4a692ae360615026b25d64c29fc7c12c0ef63c5f
Merge: bfed6efb8e13 b64dfcde1ca9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 10 09:51:38 2022 -0800

    Merge tag 'x86_mm_for_v5.17_rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm updates from Borislav Petkov:
    
     - Flush *all* mappings from the TLB after switching to the trampoline
       pagetable to prevent any stale entries' presence
    
     - Flush global mappings from the TLB, in addition to the CR3-write,
       after switching off of the trampoline_pgd during boot to clear the
       identity mappings
    
     - Prevent instrumentation issues resulting from the above changes
    
    * tag 'x86_mm_for_v5.17_rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm: Prevent early boot triple-faults with instrumentation
      x86/mm: Include spinlock_t definition in pgtable.
      x86/mm: Flush global TLB when switching to trampoline page-table
      x86/mm/64: Flush global TLB on boot and AP bringup
      x86/realmode: Add comment for Global bit usage in trampoline_pgd
      x86/mm: Add missing <asm/cpufeatures.h> dependency to <asm/page_64.h>

commit 361c81dbc58c8aa230e1f2d556045fa7bc3eb4a3
Author: Wander Lairson Costa <wander@redhat.com>
Date:   Mon Dec 20 16:28:27 2021 -0300

    blktrace: switch trace spinlock to a raw spinlock
    
    The running_trace_lock protects running_trace_list and is acquired
    within the tracepoint which implies disabled preemption. The spinlock_t
    typed lock can not be acquired with disabled preemption on PREEMPT_RT
    because it becomes a sleeping lock.
    The runtime of the tracepoint depends on the number of entries in
    running_trace_list and has no limit. The blk-tracer is considered debug
    code and higher latencies here are okay.
    
    Make running_trace_lock a raw_spinlock_t.
    
    Signed-off-by: Wander Lairson Costa <wander@redhat.com>
    Link: https://lore.kernel.org/r/20211220192827.38297-1-wander@redhat.com
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit ab39d6988dd53f354130438d8afa5596a2440fed
Author: Iwona Winiarska <iwona.winiarska@intel.com>
Date:   Sat Dec 4 18:10:27 2021 +0100

    gpio: aspeed-sgpio: Convert aspeed_sgpio.lock to raw_spinlock
    
    The gpio-aspeed-sgpio driver implements an irq_chip which need to be
    invoked from hardirq context. Since spin_lock() can sleep with
    PREEMPT_RT, it is no longer legal to invoke it while interrupts are
    disabled.
    This also causes lockdep to complain about:
    [   25.919465] [ BUG: Invalid wait context ]
    because aspeed_sgpio.lock (spin_lock_t) is taken under irq_desc.lock
    (raw_spinlock_t).
    Let's use of raw_spinlock_t instead of spinlock_t.
    
    Signed-off-by: Iwona Winiarska <iwona.winiarska@intel.com>
    Signed-off-by: Bartosz Golaszewski <brgl@bgdev.pl>

commit 61a7904b6ace99b1bde0d0e867fa3097f5c8cee2
Author: Iwona Winiarska <iwona.winiarska@intel.com>
Date:   Sat Dec 4 18:10:26 2021 +0100

    gpio: aspeed: Convert aspeed_gpio.lock to raw_spinlock
    
    The gpio-aspeed driver implements an irq_chip which need to be invoked
    from hardirq context. Since spin_lock() can sleep with PREEMPT_RT, it is
    no longer legal to invoke it while interrupts are disabled.
    This also causes lockdep to complain about:
    [    0.649797] [ BUG: Invalid wait context ]
    because aspeed_gpio.lock (spin_lock_t) is taken under irq_desc.lock
    (raw_spinlock_t).
    Let's use of raw_spinlock_t instead of spinlock_t.
    
    Signed-off-by: Iwona Winiarska <iwona.winiarska@intel.com>
    Signed-off-by: Bartosz Golaszewski <brgl@bgdev.pl>

commit 35fa745286ac44ee26ed100c2bd2553368ad193b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Nov 2 17:52:24 2021 +0100

    x86/mm: Include spinlock_t definition in pgtable.
    
    This header file provides forward declartion for pgd_lock but does not
    include the header defining its type. This works since the definition of
    spinlock_t is usually included somehow via printk.
    
    By trying to avoid recursive includes on PREEMPT_RT I avoided the loop
    in printk and as a consequnce kernel/intel.c failed to compile due to
    missing type definition.
    
    Include the needed definition for spinlock_t.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Link: https://lkml.kernel.org/r/20211102165224.wpz4zyhsvwccx5p3@linutronix.de

commit 77993b595ada5731e513eb06a0f4bf4b9f1e9532
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Nov 29 18:46:54 2021 +0100

    locking: Allow to include asm/spinlock_types.h from linux/spinlock_types_raw.h
    
    The printk header file includes ratelimit_types.h for its __ratelimit()
    based usage. It is required for the static initializer used in
    printk_ratelimited(). It uses a raw_spinlock_t and includes the
    spinlock_types.h.
    
    PREEMPT_RT substitutes spinlock_t with a rtmutex based implementation and so
    its spinlock_t implmentation (provided by spinlock_rt.h) includes rtmutex.h and
    atomic.h which leads to recursive includes where defines are missing.
    
    By including only the raw_spinlock_t defines it avoids the atomic.h
    related includes at this stage.
    
    An example on powerpc:
    
    |  CALL    scripts/atomic/check-atomics.sh
    |In file included from include/linux/bug.h:5,
    |                 from include/linux/page-flags.h:10,
    |                 from kernel/bounds.c:10:
    |arch/powerpc/include/asm/page_32.h: In function âclear_pageâ:
    |arch/powerpc/include/asm/bug.h:87:4: error: implicit declaration of function â=80=98__WARNâ=80=99 [-Werror=3Dimplicit-function-declaration]
    |   87 |    __WARN();    \
    |      |    ^~~~~~
    |arch/powerpc/include/asm/page_32.h:48:2: note: in expansion of macro âWARN_ONâ=99
    |   48 |  WARN_ON((unsigned long)addr & (L1_CACHE_BYTES - 1));
    |      |  ^~~~~~~
    |arch/powerpc/include/asm/bug.h:58:17: error: invalid application of âsizeofâ=99 to incomplete type âstruct bug_entryâ=99
    |   58 |     "i" (sizeof(struct bug_entry)), \
    |      |                 ^~~~~~
    |arch/powerpc/include/asm/bug.h:89:3: note: in expansion of macro âBUG_ENTRYâ=99
    |   89 |   BUG_ENTRY(PPC_TLNEI " %4, 0",   \
    |      |   ^~~~~~~~~
    |arch/powerpc/include/asm/page_32.h:48:2: note: in expansion of macro âWARN_ONâ=99
    |   48 |  WARN_ON((unsigned long)addr & (L1_CACHE_BYTES - 1));
    |      |  ^~~~~~~
    |In file included from arch/powerpc/include/asm/ptrace.h:298,
    |                 from arch/powerpc/include/asm/hw_irq.h:12,
    |                 from arch/powerpc/include/asm/irqflags.h:12,
    |                 from include/linux/irqflags.h:16,
    |                 from include/asm-generic/cmpxchg-local.h:6,
    |                 from arch/powerpc/include/asm/cmpxchg.h:526,
    |                 from arch/powerpc/include/asm/atomic.h:11,
    |                 from include/linux/atomic.h:7,
    |                 from include/linux/rwbase_rt.h:6,
    |                 from include/linux/rwlock_types.h:55,
    |                 from include/linux/spinlock_types.h:74,
    |                 from include/linux/ratelimit_types.h:7,
    |                 from include/linux/printk.h:10,
    |                 from include/asm-generic/bug.h:22,
    |                 from arch/powerpc/include/asm/bug.h:109,
    |                 from include/linux/bug.h:5,
    |                 from include/linux/page-flags.h:10,
    |                 from kernel/bounds.c:10:
    |include/linux/thread_info.h: In function â=80=98copy_overflowâ=80=99:
    |include/linux/thread_info.h:210:2: error: implicit declaration of function â=80=98WARNâ=80=99 [-Werror=3Dimplicit-function-declaration]
    |  210 |  WARN(1, "Buffer overflow detected (%d < %lu)!\n", size, count);
    |      |  ^~~~
    
    The WARN / BUG include pulls in printk.h and then ptrace.h expects WARN
    (from bug.h) which is not yet complete. Even hw_irq.h has WARN_ON()
    statements.
    
    On POWERPC64 there are missing atomic64 defines while building 32bit
    VDSO:
    |  VDSO32C arch/powerpc/kernel/vdso32/vgettimeofday.o
    |In file included from include/linux/atomic.h:80,
    |                 from include/linux/rwbase_rt.h:6,
    |                 from include/linux/rwlock_types.h:55,
    |                 from include/linux/spinlock_types.h:74,
    |                 from include/linux/ratelimit_types.h:7,
    |                 from include/linux/printk.h:10,
    |                 from include/linux/kernel.h:19,
    |                 from arch/powerpc/include/asm/page.h:11,
    |                 from arch/powerpc/include/asm/vdso/gettimeofday.h:5,
    |                 from include/vdso/datapage.h:137,
    |                 from lib/vdso/gettimeofday.c:5,
    |                 from <command-line>:
    |include/linux/atomic-arch-fallback.h: In function âarch_atomic64_incâ=99:
    |include/linux/atomic-arch-fallback.h:1447:2: error: implicit declaration of function âarch_atomic64_addâ; did you mean âarch_atomic_addâ? [-Werror=3Dimpl
    |icit-function-declaration]
    | 1447 |  arch_atomic64_add(1, v);
    |      |  ^~~~~~~~~~~~~~~~~
    |      |  arch_atomic_add
    
    The generic fallback is not included, atomics itself are not used. If
    kernel.h does not include printk.h then it comes later from the bug.h
    include.
    
    Allow asm/spinlock_types.h to be included from
    linux/spinlock_types_raw.h.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20211129174654.668506-12-bigeasy@linutronix.de

commit 0cf292b569bc9bc87d29ac87ca5c47fdd5882e10
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Nov 29 18:46:53 2021 +0100

    x86/mm: Include spinlock_t definition in pgtable.
    
    This header file provides forward declartion for pgd_lock but does not
    include the header defining its type. This works since the definition of
    spinlock_t is usually included somehow via printk.
    
    By trying to avoid recursive includes on PREEMPT_RT I avoided the loop
    in printk and as a consequnce kernel/intel.c failed to compile due to
    missing type definition.
    
    Include the needed definition for spinlock_t.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20211129174654.668506-11-bigeasy@linutronix.de

commit 0f8821da48458982cf379eb4432f23958f2e3a6c
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Nov 25 13:07:11 2021 +0100

    fs/namespace: Boost the mount_lock.lock owner instead of spinning on PREEMPT_RT.
    
    The MNT_WRITE_HOLD flag is used to hold back any new writers while the
    mount point is about to be made read-only. __mnt_want_write() then loops
    with disabled preemption until this flag disappears. Callers of
    mnt_hold_writers() (which sets the flag) hold the spinlock_t of
    mount_lock (seqlock_t) which disables preemption on !PREEMPT_RT and
    ensures the task is not scheduled away so that the spinning side spins
    for a long time.
    
    On PREEMPT_RT the spinlock_t does not disable preemption and so it is
    possible that the task setting MNT_WRITE_HOLD is preempted by task with
    higher priority which then spins infinitely waiting for MNT_WRITE_HOLD
    to get removed.
    
    Acquire mount_lock::lock which is held by setter of MNT_WRITE_HOLD. This
    will PI-boost the owner and wait until the lock is dropped and which
    means that MNT_WRITE_HOLD is cleared again.
    
    Link: https://lore.kernel.org/r/20211025152218.opvcqfku2lhqvp4o@linutronix.de
    Link: https://lore.kernel.org/r/20211125120711.dgbsienyrsxfzpoi@linutronix.de
    Acked-by: Christian Brauner <christian.brauner@ubuntu.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit c34f100590f1239811db576d9923e77b75f627db
Author: Lorenzo Bianconi <lorenzo@kernel.org>
Date:   Mon Sep 13 11:25:04 2021 +0200

    mt76: substitute sk_buff_head status_list with spinlock_t status_lock
    
    Substitute sk_buff_head status_list with spinlock_t status_lock since we
    just need it for locking
    
    Tested-by: mrkiko.rs@gmail.com
    Signed-off-by: Lorenzo Bianconi <lorenzo@kernel.org>
    Signed-off-by: Felix Fietkau <nbd@nbd.name>

commit b4c6f86ec2f648b5e6d4b04564fbc6d5351160a8
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Oct 7 11:26:46 2021 +0200

    irq_work: Handle some irq_work in a per-CPU thread on PREEMPT_RT
    
    The irq_work callback is invoked in hard IRQ context. By default all
    callbacks are scheduled for invocation right away (given supported by
    the architecture) except for the ones marked IRQ_WORK_LAZY which are
    delayed until the next timer-tick.
    
    While looking over the callbacks, some of them may acquire locks
    (spinlock_t, rwlock_t) which are transformed into sleeping locks on
    PREEMPT_RT and must not be acquired in hard IRQ context.
    Changing the locks into locks which could be acquired in this context
    will lead to other problems such as increased latencies if everything
    in the chain has IRQ-off locks. This will not solve all the issues as
    one callback has been noticed which invoked kref_put() and its callback
    invokes kfree() and this can not be invoked in hardirq context.
    
    Some callbacks are required to be invoked in hardirq context even on
    PREEMPT_RT to work properly. This includes for instance the NO_HZ
    callback which needs to be able to observe the idle context.
    
    The callbacks which require to be run in hardirq have already been
    marked. Use this information to split the callbacks onto the two lists
    on PREEMPT_RT:
    - lazy_list
      Work items which are not marked with IRQ_WORK_HARD_IRQ will be added
      to this list. Callbacks on this list will be invoked from a per-CPU
      thread.
      The handler here may acquire sleeping locks such as spinlock_t and
      invoke kfree().
    
    - raised_list
      Work items which are marked with IRQ_WORK_HARD_IRQ will be added to
      this list. They will be invoked in hardirq context and must not
      acquire any sleeping locks.
    
    The wake up of the per-CPU thread occurs from irq_work handler/
    hardirq context. The thread runs with lowest RT priority to ensure it
    runs before any SCHED_OTHER tasks do.
    
    [bigeasy: melt tglx's irq_work_tick_soft() which splits irq_work_tick() into a
              hard and soft variant. Collected fixes over time from Steven
              Rostedt and Mike Galbraith. Move to per-CPU threads instead of
              softirq as suggested by PeterZ.]
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20211007092646.uhshe3ut2wkrcfzv@linutronix.de

commit 97b31c1f8eb865bc3aa5f4a08286a6406d782ea8
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Oct 13 11:37:19 2021 +0200

    leds: trigger: Disable CPU trigger on PREEMPT_RT
    
    The CPU trigger is invoked on ARM from CPU-idle. That trigger later
    invokes led_trigger_event() which may invoke the callback of the actual driver.
    That driver can acquire a spinlock_t which is okay on kernel without
    PREEMPT_RT. On a PREEMPT_RT enabled kernel this lock is turned into a
    sleeping lock and must not be acquired with disabled interrupts.
    
    Disable the CPU trigger on PREEMPT_RT.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Link: https://lkml.kernel.org/r/20210924111501.m57cwwn7ahiyxxdd@linutronix.de
    Signed-off-by: Pavel Machek <pavel@ucw.cz>

commit f4a79b7d8bd6171ce60902a5d3ef9badec397ab3
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Aug 11 21:14:31 2021 +0100

    PM: cpu: Make notifier chain use a raw_spinlock_t
    
    [ Upstream commit b2f6662ac08d0e7c25574ce53623c71bdae9dd78 ]
    
    Invoking atomic_notifier_chain_notify() requires acquiring a spinlock_t,
    which can block under CONFIG_PREEMPT_RT. Notifications for members of the
    cpu_pm notification chain will be issued by the idle task, which can never
    block.
    
    Making *all* atomic_notifiers use a raw_spinlock is too big of a hammer, as
    only notifications issued by the idle task are problematic.
    
    Special-case cpu_pm_notifier_chain by kludging a raw_notifier and
    raw_spinlock_t together, matching the atomic_notifier behavior with a
    raw_spinlock_t.
    
    Fixes: 70d932985757 ("notifier: Fix broken error handling pattern")
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2f20c3cb46d97e97ef72d29b4794a3072f0a2e17
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Aug 11 21:14:31 2021 +0100

    PM: cpu: Make notifier chain use a raw_spinlock_t
    
    [ Upstream commit b2f6662ac08d0e7c25574ce53623c71bdae9dd78 ]
    
    Invoking atomic_notifier_chain_notify() requires acquiring a spinlock_t,
    which can block under CONFIG_PREEMPT_RT. Notifications for members of the
    cpu_pm notification chain will be issued by the idle task, which can never
    block.
    
    Making *all* atomic_notifiers use a raw_spinlock is too big of a hammer, as
    only notifications issued by the idle task are problematic.
    
    Special-case cpu_pm_notifier_chain by kludging a raw_notifier and
    raw_spinlock_t together, matching the atomic_notifier behavior with a
    raw_spinlock_t.
    
    Fixes: 70d932985757 ("notifier: Fix broken error handling pattern")
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 4b7874a32ec23cc4892e7c9ffac1dd8160ea3697
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Aug 11 21:14:31 2021 +0100

    PM: cpu: Make notifier chain use a raw_spinlock_t
    
    [ Upstream commit b2f6662ac08d0e7c25574ce53623c71bdae9dd78 ]
    
    Invoking atomic_notifier_chain_notify() requires acquiring a spinlock_t,
    which can block under CONFIG_PREEMPT_RT. Notifications for members of the
    cpu_pm notification chain will be issued by the idle task, which can never
    block.
    
    Making *all* atomic_notifiers use a raw_spinlock is too big of a hammer, as
    only notifications issued by the idle task are problematic.
    
    Special-case cpu_pm_notifier_chain by kludging a raw_notifier and
    raw_spinlock_t together, matching the atomic_notifier behavior with a
    raw_spinlock_t.
    
    Fixes: 70d932985757 ("notifier: Fix broken error handling pattern")
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit cc09ee80c3b18ae1a897a30a17fe710b2b2f620a
Merge: 49832c819ab8 bd0e7491a931
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 8 12:36:00 2021 -0700

    Merge tag 'mm-slub-5.15-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/vbabka/linux
    
    Pull SLUB updates from Vlastimil Babka:
     "SLUB: reduce irq disabled scope and make it RT compatible
    
      This series was initially inspired by Mel's pcplist local_lock
      rewrite, and also interest to better understand SLUB's locking and the
      new primitives and RT variants and implications. It makes SLUB
      compatible with PREEMPT_RT and generally more preemption-friendly,
      apparently without significant regressions, as the fast paths are not
      affected.
    
      The main changes to SLUB by this series:
    
       - irq disabling is now only done for minimum amount of time needed to
         protect the strict kmem_cache_cpu fields, and as part of spin lock,
         local lock and bit lock operations to make them irq-safe
    
       - SLUB is fully PREEMPT_RT compatible
    
      The series should now be sufficiently tested in both RT and !RT
      configs, mainly thanks to Mike.
    
      The RFC/v1 version also got basic performance screening by Mel that
      didn't show major regressions. Mike's testing with hackbench of v2 on
      !RT reported negligible differences [6]:
    
        virgin(ish) tip
        5.13.0.g60ab3ed-tip
                  7,320.67 msec task-clock                #    7.792 CPUs utilized            ( +-  0.31% )
                   221,215      context-switches          #    0.030 M/sec                    ( +-  3.97% )
                    16,234      cpu-migrations            #    0.002 M/sec                    ( +-  4.07% )
                    13,233      page-faults               #    0.002 M/sec                    ( +-  0.91% )
            27,592,205,252      cycles                    #    3.769 GHz                      ( +-  0.32% )
             8,309,495,040      instructions              #    0.30  insn per cycle           ( +-  0.37% )
             1,555,210,607      branches                  #  212.441 M/sec                    ( +-  0.42% )
                 5,484,209      branch-misses             #    0.35% of all branches          ( +-  2.13% )
    
                   0.93949 +- 0.00423 seconds time elapsed  ( +-  0.45% )
                   0.94608 +- 0.00384 seconds time elapsed  ( +-  0.41% ) (repeat)
                   0.94422 +- 0.00410 seconds time elapsed  ( +-  0.43% )
    
        5.13.0.g60ab3ed-tip +slub-local-lock-v2r3
                  7,343.57 msec task-clock                #    7.776 CPUs utilized            ( +-  0.44% )
                   223,044      context-switches          #    0.030 M/sec                    ( +-  3.02% )
                    16,057      cpu-migrations            #    0.002 M/sec                    ( +-  4.03% )
                    13,164      page-faults               #    0.002 M/sec                    ( +-  0.97% )
            27,684,906,017      cycles                    #    3.770 GHz                      ( +-  0.45% )
             8,323,273,871      instructions              #    0.30  insn per cycle           ( +-  0.28% )
             1,556,106,680      branches                  #  211.901 M/sec                    ( +-  0.31% )
                 5,463,468      branch-misses             #    0.35% of all branches          ( +-  1.33% )
    
                   0.94440 +- 0.00352 seconds time elapsed  ( +-  0.37% )
                   0.94830 +- 0.00228 seconds time elapsed  ( +-  0.24% ) (repeat)
                   0.93813 +- 0.00440 seconds time elapsed  ( +-  0.47% ) (repeat)
    
      RT configs showed some throughput regressions, but that's expected
      tradeoff for the preemption improvements through the RT mutex. It
      didn't prevent the v2 to be incorporated to the 5.13 RT tree [7],
      leading to testing exposure and bugfixes.
    
      Before the series, SLUB is lockless in both allocation and free fast
      paths, but elsewhere, it's disabling irqs for considerable periods of
      time - especially in allocation slowpath and the bulk allocation,
      where IRQs are re-enabled only when a new page from the page allocator
      is needed, and the context allows blocking. The irq disabled sections
      can then include deactivate_slab() which walks a full freelist and
      frees the slab back to page allocator or unfreeze_partials() going
      through a list of percpu partial slabs. The RT tree currently has some
      patches mitigating these, but we can do much better in mainline too.
    
      Patches 1-6 are straightforward improvements or cleanups that could
      exist outside of this series too, but are prerequsities.
    
      Patches 7-9 are also preparatory code changes without functional
      changes, but not so useful without the rest of the series.
    
      Patch 10 simplifies the fast paths on systems with preemption, based
      on (hopefully correct) observation that the current loops to verify
      tid are unnecessary.
    
      Patches 11-20 focus on reducing irq disabled scope in the allocation
      slowpath:
    
       - patch 11 moves disabling of irqs into ___slab_alloc() from its
         callers, which are the allocation slowpath, and bulk allocation.
         Instead these callers only disable preemption to stabilize the cpu.
    
       - The following patches then gradually reduce the scope of disabled
         irqs in ___slab_alloc() and the functions called from there. As of
         patch 14, the re-enabling of irqs based on gfp flags before calling
         the page allocator is removed from allocate_slab(). As of patch 17,
         it's possible to reach the page allocator (in case of existing
         slabs depleted) without disabling and re-enabling irqs a single
         time.
    
      Pathces 21-26 reduce the scope of disabled irqs in functions related
      to unfreezing percpu partial slab.
    
      Patch 27 is preparatory. Patch 28 is adopted from the RT tree and
      converts the flushing of percpu slabs on all cpus from using IPI to
      workqueue, so that the processing isn't happening with irqs disabled
      in the IPI handler. The flushing is not performance critical so it
      should be acceptable.
    
      Patch 29 also comes from RT tree and makes object_map_lock RT
      compatible.
    
      Patch 30 make slab_lock irq-safe on RT where we cannot rely on having
      irq disabled from the list_lock spin lock usage.
    
      Patch 31 changes kmem_cache_cpu->partial handling in put_cpu_partial()
      from cmpxchg loop to a short irq disabled section, which is used by
      all other code modifying the field. This addresses a theoretical race
      scenario pointed out by Jann, and makes the critical section safe wrt
      with RT local_lock semantics after the conversion in patch 35.
    
      Patch 32 changes preempt disable to migrate disable, so that the
      nested list_lock spinlock is safe to take on RT. Because
      migrate_disable() is a function call even on !RT, a small set of
      private wrappers is introduced to keep using the cheaper
      preempt_disable() on !PREEMPT_RT configurations. As of this patch,
      SLUB should be already compatible with RT's lock semantics.
    
      Finally, patch 33 changes irq disabled sections that protect
      kmem_cache_cpu fields in the slow paths, with a local lock. However on
      PREEMPT_RT it means the lockless fast paths can now preempt slow paths
      which don't expect that, so the local lock has to be taken also in the
      fast paths and they are no longer lockless. RT folks seem to not mind
      this tradeoff. The patch also updates the locking documentation in the
      file's comment"
    
    Mike Galbraith and Mel Gorman verified that their earlier testing
    observations still hold for the final series:
    
    Link: https://lore.kernel.org/lkml/89ba4f783114520c167cc915ba949ad2c04d6790.camel@gmx.de/
    Link: https://lore.kernel.org/lkml/20210907082010.GB3959@techsingularity.net/
    
    * tag 'mm-slub-5.15-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/vbabka/linux: (33 commits)
      mm, slub: convert kmem_cpu_slab protection to local_lock
      mm, slub: use migrate_disable() on PREEMPT_RT
      mm, slub: protect put_cpu_partial() with disabled irqs instead of cmpxchg
      mm, slub: make slab_lock() disable irqs with PREEMPT_RT
      mm: slub: make object_map_lock a raw_spinlock_t
      mm: slub: move flush_cpu_slab() invocations __free_slab() invocations out of IRQ context
      mm, slab: split out the cpu offline variant of flush_slab()
      mm, slub: don't disable irqs in slub_cpu_dead()
      mm, slub: only disable irq with spin_lock in __unfreeze_partials()
      mm, slub: separate detaching of partial list in unfreeze_partials() from unfreezing
      mm, slub: detach whole partial list at once in unfreeze_partials()
      mm, slub: discard slabs in unfreeze_partials() without irqs disabled
      mm, slub: move irq control into unfreeze_partials()
      mm, slub: call deactivate_slab() without disabling irqs
      mm, slub: make locking in deactivate_slab() irq-safe
      mm, slub: move reset of c->page and freelist out of deactivate_slab()
      mm, slub: stop disabling irqs around get_partial()
      mm, slub: check new pages with restored irqs
      mm, slub: validate slab from partial list or page allocator before making it cpu slab
      mm, slub: restore irqs around calling new_slab()
      ...

commit 94ef0304e2b8dc942f46c74a13841d6b61f61d2f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jul 16 18:47:50 2020 +0200

    mm: slub: make object_map_lock a raw_spinlock_t
    
    The variable object_map is protected by object_map_lock. The lock is always
    acquired in debug code and within already atomic context
    
    Make object_map_lock a raw_spinlock_t.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>

commit bf11b9a8e9a93c1fc0ebfc2929622d5cf7d43888
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Sep 2 14:54:03 2021 -0700

    shmem: use raw_spinlock_t for ->stat_lock
    
    Each CPU has SHMEM_INO_BATCH inodes available in `->ino_batch' which is
    per-CPU.  Access here is serialized by disabling preemption.  If the pool
    is empty, it gets reloaded from `->next_ino'.  Access here is serialized
    by ->stat_lock which is a spinlock_t and can not be acquired with disabled
    preemption.
    
    One way around it would make per-CPU ino_batch struct containing the inode
    number a local_lock_t.
    
    Another solution is to promote ->stat_lock to a raw_spinlock_t.  The
    critical sections are short.  The mpol_put() must be moved outside of the
    critical section to avoid invoking the destructor with disabled
    preemption.
    
    Link: https://lkml.kernel.org/r/20210806142916.jdwkb5bx62q5fwfo@linutronix.de
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 5cbba60596b1f32f637190ca9ed5b1acdadb852c
Merge: 9b2eacd8f046 fe583359ddf0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 31 13:21:58 2021 -0700

    Merge tag 'pm-5.15-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "These address some PCI device power management issues, add new
      hardware support to the RAPL power capping driver, add HWP guaranteed
      performance change notification support to the intel_pstate driver,
      replace deprecated CPU-hotplug functions in a few places, update CPU
      PM notifiers to use raw spinlocks, update the PM domains framework
      (new DT property support, Kconfig fix), do a couple of cleanups in
      code related to system sleep, and improve the energy model and the
      schedutil cpufreq governor.
    
      Specifics:
    
       - Address 3 PCI device power management issues (Rafael Wysocki).
    
       - Add Power Limit4 support for Alder Lake to the Intel RAPL power
         capping driver (Sumeet Pawnikar).
    
       - Add HWP guaranteed performance change notification support to the
         intel_pstate driver (Srinivas Pandruvada).
    
       - Replace deprecated CPU-hotplug functions in code related to power
         management (Sebastian Andrzej Siewior).
    
       - Update CPU PM notifiers to use raw spinlocks (Valentin Schneider).
    
       - Add support for 'required-opps' DT property to the generic power
         domains (genpd) framework and use this property for I2C on ARM64
         sc7180 (Rajendra Nayak).
    
       - Fix Kconfig issue related to genpd (Geert Uytterhoeven).
    
       - Increase energy calculation precision in the Energy Model (Lukasz
         Luba).
    
       - Fix kobject deletion in the exit code of the schedutil cpufreq
         governor (Kevin Hao).
    
       - Unmark some functions as kernel-doc in the PM core to avoid
         false-positive documentation build warnings (Randy Dunlap).
    
       - Check RTC features instead of ops in suspend_test Alexandre
         Belloni)"
    
    * tag 'pm-5.15-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm:
      PM: domains: Fix domain attach for CONFIG_PM_OPP=n
      powercap: Add Power Limit4 support for Alder Lake SoC
      cpufreq: intel_pstate: Process HWP Guaranteed change notification
      thermal: intel: Allow processing of HWP interrupt
      notifier: Remove atomic_notifier_call_chain_robust()
      PM: cpu: Make notifier chain use a raw_spinlock_t
      PM: sleep: unmark 'state' functions as kernel-doc
      arm64: dts: sc7180: Add required-opps for i2c
      PM: domains: Add support for 'required-opps' to set default perf state
      opp: Don't print an error if required-opps is missing
      cpufreq: schedutil: Use kobject release() method to free sugov_tunables
      PM: EM: Increase energy calculation precision
      PM: sleep: check RTC features instead of ops in suspend_test
      PM: sleep: s2idle: Replace deprecated CPU-hotplug functions
      cpufreq: Replace deprecated CPU-hotplug functions
      powercap: intel_rapl: Replace deprecated CPU-hotplug functions
      PCI: PM: Enable PME if it can be signaled from D3cold
      PCI: PM: Avoid forcing PCI_D0 for wakeup reasons inconsistently
      PCI: Use pci_update_current_state() in pci_enable_device_flags()

commit e5e726f7bb9f711102edea7e5bd511835640e3b4
Merge: 08403e2174c4 a055fcc132d4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 30 14:26:36 2021 -0700

    Merge tag 'locking-core-2021-08-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking and atomics updates from Thomas Gleixner:
     "The regular pile:
    
       - A few improvements to the mutex code
    
       - Documentation updates for atomics to clarify the difference between
         cmpxchg() and try_cmpxchg() and to explain the forward progress
         expectations.
    
       - Simplification of the atomics fallback generator
    
       - The addition of arch_atomic_long*() variants and generic arch_*()
         bitops based on them.
    
       - Add the missing might_sleep() invocations to the down*() operations
         of semaphores.
    
      The PREEMPT_RT locking core:
    
       - Scheduler updates to support the state preserving mechanism for
         'sleeping' spin- and rwlocks on RT.
    
         This mechanism is carefully preserving the state of the task when
         blocking on a 'sleeping' spin- or rwlock and takes regular wake-ups
         targeted at the same task into account. The preserved or updated
         (via a regular wakeup) state is restored when the lock has been
         acquired.
    
       - Restructuring of the rtmutex code so it can be utilized and
         extended for the RT specific lock variants.
    
       - Restructuring of the ww_mutex code to allow sharing of the ww_mutex
         specific functionality for rtmutex based ww_mutexes.
    
       - Header file disentangling to allow substitution of the regular lock
         implementations with the PREEMPT_RT variants without creating an
         unmaintainable #ifdef mess.
    
       - Shared base code for the PREEMPT_RT specific rw_semaphore and
         rwlock implementations.
    
         Contrary to the regular rw_semaphores and rwlocks the PREEMPT_RT
         implementation is writer unfair because it is infeasible to do
         priority inheritance on multiple readers. Experience over the years
         has shown that real-time workloads are not the typical workloads
         which are sensitive to writer starvation.
    
         The alternative solution would be to allow only a single reader
         which has been tried and discarded as it is a major bottleneck
         especially for mmap_sem. Aside of that many of the writer
         starvation critical usage sites have been converted to a writer
         side mutex/spinlock and RCU read side protections in the past
         decade so that the issue is less prominent than it used to be.
    
       - The actual rtmutex based lock substitutions for PREEMPT_RT enabled
         kernels which affect mutex, ww_mutex, rw_semaphore, spinlock_t and
         rwlock_t. The spin/rw_lock*() functions disable migration across
         the critical section to preserve the existing semantics vs per-CPU
         variables.
    
       - Rework of the futex REQUEUE_PI mechanism to handle the case of
         early wake-ups which interleave with a re-queue operation to
         prevent the situation that a task would be blocked on both the
         rtmutex associated to the outer futex and the rtmutex based hash
         bucket spinlock.
    
         While this situation cannot happen on !RT enabled kernels the
         changes make the underlying concurrency problems easier to
         understand in general. As a result the difference between !RT and
         RT kernels is reduced to the handling of waiting for the critical
         section. !RT kernels simply spin-wait as before and RT kernels
         utilize rcu_wait().
    
       - The substitution of local_lock for PREEMPT_RT with a spinlock which
         protects the critical section while staying preemptible. The CPU
         locality is established by disabling migration.
    
      The underlying concepts of this code have been in use in PREEMPT_RT for
      way more than a decade. The code has been refactored several times over
      the years and this final incarnation has been optimized once again to be
      as non-intrusive as possible, i.e. the RT specific parts are mostly
      isolated.
    
      It has been extensively tested in the 5.14-rt patch series and it has
      been verified that !RT kernels are not affected by these changes"
    
    * tag 'locking-core-2021-08-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (92 commits)
      locking/rtmutex: Return success on deadlock for ww_mutex waiters
      locking/rtmutex: Prevent spurious EDEADLK return caused by ww_mutexes
      locking/rtmutex: Dequeue waiter on ww_mutex deadlock
      locking/rtmutex: Dont dereference waiter lockless
      locking/semaphore: Add might_sleep() to down_*() family
      locking/ww_mutex: Initialize waiter.ww_ctx properly
      static_call: Update API documentation
      locking/local_lock: Add PREEMPT_RT support
      locking/spinlock/rt: Prepare for RT local_lock
      locking/rtmutex: Add adaptive spinwait mechanism
      locking/rtmutex: Implement equal priority lock stealing
      preempt: Adjust PREEMPT_LOCK_OFFSET for RT
      locking/rtmutex: Prevent lockdep false positive with PI futexes
      futex: Prevent requeue_pi() lock nesting issue on RT
      futex: Simplify handle_early_requeue_pi_wakeup()
      futex: Reorder sanity checks in futex_requeue()
      futex: Clarify comment in futex_requeue()
      futex: Restructure futex_requeue()
      futex: Correct the number of requeued waiters for PI
      futex: Remove bogus condition for requeue PI
      ...

commit 88e9c0bf1ca38b11b48b97b5821a7ac99d42e825
Merge: 7ee5fd12e8ca d0e936adbd22 15538a20579f 7fcc17d0cb12
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Aug 30 19:25:13 2021 +0200

    Merge branches 'pm-cpufreq', 'pm-cpu' and 'pm-em'
    
    * pm-cpufreq:
      cpufreq: intel_pstate: Process HWP Guaranteed change notification
      thermal: intel: Allow processing of HWP interrupt
      cpufreq: schedutil: Use kobject release() method to free sugov_tunables
      cpufreq: Replace deprecated CPU-hotplug functions
    
    * pm-cpu:
      notifier: Remove atomic_notifier_call_chain_robust()
      PM: cpu: Make notifier chain use a raw_spinlock_t
    
    * pm-em:
      PM: EM: Increase energy calculation precision

commit 051790eecc03aff6978763791d38c1daea94c2f8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Aug 15 23:28:22 2021 +0200

    locking/spinlock: Provide RT specific spinlock_t
    
    RT replaces spinlocks with a simple wrapper around an rtmutex, which turns
    spinlocks on RT into 'sleeping' spinlocks. The actual implementation of the
    spinlock API differs from a regular rtmutex, as it does neither handle
    timeouts nor signals and it is state preserving across the lock operation.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20210815211303.654230709@linutronix.de

commit cbcebf5bd3d056d7a0ae332118888d867ac346c0
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sun Aug 15 23:28:17 2021 +0200

    locking/lockdep: Reduce header dependencies in <linux/debug_locks.h>
    
    The inclusion of printk.h leads to a circular dependency if spinlock_t is
    based on rtmutexes on RT enabled kernels.
    
    Include only atomic.h (xchg()) and cache.h (__read_mostly) which is all
    what debug_locks.h requires.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20210815211303.484161136@linutronix.de

commit a403abbdc715986760821e67731d60ff65bde4bd
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sun Aug 15 23:28:16 2021 +0200

    locking/rtmutex: Prevent future include recursion hell
    
    rtmutex only needs raw_spinlock_t, but it includes spinlock_types.h, which
    is not a problem on an non RT enabled kernel.
    
    RT kernels substitute regular spinlocks with 'sleeping' spinlocks, which
    are based on rtmutexes, and therefore must be able to include rtmutex.h.
    
    Include <linux/spinlock_types_raw.h> instead.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20210815211303.428224188@linutronix.de

commit 4f084ca74c3f0eb321ab50e69afd27c8fcb96a99
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Aug 15 23:28:14 2021 +0200

    locking/spinlock: Split the lock types header, and move the raw types into <linux/spinlock_types_raw.h>
    
    Move raw_spinlock into its own file. Prepare for RT 'sleeping spinlocks', to
    avoid header recursion, as RT locks require rtmutex.h, which in turn requires
    the raw spinlock types.
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20210815211303.371269088@linutronix.de

commit b2f6662ac08d0e7c25574ce53623c71bdae9dd78
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Aug 11 21:14:31 2021 +0100

    PM: cpu: Make notifier chain use a raw_spinlock_t
    
    Invoking atomic_notifier_chain_notify() requires acquiring a spinlock_t,
    which can block under CONFIG_PREEMPT_RT. Notifications for members of the
    cpu_pm notification chain will be issued by the idle task, which can never
    block.
    
    Making *all* atomic_notifiers use a raw_spinlock is too big of a hammer, as
    only notifications issued by the idle task are problematic.
    
    Special-case cpu_pm_notifier_chain by kludging a raw_notifier and
    raw_spinlock_t together, matching the atomic_notifier behavior with a
    raw_spinlock_t.
    
    Fixes: 70d932985757 ("notifier: Fix broken error handling pattern")
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 9755a447ec42dacef80c201039935dbcb57e88b0
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Thu Jul 29 11:34:49 2021 +0530

    powerpc/pseries: Fix regression while building external modules
    
    commit 333cf507465fbebb3727f5b53e77538467df312a upstream.
    
    With commit c9f3401313a5 ("powerpc: Always enable queued spinlocks for
    64s, disable for others") CONFIG_PPC_QUEUED_SPINLOCKS is always
    enabled on ppc64le, external modules that use spinlock APIs are
    failing.
    
      ERROR: modpost: GPL-incompatible module XXX.ko uses GPL-only symbol 'shared_processor'
    
    Before the above commit, modules were able to build without any
    issues. Also this problem is not seen on other architectures. This
    problem can be workaround if CONFIG_UNINLINE_SPIN_UNLOCK is enabled in
    the config. However CONFIG_UNINLINE_SPIN_UNLOCK is not enabled by
    default and only enabled in certain conditions like
    CONFIG_DEBUG_SPINLOCKS is set in the kernel config.
    
      #include <linux/module.h>
      spinlock_t spLock;
    
      static int __init spinlock_test_init(void)
      {
              spin_lock_init(&spLock);
              spin_lock(&spLock);
              spin_unlock(&spLock);
              return 0;
      }
    
      static void __exit spinlock_test_exit(void)
      {
            printk("spinlock_test unloaded\n");
      }
      module_init(spinlock_test_init);
      module_exit(spinlock_test_exit);
    
      MODULE_DESCRIPTION ("spinlock_test");
      MODULE_LICENSE ("non-GPL");
      MODULE_AUTHOR ("Srikar Dronamraju");
    
    Given that spin locks are one of the basic facilities for module code,
    this effectively makes it impossible to build/load almost any non GPL
    modules on ppc64le.
    
    This was first reported at https://github.com/openzfs/zfs/issues/11172
    
    Currently shared_processor is exported as GPL only symbol.
    Fix this for parity with other architectures by exposing
    shared_processor to non-GPL modules too.
    
    Fixes: 14c73bd344da ("powerpc/vcpu: Assume dedicated processors as non-preempt")
    Cc: stable@vger.kernel.org # v5.5+
    Reported-by: marc.c.dionne@gmail.com
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20210729060449.292780-1-srikar@linux.vnet.ibm.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c14cee5bc466dd09918e2b749bcf2ba9babfb7d5
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Thu Jul 29 11:34:49 2021 +0530

    powerpc/pseries: Fix regression while building external modules
    
    commit 333cf507465fbebb3727f5b53e77538467df312a upstream.
    
    With commit c9f3401313a5 ("powerpc: Always enable queued spinlocks for
    64s, disable for others") CONFIG_PPC_QUEUED_SPINLOCKS is always
    enabled on ppc64le, external modules that use spinlock APIs are
    failing.
    
      ERROR: modpost: GPL-incompatible module XXX.ko uses GPL-only symbol 'shared_processor'
    
    Before the above commit, modules were able to build without any
    issues. Also this problem is not seen on other architectures. This
    problem can be workaround if CONFIG_UNINLINE_SPIN_UNLOCK is enabled in
    the config. However CONFIG_UNINLINE_SPIN_UNLOCK is not enabled by
    default and only enabled in certain conditions like
    CONFIG_DEBUG_SPINLOCKS is set in the kernel config.
    
      #include <linux/module.h>
      spinlock_t spLock;
    
      static int __init spinlock_test_init(void)
      {
              spin_lock_init(&spLock);
              spin_lock(&spLock);
              spin_unlock(&spLock);
              return 0;
      }
    
      static void __exit spinlock_test_exit(void)
      {
            printk("spinlock_test unloaded\n");
      }
      module_init(spinlock_test_init);
      module_exit(spinlock_test_exit);
    
      MODULE_DESCRIPTION ("spinlock_test");
      MODULE_LICENSE ("non-GPL");
      MODULE_AUTHOR ("Srikar Dronamraju");
    
    Given that spin locks are one of the basic facilities for module code,
    this effectively makes it impossible to build/load almost any non GPL
    modules on ppc64le.
    
    This was first reported at https://github.com/openzfs/zfs/issues/11172
    
    Currently shared_processor is exported as GPL only symbol.
    Fix this for parity with other architectures by exposing
    shared_processor to non-GPL modules too.
    
    Fixes: 14c73bd344da ("powerpc/vcpu: Assume dedicated processors as non-preempt")
    Cc: stable@vger.kernel.org # v5.5+
    Reported-by: marc.c.dionne@gmail.com
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20210729060449.292780-1-srikar@linux.vnet.ibm.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 16447b2f5c66b160b731ba3e521f846d03c030d0
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Thu Jul 29 11:34:49 2021 +0530

    powerpc/pseries: Fix regression while building external modules
    
    commit 333cf507465fbebb3727f5b53e77538467df312a upstream.
    
    With commit c9f3401313a5 ("powerpc: Always enable queued spinlocks for
    64s, disable for others") CONFIG_PPC_QUEUED_SPINLOCKS is always
    enabled on ppc64le, external modules that use spinlock APIs are
    failing.
    
      ERROR: modpost: GPL-incompatible module XXX.ko uses GPL-only symbol 'shared_processor'
    
    Before the above commit, modules were able to build without any
    issues. Also this problem is not seen on other architectures. This
    problem can be workaround if CONFIG_UNINLINE_SPIN_UNLOCK is enabled in
    the config. However CONFIG_UNINLINE_SPIN_UNLOCK is not enabled by
    default and only enabled in certain conditions like
    CONFIG_DEBUG_SPINLOCKS is set in the kernel config.
    
      #include <linux/module.h>
      spinlock_t spLock;
    
      static int __init spinlock_test_init(void)
      {
              spin_lock_init(&spLock);
              spin_lock(&spLock);
              spin_unlock(&spLock);
              return 0;
      }
    
      static void __exit spinlock_test_exit(void)
      {
            printk("spinlock_test unloaded\n");
      }
      module_init(spinlock_test_init);
      module_exit(spinlock_test_exit);
    
      MODULE_DESCRIPTION ("spinlock_test");
      MODULE_LICENSE ("non-GPL");
      MODULE_AUTHOR ("Srikar Dronamraju");
    
    Given that spin locks are one of the basic facilities for module code,
    this effectively makes it impossible to build/load almost any non GPL
    modules on ppc64le.
    
    This was first reported at https://github.com/openzfs/zfs/issues/11172
    
    Currently shared_processor is exported as GPL only symbol.
    Fix this for parity with other architectures by exposing
    shared_processor to non-GPL modules too.
    
    Fixes: 14c73bd344da ("powerpc/vcpu: Assume dedicated processors as non-preempt")
    Cc: stable@vger.kernel.org # v5.5+
    Reported-by: marc.c.dionne@gmail.com
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20210729060449.292780-1-srikar@linux.vnet.ibm.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 91bbeacf5a4c78176042aca62708b6bb5ab68a37
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Thu Jul 29 11:34:49 2021 +0530

    powerpc/pseries: Fix regression while building external modules
    
    commit 333cf507465fbebb3727f5b53e77538467df312a upstream.
    
    With commit c9f3401313a5 ("powerpc: Always enable queued spinlocks for
    64s, disable for others") CONFIG_PPC_QUEUED_SPINLOCKS is always
    enabled on ppc64le, external modules that use spinlock APIs are
    failing.
    
      ERROR: modpost: GPL-incompatible module XXX.ko uses GPL-only symbol 'shared_processor'
    
    Before the above commit, modules were able to build without any
    issues. Also this problem is not seen on other architectures. This
    problem can be workaround if CONFIG_UNINLINE_SPIN_UNLOCK is enabled in
    the config. However CONFIG_UNINLINE_SPIN_UNLOCK is not enabled by
    default and only enabled in certain conditions like
    CONFIG_DEBUG_SPINLOCKS is set in the kernel config.
    
      #include <linux/module.h>
      spinlock_t spLock;
    
      static int __init spinlock_test_init(void)
      {
              spin_lock_init(&spLock);
              spin_lock(&spLock);
              spin_unlock(&spLock);
              return 0;
      }
    
      static void __exit spinlock_test_exit(void)
      {
            printk("spinlock_test unloaded\n");
      }
      module_init(spinlock_test_init);
      module_exit(spinlock_test_exit);
    
      MODULE_DESCRIPTION ("spinlock_test");
      MODULE_LICENSE ("non-GPL");
      MODULE_AUTHOR ("Srikar Dronamraju");
    
    Given that spin locks are one of the basic facilities for module code,
    this effectively makes it impossible to build/load almost any non GPL
    modules on ppc64le.
    
    This was first reported at https://github.com/openzfs/zfs/issues/11172
    
    Currently shared_processor is exported as GPL only symbol.
    Fix this for parity with other architectures by exposing
    shared_processor to non-GPL modules too.
    
    Fixes: 14c73bd344da ("powerpc/vcpu: Assume dedicated processors as non-preempt")
    Cc: stable@vger.kernel.org # v5.5+
    Reported-by: marc.c.dionne@gmail.com
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20210729060449.292780-1-srikar@linux.vnet.ibm.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 333cf507465fbebb3727f5b53e77538467df312a
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Thu Jul 29 11:34:49 2021 +0530

    powerpc/pseries: Fix regression while building external modules
    
    With commit c9f3401313a5 ("powerpc: Always enable queued spinlocks for
    64s, disable for others") CONFIG_PPC_QUEUED_SPINLOCKS is always
    enabled on ppc64le, external modules that use spinlock APIs are
    failing.
    
      ERROR: modpost: GPL-incompatible module XXX.ko uses GPL-only symbol 'shared_processor'
    
    Before the above commit, modules were able to build without any
    issues. Also this problem is not seen on other architectures. This
    problem can be workaround if CONFIG_UNINLINE_SPIN_UNLOCK is enabled in
    the config. However CONFIG_UNINLINE_SPIN_UNLOCK is not enabled by
    default and only enabled in certain conditions like
    CONFIG_DEBUG_SPINLOCKS is set in the kernel config.
    
      #include <linux/module.h>
      spinlock_t spLock;
    
      static int __init spinlock_test_init(void)
      {
              spin_lock_init(&spLock);
              spin_lock(&spLock);
              spin_unlock(&spLock);
              return 0;
      }
    
      static void __exit spinlock_test_exit(void)
      {
            printk("spinlock_test unloaded\n");
      }
      module_init(spinlock_test_init);
      module_exit(spinlock_test_exit);
    
      MODULE_DESCRIPTION ("spinlock_test");
      MODULE_LICENSE ("non-GPL");
      MODULE_AUTHOR ("Srikar Dronamraju");
    
    Given that spin locks are one of the basic facilities for module code,
    this effectively makes it impossible to build/load almost any non GPL
    modules on ppc64le.
    
    This was first reported at https://github.com/openzfs/zfs/issues/11172
    
    Currently shared_processor is exported as GPL only symbol.
    Fix this for parity with other architectures by exposing
    shared_processor to non-GPL modules too.
    
    Fixes: 14c73bd344da ("powerpc/vcpu: Assume dedicated processors as non-preempt")
    Cc: stable@vger.kernel.org # v5.5+
    Reported-by: marc.c.dionne@gmail.com
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20210729060449.292780-1-srikar@linux.vnet.ibm.com

commit 10e2c4c399a60e627b1483c08ef5842fc1d3be51
Author: Maurizio Lombardi <mlombard@redhat.com>
Date:   Mon May 31 14:13:26 2021 +0200

    scsi: target: core: Fix warning on realtime kernels
    
    [ Upstream commit 515da6f4295c2c42b8c54572cce3d2dd1167c41e ]
    
    On realtime kernels, spin_lock_irq*(spinlock_t) do not disable the
    interrupts, a call to irqs_disabled() will return false thus firing a
    warning in __transport_wait_for_tasks().
    
    Remove the warning and also replace assert_spin_locked() with
    lockdep_assert_held()
    
    Link: https://lore.kernel.org/r/20210531121326.3649-1-mlombard@redhat.com
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Maurizio Lombardi <mlombard@redhat.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 02c1b02b69b5292dd3e7e9634b0fa21abab411e3
Author: Maurizio Lombardi <mlombard@redhat.com>
Date:   Mon May 31 14:13:26 2021 +0200

    scsi: target: core: Fix warning on realtime kernels
    
    [ Upstream commit 515da6f4295c2c42b8c54572cce3d2dd1167c41e ]
    
    On realtime kernels, spin_lock_irq*(spinlock_t) do not disable the
    interrupts, a call to irqs_disabled() will return false thus firing a
    warning in __transport_wait_for_tasks().
    
    Remove the warning and also replace assert_spin_locked() with
    lockdep_assert_held()
    
    Link: https://lore.kernel.org/r/20210531121326.3649-1-mlombard@redhat.com
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Maurizio Lombardi <mlombard@redhat.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 376a4d9cbdb6f666083a6c37eaf63b50ad18704c
Author: Maurizio Lombardi <mlombard@redhat.com>
Date:   Mon May 31 14:13:26 2021 +0200

    scsi: target: core: Fix warning on realtime kernels
    
    [ Upstream commit 515da6f4295c2c42b8c54572cce3d2dd1167c41e ]
    
    On realtime kernels, spin_lock_irq*(spinlock_t) do not disable the
    interrupts, a call to irqs_disabled() will return false thus firing a
    warning in __transport_wait_for_tasks().
    
    Remove the warning and also replace assert_spin_locked() with
    lockdep_assert_held()
    
    Link: https://lore.kernel.org/r/20210531121326.3649-1-mlombard@redhat.com
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Maurizio Lombardi <mlombard@redhat.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 8fa2bb8837ea92f725e63ec38935195bc0266668
Author: Maurizio Lombardi <mlombard@redhat.com>
Date:   Mon May 31 14:13:26 2021 +0200

    scsi: target: core: Fix warning on realtime kernels
    
    [ Upstream commit 515da6f4295c2c42b8c54572cce3d2dd1167c41e ]
    
    On realtime kernels, spin_lock_irq*(spinlock_t) do not disable the
    interrupts, a call to irqs_disabled() will return false thus firing a
    warning in __transport_wait_for_tasks().
    
    Remove the warning and also replace assert_spin_locked() with
    lockdep_assert_held()
    
    Link: https://lore.kernel.org/r/20210531121326.3649-1-mlombard@redhat.com
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Maurizio Lombardi <mlombard@redhat.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit b2802e78beca7c98805f20cbb4adb97c000703b1
Author: Srinivas Goud <srinivas.goud@xilinx.com>
Date:   Mon Mar 29 21:49:35 2021 +0530

    watchdog: of_xilinx_wdt: Add comment to spinlock
    
    Based on checkpatch every spinlock should be documented.
    The patch is fixing this issue:
    ./scripts/checkpatch.pl --strict -f drivers/watchdog/of_xilinx_wdt.c
    CHECK: spinlock_t definition without comment
    +       spinlock_t spinlock;
    
    Signed-off-by: Srinivas Goud <srinivas.goud@xilinx.com>
    Signed-off-by: Michal Simek <michal.simek@xilinx.com>
    Signed-off-by: Srinivas Neeli <srinivas.neeli@xilinx.com>
    Reviewed-by: Guenter Roeck <linux@roeck-us.net>
    Link: https://lore.kernel.org/r/20210329161939.37680-2-srinivas.neeli@xilinx.com
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Wim Van Sebroeck <wim@linux-watchdog.org>

commit 20822024c3942023a2d6167dcbd98d053e799bea
Author: Maurizio Lombardi <mlombard@redhat.com>
Date:   Mon May 31 14:13:26 2021 +0200

    scsi: target: core: Fix warning on realtime kernels
    
    [ Upstream commit 515da6f4295c2c42b8c54572cce3d2dd1167c41e ]
    
    On realtime kernels, spin_lock_irq*(spinlock_t) do not disable the
    interrupts, a call to irqs_disabled() will return false thus firing a
    warning in __transport_wait_for_tasks().
    
    Remove the warning and also replace assert_spin_locked() with
    lockdep_assert_held()
    
    Link: https://lore.kernel.org/r/20210531121326.3649-1-mlombard@redhat.com
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Maurizio Lombardi <mlombard@redhat.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 5d5f0d945d475b01e0d81894345f0b36acaf4d02
Author: Maurizio Lombardi <mlombard@redhat.com>
Date:   Mon May 31 14:13:26 2021 +0200

    scsi: target: core: Fix warning on realtime kernels
    
    [ Upstream commit 515da6f4295c2c42b8c54572cce3d2dd1167c41e ]
    
    On realtime kernels, spin_lock_irq*(spinlock_t) do not disable the
    interrupts, a call to irqs_disabled() will return false thus firing a
    warning in __transport_wait_for_tasks().
    
    Remove the warning and also replace assert_spin_locked() with
    lockdep_assert_held()
    
    Link: https://lore.kernel.org/r/20210531121326.3649-1-mlombard@redhat.com
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Maurizio Lombardi <mlombard@redhat.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit ea4a9a34c9b2bdb91053f7edaa0713ff481c7bc6
Author: Maurizio Lombardi <mlombard@redhat.com>
Date:   Mon May 31 14:13:26 2021 +0200

    scsi: target: core: Fix warning on realtime kernels
    
    [ Upstream commit 515da6f4295c2c42b8c54572cce3d2dd1167c41e ]
    
    On realtime kernels, spin_lock_irq*(spinlock_t) do not disable the
    interrupts, a call to irqs_disabled() will return false thus firing a
    warning in __transport_wait_for_tasks().
    
    Remove the warning and also replace assert_spin_locked() with
    lockdep_assert_held()
    
    Link: https://lore.kernel.org/r/20210531121326.3649-1-mlombard@redhat.com
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Maurizio Lombardi <mlombard@redhat.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 515da6f4295c2c42b8c54572cce3d2dd1167c41e
Author: Maurizio Lombardi <mlombard@redhat.com>
Date:   Mon May 31 14:13:26 2021 +0200

    scsi: target: core: Fix warning on realtime kernels
    
    On realtime kernels, spin_lock_irq*(spinlock_t) do not disable the
    interrupts, a call to irqs_disabled() will return false thus firing a
    warning in __transport_wait_for_tasks().
    
    Remove the warning and also replace assert_spin_locked() with
    lockdep_assert_held()
    
    Link: https://lore.kernel.org/r/20210531121326.3649-1-mlombard@redhat.com
    Reviewed-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Maurizio Lombardi <mlombard@redhat.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit 77de34b9f5029d68a47c00d9b462e425c546d679
Author: Luca Fancellu <luca.fancellu@arm.com>
Date:   Tue Apr 6 11:51:04 2021 +0100

    xen/evtchn: Change irq_info lock to raw_spinlock_t
    
    commit d120198bd5ff1d41808b6914e1eb89aff937415c upstream.
    
    Unmask operation must be called with interrupt disabled,
    on preempt_rt spin_lock_irqsave/spin_unlock_irqrestore
    don't disable/enable interrupts, so use raw_* implementation
    and change lock variable in struct irq_info from spinlock_t
    to raw_spinlock_t
    
    Cc: stable@vger.kernel.org
    Fixes: 25da4618af24 ("xen/events: don't unmask an event channel when an eoi is pending")
    Signed-off-by: Luca Fancellu <luca.fancellu@arm.com>
    Reviewed-by: Julien Grall <jgrall@amazon.com>
    Reviewed-by: Wei Liu <wei.liu@kernel.org>
    Link: https://lore.kernel.org/r/20210406105105.10141-1-luca.fancellu@arm.com
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit af48f1856d655a3abe9704f9e13532a21dd28ce3
Author: Luca Fancellu <luca.fancellu@arm.com>
Date:   Tue Apr 6 11:51:04 2021 +0100

    xen/evtchn: Change irq_info lock to raw_spinlock_t
    
    commit d120198bd5ff1d41808b6914e1eb89aff937415c upstream.
    
    Unmask operation must be called with interrupt disabled,
    on preempt_rt spin_lock_irqsave/spin_unlock_irqrestore
    don't disable/enable interrupts, so use raw_* implementation
    and change lock variable in struct irq_info from spinlock_t
    to raw_spinlock_t
    
    Cc: stable@vger.kernel.org
    Fixes: 25da4618af24 ("xen/events: don't unmask an event channel when an eoi is pending")
    Signed-off-by: Luca Fancellu <luca.fancellu@arm.com>
    Reviewed-by: Julien Grall <jgrall@amazon.com>
    Reviewed-by: Wei Liu <wei.liu@kernel.org>
    Link: https://lore.kernel.org/r/20210406105105.10141-1-luca.fancellu@arm.com
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c9697faba3a144454d1f0aac47d27d150c0bc2cc
Author: Luca Fancellu <luca.fancellu@arm.com>
Date:   Tue Apr 6 11:51:04 2021 +0100

    xen/evtchn: Change irq_info lock to raw_spinlock_t
    
    commit d120198bd5ff1d41808b6914e1eb89aff937415c upstream.
    
    Unmask operation must be called with interrupt disabled,
    on preempt_rt spin_lock_irqsave/spin_unlock_irqrestore
    don't disable/enable interrupts, so use raw_* implementation
    and change lock variable in struct irq_info from spinlock_t
    to raw_spinlock_t
    
    Cc: stable@vger.kernel.org
    Fixes: 25da4618af24 ("xen/events: don't unmask an event channel when an eoi is pending")
    Signed-off-by: Luca Fancellu <luca.fancellu@arm.com>
    Reviewed-by: Julien Grall <jgrall@amazon.com>
    Reviewed-by: Wei Liu <wei.liu@kernel.org>
    Link: https://lore.kernel.org/r/20210406105105.10141-1-luca.fancellu@arm.com
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 108b525bc87a6c337cd6582dad04fe11716d28a0
Author: Luca Fancellu <luca.fancellu@arm.com>
Date:   Tue Apr 6 11:51:04 2021 +0100

    xen/evtchn: Change irq_info lock to raw_spinlock_t
    
    commit d120198bd5ff1d41808b6914e1eb89aff937415c upstream.
    
    Unmask operation must be called with interrupt disabled,
    on preempt_rt spin_lock_irqsave/spin_unlock_irqrestore
    don't disable/enable interrupts, so use raw_* implementation
    and change lock variable in struct irq_info from spinlock_t
    to raw_spinlock_t
    
    Cc: stable@vger.kernel.org
    Fixes: 25da4618af24 ("xen/events: don't unmask an event channel when an eoi is pending")
    Signed-off-by: Luca Fancellu <luca.fancellu@arm.com>
    Reviewed-by: Julien Grall <jgrall@amazon.com>
    Reviewed-by: Wei Liu <wei.liu@kernel.org>
    Link: https://lore.kernel.org/r/20210406105105.10141-1-luca.fancellu@arm.com
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 005c5afa9f8557b92df6708478bb2729f523af33
Author: Luca Fancellu <luca.fancellu@arm.com>
Date:   Tue Apr 6 11:51:04 2021 +0100

    xen/evtchn: Change irq_info lock to raw_spinlock_t
    
    commit d120198bd5ff1d41808b6914e1eb89aff937415c upstream.
    
    Unmask operation must be called with interrupt disabled,
    on preempt_rt spin_lock_irqsave/spin_unlock_irqrestore
    don't disable/enable interrupts, so use raw_* implementation
    and change lock variable in struct irq_info from spinlock_t
    to raw_spinlock_t
    
    Cc: stable@vger.kernel.org
    Fixes: 25da4618af24 ("xen/events: don't unmask an event channel when an eoi is pending")
    Signed-off-by: Luca Fancellu <luca.fancellu@arm.com>
    Reviewed-by: Julien Grall <jgrall@amazon.com>
    Reviewed-by: Wei Liu <wei.liu@kernel.org>
    Link: https://lore.kernel.org/r/20210406105105.10141-1-luca.fancellu@arm.com
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a09acbb53934fab5ebfcf97e4f967327abd0c121
Author: Luca Fancellu <luca.fancellu@arm.com>
Date:   Tue Apr 6 11:51:04 2021 +0100

    xen/evtchn: Change irq_info lock to raw_spinlock_t
    
    commit d120198bd5ff1d41808b6914e1eb89aff937415c upstream.
    
    Unmask operation must be called with interrupt disabled,
    on preempt_rt spin_lock_irqsave/spin_unlock_irqrestore
    don't disable/enable interrupts, so use raw_* implementation
    and change lock variable in struct irq_info from spinlock_t
    to raw_spinlock_t
    
    Cc: stable@vger.kernel.org
    Fixes: 25da4618af24 ("xen/events: don't unmask an event channel when an eoi is pending")
    Signed-off-by: Luca Fancellu <luca.fancellu@arm.com>
    Reviewed-by: Julien Grall <jgrall@amazon.com>
    Reviewed-by: Wei Liu <wei.liu@kernel.org>
    Link: https://lore.kernel.org/r/20210406105105.10141-1-luca.fancellu@arm.com
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit df720c5687aeae1f99ded448d26893e288702ff3
Author: Luca Fancellu <luca.fancellu@arm.com>
Date:   Tue Apr 6 11:51:04 2021 +0100

    xen/evtchn: Change irq_info lock to raw_spinlock_t
    
    commit d120198bd5ff1d41808b6914e1eb89aff937415c upstream.
    
    Unmask operation must be called with interrupt disabled,
    on preempt_rt spin_lock_irqsave/spin_unlock_irqrestore
    don't disable/enable interrupts, so use raw_* implementation
    and change lock variable in struct irq_info from spinlock_t
    to raw_spinlock_t
    
    Cc: stable@vger.kernel.org
    Fixes: 25da4618af24 ("xen/events: don't unmask an event channel when an eoi is pending")
    Signed-off-by: Luca Fancellu <luca.fancellu@arm.com>
    Reviewed-by: Julien Grall <jgrall@amazon.com>
    Reviewed-by: Wei Liu <wei.liu@kernel.org>
    Link: https://lore.kernel.org/r/20210406105105.10141-1-luca.fancellu@arm.com
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0b76088799cc8db1ea9c626e54e7bc65f605e65f
Merge: ccd6c35c72c7 d120198bd5ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 9 09:58:42 2021 -0700

    Merge tag 'for-linus-5.12b-rc7-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen fix from Juergen Gross:
     "A single fix of a 5.12 patch for the rather uncommon problem of
      running as a Xen guest with a real time kernel config"
    
    * tag 'for-linus-5.12b-rc7-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/evtchn: Change irq_info lock to raw_spinlock_t

commit d120198bd5ff1d41808b6914e1eb89aff937415c
Author: Luca Fancellu <luca.fancellu@arm.com>
Date:   Tue Apr 6 11:51:04 2021 +0100

    xen/evtchn: Change irq_info lock to raw_spinlock_t
    
    Unmask operation must be called with interrupt disabled,
    on preempt_rt spin_lock_irqsave/spin_unlock_irqrestore
    don't disable/enable interrupts, so use raw_* implementation
    and change lock variable in struct irq_info from spinlock_t
    to raw_spinlock_t
    
    Cc: stable@vger.kernel.org
    Fixes: 25da4618af24 ("xen/events: don't unmask an event channel when an eoi is pending")
    Signed-off-by: Luca Fancellu <luca.fancellu@arm.com>
    Reviewed-by: Julien Grall <jgrall@amazon.com>
    Reviewed-by: Wei Liu <wei.liu@kernel.org>
    Link: https://lore.kernel.org/r/20210406105105.10141-1-luca.fancellu@arm.com
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

commit 7dd8ed09430465d137330e0810a2a90e06770898
Author: Vineeth Vijayan <vneethv@linux.ibm.com>
Date:   Tue Mar 30 20:46:57 2021 +0200

    s390: use DEFINE_SPINLOCK for initialization
    
    For static initialization of spinlock_t variable, use DEFINE_SPINLOCK
    instead of explicitly calling spin_lock_init().
    
    Signed-off-by: Vineeth Vijayan <vneethv@linux.ibm.com>
    Signed-off-by: Heiko Carstens <hca@linux.ibm.com>

commit 263df6e485445aff8f6189c1913b916b8c7f4f1d
Author: Heiko Carstens <hca@linux.ibm.com>
Date:   Mon Mar 22 13:44:47 2021 +0100

    s390/spinlock: remove align attribute from arch_spinlock_t
    
    No need to add an align attribute for an integer.
    The alignment is correct anyway.
    
    Signed-off-by: Heiko Carstens <hca@linux.ibm.com>

commit 773d5be7907f51c219b677423e46eb69737556c9
Author: Marco Cesati <marcocesati@gmail.com>
Date:   Wed Mar 24 13:44:52 2021 +0100

    Staging: rtl8723bs: remove named enums in odm_types.h
    
    Remove the following unnecessary enum definition in
    hal/odm_types.h:
    
            enum rt_spinlock_type
    
    Signed-off-by: Marco Cesati <marcocesati@gmail.com>
    Link: https://lore.kernel.org/r/20210324124456.25221-30-marcocesati@gmail.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bc8e0adff343d992ca76d871e8b5e6cb86e1fad9
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Tue Mar 16 11:56:30 2021 +0100

    net: xfrm: Use sequence counter with associated spinlock
    
    A sequence counter write section must be serialized or its internal
    state can get corrupted. A plain seqcount_t does not contain the
    information of which lock must be held to guaranteee write side
    serialization.
    
    For xfrm_state_hash_generation, use seqcount_spinlock_t instead of plain
    seqcount_t.  This allows to associate the spinlock used for write
    serialization with the sequence counter. It thus enables lockdep to
    verify that the write serialization lock is indeed held before entering
    the sequence counter write section.
    
    If lockdep is disabled, this lock association is compiled out and has
    neither storage size nor runtime overhead.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Steffen Klassert <steffen.klassert@secunet.com>

commit 00d5865c960a6de5a8b7cbdcd24d3c33cc307c0a
Author: Marco Cesati <marcocesati@gmail.com>
Date:   Fri Mar 12 09:26:33 2021 +0100

    staging: rtl8723bs: remove typedefs in osdep_service_linux.h
    
    This commit fixes the following checkpatch.pl warnings:
    
        WARNING: do not add new typedefs
        #43: FILE: include/osdep_service_linux.h:43:
        +   typedef spinlock_t      _lock;
    
        WARNING: do not add new typedefs
        #44: FILE: include/osdep_service_linux.h:44:
        +   typedef struct mutex            _mutex;
    
        WARNING: do not add new typedefs
        #45: FILE: include/osdep_service_linux.h:45:
        +   typedef struct timer_list _timer;
    
        WARNING: do not add new typedefs
        #52: FILE: include/osdep_service_linux.h:52:
        +   typedef struct sk_buff  _pkt;
    
        WARNING: do not add new typedefs
        #53: FILE: include/osdep_service_linux.h:53:
        +   typedef unsigned char _buffer;
    
        WARNING: do not add new typedefs
        #55: FILE: include/osdep_service_linux.h:55:
        +   typedef int     _OS_STATUS;
    
        WARNING: do not add new typedefs
        #57: FILE: include/osdep_service_linux.h:57:
        +   typedef unsigned long _irqL;
    
        WARNING: do not add new typedefs
        #58: FILE: include/osdep_service_linux.h:58:
        +   typedef struct  net_device * _nic_hdl;
    
        WARNING: do not add new typedefs
        #62: FILE: include/osdep_service_linux.h:62:
        +   typedef void timer_hdl_return;
    
        WARNING: do not add new typedefs
        #63: FILE: include/osdep_service_linux.h:63:
        +   typedef void* timer_hdl_context;
    
        WARNING: do not add new typedefs
        #65: FILE: include/osdep_service_linux.h:65:
        +   typedef struct work_struct _workitem;
    
    Signed-off-by: Marco Cesati <marco.cesati@gmail.com>
    Link: https://lore.kernel.org/r/20210312082638.25512-29-marco.cesati@gmail.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4bb9f2e48299d068a704f490c1be4b1fb6d278ce
Author: Lijun Pan <lijunp213@gmail.com>
Date:   Thu Feb 11 00:43:24 2021 -0600

    ibmvnic: remove unused spinlock_t stats_lock definition
    
    stats_lock is no longer used. So remove it.
    
    Signed-off-by: Lijun Pan <lijunp213@gmail.com>
    Reviewed-by: Saeed Mahameed <saeedm@nvidia.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a369d96ca55401c29ca05a41c3aa15d65948c599
Author: Lijun Pan <lijunp213@gmail.com>
Date:   Thu Feb 11 00:43:23 2021 -0600

    ibmvnic: add comments for spinlock_t definitions
    
    There are several spinlock_t definitions without comments.
    Add them.
    
    Signed-off-by: Lijun Pan <lijunp213@gmail.com>
    Reviewed-by: Saeed Mahameed <saeedm@nvidia.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a14e3caaaa72e9c5c91e823dde3383122215207d
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Feb 8 20:33:47 2021 +0100

    RDMA/qedr: Remove in_irq() usage from debug output
    
    qedr_gsi_post_send() has a debug output which prints the return value of
    in_irq() and irqs_disabled().
    
    The result of the in_irq(), even if invoked from an interrupt handler, is
    subject to change depending on the `threadirqs' command line switch.  The
    result of irqs_disabled() is always be 1 because the function acquires
    spinlock_t with spin_lock_irqsave().
    
    Remove in_irq() and irqs_disabled() from the debug output because it
    provides little value.
    
    Link: https://lore.kernel.org/r/20210208193347.383254-1-bigeasy@linutronix.de
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Jason Gunthorpe <jgg@nvidia.com>

commit ef1e21503cc41937b53d436c8f744ded95ab954b
Author: Chanho Park <chanho61.park@samsung.com>
Date:   Wed Jan 27 09:16:31 2021 +0900

    pinctrl: samsung: use raw_spinlock for s3c64xx
    
    Convert spin_[lock|unlock] functions of pin bank to
    raw_spinlock to support preempt-rt for pinctrl-s3c64xx. Below patch
    converted spinlock_t to raw_spinlock_t but it didn't convert the
    s3c64xx's spinlock.
    
    Fixes: 1f306ecbe0f6 ("pinctrl: samsung: use raw_spinlock for locking")
    Cc: Tomasz Figa <tomasz.figa@gmail.com>
    Cc: Krzysztof Kozlowski <krzk@kernel.org>
    Cc: Sylwester Nawrocki <s.nawrocki@samsung.com>
    Cc: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Chanho Park <chanho61.park@samsung.com>
    Link: https://lore.kernel.org/r/20210127001631.91209-1-chanho61.park@samsung.com
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit b491e6a7391e3ecdebdd7a097550195cc878924a
Author: Xie He <xie.he.0141@gmail.com>
Date:   Mon Jan 25 20:09:39 2021 -0800

    net: lapb: Add locking to the lapb module
    
    In the lapb module, the timers may run concurrently with other code in
    this module, and there is currently no locking to prevent the code from
    racing on "struct lapb_cb". This patch adds locking to prevent racing.
    
    1. Add "spinlock_t lock" to "struct lapb_cb"; Add "spin_lock_bh" and
    "spin_unlock_bh" to APIs, timer functions and notifier functions.
    
    2. Add "bool t1timer_stop, t2timer_stop" to "struct lapb_cb" to make us
    able to ask running timers to abort; Modify "lapb_stop_t1timer" and
    "lapb_stop_t2timer" to make them able to abort running timers;
    Modify "lapb_t2timer_expiry" and "lapb_t1timer_expiry" to make them
    abort after they are stopped by "lapb_stop_t1timer", "lapb_stop_t2timer",
    and "lapb_start_t1timer", "lapb_start_t2timer".
    
    3. Let lapb_unregister wait for other API functions and running timers
    to stop.
    
    4. The lapb_device_event function calls lapb_disconnect_request. In
    order to avoid trying to hold the lock twice, add a new function named
    "__lapb_disconnect_request" which assumes the lock is held, and make
    it called by lapb_disconnect_request and lapb_device_event.
    
    Fixes: 1da177e4c3f4 ("Linux-2.6.12-rc2")
    Cc: Martin Schiller <ms@dev.tdt.de>
    Signed-off-by: Xie He <xie.he.0141@gmail.com>
    Link: https://lore.kernel.org/r/20210126040939.69995-1-xie.he.0141@gmail.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>

commit 1f306ecbe0f66681bd87a2bb9013630233a32f7f
Author: Chanho Park <chanho61.park@samsung.com>
Date:   Thu Jan 21 12:00:09 2021 +0900

    pinctrl: samsung: use raw_spinlock for locking
    
    This patch converts spin_[lock|unlock] functions of pin bank to
    raw_spinlock to support preempt-rt. This can avoid BUG() assertion when
    irqchip callbacks are triggerred. Spinlocks can be converted rt_mutex
    which is preemptible when we apply preempt-rt patches.
    
    According to "Documentation/driver-api/gpio/driver.rst",
    
    "Realtime considerations: a realtime compliant GPIO driver should not
    use spinlock_t or any sleepable APIs (like PM runtime) as part of its
    irqchip implementation.
    
    - spinlock_t should be replaced with raw_spinlock_t.[1]
    "
    
    Cc: Tomasz Figa <tomasz.figa@gmail.com>
    Cc: Krzysztof Kozlowski <krzk@kernel.org>
    Cc: Sylwester Nawrocki <s.nawrocki@samsung.com>
    Cc: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Chanho Park <chanho61.park@samsung.com>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Reviewed-by: Krzysztof Kozlowski <krzk@kernel.org>
    Link: https://lore.kernel.org/r/20210121030009.25673-1-chanho61.park@samsung.com
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 9b81af9c845559f8cf9fad2dfc377e548be6da59
Author: Uladzislau Rezki (Sony) <urezki@gmail.com>
Date:   Thu Oct 29 17:50:04 2020 +0100

    rcu/tree: Defer kvfree_rcu() allocation to a clean context
    
    [ Upstream commit 56292e8609e39537297a7468dda4d87b9bd81d6a ]
    
    The current memmory-allocation interface causes the following difficulties
    for kvfree_rcu():
    
    a) If built with CONFIG_PROVE_RAW_LOCK_NESTING, the lockdep will
       complain about violation of the nesting rules, as in "BUG: Invalid
       wait context".  This Kconfig option checks for proper raw_spinlock
       vs. spinlock nesting, in particular, it is not legal to acquire a
       spinlock_t while holding a raw_spinlock_t.
    
       This is a problem because kfree_rcu() uses raw_spinlock_t whereas the
       "page allocator" internally deals with spinlock_t to access to its
       zones. The code also can be broken from higher level of view:
       <snip>
           raw_spin_lock(&some_lock);
           kfree_rcu(some_pointer, some_field_offset);
       <snip>
    
    b) If built with CONFIG_PREEMPT_RT, spinlock_t is converted into
       sleeplock.  This means that invoking the page allocator from atomic
       contexts results in "BUG: scheduling while atomic".
    
    c) Please note that call_rcu() is already invoked from raw atomic context,
       so it is only reasonable to expaect that kfree_rcu() and kvfree_rcu()
       will also be called from atomic raw context.
    
    This commit therefore defers page allocation to a clean context using the
    combination of an hrtimer and a workqueue.  The hrtimer stage is required
    in order to avoid deadlocks with the scheduler.  This deferred allocation
    is required only when kvfree_rcu()'s per-CPU page cache is empty.
    
    Link: https://lore.kernel.org/lkml/20200630164543.4mdcf6zb4zfclhln@linutronix.de/
    Fixes: 3042f83f19be ("rcu: Support reclaim for head-less object")
    Reported-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 45e3d5a2af1d53164cc5fbd22c5ceea0d163ad45
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Dec 15 20:43:13 2020 -0800

    lib/test_lockup.c: minimum fix to get it compiled on PREEMPT_RT
    
    On PREEMPT_RT the locks are quite different so they can't be tested as it
    is done below.  The alternative is to test for the waitlock within
    rtmutex.
    
    This is the bare minimun to get it compiled.  Problems which exist on
    PREEMP_RT:
    
     - none of the locks (spinlock_t, rwlock_t, mutex_t, rw_semaphore) may
       be acquired with disabled preemption or interrupts.
    
       If I read the code correct the it is possible to acquire a mutex_t
       with disabled interrupts.
    
       I don't know how to obtain a lock pointer. Technically they are not
       exported to userland.
    
     - memory can not be allocated with disabled preemption or interrupts
       even with GFP_ATOMIC.
    
    Link: https://lkml.kernel.org/r/20201028181041.xyeothhkouc3p4md@linutronix.de
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit dc516164869300efd0bdbf6f894defc306588b75
Author: David Sterba <dsterba@suse.com>
Date:   Thu Oct 29 15:33:45 2020 +0100

    btrfs: reorder extent buffer members for better packing
    
    After the rwsem replaced the tree lock implementation, the extent buffer
    got smaller but leaving some holes behind. By changing log_index type
    and reordering, we can squeeze the size further to 240 bytes, measured on
    release config on x86_64. Log_index spans only 3 values and needs to be
    signed.
    
    Before:
    
    struct extent_buffer {
            u64                        start;                /*     0     8 */
            long unsigned int          len;                  /*     8     8 */
            long unsigned int          bflags;               /*    16     8 */
            struct btrfs_fs_info *     fs_info;              /*    24     8 */
            spinlock_t                 refs_lock;            /*    32     4 */
            atomic_t                   refs;                 /*    36     4 */
            atomic_t                   io_pages;             /*    40     4 */
            int                        read_mirror;          /*    44     4 */
            struct callback_head       callback_head __attribute__((__aligned__(8))); /*    48    16 */
            /* --- cacheline 1 boundary (64 bytes) --- */
            pid_t                      lock_owner;           /*    64     4 */
            bool                       lock_recursed;        /*    68     1 */
    
            /* XXX 3 bytes hole, try to pack */
    
            struct rw_semaphore        lock;                 /*    72    40 */
            short int                  log_index;            /*   112     2 */
    
            /* XXX 6 bytes hole, try to pack */
    
            struct page *              pages[16];            /*   120   128 */
    
            /* size: 248, cachelines: 4, members: 14 */
            /* sum members: 239, holes: 2, sum holes: 9 */
            /* forced alignments: 1 */
            /* last cacheline: 56 bytes */
    } __attribute__((__aligned__(8)));
    
    After:
    
    struct extent_buffer {
            u64                        start;                /*     0     8 */
            long unsigned int          len;                  /*     8     8 */
            long unsigned int          bflags;               /*    16     8 */
            struct btrfs_fs_info *     fs_info;              /*    24     8 */
            spinlock_t                 refs_lock;            /*    32     4 */
            atomic_t                   refs;                 /*    36     4 */
            atomic_t                   io_pages;             /*    40     4 */
            int                        read_mirror;          /*    44     4 */
            struct callback_head       callback_head __attribute__((__aligned__(8))); /*    48    16 */
            /* --- cacheline 1 boundary (64 bytes) --- */
            pid_t                      lock_owner;           /*    64     4 */
            bool                       lock_recursed;        /*    68     1 */
            s8                         log_index;            /*    69     1 */
    
            /* XXX 2 bytes hole, try to pack */
    
            struct rw_semaphore        lock;                 /*    72    40 */
            struct page *              pages[16];            /*   112   128 */
    
            /* size: 240, cachelines: 4, members: 14 */
            /* sum members: 238, holes: 1, sum holes: 2 */
            /* forced alignments: 1 */
            /* last cacheline: 48 bytes */
    } __attribute__((__aligned__(8)));
    
    Reviewed-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit a2e9ae58d5042b3aa4a61f676ff6975ff3bc7bc7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Oct 30 12:37:43 2020 +0100

    lockdep/selftests: Fix PROVE_RAW_LOCK_NESTING
    
    The selftest nests rwlock_t inside raw_spinlock_t, this is invalid.
    
    Reported-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>

commit 56292e8609e39537297a7468dda4d87b9bd81d6a
Author: Uladzislau Rezki (Sony) <urezki@gmail.com>
Date:   Thu Oct 29 17:50:04 2020 +0100

    rcu/tree: Defer kvfree_rcu() allocation to a clean context
    
    The current memmory-allocation interface causes the following difficulties
    for kvfree_rcu():
    
    a) If built with CONFIG_PROVE_RAW_LOCK_NESTING, the lockdep will
       complain about violation of the nesting rules, as in "BUG: Invalid
       wait context".  This Kconfig option checks for proper raw_spinlock
       vs. spinlock nesting, in particular, it is not legal to acquire a
       spinlock_t while holding a raw_spinlock_t.
    
       This is a problem because kfree_rcu() uses raw_spinlock_t whereas the
       "page allocator" internally deals with spinlock_t to access to its
       zones. The code also can be broken from higher level of view:
       <snip>
           raw_spin_lock(&some_lock);
           kfree_rcu(some_pointer, some_field_offset);
       <snip>
    
    b) If built with CONFIG_PREEMPT_RT, spinlock_t is converted into
       sleeplock.  This means that invoking the page allocator from atomic
       contexts results in "BUG: scheduling while atomic".
    
    c) Please note that call_rcu() is already invoked from raw atomic context,
       so it is only reasonable to expaect that kfree_rcu() and kvfree_rcu()
       will also be called from atomic raw context.
    
    This commit therefore defers page allocation to a clean context using the
    combination of an hrtimer and a workqueue.  The hrtimer stage is required
    in order to avoid deadlocks with the scheduler.  This deferred allocation
    is required only when kvfree_rcu()'s per-CPU page cache is empty.
    
    Link: https://lore.kernel.org/lkml/20200630164543.4mdcf6zb4zfclhln@linutronix.de/
    Fixes: 3042f83f19be ("rcu: Support reclaim for head-less object")
    Reported-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Uladzislau Rezki (Sony) <urezki@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit 89e5193ace1af78a2cee0fc83dfcf73b4c5c83d8
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Sep 1 10:41:46 2020 +0200

    io_wq: Make io_wqe::lock a raw_spinlock_t
    
    commit 95da84659226d75698a1ab958be0af21d9cc2a9c upstream.
    
    During a context switch the scheduler invokes wq_worker_sleeping() with
    disabled preemption. Disabling preemption is needed because it protects
    access to `worker->sleeping'. As an optimisation it avoids invoking
    schedule() within the schedule path as part of possible wake up (thus
    preempt_enable_no_resched() afterwards).
    
    The io-wq has been added to the mix in the same section with disabled
    preemption. This breaks on PREEMPT_RT because io_wq_worker_sleeping()
    acquires a spinlock_t. Also within the schedule() the spinlock_t must be
    acquired after tsk_is_pi_blocked() otherwise it will block on the
    sleeping lock again while scheduling out.
    
    While playing with `io_uring-bench' I didn't notice a significant
    latency spike after converting io_wqe::lock to a raw_spinlock_t. The
    latency was more or less the same.
    
    In order to keep the spinlock_t it would have to be moved after the
    tsk_is_pi_blocked() check which would introduce a branch instruction
    into the hot path.
    
    The lock is used to maintain the `work_list' and wakes one task up at
    most.
    Should io_wqe_cancel_pending_work() cause latency spikes, while
    searching for a specific item, then it would need to drop the lock
    during iterations.
    revert_creds() is also invoked under the lock. According to debug
    cred::non_rcu is 0. Otherwise it should be moved outside of the locked
    section because put_cred_rcu()->free_uid() acquires a sleeping lock.
    
    Convert io_wqe::lock to a raw_spinlock_t.c
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4863be6534250e0adb3236af65f1a7ec92c6b6bd
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Sep 1 10:41:46 2020 +0200

    io_wq: Make io_wqe::lock a raw_spinlock_t
    
    commit 95da84659226d75698a1ab958be0af21d9cc2a9c upstream.
    
    During a context switch the scheduler invokes wq_worker_sleeping() with
    disabled preemption. Disabling preemption is needed because it protects
    access to `worker->sleeping'. As an optimisation it avoids invoking
    schedule() within the schedule path as part of possible wake up (thus
    preempt_enable_no_resched() afterwards).
    
    The io-wq has been added to the mix in the same section with disabled
    preemption. This breaks on PREEMPT_RT because io_wq_worker_sleeping()
    acquires a spinlock_t. Also within the schedule() the spinlock_t must be
    acquired after tsk_is_pi_blocked() otherwise it will block on the
    sleeping lock again while scheduling out.
    
    While playing with `io_uring-bench' I didn't notice a significant
    latency spike after converting io_wqe::lock to a raw_spinlock_t. The
    latency was more or less the same.
    
    In order to keep the spinlock_t it would have to be moved after the
    tsk_is_pi_blocked() check which would introduce a branch instruction
    into the hot path.
    
    The lock is used to maintain the `work_list' and wakes one task up at
    most.
    Should io_wqe_cancel_pending_work() cause latency spikes, while
    searching for a specific item, then it would need to drop the lock
    during iterations.
    revert_creds() is also invoked under the lock. According to debug
    cred::non_rcu is 0. Otherwise it should be moved outside of the locked
    section because put_cred_rcu()->free_uid() acquires a sleeping lock.
    
    Convert io_wqe::lock to a raw_spinlock_t.c
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1a2b85f1e2a93a3f84243e654d225e4088735336
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Wed Oct 21 12:07:49 2020 -0700

    timekeeping: Convert jiffies_seq to seqcount_raw_spinlock_t
    
    Use the new api and associate the seqcounter to the jiffies_lock enabling
    lockdep support - although for this particular case the write-side locking
    and non-preemptibility are quite obvious.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20201021190749.19363-1-dave@stgolabs.net

commit 95da84659226d75698a1ab958be0af21d9cc2a9c
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Sep 1 10:41:46 2020 +0200

    io_wq: Make io_wqe::lock a raw_spinlock_t
    
    During a context switch the scheduler invokes wq_worker_sleeping() with
    disabled preemption. Disabling preemption is needed because it protects
    access to `worker->sleeping'. As an optimisation it avoids invoking
    schedule() within the schedule path as part of possible wake up (thus
    preempt_enable_no_resched() afterwards).
    
    The io-wq has been added to the mix in the same section with disabled
    preemption. This breaks on PREEMPT_RT because io_wq_worker_sleeping()
    acquires a spinlock_t. Also within the schedule() the spinlock_t must be
    acquired after tsk_is_pi_blocked() otherwise it will block on the
    sleeping lock again while scheduling out.
    
    While playing with `io_uring-bench' I didn't notice a significant
    latency spike after converting io_wqe::lock to a raw_spinlock_t. The
    latency was more or less the same.
    
    In order to keep the spinlock_t it would have to be moved after the
    tsk_is_pi_blocked() check which would introduce a branch instruction
    into the hot path.
    
    The lock is used to maintain the `work_list' and wakes one task up at
    most.
    Should io_wqe_cancel_pending_work() cause latency spikes, while
    searching for a specific item, then it would need to drop the lock
    during iterations.
    revert_creds() is also invoked under the lock. According to debug
    cred::non_rcu is 0. Otherwise it should be moved outside of the locked
    section because put_cred_rcu()->free_uid() acquires a sleeping lock.
    
    Convert io_wqe::lock to a raw_spinlock_t.c
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 1909760f5fc3f123e47b4e24e0ccdc0fc8f3f106
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Fri Sep 4 17:32:31 2020 +0200

    seqlock: PREEMPT_RT: Do not starve seqlock_t writers
    
    On PREEMPT_RT, seqlock_t is transformed to a sleeping lock that do not
    disable preemption. A seqlock_t reader can thus preempt its write side
    section and spin for the enter scheduler tick. If that reader belongs to
    a real-time scheduling class, it can spin forever and the kernel will
    livelock.
    
    To break this livelock possibility on PREEMPT_RT, implement seqlock_t in
    terms of "seqcount_spinlock_t" instead of plain "seqcount_t".
    
    Beside its pure annotational value, this will leverage the existing
    seqcount_LOCKNAME_T PREEMPT_RT anti-livelock mechanisms, without adding
    any extra code.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200904153231.11994-6-a.darwish@linutronix.de

commit 8117ab508f9c476e0a10b9db7f4818f784cf3176
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Fri Sep 4 17:32:30 2020 +0200

    seqlock: seqcount_LOCKNAME_t: Introduce PREEMPT_RT support
    
    Preemption must be disabled before entering a sequence counter write
    side critical section.  Otherwise the read side section can preempt the
    write side section and spin for the entire scheduler tick.  If that
    reader belongs to a real-time scheduling class, it can spin forever and
    the kernel will livelock.
    
    Disabling preemption cannot be done for PREEMPT_RT though: it can lead
    to higher latencies, and the write side sections will not be able to
    acquire locks which become sleeping locks (e.g. spinlock_t).
    
    To remain preemptible, while avoiding a possible livelock caused by the
    reader preempting the writer, use a different technique: let the reader
    detect if a seqcount_LOCKNAME_t writer is in progress. If that's the
    case, acquire then release the associated LOCKNAME writer serialization
    lock. This will allow any possibly-preempted writer to make progress
    until the end of its writer serialization lock critical section.
    
    Implement this lock-unlock technique for all seqcount_LOCKNAME_t with
    an associated (PREEMPT_RT) sleeping lock.
    
    References: 55f3560df975 ("seqlock: Extend seqcount API with associated locks")
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200519214547.352050-1-a.darwish@linutronix.de

commit 249d053835320cb3e7c00066cf085a6ba9b1f126
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Thu Aug 27 13:40:41 2020 +0200

    timekeeping: Use seqcount_latch_t
    
    Latch sequence counters are a multiversion concurrency control mechanism
    where the seqcount_t counter even/odd value is used to switch between
    two data storage copies. This allows the seqcount_t read path to safely
    interrupt its write side critical section (e.g. from NMIs).
    
    Initially, latch sequence counters were implemented as a single write
    function, raw_write_seqcount_latch(), above plain seqcount_t. The read
    path was expected to use plain seqcount_t raw_read_seqcount().
    
    A specialized read function was later added, raw_read_seqcount_latch(),
    and became the standardized way for latch read paths. Having unique read
    and write APIs meant that latch sequence counters are basically a data
    type of their own -- just inappropriately overloading plain seqcount_t.
    The seqcount_latch_t data type was thus introduced at seqlock.h.
    
    Use that new data type instead of seqcount_raw_spinlock_t. This ensures
    that only latch-safe APIs are to be used with the sequence counter.
    
    Note that the use of seqcount_raw_spinlock_t was not very useful in the
    first place. Only the "raw_" subset of seqcount_t APIs were used at
    timekeeping.c. This subset was created for contexts where lockdep cannot
    be used. seqcount_LOCKTYPE_t's raison d'être -- verifying that the
    seqcount_t writer serialization lock is held -- cannot thus be done.
    
    References: 0c3351d451ae ("seqlock: Use raw_ prefix instead of _no_lockdep")
    References: 55f3560df975 ("seqlock: Extend seqcount API with associated locks")
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200827114044.11173-6-a.darwish@linutronix.de

commit 80793c3471d90d4dc2b48deadb6413bdfe39500f
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Thu Aug 27 13:40:39 2020 +0200

    seqlock: Introduce seqcount_latch_t
    
    Latch sequence counters are a multiversion concurrency control mechanism
    where the seqcount_t counter even/odd value is used to switch between
    two copies of protected data. This allows the seqcount_t read path to
    safely interrupt its write side critical section (e.g. from NMIs).
    
    Initially, latch sequence counters were implemented as a single write
    function above plain seqcount_t: raw_write_seqcount_latch(). The read
    side was expected to use plain seqcount_t raw_read_seqcount().
    
    A specialized latch read function, raw_read_seqcount_latch(), was later
    added. It became the standardized way for latch read paths.  Due to the
    dependent load, it has one read memory barrier less than the plain
    seqcount_t raw_read_seqcount() API.
    
    Only raw_write_seqcount_latch() and raw_read_seqcount_latch() should be
    used with latch sequence counters. Having *unique* read and write path
    APIs means that latch sequence counters are actually a data type of
    their own -- just inappropriately overloading plain seqcount_t.
    
    Introduce seqcount_latch_t. This adds type-safety and ensures that only
    the correct latch-safe APIs are to be used.
    
    Not to break bisection, let the latch APIs also accept plain seqcount_t
    or seqcount_raw_spinlock_t. After converting all call sites to
    seqcount_latch_t, only that new data type will be allowed.
    
    References: 9b0fd802e8c0 ("seqcount: Add raw_write_seqcount_latch()")
    References: 7fc26327b756 ("seqlock: Introduce raw_read_seqcount_latch()")
    References: aadd6e5caaac ("time/sched_clock: Use raw_read_seqcount_latch()")
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200827114044.11173-4-a.darwish@linutronix.de

commit f7987153ab716d2e87dd694926c486c55313c8b3
Author: Bartosz Golaszewski <bgolaszewski@baylibre.com>
Date:   Mon Jun 15 09:44:45 2020 +0200

    irqchip/irq-mtk-sysirq: Replace spinlock with raw_spinlock
    
    [ Upstream commit 6eeb997ab5075e770a002c51351fa4ec2c6b5c39 ]
    
    This driver may take a regular spinlock when a raw spinlock
    (irq_desc->lock) is already taken which results in the following
    lockdep splat:
    
    =============================
    [ BUG: Invalid wait context ]
    5.7.0-rc7 #1 Not tainted
    -----------------------------
    swapper/0/0 is trying to lock:
    ffffff800303b798 (&chip_data->lock){....}-{3:3}, at: mtk_sysirq_set_type+0x48/0xc0
    other info that might help us debug this:
    context-{5:5}
    2 locks held by swapper/0/0:
     #0: ffffff800302ee68 (&desc->request_mutex){....}-{4:4}, at: __setup_irq+0xc4/0x8a0
     #1: ffffff800302ecf0 (&irq_desc_lock_class){....}-{2:2}, at: __setup_irq+0xe4/0x8a0
    stack backtrace:
    CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.7.0-rc7 #1
    Hardware name: Pumpkin MT8516 (DT)
    Call trace:
     dump_backtrace+0x0/0x180
     show_stack+0x14/0x20
     dump_stack+0xd0/0x118
     __lock_acquire+0x8c8/0x2270
     lock_acquire+0xf8/0x470
     _raw_spin_lock_irqsave+0x50/0x78
     mtk_sysirq_set_type+0x48/0xc0
     __irq_set_trigger+0x58/0x170
     __setup_irq+0x420/0x8a0
     request_threaded_irq+0xd8/0x190
     timer_of_init+0x1e8/0x2c4
     mtk_gpt_init+0x5c/0x1dc
     timer_probe+0x74/0xf4
     time_init+0x14/0x44
     start_kernel+0x394/0x4f0
    
    Replace the spinlock_t with raw_spinlock_t to avoid this warning.
    
    Signed-off-by: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200615074445.3579-1-brgl@bgdev.pl
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit b70d115ebdcf97cc605716329c754676df881617
Author: Bartosz Golaszewski <bgolaszewski@baylibre.com>
Date:   Mon Jun 15 09:44:45 2020 +0200

    irqchip/irq-mtk-sysirq: Replace spinlock with raw_spinlock
    
    [ Upstream commit 6eeb997ab5075e770a002c51351fa4ec2c6b5c39 ]
    
    This driver may take a regular spinlock when a raw spinlock
    (irq_desc->lock) is already taken which results in the following
    lockdep splat:
    
    =============================
    [ BUG: Invalid wait context ]
    5.7.0-rc7 #1 Not tainted
    -----------------------------
    swapper/0/0 is trying to lock:
    ffffff800303b798 (&chip_data->lock){....}-{3:3}, at: mtk_sysirq_set_type+0x48/0xc0
    other info that might help us debug this:
    context-{5:5}
    2 locks held by swapper/0/0:
     #0: ffffff800302ee68 (&desc->request_mutex){....}-{4:4}, at: __setup_irq+0xc4/0x8a0
     #1: ffffff800302ecf0 (&irq_desc_lock_class){....}-{2:2}, at: __setup_irq+0xe4/0x8a0
    stack backtrace:
    CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.7.0-rc7 #1
    Hardware name: Pumpkin MT8516 (DT)
    Call trace:
     dump_backtrace+0x0/0x180
     show_stack+0x14/0x20
     dump_stack+0xd0/0x118
     __lock_acquire+0x8c8/0x2270
     lock_acquire+0xf8/0x470
     _raw_spin_lock_irqsave+0x50/0x78
     mtk_sysirq_set_type+0x48/0xc0
     __irq_set_trigger+0x58/0x170
     __setup_irq+0x420/0x8a0
     request_threaded_irq+0xd8/0x190
     timer_of_init+0x1e8/0x2c4
     mtk_gpt_init+0x5c/0x1dc
     timer_probe+0x74/0xf4
     time_init+0x14/0x44
     start_kernel+0x394/0x4f0
    
    Replace the spinlock_t with raw_spinlock_t to avoid this warning.
    
    Signed-off-by: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200615074445.3579-1-brgl@bgdev.pl
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 950a1a7f661d7c9e7860cc33fd7057073ea90e19
Author: Bartosz Golaszewski <bgolaszewski@baylibre.com>
Date:   Mon Jun 15 09:44:45 2020 +0200

    irqchip/irq-mtk-sysirq: Replace spinlock with raw_spinlock
    
    [ Upstream commit 6eeb997ab5075e770a002c51351fa4ec2c6b5c39 ]
    
    This driver may take a regular spinlock when a raw spinlock
    (irq_desc->lock) is already taken which results in the following
    lockdep splat:
    
    =============================
    [ BUG: Invalid wait context ]
    5.7.0-rc7 #1 Not tainted
    -----------------------------
    swapper/0/0 is trying to lock:
    ffffff800303b798 (&chip_data->lock){....}-{3:3}, at: mtk_sysirq_set_type+0x48/0xc0
    other info that might help us debug this:
    context-{5:5}
    2 locks held by swapper/0/0:
     #0: ffffff800302ee68 (&desc->request_mutex){....}-{4:4}, at: __setup_irq+0xc4/0x8a0
     #1: ffffff800302ecf0 (&irq_desc_lock_class){....}-{2:2}, at: __setup_irq+0xe4/0x8a0
    stack backtrace:
    CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.7.0-rc7 #1
    Hardware name: Pumpkin MT8516 (DT)
    Call trace:
     dump_backtrace+0x0/0x180
     show_stack+0x14/0x20
     dump_stack+0xd0/0x118
     __lock_acquire+0x8c8/0x2270
     lock_acquire+0xf8/0x470
     _raw_spin_lock_irqsave+0x50/0x78
     mtk_sysirq_set_type+0x48/0xc0
     __irq_set_trigger+0x58/0x170
     __setup_irq+0x420/0x8a0
     request_threaded_irq+0xd8/0x190
     timer_of_init+0x1e8/0x2c4
     mtk_gpt_init+0x5c/0x1dc
     timer_probe+0x74/0xf4
     time_init+0x14/0x44
     start_kernel+0x394/0x4f0
    
    Replace the spinlock_t with raw_spinlock_t to avoid this warning.
    
    Signed-off-by: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200615074445.3579-1-brgl@bgdev.pl
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit ae3033d3859691136642ca01e8fdae2a05ffcd3c
Author: Bartosz Golaszewski <bgolaszewski@baylibre.com>
Date:   Mon Jun 15 09:44:45 2020 +0200

    irqchip/irq-mtk-sysirq: Replace spinlock with raw_spinlock
    
    [ Upstream commit 6eeb997ab5075e770a002c51351fa4ec2c6b5c39 ]
    
    This driver may take a regular spinlock when a raw spinlock
    (irq_desc->lock) is already taken which results in the following
    lockdep splat:
    
    =============================
    [ BUG: Invalid wait context ]
    5.7.0-rc7 #1 Not tainted
    -----------------------------
    swapper/0/0 is trying to lock:
    ffffff800303b798 (&chip_data->lock){....}-{3:3}, at: mtk_sysirq_set_type+0x48/0xc0
    other info that might help us debug this:
    context-{5:5}
    2 locks held by swapper/0/0:
     #0: ffffff800302ee68 (&desc->request_mutex){....}-{4:4}, at: __setup_irq+0xc4/0x8a0
     #1: ffffff800302ecf0 (&irq_desc_lock_class){....}-{2:2}, at: __setup_irq+0xe4/0x8a0
    stack backtrace:
    CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.7.0-rc7 #1
    Hardware name: Pumpkin MT8516 (DT)
    Call trace:
     dump_backtrace+0x0/0x180
     show_stack+0x14/0x20
     dump_stack+0xd0/0x118
     __lock_acquire+0x8c8/0x2270
     lock_acquire+0xf8/0x470
     _raw_spin_lock_irqsave+0x50/0x78
     mtk_sysirq_set_type+0x48/0xc0
     __irq_set_trigger+0x58/0x170
     __setup_irq+0x420/0x8a0
     request_threaded_irq+0xd8/0x190
     timer_of_init+0x1e8/0x2c4
     mtk_gpt_init+0x5c/0x1dc
     timer_probe+0x74/0xf4
     time_init+0x14/0x44
     start_kernel+0x394/0x4f0
    
    Replace the spinlock_t with raw_spinlock_t to avoid this warning.
    
    Signed-off-by: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200615074445.3579-1-brgl@bgdev.pl
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 1016f98b586dce303561d4e32c1173e1cecd7131
Author: Bartosz Golaszewski <bgolaszewski@baylibre.com>
Date:   Mon Jun 15 09:44:45 2020 +0200

    irqchip/irq-mtk-sysirq: Replace spinlock with raw_spinlock
    
    [ Upstream commit 6eeb997ab5075e770a002c51351fa4ec2c6b5c39 ]
    
    This driver may take a regular spinlock when a raw spinlock
    (irq_desc->lock) is already taken which results in the following
    lockdep splat:
    
    =============================
    [ BUG: Invalid wait context ]
    5.7.0-rc7 #1 Not tainted
    -----------------------------
    swapper/0/0 is trying to lock:
    ffffff800303b798 (&chip_data->lock){....}-{3:3}, at: mtk_sysirq_set_type+0x48/0xc0
    other info that might help us debug this:
    context-{5:5}
    2 locks held by swapper/0/0:
     #0: ffffff800302ee68 (&desc->request_mutex){....}-{4:4}, at: __setup_irq+0xc4/0x8a0
     #1: ffffff800302ecf0 (&irq_desc_lock_class){....}-{2:2}, at: __setup_irq+0xe4/0x8a0
    stack backtrace:
    CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.7.0-rc7 #1
    Hardware name: Pumpkin MT8516 (DT)
    Call trace:
     dump_backtrace+0x0/0x180
     show_stack+0x14/0x20
     dump_stack+0xd0/0x118
     __lock_acquire+0x8c8/0x2270
     lock_acquire+0xf8/0x470
     _raw_spin_lock_irqsave+0x50/0x78
     mtk_sysirq_set_type+0x48/0xc0
     __irq_set_trigger+0x58/0x170
     __setup_irq+0x420/0x8a0
     request_threaded_irq+0xd8/0x190
     timer_of_init+0x1e8/0x2c4
     mtk_gpt_init+0x5c/0x1dc
     timer_probe+0x74/0xf4
     time_init+0x14/0x44
     start_kernel+0x394/0x4f0
    
    Replace the spinlock_t with raw_spinlock_t to avoid this warning.
    
    Signed-off-by: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200615074445.3579-1-brgl@bgdev.pl
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 9ba19ccd2d283a79dd29e8130819c59beca80f62
Merge: 8f0cb6660acb 992414a18cd4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 3 14:39:35 2020 -0700

    Merge tag 'locking-core-2020-08-03' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
    
     - LKMM updates: mostly documentation changes, but also some new litmus
       tests for atomic ops.
    
     - KCSAN updates: the most important change is that GCC 11 now has all
       fixes in place to support KCSAN, so GCC support can be enabled again.
       Also more annotations.
    
     - futex updates: minor cleanups and simplifications
    
     - seqlock updates: merge preparatory changes/cleanups for the
       'associated locks' facilities.
    
     - lockdep updates:
        - simplify IRQ trace event handling
        - add various new debug checks
        - simplify header dependencies, split out <linux/lockdep_types.h>,
          decouple lockdep from other low level headers some more
        - fix NMI handling
    
     - misc cleanups and smaller fixes
    
    * tag 'locking-core-2020-08-03' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (60 commits)
      kcsan: Improve IRQ state trace reporting
      lockdep: Refactor IRQ trace events fields into struct
      seqlock: lockdep assert non-preemptibility on seqcount_t write
      lockdep: Add preemption enabled/disabled assertion APIs
      seqlock: Implement raw_seqcount_begin() in terms of raw_read_seqcount()
      seqlock: Add kernel-doc for seqcount_t and seqlock_t APIs
      seqlock: Reorder seqcount_t and seqlock_t API definitions
      seqlock: seqcount_t latch: End read sections with read_seqcount_retry()
      seqlock: Properly format kernel-doc code samples
      Documentation: locking: Describe seqlock design and usage
      locking/qspinlock: Do not include atomic.h from qspinlock_types.h
      locking/atomic: Move ATOMIC_INIT into linux/types.h
      lockdep: Move list.h inclusion into lockdep.h
      locking/lockdep: Fix TRACE_IRQFLAGS vs. NMIs
      futex: Remove unused or redundant includes
      futex: Consistently use fshared as boolean
      futex: Remove needless goto's
      futex: Remove put_futex_key()
      rwsem: fix commas in initialisation
      docs: locking: Replace HTTP links with HTTPS ones
      ...

commit af5a06b582ec3d7b0160b4faaa65f73d8dcf989f
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Mon Jul 20 17:55:30 2020 +0200

    hrtimer: Use sequence counter with associated raw spinlock
    
    A sequence counter write side critical section must be protected by some
    form of locking to serialize writers. A plain seqcount_t does not
    contain the information of which lock must be held when entering a write
    side critical section.
    
    Use the new seqcount_raw_spinlock_t data type, which allows to associate
    a raw spinlock with the sequence counter. This enables lockdep to verify
    that the raw spinlock used for writer serialization is held when the
    write side critical section is entered.
    
    If lockdep is disabled this lock association is compiled out and has
    neither storage size nor runtime overhead.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200720155530.1173732-25-a.darwish@linutronix.de

commit 5c73b9a2b1b4ecc809a914aa64970157b3d8c936
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Mon Jul 20 17:55:29 2020 +0200

    kvm/eventfd: Use sequence counter with associated spinlock
    
    A sequence counter write side critical section must be protected by some
    form of locking to serialize writers. A plain seqcount_t does not
    contain the information of which lock must be held when entering a write
    side critical section.
    
    Use the new seqcount_spinlock_t data type, which allows to associate a
    spinlock with the sequence counter. This enables lockdep to verify that
    the spinlock used for writer serialization is held when the write side
    critical section is entered.
    
    If lockdep is disabled this lock association is compiled out and has
    neither storage size nor runtime overhead.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Link: https://lkml.kernel.org/r/20200720155530.1173732-24-a.darwish@linutronix.de

commit 2ca97ac8bdcc31fdc868f40c73c017f0a648dc07
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Mon Jul 20 17:55:28 2020 +0200

    userfaultfd: Use sequence counter with associated spinlock
    
    A sequence counter write side critical section must be protected by some
    form of locking to serialize writers. A plain seqcount_t does not
    contain the information of which lock must be held when entering a write
    side critical section.
    
    Use the new seqcount_spinlock_t data type, which allows to associate a
    spinlock with the sequence counter. This enables lockdep to verify that
    the spinlock used for writer serialization is held when the write side
    critical section is entered.
    
    If lockdep is disabled this lock association is compiled out and has
    neither storage size nor runtime overhead.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200720155530.1173732-23-a.darwish@linutronix.de

commit 76246c9219726c71e3f470344d8c6a566fa2535b
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Mon Jul 20 17:55:27 2020 +0200

    NFSv4: Use sequence counter with associated spinlock
    
    A sequence counter write side critical section must be protected by some
    form of locking to serialize writers. A plain seqcount_t does not
    contain the information of which lock must be held when entering a write
    side critical section.
    
    Use the new seqcount_spinlock_t data type, which allows to associate a
    spinlock with the sequence counter. This enables lockdep to verify that
    the spinlock used for writer serialization is held when the write side
    critical section is entered.
    
    If lockdep is disabled this lock association is compiled out and has
    neither storage size nor runtime overhead.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200720155530.1173732-22-a.darwish@linutronix.de

commit 67b7b641ca69cafb467f7560316b09b8ff0fa5c9
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Mon Jul 20 17:55:26 2020 +0200

    iocost: Use sequence counter with associated spinlock
    
    A sequence counter write side critical section must be protected by some
    form of locking to serialize writers. A plain seqcount_t does not
    contain the information of which lock must be held when entering a write
    side critical section.
    
    Use the new seqcount_spinlock_t data type, which allows to associate a
    spinlock with the sequence counter. This enables lockdep to verify that
    the spinlock used for writer serialization is held when the write side
    critical section is entered.
    
    If lockdep is disabled this lock association is compiled out and has
    neither storage size nor runtime overhead.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Daniel Wagner <dwagner@suse.de>
    Link: https://lkml.kernel.org/r/20200720155530.1173732-21-a.darwish@linutronix.de

commit 0a87b25ff2eb6169403c88b0d5f3c97bdaa3c930
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Mon Jul 20 17:55:25 2020 +0200

    raid5: Use sequence counter with associated spinlock
    
    A sequence counter write side critical section must be protected by some
    form of locking to serialize writers. A plain seqcount_t does not
    contain the information of which lock must be held when entering a write
    side critical section.
    
    Use the new seqcount_spinlock_t data type, which allows to associate a
    spinlock with the sequence counter. This enables lockdep to verify that
    the spinlock used for writer serialization is held when the write side
    critical section is entered.
    
    If lockdep is disabled this lock association is compiled out and has
    neither storage size nor runtime overhead.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Song Liu <song@kernel.org>
    Link: https://lkml.kernel.org/r/20200720155530.1173732-20-a.darwish@linutronix.de

commit 26475371976c69489d3a8e6c8bbf35afbbc25055
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Mon Jul 20 17:55:24 2020 +0200

    vfs: Use sequence counter with associated spinlock
    
    A sequence counter write side critical section must be protected by some
    form of locking to serialize writers. A plain seqcount_t does not
    contain the information of which lock must be held when entering a write
    side critical section.
    
    Use the new seqcount_spinlock_t data type, which allows to associate a
    spinlock with the sequence counter. This enables lockdep to verify that
    the spinlock used for writer serialization is held when the write side
    critical section is entered.
    
    If lockdep is disabled this lock association is compiled out and has
    neither storage size nor runtime overhead.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200720155530.1173732-19-a.darwish@linutronix.de

commit 025e82bcbc34cd071390e72fd0b31593282f9425
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Mon Jul 20 17:55:23 2020 +0200

    timekeeping: Use sequence counter with associated raw spinlock
    
    A sequence counter write side critical section must be protected by some
    form of locking to serialize writers. A plain seqcount_t does not
    contain the information of which lock must be held when entering a write
    side critical section.
    
    Use the new seqcount_raw_spinlock_t data type, which allows to associate
    a raw spinlock with the sequence counter. This enables lockdep to verify
    that the raw spinlock used for writer serialization is held when the
    write side critical section is entered.
    
    If lockdep is disabled this lock association is compiled out and has
    neither storage size nor runtime overhead.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200720155530.1173732-18-a.darwish@linutronix.de

commit 77cc278f7b202e4f16f8596837219d02cb090b96
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Mon Jul 20 17:55:22 2020 +0200

    xfrm: policy: Use sequence counters with associated lock
    
    A sequence counter write side critical section must be protected by some
    form of locking to serialize writers. If the serialization primitive is
    not disabling preemption implicitly, preemption has to be explicitly
    disabled before entering the sequence counter write side critical
    section.
    
    A plain seqcount_t does not contain the information of which lock must
    be held when entering a write side critical section.
    
    Use the new seqcount_spinlock_t and seqcount_mutex_t data types instead,
    which allow to associate a lock with the sequence counter. This enables
    lockdep to verify that the lock used for writer serialization is held
    when the write side critical section is entered.
    
    If lockdep is disabled this lock association is compiled out and has
    neither storage size nor runtime overhead.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200720155530.1173732-17-a.darwish@linutronix.de

commit 8201d923f492703a7d6c980cff3034759a452b86
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Mon Jul 20 17:55:20 2020 +0200

    netfilter: conntrack: Use sequence counter with associated spinlock
    
    A sequence counter write side critical section must be protected by some
    form of locking to serialize writers. A plain seqcount_t does not
    contain the information of which lock must be held when entering a write
    side critical section.
    
    Use the new seqcount_spinlock_t data type, which allows to associate a
    spinlock with the sequence counter. This enables lockdep to verify that
    the spinlock used for writer serialization is held when the write side
    critical section is entered.
    
    If lockdep is disabled this lock association is compiled out and has
    neither storage size nor runtime overhead.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200720155530.1173732-15-a.darwish@linutronix.de

commit b75058614fdd3140074a640b514f6a0b4d485a2d
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Mon Jul 20 17:55:19 2020 +0200

    sched: tasks: Use sequence counter with associated spinlock
    
    A sequence counter write side critical section must be protected by some
    form of locking to serialize writers. A plain seqcount_t does not
    contain the information of which lock must be held when entering a write
    side critical section.
    
    Use the new seqcount_spinlock_t data type, which allows to associate a
    spinlock with the sequence counter. This enables lockdep to verify that
    the spinlock used for writer serialization is held when the write side
    critical section is entered.
    
    If lockdep is disabled this lock association is compiled out and has
    neither storage size nor runtime overhead.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200720155530.1173732-14-a.darwish@linutronix.de

commit 55f3560df975f557c48aa6afc636808f31ecb87a
Author: Ahmed S. Darwish <a.darwish@linutronix.de>
Date:   Mon Jul 20 17:55:15 2020 +0200

    seqlock: Extend seqcount API with associated locks
    
    A sequence counter write side critical section must be protected by some
    form of locking to serialize writers. If the serialization primitive is
    not disabling preemption implicitly, preemption has to be explicitly
    disabled before entering the write side critical section.
    
    There is no built-in debugging mechanism to verify that the lock used
    for writer serialization is held and preemption is disabled. Some usage
    sites like dma-buf have explicit lockdep checks for the writer-side
    lock, but this covers only a small portion of the sequence counter usage
    in the kernel.
    
    Add new sequence counter types which allows to associate a lock to the
    sequence counter at initialization time. The seqcount API functions are
    extended to provide appropriate lockdep assertions depending on the
    seqcount/lock type.
    
    For sequence counters with associated locks that do not implicitly
    disable preemption, preemption protection is enforced in the sequence
    counter write side functions. This removes the need to explicitly add
    preempt_disable/enable() around the write side critical sections: the
    write_begin/end() functions for these new sequence counter types
    automatically do this.
    
    Introduce the following seqcount types with associated locks:
    
         seqcount_spinlock_t
         seqcount_raw_spinlock_t
         seqcount_rwlock_t
         seqcount_mutex_t
         seqcount_ww_mutex_t
    
    Extend the seqcount read and write functions to branch out to the
    specific seqcount_LOCKTYPE_t implementation at compile-time. This avoids
    kernel API explosion per each new seqcount_LOCKTYPE_t added. Add such
    compile-time type detection logic into a new, internal, seqlock header.
    
    Document the proper seqcount_LOCKTYPE_t usage, and rationale, at
    Documentation/locking/seqlock.rst.
    
    If lockdep is disabled, this lock association is compiled out and has
    neither storage size nor runtime overhead.
    
    Signed-off-by: Ahmed S. Darwish <a.darwish@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200720155530.1173732-10-a.darwish@linutronix.de

commit 459e39538e612b8dd130d34b93c9bfc89ecc836c
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Wed Jul 29 22:33:16 2020 +1000

    locking/qspinlock: Do not include atomic.h from qspinlock_types.h
    
    This patch breaks a header loop involving qspinlock_types.h.
    The issue is that qspinlock_types.h includes atomic.h, which then
    eventually includes kernel.h which could lead back to the original
    file via spinlock_types.h.
    
    As ATOMIC_INIT is now defined by linux/types.h, there is no longer
    any need to include atomic.h from qspinlock_types.h.  This also
    allows the CONFIG_PARAVIRT hack to be removed since it was trying
    to prevent exactly this loop.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Link: https://lkml.kernel.org/r/20200729123316.GC7047@gondor.apana.org.au

commit 722c6e954c90eca93ac89980e7b79577cd141777
Author: Charan Teja Kalla <charante@codeaurora.org>
Date:   Fri Jun 19 17:27:19 2020 +0530

    dmabuf: use spinlock to access dmabuf->name
    
    [ Upstream commit 6348dd291e3653534a9e28e6917569bc9967b35b ]
    
    There exists a sleep-while-atomic bug while accessing the dmabuf->name
    under mutex in the dmabuffs_dname(). This is caused from the SELinux
    permissions checks on a process where it tries to validate the inherited
    files from fork() by traversing them through iterate_fd() (which
    traverse files under spin_lock) and call
    match_file(security/selinux/hooks.c) where the permission checks happen.
    This audit information is logged using dump_common_audit_data() where it
    calls d_path() to get the file path name. If the file check happen on
    the dmabuf's fd, then it ends up in ->dmabuffs_dname() and use mutex to
    access dmabuf->name. The flow will be like below:
    flush_unauthorized_files()
      iterate_fd()
        spin_lock() --> Start of the atomic section.
          match_file()
            file_has_perm()
              avc_has_perm()
                avc_audit()
                  slow_avc_audit()
                    common_lsm_audit()
                      dump_common_audit_data()
                        audit_log_d_path()
                          d_path()
                            dmabuffs_dname()
                              mutex_lock()--> Sleep while atomic.
    
    Call trace captured (on 4.19 kernels) is below:
    ___might_sleep+0x204/0x208
    __might_sleep+0x50/0x88
    __mutex_lock_common+0x5c/0x1068
    __mutex_lock_common+0x5c/0x1068
    mutex_lock_nested+0x40/0x50
    dmabuffs_dname+0xa0/0x170
    d_path+0x84/0x290
    audit_log_d_path+0x74/0x130
    common_lsm_audit+0x334/0x6e8
    slow_avc_audit+0xb8/0xf8
    avc_has_perm+0x154/0x218
    file_has_perm+0x70/0x180
    match_file+0x60/0x78
    iterate_fd+0x128/0x168
    selinux_bprm_committing_creds+0x178/0x248
    security_bprm_committing_creds+0x30/0x48
    install_exec_creds+0x1c/0x68
    load_elf_binary+0x3a4/0x14e0
    search_binary_handler+0xb0/0x1e0
    
    So, use spinlock to access dmabuf->name to avoid sleep-while-atomic.
    
    Cc: <stable@vger.kernel.org> [5.3+]
    Signed-off-by: Charan Teja Kalla <charante@codeaurora.org>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Acked-by: Christian König <christian.koenig@amd.com>
     [sumits: added comment to spinlock_t definition to avoid warning]
    Signed-off-by: Sumit Semwal <sumit.semwal@linaro.org>
    Link: https://patchwork.freedesktop.org/patch/msgid/a83e7f0d-4e54-9848-4b58-e1acdbe06735@codeaurora.org
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 855bd4f219f0d078356971796fa7e39383eba609
Author: Charan Teja Kalla <charante@codeaurora.org>
Date:   Fri Jun 19 17:27:19 2020 +0530

    dmabuf: use spinlock to access dmabuf->name
    
    commit 6348dd291e3653534a9e28e6917569bc9967b35b upstream.
    
    There exists a sleep-while-atomic bug while accessing the dmabuf->name
    under mutex in the dmabuffs_dname(). This is caused from the SELinux
    permissions checks on a process where it tries to validate the inherited
    files from fork() by traversing them through iterate_fd() (which
    traverse files under spin_lock) and call
    match_file(security/selinux/hooks.c) where the permission checks happen.
    This audit information is logged using dump_common_audit_data() where it
    calls d_path() to get the file path name. If the file check happen on
    the dmabuf's fd, then it ends up in ->dmabuffs_dname() and use mutex to
    access dmabuf->name. The flow will be like below:
    flush_unauthorized_files()
      iterate_fd()
        spin_lock() --> Start of the atomic section.
          match_file()
            file_has_perm()
              avc_has_perm()
                avc_audit()
                  slow_avc_audit()
                    common_lsm_audit()
                      dump_common_audit_data()
                        audit_log_d_path()
                          d_path()
                            dmabuffs_dname()
                              mutex_lock()--> Sleep while atomic.
    
    Call trace captured (on 4.19 kernels) is below:
    ___might_sleep+0x204/0x208
    __might_sleep+0x50/0x88
    __mutex_lock_common+0x5c/0x1068
    __mutex_lock_common+0x5c/0x1068
    mutex_lock_nested+0x40/0x50
    dmabuffs_dname+0xa0/0x170
    d_path+0x84/0x290
    audit_log_d_path+0x74/0x130
    common_lsm_audit+0x334/0x6e8
    slow_avc_audit+0xb8/0xf8
    avc_has_perm+0x154/0x218
    file_has_perm+0x70/0x180
    match_file+0x60/0x78
    iterate_fd+0x128/0x168
    selinux_bprm_committing_creds+0x178/0x248
    security_bprm_committing_creds+0x30/0x48
    install_exec_creds+0x1c/0x68
    load_elf_binary+0x3a4/0x14e0
    search_binary_handler+0xb0/0x1e0
    
    So, use spinlock to access dmabuf->name to avoid sleep-while-atomic.
    
    Cc: <stable@vger.kernel.org> [5.3+]
    Signed-off-by: Charan Teja Kalla <charante@codeaurora.org>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Acked-by: Christian König <christian.koenig@amd.com>
     [sumits: added comment to spinlock_t definition to avoid warning]
    Signed-off-by: Sumit Semwal <sumit.semwal@linaro.org>
    Link: https://patchwork.freedesktop.org/patch/msgid/a83e7f0d-4e54-9848-4b58-e1acdbe06735@codeaurora.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6348dd291e3653534a9e28e6917569bc9967b35b
Author: Charan Teja Kalla <charante@codeaurora.org>
Date:   Fri Jun 19 17:27:19 2020 +0530

    dmabuf: use spinlock to access dmabuf->name
    
    There exists a sleep-while-atomic bug while accessing the dmabuf->name
    under mutex in the dmabuffs_dname(). This is caused from the SELinux
    permissions checks on a process where it tries to validate the inherited
    files from fork() by traversing them through iterate_fd() (which
    traverse files under spin_lock) and call
    match_file(security/selinux/hooks.c) where the permission checks happen.
    This audit information is logged using dump_common_audit_data() where it
    calls d_path() to get the file path name. If the file check happen on
    the dmabuf's fd, then it ends up in ->dmabuffs_dname() and use mutex to
    access dmabuf->name. The flow will be like below:
    flush_unauthorized_files()
      iterate_fd()
        spin_lock() --> Start of the atomic section.
          match_file()
            file_has_perm()
              avc_has_perm()
                avc_audit()
                  slow_avc_audit()
                    common_lsm_audit()
                      dump_common_audit_data()
                        audit_log_d_path()
                          d_path()
                            dmabuffs_dname()
                              mutex_lock()--> Sleep while atomic.
    
    Call trace captured (on 4.19 kernels) is below:
    ___might_sleep+0x204/0x208
    __might_sleep+0x50/0x88
    __mutex_lock_common+0x5c/0x1068
    __mutex_lock_common+0x5c/0x1068
    mutex_lock_nested+0x40/0x50
    dmabuffs_dname+0xa0/0x170
    d_path+0x84/0x290
    audit_log_d_path+0x74/0x130
    common_lsm_audit+0x334/0x6e8
    slow_avc_audit+0xb8/0xf8
    avc_has_perm+0x154/0x218
    file_has_perm+0x70/0x180
    match_file+0x60/0x78
    iterate_fd+0x128/0x168
    selinux_bprm_committing_creds+0x178/0x248
    security_bprm_committing_creds+0x30/0x48
    install_exec_creds+0x1c/0x68
    load_elf_binary+0x3a4/0x14e0
    search_binary_handler+0xb0/0x1e0
    
    So, use spinlock to access dmabuf->name to avoid sleep-while-atomic.
    
    Cc: <stable@vger.kernel.org> [5.3+]
    Signed-off-by: Charan Teja Kalla <charante@codeaurora.org>
    Reviewed-by: Michael J. Ruhl <michael.j.ruhl@intel.com>
    Acked-by: Christian König <christian.koenig@amd.com>
     [sumits: added comment to spinlock_t definition to avoid warning]
    Signed-off-by: Sumit Semwal <sumit.semwal@linaro.org>
    Link: https://patchwork.freedesktop.org/patch/msgid/a83e7f0d-4e54-9848-4b58-e1acdbe06735@codeaurora.org

commit d6bdceb6c2276276c0392b926ccd2e5991d5cb9a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 29 22:41:01 2020 +0200

    powerpc64: Break asm/percpu.h vs spinlock_types.h dependency
    
    In order to use <asm/percpu.h> in lockdep.h, we need to make sure
    asm/percpu.h does not itself depend on lockdep.
    
    The below seems to make that so and builds powerpc64-defconfig +
    PROVE_LOCKING.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    https://lkml.kernel.org/r/20200623083721.336906073@infradead.org

commit bde50d8ff83e4ce9e576f7c5ba1edb48a3610a5b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue May 26 15:41:34 2020 +0200

    srcu: Avoid local_irq_save() before acquiring spinlock_t
    
    SRCU disables interrupts to get a stable per-CPU pointer and then
    acquires the spinlock which is in the per-CPU data structure. The
    release uses spin_unlock_irqrestore(). While this is correct on a non-RT
    kernel, this conflicts with the RT semantics because the spinlock is
    converted to a 'sleeping' spinlock. Sleeping locks can obviously not be
    acquired with interrupts disabled.
    
    Acquire the per-CPU pointer `ssp->sda' without disabling preemption and
    then acquire the spinlock_t of the per-CPU data structure. The lock will
    ensure that the data is consistent.
    
    The added call to check_init_srcu_struct() is now needed because a
    statically defined srcu_struct may remain uninitialized until this
    point and the newly introduced locking operation requires an initialized
    spinlock_t.
    
    This change was tested for four hours with 8*SRCU-N and 8*SRCU-P without
    causing any warnings.
    
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: "Paul E. McKenney" <paulmck@kernel.org>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: rcu@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit 6eeb997ab5075e770a002c51351fa4ec2c6b5c39
Author: Bartosz Golaszewski <bgolaszewski@baylibre.com>
Date:   Mon Jun 15 09:44:45 2020 +0200

    irqchip/irq-mtk-sysirq: Replace spinlock with raw_spinlock
    
    This driver may take a regular spinlock when a raw spinlock
    (irq_desc->lock) is already taken which results in the following
    lockdep splat:
    
    =============================
    [ BUG: Invalid wait context ]
    5.7.0-rc7 #1 Not tainted
    -----------------------------
    swapper/0/0 is trying to lock:
    ffffff800303b798 (&chip_data->lock){....}-{3:3}, at: mtk_sysirq_set_type+0x48/0xc0
    other info that might help us debug this:
    context-{5:5}
    2 locks held by swapper/0/0:
     #0: ffffff800302ee68 (&desc->request_mutex){....}-{4:4}, at: __setup_irq+0xc4/0x8a0
     #1: ffffff800302ecf0 (&irq_desc_lock_class){....}-{2:2}, at: __setup_irq+0xe4/0x8a0
    stack backtrace:
    CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.7.0-rc7 #1
    Hardware name: Pumpkin MT8516 (DT)
    Call trace:
     dump_backtrace+0x0/0x180
     show_stack+0x14/0x20
     dump_stack+0xd0/0x118
     __lock_acquire+0x8c8/0x2270
     lock_acquire+0xf8/0x470
     _raw_spin_lock_irqsave+0x50/0x78
     mtk_sysirq_set_type+0x48/0xc0
     __irq_set_trigger+0x58/0x170
     __setup_irq+0x420/0x8a0
     request_threaded_irq+0xd8/0x190
     timer_of_init+0x1e8/0x2c4
     mtk_gpt_init+0x5c/0x1dc
     timer_probe+0x74/0xf4
     time_init+0x14/0x44
     start_kernel+0x394/0x4f0
    
    Replace the spinlock_t with raw_spinlock_t to avoid this warning.
    
    Signed-off-by: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200615074445.3579-1-brgl@bgdev.pl

commit c935cd62d3fe985d7f0ebea185d2759e8992e96f
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Wed Jun 17 17:17:19 2020 +1000

    lockdep: Split header file into lockdep and lockdep_types
    
    There is a header file inclusion loop between asm-generic/bug.h
    and linux/kernel.h.  This causes potential compile failurs depending
    on the which file is included first.  One way of breaking this loop
    is to stop spinlock_types.h from including lockdep.h.  This patch
    splits lockdep.h into two files for this purpose.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Reviewed-by: Andy Shevchenko <andy.shevchenko@gmail.com>
    Acked-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Link: https://lkml.kernel.org/r/E1jlSJz-0003hE-8g@fornost.hmeau.com

commit fe3bc8a988a4d38dc090e77071ff9b8ea266528a
Merge: 4a7e89c5ec02 10cdb1575954
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 6 10:01:48 2020 -0700

    Merge branch 'for-5.8' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    Pull workqueue updates from Tejun Heo:
     "Mostly cleanups and other trivial changes.
    
      The only interesting change is Sebastian's rcuwait conversion for RT"
    
    * 'for-5.8' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq:
      workqueue: use BUILD_BUG_ON() for compile time test instead of WARN_ON()
      workqueue: fix a piece of comment about reserved bits for work flags
      workqueue: remove useless unlock() and lock() in series
      workqueue: void unneeded requeuing the pwq in rescuer thread
      workqueue: Convert the pool::lock and wq_mayday_lock to raw_spinlock_t
      workqueue: Use rcuwait for wq_manager_wait
      workqueue: Remove unnecessary kfree() call in rcu_free_wq()
      workqueue: Fix an use after free in init_rescuer()
      workqueue: Use IS_ERR and PTR_ERR instead of PTR_ERR_OR_ZERO.

commit a9b8a985294debae00f6c087dfec8c384d30a3b9
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed May 27 21:46:33 2020 +0200

    workqueue: Convert the pool::lock and wq_mayday_lock to raw_spinlock_t
    
    The workqueue code has it's internal spinlocks (pool::lock), which
    are acquired on most workqueue operations. These spinlocks are
    converted to 'sleeping' spinlocks on a RT-kernel.
    
    Workqueue functions can be invoked from contexts which are truly atomic
    even on a PREEMPT_RT enabled kernel. Taking sleeping locks from such
    contexts is forbidden.
    
    The pool::lock hold times are bound and the code sections are
    relatively short, which allows to convert pool::lock and as a
    consequence wq_mayday_lock to raw spinlocks which are truly spinning
    locks even on a PREEMPT_RT kernel.
    
    With the previous conversion of the manager waitqueue to a simple
    waitqueue workqueues are now fully RT compliant.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Lai Jiangshan <jiangshanlai@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit e6da0edc24eecef2f6964d92fa9044e1821deace
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu May 28 14:35:12 2020 +0200

    Bluetooth: Acquire sk_lock.slock without disabling interrupts
    
    There was a lockdep which led to commit
       fad003b6c8e3d ("Bluetooth: Fix inconsistent lock state with RFCOMM")
    
    Lockdep noticed that `sk->sk_lock.slock' was acquired without disabling
    the softirq while the lock was also used in softirq context.
    Unfortunately the solution back then was to disable interrupts before
    acquiring the lock which however made lockdep happy.
    It would have been enough to simply disable the softirq. Disabling
    interrupts before acquiring a spinlock_t is not allowed on PREEMPT_RT
    because these locks are converted to 'sleeping' spinlocks.
    
    Use spin_lock_bh() in order to acquire the `sk_lock.slock'.
    
    Reported-by: Luis Claudio R. Goncalves <lclaudio@uudg.org>
    Reported-by: kbuild test robot <lkp@intel.com> [missing unlock]
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Marcel Holtmann <marcel@holtmann.org>

commit 2ca41f555e857ec5beef6063bfa43a17ee76d7ec
Author: Waiman Long <longman@redhat.com>
Date:   Tue May 26 08:20:14 2020 -0400

    x86/spinlock: Remove obsolete ticket spinlock macros and types
    
    Even though the x86 ticket spinlock code has been removed with
    
      cfd8983f03c7 ("x86, locking/spinlocks: Remove ticket (spin)lock implementation")
    
    a while ago, there are still some ticket spinlock specific macros and
    types left in the asm/spinlock_types.h header file that are no longer
    used. Remove those as well to avoid confusion.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200526122014.25241-1-longman@redhat.com

commit b01b2141999936ac3e4746b7f76c0f204ae4b445
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed May 27 22:11:15 2020 +0200

    mm/swap: Use local_lock for protection
    
    The various struct pagevec per CPU variables are protected by disabling
    either preemption or interrupts across the critical sections. Inside
    these sections spinlocks have to be acquired.
    
    These spinlocks are regular spinlock_t types which are converted to
    "sleeping" spinlocks on PREEMPT_RT enabled kernels. Obviously sleeping
    locks cannot be acquired in preemption or interrupt disabled sections.
    
    local locks provide a trivial way to substitute preempt and interrupt
    disable instances. On a non PREEMPT_RT enabled kernel local_lock() maps
    to preempt_disable() and local_lock_irq() to local_irq_disable().
    
    Create lru_rotate_pvecs containing the pagevec and the locallock.
    Create lru_pvecs containing the remaining pagevecs and the locallock.
    Add lru_add_drain_cpu_zone() which is used from compact_zone() to avoid
    exporting the pvec structure.
    
    Change the relevant call sites to acquire these locks instead of using
    preempt_disable() / get_cpu() / get_cpu_var() and local_irq_disable() /
    local_irq_save().
    
    There is neither a functional change nor a change in the generated
    binary code for non PREEMPT_RT enabled non-debug kernels.
    
    When lockdep is enabled local locks have lockdep maps embedded. These
    allow lockdep to validate the protections, i.e. inappropriate usage of a
    preemption only protected sections would result in a lockdep warning
    while the same problem would not be noticed with a plain
    preempt_disable() based protection.
    
    local locks also improve readability as they provide a named scope for
    the protections while preempt/interrupt disable are opaque scopeless.
    
    Finally local locks allow PREEMPT_RT to substitute them with real
    locking primitives to ensure the correctness of operation in a fully
    preemptible kernel.
    
    [ bigeasy: Adopted to use local_lock ]
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lore.kernel.org/r/20200527201119.1692513-4-bigeasy@linutronix.de

commit 91710728d1725de51d06b40674abf6e860d592c7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 27 22:11:13 2020 +0200

    locking: Introduce local_lock()
    
    preempt_disable() and local_irq_disable/save() are in principle per CPU big
    kernel locks. This has several downsides:
    
      - The protection scope is unknown
    
      - Violation of protection rules is hard to detect by instrumentation
    
      - For PREEMPT_RT such sections, unless in low level critical code, can
        violate the preemptability constraints.
    
    To address this PREEMPT_RT introduced the concept of local_locks which are
    strictly per CPU.
    
    The lock operations map to preempt_disable(), local_irq_disable/save() and
    the enabling counterparts on non RT enabled kernels.
    
    If lockdep is enabled local locks gain a lock map which tracks the usage
    context. This will catch cases where an area is protected by
    preempt_disable() but the access also happens from interrupt context. local
    locks have identified quite a few such issues over the years, the most
    recent example is:
    
      b7d5dc21072cd ("random: add a spinlock_t to struct batched_entropy")
    
    Aside of the lockdep coverage this also improves code readability as it
    precisely annotates the protection scope.
    
    PREEMPT_RT substitutes these local locks with 'sleeping' spinlocks to
    protect such sections while maintaining preemtability and CPU locality.
    
    local locks can replace:
    
      - preempt_enable()/disable() pairs
      - local_irq_disable/enable() pairs
      - local_irq_save/restore() pairs
    
    They are also used to replace code which implicitly disables preemption
    like:
    
      - get_cpu()/put_cpu()
      - get_cpu_var()/put_cpu_var()
    
    with PREEMPT_RT friendly constructs.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lore.kernel.org/r/20200527201119.1692513-2-bigeasy@linutronix.de

commit 34183ddd13dbfa859c4b68d16a30aad2cce72b11
Merge: 8645f09bad14 11700fcb90b4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 7 20:00:16 2020 -0700

    Merge tag 'thermal-v5.7-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/thermal/linux
    
    Pull thermal updates from Daniel Lezcano:
    
     - Convert tsens configuration DT binding to yaml (Rajeshwari)
    
     - Add interrupt support on the rcar sensor (Niklas Söderlund)
    
     - Add a new Spreadtrum thermal driver (Baolin Wang)
    
     - Add thermal binding for the fsl scu board, a new API to retrieve the
       sensor id bound to the thermal zone and i.MX system controller sensor
       (Anson Huang))
    
     - Remove warning log when a deferred probe is requested on Exynos
       (Marek Szyprowski)
    
     - Add the thermal monitoring unit support for imx8mm with its DT
       bindings (Anson Huang)
    
     - Rephrase the Kconfig text for clarity (Linus Walleij)
    
     - Use the gpio descriptor for the ti-soc-thermal (Linus Walleij)
    
     - Align msg structure to 4 bytes for i.MX SC, fix the Kconfig
       dependency, add the __may_be unused annotation for PM functions and
       the COMPILE_TEST option for imx8mm (Anson Huang)
    
     - Fix a dependency on regmap in Kconfig for qoriq (Yuantian Tang)
    
     - Add DT binding and support for the rcar gen3 r8a77961 and improve the
       error path on the rcar init function (Niklas Söderlund)
    
     - Cleanup and improvements for the tsens Qcom sensor (Amit Kucheria)
    
     - Improve code by removing lock and caching values in the rcar thermal
       sensor (Niklas Söderlund)
    
     - Cleanup in the qoriq drivers and add a call to
       imx_thermal_unregister_legacy_cooling in the removal function (Anson
       Huang)
    
     - Remove redundant 'maxItems' in tsens and sprd DT bindings (Rob
       Herring)
    
     - Change the thermal DT bindings by making the cooling-maps optional
       (Yuantian Tang)
    
     - Add Tiger Lake support (Sumeet Pawnikar)
    
     - Use scnprintf() for avoiding potential buffer overflow (Takashi Iwai)
    
     - Make pkg_temp_lock a raw_spinlock_t(Clark Williams)
    
     - Fix incorrect data types by changing them to signed on i.MX SC (Anson
       Huang)
    
     - Replace zero-length array with flexible-array member (Gustavo A. R.
       Silva)
    
     - Add support for i.MX8MP in the driver and in the DT bindings (Anson
       Huang)
    
     - Fix return value of the cpufreq_set_cur_state() function (Willy
       Wolff)
    
     - Remove abusing and scary WARN_ON in the cpufreq cooling device
       (Daniel Lezcano)
    
     - Fix build warning of incorrect argument type reported by sparse on
       imx8mm (Anson Huang)
    
     - Fix stub for the devfreq cooling device (Martin Blumenstingl)
    
     - Fix cpu idle cooling documentation (Sergey Vidishev)
    
    * tag 'thermal-v5.7-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/thermal/linux: (52 commits)
      Documentation: cpu-idle-cooling: Fix diagram for 33% duty cycle
      thermal: devfreq_cooling: inline all stubs for CONFIG_DEVFREQ_THERMAL=n
      thermal: imx8mm: Fix build warning of incorrect argument type
      thermal/drivers/cpufreq_cooling: Remove abusing WARN_ON
      thermal/drivers/cpufreq_cooling: Fix return of cpufreq_set_cur_state
      thermal: imx8mm: Add i.MX8MP support
      dt-bindings: thermal: imx8mm-thermal: Add support for i.MX8MP
      thermal: qcom: tsens.h: Replace zero-length array with flexible-array member
      thermal: imx_sc_thermal: Fix incorrect data type
      thermal: int340x_thermal: Use scnprintf() for avoiding potential buffer overflow
      thermal: int340x: processor_thermal: Add Tiger Lake support
      thermal/x86_pkg_temp: Make pkg_temp_lock a raw_spinlock_t
      dt-bindings: thermal: make cooling-maps property optional
      dt-bindings: thermal: qcom-tsens: Remove redundant 'maxItems'
      dt-bindings: thermal: sprd: Remove redundant 'maxItems'
      thermal: imx: Calling imx_thermal_unregister_legacy_cooling() in .remove
      thermal: qoriq: Sort includes alphabetically
      thermal: qoriq: Use devm_add_action_or_reset() to handle all cleanups
      thermal: rcar_thermal: Remove lock in rcar_thermal_get_current_temp()
      thermal: rcar_thermal: Do not store ctemp in rcar_thermal_priv
      ...

commit 4b9fd8a829a1eec7442e38afff21d610604de56a
Merge: a776c270a0b2 f1e67e355c2a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 16:17:15 2020 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Continued user-access cleanups in the futex code.
    
       - percpu-rwsem rewrite that uses its own waitqueue and atomic_t
         instead of an embedded rwsem. This addresses a couple of
         weaknesses, but the primary motivation was complications on the -rt
         kernel.
    
       - Introduce raw lock nesting detection on lockdep
         (CONFIG_PROVE_RAW_LOCK_NESTING=y), document the raw_lock vs. normal
         lock differences. This too originates from -rt.
    
       - Reuse lockdep zapped chain_hlocks entries, to conserve RAM
         footprint on distro-ish kernels running into the "BUG:
         MAX_LOCKDEP_CHAIN_HLOCKS too low!" depletion of the lockdep
         chain-entries pool.
    
       - Misc cleanups, smaller fixes and enhancements - see the changelog
         for details"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (55 commits)
      fs/buffer: Make BH_Uptodate_Lock bit_spin_lock a regular spinlock_t
      thermal/x86_pkg_temp: Make pkg_temp_lock a raw_spinlock_t
      Documentation/locking/locktypes: Minor copy editor fixes
      Documentation/locking/locktypes: Further clarifications and wordsmithing
      m68knommu: Remove mm.h include from uaccess_no.h
      x86: get rid of user_atomic_cmpxchg_inatomic()
      generic arch_futex_atomic_op_inuser() doesn't need access_ok()
      x86: don't reload after cmpxchg in unsafe_atomic_op2() loop
      x86: convert arch_futex_atomic_op_inuser() to user_access_begin/user_access_end()
      objtool: whitelist __sanitizer_cov_trace_switch()
      [parisc, s390, sparc64] no need for access_ok() in futex handling
      sh: no need of access_ok() in arch_futex_atomic_op_inuser()
      futex: arch_futex_atomic_op_inuser() calling conventions change
      completion: Use lockdep_assert_RT_in_threaded_ctx() in complete_all()
      lockdep: Add posixtimer context tracing bits
      lockdep: Annotate irq_work
      lockdep: Add hrtimer context tracing bits
      lockdep: Introduce wait-type checks
      completion: Use simple wait queues
      sched/swait: Prepare usage in completions
      ...

commit f1e67e355c2aafeddf1eac31335709236996d2fe
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 18 14:28:24 2019 +0100

    fs/buffer: Make BH_Uptodate_Lock bit_spin_lock a regular spinlock_t
    
    Bit spinlocks are problematic if PREEMPT_RT is enabled, because they
    disable preemption, which is undesired for latency reasons and breaks when
    regular spinlocks are taken within the bit_spinlock locked region because
    regular spinlocks are converted to 'sleeping spinlocks' on RT.
    
    PREEMPT_RT replaced the bit spinlocks with regular spinlocks to avoid this
    problem. The replacement was done conditionaly at compile time, but
    Christoph requested to do an unconditional conversion.
    
    Jan suggested to move the spinlock into a existing padding hole which
    avoids a size increase of struct buffer_head on production kernels.
    
    As a benefit the lock gains lockdep coverage.
    
    [ bigeasy: Remove the wrapper and use always spinlock_t and move it into
               the padding hole ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Christoph Hellwig <hch@infradead.org>
    Link: https://lkml.kernel.org/r/20191118132824.rclhrbujqh4b4g4d@linutronix.de

commit fc32150e6f43d6cb93ea75937bb6a88a1764cc37
Author: Clark Williams <williams@redhat.com>
Date:   Tue Oct 8 13:00:21 2019 +0200

    thermal/x86_pkg_temp: Make pkg_temp_lock a raw_spinlock_t
    
    The pkg_temp_lock spinlock is acquired in the thermal vector handler which
    is truly atomic context even on PREEMPT_RT kernels.
    
    The critical sections are tiny, so change it to a raw spinlock.
    
    Signed-off-by: Clark Williams <williams@redhat.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20191008110021.2j44ayunal7fkb7i@linutronix.de

commit 49915ac35ca7b07c54295a72d905be5064afb89e
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sat Mar 21 12:26:03 2020 +0100

    lockdep: Annotate irq_work
    
    Mark irq_work items with IRQ_WORK_HARD_IRQ which should be invoked in
    hardirq context even on PREEMPT_RT. IRQ_WORK without this flag will be
    invoked in softirq context on PREEMPT_RT.
    
    Set ->irq_config to 1 for the IRQ_WORK items which are invoked in softirq
    context so lockdep knows that these can safely acquire a spinlock_t.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200321113242.643576700@linutronix.de

commit de8f5e4f2dc1f032b46afda0a78cab5456974f89
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Mar 21 12:26:01 2020 +0100

    lockdep: Introduce wait-type checks
    
    Extend lockdep to validate lock wait-type context.
    
    The current wait-types are:
    
            LD_WAIT_FREE,           /* wait free, rcu etc.. */
            LD_WAIT_SPIN,           /* spin loops, raw_spinlock_t etc.. */
            LD_WAIT_CONFIG,         /* CONFIG_PREEMPT_LOCK, spinlock_t etc.. */
            LD_WAIT_SLEEP,          /* sleeping locks, mutex_t etc.. */
    
    Where lockdep validates that the current lock (the one being acquired)
    fits in the current wait-context (as generated by the held stack).
    
    This ensures that there is no attempt to acquire mutexes while holding
    spinlocks, to acquire spinlocks while holding raw_spinlocks and so on. In
    other words, its a more fancy might_sleep().
    
    Obviously RCU made the entire ordeal more complex than a simple single
    value test because RCU can be acquired in (pretty much) any context and
    while it presents a context to nested locks it is not the same as it
    got acquired in.
    
    Therefore its necessary to split the wait_type into two values, one
    representing the acquire (outer) and one representing the nested context
    (inner). For most 'normal' locks these two are the same.
    
    [ To make static initialization easier we have the rule that:
      .outer == INV means .outer == .inner; because INV == 0. ]
    
    It further means that its required to find the minimal .inner of the held
    stack to compare against the outer of the new lock; because while 'normal'
    RCU presents a CONFIG type to nested locks, if it is taken while already
    holding a SPIN type it obviously doesn't relax the rules.
    
    Below is an example output generated by the trivial test code:
    
      raw_spin_lock(&foo);
      spin_lock(&bar);
      spin_unlock(&bar);
      raw_spin_unlock(&foo);
    
     [ BUG: Invalid wait context ]
     -----------------------------
     swapper/0/1 is trying to lock:
     ffffc90000013f20 (&bar){....}-{3:3}, at: kernel_init+0xdb/0x187
     other info that might help us debug this:
     1 lock held by swapper/0/1:
      #0: ffffc90000013ee0 (&foo){+.+.}-{2:2}, at: kernel_init+0xd1/0x187
    
    The way to read it is to look at the new -{n,m} part in the lock
    description; -{3:3} for the attempted lock, and try and match that up to
    the held locks, which in this case is the one: -{2,2}.
    
    This tells that the acquiring lock requires a more relaxed environment than
    presented by the lock stack.
    
    Currently only the normal locks and RCU are converted, the rest of the
    lockdep users defaults to .inner = INV which is ignored. More conversions
    can be done when desired.
    
    The check for spinlock_t nesting is not enabled by default. It's a separate
    config option for now as there are known problems which are currently
    addressed. The config option allows to identify these problems and to
    verify that the solutions found are indeed solving them.
    
    The config switch will be removed and the checks will permanently enabled
    once the vast majority of issues has been addressed.
    
    [ bigeasy: Move LD_WAIT_FREE,… out of CONFIG_LOCKDEP to avoid compile
               failure with CONFIG_DEBUG_SPINLOCK + !CONFIG_LOCKDEP]
    [ tglx: Add the config option ]
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200321113242.427089655@linutronix.de

commit a5c6234e10280b3ec65e2410ce34904a2580e5f8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Mar 21 12:26:00 2020 +0100

    completion: Use simple wait queues
    
    completion uses a wait_queue_head_t to enqueue waiters.
    
    wait_queue_head_t contains a spinlock_t to protect the list of waiters
    which excludes it from being used in truly atomic context on a PREEMPT_RT
    enabled kernel.
    
    The spinlock in the wait queue head cannot be replaced by a raw_spinlock
    because:
    
      - wait queues can have custom wakeup callbacks, which acquire other
        spinlock_t locks and have potentially long execution times
    
      - wake_up() walks an unbounded number of list entries during the wake up
        and may wake an unbounded number of waiters.
    
    For simplicity and performance reasons complete() should be usable on
    PREEMPT_RT enabled kernels.
    
    completions do not use custom wakeup callbacks and are usually single
    waiter, except for a few corner cases.
    
    Replace the wait queue in the completion with a simple wait queue (swait),
    which uses a raw_spinlock_t for protecting the waiter list and therefore is
    safe to use inside truly atomic regions on PREEMPT_RT.
    
    There is no semantical or functional change:
    
      - completions use the exclusive wait mode which is what swait provides
    
      - complete() wakes one exclusive waiter
    
      - complete_all() wakes all waiters while holding the lock which protects
        the wait queue against newly incoming waiters. The conversion to swait
        preserves this behaviour.
    
    complete_all() might cause unbound latencies with a large number of waiters
    being woken at once, but most complete_all() usage sites are either in
    testing or initialization code or have only a really small number of
    concurrent waiters which for now does not cause a latency problem. Keep it
    simple for now.
    
    The fixup of the warning check in the USB gadget driver is just a straight
    forward conversion of the lockless waiter check from one waitqueue type to
    the other.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: Davidlohr Bueso <dbueso@suse.de>
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Link: https://lkml.kernel.org/r/20200321113242.317954042@linutronix.de

commit e5d4d1756b07d9490a0269a9e68c1e05ee1feb9b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Mar 21 12:25:58 2020 +0100

    timekeeping: Split jiffies seqlock
    
    seqlock consists of a sequence counter and a spinlock_t which is used to
    serialize the writers. spinlock_t is substituted by a "sleeping" spinlock
    on PREEMPT_RT enabled kernels which breaks the usage in the timekeeping
    code as the writers are executed in hard interrupt and therefore
    non-preemptible context even on PREEMPT_RT.
    
    The spinlock in seqlock cannot be unconditionally replaced by a
    raw_spinlock_t as many seqlock users have nesting spinlock sections or
    other code which is not suitable to run in truly atomic context on RT.
    
    Instead of providing a raw_seqlock API for a single use case, open code the
    seqlock for the jiffies use case and implement it with a raw_spinlock_t and
    a sequence counter.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200321113242.120587764@linutronix.de

commit afa58b49ac52dff053e1bb363a425f09dbebc0a3
Author: Clark Williams <williams@redhat.com>
Date:   Tue Oct 8 13:00:21 2019 +0200

    thermal/x86_pkg_temp: Make pkg_temp_lock a raw_spinlock_t
    
    The spinlock pkg_temp_lock has the potential of being taken in atomic
    context because it can be acquired from the thermal IRQ vector.
    It's static and limited scope so go ahead and make it a raw spinlock.
    
    Signed-off-by: Clark Williams <williams@redhat.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Zhang Rui <rui.zhang@intel.com>
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Link: https://lore.kernel.org/r/20191008110021.2j44ayunal7fkb7i@linutronix.de

commit 44e5f9311cf0116635541099023ff54a86dcfb51
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Feb 24 15:07:25 2020 +0100

    drm/vmwgfx: Drop preempt_disable() in vmw_fifo_ping_host()
    
    vmw_fifo_ping_host() disables preemption around a test and a register
    write via vmw_write(). The write function acquires a spinlock_t typed
    lock which is not allowed in a preempt_disable()ed section on
    PREEMPT_RT. This has been reported in the bugzilla.
    
    It has been explained by Thomas Hellstrom that this preempt_disable()ed
    section is not required for correctness.
    
    Remove the preempt_disable() section.
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=206591
    Link: https://lkml.kernel.org/r/0b5e1c65d89951de993deab06d1d197b40fd67aa.camel@vmware.com
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Thomas Hellstrom <thellstrom@vmware.com>
    Signed-off-by: Thomas Hellstrom <thellstrom@vmware.com>

commit 200452f1cb437895cb4bb0fcb0d5b74afdbf17ac
Author: Jani Nikula <jani.nikula@intel.com>
Date:   Mon Feb 17 20:42:19 2020 +0200

    drm/i915/gem: use spinlock_t instead of struct spinlock
    
    spinlock_t is one case where the typedef is to be preferred over struct
    spinlock.
    
    Fixes: 42fb60de3129 ("drm/i915/gem: Don't leak non-persistent requests on changing engines")
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200217184219.15325-1-jani.nikula@intel.com

commit 8c96f1bc6fc49c724c4cdd22d3e99260263b7384
Author: He Zhe <zhe.he@windriver.com>
Date:   Thu Jan 30 22:12:00 2020 -0800

    mm/kmemleak: turn kmemleak_lock and object->lock to raw_spinlock_t
    
    kmemleak_lock as a rwlock on RT can possibly be acquired in atomic
    context which does work.
    
    Since the kmemleak operation is performed in atomic context make it a
    raw_spinlock_t so it can also be acquired on RT.  This is used for
    debugging and is not enabled by default in a production like environment
    (where performance/latency matters) so it makes sense to make it a
    raw_spinlock_t instead trying to get rid of the atomic context.  Turn
    also the kmemleak_object->lock into raw_spinlock_t which is acquired
    (nested) while the kmemleak_lock is held.
    
    The time spent in "echo scan > kmemleak" slightly improved on 64core box
    with this patch applied after boot.
    
    [bigeasy@linutronix.de: redo the description, update comments. Merge the individual bits:  He Zhe did the kmemleak_lock, Liu Haitao the ->lock and Yongxin Liu forwarded Liu's patch.]
    Link: http://lkml.kernel.org/r/20191219170834.4tah3prf2gdothz4@linutronix.de
    Link: https://lkml.kernel.org/r/20181218150744.GB20197@arrakis.emea.arm.com
    Link: https://lkml.kernel.org/r/1542877459-144382-1-git-send-email-zhe.he@windriver.com
    Link: https://lkml.kernel.org/r/20190927082230.34152-1-yongxin.liu@windriver.com
    Signed-off-by: He Zhe <zhe.he@windriver.com>
    Signed-off-by: Liu Haitao <haitao.liu@windriver.com>
    Signed-off-by: Yongxin Liu <yongxin.liu@windriver.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 75ccae62cb8d42a619323a85c577107b8b37d797
Author: Toke Høiland-Jørgensen <toke@redhat.com>
Date:   Thu Jan 16 16:14:44 2020 +0100

    xdp: Move devmap bulk queue into struct net_device
    
    Commit 96360004b862 ("xdp: Make devmap flush_list common for all map
    instances"), changed devmap flushing to be a global operation instead of a
    per-map operation. However, the queue structure used for bulking was still
    allocated as part of the containing map.
    
    This patch moves the devmap bulk queue into struct net_device. The
    motivation for this is reusing it for the non-map variant of XDP_REDIRECT,
    which will be changed in a subsequent commit.  To avoid other fields of
    struct net_device moving to different cache lines, we also move a couple of
    other members around.
    
    We defer the actual allocation of the bulk queue structure until the
    NETDEV_REGISTER notification devmap.c. This makes it possible to check for
    ndo_xdp_xmit support before allocating the structure, which is not possible
    at the time struct net_device is allocated. However, we keep the freeing in
    free_netdev() to avoid adding another RCU callback on NETDEV_UNREGISTER.
    
    Because of this change, we lose the reference back to the map that
    originated the redirect, so change the tracepoint to always return 0 as the
    map ID and index. Otherwise no functional change is intended with this
    patch.
    
    After this patch, the relevant part of struct net_device looks like this,
    according to pahole:
    
            /* --- cacheline 14 boundary (896 bytes) --- */
            struct netdev_queue *      _tx __attribute__((__aligned__(64))); /*   896     8 */
            unsigned int               num_tx_queues;        /*   904     4 */
            unsigned int               real_num_tx_queues;   /*   908     4 */
            struct Qdisc *             qdisc;                /*   912     8 */
            unsigned int               tx_queue_len;         /*   920     4 */
            spinlock_t                 tx_global_lock;       /*   924     4 */
            struct xdp_dev_bulk_queue * xdp_bulkq;           /*   928     8 */
            struct xps_dev_maps *      xps_cpus_map;         /*   936     8 */
            struct xps_dev_maps *      xps_rxqs_map;         /*   944     8 */
            struct mini_Qdisc *        miniq_egress;         /*   952     8 */
            /* --- cacheline 15 boundary (960 bytes) --- */
            struct hlist_head  qdisc_hash[16];               /*   960   128 */
            /* --- cacheline 17 boundary (1088 bytes) --- */
            struct timer_list  watchdog_timer;               /*  1088    40 */
    
            /* XXX last struct has 4 bytes of padding */
    
            int                        watchdog_timeo;       /*  1128     4 */
    
            /* XXX 4 bytes hole, try to pack */
    
            struct list_head   todo_list;                    /*  1136    16 */
            /* --- cacheline 18 boundary (1152 bytes) --- */
    
    Signed-off-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Björn Töpel <bjorn.topel@intel.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/157918768397.1458396.12673224324627072349.stgit@toke.dk

commit f506ed55388432e56b5d9c12f0da3a02b149b53d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 1 11:18:37 2019 +0200

    sched/core: Avoid spurious lock dependencies
    
    [ Upstream commit ff51ff84d82aea5a889b85f2b9fb3aa2b8691668 ]
    
    While seemingly harmless, __sched_fork() does hrtimer_init(), which,
    when DEBUG_OBJETS, can end up doing allocations.
    
    This then results in the following lock order:
    
      rq->lock
        zone->lock.rlock
          batched_entropy_u64.lock
    
    Which in turn causes deadlocks when we do wakeups while holding that
    batched_entropy lock -- as the random code does.
    
    Solve this by moving __sched_fork() out from under rq->lock. This is
    safe because nothing there relies on rq->lock, as also evident from the
    other __sched_fork() callsite.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: bigeasy@linutronix.de
    Cc: cl@linux.com
    Cc: keescook@chromium.org
    Cc: penberg@kernel.org
    Cc: rientjes@google.com
    Cc: thgarnie@google.com
    Cc: tytso@mit.edu
    Cc: will@kernel.org
    Fixes: b7d5dc21072c ("random: add a spinlock_t to struct batched_entropy")
    Link: https://lkml.kernel.org/r/20191001091837.GK4536@hirez.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 870083b6af3585601b69232f2ffdd362ac1bde7b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 1 11:18:37 2019 +0200

    sched/core: Avoid spurious lock dependencies
    
    [ Upstream commit ff51ff84d82aea5a889b85f2b9fb3aa2b8691668 ]
    
    While seemingly harmless, __sched_fork() does hrtimer_init(), which,
    when DEBUG_OBJETS, can end up doing allocations.
    
    This then results in the following lock order:
    
      rq->lock
        zone->lock.rlock
          batched_entropy_u64.lock
    
    Which in turn causes deadlocks when we do wakeups while holding that
    batched_entropy lock -- as the random code does.
    
    Solve this by moving __sched_fork() out from under rq->lock. This is
    safe because nothing there relies on rq->lock, as also evident from the
    other __sched_fork() callsite.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: bigeasy@linutronix.de
    Cc: cl@linux.com
    Cc: keescook@chromium.org
    Cc: penberg@kernel.org
    Cc: rientjes@google.com
    Cc: thgarnie@google.com
    Cc: tytso@mit.edu
    Cc: will@kernel.org
    Fixes: b7d5dc21072c ("random: add a spinlock_t to struct batched_entropy")
    Link: https://lkml.kernel.org/r/20191001091837.GK4536@hirez.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit c767cc7f9e22ea8fe2bae3c2c66d91436c1333b7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 1 11:18:37 2019 +0200

    sched/core: Avoid spurious lock dependencies
    
    [ Upstream commit ff51ff84d82aea5a889b85f2b9fb3aa2b8691668 ]
    
    While seemingly harmless, __sched_fork() does hrtimer_init(), which,
    when DEBUG_OBJETS, can end up doing allocations.
    
    This then results in the following lock order:
    
      rq->lock
        zone->lock.rlock
          batched_entropy_u64.lock
    
    Which in turn causes deadlocks when we do wakeups while holding that
    batched_entropy lock -- as the random code does.
    
    Solve this by moving __sched_fork() out from under rq->lock. This is
    safe because nothing there relies on rq->lock, as also evident from the
    other __sched_fork() callsite.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: bigeasy@linutronix.de
    Cc: cl@linux.com
    Cc: keescook@chromium.org
    Cc: penberg@kernel.org
    Cc: rientjes@google.com
    Cc: thgarnie@google.com
    Cc: tytso@mit.edu
    Cc: will@kernel.org
    Fixes: b7d5dc21072c ("random: add a spinlock_t to struct batched_entropy")
    Link: https://lkml.kernel.org/r/20191001091837.GK4536@hirez.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit ff6618e06cb4548b8ceaed1855ee39976a1f4aaf
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Oct 19 10:52:52 2018 -0700

    sparc: Fix parport build warnings.
    
    [ Upstream commit 46b8306480fb424abd525acc1763da1c63a27d8a ]
    
    If PARPORT_PC_FIFO is not enabled, do not provide the dma lock
    macros and lock definition.  Otherwise:
    
    ./arch/sparc/include/asm/parport.h:24:24: warning: ‘dma_spin_lock’ defined but not used [-Wunused-variable]
     static DEFINE_SPINLOCK(dma_spin_lock);
                            ^~~~~~~~~~~~~
    ./include/linux/spinlock_types.h:81:39: note: in definition of macro ‘DEFINE_SPINLOCK’
     #define DEFINE_SPINLOCK(x) spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d50730b4d6cefd1a5bcda0ab2ff2f7e90306d18f
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Oct 19 10:52:52 2018 -0700

    sparc: Fix parport build warnings.
    
    [ Upstream commit 46b8306480fb424abd525acc1763da1c63a27d8a ]
    
    If PARPORT_PC_FIFO is not enabled, do not provide the dma lock
    macros and lock definition.  Otherwise:
    
    ./arch/sparc/include/asm/parport.h:24:24: warning: ‘dma_spin_lock’ defined but not used [-Wunused-variable]
     static DEFINE_SPINLOCK(dma_spin_lock);
                            ^~~~~~~~~~~~~
    ./include/linux/spinlock_types.h:81:39: note: in definition of macro ‘DEFINE_SPINLOCK’
     #define DEFINE_SPINLOCK(x) spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f7bcd7dbc811a9292e83a6ef1f238c31b6105fd5
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Oct 19 10:52:52 2018 -0700

    sparc: Fix parport build warnings.
    
    [ Upstream commit 46b8306480fb424abd525acc1763da1c63a27d8a ]
    
    If PARPORT_PC_FIFO is not enabled, do not provide the dma lock
    macros and lock definition.  Otherwise:
    
    ./arch/sparc/include/asm/parport.h:24:24: warning: ‘dma_spin_lock’ defined but not used [-Wunused-variable]
     static DEFINE_SPINLOCK(dma_spin_lock);
                            ^~~~~~~~~~~~~
    ./include/linux/spinlock_types.h:81:39: note: in definition of macro ‘DEFINE_SPINLOCK’
     #define DEFINE_SPINLOCK(x) spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 0b13f3a612d76eaa29bad32c2a93f085faf21120
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Oct 19 10:52:52 2018 -0700

    sparc: Fix parport build warnings.
    
    [ Upstream commit 46b8306480fb424abd525acc1763da1c63a27d8a ]
    
    If PARPORT_PC_FIFO is not enabled, do not provide the dma lock
    macros and lock definition.  Otherwise:
    
    ./arch/sparc/include/asm/parport.h:24:24: warning: ‘dma_spin_lock’ defined but not used [-Wunused-variable]
     static DEFINE_SPINLOCK(dma_spin_lock);
                            ^~~~~~~~~~~~~
    ./include/linux/spinlock_types.h:81:39: note: in definition of macro ‘DEFINE_SPINLOCK’
     #define DEFINE_SPINLOCK(x) spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 9e8d42a0f7eb9056f8bdb241b91738b5a2923f4c
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Nov 8 18:35:53 2019 +0100

    percpu-refcount: Use normal instead of RCU-sched"
    
    This is a revert of commit
       a4244454df129 ("percpu-refcount: use RCU-sched insted of normal RCU")
    
    which claims the only reason for using RCU-sched is
       "rcu_read_[un]lock() … are slightly more expensive than preempt_disable/enable()"
    
    and
        "As the RCU critical sections are extremely short, using sched-RCU
        shouldn't have any latency implications."
    
    The problem with using RCU-sched here is that it disables preemption and
    the release callback (called from percpu_ref_put_many()) must not
    acquire any sleeping locks like spinlock_t. This breaks PREEMPT_RT
    because some of the users acquire spinlock_t locks in their callbacks.
    
    Using rcu_read_lock() on PREEMPTION=n kernels is not any different
    compared to rcu_read_lock_sched(). On PREEMPTION=y kernels there are
    already performance issues due to additional preemption points.
    Looking at the code, the rcu_read_lock() is just an increment and unlock
    is almost just a decrement unless there is something special to do. Both
    are functions while disabling preemption is inlined.
    Doing a small benchmark, the minimal amount of time required was mostly
    the same. The average time required was higher due to the higher MAX
    value (which could be preemption). With DEBUG_PREEMPT=y it is
    rcu_read_lock_sched() that takes a little longer due to the additional
    debug code.
    
    Convert back to normal RCU.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Dennis Zhou <dennis@kernel.org>

commit ff51ff84d82aea5a889b85f2b9fb3aa2b8691668
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 1 11:18:37 2019 +0200

    sched/core: Avoid spurious lock dependencies
    
    While seemingly harmless, __sched_fork() does hrtimer_init(), which,
    when DEBUG_OBJETS, can end up doing allocations.
    
    This then results in the following lock order:
    
      rq->lock
        zone->lock.rlock
          batched_entropy_u64.lock
    
    Which in turn causes deadlocks when we do wakeups while holding that
    batched_entropy lock -- as the random code does.
    
    Solve this by moving __sched_fork() out from under rq->lock. This is
    safe because nothing there relies on rq->lock, as also evident from the
    other __sched_fork() callsite.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: bigeasy@linutronix.de
    Cc: cl@linux.com
    Cc: keescook@chromium.org
    Cc: penberg@kernel.org
    Cc: rientjes@google.com
    Cc: thgarnie@google.com
    Cc: tytso@mit.edu
    Cc: will@kernel.org
    Fixes: b7d5dc21072c ("random: add a spinlock_t to struct batched_entropy")
    Link: https://lkml.kernel.org/r/20191001091837.GK4536@hirez.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 564577dfee4e55e33ae64e64a866a212e584d58f
Author: Marc Kleine-Budde <mkl@pengutronix.de>
Date:   Mon Oct 8 09:02:30 2018 +0200

    can: netns: remove "can_" prefix from members struct netns_can
    
    This patch improves the code reability by removing the redundant "can_"
    prefix from the members of struct netns_can (as the struct netns_can itself
    is the member "can" of the struct net.)
    
    The conversion is done with:
    
            sed -i \
                    -e "s/struct can_dev_rcv_lists \*can_rx_alldev_list;/struct can_dev_rcv_lists *rx_alldev_list;/" \
                    -e "s/spinlock_t can_rcvlists_lock;/spinlock_t rcvlists_lock;/" \
                    -e "s/struct timer_list can_stattimer;/struct timer_list stattimer; /" \
                    -e "s/can\.can_rx_alldev_list/can.rx_alldev_list/g" \
                    -e "s/can\.can_rcvlists_lock/can.rcvlists_lock/g" \
                    -e "s/can\.can_stattimer/can.stattimer/g" \
                    include/net/netns/can.h \
                    net/can/*.[ch]
    
    Signed-off-by: Oleksij Rempel <o.rempel@pengutronix.de>
    Acked-by: Oliver Hartkopp <socketcan@hartkopp.net>
    Signed-off-by: Marc Kleine-Budde <mkl@pengutronix.de>

commit 01921d53f870653f04ebf8d3c375029ee3ca4a31
Author: Ido Schimmel <idosch@mellanox.com>
Date:   Tue Aug 6 16:19:53 2019 +0300

    drop_monitor: Document scope of spinlock
    
    While 'per_cpu_dm_data' is a per-CPU variable, its 'skb' and
    'send_timer' fields can be accessed concurrently by the CPU sending the
    netlink notification to user space from the workqueue and the CPU
    tracing kfree_skb(). This spinlock is meant to protect against that.
    
    Document its scope and suppress the checkpatch message "spinlock_t
    definition without comment".
    
    Signed-off-by: Ido Schimmel <idosch@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a20eefaee64610541d125cd2c10fa25e278f0bee
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Jul 26 13:22:14 2019 +0200

    staging: most: Use DEFINE_SPINLOCK() instead of struct spinlock
    
    For spinlocks the type spinlock_t should be used instead of "struct
    spinlock".
    
    Use DEFINE_SPINLOCK() and spare the run time initialization
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190704153803.12739-5-bigeasy@linutronix.de
    Link: https://lore.kernel.org/r/alpine.DEB.2.21.1907261319100.1791@nanos.tec.linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5c9254ad7ae32a124fb084badd0cb3720f7c49cb
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jul 4 17:37:57 2019 +0200

    crypto: ux500 - Use spinlock_t instead of struct spinlock
    
    For spinlocks the type spinlock_t should be used instead of "struct
    spinlock".
    
    Use spinlock_t for spinlock's definition.
    
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: linux-crypto@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit d77e9e4e18ce9da3b4981a5c537979c42b06638c
Merge: 2ae048e16636 7fb832ae7294
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 18 10:47:59 2019 -0700

    Merge tag 'linux-watchdog-5.3-rc1' of git://www.linux-watchdog.org/linux-watchdog
    
    Pull watchdog updates from Wim Van Sebroeck:
    
     - add Allwinner H6 watchdog
    
     - drop warning after registering device patches
    
     - hpwdt improvements
    
     - gpio: add support for nowayout option
    
     - introduce CONFIG_WATCHDOG_OPEN_TIMEOUT
    
     - convert remaining drivers to use SPDX license identifier
    
     - Fixes and improvements on several watchdog device drivers
    
    * tag 'linux-watchdog-5.3-rc1' of git://www.linux-watchdog.org/linux-watchdog: (74 commits)
      watchdog: digicolor_wdt: Remove unused variable in dc_wdt_probe
      watchdog: ie6xx_wdt: Use spinlock_t instead of struct spinlock
      watchdog: atmel: atmel-sama5d4-wdt: Disable watchdog on system suspend
      watchdog: convert remaining drivers to use SPDX license identifier
      dt-bindings: watchdog: Rename bindings documentation file
      watchdog: mei_wdt: no need to check return value of debugfs_create functions
      watchdog: bcm_kona_wdt: no need to check return value of debugfs_create functions
      docs: watchdog: Fix build error.
      docs: watchdog: convert docs to ReST and rename to *.rst
      watchdog: make the device time out at open_deadline when open_timeout is used
      watchdog: introduce CONFIG_WATCHDOG_OPEN_TIMEOUT
      watchdog: introduce watchdog.open_timeout commandline parameter
      dt-bindings: watchdog: move i.MX system controller watchdog binding to SCU
      watchdog: imx_sc: Add pretimeout support
      watchdog: renesas_wdt: Add a few cycles delay
      watchdog: gpio: add support for nowayout option
      watchdog: renesas_wdt: Use 'dev' instead of dereferencing it repeatedly
      dt-bindings: watchdog: add Allwinner H6 watchdog
      watchdog: jz4740: Avoid starting watchdog in set_timeout
      watchdog: jz4740: Use register names from <linux/mfd/ingenic-tcu.h>
      ...

commit ef8f3d48afd6a17a0dae8c277c2f539c2f19fd16
Merge: d7d170a8e357 2c207985f354
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 12 11:40:28 2019 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge updates from Andrew Morton:
     "Am experimenting with splitting MM up into identifiable subsystems
      perhaps with a view to gitifying it in complex ways. Also with more
      verbose "incoming" emails.
    
      Most of MM is here and a few other trees.
    
      Subsystems affected by this patch series:
       - hotfixes
       - iommu
       - scripts
       - arch/sh
       - ocfs2
       - mm:slab-generic
       - mm:slub
       - mm:kmemleak
       - mm:kasan
       - mm:cleanups
       - mm:debug
       - mm:pagecache
       - mm:swap
       - mm:memcg
       - mm:gup
       - mm:pagemap
       - mm:infrastructure
       - mm:vmalloc
       - mm:initialization
       - mm:pagealloc
       - mm:vmscan
       - mm:tools
       - mm:proc
       - mm:ras
       - mm:oom-kill
    
      hotfixes:
          mm: vmscan: scan anonymous pages on file refaults
          mm/nvdimm: add is_ioremap_addr and use that to check ioremap address
          mm/memcontrol: fix wrong statistics in memory.stat
          mm/z3fold.c: lock z3fold page before  __SetPageMovable()
          nilfs2: do not use unexported cpu_to_le32()/le32_to_cpu() in uapi header
          MAINTAINERS: nilfs2: update email address
    
      iommu:
          include/linux/dmar.h: replace single-char identifiers in macros
    
      scripts:
          scripts/decode_stacktrace: match basepath using shell prefix operator, not regex
          scripts/decode_stacktrace: look for modules with .ko.debug extension
          scripts/spelling.txt: drop "sepc" from the misspelling list
          scripts/spelling.txt: add spelling fix for prohibited
          scripts/decode_stacktrace: Accept dash/underscore in modules
          scripts/spelling.txt: add more spellings to spelling.txt
    
      arch/sh:
          arch/sh/configs/sdk7786_defconfig: remove CONFIG_LOGFS
          sh: config: remove left-over BACKLIGHT_LCD_SUPPORT
          sh: prevent warnings when using iounmap
    
      ocfs2:
          fs: ocfs: fix spelling mistake "hearbeating" -> "heartbeat"
          ocfs2/dlm: use struct_size() helper
          ocfs2: add last unlock times in locking_state
          ocfs2: add locking filter debugfs file
          ocfs2: add first lock wait time in locking_state
          ocfs: no need to check return value of debugfs_create functions
          fs/ocfs2/dlmglue.c: unneeded variable: "status"
          ocfs2: use kmemdup rather than duplicating its implementation
    
      mm:slab-generic:
        Patch series "mm/slab: Improved sanity checking":
          mm/slab: validate cache membership under freelist hardening
          mm/slab: sanity-check page type when looking up cache
          lkdtm/heap: add tests for freelist hardening
    
      mm:slub:
          mm/slub.c: avoid double string traverse in kmem_cache_flags()
          slub: don't panic for memcg kmem cache creation failure
    
      mm:kmemleak:
          mm/kmemleak.c: fix check for softirq context
          mm/kmemleak.c: change error at _write when kmemleak is disabled
          docs: kmemleak: add more documentation details
    
      mm:kasan:
          mm/kasan: print frame description for stack bugs
          Patch series "Bitops instrumentation for KASAN", v5:
            lib/test_kasan: add bitops tests
            x86: use static_cpu_has in uaccess region to avoid instrumentation
            asm-generic, x86: add bitops instrumentation for KASAN
          Patch series "mm/kasan: Add object validation in ksize()", v3:
            mm/kasan: introduce __kasan_check_{read,write}
            mm/kasan: change kasan_check_{read,write} to return boolean
            lib/test_kasan: Add test for double-kzfree detection
            mm/slab: refactor common ksize KASAN logic into slab_common.c
            mm/kasan: add object validation in ksize()
    
      mm:cleanups:
          include/linux/pfn_t.h: remove pfn_t_to_virt()
          Patch series "remove ARCH_SELECT_MEMORY_MODEL where it has no effect":
            arm: remove ARCH_SELECT_MEMORY_MODEL
            s390: remove ARCH_SELECT_MEMORY_MODEL
            sparc: remove ARCH_SELECT_MEMORY_MODEL
          mm/gup.c: make follow_page_mask() static
          mm/memory.c: trivial clean up in insert_page()
          mm: make !CONFIG_HUGE_PAGE wrappers into static inlines
          include/linux/mm_types.h: ifdef struct vm_area_struct::swap_readahead_info
          mm: remove the account_page_dirtied export
          mm/page_isolation.c: change the prototype of undo_isolate_page_range()
          include/linux/vmpressure.h: use spinlock_t instead of struct spinlock
          mm: remove the exporting of totalram_pages
          include/linux/pagemap.h: document trylock_page() return value
    
      mm:debug:
          mm/failslab.c: by default, do not fail allocations with direct reclaim only
          Patch series "debug_pagealloc improvements":
            mm, debug_pagelloc: use static keys to enable debugging
            mm, page_alloc: more extensive free page checking with debug_pagealloc
            mm, debug_pagealloc: use a page type instead of page_ext flag
    
      mm:pagecache:
          Patch series "fix filler_t callback type mismatches", v2:
            mm/filemap.c: fix an overly long line in read_cache_page
            mm/filemap: don't cast ->readpage to filler_t for do_read_cache_page
            jffs2: pass the correct prototype to read_cache_page
            9p: pass the correct prototype to read_cache_page
          mm/filemap.c: correct the comment about VM_FAULT_RETRY
    
      mm:swap:
          mm, swap: fix race between swapoff and some swap operations
          mm/swap_state.c: simplify total_swapcache_pages() with get_swap_device()
          mm, swap: use rbtree for swap_extent
          mm/mincore.c: fix race between swapoff and mincore
    
      mm:memcg:
          memcg, oom: no oom-kill for __GFP_RETRY_MAYFAIL
          memcg, fsnotify: no oom-kill for remote memcg charging
          mm, memcg: introduce memory.events.local
          mm: memcontrol: dump memory.stat during cgroup OOM
          Patch series "mm: reparent slab memory on cgroup removal", v7:
            mm: memcg/slab: postpone kmem_cache memcg pointer initialization to memcg_link_cache()
            mm: memcg/slab: rename slab delayed deactivation functions and fields
            mm: memcg/slab: generalize postponed non-root kmem_cache deactivation
            mm: memcg/slab: introduce __memcg_kmem_uncharge_memcg()
            mm: memcg/slab: unify SLAB and SLUB page accounting
            mm: memcg/slab: don't check the dying flag on kmem_cache creation
            mm: memcg/slab: synchronize access to kmem_cache dying flag using a spinlock
            mm: memcg/slab: rework non-root kmem_cache lifecycle management
            mm: memcg/slab: stop setting page->mem_cgroup pointer for slab pages
            mm: memcg/slab: reparent memcg kmem_caches on cgroup removal
          mm, memcg: add a memcg_slabinfo debugfs file
    
      mm:gup:
          Patch series "switch the remaining architectures to use generic GUP", v4:
            mm: use untagged_addr() for get_user_pages_fast addresses
            mm: simplify gup_fast_permitted
            mm: lift the x86_32 PAE version of gup_get_pte to common code
            MIPS: use the generic get_user_pages_fast code
            sh: add the missing pud_page definition
            sh: use the generic get_user_pages_fast code
            sparc64: add the missing pgd_page definition
            sparc64: define untagged_addr()
            sparc64: use the generic get_user_pages_fast code
            mm: rename CONFIG_HAVE_GENERIC_GUP to CONFIG_HAVE_FAST_GUP
            mm: reorder code blocks in gup.c
            mm: consolidate the get_user_pages* implementations
            mm: validate get_user_pages_fast flags
            mm: move the powerpc hugepd code to mm/gup.c
            mm: switch gup_hugepte to use try_get_compound_head
            mm: mark the page referenced in gup_hugepte
          mm/gup: speed up check_and_migrate_cma_pages() on huge page
          mm/gup.c: remove some BUG_ONs from get_gate_page()
          mm/gup.c: mark undo_dev_pagemap as __maybe_unused
    
      mm:pagemap:
          asm-generic, x86: introduce generic pte_{alloc,free}_one[_kernel]
          alpha: switch to generic version of pte allocation
          arm: switch to generic version of pte allocation
          arm64: switch to generic version of pte allocation
          csky: switch to generic version of pte allocation
          m68k: sun3: switch to generic version of pte allocation
          mips: switch to generic version of pte allocation
          nds32: switch to generic version of pte allocation
          nios2: switch to generic version of pte allocation
          parisc: switch to generic version of pte allocation
          riscv: switch to generic version of pte allocation
          um: switch to generic version of pte allocation
          unicore32: switch to generic version of pte allocation
          mm/pgtable: drop pgtable_t variable from pte_fn_t functions
          mm/memory.c: fail when offset == num in first check of __vm_map_pages()
    
      mm:infrastructure:
          mm/mmu_notifier: use hlist_add_head_rcu()
    
      mm:vmalloc:
          Patch series "Some cleanups for the KVA/vmalloc", v5:
            mm/vmalloc.c: remove "node" argument
            mm/vmalloc.c: preload a CPU with one object for split purpose
            mm/vmalloc.c: get rid of one single unlink_va() when merge
            mm/vmalloc.c: switch to WARN_ON() and move it under unlink_va()
          mm/vmalloc.c: spelling> s/informaion/information/
    
      mm:initialization:
          mm/large system hash: use vmalloc for size > MAX_ORDER when !hashdist
          mm/large system hash: clear hashdist when only one node with memory is booted
    
      mm:pagealloc:
          arm64: move jump_label_init() before parse_early_param()
          Patch series "add init_on_alloc/init_on_free boot options", v10:
            mm: security: introduce init_on_alloc=1 and init_on_free=1 boot options
            mm: init: report memory auto-initialization features at boot time
    
      mm:vmscan:
          mm: vmscan: remove double slab pressure by inc'ing sc->nr_scanned
          mm: vmscan: correct some vmscan counters for THP swapout
    
      mm:tools:
          tools/vm/slabinfo: order command line options
          tools/vm/slabinfo: add partial slab listing to -X
          tools/vm/slabinfo: add option to sort by partial slabs
          tools/vm/slabinfo: add sorting info to help menu
    
      mm:proc:
          proc: use down_read_killable mmap_sem for /proc/pid/maps
          proc: use down_read_killable mmap_sem for /proc/pid/smaps_rollup
          proc: use down_read_killable mmap_sem for /proc/pid/pagemap
          proc: use down_read_killable mmap_sem for /proc/pid/clear_refs
          proc: use down_read_killable mmap_sem for /proc/pid/map_files
          mm: use down_read_killable for locking mmap_sem in access_remote_vm
          mm: smaps: split PSS into components
          mm: vmalloc: show number of vmalloc pages in /proc/meminfo
    
      mm:ras:
          mm/memory-failure.c: clarify error message
    
      mm:oom-kill:
          mm: memcontrol: use CSS_TASK_ITER_PROCS at mem_cgroup_scan_tasks()
          mm, oom: refactor dump_tasks for memcg OOMs
          mm, oom: remove redundant task_in_mem_cgroup() check
          oom: decouple mems_allowed from oom_unkillable_task
          mm/oom_kill.c: remove redundant OOM score normalization in select_bad_process()"
    
    * akpm: (147 commits)
      mm/oom_kill.c: remove redundant OOM score normalization in select_bad_process()
      oom: decouple mems_allowed from oom_unkillable_task
      mm, oom: remove redundant task_in_mem_cgroup() check
      mm, oom: refactor dump_tasks for memcg OOMs
      mm: memcontrol: use CSS_TASK_ITER_PROCS at mem_cgroup_scan_tasks()
      mm/memory-failure.c: clarify error message
      mm: vmalloc: show number of vmalloc pages in /proc/meminfo
      mm: smaps: split PSS into components
      mm: use down_read_killable for locking mmap_sem in access_remote_vm
      proc: use down_read_killable mmap_sem for /proc/pid/map_files
      proc: use down_read_killable mmap_sem for /proc/pid/clear_refs
      proc: use down_read_killable mmap_sem for /proc/pid/pagemap
      proc: use down_read_killable mmap_sem for /proc/pid/smaps_rollup
      proc: use down_read_killable mmap_sem for /proc/pid/maps
      tools/vm/slabinfo: add sorting info to help menu
      tools/vm/slabinfo: add option to sort by partial slabs
      tools/vm/slabinfo: add partial slab listing to -X
      tools/vm/slabinfo: order command line options
      mm: vmscan: correct some vmscan counters for THP swapout
      mm: vmscan: remove double slab pressure by inc'ing sc->nr_scanned
      ...

commit 51b176290496518d6701bc40e63f70e4b6870198
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jul 11 20:54:52 2019 -0700

    include/linux/vmpressure.h: use spinlock_t instead of struct spinlock
    
    For spinlocks the type spinlock_t should be used instead of "struct
    spinlock".
    
    Use spinlock_t for spinlock's definition.
    
    Link: http://lkml.kernel.org/r/20190704153803.12739-3-bigeasy@linutronix.de
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 2ec98f567888501df0140c858af5f5ea10216a6f
Merge: 96407298ff6e 9b3b623804a6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 9 09:07:00 2019 -0700

    Merge tag 'gpio-v5.3-1' of git://git.kernel.org/pub/scm/linux/kernel/git/linusw/linux-gpio
    
    Pull GPIO updates from Linus Walleij:
     "This is the big slew of GPIO changes for the v5.3 kernel cycle. This
      is mostly incremental work this time.
    
      Three important things:
    
       - The FMC subsystem is deleted through my tree. This happens through
         GPIO as its demise was discussed in relation to a patch decoupling
         its GPIO implementation from the standard way of handling GPIO. As
         it turns out, that is not the only subsystem it reimplements and
         the authors think it is better do scratch it and start over using
         the proper kernel subsystems than try to polish the rust shiny. See
         the commit (ACKed by the maintainers) for details.
    
       - Arnd made a small devres patch that was ACKed by Greg and goes into
         the device core.
    
       - SPDX header change colissions may happen, because at times I've
         seen that quite a lot changed during the -rc:s in regards to SPDX.
         (It is good stuff, tglx has me convinced, and it is worth the
         occasional pain.)
    
      Apart from this is is nothing controversial or problematic.
    
      Summary:
    
      Core:
    
       - When a gpio_chip request GPIOs from itself, it can now fully
         control the line characteristics, both machine and consumer flags.
         This makes a lot of sense, but took some time before I figured out
         that this is how it has to work.
    
       - Several smallish documentation fixes.
    
      New drivers:
    
       - The PCA953x driver now supports the TI TCA9539.
    
       - The DaVinci driver now supports the K3 AM654 SoCs.
    
      Driver improvements:
    
       - Major overhaul and hardening of the OMAP driver by Russell King.
    
       - Starting to move some drivers to the new API passing irq_chip along
         with the gpio_chip when adding the gpio_chip instead of adding it
         separately.
    
      Unrelated:
    
       - Delete the FMC subsystem"
    
    * tag 'gpio-v5.3-1' of git://git.kernel.org/pub/scm/linux/kernel/git/linusw/linux-gpio: (87 commits)
      Revert "gpio: tegra: Clean-up debugfs initialisation"
      gpiolib: Use spinlock_t instead of struct spinlock
      gpio: stp-xway: allow compile-testing
      gpio: stp-xway: get rid of the #include <lantiq_soc.h> dependency
      gpio: stp-xway: improve module clock error handling
      gpio: stp-xway: simplify error handling in xway_stp_probe()
      gpiolib: Clarify use of non-sleeping functions
      gpiolib: Fix references to gpiod_[gs]et_*value_cansleep() variants
      gpiolib: Document new gpio_chip.init_valid_mask field
      Documentation: gpio: Fix reference to gpiod_get_array()
      gpio: pl061: drop duplicate printing of device name
      gpio: altera: Pass irqchip when adding gpiochip
      gpio: siox: Use devm_ managed gpiochip
      gpio: siox: Add struct device *dev helper variable
      gpio: siox: Pass irqchip when adding gpiochip
      drivers: gpio: amd-fch: make resource struct const
      devres: allow const resource arguments
      gpio: ath79: Pass irqchip when adding gpiochip
      gpio: tegra: Clean-up debugfs initialisation
      gpio: siox: Switch to IRQ_TYPE_NONE
      ...

commit 053bc5764bb0a84ef0b26a8e4ddd3e2f4b4f8215
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jul 4 17:38:01 2019 +0200

    watchdog: ie6xx_wdt: Use spinlock_t instead of struct spinlock
    
    For spinlocks the type spinlock_t should be used instead of "struct
    spinlock".
    
    Use spinlock_t for spinlock's definition.
    
    Cc: Wim Van Sebroeck <wim@linux-watchdog.org>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: linux-watchdog@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Wim Van Sebroeck <wim@linux-watchdog.org>

commit f0b40863bee4e5f689f6c839b4d0b512b8f0fdbe
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jul 4 17:38:03 2019 +0200

    gpiolib: Use spinlock_t instead of struct spinlock
    
    For spinlocks the type spinlock_t should be used instead of "struct
    spinlock".
    
    Use spinlock_t for spinlock's definition.
    
    Cc: linux-gpio@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Link: https://lore.kernel.org/r/20190704153803.12739-8-bigeasy@linutronix.de
    Reviewed-by: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit f654e676702e88650e78ed8ea1b118662f80b28a
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Jul 4 17:38:02 2019 +0200

    nfp: Use spinlock_t instead of struct spinlock
    
    For spinlocks the type spinlock_t should be used instead of "struct
    spinlock".
    
    Use spinlock_t for spinlock's definition.
    
    Cc: Jakub Kicinski <jakub.kicinski@netronome.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: oss-drivers@netronome.com
    Cc: netdev@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a7d881464194920a04fb9b8c324877b32fd58a1f
Merge: f7fd41afa15d a5a9dffcc903
Author: Olof Johansson <olof@lixom.net>
Date:   Tue Jun 25 05:51:56 2019 -0700

    Merge tag 'imx-soc-5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/shawnguo/linux into arm/soc
    
    i.MX SoC changes for 5.3:
     - Switch imx7d to imx-cpufreq-dt for speed-grading, as imx-cpufreq-dt
       driver can handle speed grading bits on imx7d just like on imx8mq.
     - Improve imx6 cpuidle driver to use raw_spinlock_t.  The change makes
       no difference for !RT build, but is required by RT kernel.
    
    * tag 'imx-soc-5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/shawnguo/linux:
      ARM: imx: Switch imx7d to imx-cpufreq-dt for speed-grading
      ARM: imx6: cpuidle: Use raw_spinlock_t
    
    Signed-off-by: Olof Johansson <olof@lixom.net>

commit 9d174476ddc137d3b47b3336f32edaa0ad40e158
Author: Bitan Biswas <bbiswas@nvidia.com>
Date:   Tue Jun 11 03:51:11 2019 -0700

    i2c: tegra: add spinlock definition comment
    
    Fix checkpatch.pl CHECK as follows:
    CHECK: spinlock_t definition without comment
    +       spinlock_t xfer_lock;
    
    Signed-off-by: Bitan Biswas <bbiswas@nvidia.com>
    Reviewed-by: Dmitry Osipenko <digetx@gmail.com>
    Signed-off-by: Wolfram Sang <wsa@the-dreams.de>

commit 99ae52edeba17c78753695e0d94d49c5f9e9a803
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed May 29 17:42:29 2019 +0200

    ARM: imx6: cpuidle: Use raw_spinlock_t
    
    The idle call back is invoked with disabled interrupts and requires
    raw_spinlock_t locks to work.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Fabio Estevam <festevam@gmail.com>
    Signed-off-by: Shawn Guo <shawnguo@kernel.org>

commit 8334ab419d7e4e0576259437bd91d41e0dd1336c
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sat Apr 20 00:09:51 2019 -0400

    random: add a spinlock_t to struct batched_entropy
    
    [ Upstream commit b7d5dc21072cda7124d13eae2aefb7343ef94197 ]
    
    The per-CPU variable batched_entropy_uXX is protected by get_cpu_var().
    This is just a preempt_disable() which ensures that the variable is only
    from the local CPU. It does not protect against users on the same CPU
    from another context. It is possible that a preemptible context reads
    slot 0 and then an interrupt occurs and the same value is read again.
    
    The above scenario is confirmed by lockdep if we add a spinlock:
    | ================================
    | WARNING: inconsistent lock state
    | 5.1.0-rc3+ #42 Not tainted
    | --------------------------------
    | inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.
    | ksoftirqd/9/56 [HC0[0]:SC1[1]:HE0:SE0] takes:
    | (____ptrval____) (batched_entropy_u32.lock){+.?.}, at: get_random_u32+0x3e/0xe0
    | {SOFTIRQ-ON-W} state was registered at:
    |   _raw_spin_lock+0x2a/0x40
    |   get_random_u32+0x3e/0xe0
    |   new_slab+0x15c/0x7b0
    |   ___slab_alloc+0x492/0x620
    |   __slab_alloc.isra.73+0x53/0xa0
    |   kmem_cache_alloc_node+0xaf/0x2a0
    |   copy_process.part.41+0x1e1/0x2370
    |   _do_fork+0xdb/0x6d0
    |   kernel_thread+0x20/0x30
    |   kthreadd+0x1ba/0x220
    |   ret_from_fork+0x3a/0x50
    …
    | other info that might help us debug this:
    |  Possible unsafe locking scenario:
    |
    |        CPU0
    |        ----
    |   lock(batched_entropy_u32.lock);
    |   <Interrupt>
    |     lock(batched_entropy_u32.lock);
    |
    |  *** DEADLOCK ***
    |
    | stack backtrace:
    | Call Trace:
    …
    |  kmem_cache_alloc_trace+0x20e/0x270
    |  ipmi_alloc_recv_msg+0x16/0x40
    …
    |  __do_softirq+0xec/0x48d
    |  run_ksoftirqd+0x37/0x60
    |  smpboot_thread_fn+0x191/0x290
    |  kthread+0xfe/0x130
    |  ret_from_fork+0x3a/0x50
    
    Add a spinlock_t to the batched_entropy data structure and acquire the
    lock while accessing it. Acquire the lock with disabled interrupts
    because this function may be used from interrupt context.
    
    Remove the batched_entropy_reset_lock lock. Now that we have a lock for
    the data scructure, we can access it from a remote CPU.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 944c58523731c1a791d324536baf3a1ff2c34f1f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sat Apr 20 00:09:51 2019 -0400

    random: add a spinlock_t to struct batched_entropy
    
    [ Upstream commit b7d5dc21072cda7124d13eae2aefb7343ef94197 ]
    
    The per-CPU variable batched_entropy_uXX is protected by get_cpu_var().
    This is just a preempt_disable() which ensures that the variable is only
    from the local CPU. It does not protect against users on the same CPU
    from another context. It is possible that a preemptible context reads
    slot 0 and then an interrupt occurs and the same value is read again.
    
    The above scenario is confirmed by lockdep if we add a spinlock:
    | ================================
    | WARNING: inconsistent lock state
    | 5.1.0-rc3+ #42 Not tainted
    | --------------------------------
    | inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.
    | ksoftirqd/9/56 [HC0[0]:SC1[1]:HE0:SE0] takes:
    | (____ptrval____) (batched_entropy_u32.lock){+.?.}, at: get_random_u32+0x3e/0xe0
    | {SOFTIRQ-ON-W} state was registered at:
    |   _raw_spin_lock+0x2a/0x40
    |   get_random_u32+0x3e/0xe0
    |   new_slab+0x15c/0x7b0
    |   ___slab_alloc+0x492/0x620
    |   __slab_alloc.isra.73+0x53/0xa0
    |   kmem_cache_alloc_node+0xaf/0x2a0
    |   copy_process.part.41+0x1e1/0x2370
    |   _do_fork+0xdb/0x6d0
    |   kernel_thread+0x20/0x30
    |   kthreadd+0x1ba/0x220
    |   ret_from_fork+0x3a/0x50
    …
    | other info that might help us debug this:
    |  Possible unsafe locking scenario:
    |
    |        CPU0
    |        ----
    |   lock(batched_entropy_u32.lock);
    |   <Interrupt>
    |     lock(batched_entropy_u32.lock);
    |
    |  *** DEADLOCK ***
    |
    | stack backtrace:
    | Call Trace:
    …
    |  kmem_cache_alloc_trace+0x20e/0x270
    |  ipmi_alloc_recv_msg+0x16/0x40
    …
    |  __do_softirq+0xec/0x48d
    |  run_ksoftirqd+0x37/0x60
    |  smpboot_thread_fn+0x191/0x290
    |  kthread+0xfe/0x130
    |  ret_from_fork+0x3a/0x50
    
    Add a spinlock_t to the batched_entropy data structure and acquire the
    lock while accessing it. Acquire the lock with disabled interrupts
    because this function may be used from interrupt context.
    
    Remove the batched_entropy_reset_lock lock. Now that we have a lock for
    the data scructure, we can access it from a remote CPU.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d6c68321e7856a247672205a33a61f6c668ac070
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sat Apr 20 00:09:51 2019 -0400

    random: add a spinlock_t to struct batched_entropy
    
    [ Upstream commit b7d5dc21072cda7124d13eae2aefb7343ef94197 ]
    
    The per-CPU variable batched_entropy_uXX is protected by get_cpu_var().
    This is just a preempt_disable() which ensures that the variable is only
    from the local CPU. It does not protect against users on the same CPU
    from another context. It is possible that a preemptible context reads
    slot 0 and then an interrupt occurs and the same value is read again.
    
    The above scenario is confirmed by lockdep if we add a spinlock:
    | ================================
    | WARNING: inconsistent lock state
    | 5.1.0-rc3+ #42 Not tainted
    | --------------------------------
    | inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.
    | ksoftirqd/9/56 [HC0[0]:SC1[1]:HE0:SE0] takes:
    | (____ptrval____) (batched_entropy_u32.lock){+.?.}, at: get_random_u32+0x3e/0xe0
    | {SOFTIRQ-ON-W} state was registered at:
    |   _raw_spin_lock+0x2a/0x40
    |   get_random_u32+0x3e/0xe0
    |   new_slab+0x15c/0x7b0
    |   ___slab_alloc+0x492/0x620
    |   __slab_alloc.isra.73+0x53/0xa0
    |   kmem_cache_alloc_node+0xaf/0x2a0
    |   copy_process.part.41+0x1e1/0x2370
    |   _do_fork+0xdb/0x6d0
    |   kernel_thread+0x20/0x30
    |   kthreadd+0x1ba/0x220
    |   ret_from_fork+0x3a/0x50
    …
    | other info that might help us debug this:
    |  Possible unsafe locking scenario:
    |
    |        CPU0
    |        ----
    |   lock(batched_entropy_u32.lock);
    |   <Interrupt>
    |     lock(batched_entropy_u32.lock);
    |
    |  *** DEADLOCK ***
    |
    | stack backtrace:
    | Call Trace:
    …
    |  kmem_cache_alloc_trace+0x20e/0x270
    |  ipmi_alloc_recv_msg+0x16/0x40
    …
    |  __do_softirq+0xec/0x48d
    |  run_ksoftirqd+0x37/0x60
    |  smpboot_thread_fn+0x191/0x290
    |  kthread+0xfe/0x130
    |  ret_from_fork+0x3a/0x50
    
    Add a spinlock_t to the batched_entropy data structure and acquire the
    lock while accessing it. Acquire the lock with disabled interrupts
    because this function may be used from interrupt context.
    
    Remove the batched_entropy_reset_lock lock. Now that we have a lock for
    the data scructure, we can access it from a remote CPU.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 50a41f60e5deb7edb36f3861db1ff45e2d9c8c63
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sat Apr 20 00:09:51 2019 -0400

    random: add a spinlock_t to struct batched_entropy
    
    [ Upstream commit b7d5dc21072cda7124d13eae2aefb7343ef94197 ]
    
    The per-CPU variable batched_entropy_uXX is protected by get_cpu_var().
    This is just a preempt_disable() which ensures that the variable is only
    from the local CPU. It does not protect against users on the same CPU
    from another context. It is possible that a preemptible context reads
    slot 0 and then an interrupt occurs and the same value is read again.
    
    The above scenario is confirmed by lockdep if we add a spinlock:
    | ================================
    | WARNING: inconsistent lock state
    | 5.1.0-rc3+ #42 Not tainted
    | --------------------------------
    | inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.
    | ksoftirqd/9/56 [HC0[0]:SC1[1]:HE0:SE0] takes:
    | (____ptrval____) (batched_entropy_u32.lock){+.?.}, at: get_random_u32+0x3e/0xe0
    | {SOFTIRQ-ON-W} state was registered at:
    |   _raw_spin_lock+0x2a/0x40
    |   get_random_u32+0x3e/0xe0
    |   new_slab+0x15c/0x7b0
    |   ___slab_alloc+0x492/0x620
    |   __slab_alloc.isra.73+0x53/0xa0
    |   kmem_cache_alloc_node+0xaf/0x2a0
    |   copy_process.part.41+0x1e1/0x2370
    |   _do_fork+0xdb/0x6d0
    |   kernel_thread+0x20/0x30
    |   kthreadd+0x1ba/0x220
    |   ret_from_fork+0x3a/0x50
    …
    | other info that might help us debug this:
    |  Possible unsafe locking scenario:
    |
    |        CPU0
    |        ----
    |   lock(batched_entropy_u32.lock);
    |   <Interrupt>
    |     lock(batched_entropy_u32.lock);
    |
    |  *** DEADLOCK ***
    |
    | stack backtrace:
    | Call Trace:
    …
    |  kmem_cache_alloc_trace+0x20e/0x270
    |  ipmi_alloc_recv_msg+0x16/0x40
    …
    |  __do_softirq+0xec/0x48d
    |  run_ksoftirqd+0x37/0x60
    |  smpboot_thread_fn+0x191/0x290
    |  kthread+0xfe/0x130
    |  ret_from_fork+0x3a/0x50
    
    Add a spinlock_t to the batched_entropy data structure and acquire the
    lock while accessing it. Acquire the lock with disabled interrupts
    because this function may be used from interrupt context.
    
    Remove the batched_entropy_reset_lock lock. Now that we have a lock for
    the data scructure, we can access it from a remote CPU.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit dd5001e21a991b731d659857cd07acc7a13e6789
Merge: a9fbcd672883 b7d5dc21072c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 7 21:42:23 2019 -0700

    Merge tag 'random_for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tytso/random
    
    Pull randomness updates from Ted Ts'o:
    
     - initialize the random driver earler
    
     - fix CRNG initialization when we trust the CPU's RNG on NUMA systems
    
     - other miscellaneous cleanups and fixes.
    
    * tag 'random_for_linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tytso/random:
      random: add a spinlock_t to struct batched_entropy
      random: document get_random_int() family
      random: fix CRNG initialization when random.trust_cpu=1
      random: move rand_initialize() earlier
      random: only read from /dev/random after its pool has received 128 bits
      drivers/char/random.c: make primary_crng static
      drivers/char/random.c: remove unused stuct poolinfo::poolbits
      drivers/char/random.c: constify poolinfo_table

commit 9076c49bdca2aa68c805f2677b2bccc4bde2bac1
Merge: 3745dc24aa7a 263d0b353341
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Sat Apr 27 09:07:49 2019 -0700

    Merge branch 'sk-local-storage'
    
    Martin KaFai Lau says:
    
    ====================
    v4:
    - Move checks to map_alloc_check in patch 1 (Stanislav Fomichev)
    - Refactor BTF encoding macros to test_btf.h at
      a new patch 4 (Stanislav Fomichev)
    - Refactor getenv and add print PASS message at the
      end of the test in patch 6 (Yonghong Song)
    
    v3:
    - Replace spinlock_types.h with spinlock.h in patch 1
      (kbuild test robot <lkp@intel.com>)
    
    v2:
    - Add the "test_maps.h" file in patch 5
    
    This series introduces the BPF sk local storage.  The
    details is in the patch 1 commit message.
    ====================
    
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

commit 4528b128bbbc138b295f1dc9ff19fb1a9552bc88
Author: Yang Shi <yang.shi@linaro.org>
Date:   Wed Feb 13 17:14:23 2019 +0100

    ARM: 8839/1: kprobe: make patch_lock a raw_spinlock_t
    
    [ Upstream commit 143c2a89e0e5fda6c6fd08d7bc1126438c19ae90 ]
    
    When running kprobe on -rt kernel, the below bug is caught:
    
    |BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:931
    |in_atomic(): 1, irqs_disabled(): 128, pid: 14, name: migration/0
    |Preemption disabled at:[<802f2b98>] cpu_stopper_thread+0xc0/0x140
    |CPU: 0 PID: 14 Comm: migration/0 Tainted: G O 4.8.3-rt2 #1
    |Hardware name: Freescale LS1021A
    |[<8025a43c>] (___might_sleep)
    |[<80b5b324>] (rt_spin_lock)
    |[<80b5c31c>] (__patch_text_real)
    |[<80b5c3ac>] (patch_text_stop_machine)
    |[<802f2920>] (multi_cpu_stop)
    
    Since patch_text_stop_machine() is called in stop_machine() which
    disables IRQ, sleepable lock should be not used in this atomic context,
     so replace patch_lock to raw lock.
    
    Signed-off-by: Yang Shi <yang.shi@linaro.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 1c2bfc4636de925f47d5f4e111e32235da0ef646
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 13 17:14:42 2019 +0100

    ARM: 8840/1: use a raw_spinlock_t in unwind
    
    [ Upstream commit 74ffe79ae538283bbf7c155e62339f1e5c87b55a ]
    
    Mostly unwind is done with irqs enabled however SLUB may call it with
    irqs disabled while creating a new SLUB cache.
    
    I had system freeze while loading a module which called
    kmem_cache_create() on init. That means SLUB's __slab_alloc() disabled
    interrupts and then
    
    ->new_slab_objects()
     ->new_slab()
      ->setup_object()
       ->setup_object_debug()
        ->init_tracking()
         ->set_track()
          ->save_stack_trace()
           ->save_stack_trace_tsk()
            ->walk_stackframe()
             ->unwind_frame()
              ->unwind_find_idx()
               =>spin_lock_irqsave(&unwind_lock);
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 61c4d9023651d8a0a2f636285a9af44bd072d519
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 13 17:14:42 2019 +0100

    ARM: 8840/1: use a raw_spinlock_t in unwind
    
    [ Upstream commit 74ffe79ae538283bbf7c155e62339f1e5c87b55a ]
    
    Mostly unwind is done with irqs enabled however SLUB may call it with
    irqs disabled while creating a new SLUB cache.
    
    I had system freeze while loading a module which called
    kmem_cache_create() on init. That means SLUB's __slab_alloc() disabled
    interrupts and then
    
    ->new_slab_objects()
     ->new_slab()
      ->setup_object()
       ->setup_object_debug()
        ->init_tracking()
         ->set_track()
          ->save_stack_trace()
           ->save_stack_trace_tsk()
            ->walk_stackframe()
             ->unwind_frame()
              ->unwind_find_idx()
               =>spin_lock_irqsave(&unwind_lock);
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 3835c46e6ff51d24698487ef1e073573264dcfd6
Author: Yang Shi <yang.shi@linaro.org>
Date:   Wed Feb 13 17:14:23 2019 +0100

    ARM: 8839/1: kprobe: make patch_lock a raw_spinlock_t
    
    [ Upstream commit 143c2a89e0e5fda6c6fd08d7bc1126438c19ae90 ]
    
    When running kprobe on -rt kernel, the below bug is caught:
    
    |BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:931
    |in_atomic(): 1, irqs_disabled(): 128, pid: 14, name: migration/0
    |Preemption disabled at:[<802f2b98>] cpu_stopper_thread+0xc0/0x140
    |CPU: 0 PID: 14 Comm: migration/0 Tainted: G O 4.8.3-rt2 #1
    |Hardware name: Freescale LS1021A
    |[<8025a43c>] (___might_sleep)
    |[<80b5b324>] (rt_spin_lock)
    |[<80b5c31c>] (__patch_text_real)
    |[<80b5c3ac>] (patch_text_stop_machine)
    |[<802f2920>] (multi_cpu_stop)
    
    Since patch_text_stop_machine() is called in stop_machine() which
    disables IRQ, sleepable lock should be not used in this atomic context,
     so replace patch_lock to raw lock.
    
    Signed-off-by: Yang Shi <yang.shi@linaro.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 91583411b5d1cee67b1e0f83754430aed5a1535f
Author: Yang Shi <yang.shi@linaro.org>
Date:   Wed Feb 13 17:14:23 2019 +0100

    ARM: 8839/1: kprobe: make patch_lock a raw_spinlock_t
    
    [ Upstream commit 143c2a89e0e5fda6c6fd08d7bc1126438c19ae90 ]
    
    When running kprobe on -rt kernel, the below bug is caught:
    
    |BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:931
    |in_atomic(): 1, irqs_disabled(): 128, pid: 14, name: migration/0
    |Preemption disabled at:[<802f2b98>] cpu_stopper_thread+0xc0/0x140
    |CPU: 0 PID: 14 Comm: migration/0 Tainted: G O 4.8.3-rt2 #1
    |Hardware name: Freescale LS1021A
    |[<8025a43c>] (___might_sleep)
    |[<80b5b324>] (rt_spin_lock)
    |[<80b5c31c>] (__patch_text_real)
    |[<80b5c3ac>] (patch_text_stop_machine)
    |[<802f2920>] (multi_cpu_stop)
    
    Since patch_text_stop_machine() is called in stop_machine() which
    disables IRQ, sleepable lock should be not used in this atomic context,
     so replace patch_lock to raw lock.
    
    Signed-off-by: Yang Shi <yang.shi@linaro.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 5e3f6ba82ed450d62ac7910b0fe026d31eb2aa5b
Author: Yang Shi <yang.shi@linaro.org>
Date:   Wed Feb 13 17:14:23 2019 +0100

    ARM: 8839/1: kprobe: make patch_lock a raw_spinlock_t
    
    [ Upstream commit 143c2a89e0e5fda6c6fd08d7bc1126438c19ae90 ]
    
    When running kprobe on -rt kernel, the below bug is caught:
    
    |BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:931
    |in_atomic(): 1, irqs_disabled(): 128, pid: 14, name: migration/0
    |Preemption disabled at:[<802f2b98>] cpu_stopper_thread+0xc0/0x140
    |CPU: 0 PID: 14 Comm: migration/0 Tainted: G O 4.8.3-rt2 #1
    |Hardware name: Freescale LS1021A
    |[<8025a43c>] (___might_sleep)
    |[<80b5b324>] (rt_spin_lock)
    |[<80b5c31c>] (__patch_text_real)
    |[<80b5c3ac>] (patch_text_stop_machine)
    |[<802f2920>] (multi_cpu_stop)
    
    Since patch_text_stop_machine() is called in stop_machine() which
    disables IRQ, sleepable lock should be not used in this atomic context,
     so replace patch_lock to raw lock.
    
    Signed-off-by: Yang Shi <yang.shi@linaro.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit a05948319bf995b68b136680ece97341a79e207e
Author: Yang Shi <yang.shi@linaro.org>
Date:   Wed Feb 13 17:14:23 2019 +0100

    ARM: 8839/1: kprobe: make patch_lock a raw_spinlock_t
    
    [ Upstream commit 143c2a89e0e5fda6c6fd08d7bc1126438c19ae90 ]
    
    When running kprobe on -rt kernel, the below bug is caught:
    
    |BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:931
    |in_atomic(): 1, irqs_disabled(): 128, pid: 14, name: migration/0
    |Preemption disabled at:[<802f2b98>] cpu_stopper_thread+0xc0/0x140
    |CPU: 0 PID: 14 Comm: migration/0 Tainted: G O 4.8.3-rt2 #1
    |Hardware name: Freescale LS1021A
    |[<8025a43c>] (___might_sleep)
    |[<80b5b324>] (rt_spin_lock)
    |[<80b5c31c>] (__patch_text_real)
    |[<80b5c3ac>] (patch_text_stop_machine)
    |[<802f2920>] (multi_cpu_stop)
    
    Since patch_text_stop_machine() is called in stop_machine() which
    disables IRQ, sleepable lock should be not used in this atomic context,
     so replace patch_lock to raw lock.
    
    Signed-off-by: Yang Shi <yang.shi@linaro.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit b7d5dc21072cda7124d13eae2aefb7343ef94197
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Sat Apr 20 00:09:51 2019 -0400

    random: add a spinlock_t to struct batched_entropy
    
    The per-CPU variable batched_entropy_uXX is protected by get_cpu_var().
    This is just a preempt_disable() which ensures that the variable is only
    from the local CPU. It does not protect against users on the same CPU
    from another context. It is possible that a preemptible context reads
    slot 0 and then an interrupt occurs and the same value is read again.
    
    The above scenario is confirmed by lockdep if we add a spinlock:
    | ================================
    | WARNING: inconsistent lock state
    | 5.1.0-rc3+ #42 Not tainted
    | --------------------------------
    | inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.
    | ksoftirqd/9/56 [HC0[0]:SC1[1]:HE0:SE0] takes:
    | (____ptrval____) (batched_entropy_u32.lock){+.?.}, at: get_random_u32+0x3e/0xe0
    | {SOFTIRQ-ON-W} state was registered at:
    |   _raw_spin_lock+0x2a/0x40
    |   get_random_u32+0x3e/0xe0
    |   new_slab+0x15c/0x7b0
    |   ___slab_alloc+0x492/0x620
    |   __slab_alloc.isra.73+0x53/0xa0
    |   kmem_cache_alloc_node+0xaf/0x2a0
    |   copy_process.part.41+0x1e1/0x2370
    |   _do_fork+0xdb/0x6d0
    |   kernel_thread+0x20/0x30
    |   kthreadd+0x1ba/0x220
    |   ret_from_fork+0x3a/0x50
    …
    | other info that might help us debug this:
    |  Possible unsafe locking scenario:
    |
    |        CPU0
    |        ----
    |   lock(batched_entropy_u32.lock);
    |   <Interrupt>
    |     lock(batched_entropy_u32.lock);
    |
    |  *** DEADLOCK ***
    |
    | stack backtrace:
    | Call Trace:
    …
    |  kmem_cache_alloc_trace+0x20e/0x270
    |  ipmi_alloc_recv_msg+0x16/0x40
    …
    |  __do_softirq+0xec/0x48d
    |  run_ksoftirqd+0x37/0x60
    |  smpboot_thread_fn+0x191/0x290
    |  kthread+0xfe/0x130
    |  ret_from_fork+0x3a/0x50
    
    Add a spinlock_t to the batched_entropy data structure and acquire the
    lock while accessing it. Acquire the lock with disabled interrupts
    because this function may be used from interrupt context.
    
    Remove the batched_entropy_reset_lock lock. Now that we have a lock for
    the data scructure, we can access it from a remote CPU.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Theodore Ts'o <tytso@mit.edu>

commit dbda5b6625bda33a1369a6aadf756287fd70c1d8
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 13 17:14:42 2019 +0100

    ARM: 8840/1: use a raw_spinlock_t in unwind
    
    [ Upstream commit 74ffe79ae538283bbf7c155e62339f1e5c87b55a ]
    
    Mostly unwind is done with irqs enabled however SLUB may call it with
    irqs disabled while creating a new SLUB cache.
    
    I had system freeze while loading a module which called
    kmem_cache_create() on init. That means SLUB's __slab_alloc() disabled
    interrupts and then
    
    ->new_slab_objects()
     ->new_slab()
      ->setup_object()
       ->setup_object_debug()
        ->init_tracking()
         ->set_track()
          ->save_stack_trace()
           ->save_stack_trace_tsk()
            ->walk_stackframe()
             ->unwind_frame()
              ->unwind_find_idx()
               =>spin_lock_irqsave(&unwind_lock);
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d81bdb3c17f1a2f760f257c60950c176df1e77ef
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 13 17:14:42 2019 +0100

    ARM: 8840/1: use a raw_spinlock_t in unwind
    
    [ Upstream commit 74ffe79ae538283bbf7c155e62339f1e5c87b55a ]
    
    Mostly unwind is done with irqs enabled however SLUB may call it with
    irqs disabled while creating a new SLUB cache.
    
    I had system freeze while loading a module which called
    kmem_cache_create() on init. That means SLUB's __slab_alloc() disabled
    interrupts and then
    
    ->new_slab_objects()
     ->new_slab()
      ->setup_object()
       ->setup_object_debug()
        ->init_tracking()
         ->set_track()
          ->save_stack_trace()
           ->save_stack_trace_tsk()
            ->walk_stackframe()
             ->unwind_frame()
              ->unwind_find_idx()
               =>spin_lock_irqsave(&unwind_lock);
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 8b847ace66d6c4d540855050c21aafc92a953213
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 13 17:14:42 2019 +0100

    ARM: 8840/1: use a raw_spinlock_t in unwind
    
    [ Upstream commit 74ffe79ae538283bbf7c155e62339f1e5c87b55a ]
    
    Mostly unwind is done with irqs enabled however SLUB may call it with
    irqs disabled while creating a new SLUB cache.
    
    I had system freeze while loading a module which called
    kmem_cache_create() on init. That means SLUB's __slab_alloc() disabled
    interrupts and then
    
    ->new_slab_objects()
     ->new_slab()
      ->setup_object()
       ->setup_object_debug()
        ->init_tracking()
         ->set_track()
          ->save_stack_trace()
           ->save_stack_trace_tsk()
            ->walk_stackframe()
             ->unwind_frame()
              ->unwind_find_idx()
               =>spin_lock_irqsave(&unwind_lock);
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 43e01fefac9d571cad781e3ae164dbcc28a146ab
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 13 17:14:42 2019 +0100

    ARM: 8840/1: use a raw_spinlock_t in unwind
    
    [ Upstream commit 74ffe79ae538283bbf7c155e62339f1e5c87b55a ]
    
    Mostly unwind is done with irqs enabled however SLUB may call it with
    irqs disabled while creating a new SLUB cache.
    
    I had system freeze while loading a module which called
    kmem_cache_create() on init. That means SLUB's __slab_alloc() disabled
    interrupts and then
    
    ->new_slab_objects()
     ->new_slab()
      ->setup_object()
       ->setup_object_debug()
        ->init_tracking()
         ->set_track()
          ->save_stack_trace()
           ->save_stack_trace_tsk()
            ->walk_stackframe()
             ->unwind_frame()
              ->unwind_find_idx()
               =>spin_lock_irqsave(&unwind_lock);
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 6ac972dd4db42ef4e298669647b946de4e0243aa
Author: Julien Grall <julien.grall@arm.com>
Date:   Wed Mar 13 11:40:34 2019 +0000

    tty/sysrq: Convert show_lock to raw_spinlock_t
    
    Systems which don't provide arch_trigger_cpumask_backtrace() will
    invoke showacpu() from a smp_call_function() function which is invoked
    with disabled interrupts even on -RT systems.
    
    The function acquires the show_lock lock which only purpose is to
    ensure that the CPUs don't print simultaneously. Otherwise the
    output would clash and it would be hard to tell the output from CPUx
    apart from CPUy.
    
    On -RT the spin_lock() can not be acquired from this context. A
    raw_spin_lock() is required. It will introduce the system's latency
    by performing the sysrq request and other CPUs will block on the lock
    until the request is done. This is okay because the user asked for a
    backtrace of all active CPUs and under "normal circumstances in
    production" this path should not be triggered.
    
    Signed-off-by: Julien Grall <julien.grall@arm.com>
    [bigeasy@linuxtronix.de: commit description]
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0be288630752e6358d02eba7b283c1783a5c7c38
Merge: e8a71a386689 4c2741ac5e10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 15 14:37:46 2019 -0700

    Merge tag 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm
    
    Pull ARM updates from Russell King:
    
     - An improvement from Ard Biesheuvel, who noted that the identity map
       setup was taking a long time due to flush_cache_louis().
    
     - Update a comment about dma_ops from Wolfram Sang.
    
     - Remove use of "-p" with ld, where this flag has been a no-op since
       2004.
    
     - Remove the printing of the virtual memory layout, which is no longer
       useful since we hide pointers.
    
     - Correct SCU help text.
    
     - Remove legacy TWD registration method.
    
     - Add pgprot_device() implementation for mapping PCI sysfs resource
       files.
    
     - Initialise PFN limits earlier for kmemleak.
    
     - Fix argument count to match macro definition (affects clang builds)
    
     - Use unified assembler language almost everywhere for clang, and other
       clang improvements (from Stefan Agner, Nathan Chancellor).
    
     - Support security extension for noMMU and other noMMU cleanups (from
       Vladimir Murzin).
    
     - Remove unnecessary SMP bringup code (which was incorrectly copy'n'
       pasted from the ARM platform implementations) and remove it from the
       arch code to discourge further copys of it appearing.
    
     - Add Cortex A9 erratum preventing kexec working on some SoCs.
    
     - AMBA bus identification updates from Mike Leach.
    
     - More use of raw spinlocks to avoid -RT kernel issues (from Yang Shi
       and Sebastian Andrzej Siewior).
    
     - MCPM hyp/svc mode mismatch fixes from Marek Szyprowski.
    
    * tag 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm: (32 commits)
      ARM: 8849/1: NOMMU: Fix encodings for PMSAv8's PRBAR4/PRLAR4
      ARM: 8848/1: virt: Align GIC version check with arm64 counterpart
      ARM: 8847/1: pm: fix HYP/SVC mode mismatch when MCPM is used
      ARM: 8845/1: use unified assembler in c files
      ARM: 8844/1: use unified assembler in assembly files
      ARM: 8843/1: use unified assembler in headers
      ARM: 8841/1: use unified assembler in macros
      ARM: 8840/1: use a raw_spinlock_t in unwind
      ARM: 8839/1: kprobe: make patch_lock a raw_spinlock_t
      ARM: 8837/1: coresight: etmv4: Update ID register table to add UCI support
      ARM: 8836/1: drivers: amba: Update component matching to use the CoreSight UCI values.
      ARM: 8838/1: drivers: amba: Updates to component identification for driver matching.
      ARM: 8833/1: Ensure that NEON code always compiles with Clang
      ARM: avoid Cortex-A9 livelock on tight dmb loops
      ARM: smp: remove arch-provided "pen_release"
      ARM: actions: remove boot_lock and pen_release
      ARM: oxnas: remove CPU hotplug implementation
      ARM: qcom: remove unnecessary boot_lock
      ARM: 8832/1: NOMMU: Limit visibility for CONFIG_FLASH_{MEM_BASE,SIZE}
      ARM: 8831/1: NOMMU: pmsa-v8: remove unneeded semicolon
      ...

commit 74ffe79ae538283bbf7c155e62339f1e5c87b55a
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Feb 13 17:14:42 2019 +0100

    ARM: 8840/1: use a raw_spinlock_t in unwind
    
    Mostly unwind is done with irqs enabled however SLUB may call it with
    irqs disabled while creating a new SLUB cache.
    
    I had system freeze while loading a module which called
    kmem_cache_create() on init. That means SLUB's __slab_alloc() disabled
    interrupts and then
    
    ->new_slab_objects()
     ->new_slab()
      ->setup_object()
       ->setup_object_debug()
        ->init_tracking()
         ->set_track()
          ->save_stack_trace()
           ->save_stack_trace_tsk()
            ->walk_stackframe()
             ->unwind_frame()
              ->unwind_find_idx()
               =>spin_lock_irqsave(&unwind_lock);
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

commit 143c2a89e0e5fda6c6fd08d7bc1126438c19ae90
Author: Yang Shi <yang.shi@linaro.org>
Date:   Wed Feb 13 17:14:23 2019 +0100

    ARM: 8839/1: kprobe: make patch_lock a raw_spinlock_t
    
    When running kprobe on -rt kernel, the below bug is caught:
    
    |BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:931
    |in_atomic(): 1, irqs_disabled(): 128, pid: 14, name: migration/0
    |Preemption disabled at:[<802f2b98>] cpu_stopper_thread+0xc0/0x140
    |CPU: 0 PID: 14 Comm: migration/0 Tainted: G O 4.8.3-rt2 #1
    |Hardware name: Freescale LS1021A
    |[<8025a43c>] (___might_sleep)
    |[<80b5b324>] (rt_spin_lock)
    |[<80b5c31c>] (__patch_text_real)
    |[<80b5c3ac>] (patch_text_stop_machine)
    |[<802f2920>] (multi_cpu_stop)
    
    Since patch_text_stop_machine() is called in stop_machine() which
    disables IRQ, sleepable lock should be not used in this atomic context,
     so replace patch_lock to raw lock.
    
    Signed-off-by: Yang Shi <yang.shi@linaro.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

commit d83525ca62cf8ebe3271d14c36fb900c294274a2
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Jan 31 15:40:04 2019 -0800

    bpf: introduce bpf_spin_lock
    
    Introduce 'struct bpf_spin_lock' and bpf_spin_lock/unlock() helpers to let
    bpf program serialize access to other variables.
    
    Example:
    struct hash_elem {
        int cnt;
        struct bpf_spin_lock lock;
    };
    struct hash_elem * val = bpf_map_lookup_elem(&hash_map, &key);
    if (val) {
        bpf_spin_lock(&val->lock);
        val->cnt++;
        bpf_spin_unlock(&val->lock);
    }
    
    Restrictions and safety checks:
    - bpf_spin_lock is only allowed inside HASH and ARRAY maps.
    - BTF description of the map is mandatory for safety analysis.
    - bpf program can take one bpf_spin_lock at a time, since two or more can
      cause dead locks.
    - only one 'struct bpf_spin_lock' is allowed per map element.
      It drastically simplifies implementation yet allows bpf program to use
      any number of bpf_spin_locks.
    - when bpf_spin_lock is taken the calls (either bpf2bpf or helpers) are not allowed.
    - bpf program must bpf_spin_unlock() before return.
    - bpf program can access 'struct bpf_spin_lock' only via
      bpf_spin_lock()/bpf_spin_unlock() helpers.
    - load/store into 'struct bpf_spin_lock lock;' field is not allowed.
    - to use bpf_spin_lock() helper the BTF description of map value must be
      a struct and have 'struct bpf_spin_lock anyname;' field at the top level.
      Nested lock inside another struct is not allowed.
    - syscall map_lookup doesn't copy bpf_spin_lock field to user space.
    - syscall map_update and program map_update do not update bpf_spin_lock field.
    - bpf_spin_lock cannot be on the stack or inside networking packet.
      bpf_spin_lock can only be inside HASH or ARRAY map value.
    - bpf_spin_lock is available to root only and to all program types.
    - bpf_spin_lock is not allowed in inner maps of map-in-map.
    - ld_abs is not allowed inside spin_lock-ed region.
    - tracing progs and socket filter progs cannot use bpf_spin_lock due to
      insufficient preemption checks
    
    Implementation details:
    - cgroup-bpf class of programs can nest with xdp/tc programs.
      Hence bpf_spin_lock is equivalent to spin_lock_irqsave.
      Other solutions to avoid nested bpf_spin_lock are possible.
      Like making sure that all networking progs run with softirq disabled.
      spin_lock_irqsave is the simplest and doesn't add overhead to the
      programs that don't use it.
    - arch_spinlock_t is used when its implemented as queued_spin_lock
    - archs can force their own arch_spinlock_t
    - on architectures where queued_spin_lock is not available and
      sizeof(arch_spinlock_t) != sizeof(__u32) trivial lock is used.
    - presence of bpf_spin_lock inside map value could have been indicated via
      extra flag during map_create, but specifying it via BTF is cleaner.
      It provides introspection for map key/value and reduces user mistakes.
    
    Next steps:
    - allow bpf_spin_lock in other map types (like cgroup local storage)
    - introduce BPF_F_LOCK flag for bpf_map_update() syscall and helper
      to request kernel to grab bpf_spin_lock before rewriting the value.
      That will serialize access to map elements.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

commit 4b527f25a4ac64c0d17c882cdf8322be66f31611
Author: Dave Airlie <airlied@redhat.com>
Date:   Thu Jan 24 18:54:15 2019 +0000

    locking/qspinlock: Pull in asm/byteorder.h to ensure correct endianness
    
    This commit is not required upstream, but is required for the 4.9.y
    stable series.
    
    Upstream commit 101110f6271c ("Kbuild: always define endianess in
    kconfig.h") ensures that either __LITTLE_ENDIAN or __BIG_ENDIAN is
    defined to reflect the endianness of the target CPU architecture
    regardless of whether or not <asm/byteorder.h> has been #included. The
    upstream definition of 'struct qspinlock' relies on this property.
    
    Unfortunately, the 4.9.y stable series does not provide this guarantee,
    so the 'spin_unlock()' routine can erroneously treat the underlying
    lockword as big-endian on little-endian architectures using native
    qspinlock (i.e. x86_64 without PV) if the caller has not included
    <asm/byteorder.h>. This can lead to hangs such as the one in
    'i915_gem_request()' reported via bugzilla:
    
      https://bugzilla.kernel.org/show_bug.cgi?id=202063
    
    Fix the issue by ensuring that <asm/byteorder.h> is #included in
    <asm/qspinlock_types.h>, where 'struct qspinlock' is defined.
    
    Cc: <stable@vger.kernel.org> # 4.9
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    [will: wrote commit message]
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5b735eb1ce481b2f1674a47c0995944b1cb6f5d5
Author: Andrea Parri <andrea.parri@amarulasolutions.com>
Date:   Mon Dec 3 15:04:49 2018 -0800

    tools/memory-model: Model smp_mb__after_unlock_lock()
    
    The kernel documents smp_mb__after_unlock_lock() the following way:
    
      "Place this after a lock-acquisition primitive to guarantee that
       an UNLOCK+LOCK pair acts as a full barrier.  This guarantee applies
       if the UNLOCK and LOCK are executed by the same CPU or if the
       UNLOCK and LOCK operate on the same lock variable."
    
    Formalize in LKMM the above guarantee by defining (new) mb-links according
    to the law:
    
      ([M] ; po ; [UL] ; (co | po) ; [LKW] ;
            fencerel(After-unlock-lock) ; [M])
    
    where the component ([UL] ; co ; [LKW]) identifies "UNLOCK+LOCK pairs on
    the same lock variable" and the component ([UL] ; po ; [LKW]) identifies
    "UNLOCK+LOCK pairs executed by the same CPU".
    
    In particular, the LKMM forbids the following two behaviors (the second
    litmus test below is based on:
    
      Documentation/RCU/Design/Memory-Ordering/Tree-RCU-Memory-Ordering.html
    
    c.f., Section "Tree RCU Grace Period Memory Ordering Building Blocks"):
    
    C after-unlock-lock-same-cpu
    
    (*
     * Result: Never
     *)
    
    {}
    
    P0(spinlock_t *s, spinlock_t *t, int *x, int *y)
    {
            int r0;
    
            spin_lock(s);
            WRITE_ONCE(*x, 1);
            spin_unlock(s);
            spin_lock(t);
            smp_mb__after_unlock_lock();
            r0 = READ_ONCE(*y);
            spin_unlock(t);
    }
    
    P1(int *x, int *y)
    {
            int r0;
    
            WRITE_ONCE(*y, 1);
            smp_mb();
            r0 = READ_ONCE(*x);
    }
    
    exists (0:r0=0 /\ 1:r0=0)
    
    C after-unlock-lock-same-lock-variable
    
    (*
     * Result: Never
     *)
    
    {}
    
    P0(spinlock_t *s, int *x, int *y)
    {
            int r0;
    
            spin_lock(s);
            WRITE_ONCE(*x, 1);
            r0 = READ_ONCE(*y);
            spin_unlock(s);
    }
    
    P1(spinlock_t *s, int *y, int *z)
    {
            int r0;
    
            spin_lock(s);
            smp_mb__after_unlock_lock();
            WRITE_ONCE(*y, 1);
            r0 = READ_ONCE(*z);
            spin_unlock(s);
    }
    
    P2(int *z, int *x)
    {
            int r0;
    
            WRITE_ONCE(*z, 1);
            smp_mb();
            r0 = READ_ONCE(*x);
    }
    
    exists (0:r0=0 /\ 1:r0=0 /\ 2:r0=0)
    
    Signed-off-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Akira Yokosawa <akiyks@gmail.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Daniel Lustig <dlustig@nvidia.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Jade Alglave <j.alglave@ucl.ac.uk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luc Maranget <luc.maranget@inria.fr>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arch@vger.kernel.org
    Cc: parri.andrea@gmail.com
    Link: http://lkml.kernel.org/r/20181203230451.28921-1-paulmck@linux.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 345671ea0f9258f410eb057b9ced9cefbbe5dc78
Merge: 4904008165c8 22146c3ce989
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 26 19:33:41 2018 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge updates from Andrew Morton:
    
     - a few misc things
    
     - ocfs2 updates
    
     - most of MM
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (132 commits)
      hugetlbfs: dirty pages as they are added to pagecache
      mm: export add_swap_extent()
      mm: split SWP_FILE into SWP_ACTIVATED and SWP_FS
      tools/testing/selftests/vm/map_fixed_noreplace.c: add test for MAP_FIXED_NOREPLACE
      mm: thp: relocate flush_cache_range() in migrate_misplaced_transhuge_page()
      mm: thp: fix mmu_notifier in migrate_misplaced_transhuge_page()
      mm: thp: fix MADV_DONTNEED vs migrate_misplaced_transhuge_page race condition
      mm/kasan/quarantine.c: make quarantine_lock a raw_spinlock_t
      mm/gup: cache dev_pagemap while pinning pages
      Revert "x86/e820: put !E820_TYPE_RAM regions into memblock.reserved"
      mm: return zero_resv_unavail optimization
      mm: zero remaining unavailable struct pages
      tools/testing/selftests/vm/gup_benchmark.c: add MAP_HUGETLB option
      tools/testing/selftests/vm/gup_benchmark.c: add MAP_SHARED option
      tools/testing/selftests/vm/gup_benchmark.c: allow user specified file
      tools/testing/selftests/vm/gup_benchmark.c: fix 'write' flag usage
      mm/gup_benchmark.c: add additional pinning methods
      mm/gup_benchmark.c: time put_page()
      mm: don't raise MEMCG_OOM event due to failed high-order allocation
      mm/page-writeback.c: fix range_cyclic writeback vs writepages deadlock
      ...

commit 026d1eaf5ef1a5d6258b46e4e411cd9f5ab8c41d
Author: Clark Williams <williams@redhat.com>
Date:   Fri Oct 26 15:10:32 2018 -0700

    mm/kasan/quarantine.c: make quarantine_lock a raw_spinlock_t
    
    The static lock quarantine_lock is used in quarantine.c to protect the
    quarantine queue datastructures.  It is taken inside quarantine queue
    manipulation routines (quarantine_put(), quarantine_reduce() and
    quarantine_remove_cache()), with IRQs disabled.  This is not a problem on
    a stock kernel but is problematic on an RT kernel where spin locks are
    sleeping spinlocks, which can sleep and can not be acquired with disabled
    interrupts.
    
    Convert the quarantine_lock to a raw spinlock_t.  The usage of
    quarantine_lock is confined to quarantine.c and the work performed while
    the lock is held is used for debug purpose.
    
    [bigeasy@linutronix.de: slightly altered the commit message]
    Link: http://lkml.kernel.org/r/20181010214945.5owshc3mlrh74z4b@linutronix.de
    Signed-off-by: Clark Williams <williams@redhat.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit cee1352f792646ae87e65f8bfb0ae91ff3d2cb95
Merge: e2b623fbe6a3 d0346559a7c3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 12:31:17 2018 +0100

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The biggest change in this cycle is the conclusion of the big
      'simplify RCU to two primary flavors' consolidation work - i.e.
      there's a single RCU flavor for any kernel variant (PREEMPT and
      !PREEMPT):
    
        - Consolidate the RCU-bh, RCU-preempt, and RCU-sched flavors into a
          single flavor similar to RCU-sched in !PREEMPT kernels and into a
          single flavor similar to RCU-preempt (but also waiting on
          preempt-disabled sequences of code) in PREEMPT kernels.
    
          This branch also includes a refactoring of
          rcu_{nmi,irq}_{enter,exit}() from Byungchul Park.
    
        - Now that there is only one RCU flavor in any given running kernel,
          the many "rsp" pointers are no longer required, and this cleanup
          series removes them.
    
        - This branch carries out additional cleanups made possible by the
          RCU flavor consolidation, including inlining now-trivial
          functions, updating comments and definitions, and removing
          now-unneeded rcutorture scenarios.
    
        - Now that there is only one flavor of RCU in any running kernel,
          there is also only on rcu_data structure per CPU. This means that
          the rcu_dynticks structure can be merged into the rcu_data
          structure, a task taken on by this branch. This branch also
          contains a -rt-related fix from Mike Galbraith.
    
      There were also other updates:
    
        - Documentation updates, including some good-eye catches from Joel
          Fernandes.
    
        - SRCU updates, most notably changes enabling call_srcu() to be
          invoked very early in the boot sequence.
    
        - Torture-test updates, including some preliminary work towards
          making rcutorture better able to find problems that result in
          insufficient grace-period forward progress.
    
        - Initial changes to RCU to better promote forward progress of grace
          periods, including fixing a bug found by Marius Hillenbrand and
          David Woodhouse, with the fix suggested by Peter Zijlstra"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (140 commits)
      srcu: Make early-boot call_srcu() reuse workqueue lists
      rcutorture: Test early boot call_srcu()
      srcu: Make call_srcu() available during very early boot
      rcu: Convert rcu_state.ofl_lock to raw_spinlock_t
      rcu: Remove obsolete ->dynticks_fqs and ->cond_resched_completed
      rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks
      rcu: Switch dyntick nesting counters to rcu_data structure
      rcu: Switch urgent quiescent-state requests to rcu_data structure
      rcu: Switch lazy counts to rcu_data structure
      rcu: Switch last accelerate/advance to rcu_data structure
      rcu: Switch ->tick_nohz_enabled_snap to rcu_data structure
      rcu: Merge rcu_dynticks structure into rcu_data structure
      rcu: Remove unused rcu_dynticks_snap() from Tiny RCU
      rcu: Convert "1UL << x" to "BIT(x)"
      rcu: Avoid resched_cpu() when rescheduling the current CPU
      rcu: More aggressively enlist scheduler aid for nohz_full CPUs
      rcu: Compute jiffies_till_sched_qs from other kernel parameters
      rcu: Provide functions for determining if call_rcu() has been invoked
      rcu: Eliminate ->rcu_qs_ctr from the rcu_dynticks structure
      rcu: Motivate Tiny RCU forward progress
      ...

commit 46b8306480fb424abd525acc1763da1c63a27d8a
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Oct 19 10:52:52 2018 -0700

    sparc: Fix parport build warnings.
    
    If PARPORT_PC_FIFO is not enabled, do not provide the dma lock
    macros and lock definition.  Otherwise:
    
    ./arch/sparc/include/asm/parport.h:24:24: warning: ‘dma_spin_lock’ defined but not used [-Wunused-variable]
     static DEFINE_SPINLOCK(dma_spin_lock);
                            ^~~~~~~~~~~~~
    ./include/linux/spinlock_types.h:81:39: note: in definition of macro ‘DEFINE_SPINLOCK’
     #define DEFINE_SPINLOCK(x) spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 894d45bbf7e7569ec2aa845155801fd503b5f1bf
Author: Mike Galbraith <efault@gmx.de>
Date:   Wed Aug 15 09:05:29 2018 -0700

    rcu: Convert rcu_state.ofl_lock to raw_spinlock_t
    
    1e64b15a4b10 ("rcu: Fix grace-period hangs due to race with CPU offline")
    added spinlock_t ofl_lock to the rcu_state structure, then takes it with
    preemption disabled during CPU offline, which gives the -rt patchset's
    sleeping spinlock heartburn.
    
    This commit therefore converts ->ofl_lock to raw_spinlock_t.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

commit c06f5a018f710ff24ef7c1b922d2b6704c35dd8c
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Apr 23 18:10:23 2018 +0200

    delayacct: Use raw_spinlocks
    
    [ Upstream commit 02acc80d19edb0d5684c997b2004ad19f9f5236e ]
    
    try_to_wake_up() might invoke delayacct_blkio_end() while holding the
    pi_lock (which is a raw_spinlock_t). delayacct_blkio_end() acquires
    task_delay_info.lock which is a spinlock_t. This causes a might sleep splat
    on -RT where non raw spinlocks are converted to 'sleeping' spinlocks.
    
    task_delay_info.lock is only held for a short amount of time so it's not a
    problem latency wise to make convert it to a raw spinlock.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Link: https://lkml.kernel.org/r/20180423161024.6710-1-bigeasy@linutronix.de
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fe52ec8c67bf1b2b026e857b7b49c6c2f3b6363a
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Apr 23 18:10:23 2018 +0200

    delayacct: Use raw_spinlocks
    
    [ Upstream commit 02acc80d19edb0d5684c997b2004ad19f9f5236e ]
    
    try_to_wake_up() might invoke delayacct_blkio_end() while holding the
    pi_lock (which is a raw_spinlock_t). delayacct_blkio_end() acquires
    task_delay_info.lock which is a spinlock_t. This causes a might sleep splat
    on -RT where non raw spinlocks are converted to 'sleeping' spinlocks.
    
    task_delay_info.lock is only held for a short amount of time so it's not a
    problem latency wise to make convert it to a raw spinlock.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Link: https://lkml.kernel.org/r/20180423161024.6710-1-bigeasy@linutronix.de
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 75a040ff14d9a99fc041f5e1d8f09541cab13ba4
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Sun Apr 1 01:00:36 2018 +0300

    locking/refcounts: Include fewer headers in <linux/refcount.h>
    
    Debloat <linux/refcount.h>'s dependencies:
    
    - <linux/kernel.h> is not needed, but <linux/compiler.h> is.
    - <linux/mutex.h> is not needed, only a forward declaration of "struct mutex".
    - <linux/spinlock.h> is not needed, <linux/spinlock_types.h> is enough.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: https://lkml.kernel.org/lkml/20180331220036.GA7676@avx2
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f4fe74cc909bf811cd9cc7fd84f5a7514e06a7e1
Merge: 3c89adb0d111 2448d1399bac
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 5 10:08:27 2018 -0700

    Merge tag 'acpi-4.18-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull ACPI updates from Rafael Wysocki:
     "These update the ACPICA code in the kernel to the 20180508 upstream
      revision and make it support the RT patch, add CPPC v3 support to the
      ACPI CPPC library, add a WDAT-based watchdog quirk to prevent clashes
      with the RTC, add quirks to the ACPI AC and battery drivers, and
      update the ACPI SoC drivers.
    
      Specifics:
    
       - Update the ACPICA code in the kernel to the 20180508 upstream
         revision including:
           * iASL -tc option enhancement (Bob Moore).
           * Debugger improvements (Bob Moore).
           * Support for tables larger than 1 MB in acpidump/acpixtract (Bob
             Moore).
           * Minor fixes and cleanups (Colin Ian King, Toomas Soome).
    
       - Make the ACPICA code in the kernel support the RT patch (Sebastian
         Andrzej Siewior, Steven Rostedt).
    
       - Add a kmemleak annotation to the ACPICA code (Larry Finger).
    
       - Add CPPC v3 support to the ACPI CPPC library and fix two issues
         related to CPPC (Prashanth Prakash, Al Stone).
    
       - Add an ACPI WDAT-based watchdog quirk to prefer iTCO_wdt on systems
         where WDAT clashes with the RTC SRAM (Mika Westerberg).
    
       - Add some quirks to the ACPI AC and battery drivers (Carlo Caione,
         Hans de Goede).
    
       - Update the ACPI SoC drivers for Intel (LPSS) and AMD (APD)
         platforms (Akshu Agrawal, Hans de Goede).
    
       - Fix up some assorted minor issues (Al Stone, Laszlo Toth, Mathieu
         Malaterre)"
    
    * tag 'acpi-4.18-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (32 commits)
      ACPICA: Mark acpi_ut_create_internal_object_dbg() memory allocations as non-leaks
      ACPI / watchdog: Prefer iTCO_wdt always when WDAT table uses RTC SRAM
      mailbox: PCC: erroneous error message when parsing ACPI PCCT
      ACPICA: Update version to 20180508
      ACPICA: acpidump/acpixtract: Support for tables larger than 1MB
      ACPI: APD: Add AMD misc clock handler support
      clk: x86: Add ST oscout platform clock
      ACPICA: Update version to 20180427
      ACPICA: Debugger: Removed direct support for EC address space in "Test Objects"
      ACPICA: Debugger: Add Package support for "test objects" command
      ACPICA: Improve error messages for the namespace root node
      ACPICA: Fix potential infinite loop in acpi_rs_dump_byte_list
      ACPICA: vsnprintf: this statement may fall through
      ACPICA: Tables: Fix spelling mistake in comment
      ACPICA: iASL: Enhance the -tc option (create AML hex file in C)
      ACPI: Add missing prototype_for arch_post_acpi_subsys_init()
      ACPI / tables: improve comments regarding acpi_parse_entries_array()
      ACPICA: Convert acpi_gbl_hardware lock back to an acpi_raw_spinlock
      ACPICA: provide abstraction for raw_spinlock_t
      ACPI / CPPC: Fix invalid PCC channel status errors
      ...

commit ba609f7f21e45fc1170aa6beffae922a82c1b2d8
Merge: 5a802a7a285c 087ec15606b4
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Jun 4 10:43:12 2018 +0200

    Merge branch 'acpica'
    
    * acpica:
      ACPICA: Mark acpi_ut_create_internal_object_dbg() memory allocations as non-leaks
      ACPICA: Update version to 20180508
      ACPICA: acpidump/acpixtract: Support for tables larger than 1MB
      ACPICA: Update version to 20180427
      ACPICA: Debugger: Removed direct support for EC address space in "Test Objects"
      ACPICA: Debugger: Add Package support for "test objects" command
      ACPICA: Improve error messages for the namespace root node
      ACPICA: Fix potential infinite loop in acpi_rs_dump_byte_list
      ACPICA: vsnprintf: this statement may fall through
      ACPICA: Tables: Fix spelling mistake in comment
      ACPICA: iASL: Enhance the -tc option (create AML hex file in C)
      ACPICA: Convert acpi_gbl_hardware lock back to an acpi_raw_spinlock
      ACPICA: provide abstraction for raw_spinlock_t

commit f0d098569a0ab4ec3c2f59ef2827d46e798ce789
Author: Andrea Parri <parri.andrea@gmail.com>
Date:   Fri Mar 9 13:13:20 2018 +0100

    riscv/spinlock: Strengthen implementations with fences
    
    [ Upstream commit 0123f4d76ca63b7b895f40089be0ce4809e392d8 ]
    
    Current implementations map locking operations using .rl and .aq
    annotations.  However, this mapping is unsound w.r.t. the kernel
    memory consistency model (LKMM) [1]:
    
    Referring to the "unlock-lock-read-ordering" test reported below,
    Daniel wrote:
    
      "I think an RCpc interpretation of .aq and .rl would in fact
       allow the two normal loads in P1 to be reordered [...]
    
       The intuition would be that the amoswap.w.aq can forward from
       the amoswap.w.rl while that's still in the store buffer, and
       then the lw x3,0(x4) can also perform while the amoswap.w.rl
       is still in the store buffer, all before the l1 x1,0(x2)
       executes.  That's not forbidden unless the amoswaps are RCsc,
       unless I'm missing something.
    
       Likewise even if the unlock()/lock() is between two stores.
       A control dependency might originate from the load part of
       the amoswap.w.aq, but there still would have to be something
       to ensure that this load part in fact performs after the store
       part of the amoswap.w.rl performs globally, and that's not
       automatic under RCpc."
    
    Simulation of the RISC-V memory consistency model confirmed this
    expectation.
    
    In order to "synchronize" LKMM and RISC-V's implementation, this
    commit strengthens the implementations of the locking operations
    by replacing .rl and .aq with the use of ("lightweigth") fences,
    resp., "fence rw,  w" and "fence r , rw".
    
    C unlock-lock-read-ordering
    
    {}
    /* s initially owned by P1 */
    
    P0(int *x, int *y)
    {
            WRITE_ONCE(*x, 1);
            smp_wmb();
            WRITE_ONCE(*y, 1);
    }
    
    P1(int *x, int *y, spinlock_t *s)
    {
            int r0;
            int r1;
    
            r0 = READ_ONCE(*y);
            spin_unlock(s);
            spin_lock(s);
            r1 = READ_ONCE(*x);
    }
    
    exists (1:r0=1 /\ 1:r1=0)
    
    [1] https://marc.info/?l=linux-kernel&m=151930201102853&w=2
        https://groups.google.com/a/groups.riscv.org/forum/#!topic/isa-dev/hKywNHBkAXM
        https://marc.info/?l=linux-kernel&m=151633436614259&w=2
    
    Signed-off-by: Andrea Parri <parri.andrea@gmail.com>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Albert Ou <albert@sifive.com>
    Cc: Daniel Lustig <dlustig@nvidia.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Jade Alglave <j.alglave@ucl.ac.uk>
    Cc: Luc Maranget <luc.maranget@inria.fr>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Akira Yokosawa <akiyks@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-riscv@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Palmer Dabbelt <palmer@sifive.com>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit dcbbf25adb31410c95ce844f80d372ed38b68b24
Author: Ross Zwisler <ross.zwisler@linux.intel.com>
Date:   Fri May 18 16:08:54 2018 -0700

    radix tree test suite: fix compilation issue
    
    Pulled from a patch from Matthew Wilcox entitled "xarray: Add definition
    of struct xarray":
    
    > From: Matthew Wilcox <mawilcox@microsoft.com>
    > Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    
      https://patchwork.kernel.org/patch/10341249/
    
    These defines fix this compilation error:
    
      In file included from ./linux/radix-tree.h:6:0,
                       from ./linux/../../../../include/linux/idr.h:15,
                       from ./linux/idr.h:1,
                       from idr.c:4:
      ./linux/../../../../include/linux/idr.h: In function `idr_init_base':
      ./linux/../../../../include/linux/radix-tree.h:129:2: warning: implicit declaration of function `spin_lock_init'; did you mean `spinlock_t'? [-Wimplicit-function-declaration]
        spin_lock_init(&(root)->xa_lock);    \
        ^
      ./linux/../../../../include/linux/idr.h:126:2: note: in expansion of macro `INIT_RADIX_TREE'
        INIT_RADIX_TREE(&idr->idr_rt, IDR_RT_MARKER);
        ^~~~~~~~~~~~~~~
    
    by providing a spin_lock_init() wrapper for the v4.17-rc* version of the
    radix tree test suite.
    
    Link: http://lkml.kernel.org/r/20180503192430.7582-3-ross.zwisler@linux.intel.com
    Signed-off-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: CR, Sapthagirish <sapthagirish.cr@intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Matthew Wilcox <willy@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 1275022a105aeacb30cf6fde480a2b75f20779e0
Author: Ajay Singh <ajay.kathat@microchip.com>
Date:   Fri May 11 13:43:28 2018 +0530

    staging: wilc1000: added comments for mutex and spinlock_t
    
    Added comments for mutex and spinlock_t to avoid checkpatch.pl script.
    
    Signed-off-by: Ajay Singh <ajay.kathat@microchip.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c3052594c8ded984ceab3725f63990dfdea1e58f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Apr 25 16:28:26 2018 +0200

    ACPICA: provide abstraction for raw_spinlock_t
    
    Provide a new lock type acpi_raw_spinlock which is implemented as
    raw_spinlock_t on Linux. This type should be used in code which covers
    small areas of code and disables interrupts only for short time even on
    a realtime OS.
    There is a fallback to spinlock_t if an OS does not provide an
    implementation for acpi_raw_spinlock.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 02acc80d19edb0d5684c997b2004ad19f9f5236e
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Apr 23 18:10:23 2018 +0200

    delayacct: Use raw_spinlocks
    
    try_to_wake_up() might invoke delayacct_blkio_end() while holding the
    pi_lock (which is a raw_spinlock_t). delayacct_blkio_end() acquires
    task_delay_info.lock which is a spinlock_t. This causes a might sleep splat
    on -RT where non raw spinlocks are converted to 'sleeping' spinlocks.
    
    task_delay_info.lock is only held for a short amount of time so it's not a
    problem latency wise to make convert it to a raw spinlock.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Link: https://lkml.kernel.org/r/20180423161024.6710-1-bigeasy@linutronix.de

commit 0123f4d76ca63b7b895f40089be0ce4809e392d8
Author: Andrea Parri <parri.andrea@gmail.com>
Date:   Fri Mar 9 13:13:20 2018 +0100

    riscv/spinlock: Strengthen implementations with fences
    
    Current implementations map locking operations using .rl and .aq
    annotations.  However, this mapping is unsound w.r.t. the kernel
    memory consistency model (LKMM) [1]:
    
    Referring to the "unlock-lock-read-ordering" test reported below,
    Daniel wrote:
    
      "I think an RCpc interpretation of .aq and .rl would in fact
       allow the two normal loads in P1 to be reordered [...]
    
       The intuition would be that the amoswap.w.aq can forward from
       the amoswap.w.rl while that's still in the store buffer, and
       then the lw x3,0(x4) can also perform while the amoswap.w.rl
       is still in the store buffer, all before the l1 x1,0(x2)
       executes.  That's not forbidden unless the amoswaps are RCsc,
       unless I'm missing something.
    
       Likewise even if the unlock()/lock() is between two stores.
       A control dependency might originate from the load part of
       the amoswap.w.aq, but there still would have to be something
       to ensure that this load part in fact performs after the store
       part of the amoswap.w.rl performs globally, and that's not
       automatic under RCpc."
    
    Simulation of the RISC-V memory consistency model confirmed this
    expectation.
    
    In order to "synchronize" LKMM and RISC-V's implementation, this
    commit strengthens the implementations of the locking operations
    by replacing .rl and .aq with the use of ("lightweigth") fences,
    resp., "fence rw,  w" and "fence r , rw".
    
    C unlock-lock-read-ordering
    
    {}
    /* s initially owned by P1 */
    
    P0(int *x, int *y)
    {
            WRITE_ONCE(*x, 1);
            smp_wmb();
            WRITE_ONCE(*y, 1);
    }
    
    P1(int *x, int *y, spinlock_t *s)
    {
            int r0;
            int r1;
    
            r0 = READ_ONCE(*y);
            spin_unlock(s);
            spin_lock(s);
            r1 = READ_ONCE(*x);
    }
    
    exists (1:r0=1 /\ 1:r1=0)
    
    [1] https://marc.info/?l=linux-kernel&m=151930201102853&w=2
        https://groups.google.com/a/groups.riscv.org/forum/#!topic/isa-dev/hKywNHBkAXM
        https://marc.info/?l=linux-kernel&m=151633436614259&w=2
    
    Signed-off-by: Andrea Parri <parri.andrea@gmail.com>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Albert Ou <albert@sifive.com>
    Cc: Daniel Lustig <dlustig@nvidia.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Jade Alglave <j.alglave@ucl.ac.uk>
    Cc: Luc Maranget <luc.maranget@inria.fr>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Akira Yokosawa <akiyks@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-riscv@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Palmer Dabbelt <palmer@sifive.com>

commit 3563289208ecef339853692ecbf8690084744b53
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Mar 16 14:37:33 2018 -0300

    perf annotate: Use the default annotation options for --stdio2
    
    With an empty '[annotate]' section in ~/.perfconfig:
    
      # perf record -a --all-kernel -e '{cycles,instructions}:P' sleep 5
      [ perf record: Woken up 1 times to write data ]
      [ perf record: Captured and wrote 2.243 MB perf.data (5513 samples) ]
      # perf annotate --stdio2 _raw_spin_lock | head -20
    
                         Disassembly of section .text:
    
                         ffffffff81868790 <_raw_spin_lock>:
                         _raw_spin_lock():
                         EXPORT_SYMBOL(_raw_spin_trylock_bh);
                         #endif
    
                         #ifndef CONFIG_INLINE_SPIN_LOCK
                         void __lockfunc _raw_spin_lock(raw_spinlock_t *lock)
                         {
                         → callq  __fentry__
                         atomic_cmpxchg():
                                 return xadd(&v->counter, -i);
                         }
    
                         static __always_inline int atomic_cmpxchg(atomic_t *v, int old, int new)
                         {
      # perf annotate --stdio2 _raw_spin_lock | head -20
                         → callq  __fentry__
                           xor    %eax,%eax
                           mov    $0x1,%edx
       87.50 100.00        lock   cmpxchg %edx,(%rdi)
        6.25   0.00        test   %eax,%eax
                         ↓ jne    16
        6.25   0.00        repz   retq
                     16:   mov    %eax,%esi
                         ↑ jmpq   ffffffff810e96b0 <queued_spin_lock_slowpath>
      #
      # cat ~/.perfconfig
      [annotate]
    
        hide_src_code = false
        show_linenr = true
      # perf annotate --stdio2 _raw_spin_lock | head -20
    
                     3   Disassembly of section .text:
    
                     5   ffffffff81868790 <_raw_spin_lock>:
                     6   _raw_spin_lock():
                     143 EXPORT_SYMBOL(_raw_spin_trylock_bh);
                     144 #endif
    
                     146 #ifndef CONFIG_INLINE_SPIN_LOCK
                     147 void __lockfunc _raw_spin_lock(raw_spinlock_t *lock)
                     148 {
                         → callq  __fentry__
                     150 atomic_cmpxchg():
                     187         return xadd(&v->counter, -i);
                     188 }
    
                     190 static __always_inline int atomic_cmpxchg(atomic_t *v, int old, int new)
                     191 {
      #
      # cat ~/.perfconfig
      [annotate]
    
        hide_src_code = true
        show_total_period = true
      # perf annotate --stdio2 _raw_spin_lock | head -20
                                   → callq  __fentry__
                                     xor    %eax,%eax
                                     mov    $0x1,%edx
          1411316      152339        lock   cmpxchg %edx,(%rdi)
           344694           0        test   %eax,%eax
                                   ↓ jne    16
            80806           0        repz   retq
                               16:   mov    %eax,%esi
                                   ↑ jmpq   ffffffff810e96b0 <queued_spin_lock_slowpath>
      #
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jin Yao <yao.jin@linux.intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: https://lkml.kernel.org/n/tip-nu4rxg5zkdtgs1b2gc40p7v7@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

commit 77720c82915a8b7797e0041af95707d7485b4a40
Author: Julia Cartwright <julia@ni.com>
Date:   Tue Mar 21 17:43:03 2017 -0500

    powerpc/mpc52xx_gpt: make use of raw_spinlock variants
    
    The mpc52xx_gpt code currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for
    the irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel. Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do
    only minimal, bounded work, and are therefore safe to do under a raw
    spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit eb3b7b848fb3dd00f7a57d633d4ae4d194aa7865
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri Mar 24 17:32:23 2017 +0100

    s390/rwlock: introduce rwlock wait queueing
    
    Like the common queued rwlock code the s390 implementation uses the
    queued spinlock code on a spinlock_t embedded in the rwlock_t to achieve
    the queueing. The encoding of the rwlock_t differs though, the counter
    field in the rwlock_t is split into two parts. The upper two bytes hold
    the write bit and the write wait counter, the lower two bytes hold the
    read counter.
    
    The arch_read_lock operation works exactly like the common qrwlock but
    the enqueue operation for a writer follows a diffent logic. After the
    failed inline try to get the rwlock in write, the writer first increases
    the write wait counter, acquires the wait spin_lock for the queueing,
    and then loops until there are no readers and the write bit is zero.
    Without the write wait counter a CPU that just released the rwlock
    could immediately reacquire the lock in the inline code, bypassing all
    outstanding read and write waiters. For s390 this would cause massive
    imbalances in favour of writers in case of a contended rwlock.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit b96f7d881ad94203e997cd2aa7112d4a06d121ef
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri Mar 24 17:25:02 2017 +0100

    s390/spinlock: introduce spinlock wait queueing
    
    The queued spinlock code for s390 follows the principles of the common
    code qspinlock implementation but with a few notable differences.
    
    The format of the spinlock_t locking word differs, s390 needs to store
    the logical CPU number of the lock holder in the spinlock_t to be able
    to use the diagnose 9c directed yield hypervisor call.
    
    The inline code sequences for spin_lock and spin_unlock are nice and
    short. The inline portion of a spin_lock now typically looks like this:
    
            lhi     %r0,0                   # 0 indicates an empty lock
            l       %r1,0x3a0               # CPU number + 1 from lowcore
            cs      %r0,%r1,<some_lock>     # lock operation
            jnz     call_wait               # on failure call wait function
    locked:
            ...
    call_wait:
            la      %r2,<some_lock>
            brasl   %r14,arch_spin_lock_wait
            j       locked
    
    A spin_unlock is as simple as before:
    
            lhi     %r0,0
            sth     %r0,2(%r2)              # unlock operation
    
    After a CPU has queued itself it may not enable interrupts again for the
    arch_spin_lock_flags() variant. The arch_spin_lock_wait_flags wait function
    is removed.
    
    To improve performance the code implements opportunistic lock stealing.
    If the wait function finds a spinlock_t that indicates that the lock is
    free but there are queued waiters, the CPU may steal the lock up to three
    times without queueing itself. The lock stealing update the steal counter
    in the lock word to prevent more than 3 steals. The counter is reset at
    the time the CPU next in the queue successfully takes the lock.
    
    While the queued spinlocks improve performance in a system with dedicated
    CPUs, in a virtualized environment with continuously overcommitted CPUs
    the queued spinlocks can have a negative effect on performance. This
    is due to the fact that a queued CPU that is preempted by the hypervisor
    will block the queue at some point even without holding the lock. With
    the classic spinlock it does not matter if a CPU is preempted that waits
    for the lock. Therefore use the queued spinlock code only if the system
    runs with dedicated CPUs and fall back to classic spinlocks when running
    with shared CPUs.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit b0ade85165b3caeb0cd908cffe5921a39f25c243
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Sun Sep 10 13:41:41 2017 +0200

    netfilter: nat: Do not use ARRAY_SIZE() on spinlocks to fix zero div
    
    If no spinlock debugging options (CONFIG_GENERIC_LOCKBREAK,
    CONFIG_DEBUG_SPINLOCK, CONFIG_DEBUG_LOCK_ALLOC) are enabled on a UP
    platform (e.g. m68k defconfig), arch_spinlock_t is an empty struct,
    hence using ARRAY_SIZE(nf_nat_locks) causes a division by zero:
    
        net/netfilter/nf_nat_core.c: In function ‘nf_nat_setup_info’:
        net/netfilter/nf_nat_core.c:432: warning: division by zero
        net/netfilter/nf_nat_core.c: In function ‘__nf_nat_cleanup_conntrack’:
        net/netfilter/nf_nat_core.c:535: warning: division by zero
        net/netfilter/nf_nat_core.c:537: warning: division by zero
        net/netfilter/nf_nat_core.c: In function ‘nf_nat_init’:
        net/netfilter/nf_nat_core.c:810: warning: division by zero
        net/netfilter/nf_nat_core.c:811: warning: division by zero
        net/netfilter/nf_nat_core.c:824: warning: division by zero
    
    Fix this by using the CONNTRACK_LOCKS definition instead.
    
    Suggested-by: Florian Westphal <fw@strlen.de>
    Fixes: 8073e960a03bf7b5 ("netfilter: nat: use keyed locks")
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

commit b5a3a128b44219f0802a8b7895e09233853c8b43
Author: Julia Cartwright <julia@ni.com>
Date:   Tue Mar 21 17:43:02 2017 -0500

    alpha: marvel: make use of raw_spinlock variants
    
    The alpha/marvel code currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Matt Turner <mattst88@gmail.com>

commit b042e42feec495dd199525d3f88ffb323e5ec199
Author: Tobias Klauser <tklauser@distanz.ch>
Date:   Mon May 15 12:23:07 2017 +0200

    clk: sunxi-ng: explicitly include linux/spinlock.h
    
    ccu_reset.h and ccu_reset.c use spinlock_t and associated functions but
    rely on implict inclusion of linux/spinlock.h which means that changes
    in other headers could break the build. Thus, add an explicit include.
    
    Signed-off-by: Tobias Klauser <tklauser@distanz.ch>
    Acked-by: Chen-Yu Tsai <wens@csie.org>
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>

commit 60925ee97e2be4993fb7a2f7e70be0fbce08cf0f
Merge: be941bf2e6a3 145d97858597
Author: David S. Miller <davem@davemloft.net>
Date:   Thu May 25 12:07:08 2017 -0700

    Merge branch 'sparc64-queued-locks'
    
    Babu Moger says:
    
    ====================
    Enable queued rwlock and queued spinlock for SPARC
    
    This series of patches enables queued rwlock and queued spinlock support
    for SPARC. These features were introduced some time ago in upstream.
    Here are some of the earlier discussions.
    https://lwn.net/Articles/572765/
    https://lwn.net/Articles/582200/
    https://lwn.net/Articles/561775/
    https://lwn.net/Articles/590243/
    
    Tests: Ran AIM7 benchmark to verify the performance on various workloads.
    https://github.com/davidlohr/areaim. Same benchmark was used when this
    feature was introduced and enabled on x86. Here are the test results.
    
    Kernel                          4.11.0-rc6     4.11.0-rc6 +     Change
                                    baseline        queued locks
                                  (Avg No.of jobs) (Avg No.of jobs)
    Workload
    High systime 10-100 user         17290.48        17295.18       +0.02
    High systime 200-1000 users     109814.95       110248.87       +0.39
    High systime 1200-2000 users    107912.40       127923.16       +18.54
    
    Disk IO 10-100 users            168910.16       158834.17       -5.96
    Disk IO 200-1000 users          242781.74       281285.80       +15.85
    Disk IO 1200-2000 users         228518.23       218421.23       -4.41
    
    Disk IO 10-100 users            183933.77       207928.67       +13.04
    Disk IO 200-1000 users          491981.56       500162.33       +1.66
    Disk IO 1200-2000 users         463395.66       467312.70       +0.84
    
    fserver 10-100 users            254177.53       270283.08       +6.33
    fserver IO 200-1000 users       269017.35       324812.2        +20.74
    fserver IO 1200-2000 users      229538.87       284713.77       +24.03
    
    Disk I/O results are little bit in negative territory. But majority of the
    performance changes are in positive and it is significant in some cases.
    
    Changes:
    v3 -> v4:
     1. Took care of Geert Uytterhoeven's comment about patch #3(def_bool y)
     2. Working on separate patch sets to define CPU_BIG_ENDIAN for all the
        default big endian architectures based on feedback from Geert and Arnd.
    
    v2 -> v3:
     1. Rebased the patches on top of 4.12-rc2.
     2. Re-ordered the patch #1 and patch #2. That is the same order I have seen
        the issues. So, it should be addressed in the same order. Patch #1 removes
        the check __LINUX_SPINLOCK_TYPES_H. Patch #2 addreses the compile error
        with qrwlock.c. This addresses the comments from Dave Miller on v2.
    
    v1 -> v2:
    Addressed the comments from David Miller.
    1. Added CPU_BIG_ENDIAN for all SPARC
    2. Removed #ifndef __LINUX_SPINLOCK_TYPES_H guard from spinlock_types.h
    3. Removed check for CONFIG_QUEUED_RWLOCKS in SPARC64 as it is the
       default definition for SPARC64 now. Cleaned-up the previous arch_read_xxx
       and arch_write_xxx definitions as it is defined now in qrwlock.h.
    4. Removed check for CONFIG_QUEUED_SPINLOCKS in SPARC64 as it is the default
       definition now for SPARC64 now. Cleaned-up the previous arch_spin_xxx
       definitions as it is defined in qspinlock.h.
    
    v1: Initial version
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 8b93b4a9e1be78930eb9d640f75818993f70e065
Author: Babu Moger <babu.moger@oracle.com>
Date:   Wed May 24 17:55:09 2017 -0600

    arch/sparc: Remove the check #ifndef __LINUX_SPINLOCK_TYPES_H
    
    Saw these compile errors on SPARC when queued rwlock feature is enabled.
    
       CC      kernel/locking/qrwlock.o
    In file included from ./include/asm-generic/qrwlock_types.h:5,
                      from ./arch/sparc/include/asm/qrwlock.h:4,
                      from kernel/locking/qrwlock.c:24:
    ./arch/sparc/include/asm/spinlock_types.h:5:3: error:
             #error "please don't include this file directly"
    
    SPARC has this guard which causes compile error when spinlock_types.h
    is included directly.
    @ifndef __LINUX_SPINLOCK_TYPES_H
    @ error "please don't include this file directly"
    @endif
    
    Remove this un-necessary "ifndef __LINUX_SPINLOCK_TYPES_H" stanza from SPARC.
    
    Signed-off-by: Babu Moger <babu.moger@oracle.com>
    Suggested-by: David Miller <davem@davemloft.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 847f716f9ec2c61f57690c871a307f1349d472d0
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 8 15:57:21 2017 -0700

    net/ipv6/ila/ila_xlat.c: simplify a strange allocation pattern
    
    alloc_ila_locks seemed to c&p from alloc_bucket_locks allocation pattern
    which is quite unusual.  The default allocation size is 320 *
    sizeof(spinlock_t) which is sub page unless lockdep is enabled when the
    performance benefit is really questionable and not worth the subtle code
    IMHO.  Also note that the context when we call ila_init_net (modprobe or
    a task creating a net namespace) has to be properly configured.
    
    Let's just simplify the code and use kvmalloc helper which is a
    transparent way to use kmalloc with vmalloc fallback.
    
    Link: http://lkml.kernel.org/r/20170306103032.2540-5-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Tom Herbert <tom@herbertland.com>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit f5139c27e734bf8458c0afc60ef86e45517e1126
Author: Julia Cartwright <julia@ni.com>
Date:   Tue Mar 21 17:43:06 2017 -0500

    mfd: tc6393xb: Make use of raw_spinlock variants
    
    The tc6393xb mfd driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Lee Jones <lee.jones@linaro.org>

commit 9fe8c2dfe165f18873042d25ef430e18c97b1dae
Author: Julia Cartwright <julia@ni.com>
Date:   Tue Mar 21 17:43:05 2017 -0500

    mfd: t7l66xb: Make use of raw_spinlock variants
    
    The t7l66xb mfd driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Lee Jones <lee.jones@linaro.org>

commit 93ad4471912029f7519c23da56538a5d54552124
Author: Julia Cartwright <julia@ni.com>
Date:   Tue Mar 21 17:43:04 2017 -0500

    mfd: asic3: Make use of raw_spinlock variants
    
    The asic3 mfd driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Lee Jones <lee.jones@linaro.org>

commit e1e37d6c4a78bb6fb9b67115f497337cb38de503
Author: Julia Cartwright <julia@ni.com>
Date:   Tue Mar 21 17:43:07 2017 -0500

    gpio: 104-idi-48: make use of raw_spinlock variants
    
    The 104-idi-48 gpio driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Acked-by: William Breathitt Gray <vilhelm.gray@gmail.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 70b7aa7a87b4593f50f634dc721e18bd1f9e5448
Author: John Keeping <john@metanate.com>
Date:   Thu Mar 23 10:59:29 2017 +0000

    pinctrl: rockchip: convert to raw spinlock
    
    This lock is used from rockchip_irq_set_type() which is part of the
    irq_chip implementation and thus must use raw_spinlock_t as documented
    in Documentation/gpio/driver.txt.
    
    Signed-off-by: John Keeping <john@metanate.com>
    Reviewed-by: Heiko Stuebner <heiko@sntech.de>
    Tested-by: Heiko Stuebner <heiko@sntech.de>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit ea38ce081d155534494bac5df2bb0d343fb1679b
Author: Julia Cartwright <julia@ni.com>
Date:   Tue Mar 21 17:43:09 2017 -0500

    gpio: pci-idio-16: make use of raw_spinlock variants
    
    The pci-idio-16 gpio driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Acked-by: William Breathitt Gray <vilhelm.gray@gmail.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 3906e8089af3225a0a22c12cc3cf10be4630976e
Author: Julia Cartwright <julia@ni.com>
Date:   Tue Mar 21 17:43:08 2017 -0500

    gpio: 104-idio-16: make use of raw_spinlock variants
    
    The 104-idio-16 gpio driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Acked-by: William Breathitt Gray <vilhelm.gray@gmail.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 8c41ab4f0521918452851392ed40ed03923623e2
Author: Julia Cartwright <julia@ni.com>
Date:   Thu Mar 9 10:21:58 2017 -0600

    gpio: zx: make use of raw_spinlock variants
    
    The zx gpio driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit a0a584f0e92af068f4308586bad1ca5ea1e28d79
Author: Julia Cartwright <julia@ni.com>
Date:   Thu Mar 9 10:21:57 2017 -0600

    gpio: ws16c48: make use of raw_spinlock variants
    
    The ws16c48 gpio driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Acked-by: William Breathitt Gray <vilhelm.gray@gmail.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 99b9b45de8808994ea9a176241e52bb6fab71cdd
Author: Julia Cartwright <julia@ni.com>
Date:   Thu Mar 9 10:21:56 2017 -0600

    gpio: pl061: make use of raw_spinlock variants
    
    The pl061 gpio driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit f9c56536d50b4c55d42d7fb3396501906f7518fa
Author: Julia Cartwright <julia@ni.com>
Date:   Thu Mar 9 10:21:55 2017 -0600

    gpio: etraxfs: make use of raw_spinlock variants
    
    The etraxfs gpio driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit a080ce53eea56407779eb7ccf0ff00af0734bcdf
Author: Julia Cartwright <julia@ni.com>
Date:   Thu Mar 9 10:21:53 2017 -0600

    gpio: ath79: make use of raw_spinlock variants
    
    The ath79 gpio driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Acked-by: Aban Bedel <albeu@free.fr>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit c69fcea57e9d2b27025235b3205861c3895fa618
Author: Julia Cartwright <julia@ni.com>
Date:   Thu Mar 9 10:21:54 2017 -0600

    gpio: bcm-kona: make use of raw_spinlock variants
    
    The bcm-kona gpio driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 45897809d518c7a84b215c79b58e4add9b8a1d40
Author: Julia Cartwright <julia@ni.com>
Date:   Thu Mar 9 10:21:52 2017 -0600

    gpio: 104-dio-48e: make use of raw_spinlock variants
    
    The 104-dio-48e gpio driver currently implements an irq_chip for
    handling interrupts; due to how irq_chip handling is done, it's
    necessary for the irq_chip methods to be invoked from hardirq context,
    even on a a real-time kernel.  Because the spinlock_t type becomes a
    "sleeping" spinlock w/ RT kernels, it is not suitable to be used with
    irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Acked-by: William Breathitt Gray <vilhelm.gray@gmail.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 21d01c9c081ab231d6ef9d94ad9345c58b6568c1
Author: Julia Cartwright <julia@ni.com>
Date:   Thu Mar 9 10:21:49 2017 -0600

    gpio: altera: make use of raw_spinlock variants
    
    The altera gpio driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a real-time
    kernel.  Because the spinlock_t type becomes a "sleeping" spinlock w/ RT
    kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit f658ed3642cd1872c536c820597b85a9da2ddded
Author: Julia Cartwright <julia@ni.com>
Date:   Thu Mar 9 10:22:06 2017 -0600

    pinctrl: sunxi: make use of raw_spinlock variants
    
    The sunxi pinctrl driver currently implement an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 82e529c1c7befe82fc7fc258b956e973056c20bb
Author: Julia Cartwright <julia@ni.com>
Date:   Thu Mar 9 10:22:05 2017 -0600

    pinctrl: sirf: atlas7: make use of raw_spinlock variants
    
    The sirf atlas7 pinctrl drivers currently implement an irq_chip for
    handling interrupts; due to how irq_chip handling is done, it's
    necessary for the irq_chip methods to be invoked from hardirq context,
    even on a a real-time kernel.  Because the spinlock_t type becomes a
    "sleeping" spinlock w/ RT kernels, it is not suitable to be used with
    irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 229710fecdd805abb753c480778ea0de47cbb1e2
Author: Julia Cartwright <julia@ni.com>
Date:   Thu Mar 9 10:22:04 2017 -0600

    pinctrl: amd: make use of raw_spinlock variants
    
    The amd pinctrl drivers currently implement an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit cb96a66243a84f16590ba0a7658d4257267e50df
Author: Julia Cartwright <julia@ni.com>
Date:   Thu Mar 9 10:22:03 2017 -0600

    pinctrl: bcm: make use of raw_spinlock variants
    
    The bcm pinctrl drivers currently implement an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 743cc37561188e222eff5200d4507eabcccf9d41
Author: Julia Cartwright <julia@ni.com>
Date:   Thu Mar 9 10:21:59 2017 -0600

    i2c: mux: pca954x: make use of raw_spinlock variants
    
    The pca954x i2c mux driver currently implements an irq_chip for handling
    interrupts; due to how irq_chip handling is done, it's necessary for the
    irq_chip methods to be invoked from hardirq context, even on a a
    real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw spinlock.
    
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Peter Rosin <peda@axentia.se>

commit a71c8e9b8efc30459d4dbae9dff90fba46f46c63
Author: Varsha Rao <rvarsha016@gmail.com>
Date:   Thu Mar 2 23:27:11 2017 +0530

    staging: emxx_udc: Add comment for spinlock_t definition.
    
    Members of nbu2ss_udc structure can change device state, maintain
    completion state and control driver. Also provide access to read and
    write to register. Hence, exclusive access to nbu2ss_udc is required.
    The lock variable of type spinlock_t guarantees the exclusive access
    and protects it.
    
    In this patch, comment is added for spinlock_t definition, to fix the
    following checkpatch issue:
    
    CHECK: spinlock_t definition without comment
    
    Signed-off-by: Varsha Rao <rvarsha016@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6b454ee19ded0051ac67b90c809d64d8cd72a96b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Sep 5 11:14:48 2014 -0700

    compiler: Allow 1- and 2-byte smp_load_acquire() and smp_store_release()
    
    commit 536fa402221f09633e7c5801b327055ab716a363 upstream.
    
    CPUs without single-byte and double-byte loads and stores place some
    "interesting" requirements on concurrent code.  For example (adapted
    from Peter Hurley's test code), suppose we have the following structure:
    
            struct foo {
                    spinlock_t lock1;
                    spinlock_t lock2;
                    char a; /* Protected by lock1. */
                    char b; /* Protected by lock2. */
            };
            struct foo *foop;
    
    Of course, it is common (and good) practice to place data protected
    by different locks in separate cache lines.  However, if the locks are
    rarely acquired (for example, only in rare error cases), and there are
    a great many instances of the data structure, then memory footprint can
    trump false-sharing concerns, so that it can be better to place them in
    the same cache cache line as above.
    
    But if the CPU does not support single-byte loads and stores, a store
    to foop->a will do a non-atomic read-modify-write operation on foop->b,
    which will come as a nasty surprise to someone holding foop->lock2.  So we
    now require CPUs to support single-byte and double-byte loads and stores.
    Therefore, this commit adjusts the definition of __native_word() to allow
    these sizes to be used by smp_load_acquire() and smp_store_release().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 15f2e88ddd4bc9b2c6b6236162993b5caa80abb9
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Fri Dec 16 14:46:09 2016 -0500

    radix tree: Add some implicit includes
    
    We were using spinlock_t and INIT_LIST_HEAD without including spinlock.h
    or list.h.  They were being implicitly included through some other header
    file, but that's fragile.
    
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>

commit 47b03ca903fb07a69ba88d2e1629fe145771f116
Author: Julia Cartwright <julia@ni.com>
Date:   Fri Jan 20 10:13:47 2017 -0600

    pinctrl: qcom: Use raw spinlock variants
    
    The MSM pinctrl driver currently implements an irq_chip for handling
    GPIO interrupts; due to how irq_chip handling is done, it's necessary
    for the irq_chip methods to be invoked from hardirq context, even on a
    a real-time kernel.  Because the spinlock_t type becomes a "sleeping"
    spinlock w/ RT kernels, it is not suitable to be used with irq_chips.
    
    A quick audit of the operations under the lock reveal that they do only
    minimal, bounded work, and are therefore safe to do under a raw
    spinlock.
    
    On real-time kernels, this fixes an OOPs which looks like the following,
    as reported by Brian Wrenn:
    
        kernel BUG at kernel/locking/rtmutex.c:1014!
        Internal error: Oops - BUG: 0 [#1] PREEMPT SMP
        Modules linked in: spidev_irq(O) smsc75xx wcn36xx [last unloaded: spidev]
        CPU: 0 PID: 1163 Comm: irq/144-mmc0 Tainted: G        W  O    4.4.9-linaro-lt-qcom #1
        PC is at rt_spin_lock_slowlock+0x80/0x2d8
        LR is at rt_spin_lock_slowlock+0x68/0x2d8
        [..]
      Call trace:
        rt_spin_lock_slowlock
        rt_spin_lock
        msm_gpio_irq_ack
        handle_edge_irq
        generic_handle_irq
        msm_gpio_irq_handler
        generic_handle_irq
        __handle_domain_irq
        gic_handle_irq
    
    Reported-by: Brian Wrenn <dcbrianw@gmail.com>
    Tested-by: Brian Wrenn <dcbrianw@gmail.com>
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Acked-by: Bjorn Andersson <bjorn.andersson@linaro.org>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 8773159a97ccf2ae95f443227774f26f430fbb8a
Author: Oleg Drokin <green@linuxhacker.ru>
Date:   Tue Dec 6 23:57:36 2016 -0500

    staging/lustre/o2iblnd: Add missing space
    
    checkpatch highlighted missing space before assignment
    for lock variable.
    
    +       spinlock_t *lock= &kiblnd_data.kib_connd_lock;
    
    Signed-off-by: Oleg Drokin <green@linuxhacker.ru>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 195512629c956286265dcfeeea2028c42ee0c483
Author: Jacob Keller <jacob.e.keller@intel.com>
Date:   Wed Oct 5 09:30:43 2016 -0700

    i40e: use a mutex instead of spinlock in PTP user entry points
    
    We need a locking mechanism to protect the hardware SYSTIME register
    which is split over 2 values, and has internal hardware latching. We
    can't allow multiple accesses at the same time. However....
    
    The spinlock_t is overkill here, especially use of spin_lock_irqsave,
    since every PTP access will halt hardirqs. Notice that the only places
    which need the SYSTIME value are user context and are capable of sleeping.
    Thus, it is safe to use a mutex here instead of the spinlock.
    
    Change-ID: I971761a89b58c6aad953590162e85a327fbba232
    Signed-off-by: Jacob Keller <jacob.e.keller@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

commit afe98b3b0e72a1238d8ef394195f3ce5aff27e05
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Sep 5 11:14:48 2014 -0700

    compiler: Allow 1- and 2-byte smp_load_acquire() and smp_store_release()
    
    commit 536fa402221f09633e7c5801b327055ab716a363 upstream.
    
    CPUs without single-byte and double-byte loads and stores place some
    "interesting" requirements on concurrent code.  For example (adapted
    from Peter Hurley's test code), suppose we have the following structure:
    
            struct foo {
                    spinlock_t lock1;
                    spinlock_t lock2;
                    char a; /* Protected by lock1. */
                    char b; /* Protected by lock2. */
            };
            struct foo *foop;
    
    Of course, it is common (and good) practice to place data protected
    by different locks in separate cache lines.  However, if the locks are
    rarely acquired (for example, only in rare error cases), and there are
    a great many instances of the data structure, then memory footprint can
    trump false-sharing concerns, so that it can be better to place them in
    the same cache cache line as above.
    
    But if the CPU does not support single-byte loads and stores, a store
    to foop->a will do a non-atomic read-modify-write operation on foop->b,
    which will come as a nasty surprise to someone holding foop->lock2.  So we
    now require CPUs to support single-byte and double-byte loads and stores.
    Therefore, this commit adjusts the definition of __native_word() to allow
    these sizes to be used by smp_load_acquire() and smp_store_release().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit 5781d89c5468dd7a9a17df7995541b284599e00a
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed May 1 05:24:03 2013 +0000

    af_unix: fix a fatal race with bit fields
    
    commit 60bc851ae59bfe99be6ee89d6bc50008c85ec75d upstream.
    
    Using bit fields is dangerous on ppc64/sparc64, as the compiler [1]
    uses 64bit instructions to manipulate them.
    If the 64bit word includes any atomic_t or spinlock_t, we can lose
    critical concurrent changes.
    
    This is happening in af_unix, where unix_sk(sk)->gc_candidate/
    gc_maybe_cycle/lock share the same 64bit word.
    
    This leads to fatal deadlock, as one/several cpus spin forever
    on a spinlock that will never be available again.
    
    A safer way would be to use a long to store flags.
    This way we are sure compiler/arch wont do bad things.
    
    As we own unix_gc_lock spinlock when clearing or setting bits,
    we can use the non atomic __set_bit()/__clear_bit().
    
    recursion_level can share the same 64bit location with the spinlock,
    as it is set only with this spinlock held.
    
    [1] bug fixed in gcc-4.8.0 :
    http://gcc.gnu.org/bugzilla/show_bug.cgi?id=52080
    
    Reported-by: Ambrose Feinstein <ambrose@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Cc: hejianet <hejianet@gmail.com>
    Signed-off-by: Zefan Li <lizefan@huawei.com>

commit 9c5d760b8d229b94c5030863a5edaee5f1a9d7b7
Author: Michal Hocko <mhocko@suse.com>
Date:   Tue Oct 11 13:56:04 2016 -0700

    mm: split gfp_mask and mapping flags into separate fields
    
    mapping->flags currently encodes two different things into a single flag.
    It contains sticky gfp_mask for page cache allocations and AS_ codes used
    to report errors/enospace and other states which are mapping specific.
    Condensing the two semantically unrelated things saves few bytes but it
    also complicates other things.  For one thing the gfp flags space is
    reduced and in fact we are already running out of available bits.  It can
    be assumed that more gfp flags will be necessary later on.
    
    To not introduce the address_space grow (at least on x86_64) we can stick
    it right after private_lock because we have a hole there.
    
    struct address_space {
            struct inode *             host;                 /*     0     8 */
            struct radix_tree_root     page_tree;            /*     8    16 */
            spinlock_t                 tree_lock;            /*    24     4 */
            atomic_t                   i_mmap_writable;      /*    28     4 */
            struct rb_root             i_mmap;               /*    32     8 */
            struct rw_semaphore        i_mmap_rwsem;         /*    40    40 */
            /* --- cacheline 1 boundary (64 bytes) was 16 bytes ago --- */
            long unsigned int          nrpages;              /*    80     8 */
            long unsigned int          nrexceptional;        /*    88     8 */
            long unsigned int          writeback_index;      /*    96     8 */
            const struct address_space_operations  * a_ops;  /*   104     8 */
            long unsigned int          flags;                /*   112     8 */
            spinlock_t                 private_lock;         /*   120     4 */
    
            /* XXX 4 bytes hole, try to pack */
    
            /* --- cacheline 2 boundary (128 bytes) --- */
            struct list_head           private_list;         /*   128    16 */
            void *                     private_data;         /*   144     8 */
    
            /* size: 152, cachelines: 3, members: 14 */
            /* sum members: 148, holes: 1, sum holes: 4 */
            /* last cacheline: 24 bytes */
    };
    
    Link: http://lkml.kernel.org/r/20160912114852.GI14524@dhcp22.suse.cz
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit cf6a599ef700914d9237f4dcf7afe01dd2ca6368
Author: Christian Gromm <christian.gromm@microchip.com>
Date:   Fri Aug 19 11:12:55 2016 +0200

    staging: most: hdm-usb: assign spinlock to local variable
    
    This patch assigns the spinlock of struct mdev to local spinlock_t
    variable to get rid of all the ugly dereferencing.
    
    Signed-off-by: Andrey Shvetsov <andrey.shvetsov@k2l.de>
    Signed-off-by: Christian Gromm <christian.gromm@microchip.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit aeb35d6b74174ed08daab84e232b456bbd89d1d9
Merge: 7a66ecfd319a a47177d360a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 1 14:23:42 2016 -0400

    Merge branch 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 header cleanups from Ingo Molnar:
     "This tree is a cleanup of the x86 tree reducing spurious uses of
      module.h - which should improve build performance a bit"
    
    * 'x86-headers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, crypto: Restore MODULE_LICENSE() to glue_helper.c so it loads
      x86/apic: Remove duplicated include from probe_64.c
      x86/ce4100: Remove duplicated include from ce4100.c
      x86/headers: Include spinlock_types.h in x8664_ksyms_64.c for missing spinlock_t
      x86/platform: Delete extraneous MODULE_* tags fromm ts5500
      x86: Audit and remove any remaining unnecessary uses of module.h
      x86/kvm: Audit and remove any unnecessary uses of module.h
      x86/xen: Audit and remove any unnecessary uses of module.h
      x86/platform: Audit and remove any unnecessary uses of module.h
      x86/lib: Audit and remove any unnecessary uses of module.h
      x86/kernel: Audit and remove any unnecessary uses of module.h
      x86/mm: Audit and remove any unnecessary uses of module.h
      x86: Don't use module.h just for AUTHOR / LICENSE tags

commit a203800d97ed635c504b1774dfffee6e7abf95f9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Mon Jul 18 18:23:24 2016 +1000

    x86/headers: Include spinlock_types.h in x8664_ksyms_64.c for missing spinlock_t
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-next@vger.kernel.org
    Fixes: 186f43608a5c ("x86/kernel: Audit and remove any unnecessary uses of module.h")
    Link: http://lkml.kernel.org/r/20160718182922.7b41f923@canb.auug.org.au
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 43d667243c734fe7de39a586a095ea8fec9a873a
Author: H Hartley Sweeten <hsweeten@visionengravers.com>
Date:   Wed Apr 20 10:36:41 2016 -0700

    staging: comedi: mite: document the mite_struct spinlock_t
    
    Add a comment to fix the checkpatch.pl issue:
    CHECK: spinlock_t definition without comment
    
    Signed-off-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Reviewed-by: Ian Abbott <abbotti@mev.co.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7587c407540006e4e8fd5ed33f66ffe6158e830a
Author: Mike Galbraith <umgwanakikbuti@gmail.com>
Date:   Tue Apr 5 15:03:21 2016 +0200

    crypto: ccp - Fix RT breaking #include <linux/rwlock_types.h>
    
    Direct include of rwlock_types.h breaks RT, use spinlock_types.h instead.
    
    Fixes: 553d2374db0b crypto: ccp - Support for multiple CCPs
    Signed-off-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 70946a44deec299ef54c0ec933e8d82ddd4bcc6a
Author: Yao Dongdong <yaodongdong@huawei.com>
Date:   Mon Mar 7 16:02:14 2016 +0800

    documentation: Make sample code and documentation consistent
    
    In the chapter 'analogy with reader-writer locking', the sample
    code uses spinlock_t in reader-writer case. Just correct it so
    that we can read the document easily.
    
    Signed-off-by: Yao Dongdong <yaodongdong@huawei.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 7d53d8f426cacdb7352e91957721b4a9962b222a
Author: John L. Hammond <john.hammond@intel.com>
Date:   Wed Mar 30 19:48:36 2016 -0400

    staging/lustre/obd: remove struct client_obd_lock
    
    Remove the definition of struct client_obd_lock and the functions
    client_obd_list_{init,lock,unlock,done}(). Use spinlock_t for the
    cl_{loi,lru}_list_lock members of struct client_obd and call
    spin_{lock,unlock}() directly.
    
    Signed-off-by: John L. Hammond <john.hammond@intel.com>
    Signed-off-by: Oleg Drokin <green@linuxhacker.ru>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6050b1cfc66c421546880c0219cbce9102800099
Author: H Hartley Sweeten <hsweeten@visionengravers.com>
Date:   Wed Mar 30 11:09:49 2016 -0700

    staging: comedi: amplc_dio200_common: document spinlock definition
    
    Fix the checkpatch.pl issue:
    CHECK: spinlock_t definition without comment
    
    Signed-off-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Reviewed-by: Ian Abbott <abbotti@mev.co.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 10142ced7dc10261b7ff3598dedb3f6c4cb64e6d
Author: H Hartley Sweeten <hsweeten@visionengravers.com>
Date:   Wed Mar 23 15:36:47 2016 -0700

    staging: comedi: ni_tio.h: tidy up struct ni_gpct
    
    Fix the checkpatch.pl issues:
    WARNING: Prefer 'unsigned int' to bare use of 'unsigned'
    CHECK: Prefer kernel type 'u64' over 'uint64_t'
    CHECK: spinlock_t definition without comment
    
    Signed-off-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Reviewed-by: Ian Abbott <abbotti@mev.co.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a38a1408868c6f476f8f41881b7d7757e24f61c2
Author: H Hartley Sweeten <hsweeten@visionengravers.com>
Date:   Wed Mar 23 15:36:46 2016 -0700

    staging: comedi: ni_tio.h: tidy up struct ni_gpct_device
    
    Fix the checkpatch.pl issues:
    WARNING: Prefer 'unsigned int' to bare use of 'unsigned'
    CHECK: spinlock_t definition without comment
    
    Signed-off-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Reviewed-by: Ian Abbott <abbotti@mev.co.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e8f6e2b98d164d94bc29bb488413301d26cc5b3c
Author: H Hartley Sweeten <hsweeten@visionengravers.com>
Date:   Tue Mar 22 11:10:36 2016 -0700

    staging: comedi: ni_660x: add comments for the spinlock_t definitions
    
    Fix the checkpatch.pl issues:
    
    CHECK: spinlock_t definition without comment
    
    For aesthetics, rename the 'soft_reg_copy_lock' to clarify what it's
    used for.
    
    Signed-off-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Reviewed-by: Ian Abbott <abbotti@mev.co.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fbed0bc0915e2dec7452fc3e66ad03dd2b0c04c7
Merge: d37a14bb5fed 38460a2178d2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 14 15:50:44 2016 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking changes from Ingo Molnar:
     "Various updates:
    
       - Futex scalability improvements: remove page lock use for shared
         futex get_futex_key(), which speeds up 'perf bench futex hash'
         benchmarks by over 40% on a 60-core Westmere.  This makes anon-mem
         shared futexes perform close to private futexes.  (Mel Gorman)
    
       - lockdep hash collision detection and fix (Alfredo Alvarez
         Fernandez)
    
       - lockdep testing enhancements (Alfredo Alvarez Fernandez)
    
       - robustify lockdep init by using hlists (Andrew Morton, Andrey
         Ryabinin)
    
       - mutex and csd_lock micro-optimizations (Davidlohr Bueso)
    
       - small x86 barriers tweaks (Michael S Tsirkin)
    
       - qspinlock updates (Waiman Long)"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (25 commits)
      locking/csd_lock: Use smp_cond_acquire() in csd_lock_wait()
      locking/csd_lock: Explicitly inline csd_lock*() helpers
      futex: Replace barrier() in unqueue_me() with READ_ONCE()
      locking/lockdep: Detect chain_key collisions
      locking/lockdep: Prevent chain_key collisions
      tools/lib/lockdep: Fix link creation warning
      tools/lib/lockdep: Add tests for AA and ABBA locking
      tools/lib/lockdep: Add userspace version of READ_ONCE()
      tools/lib/lockdep: Fix the build on recent kernels
      locking/qspinlock: Move __ARCH_SPIN_LOCK_UNLOCKED to qspinlock_types.h
      locking/mutex: Allow next waiter lockless wakeup
      locking/pvqspinlock: Enable slowpath locking count tracking
      locking/qspinlock: Use smp_cond_acquire() in pending code
      locking/pvqspinlock: Move lock stealing count tracking code into pv_queued_spin_steal_lock()
      locking/mcs: Fix mcs_spin_lock() ordering
      futex: Remove requirement for lock_page() in get_futex_key()
      futex: Rename barrier references in ordering guarantees
      locking/atomics: Update comment about READ_ONCE() and structures
      locking/lockdep: Eliminate lockdep_init()
      locking/lockdep: Convert hash tables to hlists
      ...

commit 6a14c5ea380c1260772c70b9fd0a1492131f6116
Author: Jubin John <jubin.john@intel.com>
Date:   Sun Feb 14 20:21:34 2016 -0800

    staging/rdma/hfi1: Add comment for spinlock_t definition
    
    Add comments describing the spinlock for spinlock_t definitions to
    fix checkpatch check:
    CHECK: spinlock_t definition without comment
    
    Reviewed-by: Dennis Dalessandro <dennis.dalessandro@intel.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Mike Marciniszyn <mike.marciniszyn@intel.com>
    Signed-off-by: Jubin John <jubin.john@intel.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

commit b82e530290a0437522720becaf4abdf8ca4cb7d2
Author: Dan Streetman <dan.streetman@canonical.com>
Date:   Fri Feb 19 13:49:27 2016 -0500

    locking/qspinlock: Move __ARCH_SPIN_LOCK_UNLOCKED to qspinlock_types.h
    
    Move the __ARCH_SPIN_LOCK_UNLOCKED definition from qspinlock.h into
    qspinlock_types.h.
    
    The definition of __ARCH_SPIN_LOCK_UNLOCKED comes from the build arch's
    include files; but on x86 when CONFIG_QUEUED_SPINLOCKS=y, it just
    it's defined in asm-generic/qspinlock.h.  In most cases, this doesn't
    matter because linux/spinlock.h includes asm/spinlock.h, which for x86
    includes asm-generic/qspinlock.h.  However, any code that only includes
    linux/mutex.h will break, because it only includes asm/spinlock_types.h.
    
    For example, this breaks systemtap, which only includes mutex.h.
    
    Signed-off-by: Dan Streetman <dan.streetman@canonical.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <Waiman.Long@hpe.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dan Streetman <ddstreet@ieee.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1455907767-17821-1-git-send-email-dan.streetman@canonical.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f1cf2312ae22803f38d37ed141c3e4f2e84f8ee1
Author: Shraddha Barke <shraddha.6596@gmail.com>
Date:   Sun Dec 27 02:44:42 2015 +0530

    Staging: gdm72xx: Add appropriate comment for spinlock_t definition
    
    Fix checkpatch issue: "CHECK: spinlock_t definition without comment".
    
    Signed-off-by: Shraddha Barke <shraddha.6596@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 24ba3c534f7f6e17fb3fbdcee9427f05a0fd074e
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed May 1 05:24:03 2013 +0000

    af_unix: fix a fatal race with bit fields
    
    commit 60bc851ae59bfe99be6ee89d6bc50008c85ec75d upstream.
    
    Using bit fields is dangerous on ppc64/sparc64, as the compiler [1]
    uses 64bit instructions to manipulate them.
    If the 64bit word includes any atomic_t or spinlock_t, we can lose
    critical concurrent changes.
    
    This is happening in af_unix, where unix_sk(sk)->gc_candidate/
    gc_maybe_cycle/lock share the same 64bit word.
    
    This leads to fatal deadlock, as one/several cpus spin forever
    on a spinlock that will never be available again.
    
    A safer way would be to use a long to store flags.
    This way we are sure compiler/arch wont do bad things.
    
    As we own unix_gc_lock spinlock when clearing or setting bits,
    we can use the non atomic __set_bit()/__clear_bit().
    
    recursion_level can share the same 64bit location with the spinlock,
    as it is set only with this spinlock held.
    
    [1] bug fixed in gcc-4.8.0 :
    http://gcc.gnu.org/bugzilla/show_bug.cgi?id=52080
    
    Reported-by: Ambrose Feinstein <ambrose@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    (cherry picked from commit 2ee9cbe7e7bfe2d36374288b818aa31b2c4981db)
    [wt: adjusted context]
    Signed-off-by: Willy Tarreau <w@1wt.eu>

commit 2ee9cbe7e7bfe2d36374288b818aa31b2c4981db
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed May 1 05:24:03 2013 +0000

    af_unix: fix a fatal race with bit fields
    
    commit 60bc851ae59bfe99be6ee89d6bc50008c85ec75d upstream.
    
    Using bit fields is dangerous on ppc64/sparc64, as the compiler [1]
    uses 64bit instructions to manipulate them.
    If the 64bit word includes any atomic_t or spinlock_t, we can lose
    critical concurrent changes.
    
    This is happening in af_unix, where unix_sk(sk)->gc_candidate/
    gc_maybe_cycle/lock share the same 64bit word.
    
    This leads to fatal deadlock, as one/several cpus spin forever
    on a spinlock that will never be available again.
    
    A safer way would be to use a long to store flags.
    This way we are sure compiler/arch wont do bad things.
    
    As we own unix_gc_lock spinlock when clearing or setting bits,
    we can use the non atomic __set_bit()/__clear_bit().
    
    recursion_level can share the same 64bit location with the spinlock,
    as it is set only with this spinlock held.
    
    [1] bug fixed in gcc-4.8.0 :
    http://gcc.gnu.org/bugzilla/show_bug.cgi?id=52080
    
    Reported-by: Ambrose Feinstein <ambrose@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit c139aa60c1007429335131167a0ca181e38c5668
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Nov 18 10:13:08 2015 +0000

    arm64: barriers: fix smp_load_acquire to work with const arguments
    
    A newly introduced function in include/net/sock.h passes a const
    argument to smp_load_acquire:
    
      static inline int sk_state_load(const struct sock *sk)
      {
            return smp_load_acquire(&sk->sk_state);
      }
    
    This cause an allmodconfig build failure, since our underlying
    load-acquire implementation does not handle const types correctly:
    
      include/net/sock.h: In function 'sk_state_load':
      ./arch/arm64/include/asm/barrier.h:71:3: error: read-only variable '___p1' used as 'asm' output
         asm volatile ("ldarb %w0, %1"    \
    
    This patch fixes the problem by reusing the trick in READ_ONCE that
    loads via a non-const member of an anonymous union. This has the
    advantage of allowing us to use smp_load_acquire on packed structures
    (e.g. arch_spinlock_t) as well as primitive types.
    
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Reported-by: Arnd Bergmann <arnd@arndb.de>
    Reported-by: David Daney <david.daney@cavium.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

commit b716d0f38305a2bfe98379ce884e34802e2fcadf
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Thu May 28 10:22:10 2015 +0200

    mtd: dc21285: use raw spinlock functions for nw_gpio_lock
    
    commit e5babdf928e5d0c432a8d4b99f20421ce14d1ab6 upstream.
    
    Since commit bd31b85960a7 (which is in 3.2-rc1) nw_gpio_lock is a raw spinlock
    that needs usage of the corresponding raw functions.
    
    This fixes:
    
      drivers/mtd/maps/dc21285.c: In function 'nw_en_write':
      drivers/mtd/maps/dc21285.c:41:340: warning: passing argument 1 of 'spinlock_check' from incompatible pointer type
        spin_lock_irqsave(&nw_gpio_lock, flags);
    
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:299:102: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
                                                                                                            ^
      drivers/mtd/maps/dc21285.c:43:25: warning: passing argument 1 of 'spin_unlock_irqrestore' from incompatible pointer type
        spin_unlock_irqrestore(&nw_gpio_lock, flags);
                               ^
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:370:91: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
    
    Fixes: bd31b85960a7 ("locking, ARM: Annotate low level hw locks as raw")
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Brian Norris <computersforpeace@gmail.com>
    Signed-off-by: Zefan Li <lizefan@huawei.com>

commit d91876496bcf2236efb75ef12378c964bbe8c970
Author: Byongho Lee <bhlee.kernel@gmail.com>
Date:   Wed Oct 14 14:05:24 2015 +0900

    btrfs: compress: put variables defined per compress type in struct to make cache friendly
    
    Below variables are defined per compress type.
     - struct list_head comp_idle_workspace[BTRFS_COMPRESS_TYPES]
     - spinlock_t comp_workspace_lock[BTRFS_COMPRESS_TYPES]
     - int comp_num_workspace[BTRFS_COMPRESS_TYPES]
     - atomic_t comp_alloc_workspace[BTRFS_COMPRESS_TYPES]
     - wait_queue_head_t comp_workspace_wait[BTRFS_COMPRESS_TYPES]
    
    BTW, while accessing one compress type of these variables, the next or
    before address is other compress types of it.
    So this patch puts these variables in a struct to make cache friendly.
    
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Byongho Lee <bhlee.kernel@gmail.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 214e38421c330a1c9a16e65e071c86986cf3952d
Author: Ian Abbott <abbotti@mev.co.uk>
Date:   Mon Oct 12 18:03:24 2015 +0100

    staging: comedi: comedidev.h: add comments to spin-lock and mutex
    
    Fix the checkpatch.pl issues:
    
    CHECK: spinlock_t definition without comment
    CHECK: struct mutes definition withoug comment
    
    Signed-off-by: Ian Abbott <abbotti@mev.co.uk>
    Reviewed-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bb7085b1fbc7eee6d254fe978bd00ffcfe959bf6
Author: H Hartley Sweeten <hsweeten@visionengravers.com>
Date:   Fri Oct 9 10:47:45 2015 -0700

    staging: comedi: addi_apci_2032: document the spinlock_t definition
    
    Add some comments to the private data tp quiet the checkpatch.pl
    issue about:
    
    CHECK: spinlock_t definition without comment
    
    Signed-off-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Reviewed-by: Ian Abbott <abbotti@mev.co.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 81b784b11ea65c5c591f4d963daed2111a1b4280
Author: Guenter Roeck <linux@roeck-us.net>
Date:   Sat Aug 1 18:29:37 2015 -0700

    Orangefs: Swap order of include files
    
    spinlock_types.h requires types from linux/types.h.
    Including spinlock_types.h first may result in the following build errors,
    as seen with arm:allmodconfig.
    
    arch/arm/include/asm/spinlock_types.h:12:3: error: unknown type name 'u32'
    arch/arm/include/asm/spinlock_types.h:16:4: error: unknown type name 'u16'
    
    Fixes: deb4fb58ff73 ("Orangefs: kernel client part 2")
    Cc: Mark Brown <broonie@kernel.org>
    Cc: Mike Marshall <hubcap@omnibond.com>
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Mike Marshall <hubcap@omnibond.com>

commit 627c89b4d7c0a916b7702e23ded6e063dcb14ad5
Author: Axel Lin <axel.lin@ingics.com>
Date:   Wed Aug 5 22:37:41 2015 +0800

    gpio: omap: Fix missing raw locks conversion
    
    Fix below build warning:
      CC      drivers/gpio/gpio-omap.o
    drivers/gpio/gpio-omap.c: In function 'omap_gpio_irq_type':
    drivers/gpio/gpio-omap.c:504:3: warning: passing argument 1 of 'spin_unlock_irqrestore' from incompatible pointer type [enabled by default]
    include/linux/spinlock.h:360:29: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
    
    Fixes: commit 4dbada2be460 ("gpio: omap: use raw locks for locking")
    Signed-off-by: Axel Lin <axel.lin@ingics.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>

commit 3d0b261c65b60df9dde561616217f93f8e411bdb
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Thu May 28 10:22:10 2015 +0200

    mtd: dc21285: use raw spinlock functions for nw_gpio_lock
    
    commit e5babdf928e5d0c432a8d4b99f20421ce14d1ab6 upstream.
    
    Since commit bd31b85960a7 (which is in 3.2-rc1) nw_gpio_lock is a raw spinlock
    that needs usage of the corresponding raw functions.
    
    This fixes:
    
      drivers/mtd/maps/dc21285.c: In function 'nw_en_write':
      drivers/mtd/maps/dc21285.c:41:340: warning: passing argument 1 of 'spinlock_check' from incompatible pointer type
        spin_lock_irqsave(&nw_gpio_lock, flags);
    
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:299:102: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
                                                                                                            ^
      drivers/mtd/maps/dc21285.c:43:25: warning: passing argument 1 of 'spin_unlock_irqrestore' from incompatible pointer type
        spin_unlock_irqrestore(&nw_gpio_lock, flags);
                               ^
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:370:91: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
    
    Fixes: bd31b85960a7 ("locking, ARM: Annotate low level hw locks as raw")
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Brian Norris <computersforpeace@gmail.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 9b35e24e866fd5456fec99277a80c4d7622941f1
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Thu May 28 10:22:10 2015 +0200

    mtd: dc21285: use raw spinlock functions for nw_gpio_lock
    
    commit e5babdf928e5d0c432a8d4b99f20421ce14d1ab6 upstream.
    
    Since commit bd31b85960a7 (which is in 3.2-rc1) nw_gpio_lock is a raw spinlock
    that needs usage of the corresponding raw functions.
    
    This fixes:
    
      drivers/mtd/maps/dc21285.c: In function 'nw_en_write':
      drivers/mtd/maps/dc21285.c:41:340: warning: passing argument 1 of 'spinlock_check' from incompatible pointer type
        spin_lock_irqsave(&nw_gpio_lock, flags);
    
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:299:102: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
                                                                                                            ^
      drivers/mtd/maps/dc21285.c:43:25: warning: passing argument 1 of 'spin_unlock_irqrestore' from incompatible pointer type
        spin_unlock_irqrestore(&nw_gpio_lock, flags);
                               ^
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:370:91: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
    
    Fixes: bd31b85960a7 ("locking, ARM: Annotate low level hw locks as raw")
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Brian Norris <computersforpeace@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7f9e2ccd7f142ca51219fdbeb3cb04b9f665966c
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Thu May 28 10:22:10 2015 +0200

    mtd: dc21285: use raw spinlock functions for nw_gpio_lock
    
    commit e5babdf928e5d0c432a8d4b99f20421ce14d1ab6 upstream.
    
    Since commit bd31b85960a7 (which is in 3.2-rc1) nw_gpio_lock is a raw spinlock
    that needs usage of the corresponding raw functions.
    
    This fixes:
    
      drivers/mtd/maps/dc21285.c: In function 'nw_en_write':
      drivers/mtd/maps/dc21285.c:41:340: warning: passing argument 1 of 'spinlock_check' from incompatible pointer type
        spin_lock_irqsave(&nw_gpio_lock, flags);
    
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:299:102: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
                                                                                                            ^
      drivers/mtd/maps/dc21285.c:43:25: warning: passing argument 1 of 'spin_unlock_irqrestore' from incompatible pointer type
        spin_unlock_irqrestore(&nw_gpio_lock, flags);
                               ^
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:370:91: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
    
    Fixes: bd31b85960a7 ("locking, ARM: Annotate low level hw locks as raw")
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Brian Norris <computersforpeace@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bd206d3d8439b3994e0c5aa88b30a47e962b9797
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Thu May 28 10:22:10 2015 +0200

    mtd: dc21285: use raw spinlock functions for nw_gpio_lock
    
    commit e5babdf928e5d0c432a8d4b99f20421ce14d1ab6 upstream.
    
    Since commit bd31b85960a7 (which is in 3.2-rc1) nw_gpio_lock is a raw spinlock
    that needs usage of the corresponding raw functions.
    
    This fixes:
    
      drivers/mtd/maps/dc21285.c: In function 'nw_en_write':
      drivers/mtd/maps/dc21285.c:41:340: warning: passing argument 1 of 'spinlock_check' from incompatible pointer type
        spin_lock_irqsave(&nw_gpio_lock, flags);
    
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:299:102: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
                                                                                                            ^
      drivers/mtd/maps/dc21285.c:43:25: warning: passing argument 1 of 'spin_unlock_irqrestore' from incompatible pointer type
        spin_unlock_irqrestore(&nw_gpio_lock, flags);
                               ^
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:370:91: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
    
    Fixes: bd31b85960a7 ("locking, ARM: Annotate low level hw locks as raw")
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Brian Norris <computersforpeace@gmail.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit 772d68355e2f65f71e0402e39aabfdea56f55083
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jul 27 11:07:03 2015 +0100

    arm64: include linux/types.h in asm/spinlock_types.h
    
    Our ticket-based spinlock structures rely on a definition of u16, so
    include linux/types.h explicitly to ensure the thing compiles.
    
    Found by a module build failure in -next:
    
      arch/arm64/include/asm/spinlock_types.h:27:2: error: unknown type name 'u16'
      arch/arm64/include/asm/spinlock_types.h:28:2: error: unknown type name 'u16'
      arch/arm64/include/asm/spinlock_types.h:33:13: error: expected declaration specifiers or '...' before numeric constant
      include/linux/spinlock_types.h:21:2: error: unknown type name 'arch_spinlock_t'
      arch/arm64/include/asm/spinlock.h:34:35: error: unknown type name 'arch_spinlock_t'
      arch/arm64/include/asm/spinlock.h:65:37: error: unknown type name 'arch_spinlock_t'
    
    Reported-by: Russell King <linux@arm.linux.org.uk>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 89e478a2aa58af2548b7f316e4d5b6bcc9eade5b
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Jul 22 07:02:00 2015 +0200

    tcp: suppress a division by zero warning
    
    Andrew Morton reported following warning on one ARM build
    with gcc-4.4 :
    
    net/ipv4/inet_hashtables.c: In function 'inet_ehash_locks_alloc':
    net/ipv4/inet_hashtables.c:617: warning: division by zero
    
    Even guarded with a test on sizeof(spinlock_t), compiler does not
    like current construct on a !CONFIG_SMP build.
    
    Remove the warning by using a temporary variable.
    
    Fixes: 095dc8e0c368 ("tcp: fix/cleanup inet_ehash_locks_alloc()")
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit cf5a5f22560fd3ec00551760264ec22bfdb3e5ee
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Thu May 28 10:22:10 2015 +0200

    mtd: dc21285: use raw spinlock functions for nw_gpio_lock
    
    commit e5babdf928e5d0c432a8d4b99f20421ce14d1ab6 upstream.
    
    Since commit bd31b85960a7 (which is in 3.2-rc1) nw_gpio_lock is a raw spinlock
    that needs usage of the corresponding raw functions.
    
    This fixes:
    
      drivers/mtd/maps/dc21285.c: In function 'nw_en_write':
      drivers/mtd/maps/dc21285.c:41:340: warning: passing argument 1 of 'spinlock_check' from incompatible pointer type
        spin_lock_irqsave(&nw_gpio_lock, flags);
    
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:299:102: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
                                                                                                            ^
      drivers/mtd/maps/dc21285.c:43:25: warning: passing argument 1 of 'spin_unlock_irqrestore' from incompatible pointer type
        spin_unlock_irqrestore(&nw_gpio_lock, flags);
                               ^
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:370:91: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
    
    Fixes: bd31b85960a7 ("locking, ARM: Annotate low level hw locks as raw")
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Brian Norris <computersforpeace@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4aa339cddbcc05b7f8ff4f0960550929aa77213e
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Thu May 28 10:22:10 2015 +0200

    mtd: dc21285: use raw spinlock functions for nw_gpio_lock
    
    commit e5babdf928e5d0c432a8d4b99f20421ce14d1ab6 upstream.
    
    Since commit bd31b85960a7 (which is in 3.2-rc1) nw_gpio_lock is a raw spinlock
    that needs usage of the corresponding raw functions.
    
    This fixes:
    
      drivers/mtd/maps/dc21285.c: In function 'nw_en_write':
      drivers/mtd/maps/dc21285.c:41:340: warning: passing argument 1 of 'spinlock_check' from incompatible pointer type
        spin_lock_irqsave(&nw_gpio_lock, flags);
    
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:299:102: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
                                                                                                            ^
      drivers/mtd/maps/dc21285.c:43:25: warning: passing argument 1 of 'spin_unlock_irqrestore' from incompatible pointer type
        spin_unlock_irqrestore(&nw_gpio_lock, flags);
                               ^
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:370:91: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
    
    Fixes: bd31b85960a7 ("locking, ARM: Annotate low level hw locks as raw")
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Brian Norris <computersforpeace@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ced5ded3421844a32405d18e3f2b58d65bc12271
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Thu May 28 10:22:10 2015 +0200

    mtd: dc21285: use raw spinlock functions for nw_gpio_lock
    
    commit e5babdf928e5d0c432a8d4b99f20421ce14d1ab6 upstream.
    
    Since commit bd31b85960a7 (which is in 3.2-rc1) nw_gpio_lock is a raw spinlock
    that needs usage of the corresponding raw functions.
    
    This fixes:
    
      drivers/mtd/maps/dc21285.c: In function 'nw_en_write':
      drivers/mtd/maps/dc21285.c:41:340: warning: passing argument 1 of 'spinlock_check' from incompatible pointer type
        spin_lock_irqsave(&nw_gpio_lock, flags);
    
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:299:102: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
                                                                                                            ^
      drivers/mtd/maps/dc21285.c:43:25: warning: passing argument 1 of 'spin_unlock_irqrestore' from incompatible pointer type
        spin_unlock_irqrestore(&nw_gpio_lock, flags);
                               ^
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:370:91: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
    
    Fixes: bd31b85960a7 ("locking, ARM: Annotate low level hw locks as raw")
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Brian Norris <computersforpeace@gmail.com>
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit 53b8762727cfc81212fd7073618cb2609bd2fd60
Merge: f09becc79f89 ca0f6a5cd99e
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Mon Jun 15 18:31:22 2015 +0200

    Merge branch 'master' of git://blackhole.kfki.hu/nf-next
    
    Jozsef Kadlecsik says:
    
    ====================
    ipset patches for nf-next
    
    Please consider to apply the next bunch of patches for ipset. First
    comes the small changes, then the bugfixes and at the end the RCU
    related patches.
    
    * Use MSEC_PER_SEC consistently instead of the number.
    * Use SET_WITH_*() helpers to test set extensions from Sergey Popovich.
    * Check extensions attributes before getting extensions from Sergey Popovich.
    * Permit CIDR equal to the host address CIDR in IPv6 from Sergey Popovich.
    * Make sure we always return line number on batch in the case of error
      from Sergey Popovich.
    * Check CIDR value only when attribute is given from Sergey Popovich.
    * Fix cidr handling for hash:*net* types, reported by Jonathan Johnson.
    * Fix parallel resizing and listing of the same set so that the original
      set is kept for the whole dumping.
    * Make sure listing doesn't grab a set which is just being destroyed.
    * Remove rbtree from ip_set_hash_netiface.c in order to introduce RCU.
    * Replace rwlock_t with spinlock_t in "struct ip_set", change the locking
      in the core and simplifications in the timeout routines.
    * Introduce RCU locking in bitmap:* types with a slight modification in the
      logic on how an element is added.
    * Introduce RCU locking in hash:* types. This is the most complex part of
      the changes.
    * Introduce RCU locking in list type where standard rculist is used.
    * Fix coding styles reported by checkpatch.pl.
    ====================
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

commit b57b2d1fa53fe8563bdfc66a33b844463b9af285
Author: Jozsef Kadlecsik <kadlec@blackhole.kfki.hu>
Date:   Sat Jun 13 14:22:25 2015 +0200

    netfilter: ipset: Prepare the ipset core to use RCU at set level
    
    Replace rwlock_t with spinlock_t in "struct ip_set" and change the locking
    accordingly. Convert the comment extension into an rcu-avare object. Also,
    simplify the timeout routines.
    
    Signed-off-by: Jozsef Kadlecsik <kadlec@blackhole.kfki.hu>

commit e5babdf928e5d0c432a8d4b99f20421ce14d1ab6
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Thu May 28 10:22:10 2015 +0200

    mtd: dc21285: use raw spinlock functions for nw_gpio_lock
    
    Since commit bd31b85960a7 (which is in 3.2-rc1) nw_gpio_lock is a raw spinlock
    that needs usage of the corresponding raw functions.
    
    This fixes:
    
      drivers/mtd/maps/dc21285.c: In function 'nw_en_write':
      drivers/mtd/maps/dc21285.c:41:340: warning: passing argument 1 of 'spinlock_check' from incompatible pointer type
        spin_lock_irqsave(&nw_gpio_lock, flags);
    
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:299:102: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
                                                                                                            ^
      drivers/mtd/maps/dc21285.c:43:25: warning: passing argument 1 of 'spin_unlock_irqrestore' from incompatible pointer type
        spin_unlock_irqrestore(&nw_gpio_lock, flags);
                               ^
      In file included from include/linux/seqlock.h:35:0,
                       from include/linux/time.h:5,
                       from include/linux/stat.h:18,
                       from include/linux/module.h:10,
                       from drivers/mtd/maps/dc21285.c:8:
      include/linux/spinlock.h:370:91: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
       static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
    
    Fixes: bd31b85960a7 ("locking, ARM: Annotate low level hw locks as raw")
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Brian Norris <computersforpeace@gmail.com>

commit 35bb871663ddb06df9d601b32deac5f4f06b65b4
Author: H Hartley Sweeten <hsweeten@visionengravers.com>
Date:   Fri May 1 15:00:12 2015 -0700

    staging: comedi: ni_stc.h: final cleanup
    
    1) Move the enum's to a better location and tidy up the whitespace.
    2) Tidy up the defines used for some array sizes in the private data.
    3) Add comments for the spinlock_t variables in the private data.
    4) Move the forward declaration to the end of the file.
    
    Signed-off-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Reviewed-by: Ian Abbott <abbotti@mev.co.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 34cb7954c0aa7c8ad1591cb6cceae36432f55bb5
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Fri Mar 20 20:39:46 2015 +1100

    KVM: PPC: Book3S HV: Convert ICS mutex lock to spin lock
    
    Replaces the ICS mutex lock with a spin lock since we will be porting
    these routines to real mode. Note that we need to disable interrupts
    before we take the lock in anticipation of the fact that on the guest
    side, we are running in the context of a hard irq and interrupts are
    disabled (EE bit off) when the lock is acquired. Again, because we
    will be acquiring the lock in hypervisor real mode, we need to use
    an arch_spinlock_t instead of a normal spinlock here as we want to
    avoid running any lockdep code (which may not be safe to execute in
    real mode).
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

commit ae75116efdc29bb42f1d99f8c51b5c52965b2413
Author: Suresh E. Warrier <warrier@linux.vnet.ibm.com>
Date:   Wed Feb 25 17:23:53 2015 -0600

    powerpc: Export __spin_yield
    
    Export __spin_yield so that the arch_spin_unlock() function can
    be invoked from a module. This will be required for modules where
    we want to take a lock that is also is acquired in hypervisor
    real mode. Because we want to avoid running any lockdep code
    (which may not be safe in real mode), this lock needs to be
    an arch_spinlock_t instead of a normal spinlock.
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Alexander Graf <agraf@suse.de>

commit e979121b1b1556e184492e6fc149bbe188fc83e6
Author: Maria Dimakopoulou <maria.n.dimakopoulou@gmail.com>
Date:   Mon Nov 17 20:06:58 2014 +0100

    perf/x86/intel: Implement cross-HT corruption bug workaround
    
    This patch implements a software workaround for a HW erratum
    on Intel SandyBridge, IvyBridge and Haswell processors
    with Hyperthreading enabled. The errata are documented for
    each processor in their respective specification update
    documents:
    
      - SandyBridge: BJ122
      - IvyBridge: BV98
      - Haswell: HSD29
    
    The bug causes silent counter corruption across hyperthreads only
    when measuring certain memory events (0xd0, 0xd1, 0xd2, 0xd3).
    Counters measuring those events may leak counts to the sibling
    counter. For instance, counter 0, thread 0 measuring event 0xd0,
    may leak to counter 0, thread 1, regardless of the event measured
    there. The size of the leak is not predictible. It all depends on
    the workload and the state of each sibling hyper-thread. The
    corrupting events do undercount as a consequence of the leak. The
    leak is compensated automatically only when the sibling counter measures
    the exact same corrupting event AND the workload is on the two threads
    is the same. Given, there is no way to guarantee this, a work-around
    is necessary. Furthermore, there is a serious problem if the leaked count
    is added to a low-occurrence event. In that case the corruption on
    the low occurrence event can be very large, e.g., orders of magnitude.
    
    There is no HW or FW workaround for this problem.
    
    The bug is very easy to reproduce on a loaded system.
    Here is an example on a Haswell client, where CPU0, CPU4
    are siblings. We load the CPUs with a simple triad app
    streaming large floating-point vector. We use 0x81d0
    corrupting event (MEM_UOPS_RETIRED:ALL_LOADS) and
    0x20cc (ROB_MISC_EVENTS:LBR_INSERTS). Given we are not
    using the LBR, the 0x20cc event should be zero.
    
      $ taskset -c 0 triad &
      $ taskset -c 4 triad &
      $ perf stat -a -C 0 -e r81d0 sleep 100 &
      $ perf stat -a -C 4 -r20cc sleep 10
      Performance counter stats for 'system wide':
            139 277 291      r20cc
           10,000969126 seconds time elapsed
    
    In this example, 0x81d0 and r20cc ar eusing sinling counters
    on CPU0 and CPU4. 0x81d0 leaks into 0x20cc and corrupts it
    from 0 to 139 millions occurrences.
    
    This patch provides a software workaround to this problem by modifying the
    way events are scheduled onto counters by the kernel. The patch forces
    cross-thread mutual exclusion between counters in case a corrupting event
    is measured by one of the hyper-threads. If thread 0, counter 0 is measuring
    event 0xd0, then nothing can be measured on counter 0, thread 1. If no corrupting
    event is measured on any hyper-thread, event scheduling proceeds as before.
    
    The same example run with the workaround enabled, yield the correct answer:
    
      $ taskset -c 0 triad &
      $ taskset -c 4 triad &
      $ perf stat -a -C 0 -e r81d0 sleep 100 &
      $ perf stat -a -C 4 -r20cc sleep 10
      Performance counter stats for 'system wide':
            0 r20cc
           10,000969126 seconds time elapsed
    
    The patch does provide correctness for all non-corrupting events. It does not
    "repatriate" the leaked counts back to the leaking counter. This is planned
    for a second patch series. This patch series makes this repatriation more
    easy by guaranteeing the sibling counter is not measuring any useful event.
    
    The patch introduces dynamic constraints for events. That means that events which
    did not have constraints, i.e., could be measured on any counters, may now be
    constrained to a subset of the counters depending on what is going on the sibling
    thread. The algorithm is similar to a cache coherency protocol. We call it XSU
    in reference to Exclusive, Shared, Unused, the 3 possible states of a PMU
    counter.
    
    As a consequence of the workaround, users may see an increased amount of event
    multiplexing, even in situtations where there are fewer events than counters
    measured on a CPU.
    
    Patch has been tested on all three impacted processors. Note that when
    HT is off, there is no corruption. However, the workaround is still enabled,
    yet not costing too much. Adding a dynamic detection of HT on turned out to
    be complex are requiring too much to code to be justified.
    
    This patch addresses the issue when PEBS is not used. A subsequent patch
    fixes the problem when PEBS is used.
    
    Signed-off-by: Maria Dimakopoulou <maria.n.dimakopoulou@gmail.com>
    [spinlock_t -> raw_spinlock_t]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Stephane Eranian <eranian@google.com>
    Cc: bp@alien8.de
    Cc: jolsa@redhat.com
    Cc: kan.liang@intel.com
    Link: http://lkml.kernel.org/r/1416251225-17721-7-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6f6539cad926f55d5eb6e79d05bbe99f0d54d56d
Author: Maria Dimakopoulou <maria.n.dimakopoulou@gmail.com>
Date:   Mon Nov 17 20:06:57 2014 +0100

    perf/x86/intel: Add cross-HT counter exclusion infrastructure
    
    This patch adds a new shared_regs style structure to the
    per-cpu x86 state (cpuc). It is used to coordinate access
    between counters which must be used with exclusion across
    HyperThreads on Intel processors. This new struct is not
    needed on each PMU, thus is is allocated on demand.
    
    Signed-off-by: Maria Dimakopoulou <maria.n.dimakopoulou@gmail.com>
    [peterz: spinlock_t -> raw_spinlock_t]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Stephane Eranian <eranian@google.com>
    Cc: bp@alien8.de
    Cc: jolsa@redhat.com
    Cc: kan.liang@intel.com
    Link: http://lkml.kernel.org/r/1416251225-17721-6-git-send-email-eranian@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1f8c82ab1b0bc7e24601c0fca411fd27b9c883ef
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Wed Mar 4 12:56:20 2015 +0100

    cpufreq/ppc: Add missing #include <asm/smp.h>
    
    If CONFIG_SMP=n, <linux/smp.h> does not include <asm/smp.h>, causing:
    
    drivers/cpufreq/ppc-corenet-cpufreq.c: In function 'corenet_cpufreq_cpu_init':
    drivers/cpufreq/ppc-corenet-cpufreq.c:173:3: error: implicit declaration of function 'get_hard_smp_processor_id' [-Werror=implicit-funcuresh E. Warrier" <warrier@linux.vnet.ibm.com>
    X-Patchwork-Id: 443703
    Message-Id: <54EE5989.7010800@linux.vnet.ibm.com>
    To: linuxppc-dev@ozlabs.org
    Date: Wed, 25 Feb 2015 17:23:53 -0600
    
    Export __spin_yield so that the arch_spin_unlock() function can
    be invoked from a module. This will be required for modules where
    we want to take a lock that is also is acquired in hypervisor
    real mode. Because we want to avoid running any lockdep code
    (which may not be safe in real mode), this lock needs to be
    an arch_spinlock_t instead of a normal spinlock.
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 1f3aefb5dfcaf28c0a0b5c270ca120818da88f58
Author: Melike Yurtoglu <aysemelikeyurtoglu@gmail.com>
Date:   Sun Feb 22 16:58:08 2015 +0200

    Staging: rtl8192e: replace memcpy() by ether_addr_copy() using coccinelle and pack variable
    
    This patch focuses on fixing the following warning generated
    by checkpatch.pl for the file rxtx.c
    
    Prefer ether_addr_copy() over memcpy() if the Ethernet addresses
    are __aligned(2)
    
    @@ expression e1, e2; @@
    
    - memcpy(e1, e2, ETH_ALEN);
    + ether_addr_copy(e1, e2);
    
    struct net_device {
            char                       name[16];             /*     0    16*/
            struct hlist_node          name_hlist;           /*    16    16*/
            char *                     ifalias;              /*    32     8*/
            long unsigned int          mem_end;              /*    40     8*/
            long unsigned int          mem_start;            /*    48     8*/
            long unsigned int          base_addr;            /*    56     8*/
            /* --- cacheline 1 boundary (64 bytes) --- */
            int                        irq;                  /*    64     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            long unsigned int          state;                /*    72     8*/
            struct list_head           dev_list;             /*    80    16*/
            struct list_head           napi_list;            /*    96    16*/
            struct list_head           unreg_list;           /*   112    16*/
            /* --- cacheline 2 boundary (128 bytes) --- */
            struct list_head           close_list;           /*   128    16*/
            struct {
                    struct list_head   upper;                /*   144    16*/
                    struct list_head   lower;                /*   160    16*/
            } adj_list;                                      /*   144    32*/
            struct {
                    struct list_head   upper;                /*   176    16*/
                    struct list_head   lower;                /*   192    16*/
            } all_adj_list;                                  /*   176    32*/
            /* --- cacheline 3 boundary (192 bytes) was 16 bytes ago --- */
            netdev_features_t          features;             /*   208     8*/
            netdev_features_t          hw_features;          /*   216     8*/
            netdev_features_t          wanted_features;      /*   224     8*/
            netdev_features_t          vlan_features;        /*   232     8*/
            netdev_features_t          hw_enc_features;      /*   240     8*/
            netdev_features_t          mpls_features;        /*   248     8*/
            /* --- cacheline 4 boundary (256 bytes) --- */
            int                        ifindex;              /*   256     4*/
            int                        iflink;               /*   260     4*/
            struct net_device_stats    stats;                /*   264   184*/
            /* --- cacheline 7 boundary (448 bytes) --- */
            atomic_long_t              rx_dropped;           /*   448     8*/
            atomic_long_t              tx_dropped;           /*   456     8*/
            atomic_t                   carrier_changes;      /*   464     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            const struct iw_handler_def  * wireless_handlers; /*   472     8*/
            struct iw_public_data *    wireless_data;        /*   480     8*/
            const struct net_device_ops  * netdev_ops;       /*   488     8*/
            const struct ethtool_ops  * ethtool_ops;         /*   496     8*/
            const struct forwarding_accel_ops  * fwd_ops;    /*   504     8*/
            /* --- cacheline 8 boundary (512 bytes) --- */
            const struct header_ops  * header_ops;           /*   512     8*/
            unsigned int               flags;                /*   520     4*/
            unsigned int               priv_flags;           /*   524     4*/
            short unsigned int         gflags;               /*   528     2*/
            short unsigned int         padded;               /*   530     2*/
            unsigned char              operstate;            /*   532     1*/
            unsigned char              link_mode;            /*   533     1*/
            unsigned char              if_port;              /*   534     1*/
            unsigned char              dma;                  /*   535     1*/
            unsigned int               mtu;                  /*   536     4*/
            short unsigned int         type;                 /*   540     2*/
            short unsigned int         hard_header_len;      /*   542     2*/
            short unsigned int         needed_headroom;      /*   544     2*/
            short unsigned int         needed_tailroom;      /*   546     2*/
            unsigned char              perm_addr[32];        /*   548    32*/
            /* --- cacheline 9 boundary (576 bytes) was 4 bytes ago --- */
            unsigned char              addr_assign_type;     /*   580     1*/
            unsigned char              addr_len;             /*   581     1*/
            short unsigned int         neigh_priv_len;       /*   582     2*/
            short unsigned int         dev_id;               /*   584     2*/
            short unsigned int         dev_port;             /*   586     2*/
            spinlock_t                 addr_list_lock;       /*   588     4*/
            struct netdev_hw_addr_list uc;                   /*   592    24*/
            struct netdev_hw_addr_list mc;                   /*   616    24*/
            /* --- cacheline 10 boundary (640 bytes) --- */
            struct netdev_hw_addr_list dev_addrs;            /*   640    24*/
            struct kset *              queues_kset;          /*   664     8*/
            unsigned char              name_assign_type;     /*   672     1*/
            bool                       uc_promisc;           /*   673     1*/
    
            /* XXX 2 bytes hole, try to pack */
    
            unsigned int               promiscuity;          /*   676     4*/
            unsigned int               allmulti;             /*   680     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            struct vlan_info *         vlan_info;            /*   688     8*/
            struct dsa_switch_tree *   dsa_ptr;              /*   696     8*/
            /* --- cacheline 11 boundary (704 bytes) --- */
            struct tipc_bearer *       tipc_ptr;             /*   704     8*/
            void *                     atalk_ptr;            /*   712     8*/
            struct in_device *         ip_ptr;               /*   720     8*/
            struct dn_dev *            dn_ptr;               /*   728     8*/
            struct inet6_dev *         ip6_ptr;              /*   736     8*/
            void *                     ax25_ptr;             /*   744     8*/
            struct wireless_dev *      ieee80211_ptr;        /*   752     8*/
            struct wpan_dev *          ieee802154_ptr;       /*   760     8*/
            /* --- cacheline 12 boundary (768 bytes) --- */
            long unsigned int          last_rx;              /*   768     8*/
            unsigned char *            dev_addr;             /*   776     8*/
            struct netdev_rx_queue *   _rx;                  /*   784     8*/
            unsigned int               num_rx_queues;        /*   792     4*/
            unsigned int               real_num_rx_queues;   /*   796     4*/
            long unsigned int          gro_flush_timeout;    /*   800     8*/
            rx_handler_func_t *        rx_handler;           /*   808     8*/
            void *                     rx_handler_data;      /*   816     8*/
            struct netdev_queue *      ingress_queue;        /*   824     8*/
            /* --- cacheline 13 boundary (832 bytes) --- */
            unsigned char              broadcast[32];        /*   832    32*/
    
            /* XXX 32 bytes hole, try to pack */
    
            /* --- cacheline 14 boundary (896 bytes) --- */
            struct netdev_queue *      _tx;                  /*   896     8*/
            unsigned int               num_tx_queues;        /*   904     4*/
            unsigned int               real_num_tx_queues;   /*   908     4*/
            struct Qdisc *             qdisc;                /*   912     8*/
            long unsigned int          tx_queue_len;         /*   920     8*/
            spinlock_t                 tx_global_lock;       /*   928     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            struct xps_dev_maps *      xps_maps;             /*   936     8*/
            struct cpu_rmap *          rx_cpu_rmap;          /*   944     8*/
            long unsigned int          trans_start;          /*   952     8*/
            /* --- cacheline 15 boundary (960 bytes) --- */
            int                        watchdog_timeo;       /*   960     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            struct timer_list          watchdog_timer;       /*   968    80*/
            /* --- cacheline 16 boundary (1024 bytes) was 24 bytes ago ---* */
            int *                      pcpu_refcnt;          /*  1048     8*/
            struct list_head           todo_list;            /*  1056    16*/
            struct hlist_node          index_hlist;          /*  1072    16*/
            /* --- cacheline 17 boundary (1088 bytes) --- */
            struct list_head           link_watch_list;      /*  1088    16*/
            enum {
                    NETREG_UNINITIALIZED = 0,
                    NETREG_REGISTERED = 1,
                    NETREG_UNREGISTERING = 2,
                    NETREG_UNREGISTERED = 3,
                    NETREG_RELEASED = 4,
                    NETREG_DUMMY = 5,
            } reg_state:8;                                     /*  1104 4 */
            /* Bitfield combined with next fields */
    
            bool                       dismantle;            /*  1105     1*/
    
            /* Bitfield combined with previous fields */
    
            enum {
                    RTNL_LINK_INITIALIZED = 0,
                    RTNL_LINK_INITIALIZING = 1,
            } rtnl_link_state:16;                               /*  1104 4 */
    
            /* XXX 4 bytes hole, try to pack */
    
            void                       (*destructor)(struct net_device *);/*  1112     8 */
            struct netpoll_info *      npinfo;               /*  1120     8*/
            struct net *               nd_net;               /*  1128     8*/
            union {
                    void *             ml_priv;              /*           8*/
                    struct pcpu_lstats * lstats;             /*           8*/
                    struct pcpu_sw_netstats * tstats;        /*           8*/
                    struct pcpu_dstats * dstats;             /*           8*/
                    struct pcpu_vstats * vstats;             /*           8*/
            };                                               /*  1136     8*/
            struct garp_port *         garp_port;            /*  1144     8*/
            /* --- cacheline 18 boundary (1152 bytes) was 4 bytes ago --- */
            struct mrp_port *          mrp_port;             /*  1152     8*/
            struct device              dev;                  /*  1160   696*/
    
            /* XXX last struct has 7 bytes of padding */
    
            /* --- cacheline 29 boundary (1856 bytes) was 4 bytes ago --- */
            const struct attribute_group  * sysfs_groups[4]; /*  1856    32*/
            const struct attribute_group  * sysfs_rx_queue_group; /*  18888 */
            const struct rtnl_link_ops  * rtnl_link_ops;     /*  1896     8*/
            unsigned int               gso_max_size;         /*  1904     4*/
            u16                        gso_max_segs;         /*  1908     2*/
            u16                        gso_min_segs;         /*  1910     2*/
            const struct dcbnl_rtnl_ops  * dcbnl_ops;        /*  1912     8*/
            /* --- cacheline 30 boundary (1920 bytes) was 4 bytes ago --- */
            u8                         num_tc;               /*  1920     1*/
    
            /* XXX 1 byte hole, try to pack */
    
            struct netdev_tc_txq       tc_to_txq[16];        /*  1922    64*/
            /* --- cacheline 31 boundary (1984 bytes) was 6 bytes ago --- */
            u8                         prio_tc_map[16];      /*  1986    16*/
    
            /* XXX 2 bytes hole, try to pack */
    
            unsigned int               fcoe_ddp_xid;         /*  2004     4*/
            struct phy_device *        phydev;               /*  2008     8*/
            struct lock_class_key *    qdisc_tx_busylock;    /*  2016     8*/
            int                        group;                /*  2024     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            struct pm_qos_request      pm_qos_req;           /*  2032   176*/
            /* --- cacheline 34 boundary (2176 bytes) was 36 bytes ago --- * */
    
            /* size: 2240, cachelines: 35, members: 120 */
            /* sum members: 2147, holes: 11, sum holes: 65 */
            /* padding: 32 */
            /* paddings: 1, sum paddings: 7 */
    
            /* BRAIN FART ALERT! 2240 != 2147 + 65(holes), diff = 28 */
    
    };
    
    Signed-off-by: Melike Yurtoglu <aysemelikeyurtoglu@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d55519ab9b20b0961bc13018506fb413d5a752d7
Author: Melike Yurtoglu <aysemelikeyurtoglu@gmail.com>
Date:   Mon Feb 23 09:11:57 2015 +0200

    Staging: rtl8712: replace memcpy() by ether_addr_copy() using coccinelle and pack variable
    
    This patch focuses on fixing the following warning generated
    by checkpatch.pl for the file rtl871x_cmd.c
    
    Prefer ether_addr_copy() over memcpy() if the Ethernet addresses
    are __aligned(2)
    
    @@ expression e1, e2; @@
    
    - memcpy(e1, e2, ETH_ALEN);
    + ether_addr_copy(e1, e2);
    
    struct _adapter {
            struct dvobj_priv          dvobjpriv;            /*     0    40*/
            struct mlme_priv           mlmepriv;             /*    40  1560*/
            /* --- cacheline 25 boundary (1600 bytes) --- */
            struct cmd_priv            cmdpriv;              /*  1600   136*/
            /* --- cacheline 27 boundary (1728 bytes) was 8 bytes ago --- */
            struct evt_priv            evtpriv;              /*  1736    96*/
            /* --- cacheline 28 boundary (1792 bytes) was 40 bytes ago --- * */
            struct io_queue *          pio_queue;            /*  1832     8*/
            struct xmit_priv           xmitpriv;             /*  1840   912*/
            /* --- cacheline 43 boundary (2752 bytes) --- */
            struct recv_priv           recvpriv;             /*  2752  1088*/
            /* --- cacheline 60 boundary (3840 bytes) --- */
            struct sta_priv            stapriv;              /*  3840   672*/
            /* --- cacheline 70 boundary (4480 bytes) was 32 bytes ago --- * */
            struct security_priv       securitypriv;         /*  4512  4816*/
            /* --- cacheline 145 boundary (9280 bytes) was 48 bytes ago --- * */
            struct registry_priv       registrypriv;         /*  9328   968*/
            /* --- cacheline 160 boundary (10240 bytes) was 56 bytes ago --- * */
            struct wlan_acl_pool       acl_list;             /* 10296  1536*/
            /* --- cacheline 184 boundary (11776 bytes) was 56 bytes ago --- * */
            struct pwrctrl_priv        pwrctrlpriv;          /* 11832   224*/
            /* --- cacheline 188 boundary (12032 bytes) was 24 bytes ago --- * */
            struct eeprom_priv         eeprompriv;           /* 12056   508*/
    
            /* XXX 4 bytes hole, try to pack */
    
            /* --- cacheline 196 boundary (12544 bytes) was 24 bytes ago --- * */
            struct hal_priv            halpriv;              /* 12568    88*/
            /* --- cacheline 197 boundary (12608 bytes) was 48 bytes ago --- * */
            struct led_priv            ledpriv;              /* 12656   304*/
            /* --- cacheline 202 boundary (12928 bytes) was 32 bytes ago --- * */
            struct mp_priv             mppriv;               /* 12960  1080*/
             /* --- cacheline 219 boundary (14016 bytes) was 24 bytes ago * --- */
            s32                        bDriverStopped;       /* 14040     4*/
            s32                        bSurpriseRemoved;     /* 14044     4*/
            u32                        IsrContent;           /* 14048     4*/
            u32                        ImrContent;           /* 14052     4*/
            u8                         EepromAddressSize;    /* 14056     1*/
            u8                         hw_init_completed;    /* 14057     1*/
    
            /* XXX 6 bytes hole, try to pack */
    
            struct task_struct *       cmdThread;            /* 14064     8*/
            pid_t                      evtThread;            /* 14072     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            /* --- cacheline 220 boundary (14080 bytes) --- */
            struct task_struct *       xmitThread;           /* 14080     8*/
            pid_t                      recvThread;           /* 14088     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            uint                       (*dvobj_init)(struct _adapter *); /*14096     8 */
            void                       (*dvobj_deinit)(struct _adapter *);/* 14104     8 */
            struct net_device *        pnetdev;              /* 14112     8*/
            int                        bup;                  /* 14120     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            struct net_device_stats    stats;                /* 14128   184*/
            /* --- cacheline 223 boundary (14272 bytes) was 40 bytes ago --- * */
             struct iw_statistics       iwstats;              /* 14312    32*/
            /* --- cacheline 224 boundary (14336 bytes) was 8 bytes ago --- * */
            int                        pid;                  /* 14344     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            struct work_struct         wkFilterRxFF0;        /* 14352    32*/
            u8                         blnEnableRxFF0Filter; /* 14384     1*/
    
            /* XXX 3 bytes hole, try to pack */
    
            spinlock_t                 lockRxFF0Filter;      /* 14388     4*/
            const struct firmware  *   fw;                   /* 14392     8*/
            /* --- cacheline 225 boundary (14400 bytes) --- */
            struct usb_interface *     pusb_intf;            /* 14400     8*/
            struct mutex               mutex_start;          /* 14408    40*/
    
            /* XXX last struct has 4 bytes of padding */
    
            struct completion          rtl8712_fw_ready;     /* 14448    32*/
            /* --- cacheline 226 boundary (14464 bytes) was 16 bytes ago --- * */
    
            /* size: 14480, cachelines: 227, members: 40 */
            /* sum members: 14451, holes: 7, sum holes: 29 */
            /* paddings: 1, sum paddings: 4 */
            /* last cacheline: 16 bytes */
    };
    
    Signed-off-by: Melike Yurtoglu <aysemelikeyurtoglu@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c7c42826f9afb017ee8927cfdc9c85f2e6d7bf0c
Author: Melike Yurtoglu <aysemelikeyurtoglu@gmail.com>
Date:   Mon Feb 23 08:42:56 2015 +0200

    Staging: rtl8712: replace memcpy() by ether_addr_copy() using coccinelle and pack variable
    
    This patch focuses on fixing the following warning generated
    by checkpatch.pl for the file rxtx.c
    
    Prefer ether_addr_copy() over memcpy() if the Ethernet addresses
    are __aligned(2)
    
    @@ expression e1, e2; @@
    
    - memcpy(e1, e2, ETH_ALEN);
    + ether_addr_copy(e1, e2);
    
    struct _adapter {
            struct dvobj_priv          dvobjpriv;            /*     0    40*/
            struct mlme_priv           mlmepriv;             /*    40  1560*/
            /* --- cacheline 25 boundary (1600 bytes) --- */
            struct cmd_priv            cmdpriv;              /*  1600   136*/
            /* --- cacheline 27 boundary (1728 bytes) was 8 bytes ago --- */
            struct evt_priv            evtpriv;              /*  1736    96*/
            /* --- cacheline 28 boundary (1792 bytes) was 40 bytes ago --- * */
            struct io_queue *          pio_queue;            /*  1832     8*/
            struct xmit_priv           xmitpriv;             /*  1840   912*/
            /* --- cacheline 43 boundary (2752 bytes) --- */
            struct recv_priv           recvpriv;             /*  2752  1088*/
            /* --- cacheline 60 boundary (3840 bytes) --- */
            struct sta_priv            stapriv;              /*  3840   672*/
            /* --- cacheline 70 boundary (4480 bytes) was 32 bytes ago --- * */
            struct security_priv       securitypriv;         /*  4512  4816*/
            /* --- cacheline 145 boundary (9280 bytes) was 48 bytes ago --- * */
            struct registry_priv       registrypriv;         /*  9328   968*/
            /* --- cacheline 160 boundary (10240 bytes) was 56 bytes ago --- * */
            struct wlan_acl_pool       acl_list;             /* 10296  1536*/
            /* --- cacheline 184 boundary (11776 bytes) was 56 bytes ago --- * */
            struct pwrctrl_priv        pwrctrlpriv;          /* 11832   224*/
            /* --- cacheline 188 boundary (12032 bytes) was 24 bytes ago --- * */
            struct eeprom_priv         eeprompriv;           /* 12056   508*/
    
            /* XXX 4 bytes hole, try to pack */
    
            /* --- cacheline 196 boundary (12544 bytes) was 24 bytes ago --- * */
            struct hal_priv            halpriv;              /* 12568    88*/
            /* --- cacheline 197 boundary (12608 bytes) was 48 bytes ago --- * */
            struct led_priv            ledpriv;              /* 12656   304*/
            /* --- cacheline 202 boundary (12928 bytes) was 32 bytes ago --- * */
            struct mp_priv             mppriv;               /* 12960  1080*/
            /* --- cacheline 219 boundary (14016 bytes) was 24 bytes ago --- * */
            s32                        bDriverStopped;       /* 14040     4*/
            s32                        bSurpriseRemoved;     /* 14044     4*/
            u32                        IsrContent;           /* 14048     4*/
            u32                        ImrContent;           /* 14052     4*/
             u8                         EepromAddressSize;    /* 14056     1*/
            u8                         hw_init_completed;    /* 14057     1*/
    
            /* XXX 6 bytes hole, try to pack */
    
            struct task_struct *       cmdThread;            /* 14064     8*/
            pid_t                      evtThread;            /* 14072     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            /* --- cacheline 220 boundary (14080 bytes) --- */
            struct task_struct *       xmitThread;           /* 14080     8*/
            pid_t                      recvThread;           /* 14088     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            uint                       (*dvobj_init)(struct _adapter *); /*14096     8 */
            void                       (*dvobj_deinit)(struct _adapter *);/* 14104     8 */
            struct net_device *        pnetdev;              /* 14112     8*/
            int                        bup;                  /* 14120     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            struct net_device_stats    stats;                /* 14128   184*/
            /* --- cacheline 223 boundary (14272 bytes) was 40 bytes ago --- * */
            struct iw_statistics       iwstats;              /* 14312    32*/
            /* --- cacheline 224 boundary (14336 bytes) was 8 bytes ago --- * */
            int                        pid;                  /* 14344     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            struct work_struct         wkFilterRxFF0;        /* 14352    32*/
            u8                         blnEnableRxFF0Filter; /* 14384     1*/
    
            /* XXX 3 bytes hole, try to pack */
    
            spinlock_t                 lockRxFF0Filter;      /* 14388     4*/
            const struct firmware  *   fw;                   /* 14392     8*/
             u8                         EepromAddressSize;    /* 14056     1*/
            u8                         hw_init_completed;    /* 14057     1*/
    
            /* XXX 6 bytes hole, try to pack */
    
            struct task_struct *       cmdThread;            /* 14064     8*/
            pid_t                      evtThread;            /* 14072     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            /* --- cacheline 220 boundary (14080 bytes) --- */
            struct task_struct *       xmitThread;           /* 14080     8*/
            pid_t                      recvThread;           /* 14088     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            uint                       (*dvobj_init)(struct _adapter *); /*14096     8 */
            void                       (*dvobj_deinit)(struct _adapter *);/* 14104     8 */
            struct net_device *        pnetdev;              /* 14112     8*/
            int                        bup;                  /* 14120     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            struct net_device_stats    stats;                /* 14128   184*/
            /* --- cacheline 223 boundary (14272 bytes) was 40 bytes ago --- * */
            struct iw_statistics       iwstats;              /* 14312    32*/
            /* --- cacheline 224 boundary (14336 bytes) was 8 bytes ago --- * */
            int                        pid;                  /* 14344     4*/
    
            /* XXX 4 bytes hole, try to pack */
    
            struct work_struct         wkFilterRxFF0;        /* 14352    32*/
            u8                         blnEnableRxFF0Filter; /* 14384     1*/
    
            /* XXX 3 bytes hole, try to pack */
    
            spinlock_t                 lockRxFF0Filter;      /* 14388     4*/
            const struct firmware  *   fw;                   /* 14392     8*/
            /* --- cacheline 225 boundary (14400 bytes) --- */
            struct usb_interface *     pusb_intf;            /* 14400     8*/
            struct mutex               mutex_start;          /* 14408    40*/
    
            /* XXX last struct has 4 bytes of padding */
    
            struct completion          rtl8712_fw_ready;     /* 14448    32*/
            /* --- cacheline 226 boundary (14464 bytes) was 16 bytes ago --- * */
    
            /* size: 14480, cachelines: 227, members: 40 */
            /* sum members: 14451, holes: 7, sum holes: 29 */
            /* paddings: 1, sum paddings: 4 */
            /* last cacheline: 16 bytes */
    };
    
    Signed-off-by: Melike Yurtoglu <aysemelikeyurtoglu@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1a118ccfd60fc78e64c0a3ab9e85075545839d6e
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Feb 11 18:20:38 2015 +0800

    f2fs: use spinlock for segmap_lock instead of rwlock
    
    rwlock can provide better concurrency when there are much more readers than
    writers because readers can hold the rwlock simultaneously.
    
    But now, for segmap_lock rwlock in struct free_segmap_info, there is only one
    reader 'mount' from below call path:
    ->f2fs_fill_super
      ->build_segment_manager
        ->build_dirty_segmap
          ->init_dirty_segmap
            ->find_next_inuse
              read_lock
              ...
              read_unlock
    
    Now that our concurrency can not be improved since there is no other reader for
    this lock, we do not need to use rwlock_t type for segmap_lock, let's replace it
    with spinlock_t type.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

commit 536fa402221f09633e7c5801b327055ab716a363
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Sep 5 11:14:48 2014 -0700

    compiler: Allow 1- and 2-byte smp_load_acquire() and smp_store_release()
    
    CPUs without single-byte and double-byte loads and stores place some
    "interesting" requirements on concurrent code.  For example (adapted
    from Peter Hurley's test code), suppose we have the following structure:
    
            struct foo {
                    spinlock_t lock1;
                    spinlock_t lock2;
                    char a; /* Protected by lock1. */
                    char b; /* Protected by lock2. */
            };
            struct foo *foop;
    
    Of course, it is common (and good) practice to place data protected
    by different locks in separate cache lines.  However, if the locks are
    rarely acquired (for example, only in rare error cases), and there are
    a great many instances of the data structure, then memory footprint can
    trump false-sharing concerns, so that it can be better to place them in
    the same cache cache line as above.
    
    But if the CPU does not support single-byte loads and stores, a store
    to foop->a will do a non-atomic read-modify-write operation on foop->b,
    which will come as a nasty surprise to someone holding foop->lock2.  So we
    now require CPUs to support single-byte and double-byte loads and stores.
    Therefore, this commit adjusts the definition of __native_word() to allow
    these sizes to be used by smp_load_acquire() and smp_store_release().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

commit 67298804f34452a53a9ec9e609d95aa35084132b
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Nov 18 11:18:36 2014 +0800

    f2fs: introduce struct inode_management to wrap inner fields
    
    Now in f2fs, we have three inode cache: ORPHAN_INO, APPEND_INO, UPDATE_INO,
    and we manage fields related to inode cache separately in struct f2fs_sb_info
    for each inode cache type.
    This makes codes a bit messy, so that this patch intorduce a new struct
    inode_management to wrap inner fields as following which make codes more neat.
    
    /* for inner inode cache management */
    struct inode_management {
            struct radix_tree_root ino_root;        /* ino entry array */
            spinlock_t ino_lock;                    /* for ino entry lock */
            struct list_head ino_list;              /* inode list head */
            unsigned long ino_num;                  /* number of entries */
    };
    
    struct f2fs_sb_info {
            ...
            struct inode_management im[MAX_INO_ENTRY];      /* manage inode cache */
            ...
    }
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

commit 2359b5c2d950a101af883e51bdbffc1bf83325c4
Author: Aya Mahfouz <mahfouz.saif.elyazal@gmail.com>
Date:   Sat Oct 11 02:42:45 2014 +0200

    staging: vt6655: replace memcpy() by ether_addr_copy() using coccinelle and pack variables
    
    This patch focuses on fixing the following warning generated
    by checkpatch.pl for the file rxtx.c :
    
    Prefer ether_addr_copy() over memcpy() if the Ethernet addresses
    are __aligned(2)
    
    The changes were applied using the following coccinelle rule:
    
    @@ expression e1, e2; @@
    
    - memcpy(e1, e2, ETH_ALEN);
    + ether_addr_copy(e1, e2);
    
    After applying the rule, some referencing operations were
    adjusted manually to avoid a gcc compilation warning.
    
    According to ether_addr_copy() description and functionality,
    all Ethernet addresses should align to the u16 datatype.
    A check was made on the following datastructures:
    
            - vnt_mic_hdr
            - tagS802_11Header
            - tagSEthernetHeader
            - vnt_rts_g
            - ieee80211_rts
            - vnt_private
    
    To maintain that the array abyCurrentNetAddr in vnt_private is aligned,
    it was placed before the boolean bLinkPass in the struct definition.
    The definition can be found in device.h. A couple of variables were
    packed in holes detected by pahole.
    
    Output of pahole when running it on rxtx.o after the changes:
    
    truct tagSEthernetHeader {
            unsigned char          abyDstAddr[6];        /* 0     6 */
            unsigned char          abySrcAddr[6];        /* 6     6 */
            short unsigned int     wType;                /* 12    2 */
    
            /* size: 14, cachelines: 1, members: 3 */
            /* last cacheline: 14 bytes */
    };
    struct tagS802_11Header {
            short unsigned int     wFrameCtl;            /* 0     2 */
            short unsigned int     wDurationID;          /* 2     2 */
            unsigned char          abyAddr1[6];          /* 4     6 */
            unsigned char          abyAddr2[6];          /* 10    6 */
            unsigned char          abyAddr3[6];          /* 16    6 */
            short unsigned int     wSeqCtl;              /* 22    2 */
            unsigned char          abyAddr4[6];          /* 24    6 */
    
            /* size: 30, cachelines: 1, members: 7 */
            /* last cacheline: 30 bytes */
    };
    struct ieee80211_rts {
            __le16                 frame_control;        /* 0     2 */
            __le16                 duration;             /* 2     2 */
            u8                     ra[6];                /* 4     6 */
            u8                     ta[6];                /* 10    6 */
    
            /* size: 16, cachelines: 1, members: 4 */
            /* last cacheline: 16 bytes */
    };
    struct vnt_private {
            struct pci_dev *       pcid;                 /* 0     4 */
            struct net_device *    dev;                  /* 4     4 */
            dma_addr_t             pool_dma;             /* 8     4 */
            dma_addr_t             rd0_pool_dma;         /* 12    4 */
            dma_addr_t             rd1_pool_dma;         /* 16    4 */
            dma_addr_t             td0_pool_dma;         /* 20    4 */
            dma_addr_t             td1_pool_dma;         /* 24    4 */
            dma_addr_t             tx_bufs_dma0;         /* 28    4 */
            dma_addr_t             tx_bufs_dma1;         /* 32    4 */
            dma_addr_t             tx_beacon_dma;        /* 36    4 */
            unsigned char *        tx0_bufs;             /* 40    4 */
            unsigned char *        tx1_bufs;             /* 44    4 */
            unsigned char *        tx_beacon_bufs;       /* 48    4 */
            CHIP_TYPE              chip_id;              /* 52    4 */
            void *                 PortOffset;           /* 56    4 */
            long unsigned int      dwIsr;                /* 60    4 */
            /*--- cacheline 1 boundary (64 bytes) ---*/
            u32                    memaddr;              /* 64    4 */
            u32                    ioaddr;               /* 68    4 */
            u32                    io_size;              /* 72    4 */
            unsigned char          byRevId;              /* 76    1 */
            unsigned char          byRxMode;             /* 77    1 */
            short unsigned int     SubSystemID;          /* 78    2 */
            short unsigned int     SubVendorID;          /* 80    2 */
            spinlock_t             lock;                 /* 82    2 */
            int                    nTxQueues;            /* 84    4 */
            volatile int           iTDUsed;              /* 88    8 */
            volatile PSTxDesc      apCurrTD;             /* 96    8 */
            volatile PSTxDesc      apTailTD;             /* 104   8 */
            volatile PSTxDesc      apTD0Rings;           /* 112   4 */
            volatile PSTxDesc      apTD1Rings;           /* 116   4 */
            volatile PSRxDesc      aRD0Ring;             /* 120   4 */
            volatile PSRxDesc      aRD1Ring;             /* 124   4 */
            /*--- cacheline 2 boundary (128 bytes) ---*/
            volatile PSRxDesc      pCurrRD;              /* 128   8 */
            SCache                 sDupRxCache;          /* 136  44 */
            SDeFragControlBlock    sRxDFCB[64];          /* 180  2048 */
            /*--- cacheline 34 boundary (2176 bytes) was 52 bytes ago ---*/
            unsigned int           cbDFCB;               /* 2228  4 */
            unsigned int           cbFreeDFCB;           /* 2232  4 */
            unsigned int           uCurrentDFCBIdx;      /* 2236  4 */
            /*--- cacheline 35 boundary (2240 bytes) ---*/
            OPTIONS                sOpts;                /* 2240 52 */
            u32                    flags;                /* 2292  4 */
            u32                    rx_buf_sz;            /* 2296  4 */
            int                    multicast_limit;      /* 2300  4 */
            /*--- cacheline 36 boundary (2304 bytes) ---*/
            pid_t                  MLMEThr_pid;          /* 2304  4 */
            struct completion      notify;               /* 2308 16 */
            struct semaphore       mlme_semaphore;       /* 2324 16 */
            u32                    rx_bytes;             /* 2340  4 */
            unsigned char          byLocalID;            /* 2344  1 */
            unsigned char          byRFType;             /* 2345  1 */
            unsigned char          byMaxPwrLevel;        /* 2346  1 */
            unsigned char          byZoneType;           /* 2347  1 */
            bool                   bZoneRegExist;        /* 2348  1 */
            unsigned char          byOriginalZonetype;   /* 2349  1 */
            unsigned char          abyMacContext[384];   /* 2350  384 */
            /*--- cacheline 42 boundary (2688 bytes) was 46 bytes ago ---*/
            unsigned char          abyCurrentNetAddr[6]; /* 2734  6 */
            bool                   bLinkPass;            /* 2740  1 */
    
            /* XXX 3 bytes hole, try to pack */
    
            SStatCounter           scStatistic;          /* 2744  776 */
            /*--- cacheline 55 boundary (3520 bytes) ---*/
            SDot11Counters         s802_11Counter;       /* 3520  172 */
            /*--- cacheline 57 boundary (3648 bytes) was 44 bytes ago ---*/
            PSMgmtObject           pMgmt;                /* 3692  4 */
            SMgmtObject            sMgmtObj;             /* 3696 95840 */
            /*--- cacheline 1555 boundary (99520 bytes) was 16 bytes ago ---*/
    
            /* Bitfield combined with previous fields */
    
            unsigned int           uCurrRSSI;            /* 0    4 */
            unsigned char          byCurrSQ;             /* 0    1 */
            long unsigned int      dwTxAntennaSel;       /* 0    4 */
            long unsigned int      dwRxAntennaSel;       /* 0    4 */
            unsigned char          byAntennaCount;       /* 0    1 */
            unsigned char          byRxAntennaMode;      /* 0    1 */
            unsigned char          byTxAntennaMode;      /* 0    1 */
            bool                   bTxRxAntInv;          /* 0    1 */
            unsigned char *        pbyTmpBuff;           /* 0    4 */
            unsigned int           uSIFS;                /* 0    4 */
            unsigned int           uDIFS;                /* 0    4 */
            unsigned int           uEIFS;                /* 0    4 */
            unsigned int           uSlot;                /* 0    4 */
            unsigned int           uCwMin;               /* 0    4 */
            unsigned int           uCwMax;               /* 0    4 */
            unsigned char          bySIFS;               /* 0    1 */
            unsigned char          byDIFS;               /* 0    1 */
            unsigned char          byEIFS;               /* 0    1 */
            unsigned char          bySlot;               /* 0    1 */
            unsigned char          byCWMaxMin;           /* 0    1 */
            CARD_PHY_TYPE          eCurrentPHYType;      /* 0    4 */
            VIA_BB_TYPE            byBBType;             /* 0    4 */
            VIA_PKT_TYPE           byPacketType;         /* 0    4 */
            short unsigned int     wBasicRate;           /* 0    2 */
            unsigned char          byACKRate;            /* 0    1 */
            unsigned char          byTopOFDMBasicRate;   /* 0    1 */
            unsigned char          byTopCCKBasicRate;    /* 0    1 */
            unsigned char          byMinChannel;         /* 0    1 */
            unsigned char          byMaxChannel;         /* 0    1 */
            unsigned int           uConnectionRate;      /* 0    4 */
            unsigned char          byPreambleType;       /* 0    1 */
            unsigned char          byShortPreamble;      /* 0    1 */
            short unsigned int     wCurrentRate;         /* 0    2 */
            short unsigned int     wRTSThreshold;        /* 0    2 */
            short unsigned int     wFragmentationThreshold; /* 0    2 */
            unsigned char          byShortRetryLimit;    /* 0    1 */
            unsigned char          byLongRetryLimit;     /* 0    1 */
            enum nl80211_iftype    op_mode;              /* 0    4 */
            unsigned char          byOpMode;             /* 0    1 */
            bool                   bBSSIDFilter;         /* 0    1 */
            short unsigned int     wMaxTransmitMSDULifetime; /* 0     2 */
            unsigned char          abyBSSID[6];          /* 0    6 */
            unsigned char          abyDesireBSSID[6];    /* 0    6 */
            short unsigned int     wACKDuration;         /* 0    2 */
            short unsigned int     wRTSTransmitLen;      /* 0    2 */
            unsigned char          byRTSServiceField;    /* 0    1 */
            unsigned char          byRTSSignalField;     /* 0    1 */
            long unsigned int      dwMaxReceiveLifetime; /* 0    4 */
            bool                   bEncryptionEnable;    /* 0    1 */
            bool                   bLongHeader;          /* 0    1 */
            bool                   bShortSlotTime;       /* 0    1 */
            bool                   bProtectMode;         /* 0    1 */
            bool                   bNonERPPresent;       /* 0    1 */
            bool                   bBarkerPreambleMd;    /* 0    1 */
            unsigned char          byERPFlag;            /* 0    1 */
            short unsigned int     wUseProtectCntDown;   /* 0    2 */
            bool                   bRadioControlOff;     /* 0    1 */
            bool                   bRadioOff;            /* 0    1 */
            bool                   bEnablePSMode;        /* 0    1 */
            short unsigned int     wListenInterval;      /* 0    2 */
            bool                   bPWBitOn;             /* 0    1 */
            WMAC_POWER_MODE        ePSMode;              /* 0    4 */
            unsigned char          byRadioCtl;           /* 0    1 */
            unsigned char          byGPIO;               /* 0    1 */
            bool                   bHWRadioOff;          /* 0    1 */
            bool                   bPrvActive4RadioOFF;  /* 0    1 */
            bool                   bGPIOBlockRead;       /* 0    1 */
            short unsigned int     wSeqCounter;          /* 0    2 */
            short unsigned int     wBCNBufLen;           /* 0    2 */
            bool                   bBeaconBufReady;      /* 0    1 */
            bool                   bBeaconSent;          /* 0    1 */
            bool                   bIsBeaconBufReadySet; /* 0    1 */
            unsigned int           cbBeaconBufReadySetCnt; /* 0     4 */
            bool                   bFixRate;             /* 0    1 */
            unsigned char          byCurrentCh;          /* 0    1 */
            unsigned int           uScanTime;            /* 0    4 */
            CMD_STATE              eCommandState;        /* 0    4 */
            CMD_CODE               eCommand;             /* 0    4 */
            bool                   bBeaconTx;            /* 0    1 */
            bool                   bStopBeacon;          /* 0    1 */
            bool                   bStopDataPkt;         /* 0    1 */
            bool                   bStopTx0Pkt;          /* 0    1 */
            unsigned int           uAutoReConnectTime;   /* 0    4 */
            CMD_ITEM               eCmdQueue[32];        /* 0  1408 */
            unsigned int           uCmdDequeueIdx;       /* 0    4 */
            unsigned int           uCmdEnqueueIdx;       /* 0    4 */
            unsigned int           cbFreeCmdQueue;       /* 0    4 */
            bool                   bCmdRunning;          /* 0    1 */
            bool                   bCmdClear;            /* 0    1 */
            bool                   bRoaming;             /* 0    1 */
            unsigned char          abyIPAddr[4];         /* 0    4 */
            long unsigned int      ulTxPower;            /* 0    4 */
            NDIS_802_11_WEP_STATUS eEncryptionStatus;    /* 0    4 */
            bool                   bTransmitKey;         /* 0    1 */
            NDIS_802_11_WEP_STATUS eOldEncryptionStatus; /* 0    4 */
            SKeyManagement         sKey;                 /* 0  3784 */
            long unsigned int      dwIVCounter;          /* 0    4 */
            u64                    qwPacketNumber;       /* 0    8 */
            unsigned int           uCurrentWEPMode;      /* 0    4 */
            RC4Ext                 SBox;                 /* 0  264 */
            unsigned char          abyPRNG[35];          /* 0   35 */
            unsigned char          byKeyIndex;           /* 0    1 */
            unsigned int           uKeyLength;           /* 0    4 */
            unsigned char          abyKey[29];           /* 0   29 */
            bool                   bAES;                 /* 0    1 */
            unsigned char          byCntMeasure;         /* 0    1 */
            unsigned int           uAssocCount;          /* 0    4 */
            bool                   bMoreData;            /* 0    1 */
            bool                   bGrpAckPolicy;        /* 0    1 */
            bool                   bAssocInfoSet;        /* 0    1 */
            unsigned char          byAutoFBCtrl;         /* 0    1 */
            bool                   bTxMICFail;           /* 0    1 */
            bool                   bRxMICFail;           /* 0    1 */
            unsigned int           uRATEIdx;             /* 0    4 */
            bool                   bUpdateBBVGA;         /* 0    1 */
            unsigned int           uBBVGADiffCount;      /* 0    4 */
            unsigned char          byBBVGANew;           /* 0    1 */
            unsigned char          byBBVGACurrent;       /* 0    1 */
            unsigned char          abyBBVGA[4];          /* 0    4 */
            long int               ldBmThreshold[4];     /* 0   16 */
            unsigned char          byBBPreEDRSSI;        /* 0    1 */
            unsigned char          byBBPreEDIndex;       /* 0    1 */
            bool                   bRadioCmd;            /* 0    1 */
            long unsigned int      dwDiagRefCount;       /* 0    4 */
            unsigned char          byFOETuning;          /* 0    1 */
            unsigned char          byAutoPwrTunning;     /* 0    1 */
            short int              sPSetPointCCK;        /* 0    2 */
            short int              sPSetPointOFDMG;      /* 0    2 */
            short int              sPSetPointOFDMA;      /* 0    2 */
            long int               lPFormulaOffset;      /* 0    4 */
            short int              sPThreshold;          /* 0    2 */
            char                   cAdjustStep;          /* 0    1 */
            char                   cMinTxAGC;            /* 0    1 */
            unsigned char          byCCKPwr;             /* 0    1 */
            unsigned char          byOFDMPwrG;           /* 0    1 */
            unsigned char          byCurPwr;             /* 0    1 */
            char                   byCurPwrdBm;          /* 0    1 */
            unsigned char          abyCCKPwrTbl[15];     /* 0   15 */
            unsigned char          abyOFDMPwrTbl[57];    /* 0   57 */
            char                   abyCCKDefaultPwr[15]; /* 0   15 */
            char                   abyOFDMDefaultPwr[57]; /* 0  57 */
            char                   abyRegPwr[57];        /* 0   57 */
            char                   abyLocalPwr[57];      /* 0   57 */
            unsigned char          byBBCR4d;             /* 0    1 */
            unsigned char          byBBCRc9;             /* 0    1 */
            unsigned char          byBBCR88;             /* 0    1 */
            unsigned char          byBBCR09;             /* 0    1 */
            struct timer_list      sTimerCommand;        /* 0   52 */
            struct timer_list      sTimerTxData;         /* 0   52 */
            long unsigned int      nTxDataTimeCout;      /* 0    4 */
            bool                   fTxDataInSleep;       /* 0    1 */
            bool                   IsTxDataTrigger;      /* 0    1 */
            bool                   fWPA_Authened;        /* 0    1 */
            unsigned char          byReAssocCount;       /* 0    1 */
            unsigned char          byLinkWaitCount;      /* 0    1 */
            unsigned char          abyNodeName[17];      /* 0   17 */
            bool                   bDiversityRegCtlON;   /* 0    1 */
            bool                   bDiversityEnable;     /* 0    1 */
            long unsigned int      ulDiversityNValue;    /* 0    4 */
            long unsigned int      ulDiversityMValue;    /* 0    4 */
            unsigned char          byTMax;               /* 0    1 */
            unsigned char          byTMax2;              /* 0    1 */
            unsigned char          byTMax3;              /* 0    1 */
            long unsigned int      ulSQ3TH;              /* 0    4 */
            long unsigned int      uDiversityCnt;        /* 0    4 */
            unsigned char          byAntennaState;       /* 0    1 */
            long unsigned int      ulRatio_State0;       /* 0    4 */
            long unsigned int      ulRatio_State1;       /* 0    4 */
            struct timer_list      TimerSQ3Tmax1;        /* 0   52 */
            struct timer_list      TimerSQ3Tmax2;        /* 0   52 */
            struct timer_list      TimerSQ3Tmax3;        /* 0   52 */
            long unsigned int      uNumSQ3[12];          /* 0   48 */
            short unsigned int     wAntDiversityMaxRate; /* 0    2 */
            SEthernetHeader        sTxEthHeader;         /* 0   14 */
            SEthernetHeader        sRxEthHeader;         /* 0   14 */
            unsigned char          abyBroadcastAddr[6];  /* 0    6 */
            unsigned char          abySNAP_RFC1042[6];   /* 0    6 */
            unsigned char          abySNAP_Bridgetunnel[6]; /* 0     6 */
            unsigned char          abyEEPROM[256];       /* 0   256 */
            SPMKID                 gsPMKID;              /* 0   360 */
            SPMKIDCandidateEvent   gsPMKIDCandidate;     /* 0    72 */
            bool                   b11hEnable;           /* 0     1 */
            unsigned char          abyCountryCode[3];    /* 0     3 */
            unsigned int           uNumOfMeasureEIDs;    /* 0     4 */
            PWLAN_IE_MEASURE_REQ   pCurrMeasureEID;      /* 0     4 */
            bool                   bMeasureInProgress;   /* 0     1 */
            unsigned char          byOrgChannel;         /* 0     1 */
            unsigned char          byOrgRCR;             /* 0     1 */
            long unsigned int      dwOrgMAR0;            /* 0     4 */
            long unsigned int      dwOrgMAR4;            /* 0     4 */
            unsigned char          byBasicMap;           /* 0     1 */
            unsigned char          byCCAFraction;        /* 0     1 */
            unsigned char          abyRPIs[8];           /* 0     8 */
            long unsigned int      dwRPIs[8];            /* 0    32 */
            bool                   bChannelSwitch;       /* 0     1 */
            unsigned char          byNewChannel;         /* 0     1 */
            unsigned char          byChannelSwitchCount; /* 0     1 */
            bool                   bQuietEnable;         /* 0     1 */
            bool                   bEnableFirstQuiet;    /* 0     1 */
            unsigned char          byQuietStartCount;    /* 0     1 */
            unsigned int           uQuietEnqueue;        /* 0     4 */
            long unsigned int      dwCurrentQuietEndTime; /* 0     4 */
            SQuietControl          sQuiet[8];            /* 0    96 */
            bool                   bCountryInfo5G;       /* 0     1 */
            bool                   bCountryInfo24G;      /* 0     1 */
            short unsigned int     wBeaconInterval;      /* 0     2 */
            struct net_device *    wpadev;               /* 0     4 */
            bool                   bWPADEVUp;            /* 0     1 */
            struct sk_buff *       skb;                  /* 0     4 */
            unsigned int           bwextcount;           /* 0     4 */
            bool                   bWPASuppWextEnabled;  /* 0     1 */
            bool                   bEnableHostapd;       /* 0     1 */
            bool                   bEnable8021x;         /* 0     1 */
            bool                   bEnableHostWEP;       /* 0     1 */
            struct net_device *    apdev;                /* 0     4 */
            int                    (*tx_80211)(struct sk_buff *,
                                     struct net_device *); /* 0     4 */
            unsigned int           uChannel;             /* 0     4 */
            bool                   bMACSuspend;          /* 0     1 */
            struct iw_statistics   wstats;               /* 0    32 */
            bool                   bCommit;              /* 0     1 */
            /*--- cacheline 1672 boundary (107008 bytes) ---*/
    
            /* size: 107008, cachelines: 1672, members: 279 */
            /* sum members: 107005, holes: 1, sum holes: 3 */
            /* padding: 41471 */
    
            /* BRAIN FART ALERT! 107008 != 107005 + 3(holes), diff = 0 */
    
    };
    struct vnt_mic_hdr {
            u8                     id;                   /* 0     1 */
            u8                     tx_priority;          /* 1     1 */
            u8                     mic_addr2[6];         /* 2     6 */
            u8                     ccmp_pn[6];           /* 8     6 */
            __be16                 payload_len;          /* 14    2 */
            __be16                 hlen;                 /* 16    2 */
            __le16                 frame_control;        /* 18    2 */
            u8                     addr1[6];             /* 20    6 */
            u8                     addr2[6];             /* 26    6 */
            u8                     addr3[6];             /* 32    6 */
            __le16                 seq_ctrl;             /* 38    2 */
            u8                     addr4[6];             /* 40    6 */
            u16                    packing;              /* 46    2 */
    
            /* size: 48, cachelines: 1, members: 13 */
            /* last cacheline: 48 bytes */
    };
    struct vnt_rts_g {
            struct vnt_phy_field   b;                    /*  0    4 */
            struct vnt_phy_field   a;                    /*  4    4 */
            __le16                 duration_ba;          /*  8    2 */
            __le16                 duration_aa;          /* 10    2 */
            __le16                 duration_bb;          /* 12    2 */
            u16                    reserved;             /* 14    2 */
            struct ieee80211_rts   data;                 /* 16   16 */
    
            /* size: 32, cachelines: 1, members: 7 */
            /* last cacheline: 32 bytes */
    };
    
    Signed-off-by: Aya Mahfouz <mahfouz.saif.elyazal@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0d36882013e61dc47aa835c4ec98b1fe776c9db8
Author: Manish Chopra <manish.chopra@qlogic.com>
Date:   Tue Sep 30 03:56:35 2014 -0400

    netxen: Fix BUG "sleeping function called from invalid context"
    
    o __netxen_nic_down() function might sleep while holding spinlock_t(tx_clean_lock).
      Acquire this lock for only releasing TX buffers instead of taking it
      for whole down path.
    
    Reported-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Manish Chopra <manish.chopra@qlogic.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 2ff810a7ef38b55ba6c7b80bb7ff22847fd3be69
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Thu Aug 14 13:27:30 2014 -0400

    locking/rwlock, x86: Clean up asm/spinlock*.h to remove old rwlock code
    
    As the x86 architecture now uses qrwlock for its read/write lock
    implementation, it is no longer necessary to keep the old rwlock code
    around. This patch removes the old rwlock code in the asm/spinlock.h
    and asm/spinlock_types.h files. Now the ARCH_USE_QUEUE_RWLOCK
    config parameter cannot be removed from x86/Kconfig or there will be
    a compilation error.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Link: http://lkml.kernel.org/r/1408037251-45918-2-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 71e70e9f6daea58bb5ef03d17ba26c630fcac1d2
Author: Ian Abbott <abbotti@mev.co.uk>
Date:   Thu Jul 31 14:47:42 2014 +0100

    staging: comedi: amplc_pci224: fix spinlock_t definition without comment
    
    Fix checkpatch issue: "CHECK: spinlock_t definition without comment".
    
    Signed-off-by: Ian Abbott <abbotti@mev.co.uk>
    Reviewed-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 70af2f8a4f48d6cebdf92d533d3aef37853ce6de
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Mon Feb 3 13:18:49 2014 +0100

    locking/rwlocks: Introduce 'qrwlocks' - fair, queued rwlocks
    
    This rwlock uses the arch_spin_lock_t as a waitqueue, and assuming the
    arch_spin_lock_t is a fair lock (ticket,mcs etc..) the resulting
    rwlock is a fair lock.
    
    It fits in the same 8 bytes as the regular rwlock_t by folding the
    reader and writer count into a single integer, using the remaining 4
    bytes for the arch_spinlock_t.
    
    Architectures that can single-copy adress bytes can optimize
    queue_write_unlock() with a 0 write to the LSB (the write count).
    
    Performance as measured by Davidlohr Bueso (rwlock_t -> qrwlock_t):
    
     +--------------+-------------+---------------+
     |   Workload   |   #users    |     delta     |
     +--------------+-------------+---------------+
     | alltests     | > 1400      | -4.83%        |
     | custom       | 0-100,> 100 | +1.43%,-1.57% |
     | high_systime | > 1000      | -2.61         |
     | shared       | all         | +0.32         |
     +--------------+-------------+---------------+
    
    http://www.stgolabs.net/qrwlock-stuff/aim7-results-vs-rwsem_optsin/
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    [peterz: near complete rewrite]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "Paul E.McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/n/tip-gac1nnl3wvs2ij87zv2xkdzq@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9fdbb461747550c933b8596c71ce1d9090feb1a5
Author: navin patidar <navin.patidar@gmail.com>
Date:   Wed May 7 09:27:26 2014 +0530

    staging: rtl8188eu: Remove 'spinlock_t lock' from struct recv_priv
    
    Remove unused variable 'spinlock_t lock'.
    
    Signed-off-by: navin patidar <navin.patidar@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5b3f683e694a835f5dfdab06102be1a50604c3b7
Author: Philipp Hachtmann <phacht@linux.vnet.ibm.com>
Date:   Mon Apr 7 18:25:23 2014 +0200

    s390/spinlock: cleanup spinlock code
    
    Improve the spinlock code in several aspects:
     - Have _raw_compare_and_swap return true if the operation has been
       successful instead of returning the old value.
     - Remove the "volatile" from arch_spinlock_t and arch_rwlock_t
     - Rename 'owner_cpu' to 'lock'
     - Add helper functions arch_spin_trylock_once / arch_spin_tryrelease_once
    
    [ Martin Schwidefsky: patch breakdown and code beautification ]
    
    Signed-off-by: Philipp Hachtmann <phacht@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit 4337ff2f8fed9230af7acb447f8c1e5a82087d97
Author: navin patidar <navin.patidar@gmail.com>
Date:   Sat May 3 17:15:22 2014 +0530

    staging: rtl8188eu: Remove 'spinlock_t recvbuf_lock' from struct recv_buf
    
    recvbuf_lock is initialized inside rtl8188eu_init_recv_priv() but never used.
    
    Signed-off-by: navin patidar <navin.patidar@gmail.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e5ac6eafba887821044c65b6fe59d9eb8b7c7f61
Author: Florian Westphal <fw@strlen.de>
Date:   Mon Mar 17 22:27:50 2014 +0100

    netfilter: connlimit: fix UP build
    
    cannot use ARRAY_SIZE() if spinlock_t is empty struct.
    
    Fixes: 1442e7507dd597 ("netfilter: connlimit: use keyed locks")
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

commit d5d20912d33f13766902a27087323f5c94e831c8
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Mar 17 13:37:53 2014 -0700

    netfilter: conntrack: Fix UP builds
    
    ARRAY_SIZE(nf_conntrack_locks) is undefined if spinlock_t is an
    empty structure. Replace it by CONNTRACK_LOCKS
    
    Fixes: 93bb0ceb75be ("netfilter: conntrack: remove central spinlock nf_conntrack_lock")
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Jesper Dangaard Brouer <brouer@redhat.com>
    Cc: Pablo Neira Ayuso <pablo@netfilter.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 6e4ae926d86530ec2a8bc462b97023022c8dbfc5
Author: Li Zhong <zhong@linux.vnet.ibm.com>
Date:   Tue Jan 28 17:52:42 2014 +0530

    powerpc/mm: Fix compile error of pgtable-ppc64.h
    
    commit fd120dc2e205d2318a8b47d6d8098b789e3af67d upstream.
    
    It seems that forward declaration couldn't work well with typedef, use
    struct spinlock directly to avoiding following build errors:
    
    In file included from include/linux/spinlock.h:81,
                     from include/linux/seqlock.h:35,
                     from include/linux/time.h:5,
                     from include/uapi/linux/timex.h:56,
                     from include/linux/timex.h:56,
                     from include/linux/sched.h:17,
                     from arch/powerpc/kernel/asm-offsets.c:17:
    include/linux/spinlock_types.h:76: error: redefinition of typedef 'spinlock_t'
    /root/linux-next/arch/powerpc/include/asm/pgtable-ppc64.h:563: note: previous declaration of 'spinlock_t' was here
    
    Signed-off-by: Li Zhong <zhong@linux.vnet.ibm.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fd120dc2e205d2318a8b47d6d8098b789e3af67d
Author: Li Zhong <zhong@linux.vnet.ibm.com>
Date:   Tue Jan 28 17:52:42 2014 +0530

    powerpc/mm: Fix compile error of pgtable-ppc64.h
    
    It seems that forward declaration couldn't work well with typedef, use
    struct spinlock directly to avoiding following build errors:
    
    In file included from include/linux/spinlock.h:81,
                     from include/linux/seqlock.h:35,
                     from include/linux/time.h:5,
                     from include/uapi/linux/timex.h:56,
                     from include/linux/timex.h:56,
                     from include/linux/sched.h:17,
                     from arch/powerpc/kernel/asm-offsets.c:17:
    include/linux/spinlock_types.h:76: error: redefinition of typedef 'spinlock_t'
    /root/linux-next/arch/powerpc/include/asm/pgtable-ppc64.h:563: note: previous declaration of 'spinlock_t' was here
    
    Signed-off-by: Li Zhong <zhong@linux.vnet.ibm.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit b30afea019a4548ee77b73e83f03104e0e3a0556
Author: Shawn Guo <shawn.guo@linaro.org>
Date:   Thu Jan 23 15:53:18 2014 -0800

    include/linux/genalloc.h: spinlock_t needs spinlock_types.h
    
    Compiling a C file which includes genalloc.h but without
    spinlock_types.h being included before, we will see the compile error
    below.
    
     include/linux/genalloc.h:54:2: error: unknown type name `spinlock_t'
    
    Include spinlock_types.h from genalloc.h to fix the problem.
    
    Signed-off-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b35f1819acd9243a3ff7ad25b1fa8bd6bfe80fb2
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Tue Jan 21 15:49:07 2014 -0800

    mm: create a separate slab for page->ptl allocation
    
    If DEBUG_SPINLOCK and DEBUG_LOCK_ALLOC are enabled spinlock_t on x86_64
    is 72 bytes.  For page->ptl they will be allocated from kmalloc-96 slab,
    so we loose 24 on each.  An average system can easily allocate few tens
    thousands of page->ptl and overhead is significant.
    
    Let's create a separate slab for page->ptl allocation to solve this.
    
    To make sure that it really works this time, some numbers from my test
    machine (just booted, no load):
    
    Before:
      # grep '^\(kmalloc-96\|page->ptl\)' /proc/slabinfo
      kmalloc-96         31987  32190    128   30    1 : tunables  120   60    8 : slabdata   1073   1073     92
    After:
      # grep '^\(kmalloc-96\|page->ptl\)' /proc/slabinfo
      page->ptl          27516  28143     72   53    1 : tunables  120   60    8 : slabdata    531    531      9
      kmalloc-96          3853   5280    128   30    1 : tunables  120   60    8 : slabdata    176    176      0
    
    Note that the patch is useful not only for debug case, but also for
    PREEMPT_RT, where spinlock_t is always bloated.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit e90b8417af0d01cf8c64da6937c914c89ccf6dc1
Merge: 845c071b7853 413541dd66d5
Author: Felipe Balbi <balbi@ti.com>
Date:   Mon Dec 23 11:22:46 2013 -0600

    Merge tag 'v3.13-rc5' into next
    
    Linux 3.13-rc5
    
    * tag 'v3.13-rc5': (231 commits)
      Linux 3.13-rc5
      aio: clean up and fix aio_setup_ring page mapping
      aio/migratepages: make aio migrate pages sane
      aio: fix kioctx leak introduced by "aio: Fix a trinity splat"
      Don't set the INITRD_COMPRESS environment variable automatically
      mm: fix build of split ptlock code
      pstore: Don't allow high traffic options on fragile devices
      mm: do not allocate page->ptl dynamically, if spinlock_t fits to long
      mm: page_alloc: revert NUMA aspect of fair allocation policy
      Revert "mm: page_alloc: exclude unreclaimable allocations from zone fairness policy"
      mm: Fix NULL pointer dereference in madvise(MADV_WILLNEED) support
      qla2xxx: Fix scsi_host leak on qlt_lport_register callback failure
      target: Remove extra percpu_ref_init
      arm64: ptrace: avoid using HW_BREAKPOINT_EMPTY for disabled events
      ARC: Allow conditional multiple inclusion of uapi/asm/unistd.h
      target/file: Update hw_max_sectors based on current block_size
      iser-target: Move INIT_WORK setup into isert_create_device_ib_res
      iscsi-target: Fix incorrect np->np_thread NULL assignment
      mm/hugetlb: check for pte NULL pointer in __page_check_address()
      fix build with make 3.80
      ...
    
    Conflicts:
            drivers/usb/phy/Kconfig

commit 40b64acd17a2200579db265048b4a51998a84729
Author: Olof Johansson <olof@lixom.net>
Date:   Fri Dec 20 14:28:05 2013 -0800

    mm: fix build of split ptlock code
    
    Commit 597d795a2a78 ('mm: do not allocate page->ptl dynamically, if
    spinlock_t fits to long') restructures some allocators that are compiled
    even if USE_SPLIT_PTLOCKS arn't used.  It results in compilation
    failure:
    
      mm/memory.c:4282:6: error: 'struct page' has no member named 'ptl'
      mm/memory.c:4288:12: error: 'struct page' has no member named 'ptl'
    
    Add in the missing ifdef.
    
    Fixes: 597d795a2a78 ('mm: do not allocate page->ptl dynamically, if spinlock_t fits to long')
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 597d795a2a786d22dd872332428e2b9439ede639
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Dec 20 13:35:58 2013 +0200

    mm: do not allocate page->ptl dynamically, if spinlock_t fits to long
    
    In struct page we have enough space to fit long-size page->ptl there,
    but we use dynamically-allocated page->ptl if size(spinlock_t) is larger
    than sizeof(int).
    
    It hurts 64-bit architectures with CONFIG_GENERIC_LOCKBREAK, where
    sizeof(spinlock_t) == 8, but it easily fits into struct page.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 83d55bd03f8a37e3a8ba4d24da2a274231a485e5
Author: H Hartley Sweeten <hsweeten@visionengravers.com>
Date:   Mon Dec 9 15:30:58 2013 -0700

    staging: comedi: pcmmio: rename 'spinlock' in the private data
    
    This spinlock_t is meant to protect the page registers in the asic.
    Rename it to make this clear.
    
    Signed-off-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Reviewed-by: Ian Abbott <abbotti@mev.co.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 25ad998892c05cf1a3f14ac882af8edbb8f62cc8
Author: H Hartley Sweeten <hsweeten@visionengravers.com>
Date:   Thu Dec 5 16:54:11 2013 -0700

    staging: comedi: pcmuio: document the spinlock_t variables
    
    Add some comments about the two spinlock_t variables in the private
    data. Also, add come comments for the functions that do not need to
    lock/unlock the spinlock.
    
    Signed-off-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Cc: Ian Abbott <abbotti@mev.co.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2a46eed54a28c1e3de701ca4237ce4f8bebf14c6
Author: Al Viro <viro@ZenIV.linux.org.uk>
Date:   Wed Nov 20 22:16:36 2013 +0000

    Wrong page freed on preallocate_pmds() failure exit
    
    Note that pmds[i] is simply uninitialized at that point...
    
    Granted, it's very hard to hit (you need split page locks *and*
    kmalloc(sizeof(spinlock_t), GFP_KERNEL) failing), but the code is
    obviously bogus.
    
    Introduced by commit 09ef4939850a ("x86: add missed
    pgtable_pmd_page_ctor/dtor calls for preallocated pmds")
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 24b9fdc59ac365e7e313e5af44c7bfe31c15c774
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Mon Nov 18 10:47:27 2013 +0200

    kernel/bounds: avoid circular dependencies in generated headers
    
    <linux/spinlock.h> has heavy dependencies on other header files.
    It triggers circular dependencies in generated headers on IA64, at
    least:
    
      CC      kernel/bounds.s
    In file included from /home/space/kas/git/public/linux/arch/ia64/include/asm/thread_info.h:9:0,
                     from include/linux/thread_info.h:54,
                     from include/asm-generic/preempt.h:4,
                     from arch/ia64/include/generated/asm/preempt.h:1,
                     from include/linux/preempt.h:18,
                     from include/linux/spinlock.h:50,
                     from kernel/bounds.c:14:
    /home/space/kas/git/public/linux/arch/ia64/include/asm/asm-offsets.h:1:35: fatal error: generated/asm-offsets.h: No such file or directory
    compilation terminated.
    
    Let's replace <linux/spinlock.h> with <linux/spinlock_types.h>, it's
    enough to find out size of spinlock_t.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reported-and-Tested-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>

commit 57f4257eae33e036125973858934730250d464e3
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 14 14:31:54 2013 -0800

    lockref: use BLOATED_SPINLOCKS to avoid explicit config dependencies
    
    Avoid the fragile Kconfig construct guestimating spinlock_t sizes; use a
    friendly compile-time test to determine this.
    
    [kirill.shutemov@linux.intel.com: drop CONFIG_CMPXCHG_LOCKREF]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ea1e7ed33708c7a760419ff9ded0a6cb90586a50
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Nov 14 14:31:53 2013 -0800

    mm: create a separate slab for page->ptl allocation
    
    If DEBUG_SPINLOCK and DEBUG_LOCK_ALLOC are enabled spinlock_t on x86_64
    is 72 bytes.  For page->ptl they will be allocated from kmalloc-96 slab,
    so we loose 24 on each.  An average system can easily allocate few tens
    thousands of page->ptl and overhead is significant.
    
    Let's create a separate slab for page->ptl allocation to solve this.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 539edb5846c740d78a8b6c2e43a99ca4323df68f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 14 14:31:52 2013 -0800

    mm: properly separate the bloated ptl from the regular case
    
    Use kernel/bounds.c to convert build-time spinlock_t size check into a
    preprocessor symbol and apply that to properly separate the page::ptl
    situation.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 49076ec2ccaf68610aa03d96bced9a6694b93ca1
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Nov 14 14:31:51 2013 -0800

    mm: dynamically allocate page->ptl if it cannot be embedded to struct page
    
    If split page table lock is in use, we embed the lock into struct page
    of table's page.  We have to disable split lock, if spinlock_t is too
    big be to be embedded, like when DEBUG_SPINLOCK or DEBUG_LOCK_ALLOC
    enabled.
    
    This patch add support for dynamic allocation of split page table lock
    if we can't embed it to struct page.
    
    page->ptl is unsigned long now and we use it as spinlock_t if
    sizeof(spinlock_t) <= sizeof(long), otherwise it's pointer to spinlock_t.
    
    The spinlock_t allocated in pgtable_page_ctor() for PTE table and in
    pgtable_pmd_page_ctor() for PMD table.  All other helpers converted to
    support dynamically allocated page->ptl.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reviewed-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 09ef4939850aa81e3822b5dfb9ba2ada5e565816
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Nov 14 14:31:13 2013 -0800

    x86: add missed pgtable_pmd_page_ctor/dtor calls for preallocated pmds
    
    In split page table lock case, we embed spinlock_t into struct page.
    For obvious reason, we don't want to increase size of struct page if
    spinlock_t is too big, like with DEBUG_SPINLOCK or DEBUG_LOCK_ALLOC or
    on -rt kernel.  So we disable split page table lock, if spinlock_t is
    too big.
    
    This patchset allows to allocate the lock dynamically if spinlock_t is
    big.  In this page->ptl is used to store pointer to spinlock instead of
    spinlock itself.  It costs additional cache line for indirect access,
    but fix page fault scalability for multi-threaded applications.
    
    LOCK_STAT depends on DEBUG_SPINLOCK, so on current kernel enabling
    LOCK_STAT to analyse scalability issues breaks scalability.  ;)
    
    The patchset mostly fixes this.  Results for ./thp_memscale -c 80 -b 512M
    on 4-socket machine:
    
    baseline, no CONFIG_LOCK_STAT:  9.115460703 seconds time elapsed
    baseline, CONFIG_LOCK_STAT=y:   53.890567123 seconds time elapsed
    patched, no CONFIG_LOCK_STAT:   8.852250368 seconds time elapsed
    patched, CONFIG_LOCK_STAT=y:    11.069770759 seconds time elapsed
    
    Patch count is scary, but most of them trivial. Overview:
    
     Patches 1-4    Few bug fixes. No dependencies to other patches.
                    Probably should applied as soon as possible.
    
     Patch 5        Changes signature of pgtable_page_ctor(). We will use it
                    for dynamic lock allocation, so it can fail.
    
     Patches 6-8    Add missing constructor/destructor calls on few archs.
                    It's fixes NR_PAGETABLE accounting and prepare to use
                    split ptl.
    
     Patches 9-33   Add pgtable_page_ctor() fail handling to all archs.
    
     Patches 34     Finally adds support of dynamically-allocated page->pte.
                    Also contains documentation for split page table lock.
    
    This patch (of 34):
    
    I've missed that we preallocate few pmds on pgd_alloc() if X86_PAE
    enabled.  Let's add missed constructor/destructor calls.
    
    I haven't noticed it during testing since prep_new_page() clears
    page->mapping and therefore page->ptl.  It's effectively equal to
    spin_lock_init(&page->ptl).
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Liqin <liqin.chen@sunplusct.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Grant Likely <grant.likely@linaro.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rob Herring <rob.herring@calxeda.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit e009bb30c8df8a52a9622b616b67436b6a03a0cd
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Nov 14 14:31:07 2013 -0800

    mm: implement split page table lock for PMD level
    
    The basic idea is the same as with PTE level: the lock is embedded into
    struct page of table's page.
    
    We can't use mm->pmd_huge_pte to store pgtables for THP, since we don't
    take mm->page_table_lock anymore.  Let's reuse page->lru of table's page
    for that.
    
    pgtable_pmd_page_ctor() returns true, if initialization is successful
    and false otherwise.  Current implementation never fails, but assumption
    that constructor can fail will help to port it to -rt where spinlock_t
    is rather huge and cannot be embedded into struct page -- dynamic
    allocation is required.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Tested-by: Alex Thorlton <athorlton@sgi.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "Eric W . Biederman" <ebiederm@xmission.com>
    Cc: "Paul E . McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Robin Holt <robinmholt@gmail.com>
    Cc: Sedat Dilek <sedat.dilek@gmail.com>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Hugh Dickins <hughd@google.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit e9bb18c7b95d4dcf8c7f0e14f920ca6f03109e75
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Nov 14 14:30:42 2013 -0800

    mm: avoid increase sizeof(struct page) due to split page table lock
    
    Alex Thorlton noticed that some massively threaded workloads work poorly,
    if THP enabled.  This patchset fixes this by introducing split page table
    lock for PMD tables.  hugetlbfs is not covered yet.
    
    This patchset is based on work by Naoya Horiguchi.
    
    : akpm result summary:
    :
    : THP off, v3.12-rc2: 18.059261877 seconds time elapsed
    : THP off, patched:   16.768027318 seconds time elapsed
    :
    : THP on, v3.12-rc2:  42.162306788 seconds time elapsed
    : THP on, patched:    8.397885779 seconds time elapsed
    :
    : HUGETLB, v3.12-rc2: 47.574936948 seconds time elapsed
    : HUGETLB, patched:   19.447481153 seconds time elapsed
    
    THP off, v3.12-rc2:
    -------------------
    
     Performance counter stats for './thp_memscale -c 80 -b 512m' (5 runs):
    
        1037072.835207 task-clock                #   57.426 CPUs utilized            ( +-  3.59% )
                95,093 context-switches          #    0.092 K/sec                    ( +-  3.93% )
                   140 cpu-migrations            #    0.000 K/sec                    ( +-  5.28% )
            10,000,550 page-faults               #    0.010 M/sec                    ( +-  0.00% )
     2,455,210,400,261 cycles                    #    2.367 GHz                      ( +-  3.62% ) [83.33%]
     2,429,281,882,056 stalled-cycles-frontend   #   98.94% frontend cycles idle     ( +-  3.67% ) [83.33%]
     1,975,960,019,659 stalled-cycles-backend    #   80.48% backend  cycles idle     ( +-  3.88% ) [66.68%]
        46,503,296,013 instructions              #    0.02  insns per cycle
                                                 #   52.24  stalled cycles per insn  ( +-  3.21% ) [83.34%]
         9,278,997,542 branches                  #    8.947 M/sec                    ( +-  4.00% ) [83.34%]
            89,881,640 branch-misses             #    0.97% of all branches          ( +-  1.17% ) [83.33%]
    
          18.059261877 seconds time elapsed                                          ( +-  2.65% )
    
    THP on, v3.12-rc2:
    ------------------
    
     Performance counter stats for './thp_memscale -c 80 -b 512m' (5 runs):
    
        3114745.395974 task-clock                #   73.875 CPUs utilized            ( +-  1.84% )
               267,356 context-switches          #    0.086 K/sec                    ( +-  1.84% )
                    99 cpu-migrations            #    0.000 K/sec                    ( +-  1.40% )
                58,313 page-faults               #    0.019 K/sec                    ( +-  0.28% )
     7,416,635,817,510 cycles                    #    2.381 GHz                      ( +-  1.83% ) [83.33%]
     7,342,619,196,993 stalled-cycles-frontend   #   99.00% frontend cycles idle     ( +-  1.88% ) [83.33%]
     6,267,671,641,967 stalled-cycles-backend    #   84.51% backend  cycles idle     ( +-  2.03% ) [66.67%]
       117,819,935,165 instructions              #    0.02  insns per cycle
                                                 #   62.32  stalled cycles per insn  ( +-  4.39% ) [83.34%]
        28,899,314,777 branches                  #    9.278 M/sec                    ( +-  4.48% ) [83.34%]
            71,787,032 branch-misses             #    0.25% of all branches          ( +-  1.03% ) [83.33%]
    
          42.162306788 seconds time elapsed                                          ( +-  1.73% )
    
    HUGETLB, v3.12-rc2:
    -------------------
    
     Performance counter stats for './thp_memscale_hugetlbfs -c 80 -b 512M' (5 runs):
    
        2588052.787264 task-clock                #   54.400 CPUs utilized            ( +-  3.69% )
               246,831 context-switches          #    0.095 K/sec                    ( +-  4.15% )
                   138 cpu-migrations            #    0.000 K/sec                    ( +-  5.30% )
                21,027 page-faults               #    0.008 K/sec                    ( +-  0.01% )
     6,166,666,307,263 cycles                    #    2.383 GHz                      ( +-  3.68% ) [83.33%]
     6,086,008,929,407 stalled-cycles-frontend   #   98.69% frontend cycles idle     ( +-  3.77% ) [83.33%]
     5,087,874,435,481 stalled-cycles-backend    #   82.51% backend  cycles idle     ( +-  4.41% ) [66.67%]
       133,782,831,249 instructions              #    0.02  insns per cycle
                                                 #   45.49  stalled cycles per insn  ( +-  4.30% ) [83.34%]
        34,026,870,541 branches                  #   13.148 M/sec                    ( +-  4.24% ) [83.34%]
            68,670,942 branch-misses             #    0.20% of all branches          ( +-  3.26% ) [83.33%]
    
          47.574936948 seconds time elapsed                                          ( +-  2.09% )
    
    THP off, patched:
    -----------------
    
     Performance counter stats for './thp_memscale -c 80 -b 512m' (5 runs):
    
         943301.957892 task-clock                #   56.256 CPUs utilized            ( +-  3.01% )
                86,218 context-switches          #    0.091 K/sec                    ( +-  3.17% )
                   121 cpu-migrations            #    0.000 K/sec                    ( +-  6.64% )
            10,000,551 page-faults               #    0.011 M/sec                    ( +-  0.00% )
     2,230,462,457,654 cycles                    #    2.365 GHz                      ( +-  3.04% ) [83.32%]
     2,204,616,385,805 stalled-cycles-frontend   #   98.84% frontend cycles idle     ( +-  3.09% ) [83.32%]
     1,778,640,046,926 stalled-cycles-backend    #   79.74% backend  cycles idle     ( +-  3.47% ) [66.69%]
        45,995,472,617 instructions              #    0.02  insns per cycle
                                                 #   47.93  stalled cycles per insn  ( +-  2.51% ) [83.34%]
         9,179,700,174 branches                  #    9.731 M/sec                    ( +-  3.04% ) [83.35%]
            89,166,529 branch-misses             #    0.97% of all branches          ( +-  1.45% ) [83.33%]
    
          16.768027318 seconds time elapsed                                          ( +-  2.47% )
    
    THP on, patched:
    ----------------
    
     Performance counter stats for './thp_memscale -c 80 -b 512m' (5 runs):
    
         458793.837905 task-clock                #   54.632 CPUs utilized            ( +-  0.79% )
                41,831 context-switches          #    0.091 K/sec                    ( +-  0.97% )
                    98 cpu-migrations            #    0.000 K/sec                    ( +-  1.66% )
                57,829 page-faults               #    0.126 K/sec                    ( +-  0.62% )
     1,077,543,336,716 cycles                    #    2.349 GHz                      ( +-  0.81% ) [83.33%]
     1,067,403,802,964 stalled-cycles-frontend   #   99.06% frontend cycles idle     ( +-  0.87% ) [83.33%]
       864,764,616,143 stalled-cycles-backend    #   80.25% backend  cycles idle     ( +-  0.73% ) [66.68%]
        16,129,177,440 instructions              #    0.01  insns per cycle
                                                 #   66.18  stalled cycles per insn  ( +-  7.94% ) [83.35%]
         3,618,938,569 branches                  #    7.888 M/sec                    ( +-  8.46% ) [83.36%]
            33,242,032 branch-misses             #    0.92% of all branches          ( +-  2.02% ) [83.32%]
    
           8.397885779 seconds time elapsed                                          ( +-  0.18% )
    
    HUGETLB, patched:
    -----------------
    
     Performance counter stats for './thp_memscale_hugetlbfs -c 80 -b 512M' (5 runs):
    
         395353.076837 task-clock                #   20.329 CPUs utilized            ( +-  8.16% )
                55,730 context-switches          #    0.141 K/sec                    ( +-  5.31% )
                   138 cpu-migrations            #    0.000 K/sec                    ( +-  4.24% )
                21,027 page-faults               #    0.053 K/sec                    ( +-  0.00% )
       930,219,717,244 cycles                    #    2.353 GHz                      ( +-  8.21% ) [83.32%]
       914,295,694,103 stalled-cycles-frontend   #   98.29% frontend cycles idle     ( +-  8.35% ) [83.33%]
       704,137,950,187 stalled-cycles-backend    #   75.70% backend  cycles idle     ( +-  9.16% ) [66.69%]
        30,541,538,385 instructions              #    0.03  insns per cycle
                                                 #   29.94  stalled cycles per insn  ( +-  3.98% ) [83.35%]
         8,415,376,631 branches                  #   21.286 M/sec                    ( +-  3.61% ) [83.36%]
            32,645,478 branch-misses             #    0.39% of all branches          ( +-  3.41% ) [83.32%]
    
          19.447481153 seconds time elapsed                                          ( +-  2.00% )
    
    This patch (of 11):
    
    CONFIG_GENERIC_LOCKBREAK increases sizeof(spinlock_t) to 8 bytes.  It
    leads to increase sizeof(struct page) by 4 bytes on 32-bit system if split
    page table lock is in use, since page->ptl shares space in union with
    longs and pointers.
    
    Let's disable split page table lock on 32-bit systems with
    GENERIC_LOCKBREAK enabled.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: "Eric W . Biederman" <ebiederm@xmission.com>
    Cc: "Paul E . McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Robin Holt <robinmholt@gmail.com>
    Cc: Sedat Dilek <sedat.dilek@gmail.com>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ab3cb2e30ec8223777f6ea4696ba24191ffc5c72
Author: Ian Abbott <abbotti@mev.co.uk>
Date:   Fri Nov 8 15:03:23 2013 +0000

    staging: comedi: add a couple of #includes to comedidev.h
    
    Two structures defined in "comedidev.h" have an element of type
    `spinlock_t`, so add `#include <linux/spinlock_types.h>` to declare it.
    One structure has an element of type `struct mutex` so add `#include
    <linux/mutex.h>` to declare it.
    
    Signed-off-by: Ian Abbott <abbotti@mev.co.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 05ad391dbc2bd27b8d868cf9c3ec1b68a2126a16
Merge: 8b5baa460b69 67317c268956
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 11 16:32:21 2013 +0900

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64
    
    Pull ARM64 update from Catalin Marinas:
     "Main features:
       - Ticket-based spinlock implementation and lockless lockref support
       - Big endian support
       - CPU hotplug support, currently for PSCI (Power State Coordination
         Interface) capable firmware
       - Virtual address space extended to 42-bit in the 64K page
         configuration (maximum VA space with 2 levels of page tables)
       - Compat (AArch32) kuser helpers updated to ARMv8 (make use of
         load-acquire/store-release instructions)
       - Code cleanup, defconfig update and minor fixes"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64: (43 commits)
      ARM64: /proc/interrupts: display IPIs of online CPUs only
      arm64: locks: Remove CONFIG_GENERIC_LOCKBREAK
      arm64: KVM: vgic: byteswap GICv2 access on world switch if BE
      arm64: KVM: initialize HYP mode following the kernel endianness
      arm64: compat: Clear the IT state independent of the 32-bit ARM or Thumb-2 mode
      arm64: Use 42-bit address space with 64K pages
      arm64: module: ensure instruction is little-endian before manipulation
      arm64: defconfig: Enable CONFIG_PREEMPT by default
      arm64: fix access to preempt_count from assembly code
      arm64: move enabling of GIC before CPUs are set online
      arm64: use generic RW_DATA_SECTION macro in linker script
      arm64: Slightly improve the warning on CPU0 enable-method
      ARM64: simplify cpu_read_bootcpu_ops using OF/DT helper
      ARM64: DT: define ARM64 specific arch_match_cpu_phys_id
      arm64: allow ioremap_cache() to use existing RAM mappings
      arm64: update 32-bit kuser helpers to ARMv8
      arm64: perf: fix event number mask
      arm64: kconfig: allow CPU_BIG_ENDIAN to be selected
      arm64: Fix the endianness of arch_spinlock_t
      arm64: big-endian: write CPU holding pen address as LE
      ...

commit 4a12cae7ef2612eb094c4b48e8b37cf837e3df55
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Oct 25 15:48:33 2013 +0100

    arm64: Fix the endianness of arch_spinlock_t
    
    The owner and next members of the arch_spinlock_t structure need to be
    swapped when compiling for big endian.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Matthew Leach <matthew.leach@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>

commit bc08b449ee14ace4d869adaa1bb35a44ce68d775
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 2 12:12:15 2013 -0700

    lockref: implement lockless reference count updates using cmpxchg()
    
    Instead of taking the spinlock, the lockless versions atomically check
    that the lock is not taken, and do the reference count update using a
    cmpxchg() loop.  This is semantically identical to doing the reference
    count update protected by the lock, but avoids the "wait for lock"
    contention that you get when accesses to the reference count are
    contended.
    
    Note that a "lockref" is absolutely _not_ equivalent to an atomic_t.
    Even when the lockref reference counts are updated atomically with
    cmpxchg, the fact that they also verify the state of the spinlock means
    that the lockless updates can never happen while somebody else holds the
    spinlock.
    
    So while "lockref_put_or_lock()" looks a lot like just another name for
    "atomic_dec_and_lock()", and both optimize to lockless updates, they are
    fundamentally different: the decrement done by atomic_dec_and_lock() is
    truly independent of any lock (as long as it doesn't decrement to zero),
    so a locked region can still see the count change.
    
    The lockref structure, in contrast, really is a *locked* reference
    count.  If you hold the spinlock, the reference count will be stable and
    you can modify the reference count without using atomics, because even
    the lockless updates will see and respect the state of the lock.
    
    In order to enable the cmpxchg lockless code, the architecture needs to
    do three things:
    
     (1) Make sure that the "arch_spinlock_t" and an "unsigned int" can fit
         in an aligned u64, and have a "cmpxchg()" implementation that works
         on such a u64 data type.
    
     (2) define a helper function to test for a spinlock being unlocked
         ("arch_spin_value_unlocked()")
    
     (3) select the "ARCH_USE_CMPXCHG_LOCKREF" config variable in its
         Kconfig file.
    
    This enables it for x86-64 (but not 32-bit, we'd need to make sure
    cmpxchg() turns into the proper cmpxchg8b in order to enable it for
    32-bit mode).
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 545ac13892ab391049a92108cf59a0d05de7e28c
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Fri Aug 9 19:51:49 2013 +0530

    x86, spinlock: Replace pv spinlocks with pv ticketlocks
    
    Rather than outright replacing the entire spinlock implementation in
    order to paravirtualize it, keep the ticket lock implementation but add
    a couple of pvops hooks on the slow patch (long spin on lock, unlocking
    a contended lock).
    
    Ticket locks have a number of nice properties, but they also have some
    surprising behaviours in virtual environments.  They enforce a strict
    FIFO ordering on cpus trying to take a lock; however, if the hypervisor
    scheduler does not schedule the cpus in the correct order, the system can
    waste a huge amount of time spinning until the next cpu can take the lock.
    
    (See Thomas Friebel's talk "Prevent Guests from Spinning Around"
    http://www.xen.org/files/xensummitboston08/LHP.pdf for more details.)
    
    To address this, we add two hooks:
     - __ticket_spin_lock which is called after the cpu has been
       spinning on the lock for a significant number of iterations but has
       failed to take the lock (presumably because the cpu holding the lock
       has been descheduled).  The lock_spinning pvop is expected to block
       the cpu until it has been kicked by the current lock holder.
     - __ticket_spin_unlock, which on releasing a contended lock
       (there are more cpus with tail tickets), it looks to see if the next
       cpu is blocked and wakes it if so.
    
    When compiled with CONFIG_PARAVIRT_SPINLOCKS disabled, a set of stub
    functions causes all the extra code to go away.
    
    Results:
    =======
    setup: 32 core machine with 32 vcpu KVM guest (HT off)  with 8GB RAM
    base = 3.11-rc
    patched = base + pvspinlock V12
    
    +-----------------+----------------+--------+
     dbench (Throughput in MB/sec. Higher is better)
    +-----------------+----------------+--------+
    |   base (stdev %)|patched(stdev%) | %gain  |
    +-----------------+----------------+--------+
    | 15035.3   (0.3) |15150.0   (0.6) |   0.8  |
    |  1470.0   (2.2) | 1713.7   (1.9) |  16.6  |
    |   848.6   (4.3) |  967.8   (4.3) |  14.0  |
    |   652.9   (3.5) |  685.3   (3.7) |   5.0  |
    +-----------------+----------------+--------+
    
    pvspinlock shows benefits for overcommit ratio > 1 for PLE enabled cases,
    and undercommits results are flat
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Link: http://lkml.kernel.org/r/1376058122-8248-2-git-send-email-raghavendra.kt@linux.vnet.ibm.com
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Tested-by: Attilio Rao <attilio.rao@citrix.com>
    [ Raghavendra: Changed SPIN_THRESHOLD, fixed redefinition of arch_spinlock_t]
    Signed-off-by: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit 86e81f0e624b55fa9f1560c3b64bc80e458c5168
Author: David Herrmann <dh.herrmann@gmail.com>
Date:   Thu Jul 25 18:02:31 2013 +0200

    drm/mm: include required headers in drm_mm.h
    
    We need BUG_ON(), spinlock_t and standard kernel data-types so include the
    right headers.
    
    Subject: [drm-intel:drm-intel-nightly 154/166] include/drm/drm_mm.h:67:2:
     error: unknown type name 'spinlock_t'
    Message-ID: <51f14693.g5HGdcuw2v3m8FOd%fengguang.wu@intel.com>
    
    In case it didn't link to it correctly. Somehow this bug doesn't occur
    here on my machine, hmm. But I think fixing drm_mm.h is better than
    changing the include-order in drm_vma_manager.h, so this is what I
    did.
    
    Signed-off-by: David Herrmann <dh.herrmann@gmail.com>
    Signed-off-by: Dave Airlie <airlied@redhat.com>

commit 60bc851ae59bfe99be6ee89d6bc50008c85ec75d
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Wed May 1 05:24:03 2013 +0000

    af_unix: fix a fatal race with bit fields
    
    Using bit fields is dangerous on ppc64/sparc64, as the compiler [1]
    uses 64bit instructions to manipulate them.
    If the 64bit word includes any atomic_t or spinlock_t, we can lose
    critical concurrent changes.
    
    This is happening in af_unix, where unix_sk(sk)->gc_candidate/
    gc_maybe_cycle/lock share the same 64bit word.
    
    This leads to fatal deadlock, as one/several cpus spin forever
    on a spinlock that will never be available again.
    
    A safer way would be to use a long to store flags.
    This way we are sure compiler/arch wont do bad things.
    
    As we own unix_gc_lock spinlock when clearing or setting bits,
    we can use the non atomic __set_bit()/__clear_bit().
    
    recursion_level can share the same 64bit location with the spinlock,
    as it is set only with this spinlock held.
    
    [1] bug fixed in gcc-4.8.0 :
    http://gcc.gnu.org/bugzilla/show_bug.cgi?id=52080
    
    Reported-by: Ambrose Feinstein <ambrose@google.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 088339a57d6042a8a19a3d5794594b558cd7b624
Author: Julian Anastasov <ja@ssi.bg>
Date:   Thu Mar 21 11:58:10 2013 +0200

    ipvs: convert connection locking
    
    Convert __ip_vs_conntbl_lock_array as follows:
    
    - readers that do not modify conn lists will use RCU lock
    - updaters that modify lists will use spinlock_t
    
    Now for conn lookups we will use RCU read-side
    critical section. Without using __ip_vs_conn_get such
    places have access to connection fields and can
    dereference some pointers like pe and pe_data plus
    the ability to update timer expiration. If full access
    is required we contend for reference.
    
    We add barrier in __ip_vs_conn_put, so that
    other CPUs see the refcnt operation after other writes.
    
    With the introduction of ip_vs_conn_unlink()
    we try to reorganize ip_vs_conn_expire(), so that
    unhashing of connections that should stay more time is
    avoided, even if it is for very short time.
    
    Signed-off-by: Julian Anastasov <ja@ssi.bg>
    Signed-off by: Hans Schillstrom <hans@schillstrom.com>
    Signed-off-by: Simon Horman <horms@verge.net.au>

commit 1b2643f0d09381ad504123809ff587bf6ab0ec7d
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Thu Feb 21 16:42:42 2013 -0800

    scripts/tags.sh: add ctags magic for declarations of popular kernel type
    
    - Add magic for declarations of variables of popular kernel type like
      spinlock_t, list_head, wait_queue_head_t and other.
    
    - Add a set of specially handled declaration extentions like
      __attribute, __aligned and other.
    
    - Simplify pci_bus_* magic
    
    Signed-off-by: Kirill V Tkhai <tkhai@yandex.ru>
    Cc: Michal Marek <mmarek@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 6e34a8b37aca63f109bf990d46131ee07206f5f1
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Mon Jan 28 23:44:49 2013 +0000

    net: cacheline adjust struct inet_frag_queue
    
    Fragmentation code cacheline adjusting of struct inet_frag_queue.
    
    Take advantage of the size of struct timer_list, and move all but
    spinlock_t lock, below the timer struct.  On 64-bit 'lru_list',
    'list' and 'refcnt', fits exactly into the next cacheline, and a
    new cacheline starts at 'fragments'.
    
    The netns_frags *net pointer is moved to the end of the struct,
    because its used in a compare, with "next/close-by" elements of
    which this struct is embedded into.
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 9d57088be9f1d166d6ca311218611a51cd3ecfc2
Author: Bruce Allan <bruce.w.allan@intel.com>
Date:   Fri Jan 4 10:06:03 2013 +0000

    e1000e: add comment to spinlock_t definition
    
    Signed-off-by: Bruce Allan <bruce.w.allan@intel.com>
    Tested-by: Aaron Brown <aaron.f.brown@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

commit a75831f3600c479054fc3f70cd11257ab07886e2
Author: Luis R. Rodriguez <mcgrof@do-not-panic.com>
Date:   Thu Nov 29 16:45:08 2012 -0300

    [media] s5p-jpeg: convert struct spinlock to spinlock_t
    
    spinlock_t should always be used.
    Could not get this to build with allmodconfig:
    mcgrof@frijol ~/linux-next (git::(no branch))$ make C=1 M=drivers/media/platform/s5p-jpeg
      WARNING: Symbol version dump /home/mcgrof/linux-next/Module.symvers
               is missing; modules will have no dependencies and modversions.
      Building modules, stage 2.
      MODPOST 0 modules
    
    Reported-by: Hauke Mehrtens <hauke@hauke-m.de>
    Signed-off-by: Luis R. Rodriguez <mcgrof@do-not-panic.com>
    Cc: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Sylwester Nawrocki <s.nawrocki@samsung.com>
    Signed-off-by: Mauro Carvalho Chehab <mchehab@redhat.com>

commit 9d193b758edaad192d05ebcb8dc4cb72711bf618
Author: Luis R. Rodriguez <mcgrof@do-not-panic.com>
Date:   Thu Nov 29 16:45:07 2012 -0300

    [media] s5p-fimc: convert struct spinlock to spinlock_t
    
    spinlock_t should always be used.
    Could not get this to build with allmodconfig:
    mcgrof@frijol ~/linux-next (git::(no branch))$ make C=1 M=drivers/media/platform/s5p-fimc/
      WARNING: Symbol version dump /home/mcgrof/linux-next/Module.symvers
               is missing; modules will have no dependencies and modversions.
      LD      drivers/media/platform/s5p-fimc/built-in.o
      Building modules, stage 2.
      MODPOST 0 modules
    
    Reported-by: Hauke Mehrtens <hauke@hauke-m.de>
    Signed-off-by: Luis R. Rodriguez <mcgrof@do-not-panic.com>
    Cc: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Sylwester Nawrocki <s.nawrocki@samsung.com>
    Signed-off-by: Mauro Carvalho Chehab <mchehab@redhat.com>

commit 88982fea52d0115d44b77619afef576f24cdb844
Author: Joe Perches <joe@perches.com>
Date:   Mon Dec 17 16:02:00 2012 -0800

    checkpatch: warn when declaring "struct spinlock foo;"
    
    spinlock_t should always be used.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Acked-by: "Luis R. Rodriguez" <mcgrof@do-not-panic.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit f241b244dd4703a560c74e2538f27fd466c8d41f
Author: Luis R. Rodriguez <mcgrof@do-not-panic.com>
Date:   Thu Nov 29 12:45:09 2012 -0800

    brcmfmac: convert struct spinlock to spinlock_t
    
    spinlock_t should always be used.
    
      LD      drivers/net/wireless/brcm80211/built-in.o
      CHECK   drivers/net/wireless/brcm80211/brcmfmac/wl_cfg80211.c
      CC [M]  drivers/net/wireless/brcm80211/brcmfmac/wl_cfg80211.o
      CHECK   drivers/net/wireless/brcm80211/brcmfmac/fwil.c
      CC [M]  drivers/net/wireless/brcm80211/brcmfmac/fwil.o
      CHECK   drivers/net/wireless/brcm80211/brcmfmac/fweh.c
      CC [M]  drivers/net/wireless/brcm80211/brcmfmac/fweh.o
      CHECK   drivers/net/wireless/brcm80211/brcmfmac/dhd_cdc.c
      CC [M]  drivers/net/wireless/brcm80211/brcmfmac/dhd_cdc.o
      CHECK   drivers/net/wireless/brcm80211/brcmfmac/dhd_common.c
      CC [M]  drivers/net/wireless/brcm80211/brcmfmac/dhd_common.o
      CHECK   drivers/net/wireless/brcm80211/brcmfmac/dhd_linux.c
      CC [M]  drivers/net/wireless/brcm80211/brcmfmac/dhd_linux.o
      CHECK   drivers/net/wireless/brcm80211/brcmfmac/dhd_sdio.c
      CC [M]  drivers/net/wireless/brcm80211/brcmfmac/dhd_sdio.o
      CHECK   drivers/net/wireless/brcm80211/brcmfmac/bcmsdh.c
      CC [M]  drivers/net/wireless/brcm80211/brcmfmac/bcmsdh.o
      CHECK   drivers/net/wireless/brcm80211/brcmfmac/bcmsdh_sdmmc.c
      CC [M]  drivers/net/wireless/brcm80211/brcmfmac/bcmsdh_sdmmc.o
      CHECK   drivers/net/wireless/brcm80211/brcmfmac/sdio_chip.c
      CC [M]  drivers/net/wireless/brcm80211/brcmfmac/sdio_chip.o
      CHECK   drivers/net/wireless/brcm80211/brcmfmac/usb.c
      CC [M]  drivers/net/wireless/brcm80211/brcmfmac/usb.o
      CHECK   drivers/net/wireless/brcm80211/brcmfmac/dhd_dbg.c
      CC [M]  drivers/net/wireless/brcm80211/brcmfmac/dhd_dbg.o
      LD [M]  drivers/net/wireless/brcm80211/brcmfmac/brcmfmac.o
      LD      drivers/net/wireless/brcm80211/brcmsmac/built-in.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/mac80211_if.c
    drivers/net/wireless/brcm80211/brcmsmac/mac80211_if.c:1311:6: warning: context imbalance in 'brcms_down' - unexpected unlock
    drivers/net/wireless/brcm80211/brcmsmac/mac80211_if.c:1598:6: warning: context imbalance in 'brcms_rfkill_set_hw_state' - unexpected unlock
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/mac80211_if.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/ucode_loader.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/ucode_loader.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/ampdu.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/ampdu.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/antsel.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/antsel.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/channel.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/channel.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/main.c
    drivers/net/wireless/brcm80211/brcmsmac/main.c:6246:36: warning: Initializer entry defined twice
    drivers/net/wireless/brcm80211/brcmsmac/main.c:6246:43:   also defined here
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/main.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/phy_shim.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/phy_shim.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/pmu.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/pmu.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/rate.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/rate.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/stf.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/stf.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/aiutils.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/aiutils.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/phy/phy_cmn.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/phy/phy_cmn.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/phy/phy_lcn.c
    drivers/net/wireless/brcm80211/brcmsmac/phy/phy_lcn.c:3313:46: warning: cast truncates bits from constant value (ffff7fff becomes 7fff)
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/phy/phy_lcn.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/phy/phy_n.c
    drivers/net/wireless/brcm80211/brcmsmac/phy/phy_n.c:17688:47: warning: cast truncates bits from constant value (ffff7fff becomes 7fff)
    drivers/net/wireless/brcm80211/brcmsmac/phy/phy_n.c:18187:53: warning: cast truncates bits from constant value (ffff3fff becomes 3fff)
    drivers/net/wireless/brcm80211/brcmsmac/phy/phy_n.c:21160:36: warning: cast truncates bits from constant value (ffff3fff becomes 3fff)
    drivers/net/wireless/brcm80211/brcmsmac/phy/phy_n.c:23321:35: warning: cast truncates bits from constant value (ffff7fff becomes 7fff)
    drivers/net/wireless/brcm80211/brcmsmac/phy/phy_n.c:28343:44: warning: cast truncates bits from constant value (ffff1fff becomes 1fff)
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/phy/phy_n.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/phy/phytbl_lcn.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/phy/phytbl_lcn.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/phy/phytbl_n.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/phy/phytbl_n.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/phy/phy_qmath.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/phy/phy_qmath.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/dma.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/dma.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/brcms_trace_events.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/brcms_trace_events.o
      CHECK   drivers/net/wireless/brcm80211/brcmsmac/debug.c
      CC [M]  drivers/net/wireless/brcm80211/brcmsmac/debug.o
      LD [M]  drivers/net/wireless/brcm80211/brcmsmac/brcmsmac.o
      LD      drivers/net/wireless/brcm80211/brcmutil/built-in.o
      CHECK   drivers/net/wireless/brcm80211/brcmutil/utils.c
      CC [M]  drivers/net/wireless/brcm80211/brcmutil/utils.o
      LD [M]  drivers/net/wireless/brcm80211/brcmutil/brcmutil.o
      Building modules, stage 2.
      MODPOST 3 modules
      CC      drivers/net/wireless/brcm80211/brcmfmac/brcmfmac.mod.o
      LD [M]  drivers/net/wireless/brcm80211/brcmfmac/brcmfmac.ko
      CC      drivers/net/wireless/brcm80211/brcmsmac/brcmsmac.mod.o
      LD [M]  drivers/net/wireless/brcm80211/brcmsmac/brcmsmac.ko
      CC      drivers/net/wireless/brcm80211/brcmutil/brcmutil.mod.o
      LD [M]  drivers/net/wireless/brcm80211/brcmutil/brcmutil.ko
    
    Cc: Brett Rudley <brudley@broadcom.com>
    Cc: Roland Vossen <rvossen@broadcom.com>
    Cc: Arend van Spriel <arend@broadcom.com>
    Cc: Franky (Zhenhui) Lin <frankyl@broadcom.com>
    Cc: Kan Yan <kanyan@broadcom.com>
    Cc: linux-wireless@vger.kernel.org
    Cc: brcm80211-dev-list@broadcom.com
    Reported-by: Hauke Mehrtens <hauke@hauke-m.de>
    Signed-off-by: Luis R. Rodriguez <mcgrof@do-not-panic.com>
    Signed-off-by: John W. Linville <linville@tuxdriver.com>

commit 99057c81037ee45e94e5c7e6b403b73caccbaf9e
Author: Luis R. Rodriguez <mcgrof@do-not-panic.com>
Date:   Thu Nov 29 12:45:06 2012 -0800

    i915: convert struct spinlock to spinlock_t
    
    spinlock_t should always be used.
    
      LD      drivers/gpu/drm/i915/built-in.o
      CHECK   drivers/gpu/drm/i915/i915_drv.c
      CC [M]  drivers/gpu/drm/i915/i915_drv.o
      CHECK   drivers/gpu/drm/i915/i915_dma.c
      CC [M]  drivers/gpu/drm/i915/i915_dma.o
      CHECK   drivers/gpu/drm/i915/i915_irq.c
      CC [M]  drivers/gpu/drm/i915/i915_irq.o
      CHECK   drivers/gpu/drm/i915/i915_debugfs.c
    drivers/gpu/drm/i915/i915_debugfs.c:558:31: warning: dereference of noderef expression
    drivers/gpu/drm/i915/i915_debugfs.c:558:39: warning: dereference of noderef expression
    drivers/gpu/drm/i915/i915_debugfs.c:558:51: warning: dereference of noderef expression
    drivers/gpu/drm/i915/i915_debugfs.c:558:63: warning: dereference of noderef expression
      CC [M]  drivers/gpu/drm/i915/i915_debugfs.o
      CHECK   drivers/gpu/drm/i915/i915_suspend.c
      CC [M]  drivers/gpu/drm/i915/i915_suspend.o
      CHECK   drivers/gpu/drm/i915/i915_gem.c
    drivers/gpu/drm/i915/i915_gem.c:3703:14: warning: incorrect type in assignment (different base types)
    drivers/gpu/drm/i915/i915_gem.c:3703:14:    expected unsigned int [unsigned] [usertype] mask
    drivers/gpu/drm/i915/i915_gem.c:3703:14:    got restricted gfp_t
    drivers/gpu/drm/i915/i915_gem.c:3706:22: warning: invalid assignment: &=
    drivers/gpu/drm/i915/i915_gem.c:3706:22:    left side has type unsigned int
    drivers/gpu/drm/i915/i915_gem.c:3706:22:    right side has type restricted gfp_t
    drivers/gpu/drm/i915/i915_gem.c:3707:22: warning: invalid assignment: |=
    drivers/gpu/drm/i915/i915_gem.c:3707:22:    left side has type unsigned int
    drivers/gpu/drm/i915/i915_gem.c:3707:22:    right side has type restricted gfp_t
    drivers/gpu/drm/i915/i915_gem.c:3711:39: warning: incorrect type in argument 2 (different base types)
    drivers/gpu/drm/i915/i915_gem.c:3711:39:    expected restricted gfp_t [usertype] mask
    drivers/gpu/drm/i915/i915_gem.c:3711:39:    got unsigned int [unsigned] [usertype] mask
      CC [M]  drivers/gpu/drm/i915/i915_gem.o
      CHECK   drivers/gpu/drm/i915/i915_gem_context.c
      CC [M]  drivers/gpu/drm/i915/i915_gem_context.o
      CHECK   drivers/gpu/drm/i915/i915_gem_debug.c
      CC [M]  drivers/gpu/drm/i915/i915_gem_debug.o
      CHECK   drivers/gpu/drm/i915/i915_gem_evict.c
      CC [M]  drivers/gpu/drm/i915/i915_gem_evict.o
      CHECK   drivers/gpu/drm/i915/i915_gem_execbuffer.c
      CC [M]  drivers/gpu/drm/i915/i915_gem_execbuffer.o
      CHECK   drivers/gpu/drm/i915/i915_gem_gtt.c
      CC [M]  drivers/gpu/drm/i915/i915_gem_gtt.o
      CHECK   drivers/gpu/drm/i915/i915_gem_stolen.c
      CC [M]  drivers/gpu/drm/i915/i915_gem_stolen.o
      CHECK   drivers/gpu/drm/i915/i915_gem_tiling.c
      CC [M]  drivers/gpu/drm/i915/i915_gem_tiling.o
      CHECK   drivers/gpu/drm/i915/i915_sysfs.c
      CC [M]  drivers/gpu/drm/i915/i915_sysfs.o
      CHECK   drivers/gpu/drm/i915/i915_trace_points.c
      CC [M]  drivers/gpu/drm/i915/i915_trace_points.o
      CHECK   drivers/gpu/drm/i915/intel_display.c
    drivers/gpu/drm/i915/intel_display.c:1736:9: warning: mixing different enum types
    drivers/gpu/drm/i915/intel_display.c:1736:9:     int enum transcoder  versus
    drivers/gpu/drm/i915/intel_display.c:1736:9:     int enum pipe
    drivers/gpu/drm/i915/intel_display.c:3659:48: warning: mixing different enum types
    drivers/gpu/drm/i915/intel_display.c:3659:48:     int enum pipe  versus
    drivers/gpu/drm/i915/intel_display.c:3659:48:     int enum transcoder
      CC [M]  drivers/gpu/drm/i915/intel_display.o
      CHECK   drivers/gpu/drm/i915/intel_crt.c
      CC [M]  drivers/gpu/drm/i915/intel_crt.o
      CHECK   drivers/gpu/drm/i915/intel_lvds.c
      CC [M]  drivers/gpu/drm/i915/intel_lvds.o
      CHECK   drivers/gpu/drm/i915/intel_bios.c
    drivers/gpu/drm/i915/intel_bios.c:706:60: warning: incorrect type in initializer (different address spaces)
    drivers/gpu/drm/i915/intel_bios.c:706:60:    expected struct vbt_header *vbt
    drivers/gpu/drm/i915/intel_bios.c:706:60:    got void [noderef] <asn:2>*vbt
    drivers/gpu/drm/i915/intel_bios.c:726:42: warning: incorrect type in argument 1 (different address spaces)
    drivers/gpu/drm/i915/intel_bios.c:726:42:    expected void const *<noident>
    drivers/gpu/drm/i915/intel_bios.c:726:42:    got unsigned char [noderef] [usertype] <asn:2>*
    drivers/gpu/drm/i915/intel_bios.c:727:40: warning: cast removes address space of expression
    drivers/gpu/drm/i915/intel_bios.c:738:24: warning: cast removes address space of expression
      CC [M]  drivers/gpu/drm/i915/intel_bios.o
      CHECK   drivers/gpu/drm/i915/intel_ddi.c
    drivers/gpu/drm/i915/intel_ddi.c:87:6: warning: symbol 'intel_prepare_ddi_buffers' was not declared. Should it be static?
    drivers/gpu/drm/i915/intel_ddi.c:1036:34: warning: mixing different enum types
    drivers/gpu/drm/i915/intel_ddi.c:1036:34:     int enum pipe  versus
    drivers/gpu/drm/i915/intel_ddi.c:1036:34:     int enum transcoder
      CC [M]  drivers/gpu/drm/i915/intel_ddi.o
    drivers/gpu/drm/i915/intel_ddi.c: In function ‘intel_ddi_setup_hw_pll_state’:
    drivers/gpu/drm/i915/intel_ddi.c:1129:2: warning: ‘port’ may be used uninitialized in this function [-Wmaybe-uninitialized]
    drivers/gpu/drm/i915/intel_ddi.c:1111:12: note: ‘port’ was declared here
      CHECK   drivers/gpu/drm/i915/intel_dp.c
      CC [M]  drivers/gpu/drm/i915/intel_dp.o
      CHECK   drivers/gpu/drm/i915/intel_hdmi.c
      CC [M]  drivers/gpu/drm/i915/intel_hdmi.o
      CHECK   drivers/gpu/drm/i915/intel_sdvo.c
      CC [M]  drivers/gpu/drm/i915/intel_sdvo.o
      CHECK   drivers/gpu/drm/i915/intel_modes.c
      CC [M]  drivers/gpu/drm/i915/intel_modes.o
      CHECK   drivers/gpu/drm/i915/intel_panel.c
      CC [M]  drivers/gpu/drm/i915/intel_panel.o
      CHECK   drivers/gpu/drm/i915/intel_pm.c
    drivers/gpu/drm/i915/intel_pm.c:2173:1: warning: symbol 'mchdev_lock' was not declared. Should it be static?
      CC [M]  drivers/gpu/drm/i915/intel_pm.o
      CHECK   drivers/gpu/drm/i915/intel_i2c.c
      CC [M]  drivers/gpu/drm/i915/intel_i2c.o
      CHECK   drivers/gpu/drm/i915/intel_fb.c
      CC [M]  drivers/gpu/drm/i915/intel_fb.o
      CHECK   drivers/gpu/drm/i915/intel_tv.c
      CC [M]  drivers/gpu/drm/i915/intel_tv.o
      CHECK   drivers/gpu/drm/i915/intel_dvo.c
      CC [M]  drivers/gpu/drm/i915/intel_dvo.o
      CHECK   drivers/gpu/drm/i915/intel_ringbuffer.c
      CC [M]  drivers/gpu/drm/i915/intel_ringbuffer.o
      CHECK   drivers/gpu/drm/i915/intel_overlay.c
      CC [M]  drivers/gpu/drm/i915/intel_overlay.o
      CHECK   drivers/gpu/drm/i915/intel_sprite.c
      CC [M]  drivers/gpu/drm/i915/intel_sprite.o
      CHECK   drivers/gpu/drm/i915/intel_opregion.c
      CC [M]  drivers/gpu/drm/i915/intel_opregion.o
      CHECK   drivers/gpu/drm/i915/dvo_ch7xxx.c
      CC [M]  drivers/gpu/drm/i915/dvo_ch7xxx.o
      CHECK   drivers/gpu/drm/i915/dvo_ch7017.c
      CC [M]  drivers/gpu/drm/i915/dvo_ch7017.o
      CHECK   drivers/gpu/drm/i915/dvo_ivch.c
      CC [M]  drivers/gpu/drm/i915/dvo_ivch.o
      CHECK   drivers/gpu/drm/i915/dvo_tfp410.c
      CC [M]  drivers/gpu/drm/i915/dvo_tfp410.o
      CHECK   drivers/gpu/drm/i915/dvo_sil164.c
      CC [M]  drivers/gpu/drm/i915/dvo_sil164.o
      CHECK   drivers/gpu/drm/i915/dvo_ns2501.c
      CC [M]  drivers/gpu/drm/i915/dvo_ns2501.o
      CHECK   drivers/gpu/drm/i915/i915_gem_dmabuf.c
      CC [M]  drivers/gpu/drm/i915/i915_gem_dmabuf.o
      CHECK   drivers/gpu/drm/i915/i915_ioc32.c
      CC [M]  drivers/gpu/drm/i915/i915_ioc32.o
      CHECK   drivers/gpu/drm/i915/intel_acpi.c
      CC [M]  drivers/gpu/drm/i915/intel_acpi.o
      LD [M]  drivers/gpu/drm/i915/i915.o
      Building modules, stage 2.
      MODPOST 1 modules
      CC      drivers/gpu/drm/i915/i915.mod.o
      LD [M]  drivers/gpu/drm/i915/i915.ko
    
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: dri-devel@lists.freedesktop.org
    Reported-by: Hauke Mehrtens <hauke@hauke-m.de>
    Signed-off-by: Luis R. Rodriguez <mcgrof@do-not-panic.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

commit f351d027eea545a7996af54fce99f5668a67fec5
Author: Feng Tang <feng.tang@intel.com>
Date:   Tue Oct 23 01:29:27 2012 +0200

    ACPI / EC: Cleanup the member name for spinlock/mutex in struct
    
    Current member names for mutex/spinlock are a little confusing.
    
    Change the
    {
            struct mutex lock;
            spinlock_t curr_lock;
    }
    to
    {
            struct mutex mutex;
            spinlock_t lock;
    }
    
    So that the code is cleaner and easier to read.
    
    Signed-off-by: Feng Tang <feng.tang@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 45ef6ac6f5d4d4ea441a042fee3790b3f33cba73
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Sun Aug 5 14:59:12 2012 +0000

    ARM: footbridge: nw_gpio_lock is raw_spin_lock
    
    bd31b85960a "locking, ARM: Annotate low level hw locks as raw"
    made nw_gpio_lock a raw spinlock, but did not change all the
    users in device drivers. This fixes the remaining ones.
    
    sound/oss/waveartist.c: In function 'vnc_mute_spkr':
    sound/oss/waveartist.c:1485:2: warning: passing argument 1 of 'spinlock_check' from incompatible pointer type [enabled by default]
    include/linux/spinlock.h:272:102: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
    drivers/char/ds1620.c: In function 'netwinder_lock':
    drivers/char/ds1620.c:77:2: warning: passing argument 1 of 'spinlock_check' from incompatible pointer type [enabled by default]
    include/linux/spinlock.h:272:102: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
    drivers/char/nwflash.c: In function 'kick_open':
    drivers/char/nwflash.c:620:2: warning: passing argument 1 of 'spinlock_check' from incompatible pointer type [enabled by default]
    include/linux/spinlock.h:272:102: note: expected 'struct spinlock_t *' but argument is of type 'struct raw_spinlock_t *'
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>

commit 67a101f573b0cb1043c8c305112113450cb9fdbf
Author: Anton Vorontsov <anton.vorontsov@linaro.org>
Date:   Tue Jul 17 11:37:07 2012 -0700

    pstore: Headers should include all stuff they use
    
    Headers should really include all the needed prototypes, types, defines
    etc. to be self-contained. This is a long-standing issue, but apparently
    the new tracing code unearthed it (SMP=n is also a prerequisite):
    
    In file included from fs/pstore/internal.h:4:0,
                     from fs/pstore/ftrace.c:21:
    include/linux/pstore.h:43:15: error: field ‘read_mutex’ has incomplete type
    
    While at it, I also added the following:
    
    linux/types.h -> size_t, phys_addr_t, uXX and friends
    linux/spinlock.h -> spinlock_t
    linux/errno.h -> Exxxx
    linux/time.h -> struct timespec (struct passed by value)
    struct module and rs_control forward declaration (passed via pointers).
    
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 514b1923e1549162f1597f81113c0e5b72aed691
Merge: 6f73b3629f77 5042ab91c427
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 23 09:04:58 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rkuo/linux-hexagon-kernel
    
    Pull Hexagon architecture changes from Richard Kuo:
     "These are mostly cleanups and feedback remaining from the original
      upstreaming."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rkuo/linux-hexagon-kernel:
      various Kconfig cleanup and old platform build code removal
      hexagon/mm/vm_fault.c: Port OOM changes to do_page_fault
      arch/hexagon/kernel/dma.c: make function static
      Remove unneeded include of version.h from arch/hexagon/include/asm/spinlock_types.h
      Hexagon: Use resource_size function

commit bfd46276fdf4e936259193a520d6bc1926800fb4
Author: Jesper Juhl <jj@chaosbits.net>
Date:   Sun Nov 20 21:59:55 2011 +0100

    Remove unneeded include of version.h from arch/hexagon/include/asm/spinlock_types.h
    
    "make versioncheck" points out that arch/hexagon/include/asm/spinlock_types.h
    does not need to include version.h .
    A quick look at the file seems to confirm its findings, so here's a patch that
    removes the include.
    
    Signed-off-by: Jesper Juhl <jj@chaosbits.net>
    Signed-off-by: Richard Kuo <rkuo@codeaurora.org>

commit 12eb9444a8df7ab4aa5f4c91f8e3049af5d9819b
Author: Kalle Valo <kvalo@qca.qualcomm.com>
Date:   Wed Mar 7 20:04:00 2012 +0200

    ath6kl: document all spinlocks
    
    Also fixes quite a few checkpatch warnings like this:
    
    ath6kl/hif.h:226: CHECK: spinlock_t definition without comment
    
    Signed-off-by: Kalle Valo <kvalo@qca.qualcomm.com>

commit d0f474e501929acdbd116cca39ef083012f70f25
Author: Roland Dreier <roland@purestorage.com>
Date:   Thu Jan 12 10:41:18 2012 -0800

    target: Use LIST_HEAD()/DEFINE_MUTEX() for static objects
    
    Instead of
    
       static struct list_head foo;
       static struct mutex bar;
    
       ...
    
       INIT_LIST_HEAD(&foo);
       mutex_init(&bar);
    
    just do
    
       static LIST_HEAD(foo);
       static DEFINE_MUTEX(bar);
    
    Also remove some superfluous struct list_head and spinlock_t
    initialization calls where the variables are already defined using
    macros that initialize them.
    
    This saves a decent amount of compiled code too:
    
        add/remove: 0/0 grow/shrink: 0/3 up/down: 0/-178 (-178)
        function                                     old     new   delta
        target_core_init_configfs                    898     850     -48
        core_scsi3_emulate_pro_preempt              1742    1683     -59
        iscsi_thread_set_init                        159      88     -71
    
    Signed-off-by: Roland Dreier <roland@purestorage.com>
    Signed-off-by: Nicholas Bellinger <nab@linux-iscsi.org>

commit 34793f20e774a02e73605de68b0505d4f12306c1
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon Jan 23 19:32:25 2012 +0000

    x86: xen: size struct xen_spinlock to always fit in arch_spinlock_t
    
    commit 7a7546b377bdaa25ac77f33d9433c59f259b9688 upstream.
    
    If NR_CPUS < 256 then arch_spinlock_t is only 16 bits wide but struct
    xen_spinlock is 32 bits.  When a spin lock is contended and
    xl->spinners is modified the two bytes immediately after the spin lock
    would be corrupted.
    
    This is a regression caused by 84eb950db13ca40a0572ce9957e14723500943d6
    (x86, ticketlock: Clean up types and accessors) which reduced the size
    of arch_spinlock_t.
    
    Fix this by making xl->spinners a u8 if NR_CPUS < 256.  A
    BUILD_BUG_ON() is also added to check the sizes of the two structures
    are compatible.
    
    In many cases this was not noticable as there would often be padding
    bytes after the lock (e.g., if any of CONFIG_GENERIC_LOCKBREAK,
    CONFIG_DEBUG_SPINLOCK, or CONFIG_DEBUG_LOCK_ALLOC were enabled).
    
    The bnx2 driver is affected. In struct bnx2, phy_lock and
    indirect_lock may have no padding after them.  Contention on phy_lock
    would corrupt indirect_lock making it appear locked and the driver
    would deadlock.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6c334f4f6aeb0916bfc15ff731073e9f24de0189
Merge: 67d2433ee7aa 69e8f430e243
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 28 18:15:33 2012 -0800

    Merge branch 'stable/for-linus-fixes-3.3' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'stable/for-linus-fixes-3.3' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/granttable: Disable grant v2 for HVM domains.
      x86: xen: size struct xen_spinlock to always fit in arch_spinlock_t

commit 7a7546b377bdaa25ac77f33d9433c59f259b9688
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Mon Jan 23 19:32:25 2012 +0000

    x86: xen: size struct xen_spinlock to always fit in arch_spinlock_t
    
    If NR_CPUS < 256 then arch_spinlock_t is only 16 bits wide but struct
    xen_spinlock is 32 bits.  When a spin lock is contended and
    xl->spinners is modified the two bytes immediately after the spin lock
    would be corrupted.
    
    This is a regression caused by 84eb950db13ca40a0572ce9957e14723500943d6
    (x86, ticketlock: Clean up types and accessors) which reduced the size
    of arch_spinlock_t.
    
    Fix this by making xl->spinners a u8 if NR_CPUS < 256.  A
    BUILD_BUG_ON() is also added to check the sizes of the two structures
    are compatible.
    
    In many cases this was not noticable as there would often be padding
    bytes after the lock (e.g., if any of CONFIG_GENERIC_LOCKBREAK,
    CONFIG_DEBUG_SPINLOCK, or CONFIG_DEBUG_LOCK_ALLOC were enabled).
    
    The bnx2 driver is affected. In struct bnx2, phy_lock and
    indirect_lock may have no padding after them.  Contention on phy_lock
    would corrupt indirect_lock making it appear locked and the driver
    would deadlock.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    CC: stable@kernel.org #only 3.2
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit 1334f32938e46fb321c67a652997d33583257249
Author: Axel Lin <axel.lin@gmail.com>
Date:   Tue Nov 29 13:54:01 2011 +0800

    watchdog: Use DEFINE_SPINLOCK() for static spinlocks
    
    Rather than just defining static spinlock_t variables and then
    initializing them later in init functions, simply define them with
    DEFINE_SPINLOCK() and remove the calls to spin_lock_init().
    
    Signed-off-by: Axel Lin <axel.lin@gmail.com>
    Cc: Nicolas Thill <nico@openwrt.org>
    Cc: Heiko Ronsdorf <hero@ihg.uni-duisburg.de>
    Cc: Rodolfo Giometti <giometti@ascensit.com>
    Cc: Andrey Panin <pazke@donpac.ru>
    Cc: Guido Guenther <agx@sigxcpu.org>
    Cc: Curt E Bruns <curt.e.bruns@intel.com>
    Cc: Deepak Saxena <dsaxena@plexity.net>
    Cc: Andrew Victor <linux@maxim.org.za>
    Cc: George G. Davis <gdavis@mvista.com>
    Cc: Sylver Bruneau <sylver.bruneau@googlemail.com>
    Signed-off-by: Wim Van Sebroeck <wim@iguana.be>

commit 3cfef9524677a4ecb392d6fbffe6ebce6302f1d4
Merge: 982653009b88 68cc3990a545
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 26 16:17:32 2011 +0200

    Merge branch 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (27 commits)
      rtmutex: Add missing rcu_read_unlock() in debug_rt_mutex_print_deadlock()
      lockdep: Comment all warnings
      lib: atomic64: Change the type of local lock to raw_spinlock_t
      locking, lib/atomic64: Annotate atomic64_lock::lock as raw
      locking, x86, iommu: Annotate qi->q_lock as raw
      locking, x86, iommu: Annotate irq_2_ir_lock as raw
      locking, x86, iommu: Annotate iommu->register_lock as raw
      locking, dma, ipu: Annotate bank_lock as raw
      locking, ARM: Annotate low level hw locks as raw
      locking, drivers/dca: Annotate dca_lock as raw
      locking, powerpc: Annotate uic->lock as raw
      locking, x86: mce: Annotate cmci_discover_lock as raw
      locking, ACPI: Annotate c3_lock as raw
      locking, oprofile: Annotate oprofilefs lock as raw
      locking, video: Annotate vga console lock as raw
      locking, latencytop: Annotate latency_lock as raw
      locking, timer_stats: Annotate table_lock as raw
      locking, rwsem: Annotate inner lock as raw
      locking, semaphores: Annotate inner lock as raw
      locking, sched: Annotate thread_group_cputimer as raw
      ...
    
    Fix up conflicts in kernel/posix-cpu-timers.c manually: making
    cputimer->cputime a raw lock conflicted with the ABBA fix in commit
    bcd5cff7216f ("cputimer: Cure lock inversion").

commit cb475de3d12df6912bc95048202ae8c280d4cad5
Author: Yong Zhang <yong.zhang0@gmail.com>
Date:   Wed Sep 14 15:49:24 2011 +0800

    lib: atomic64: Change the type of local lock to raw_spinlock_t
    
    There are still some leftovers of commit f59ca058
    [locking, lib/atomic64: Annotate atomic64_lock::lock as raw]
    
    [ tglx: Seems I picked the wrong version of that patch :( ]
    
    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Shan Hai <haishan.bai@gmail.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Link: http://lkml.kernel.org/r/20110914074924.GA16096@zhy
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit bd5fe6c5eb9c548d7f07fe8f89a150bb6705e8e3
Author: Christoph Hellwig <hch@infradead.org>
Date:   Fri Jun 24 14:29:43 2011 -0400

    fs: kill i_alloc_sem
    
    i_alloc_sem is a rather special rw_semaphore.  It's the last one that may
    be released by a non-owner, and it's write side is always mirrored by
    real exclusion.  It's intended use it to wait for all pending direct I/O
    requests to finish before starting a truncate.
    
    Replace it with a hand-grown construct:
    
     - exclusion for truncates is already guaranteed by i_mutex, so it can
       simply fall way
     - the reader side is replaced by an i_dio_count member in struct inode
       that counts the number of pending direct I/O requests.  Truncate can't
       proceed as long as it's non-zero
     - when i_dio_count reaches non-zero we wake up a pending truncate using
       wake_up_bit on a new bit in i_flags
     - new references to i_dio_count can't appear while we are waiting for
       it to read zero because the direct I/O count always needs i_mutex
       (or an equivalent like XFS's i_iolock) for starting a new operation.
    
    This scheme is much simpler, and saves the space of a spinlock_t and a
    struct list_head in struct inode (typically 160 bits on a non-debug 64-bit
    system).
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 08d63aac436b2ad35d9e864b9ebb05bc8b9bb653
Merge: 152b92db7ad2 e5ea3f12d41d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jun 12 11:03:29 2011 -0700

    Merge branch 'gpio/merge' of git://git.secretlab.ca/git/linux-2.6
    
    * 'gpio/merge' of git://git.secretlab.ca/git/linux-2.6:
      gpio/basic_mmio: add missing include of spinlock_types.h
      gpio/nomadik: fix sleepmode for elder Nomadik

commit e5ea3f12d41d96edf4ad9374db2b1725e457acec
Author: Jamie Iles <jamie@jamieiles.com>
Date:   Fri Jun 10 13:44:49 2011 +0100

    gpio/basic_mmio: add missing include of spinlock_types.h
    
    include/linux/basic_mmio_gpio.h uses a spinlock_t without including any
    of the spinlock headers resulting in this compiler warning.
    
    include/linux/basic_mmio_gpio.h:51:2: error: expected specifier-qualifier-list before 'spinlock_t'
    
    Explicitly include linux/spinlock_types.h to fix it.
    
    Signed-off-by: Jamie Iles <jamie@jamieiles.com>
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>

commit 961ec6daa7b14f376c30d447a830fa4783a2112c
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Dec 2 18:01:49 2010 +0100

    ARM: 6521/1: perf: use raw_spinlock_t for pmu_lock
    
    For kernels built with PREEMPT_RT, critical sections protected
    by standard spinlocks are preemptible. This is not acceptable
    on perf as (a) we may be scheduled onto a different CPU whilst
    reading/writing banked PMU registers and (b) the latency when
    reading the PMU registers becomes unpredictable.
    
    This patch upgrades the pmu_lock spinlock to a raw_spinlock
    instead.
    
    Reported-by: Jamie Iles <jamie@jamieiles.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit f4f510508741680e423524c222f615276ca6222c
Author: Avi Kivity <avi@redhat.com>
Date:   Sun Sep 19 18:44:07 2010 +0200

    KVM: Convert PIC lock from raw spinlock to ordinary spinlock
    
    The PIC code used to be called from preempt_disable() context, which
    wasn't very good for PREEMPT_RT.  That is no longer the case, so move
    back from raw_spinlock_t to spinlock_t.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

commit 1612ae996ec16ed358f0e9afacb0a262b1cf97a4
Author: Sven Eckelmann <sven.eckelmann@gmx.de>
Date:   Sat Sep 18 21:01:11 2010 +0200

    Staging: batman-adv: checkpatch cleanup of comments
    
    checkpatch now detects the start of a comment and warns about usage of
    multiple spaces at the beginning of a line. We have to replace the '   '
    in multiple lines comments by ' * ' to fix it.
    
    Checkpatch also wants a comment after a definition of a spinlock_t which
    describes what it protects. It is currently not possible to add it
    before the actual struct which includes the spinlock.
    
    Signed-off-by: Sven Eckelmann <sven.eckelmann@gmx.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 5605a6f71afc6e76f9ef7d6e18dd042024bc4f61
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Mon Aug 2 08:32:22 2010 +0100

    ARM: 6280/1: imx: Fix build failure when including <mach/gpio.h> without <linux/spinlock.h>
    
    commit 868003ca7ad17ac6c1606dc36101f10a7825b399 upstream.
    
    This is a follow up to
    
            14cb0de (arm/imx/gpio: add spinlock protection)
    
    and fixes the following build failure:
    
              CC      arch/arm/mach-imx/pcm970-baseboard.o
            In file included from arch/arm/include/asm/gpio.h:6,
                             from include/linux/gpio.h:8,
                             from arch/arm/mach-imx/pcm970-baseboard.c:20:
            arch/arm/plat-mxc/include/mach/gpio.h:40: error: expected specifier-qualifier-list before 'spinlock_t'
    
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 8c4db5ab03ff9fc177c4cf0ac349839ca5993099
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Mon Aug 2 08:32:22 2010 +0100

    ARM: 6280/1: imx: Fix build failure when including <mach/gpio.h> without <linux/spinlock.h>
    
    commit 868003ca7ad17ac6c1606dc36101f10a7825b399 upstream.
    
    This is a follow up to
    
            14cb0de (arm/imx/gpio: add spinlock protection)
    
    and fixes the following build failure:
    
              CC      arch/arm/mach-imx/pcm970-baseboard.o
            In file included from arch/arm/include/asm/gpio.h:6,
                             from include/linux/gpio.h:8,
                             from arch/arm/mach-imx/pcm970-baseboard.c:20:
            arch/arm/plat-mxc/include/mach/gpio.h:40: error: expected specifier-qualifier-list before 'spinlock_t'
    
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit ba4719835c88aaa7b059e3fe2055adebf3608aa3
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Mon Aug 2 08:32:22 2010 +0100

    ARM: 6280/1: imx: Fix build failure when including <mach/gpio.h> without <linux/spinlock.h>
    
    commit 868003ca7ad17ac6c1606dc36101f10a7825b399 upstream.
    
    This is a follow up to
    
            14cb0de (arm/imx/gpio: add spinlock protection)
    
    and fixes the following build failure:
    
              CC      arch/arm/mach-imx/pcm970-baseboard.o
            In file included from arch/arm/include/asm/gpio.h:6,
                             from include/linux/gpio.h:8,
                             from arch/arm/mach-imx/pcm970-baseboard.c:20:
            arch/arm/plat-mxc/include/mach/gpio.h:40: error: expected specifier-qualifier-list before 'spinlock_t'
    
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 868003ca7ad17ac6c1606dc36101f10a7825b399
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Mon Aug 2 08:32:22 2010 +0100

    ARM: 6280/1: imx: Fix build failure when including <mach/gpio.h> without <linux/spinlock.h>
    
    This is a follow up to
    
            14cb0de (arm/imx/gpio: add spinlock protection)
    
    and fixes the following build failure:
    
              CC      arch/arm/mach-imx/pcm970-baseboard.o
            In file included from arch/arm/include/asm/gpio.h:6,
                             from include/linux/gpio.h:8,
                             from arch/arm/mach-imx/pcm970-baseboard.c:20:
            arch/arm/plat-mxc/include/mach/gpio.h:40: error: expected specifier-qualifier-list before 'spinlock_t'
    
    Cc: stable@kernel.org
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 1d747c7e7b754cacb9bc8d31854bdc3a58d70597
Author: Helge Deller <deller@gmx.de>
Date:   Tue Feb 2 19:06:23 2010 +0000

    parisc: drop unnecessary cast in __ldcw_align() macro
    
    __ldcw_align() can directly access the slock member of struct arch_spinlock_t
    instead of using an ugly cast.
    
    Signed-off-by: Helge Deller <deller@gmx.de>
    Signed-off-by: Kyle McMartin <kyle@mcmartin.ca>

commit c812a51d11bbe983f4c24e32b59b265705ddd3c2
Merge: 9467c4fdd66f d2be1651b736
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 5 13:12:34 2010 -0800

    Merge branch 'kvm-updates/2.6.34' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    * 'kvm-updates/2.6.34' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (145 commits)
      KVM: x86: Add KVM_CAP_X86_ROBUST_SINGLESTEP
      KVM: VMX: Update instruction length on intercepted BP
      KVM: Fix emulate_sys[call, enter, exit]()'s fault handling
      KVM: Fix segment descriptor loading
      KVM: Fix load_guest_segment_descriptor() to inject page fault
      KVM: x86 emulator: Forbid modifying CS segment register by mov instruction
      KVM: Convert kvm->requests_lock to raw_spinlock_t
      KVM: Convert i8254/i8259 locks to raw_spinlocks
      KVM: x86 emulator: disallow opcode 82 in 64-bit mode
      KVM: x86 emulator: code style cleanup
      KVM: Plan obsolescence of kernel allocated slots, paravirt mmu
      KVM: x86 emulator: Add LOCK prefix validity checking
      KVM: x86 emulator: Check CPL level during privilege instruction emulation
      KVM: x86 emulator: Fix popf emulation
      KVM: x86 emulator: Check IOPL level during io instruction emulation
      KVM: x86 emulator: fix memory access during x86 emulation
      KVM: x86 emulator: Add Virtual-8086 mode of emulation
      KVM: x86 emulator: Add group9 instruction decoding
      KVM: x86 emulator: Add group8 instruction decoding
      KVM: do not store wqh in irqfd
      ...
    
    Trivial conflicts in Documentation/feature-removal-schedule.txt

commit 70e335e16882df5b5d6971022e63c3603a1e8c23
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Feb 18 11:25:22 2010 +0200

    KVM: Convert kvm->requests_lock to raw_spinlock_t
    
    The code relies on kvm->requests_lock inhibiting preemption.
    
    Noted by Jan Kiszka.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

commit a662b8135a1f9fee7d3f9129498cb03f3d6ce772
Author: Tony Luck <tony.luck@intel.com>
Date:   Thu Dec 17 17:05:03 2009 -0800

    KVM: ia64: fix build breakage due to host spinlock change
    
    Len Brown pointed out that allmodconfig is broken for
    ia64 because of:
    
    arch/ia64/kvm/vmm.c: In function 'vmm_spin_unlock':
    arch/ia64/kvm/vmm.c:70: error: 'spinlock_t' has no member named 'raw_lock'
    
    KVM has it's own spinlock routines. It should not depend on the base kernel
    spinlock_t type (which changed when ia64 switched to ticket locks).  Define
    its own vmm_spinlock_t type.
    
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

commit a70caa8ba48f21f46d3b4e71b6b8d14080bbd57a
Author: Hugh Dickins <hugh.dickins@tiscali.co.uk>
Date:   Mon Dec 14 17:59:02 2009 -0800

    mm: stop ptlock enlarging struct page
    
    CONFIG_DEBUG_SPINLOCK adds 12 or 16 bytes to a 32- or 64-bit spinlock_t,
    and CONFIG_DEBUG_LOCK_ALLOC adds another 12 or 24 bytes to it: lockdep
    enables both of those, and CONFIG_LOCK_STAT adds 8 or 16 bytes to that.
    
    When 2.6.15 placed the split page table lock inside struct page (usually
    sized 32 or 56 bytes), only CONFIG_DEBUG_SPINLOCK was a possibility, and
    we ignored the enlargement (but fitted in CONFIG_GENERIC_LOCKBREAK's 4 by
    letting the spinlock_t occupy both page->private and page->mapping).
    
    Should these debugging options be allowed to double the size of a struct
    page, when only one minority use of the page (as a page table) needs to
    fit a spinlock in there?  Perhaps not.
    
    Take the easy way out: switch off SPLIT_PTLOCK_CPUS when DEBUG_SPINLOCK or
    DEBUG_LOCK_ALLOC is in force.  I've sometimes tried to be cleverer,
    kmallocing a cacheline for the spinlock when it doesn't fit, but given up
    each time.  Falling back to mm->page_table_lock (as we do when ptlock is
    not split) lets lockdep check out the strictest path anyway.
    
    And now that some arches allow 8192 cpus, use 999999 for infinity.
    
    (What has this got to do with KSM swapping?  It doesn't care about the
    size of struct page, but may care about random junk in page->mapping - to
    be explained separately later.)
    
    Signed-off-by: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: Izik Eidus <ieidus@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 5f6384c5fb6bfc9aac506e058974d3ba293951b3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 8 16:16:41 2009 +0100

    alpha: Fix fallout from locking changes
    
    spin_* functions are mostly static inline now. That causes the alpha
    compile to fail:
    
    CC      arch/alpha/kernel/sys_sable.o
    cc1: warnings being treated as errors
    In file included from arch/alpha/kernel/sys_sable.c:25:
    arch/alpha/include/asm/core_t2.h: In function 't2_readb':
    arch/alpha/include/asm/core_t2.h:451: error: 'spinlock_check' is static but \
            used in inline function 't2_readb' which is not static
    arch/alpha/include/asm/core_t2.h:456: error: 'spin_unlock_irqrestore' is \
            static but used in inline function 't2_readb' which is not static
    
    That's caused by the "extern inline" magic which is used for the
    subarch specific read/write[bwl] functions. I tried to distangle the
    uncountable macro onion layers, but failed miserably.
    
    Last resort solution: switch the t2_hae_lock to raw_spinlock_t so the
    lock functions are pure macros and function calls again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: linux-alpha@vger.kernel.org

commit 3fff4c42bd0a89869a0eb1e7874cc06ffa4aa0f5
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Sep 22 16:18:09 2009 +0200

    printk: Remove ratelimit.h from kernel.h
    
    Decouple kernel.h from ratelimit.h: the global declaration of
    printk's ratelimit_state is not needed, and it leads to messy
    circular dependencies due to ratelimit.h's (new) adding of a
    spinlock_types.h include.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: David S. Miller <davem@davemloft.net>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 6276e08a9bdf645b71a092fb4530baf4f6c4c6eb
Author: Roland Dreier <rolandd@cisco.com>
Date:   Sat Sep 5 20:24:23 2009 -0700

    IB: Use DEFINE_SPINLOCK() for static spinlocks
    
    Rather than just defining static spinlock_t variables and then
    initializing them later in init functions, simply define them with
    DEFINE_SPINLOCK() and remove the calls to spin_lock_init().  This cleans
    up the source a tad and also shrinks the compiled code; eg on x86-64:
    
    add/remove: 0/0 grow/shrink: 0/3 up/down: 0/-40 (-40)
    function                                     old     new   delta
    ib_uverbs_init                               336     326     -10
    ib_mad_init_module                           147     137     -10
    ib_sa_init                                   123     103     -20
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

commit f3c737de8f57b5ce756010c2175f7d574194b30d
Author: Sage Weil <sage@newdream.ne>
Date:   Thu Apr 23 08:37:58 2009 +0200

    umem: fix request_queue lock warning
    
    The umem driver issues two warnings on boot, due to blk_plug_device() and
    blk_remove_plug() being called without q->queue_lock held.  Starting with
    e48ec690 (block: extend queue_flag bitops), the queue_flag_* functions
    warn if q->queue_lock doesn't appear to be locked.  In fact, q->queue_lock
    is NULL (though that apparently isn't otherwise a problem as the driver is
    using card->lock for everything).
    
    Although blk_init_queue() with take a request_fn_proc and spinlock_t*,
    there isn't a corresponding init helper that takes a make_request_fn.
    Setting queue_lock to &card->lock explicitly seems to work fine for me.
    The warning goes away and the device appears to behave.
    
    [    1.531881] v2.3 : Micro Memory(tm) PCI memory board block driver
    [    1.538136] umem 0000:02:01.0: PCI INT A -> GSI 20 (level, low) -> IRQ 20
    [    1.545018] umem 0000:02:01.0: Micro Memory(tm) controller found (PCI Mem Module (Battery Backup))
    [    1.554176] umem 0000:02:01.0: CSR 0xfc9ffc00 -> 0xffffc200013d0c00 (0x100)
    [    1.561279] umem 0000:02:01.0: Size 1048576 KB, Battery 1 Disabled (FAILURE), Battery 2 Disabled (FAILURE)
    [    1.571114] umem 0000:02:01.0: Window size 16777216 bytes, IRQ 20
    [    1.577304] umem 0000:02:01.0: memory NOT initialized. Consider over-writing whole device.
    [    1.585989]  umema:<4>------------[ cut here ]------------
    [    1.591775] WARNING: at include/linux/blkdev.h:492 blk_plug_device+0x6d/0x106()
    [    1.592025] Hardware name: H8SSL
    [    1.592025] Modules linked in:
    [    1.592025] Pid: 1, comm: swapper Not tainted 2.6.29 #8
    [    1.592025] Call Trace:
    [    1.592025]  [<ffffffff8023c994>] warn_slowpath+0xd3/0xf2
    [    1.592025]  [<ffffffff8025a5b5>] ? save_trace+0x3f/0x9b
    [    1.592025]  [<ffffffff8025a68b>] ? add_lock_to_list+0x7a/0xba
    [    1.592025]  [<ffffffff8025e609>] ? validate_chain+0xb3b/0xce8
    [    1.592025]  [<ffffffff80441556>] ? mm_make_request+0x27/0x59
    [    1.592025]  [<ffffffff80441556>] ? mm_make_request+0x27/0x59
    [    1.592025]  [<ffffffff8025ef04>] ? __lock_acquire+0x74e/0x7b9
    [    1.592025]  [<ffffffff8025a70e>] ? get_lock_stats+0x34/0x5e
    [    1.592025]  [<ffffffff8025a746>] ? put_lock_stats+0xe/0x27
    [    1.592025]  [<ffffffff80441556>] ? mm_make_request+0x27/0x59
    [    1.592025]  [<ffffffff803ad165>] blk_plug_device+0x6d/0x106
    [    1.592025]  [<ffffffff80441575>] mm_make_request+0x46/0x59
    [    1.592025]  [<ffffffff803ac2d9>] generic_make_request+0x335/0x3cf
    [    1.592025]  [<ffffffff8027fcc7>] ? mempool_alloc_slab+0x11/0x13
    [    1.592025]  [<ffffffff8027fdce>] ? mempool_alloc+0x45/0x101
    [    1.592025]  [<ffffffff8025a746>] ? put_lock_stats+0xe/0x27
    [    1.592025]  [<ffffffff803adda5>] submit_bio+0x10a/0x119
    [    1.592025]  [<ffffffff802c8d00>] submit_bh+0xe5/0x109
    [    1.592025]  [<ffffffff802cbf43>] block_read_full_page+0x2aa/0x2cb
    [    1.592025]  [<ffffffff802cf4c4>] ? blkdev_get_block+0x0/0x4c
    [    1.592025]  [<ffffffff805c90a8>] ? _spin_unlock_irq+0x36/0x51
    [    1.592025]  [<ffffffff80286836>] ? __lru_cache_add+0x92/0xb2
    [    1.592025]  [<ffffffff802cf008>] blkdev_readpage+0x13/0x15
    [    1.592025]  [<ffffffff8027de06>] read_cache_page_async+0x90/0x134
    [    1.592025]  [<ffffffff802ceff5>] ? blkdev_readpage+0x0/0x15
    [    1.592025]  [<ffffffff802f5f1c>] ? adfspart_check_ICS+0x0/0x16c
    [    1.592025]  [<ffffffff8027deb8>] read_cache_page+0xe/0x45
    [    1.592025]  [<ffffffff802f5170>] read_dev_sector+0x2e/0x93
    [    1.592025]  [<ffffffff802f5f44>] adfspart_check_ICS+0x28/0x16c
    [    1.592025]  [<ffffffff8025d427>] ? trace_hardirqs_on+0xd/0xf
    [    1.592025]  [<ffffffff802f5f1c>] ? adfspart_check_ICS+0x0/0x16c
    [    1.592025]  [<ffffffff802f59c5>] rescan_partitions+0x168/0x2fb
    [    1.592025]  [<ffffffff802ceae9>] __blkdev_get+0x259/0x336
    [    1.592025]  [<ffffffff803ca1e2>] ? kobject_put+0x47/0x4b
    [    1.592025]  [<ffffffff802cebd1>] blkdev_get+0xb/0xd
    [    1.592025]  [<ffffffff802f5773>] register_disk+0xc4/0x12b
    [    1.592025]  [<ffffffff803b2a7b>] add_disk+0xc3/0x12d
    [    1.592025]  [<ffffffff808a1d4a>] ? mm_init+0x0/0x1a5
    [    1.592025]  [<ffffffff808a1e73>] mm_init+0x129/0x1a5
    [    1.592025]  [<ffffffff808a1d4a>] ? mm_init+0x0/0x1a5
    [    1.592025]  [<ffffffff80209056>] _stext+0x56/0x130
    [    1.592025]  [<ffffffff80274932>] ? register_irq_proc+0xae/0xca
    [    1.592025]  [<ffffffff802f0000>] ? proc_pid_lookup+0xb4/0x18b
    [    1.592025]  [<ffffffff8087f975>] kernel_init+0x132/0x18b
    [    1.592025]  [<ffffffff8020d17a>] child_rip+0xa/0x20
    [    1.592025]  [<ffffffff8020cb40>] ? restore_args+0x0/0x30
    [    1.592025]  [<ffffffff8087f843>] ? kernel_init+0x0/0x18b
    [    1.592025]  [<ffffffff8020d170>] ? child_rip+0x0/0x20
    [    1.592025] ---[ end trace 7150b3b86da74e1e ]---
    [    1.889858] ------------[ cut here ]------------[ve_plug+0x5f/0x91()
    [    1.893848] Hardware name: H8SSL
    [    1.893848] Modules linked in:
    [    1.893848] Pid: 1, comm: swapper Tainted: G        W  2.6.29 #8
    [    1.893848] Call Trace:
    [    1.893848]  [<ffffffff8023c994>] warn_slowpath+0xd3/0xf2
    [    1.893848]  [<ffffffff805c8411>] ? trace_hardirqs_on_thunk+0x3a/0x3f
    [    1.893848]  [<ffffffff8020cb40>] ? restore_args+0x0/0x30
    [    1.893848]  [<ffffffff80254245>] ? __atomic_notifier_call_chain+0x0/0xb2
    [    1.893848]  [<ffffffff805c90a3>] ? _spin_unlock_irq+0x31/0x51
    [    1.893848]  [<ffffffff805c90bf>] ? _spin_unlock_irq+0x4d/0x51
    [    1.893848]  [<ffffffff8044157d>] ? mm_make_request+0x4e/0x59
    [    1.893848]  [<ffffffff8025a70e>] ? get_lock_stats+0x34/0x5e
    [    1.893848]  [<ffffffff8025a75d>] ? put_lock_stats+0x25/0x27
    [    1.893848]  [<ffffffff80441504>] ? mm_unplug_device+0x25/0x50
    [    1.893848]  [<ffffffff803acf23>] blk_remove_plug+0x5f/0x91
    [    1.893848]  [<ffffffff8044150f>] mm_unplug_device+0x30/0x50
    [    1.893848]  [<ffffffff803ab74a>] blk_unplug+0x78/0x7d
    [    1.893848]  [<ffffffff803ab75c>] blk_backing_dev_unplug+0xd/0xf
    [    1.893848]  [<ffffffff802c853c>] block_sync_page+0x4a/0x4c
    [    1.893848]  [<ffffffff8027da1c>] sync_page+0x44/0x4d
    [    1.893848]  [<ffffffff805c66fd>] __wait_on_bit_lock+0x42/0x8a
    [    1.893848]  [<ffffffff8027d9d8>] ? sync_page+0x0/0x4d
    [    1.893848]  [<ffffffff8027d9c4>] __lock_page+0x64/0x6b
    [    1.893848]  [<ffffffff802508db>] ? wake_bit_function+0x0/0x2a
    [    1.893848]  [<ffffffff8027de4a>] read_cache_page_async+0xd4/0x134
    [    1.893848]  [<ffffffff802ceff5>] ? blkdev_readpage+0x0/0x15
    [    1.893848]  [<ffffffff802f5f1c>] ? adfspart_check_ICS+0x0/0x16c
    [    1.893848]  [<ffffffff8027deb8>] read_cache_page+0xe/0x45
    [    1.893848]  [<ffffffff802f5170>] read_dev_sector+0x2e/0x93
    [    1.893848]  [<ffffffff802f5f44>] adfspart_check_ICS+0x28/0x16c
    [    1.893848]  [<ffffffff8025d427>] ? trace_hardirqs_on+0xd/0xf
    [    1.893848]  [<ffffffff802f5f1c>] ? adfspart_check_ICS+0x0/0x16c
    [    1.893848]  [<ffffffff802f59c5>] rescan_partitions+0x168/0x2fb
    [    1.893848]  [<ffffffff802ceae9>] __blkdev_get+0x259/0x336
    [    1.893848]  [<ffffffff803ca1e2>] ? kobject_put+0x47/0x4b
    [    1.893848]  [<ffffffff802cebd1>] blkdev_get+0xb/0xd
    [    1.893848]  [<ffffffff802f5773>] register_disk+0xc4/0x12b
    [    1.893848]  [<ffffffff803b2a7b>] add_disk+0xc3/0x12d
    [    1.893848]  [<ffffffff808a1d4a>] ? mm_init+0x0/0x1a5
    [    1.893848]  [<ffffffff808a1e73>] mm_init+0x129/0x1a5
    [    1.893848]  [<ffffffff808a1d4a>] ? mm_init+0x0/0x1a5
    [    1.893848]  [<ffffffff80209056>] _stext+0x56/0x130
    [    1.893848]  [<ffffffff80274932>] ? register_irq_proc+0xae/0xca
    [    1.893848]  [<ffffffff802f0000>] ? proc_pid_lookup+0xb4/0x18b
    [    1.893848]  [<ffffffff8087f975>] kernel_init+0x132/0x18b
    [    1.893848]  [<ffffffff8020d17a>] child_rip+0xa/0x20
    [    1.893848]  [<ffffffff8020cb40>] ? restore_args+0x0/0x30
    [    1.893848]  [<ffffffff8087f843>] ? kernel_init+0x0/0x18b
    [    1.893848]  [<ffffffff8020d170>] ? child_rip+0x0/0x20
    [    1.893848] ---[ end trace 7150b3b86da74e1f ]---
    
    Signed-off-by: Sage Weil <sage@newdream.net>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

commit fc0cb7d166f3c8a318d8b0c9580cfe12fed9234c
Author: Julia Lawall <julia@diku.dk>
Date:   Thu Dec 25 15:34:44 2008 +0100

    Staging: comedi: Use DEFINE_SPINLOCK
    
    SPIN_LOCK_UNLOCKED is deprecated.  The following makes the change suggested
    in Documentation/spinlocks.txt
    
    The semantic patch that makes this change is as follows:
    (http://www.emn.fr/x-info/coccinelle/)
    
    // <smpl>
    @@
    declarer name DEFINE_SPINLOCK;
    identifier xxx_lock;
    @@
    
    - spinlock_t xxx_lock = SPIN_LOCK_UNLOCKED;
    + DEFINE_SPINLOCK(xxx_lock);
    // </smpl>
    
    Signed-off-by: Julia Lawall <julia@diku.dk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 3f4577bff8b8d03e56f1b62bedca5cd3b18f919a
Author: Julia Lawall <julia@diku.dk>
Date:   Thu Dec 25 15:35:05 2008 +0100

    Staging: meilhaus: Use DEFINE_SPINLOCK
    
    SPIN_LOCK_UNLOCKED is deprecated.  The following makes the change suggested
    in Documentation/spinlocks.txt
    
    The semantic patch that makes this change is as follows:
    (http://www.emn.fr/x-info/coccinelle/)
    
    // <smpl>
    @@
    declarer name DEFINE_SPINLOCK;
    identifier xxx_lock;
    @@
    
    - spinlock_t xxx_lock = SPIN_LOCK_UNLOCKED;
    + DEFINE_SPINLOCK(xxx_lock);
    // </smpl>
    
    Signed-off-by: Julia Lawall <julia@diku.dk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 16fd8be997245025ed5252f12306fff28801335b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jan 23 17:37:49 2009 +0100

    x86, mm: fix pte_free()
    
    commit 42ef73fe134732b2e91c0326df5fd568da17c4b2 upstream.
    
    On -rt we were seeing spurious bad page states like:
    
    Bad page state in process 'firefox'
    page:c1bc2380 flags:0x40000000 mapping:c1bc2390 mapcount:0 count:0
    Trying to fix it up, but a reboot is needed
    Backtrace:
    Pid: 503, comm: firefox Not tainted 2.6.26.8-rt13 #3
    [<c043d0f3>] ? printk+0x14/0x19
    [<c0272d4e>] bad_page+0x4e/0x79
    [<c0273831>] free_hot_cold_page+0x5b/0x1d3
    [<c02739f6>] free_hot_page+0xf/0x11
    [<c0273a18>] __free_pages+0x20/0x2b
    [<c027d170>] __pte_alloc+0x87/0x91
    [<c027d25e>] handle_mm_fault+0xe4/0x733
    [<c043f680>] ? rt_mutex_down_read_trylock+0x57/0x63
    [<c043f680>] ? rt_mutex_down_read_trylock+0x57/0x63
    [<c0218875>] do_page_fault+0x36f/0x88a
    
    This is the case where a concurrent fault already installed the PTE and
    we get to free the newly allocated one.
    
    This is due to pgtable_page_ctor() doing the spin_lock_init(&page->ptl)
    which is overlaid with the {private, mapping} struct.
    
    union {
        struct {
            unsigned long private;
            struct address_space *mapping;
        };
        spinlock_t ptl;
        struct kmem_cache *slab;
        struct page *first_page;
    };
    
    Normally the spinlock is small enough to not stomp on page->mapping, but
    PREEMPT_RT=y has huge 'spin'locks.
    
    But lockdep kernels should also be able to trigger this splat, as the
    lock tracking code grows the spinlock to cover page->mapping.
    
    The obvious fix is calling pgtable_page_dtor() like the regular pte free
    path __pte_free_tlb() does.
    
    It seems all architectures except x86 and nm10300 already do this, and
    nm10300 doesn't seem to use pgtable_page_ctor(), which suggests it
    doesn't do SMP or simply doesnt do MMU at all or something.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlsta@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 8ca2918f99b5861359de1805f27b08023c82abd2
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jan 23 17:37:49 2009 +0100

    x86, mm: fix pte_free()
    
    commit 42ef73fe134732b2e91c0326df5fd568da17c4b2 upstream.
    
    On -rt we were seeing spurious bad page states like:
    
    Bad page state in process 'firefox'
    page:c1bc2380 flags:0x40000000 mapping:c1bc2390 mapcount:0 count:0
    Trying to fix it up, but a reboot is needed
    Backtrace:
    Pid: 503, comm: firefox Not tainted 2.6.26.8-rt13 #3
    [<c043d0f3>] ? printk+0x14/0x19
    [<c0272d4e>] bad_page+0x4e/0x79
    [<c0273831>] free_hot_cold_page+0x5b/0x1d3
    [<c02739f6>] free_hot_page+0xf/0x11
    [<c0273a18>] __free_pages+0x20/0x2b
    [<c027d170>] __pte_alloc+0x87/0x91
    [<c027d25e>] handle_mm_fault+0xe4/0x733
    [<c043f680>] ? rt_mutex_down_read_trylock+0x57/0x63
    [<c043f680>] ? rt_mutex_down_read_trylock+0x57/0x63
    [<c0218875>] do_page_fault+0x36f/0x88a
    
    This is the case where a concurrent fault already installed the PTE and
    we get to free the newly allocated one.
    
    This is due to pgtable_page_ctor() doing the spin_lock_init(&page->ptl)
    which is overlaid with the {private, mapping} struct.
    
    union {
        struct {
            unsigned long private;
            struct address_space *mapping;
        };
        spinlock_t ptl;
        struct kmem_cache *slab;
        struct page *first_page;
    };
    
    Normally the spinlock is small enough to not stomp on page->mapping, but
    PREEMPT_RT=y has huge 'spin'locks.
    
    But lockdep kernels should also be able to trigger this splat, as the
    lock tracking code grows the spinlock to cover page->mapping.
    
    The obvious fix is calling pgtable_page_dtor() like the regular pte free
    path __pte_free_tlb() does.
    
    It seems all architectures except x86 and nm10300 already do this, and
    nm10300 doesn't seem to use pgtable_page_ctor(), which suggests it
    doesn't do SMP or simply doesnt do MMU at all or something.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlsta@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 94cd3e6cbebf85903b4d53ed2147bdb4c6e08625
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Tue Jan 27 17:45:10 2009 -0800

    net: wrong test in inet_ehash_locks_alloc()
    
    In commit 9db66bdcc83749affe61c61eb8ff3cf08f42afec (net: convert
    TCP/DCCP ehash rwlocks to spinlocks), I forgot to change one
    occurrence of rwlock_t to spinlock_t
    
    I believe sizeof(raw_spinlock_t) might be > 0 on !CONFIG_SMP if
    CONFIG_DEBUG_SPINLOCK while sizeof(raw_rwlock_t) should be 0 in this
    case.
    
    Fortunatly, CONFIG_DEBUG_SPINLOCK adds fields to both spinlock_t and
    rwlock_t, but at this might change in the future (being able to debug
    spinlocks but not rwlocks for example), better to be safe.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 42ef73fe134732b2e91c0326df5fd568da17c4b2
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jan 23 17:37:49 2009 +0100

    x86, mm: fix pte_free()
    
    On -rt we were seeing spurious bad page states like:
    
    Bad page state in process 'firefox'
    page:c1bc2380 flags:0x40000000 mapping:c1bc2390 mapcount:0 count:0
    Trying to fix it up, but a reboot is needed
    Backtrace:
    Pid: 503, comm: firefox Not tainted 2.6.26.8-rt13 #3
    [<c043d0f3>] ? printk+0x14/0x19
    [<c0272d4e>] bad_page+0x4e/0x79
    [<c0273831>] free_hot_cold_page+0x5b/0x1d3
    [<c02739f6>] free_hot_page+0xf/0x11
    [<c0273a18>] __free_pages+0x20/0x2b
    [<c027d170>] __pte_alloc+0x87/0x91
    [<c027d25e>] handle_mm_fault+0xe4/0x733
    [<c043f680>] ? rt_mutex_down_read_trylock+0x57/0x63
    [<c043f680>] ? rt_mutex_down_read_trylock+0x57/0x63
    [<c0218875>] do_page_fault+0x36f/0x88a
    
    This is the case where a concurrent fault already installed the PTE and
    we get to free the newly allocated one.
    
    This is due to pgtable_page_ctor() doing the spin_lock_init(&page->ptl)
    which is overlaid with the {private, mapping} struct.
    
    union {
        struct {
            unsigned long private;
            struct address_space *mapping;
        };
        spinlock_t ptl;
        struct kmem_cache *slab;
        struct page *first_page;
    };
    
    Normally the spinlock is small enough to not stomp on page->mapping, but
    PREEMPT_RT=y has huge 'spin'locks.
    
    But lockdep kernels should also be able to trigger this splat, as the
    lock tracking code grows the spinlock to cover page->mapping.
    
    The obvious fix is calling pgtable_page_dtor() like the regular pte free
    path __pte_free_tlb() does.
    
    It seems all architectures except x86 and nm10300 already do this, and
    nm10300 doesn't seem to use pgtable_page_ctor(), which suggests it
    doesn't do SMP or simply doesnt do MMU at all or something.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlsta@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: <stable@kernel.org>

commit 1d5bc03a8183d12c7daf4e7c69cce8d9c4b9a86b
Author: Julia Lawall <julia@diku.dk>
Date:   Thu Dec 25 04:34:04 2008 +0000

    powerpc/pasemi: Use DEFINE_SPINLOCK
    
    SPIN_LOCK_UNLOCKED is deprecated.  The following makes the change suggested
    in Documentation/spinlocks.txt
    
    The semantic patch that makes this change is as follows:
    (http://www.emn.fr/x-info/coccinelle/)
    
    // <smpl>
    @@
    declarer name DEFINE_SPINLOCK;
    identifier xxx_lock;
    @@
    
    - spinlock_t xxx_lock = SPIN_LOCK_UNLOCKED;
    + DEFINE_SPINLOCK(xxx_lock);
    // </smpl>
    
    Signed-off-by: Julia Lawall <julia@diku.dk>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 2701a1ad04ebab0c8fae8beb4586383a5a70ba0c
Author: Julia Lawall <julia@diku.dk>
Date:   Thu Dec 25 04:33:34 2008 +0000

    powerpc/52xx: Use DEFINE_SPINLOCK
    
    SPIN_LOCK_UNLOCKED is deprecated.  The following makes the change suggested
    in Documentation/spinlocks.txt
    
    The semantic patch that makes this change is as follows:
    (http://www.emn.fr/x-info/coccinelle/)
    
    // <smpl>
    @@
    declarer name DEFINE_SPINLOCK;
    identifier xxx_lock;
    @@
    
    - spinlock_t xxx_lock = SPIN_LOCK_UNLOCKED;
    + DEFINE_SPINLOCK(xxx_lock);
    // </smpl>
    
    Signed-off-by: Julia Lawall <julia@diku.dk>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit aefa8b6bf48fdcc904de4f166e59ab37fb750dec
Author: Julia Lawall <julia@diku.dk>
Date:   Thu Dec 25 19:33:11 2008 +0000

    parisc: Use DEFINE_SPINLOCK
    
    SPIN_LOCK_UNLOCKED is deprecated.  The following makes the change suggested
    in Documentation/spinlocks.txt
    
    The semantic patch that makes this change is as follows:
    (http://www.emn.fr/x-info/coccinelle/)
    
    // <smpl>
    @@
    declarer name DEFINE_SPINLOCK;
    identifier xxx_lock;
    @@
    
    - spinlock_t xxx_lock = SPIN_LOCK_UNLOCKED;
    + DEFINE_SPINLOCK(xxx_lock);
    // </smpl>
    
    Signed-off-by: Julia Lawall <julia@diku.dk>
    Signed-off-by: Kyle McMartin <kyle@mcmartin.ca>

commit eb8374e71f941a1b3c2ed6ea19dc809e7124dc5d
Author: Julia Lawall <julia@diku.dk>
Date:   Thu Dec 25 15:35:27 2008 +0100

    GFS2: Use DEFINE_SPINLOCK
    
    SPIN_LOCK_UNLOCKED is deprecated.  The following makes the change suggested
    in Documentation/spinlocks.txt
    
    The semantic patch that makes this change is as follows:
    (http://www.emn.fr/x-info/coccinelle/)
    
    // <smpl>
    @@
    declarer name DEFINE_SPINLOCK;
    identifier xxx_lock;
    @@
    
    - spinlock_t xxx_lock = SPIN_LOCK_UNLOCKED;
    + DEFINE_SPINLOCK(xxx_lock);
    // </smpl>
    
    Signed-off-by: Julia Lawall <julia@diku.dk>
    Signed-off-by: Steven Whitehouse <swhiteho@redhat.com>

commit 087052b02f42b50316c6e4d7f2d8dfba3de6fc2e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Oct 17 16:09:57 2008 +0200

    x86: fix default_spin_lock_flags() prototype
    
    these warnings:
    
      arch/x86/kernel/paravirt-spinlocks.c: In function ‘default_spin_lock_flags’:
      arch/x86/kernel/paravirt-spinlocks.c:12: warning: passing argument 1 of ‘__raw_spin_lock’ from incompatible pointer type
      arch/x86/kernel/paravirt-spinlocks.c: At top level:
      arch/x86/kernel/paravirt-spinlocks.c:11: warning: ‘default_spin_lock_flags’ defined but not used
    
    showed that the prototype of default_spin_lock_flags() was confused about
    what type spinlocks have.
    
    the proper type on UP is raw_spinlock_t.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit c60ff51eb26dfcfb0bdc807b09a096aeadd01325
Author: Xiantao Zhang <xiantao.zhang@intel.com>
Date:   Sat Nov 8 15:46:59 2008 +0800

    KVM: ia64: fix vmm_spin_{un}lock for !CONFIG_SMP
    
    In the case of !CONFIG_SMP, raw_spinlock_t is empty and the spinlock functions
    don't build.  Fix by defining spinlock functions for the uniprocessor case.
    
    Signed-off-by: Xiantao Zhang <xiantao.zhang@intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

commit 5e3609f60c09f0f15f71f80c6d7933b2c7be71a6
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Thu Jun 19 22:39:00 2008 +0200

    sparc: merge header files with trivial differences
    
    A manual inspection revealed that the following headerfiles
    contained only trivial differences:
    hw_irq.h idprom.h kmap_types.h kvm.h spinlock_types.h sunbpp.h unaligned.h
    
    The only noteworthy change are that sparc64 had a volatile
    qualifer that sparc missed in spinlock_types.h.
    
    In addition a few comments were updated.
    
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

commit 2da676594a73825f10d2a99358cc7465119684f9
Author: Pradeep Singh Rautela <rautelap@gmail.com>
Date:   Thu May 29 23:28:14 2008 +0530

    ata: Convert to static DEFINE_SPINLOCK(lock)
    
    Replace deprecated static spinlock_t instance to static DEFINE_SPINLOCK(lock).
    
    Signed-off-by: Pradeep Singh <rautelap@gmail.com>
    Signed-off-by: Jeff Garzik <jgarzik@redhat.com>

commit 522d8dc08b16deb51c128d544ab1cb9c621c950e
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Sat Feb 9 18:24:31 2008 +0100

    [S390] VMEM_MAX_PHYS overflow on 31 bit.
    
    With the new space saving spinlock_t and a non-debug configuration
    the struct page only has 32 bytes for 31 bit s390. The causes an
    overflow in the calculation of VMEM_MAX_PHYS which renders the
    kernel unbootable.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit 36e41eebdafc8b5fabdf66f59d0d43b0b60f0fdb
Author: Eric Sandeen <sandeen@sandeen.net>
Date:   Thu Oct 11 17:43:43 2007 +1000

    [XFS] Cleanup lock goop.
    
    Switch last couple lock_t's to spinlock_t's. Remove now-unused
    spinlock-related macros & types.
    
    SGI-PV: 970382
    SGI-Modid: xfs-linux-melb:xfs-kern:29748a
    
    Signed-off-by: Eric Sandeen <sandeen@sandeen.net>
    Signed-off-by: Donald Douwsma <donaldd@sgi.com>
    Signed-off-by: Tim Shimmin <tes@sgi.com>

commit 3685c2a1d773781608c9e281a6ff6b4c8ea8f6f9
Author: Eric Sandeen <sandeen@sandeen.net>
Date:   Thu Oct 11 17:42:32 2007 +1000

    [XFS] Unwrap XFS_SB_LOCK.
    
    Un-obfuscate XFS_SB_LOCK, remove XFS_SB_LOCK->mutex_lock->spin_lock
    macros, call spin_lock directly, remove extraneous cookie holdover from
    old xfs code, and change lock type to spinlock_t.
    
    SGI-PV: 970382
    SGI-Modid: xfs-linux-melb:xfs-kern:29746a
    
    Signed-off-by: Eric Sandeen <sandeen@sandeen.net>
    Signed-off-by: Donald Douwsma <donaldd@sgi.com>
    Signed-off-by: Tim Shimmin <tes@sgi.com>

commit 703e1f0fd2edc2978bde3b4536e78b577318c090
Author: Eric Sandeen <sandeen@sandeen.net>
Date:   Thu Oct 11 17:41:21 2007 +1000

    [XFS] Unwrap xfs_dabuf_global_lock
    
    Un-obfuscate dabuf_global_lock, remove mutex_lock->spin_lock macros, call
    spin_lock directly, remove extraneous cookie holdover from old xfs code,
    and change lock type to spinlock_t.
    
    SGI-PV: 970382
    SGI-Modid: xfs-linux-melb:xfs-kern:29744a
    
    Signed-off-by: Eric Sandeen <sandeen@sandeen.net>
    Signed-off-by: Donald Douwsma <donaldd@sgi.com>
    Signed-off-by: Tim Shimmin <tes@sgi.com>

commit 64137e56d76a5c05aa4411e2f5d7121593dd9478
Author: Eric Sandeen <sandeen@sandeen.net>
Date:   Thu Oct 11 17:38:28 2007 +1000

    [XFS] Unwrap pagb_lock.
    
    Un-obfuscate pagb_lock, remove mutex_lock->spin_lock macros, call
    spin_lock directly, remove extraneous cookie holdover from old xfs code,
    and change lock type to spinlock_t.
    
    SGI-PV: 970382
    SGI-Modid: xfs-linux-melb:xfs-kern:29743a
    
    Signed-off-by: Eric Sandeen <sandeen@sandeen.net>
    Signed-off-by: Donald Douwsma <donaldd@sgi.com>
    Signed-off-by: Tim Shimmin <tes@sgi.com>

commit 869b906078720b68711569b68de0acca6b73b675
Author: Eric Sandeen <sandeen@sandeen.net>
Date:   Thu Oct 11 17:38:18 2007 +1000

    [XFS] Unwrap XFS_DQ_PINUNLOCK.
    
    Un-obfuscate DQ_PINLOCK, remove DQ_PINLOCK->mutex_lock->spin_lock macros,
    call spin_lock directly, remove extraneous cookie holdover from old xfs
    code, and change lock type to spinlock_t.
    
    SGI-PV: 970382
    SGI-Modid: xfs-linux-melb:xfs-kern:29742a
    
    Signed-off-by: Eric Sandeen <sandeen@sandeen.net>
    Signed-off-by: Donald Douwsma <donaldd@sgi.com>
    Signed-off-by: Tim Shimmin <tes@sgi.com>

commit c8b5ea289fed15a7d7a4d6e911987ff16499aed7
Author: Eric Sandeen <sandeen@sandeen.net>
Date:   Thu Oct 11 17:37:31 2007 +1000

    [XFS] Unwrap GRANT_LOCK.
    
    Un-obfuscate GRANT_LOCK, remove GRANT_LOCK->mutex_lock->spin_lock macros,
    call spin_lock directly, remove extraneous cookie holdover from old xfs
    code, and change lock type to spinlock_t.
    
    SGI-PV: 970382
    SGI-Modid: xfs-linux-melb:xfs-kern:29741a
    
    Signed-off-by: Eric Sandeen <sandeen@sandeen.net>
    Signed-off-by: Donald Douwsma <donaldd@sgi.com>
    Signed-off-by: Tim Shimmin <tes@sgi.com>

commit b22cd72c95df0414e0502a0999624d460ba66126
Author: Eric Sandeen <sandeen@sandeen.net>
Date:   Thu Oct 11 17:37:10 2007 +1000

    [XFS] Unwrap LOG_LOCK.
    
    Un-obfuscate LOG_LOCK, remove LOG_LOCK->mutex_lock->spin_lock macros, call
    spin_lock directly, remove extraneous cookie holdover from old xfs code,
    and change lock type to spinlock_t.
    
    SGI-PV: 970382
    SGI-Modid: xfs-linux-melb:xfs-kern:29740a
    
    Signed-off-by: Eric Sandeen <sandeen@sandeen.net>
    Signed-off-by: Donald Douwsma <donaldd@sgi.com>
    Signed-off-by: Tim Shimmin <tes@sgi.com>

commit 853265e588153dca22c82b84df39bbb40abd7ad0
Author: Valentine Barshak <vbarshak@ru.mvista.com>
Date:   Tue Feb 5 01:57:55 2008 +1100

    [POWERPC] Add missing native dcr dcr_ind_lock spinlock
    
    The include/asm-powerpc/dcr-native.h declares extern spinlock_t dcr_ind_lock;
    but it's actually isn't defined. This patch adds a missing dcr_ind_lock.
    
    Signed-off-by: Valentine Barshak <vbarshak@ru.mvista.com>
    Signed-off-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>

commit 2c81210a26fb84c0af6aad95f6ec1d61cf276cd0
Author: Uwe Kleine-König <Uwe.Kleine-Koenig@digi.com>
Date:   Tue Jan 8 15:16:24 2008 +0100

    kbuild: ignore cache modifiers for generating the tags files
    
    With this patch I'm able to find the definition of _xmit_lock defined in
    include/linux/netdevice.h as follows:
    
            struct net_device {
                    ...
                    spinlock_t _xmit_lock ____cacheline_aligned_in_smp;
            }
    
    Otherwise this counts as definition of ____cacheline_aligned_in_smp.
    
    Signed-off-by: Uwe Kleine-König <Uwe.Kleine-Koenig@digi.com>
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>

commit 3b4beb31759765efdda9f9431aebfedf828bbfe0
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Sat Jan 26 14:11:03 2008 +0100

    [S390] Remove owner_pc member from raw_spinlock_t.
    
    Used to contain the address of the holder of the lock. But since the
    spinlock code is not inlined anymore all locks contain the same address
    anyway. And since in addtition nobody complained about that for ages
    its obviously unused. So remove it.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit 4fe87745a6722d42ff27a60768c77958fa1fc498
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jul 19 01:48:58 2007 -0700

    lockstat: hook into spinlock_t, rwlock_t, rwsem and mutex
    
    Call the new lockstat tracking functions from the various lock primitives.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Jason Baron <jbaron@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 21f8ca3bf6198bd21e3c4cc820af2ccf753a6ec8
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jul 19 01:48:53 2007 -0700

    fix raw_spinlock_t vs lockdep
    
    Use the lockdep infrastructure to track lock contention and other lock
    statistics.
    
    It tracks lock contention events, and the first four unique call-sites that
    encountered contention.
    
    It also measures lock wait-time and hold-time in nanoseconds. The minimum and
    maximum times are tracked, as well as a total (which together with the number
    of event can give the avg).
    
    All statistics are done per lock class, per write (exclusive state) and per read
    (shared state).
    
    The statistics are collected per-cpu, so that the collection overhead is
    minimized via having no global cachemisses.
    
    This new lock statistics feature is independent of the lock dependency checking
    traditionally done by lockdep; it just shares the lock tracking code. It is
    also possible to enable both and runtime disabled either component - thereby
    avoiding the O(n^2) lock chain walks for instance.
    
    This patch:
    
    raw_spinlock_t should not use lockdep (and doesn't) since lockdep itself
    relies on it.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 017f021c7e5fe3f82ccc5cbb7b1750e66e00f527
Author: Ed L. Cashin <ecashin@coraid.com>
Date:   Sun Jul 15 23:41:50 2007 -0700

    docs: static initialization of spinlocks is OK
    
    Static initialization of spinlocks is preferable to dynamic initialization
    when it is practical.  This patch updates documentation for consistency
    with comments in spinlock_types.h.
    
    Signed-off-by: Ed L. Cashin <ecashin@coraid.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 4a0df2ef4569ac57cb18cd97e34c0c4733e829d9
Author: Andy Whitcroft <apw@shadowen.org>
Date:   Fri Jun 8 13:46:39 2007 -0700

    update checkpatch.pl to version 0.03
    
    This version brings a host of changes to cure false positives and
    bugs detected on patches submitted to lkml and -mm.  It also brings
    a number of new tests in response to reviews, of particular note:
    
      - catch use of volatile
      - allow deprecated functions to be listed in feature-removal-schedule.txt
      - warn about #ifdef's in c files
      - check that spinlock_t and struct mutex use is commented
      - report on architecture specific defines being used
      - report memory barriers without an associated comment
    
    Full changelog:
    
          catch use of volatile
          convert other quoted string checks to common routine
          alloc deprecated functions to be listed in feature-removal-schedule.txt
          split out the line length and indent for each line
          improve switch block handling
          handle GNU diff context lines with no leading space
          warn about #ifdef's in c files
          tidy up tests for signed-off-by using raw mode
          check that spinlock_t and struct mutex use is commented
          syntax checks for open brace placement may drop off the bottom of hunk
          report memory barriers without an associated comment
          when a sign off is present but ugly do not report it missing
          do not mistake bitfield definitions for indented labels
          report on architecture specific defines being used
          major update to the operator checks
          prevent switch/if/while etc matching foo_switch
          generify assignement in condition error message
          introduce an operator context marker
          Version: 0.03
    
    Signed-off-by: Andy Whitcroft <apw@shadowen.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 8ffa68755a0eddf3baeecd0e7612a5106cf2db23
Author: Christoph Lameter <clameter@sgi.com>
Date:   Thu May 31 00:40:51 2007 -0700

    SLUB: Fix NUMA / SYSFS bootstrap issue
    
    We need this patch in ASAP.  Patch fixes the mysterious hang that remained
    on some particular configurations with lockdep on after the first fix that
    moved the #idef CONFIG_SLUB_DEBUG to the right location.  See
    http://marc.info/?t=117963072300001&r=1&w=2
    
    The kmem_cache_node cache is very special because it is needed for NUMA
    bootstrap.  Under certain conditions (like for example if lockdep is
    enabled and significantly increases the size of spinlock_t) the structure
    may become exactly the size as one of the larger caches in the kmalloc
    array.
    
    That early during bootstrap we cannot perform merging properly.  The unique
    id for the kmem_cache_node cache will match one of the kmalloc array.
    Sysfs will complain about a duplicate directory entry.  All of this occurs
    while the console is not yet fully operational.  Thus boot may appear to be
    silently failing.
    
    The kmem_cache_node cache is very special.  During early boostrap the main
    allocation function is not operational yet and so we have to run our own
    small special alloc function during early boot.  It is also special in that
    it is never freed.
    
    We really do not want any merging on that cache.  Set the refcount -1 and
    forbid merging of slabs that have a negative refcount.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 9490991482a2091a828d997adbc088e24c310a4d
Author: Borislav Petkov <bbpetkov@yahoo.de>
Date:   Sun May 6 14:49:17 2007 -0700

    Add unitialized_var() macro for suppressing gcc warnings
    
    Introduce a macro for suppressing gcc from generating a warning about a
    probable uninitialized state of a variable.
    
    Example:
    
    -       spinlock_t *ptl;
    +       spinlock_t *uninitialized_var(ptl);
    
    Not a happy solution, but those warnings are obnoxious.
    
    - Using the usual pointlessly-set-it-to-zero approach wastes several
      bytes of text.
    
    - Using a macro means we can (hopefully) do something else if gcc changes
      cause the `x = x' hack to stop working
    
    - Using a macro means that people who are worried about hiding true bugs
      can easily turn it off.
    
    Signed-off-by: Borislav Petkov <bbpetkov@yahoo.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 3c97b5e93faf9262407835046effc886efefa0a6
Author: Kyle McMartin <kyle@mako.i.cabal.ca>
Date:   Thu Dec 7 23:52:27 2006 -0500

    [PARISC] Move spinlock_t out of struct cpu_data
    
    Signed-off-by: Kyle McMartin <kyle@parisc-linux.org>

commit 12d40e43d251de4fa1f982567fc8b4ee5e858367
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Wed Dec 6 20:39:53 2006 -0800

    [PATCH] Save some bytes in struct inode
    
    [acme@newtoy net-2.6.20]$ pahole --cacheline 64 fs/inode.o inode
    /* /pub/scm/linux/kernel/git/acme/net-2.6.20/include/linux/dcache.h:86 */
    struct inode {
            struct hlist_node          i_hash;               /*     0     8 */
            struct list_head           i_list;               /*     8     8 */
            struct list_head           i_sb_list;            /*    16     8 */
            struct list_head           i_dentry;             /*    24     8 */
            long unsigned int          i_ino;                /*    32     4 */
            atomic_t                   i_count;              /*    36     4 */
            umode_t                    i_mode;               /*    40     2 */
    
            /* XXX 2 bytes hole, try to pack */
    
            unsigned int               i_nlink;              /*    44     4 */
            uid_t                      i_uid;                /*    48     4 */
            gid_t                      i_gid;                /*    52     4 */
            dev_t                      i_rdev;               /*    56     4 */
            loff_t                     i_size;               /*    60     8 */
            struct timespec            i_atime;              /*    68     8 */
            struct timespec            i_mtime;              /*    76     8 */
            struct timespec            i_ctime;              /*    84     8 */
            unsigned int               i_blkbits;            /*    92     4 */
            long unsigned int          i_version;            /*    96     4 */
            blkcnt_t                   i_blocks;             /*   100     4 */
            short unsigned int         i_bytes;              /*   104     2 */
    
            /* XXX 2 bytes hole, try to pack */
    
            spinlock_t                 i_lock;               /*   108    40 */
            struct mutex               i_mutex;              /*   148    76 */
            struct rw_semaphore        i_alloc_sem;          /*   224    64 */
            struct inode_operations *  i_op;                 /*   288     4 */
            const struct file_operations  * i_fop;           /*   292     4 */
            struct super_block *       i_sb;                 /*   296     4 */
            struct file_lock *         i_flock;              /*   300     4 */
            struct address_space *     i_mapping;            /*   304     4 */
            struct address_space       i_data;               /*   308   188 */
            struct list_head           i_devices;            /*   496     8 */
            union                      ;                     /*   504     4 */
            int                        i_cindex;             /*   508     4 */
            __u32                      i_generation;         /*   512     4 */
            /* ---------- cacheline 8 boundary ---------- */
            long unsigned int          i_dnotify_mask;       /*   516     4 */
            struct dnotify_struct *    i_dnotify;            /*   520     4 */
            struct list_head           inotify_watches;      /*   524     8 */
            struct mutex               inotify_mutex;        /*   532    76 */
            long unsigned int          i_state;              /*   608     4 */
            long unsigned int          dirtied_when;         /*   612     4 */
            unsigned int               i_flags;              /*   616     4 */
            atomic_t                   i_writecount;         /*   620     4 */
            void *                     i_security;           /*   624     4 */
            void *                     i_private;            /*   628     4 */
    }; /* size: 632, sum members: 628, holes: 2, sum holes: 4 */
    
    [acme@newtoy net-2.6.20]$
    
    So just moving i_mode to after i_bytes we save 4 bytes by nuking both holes:
    
    [acme@newtoy net-2.6.20]$ codiff -V /tmp/inode.o.before fs/inode.o
    /pub/scm/linux/kernel/git/acme/net-2.6.20/fs/inode.c:
      struct inode |   -4
        i_mode;
         from: umode_t               /*    40(0)     2(0) */
         to:   umode_t               /*   102(0)     2(0) */
     1 struct changed
    [acme@newtoy net-2.6.20]$
    
    I've prunned all the other offset changes, only this one is of interest here.
    
    So now we have:
    
    [acme@newtoy net-2.6.20]$ pahole --cacheline 64 ../OUTPUT/qemu/net-2.6.20/fs/inode.o inode
    /* /pub/scm/linux/kernel/git/acme/net-2.6.20/include/linux/dcache.h:86 */
    struct inode {
            struct hlist_node          i_hash;               /*     0     8 */
            struct list_head           i_list;               /*     8     8 */
            struct list_head           i_sb_list;            /*    16     8 */
            struct list_head           i_dentry;             /*    24     8 */
            long unsigned int          i_ino;                /*    32     4 */
            atomic_t                   i_count;              /*    36     4 */
            unsigned int               i_nlink;              /*    40     4 */
            uid_t                      i_uid;                /*    44     4 */
            gid_t                      i_gid;                /*    48     4 */
            dev_t                      i_rdev;               /*    52     4 */
            loff_t                     i_size;               /*    56     8 */
            /* ---------- cacheline 1 boundary ---------- */
            struct timespec            i_atime;              /*    64     8 */
            struct timespec            i_mtime;              /*    72     8 */
            struct timespec            i_ctime;              /*    80     8 */
            unsigned int               i_blkbits;            /*    88     4 */
            long unsigned int          i_version;            /*    92     4 */
            blkcnt_t                   i_blocks;             /*    96     4 */
            short unsigned int         i_bytes;              /*   100     2 */
            umode_t                    i_mode;               /*   102     2 */
            spinlock_t                 i_lock;               /*   104    40 */
            struct mutex               i_mutex;              /*   144    76 */
            struct rw_semaphore        i_alloc_sem;          /*   220    64 */
            struct inode_operations *  i_op;                 /*   284     4 */
            const struct file_operations  * i_fop;           /*   288     4 */
            struct super_block *       i_sb;                 /*   292     4 */
            struct file_lock *         i_flock;              /*   296     4 */
            struct address_space *     i_mapping;            /*   300     4 */
            struct address_space       i_data;               /*   304   188 */
            struct list_head           i_devices;            /*   492     8 */
            union                      ;                     /*   500     4 */
            int                        i_cindex;             /*   504     4 */
            __u32                      i_generation;         /*   508     4 */
            /* ---------- cacheline 8 boundary ---------- */
            long unsigned int          i_dnotify_mask;       /*   512     4 */
            struct dnotify_struct *    i_dnotify;            /*   516     4 */
            struct list_head           inotify_watches;      /*   520     8 */
            struct mutex               inotify_mutex;        /*   528    76 */
            long unsigned int          i_state;              /*   604     4 */
            long unsigned int          dirtied_when;         /*   608     4 */
            unsigned int               i_flags;              /*   612     4 */
            atomic_t                   i_writecount;         /*   616     4 */
            void *                     i_security;           /*   620     4 */
            void *                     i_private;            /*   624     4 */
    }; /* size: 628 */
    
    [acme@newtoy net-2.6.20]$
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 36de6437866bbb1d37e2312ff4f95ee4ed6d2b61
Author: Arnaldo Carvalho de Melo <acme@mandriva.com>
Date:   Wed Dec 6 20:33:42 2006 -0800

    [PATCH] Save some bytes in struct mm_struct
    
    Before:
    [acme@newtoy net-2.6.20]$ pahole --cacheline 32 kernel/sched.o mm_struct
    
    /* include2/asm/processor.h:542 */
    struct mm_struct {
            struct vm_area_struct *    mmap;                 /*     0     4 */
            struct rb_root             mm_rb;                /*     4     4 */
            struct vm_area_struct *    mmap_cache;           /*     8     4 */
            long unsigned int          (*get_unmapped_area)(); /*    12     4 */
            void                       (*unmap_area)();      /*    16     4 */
            long unsigned int          mmap_base;            /*    20     4 */
            long unsigned int          task_size;            /*    24     4 */
            long unsigned int          cached_hole_size;     /*    28     4 */
            /* ---------- cacheline 1 boundary ---------- */
            long unsigned int          free_area_cache;      /*    32     4 */
            pgd_t *                    pgd;                  /*    36     4 */
            atomic_t                   mm_users;             /*    40     4 */
            atomic_t                   mm_count;             /*    44     4 */
            int                        map_count;            /*    48     4 */
            struct rw_semaphore        mmap_sem;             /*    52    64 */
            spinlock_t                 page_table_lock;      /*   116    40 */
            struct list_head           mmlist;               /*   156     8 */
            mm_counter_t               _file_rss;            /*   164     4 */
            mm_counter_t               _anon_rss;            /*   168     4 */
            long unsigned int          hiwater_rss;          /*   172     4 */
            long unsigned int          hiwater_vm;           /*   176     4 */
            long unsigned int          total_vm;             /*   180     4 */
            long unsigned int          locked_vm;            /*   184     4 */
            long unsigned int          shared_vm;            /*   188     4 */
            /* ---------- cacheline 6 boundary ---------- */
            long unsigned int          exec_vm;              /*   192     4 */
            long unsigned int          stack_vm;             /*   196     4 */
            long unsigned int          reserved_vm;          /*   200     4 */
            long unsigned int          def_flags;            /*   204     4 */
            long unsigned int          nr_ptes;              /*   208     4 */
            long unsigned int          start_code;           /*   212     4 */
            long unsigned int          end_code;             /*   216     4 */
            long unsigned int          start_data;           /*   220     4 */
            /* ---------- cacheline 7 boundary ---------- */
            long unsigned int          end_data;             /*   224     4 */
            long unsigned int          start_brk;            /*   228     4 */
            long unsigned int          brk;                  /*   232     4 */
            long unsigned int          start_stack;          /*   236     4 */
            long unsigned int          arg_start;            /*   240     4 */
            long unsigned int          arg_end;              /*   244     4 */
            long unsigned int          env_start;            /*   248     4 */
            long unsigned int          env_end;              /*   252     4 */
            /* ---------- cacheline 8 boundary ---------- */
            long unsigned int          saved_auxv[44];       /*   256   176 */
            unsigned int               dumpable:2;           /*   432     4 */
            cpumask_t                  cpu_vm_mask;          /*   436     4 */
            mm_context_t               context;              /*   440    68 */
            long unsigned int          swap_token_time;      /*   508     4 */
            /* ---------- cacheline 16 boundary ---------- */
            char                       recent_pagein;        /*   512     1 */
    
            /* XXX 3 bytes hole, try to pack */
    
            int                        core_waiters;         /*   516     4 */
            struct completion *        core_startup_done;    /*   520     4 */
            struct completion          core_done;            /*   524    52 */
            rwlock_t                   ioctx_list_lock;      /*   576    36 */
            struct kioctx *            ioctx_list;           /*   612     4 */
    }; /* size: 616, sum members: 613, holes: 1, sum holes: 3, cachelines: 20,
          last cacheline: 8 bytes */
    
    After:
    
    [acme@newtoy net-2.6.20]$ pahole --cacheline 32 kernel/sched.o mm_struct
    /* include2/asm/processor.h:542 */
    struct mm_struct {
            struct vm_area_struct *    mmap;                 /*     0     4 */
            struct rb_root             mm_rb;                /*     4     4 */
            struct vm_area_struct *    mmap_cache;           /*     8     4 */
            long unsigned int          (*get_unmapped_area)(); /*    12     4 */
            void                       (*unmap_area)();      /*    16     4 */
            long unsigned int          mmap_base;            /*    20     4 */
            long unsigned int          task_size;            /*    24     4 */
            long unsigned int          cached_hole_size;     /*    28     4 */
            /* ---------- cacheline 1 boundary ---------- */
            long unsigned int          free_area_cache;      /*    32     4 */
            pgd_t *                    pgd;                  /*    36     4 */
            atomic_t                   mm_users;             /*    40     4 */
            atomic_t                   mm_count;             /*    44     4 */
            int                        map_count;            /*    48     4 */
            struct rw_semaphore        mmap_sem;             /*    52    64 */
            spinlock_t                 page_table_lock;      /*   116    40 */
            struct list_head           mmlist;               /*   156     8 */
            mm_counter_t               _file_rss;            /*   164     4 */
            mm_counter_t               _anon_rss;            /*   168     4 */
            long unsigned int          hiwater_rss;          /*   172     4 */
            long unsigned int          hiwater_vm;           /*   176     4 */
            long unsigned int          total_vm;             /*   180     4 */
            long unsigned int          locked_vm;            /*   184     4 */
            long unsigned int          shared_vm;            /*   188     4 */
            /* ---------- cacheline 6 boundary ---------- */
            long unsigned int          exec_vm;              /*   192     4 */
            long unsigned int          stack_vm;             /*   196     4 */
            long unsigned int          reserved_vm;          /*   200     4 */
            long unsigned int          def_flags;            /*   204     4 */
            long unsigned int          nr_ptes;              /*   208     4 */
            long unsigned int          start_code;           /*   212     4 */
            long unsigned int          end_code;             /*   216     4 */
            long unsigned int          start_data;           /*   220     4 */
            /* ---------- cacheline 7 boundary ---------- */
            long unsigned int          end_data;             /*   224     4 */
            long unsigned int          start_brk;            /*   228     4 */
            long unsigned int          brk;                  /*   232     4 */
            long unsigned int          start_stack;          /*   236     4 */
            long unsigned int          arg_start;            /*   240     4 */
            long unsigned int          arg_end;              /*   244     4 */
            long unsigned int          env_start;            /*   248     4 */
            long unsigned int          env_end;              /*   252     4 */
            /* ---------- cacheline 8 boundary ---------- */
            long unsigned int          saved_auxv[44];       /*   256   176 */
            cpumask_t                  cpu_vm_mask;          /*   432     4 */
            mm_context_t               context;              /*   436    68 */
            long unsigned int          swap_token_time;      /*   504     4 */
            char                       recent_pagein;        /*   508     1 */
            unsigned char              dumpable:2;           /*   509     1 */
    
            /* XXX 2 bytes hole, try to pack */
    
            int                        core_waiters;         /*   512     4 */
            struct completion *        core_startup_done;    /*   516     4 */
            struct completion          core_done;            /*   520    52 */
            rwlock_t                   ioctx_list_lock;      /*   572    36 */
            struct kioctx *            ioctx_list;           /*   608     4 */
    }; /* size: 612, sum members: 610, holes: 1, sum holes: 2, cachelines: 20,
          last cacheline: 4 bytes */
    
    [acme@newtoy net-2.6.20]$ codiff -V /tmp/sched.o.before kernel/sched.o
    /pub/scm/linux/kernel/git/acme/net-2.6.20/kernel/sched.c:
      struct mm_struct |   -4
        dumpable:2;
         from: unsigned int          /*   432(30)    4(2) */
         to:   unsigned char         /*   509(6)     1(2) */
    < SNIP other offset changes >
     1 struct changed
    [acme@newtoy net-2.6.20]$
    
    I'm not aware of any problem about using 2 byte wide bitfields where
    previously a 4 byte wide one was, holler if there is any, I wouldn't be
    surprised, bitfields are things from hell.
    
    For the curious, 432(30) means: at offset 432 from the struct start, at
    offset 30 in the bitfield (yeah, it comes backwards, hellish, huh?) ditto
    for 509(6), while 4(2) and 1(2) means "struct field size(bitfield size)".
    
    Now we have a 2 bytes hole and are using only 4 bytes of the last 32
    bytes cacheline, any takers? :-)
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@mandriva.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 16afea0255cf6963eb924d4334cdb5acb9074581
Author: Art Haas <ahaas@airmail.net>
Date:   Wed Dec 6 14:45:53 2006 -0600

    [PATCH] Remove 'volatile' from spinlock_types
    
    This is a resubmission of patches originally created by Ingo Molnar.
    The link below is the initial (?) posting of the patch.
    
      http://marc.theaimsgroup.com/?l=linux-kernel&m=115217423929806&w=2
    
    Remove 'volatile' from spinlock_types as it causes GCC to generate bad
    code (see link) and locking should be used on kernel data.
    
    Signed-off-by: Art Haas <ahaas@airmail.net>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 4068d93cd17561bcbfc821c831cb048385320bd6
Author: Kyle McMartin <kyle@parisc-linux.org>
Date:   Sun Aug 13 22:15:47 2006 -0400

    [PARISC] Untangle <asm/processor.h> header include mess
    
    asm/processor.h on parisc wants spinlocks for cpuinfo, but
    linux/spinlock_types.h needs lockdep, and lockdep wants prefetch.
    
    This leads to a horrible circular dependancy, because <asm/processor.h>
    is including something which depends on things which are not defined
    until the end of the file.
    
    Kludge around this by moving prefetch related code into <asm/prefetch.h>
    and including it before <linux/spinlock_types.h>, however this is just
    a temporary solution until this mess can be cleaned up.
    
    Signed-off-by: Kyle McMartin <kyle@parisc-linux.org>

commit 9f50b93f066f8dc339de9b0eb78a22a75e6c8f8f
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Fri Sep 29 02:00:59 2006 -0700

    [PATCH] Make spinlock/rwlock annotations more accurate by using parameters, not types
    
    The lock annotations used on spinlocks and rwlocks currently use
    __{acquires,releases}(spinlock_t) and __{acquires,releases}(rwlock_t),
    respectively.  This loses the information of which lock actually got
    acquired or released, and assumes a different type for the parameter of
    __acquires and __releases than the rest of the kernel.  While the current
    implementations of __acquires and __releases throw away their argument,
    this will not always remain the case.  Change this to use the lock
    parameter instead, to preserve this information and increase consistency in
    usage of __acquires and __releases.
    
    Signed-off-by: Josh Triplett <josh@freedesktop.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 2cd6b01ad38d34b805a0a052725ec3f37e387ecd
Author: Christian Borntraeger <borntrae@de.ibm.com>
Date:   Wed Aug 30 07:38:11 2006 +0200

    bug in futex unqueue_me
    
    This patch adds a barrier() in futex unqueue_me to avoid aliasing of two
    pointers.
    
    On my s390x system I saw the following oops:
    
    Unable to handle kernel pointer dereference at virtual kernel address
    0000000000000000
    Oops: 0004 [#1]
    CPU:    0    Not tainted
    Process mytool (pid: 13613, task: 000000003ecb6ac0, ksp: 00000000366bdbd8)
    Krnl PSW : 0704d00180000000 00000000003c9ac2 (_spin_lock+0xe/0x30)
    Krnl GPRS: 00000000ffffffff 000000003ecb6ac0 0000000000000000 0700000000000000
               0000000000000000 0000000000000000 000001fe00002028 00000000000c091f
               000001fe00002054 000001fe00002054 0000000000000000 00000000366bddc0
               00000000005ef8c0 00000000003d00e8 0000000000144f91 00000000366bdcb8
    Krnl Code: ba 4e 20 00 12 44 b9 16 00 3e a7 84 00 08 e3 e0 f0 88 00 04
    Call Trace:
    ([<0000000000144f90>] unqueue_me+0x40/0xe4)
     [<0000000000145a0c>] do_futex+0x33c/0xc40
     [<000000000014643e>] sys_futex+0x12e/0x144
     [<000000000010bb00>] sysc_noemu+0x10/0x16
     [<000002000003741c>] 0x2000003741c
    
    The code in question is:
    
    static int unqueue_me(struct futex_q *q)
    {
            int ret = 0;
            spinlock_t *lock_ptr;
    
            /* In the common case we don't take the spinlock, which is nice. */
     retry:
            lock_ptr = q->lock_ptr;
            if (lock_ptr != 0) {
                    spin_lock(lock_ptr);
                    /*
                     * q->lock_ptr can change between reading it and
                     * spin_lock(), causing us to take the wrong lock.  This
                     * corrects the race condition.
    [...]
    
    and my compiler (gcc 4.1.0) makes the following out of it:
    
    00000000000003c8 <unqueue_me>:
         3c8:       eb bf f0 70 00 24       stmg    %r11,%r15,112(%r15)
         3ce:       c0 d0 00 00 00 00       larl    %r13,3ce <unqueue_me+0x6>
                            3d0: R_390_PC32DBL      .rodata+0x2a
         3d4:       a7 f1 1e 00             tml     %r15,7680
         3d8:       a7 84 00 01             je      3da <unqueue_me+0x12>
         3dc:       b9 04 00 ef             lgr     %r14,%r15
         3e0:       a7 fb ff d0             aghi    %r15,-48
         3e4:       b9 04 00 b2             lgr     %r11,%r2
         3e8:       e3 e0 f0 98 00 24       stg     %r14,152(%r15)
         3ee:       e3 c0 b0 28 00 04       lg      %r12,40(%r11)
                    /* write q->lock_ptr in r12 */
         3f4:       b9 02 00 cc             ltgr    %r12,%r12
         3f8:       a7 84 00 4b             je      48e <unqueue_me+0xc6>
                    /* if r12 is zero then jump over the code.... */
         3fc:       e3 20 b0 28 00 04       lg      %r2,40(%r11)
                    /* write q->lock_ptr in r2 */
         402:       c0 e5 00 00 00 00       brasl   %r14,402 <unqueue_me+0x3a>
                            404: R_390_PC32DBL      _spin_lock+0x2
                    /* use r2 as parameter for spin_lock */
    
    So the code becomes more or less:
    if (q->lock_ptr != 0) spin_lock(q->lock_ptr)
    instead of
    if (lock_ptr != 0) spin_lock(lock_ptr)
    
    Which caused the oops from above.
    After adding a barrier gcc creates code without this problem:
    [...] (the same)
         3ee:       e3 c0 b0 28 00 04       lg      %r12,40(%r11)
         3f4:       b9 02 00 cc             ltgr    %r12,%r12
         3f8:       b9 04 00 2c             lgr     %r2,%r12
         3fc:       a7 84 00 48             je      48c <unqueue_me+0xc4>
         400:       c0 e5 00 00 00 00       brasl   %r14,400 <unqueue_me+0x38>
                            402: R_390_PC32DBL      _spin_lock+0x2
    
    As a general note, this code of unqueue_me seems a bit fishy. The retry logic
    of unqueue_me only works if we can guarantee, that the original value of
    q->lock_ptr is always a spinlock (Otherwise we overwrite kernel memory). We
    know that q->lock_ptr can change. I dont know what happens with the original
    spinlock, as I am not an expert with the futex code.
    
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@timesys.com>
    Signed-off-by: Christian Borntraeger <borntrae@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 397f9fde9b9a9149be2249e1439ccb3a74b85594
Author: Christian Borntraeger <borntrae@de.ibm.com>
Date:   Wed Sep 6 16:01:43 2006 +0200

    fix misoptimization in futex unqueue_me
    
    This patch adds a barrier() in futex unqueue_me to avoid aliasing of two
    pointers.
    
    On my s390x system I saw the following oops:
    
    Unable to handle kernel pointer dereference at virtual kernel address
    0000000000000000
    Oops: 0004 [#1]
    CPU:    0    Not tainted
    Process mytool (pid: 13613, task: 000000003ecb6ac0, ksp: 00000000366bdbd8)
    Krnl PSW : 0704d00180000000 00000000003c9ac2 (_spin_lock+0xe/0x30)
    Krnl GPRS: 00000000ffffffff 000000003ecb6ac0 0000000000000000 0700000000000000
               0000000000000000 0000000000000000 000001fe00002028 00000000000c091f
               000001fe00002054 000001fe00002054 0000000000000000 00000000366bddc0
               00000000005ef8c0 00000000003d00e8 0000000000144f91 00000000366bdcb8
    Krnl Code: ba 4e 20 00 12 44 b9 16 00 3e a7 84 00 08 e3 e0 f0 88 00 04
    Call Trace:
    ([<0000000000144f90>] unqueue_me+0x40/0xe4)
     [<0000000000145a0c>] do_futex+0x33c/0xc40
     [<000000000014643e>] sys_futex+0x12e/0x144
     [<000000000010bb00>] sysc_noemu+0x10/0x16
     [<000002000003741c>] 0x2000003741c
    
    The code in question is:
    
    static int unqueue_me(struct futex_q *q)
    {
            int ret = 0;
            spinlock_t *lock_ptr;
    
            /* In the common case we don't take the spinlock, which is nice. */
     retry:
            lock_ptr = q->lock_ptr;
            if (lock_ptr != 0) {
                    spin_lock(lock_ptr);
                    /*
                     * q->lock_ptr can change between reading it and
                     * spin_lock(), causing us to take the wrong lock.  This
                     * corrects the race condition.
    [...]
    
    and my compiler (gcc 4.1.0) makes the following out of it:
    
    00000000000003c8 <unqueue_me>:
         3c8:       eb bf f0 70 00 24       stmg    %r11,%r15,112(%r15)
         3ce:       c0 d0 00 00 00 00       larl    %r13,3ce <unqueue_me+0x6>
                            3d0: R_390_PC32DBL      .rodata+0x2a
         3d4:       a7 f1 1e 00             tml     %r15,7680
         3d8:       a7 84 00 01             je      3da <unqueue_me+0x12>
         3dc:       b9 04 00 ef             lgr     %r14,%r15
         3e0:       a7 fb ff d0             aghi    %r15,-48
         3e4:       b9 04 00 b2             lgr     %r11,%r2
         3e8:       e3 e0 f0 98 00 24       stg     %r14,152(%r15)
         3ee:       e3 c0 b0 28 00 04       lg      %r12,40(%r11)
                    /* write q->lock_ptr in r12 */
         3f4:       b9 02 00 cc             ltgr    %r12,%r12
         3f8:       a7 84 00 4b             je      48e <unqueue_me+0xc6>
                    /* if r12 is zero then jump over the code.... */
         3fc:       e3 20 b0 28 00 04       lg      %r2,40(%r11)
                    /* write q->lock_ptr in r2 */
         402:       c0 e5 00 00 00 00       brasl   %r14,402 <unqueue_me+0x3a>
                            404: R_390_PC32DBL      _spin_lock+0x2
                    /* use r2 as parameter for spin_lock */
    
    So the code becomes more or less:
    if (q->lock_ptr != 0) spin_lock(q->lock_ptr)
    instead of
    if (lock_ptr != 0) spin_lock(lock_ptr)
    
    Which caused the oops from above.
    After adding a barrier gcc creates code without this problem:
    [...] (the same)
         3ee:       e3 c0 b0 28 00 04       lg      %r12,40(%r11)
         3f4:       b9 02 00 cc             ltgr    %r12,%r12
         3f8:       b9 04 00 2c             lgr     %r2,%r12
         3fc:       a7 84 00 48             je      48c <unqueue_me+0xc4>
         400:       c0 e5 00 00 00 00       brasl   %r14,400 <unqueue_me+0x38>
                            402: R_390_PC32DBL      _spin_lock+0x2
    
    As a general note, this code of unqueue_me seems a bit fishy. The retry logic
    of unqueue_me only works if we can guarantee, that the original value of
    q->lock_ptr is always a spinlock (Otherwise we overwrite kernel memory). We
    know that q->lock_ptr can change. I dont know what happens with the original
    spinlock, as I am not an expert with the futex code.
    
    Signed-off-by: Christian Borntraeger <borntrae@de.ibm.com>
    Acked-by: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

commit e91467ecd1ef381377fd327c0ded922835ec52ab
Author: Christian Borntraeger <borntrae@de.ibm.com>
Date:   Sat Aug 5 12:13:52 2006 -0700

    [PATCH] bug in futex unqueue_me
    
    This patch adds a barrier() in futex unqueue_me to avoid aliasing of two
    pointers.
    
    On my s390x system I saw the following oops:
    
    Unable to handle kernel pointer dereference at virtual kernel address
    0000000000000000
    Oops: 0004 [#1]
    CPU:    0    Not tainted
    Process mytool (pid: 13613, task: 000000003ecb6ac0, ksp: 00000000366bdbd8)
    Krnl PSW : 0704d00180000000 00000000003c9ac2 (_spin_lock+0xe/0x30)
    Krnl GPRS: 00000000ffffffff 000000003ecb6ac0 0000000000000000 0700000000000000
               0000000000000000 0000000000000000 000001fe00002028 00000000000c091f
               000001fe00002054 000001fe00002054 0000000000000000 00000000366bddc0
               00000000005ef8c0 00000000003d00e8 0000000000144f91 00000000366bdcb8
    Krnl Code: ba 4e 20 00 12 44 b9 16 00 3e a7 84 00 08 e3 e0 f0 88 00 04
    Call Trace:
    ([<0000000000144f90>] unqueue_me+0x40/0xe4)
     [<0000000000145a0c>] do_futex+0x33c/0xc40
     [<000000000014643e>] sys_futex+0x12e/0x144
     [<000000000010bb00>] sysc_noemu+0x10/0x16
     [<000002000003741c>] 0x2000003741c
    
    The code in question is:
    
    static int unqueue_me(struct futex_q *q)
    {
            int ret = 0;
            spinlock_t *lock_ptr;
    
            /* In the common case we don't take the spinlock, which is nice. */
     retry:
            lock_ptr = q->lock_ptr;
            if (lock_ptr != 0) {
                    spin_lock(lock_ptr);
                    /*
                     * q->lock_ptr can change between reading it and
                     * spin_lock(), causing us to take the wrong lock.  This
                     * corrects the race condition.
    [...]
    
    and my compiler (gcc 4.1.0) makes the following out of it:
    
    00000000000003c8 <unqueue_me>:
         3c8:       eb bf f0 70 00 24       stmg    %r11,%r15,112(%r15)
         3ce:       c0 d0 00 00 00 00       larl    %r13,3ce <unqueue_me+0x6>
                            3d0: R_390_PC32DBL      .rodata+0x2a
         3d4:       a7 f1 1e 00             tml     %r15,7680
         3d8:       a7 84 00 01             je      3da <unqueue_me+0x12>
         3dc:       b9 04 00 ef             lgr     %r14,%r15
         3e0:       a7 fb ff d0             aghi    %r15,-48
         3e4:       b9 04 00 b2             lgr     %r11,%r2
         3e8:       e3 e0 f0 98 00 24       stg     %r14,152(%r15)
         3ee:       e3 c0 b0 28 00 04       lg      %r12,40(%r11)
                    /* write q->lock_ptr in r12 */
         3f4:       b9 02 00 cc             ltgr    %r12,%r12
         3f8:       a7 84 00 4b             je      48e <unqueue_me+0xc6>
                    /* if r12 is zero then jump over the code.... */
         3fc:       e3 20 b0 28 00 04       lg      %r2,40(%r11)
                    /* write q->lock_ptr in r2 */
         402:       c0 e5 00 00 00 00       brasl   %r14,402 <unqueue_me+0x3a>
                            404: R_390_PC32DBL      _spin_lock+0x2
                    /* use r2 as parameter for spin_lock */
    
    So the code becomes more or less:
    if (q->lock_ptr != 0) spin_lock(q->lock_ptr)
    instead of
    if (lock_ptr != 0) spin_lock(lock_ptr)
    
    Which caused the oops from above.
    After adding a barrier gcc creates code without this problem:
    [...] (the same)
         3ee:       e3 c0 b0 28 00 04       lg      %r12,40(%r11)
         3f4:       b9 02 00 cc             ltgr    %r12,%r12
         3f8:       b9 04 00 2c             lgr     %r2,%r12
         3fc:       a7 84 00 48             je      48c <unqueue_me+0xc4>
         400:       c0 e5 00 00 00 00       brasl   %r14,400 <unqueue_me+0x38>
                            402: R_390_PC32DBL      _spin_lock+0x2
    
    As a general note, this code of unqueue_me seems a bit fishy. The retry logic
    of unqueue_me only works if we can guarantee, that the original value of
    q->lock_ptr is always a spinlock (Otherwise we overwrite kernel memory). We
    know that q->lock_ptr can change. I dont know what happens with the original
    spinlock, as I am not an expert with the futex code.
    
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@timesys.com>
    Signed-off-by: Christian Borntraeger <borntrae@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 6205120044bb75ca06019491d1aa0e727fdd35be
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:59 2006 -0700

    [PATCH] lockdep: fix RT_HASH_LOCK_SZ
    
    On lockdep we have a quite big spinlock_t, so keep the size down.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 6d6e420005f3753392b608a614eee8475bdc16f7
Author: Prarit Bhargava <prarit@sgi.com>
Date:   Fri Dec 23 13:33:25 2005 -0500

    [IA64-SGI] Fix sn_flush_device_kernel & spinlock initialization
    
    This patch separates the sn_flush_device_list struct into kernel and
    common (both kernel and PROM accessible) structures.  As it was, if the
    size of a spinlock_t changed (due to additional CONFIG options, etc.) the
    sal call which populated the sn_flush_device_list structs would erroneously
    write data (and cause memory corruption and/or a panic).
    
    This patch does the following:
    
    1.  Removes sn_flush_device_list and adds sn_flush_device_common and
    sn_flush_device_kernel.
    
    2.  Adds a new SAL call to populate a sn_flush_device_common struct per
    device, not per widget as previously done.
    
    3.  Correctly initializes each device's sn_flush_device_kernel spinlock_t
    struct (before it was only doing each widget's first device).
    
    Signed-off-by: Prarit Bhargava <prarit@sgi.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>

commit 181a46a56e9f852060c54247209e93740329b6eb
Author: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
Date:   Wed Jan 4 13:56:54 2006 -0800

    [NETFILTER]: Use macro for spinlock_t/rwlock_t initializations/definition.
    
    Signed-off-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit ca4033024858fcbd392465ba9cbf4c838aedfb58
Author: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
Date:   Wed Jan 4 13:56:08 2006 -0800

    [ECONET]: Use macro for spinlock_t definition.
    
    Signed-off-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 7b6ac9dffe6f4dd8776908b234ac1410ed15f112
Author: Hugh Dickins <hugh@veritas.com>
Date:   Wed Nov 23 13:37:37 2005 -0800

    [PATCH] mm: update split ptlock Kconfig
    
    Closer attention to the arithmetic shows that neither ppc64 nor sparc really
    uses one page for multiple page tables: how on earth could they, while
    pte_alloc_one returns just a struct page pointer, with no offset?
    
    Well, arm26 manages it by returning a pte_t pointer cast to a struct page
    pointer, harumph, then compensating in its pmd_populate.  But arm26 is never
    SMP, so it's not a problem for split ptlock either.
    
    And the PA-RISC situation has been recently improved: CONFIG_PA20 works
    without the 16-byte alignment which inflated its spinlock_t.  But the current
    union of spinlock_t with private does make the 7xxx struct page significantly
    larger, even without debug, so disable its split ptlock.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 57468af3267bfb89391f9c607a9637e86e55d299
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Mon Oct 3 13:40:26 2005 +0100

    Define and initialize kdb_lock using DEFINE_SPINLOCK.
    Convert kgdb_cpulock into a raw_spinlock_t.
    
    SPIN_LOCK_UNLOCKED is deprecated and it's replacement DEFINE_SPINLOCK is
    not suitable for arrays of spinlocks.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 04d472dc83388c59deb6241e9aed841926aa1c8c
Author: Grant Grundler <grundler@parisc-linux.org>
Date:   Fri Oct 21 22:40:24 2005 -0400

    [PARISC] Move pa_tlb_lock to tlb_flush.h
    
    move pa_tlb_lock and it's primary consumers to tlb_flush.h
    Future step will be to move spinlock_t definition out of system.h.
    
    Signed-off-by: Grant Grundler <grundler@parisc-linux.org>
    
    Signed-off-by: Kyle McMartin <kyle@parisc-linux.org>

commit 3efc333e7fdb6fab9d4eae129e2b249c6483b250
Author: Kumar Gala <galak@freescale.com>
Date:   Thu Sep 22 10:13:31 2005 -0500

    [PATCH] powerpc: Fix building of power3 config on ppc32
    
    The spinlock_types.h merge renamed the structure for raw_spinlock_t to
    match ppc64.  In doing so some of the spinlock macros/functions needed to
    be updated to match.  Apparently, this seems to only be caught when
    building power3.
    
    Signed-off-by: Kumar Gala <kumar.gala@freescale.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

commit 8a4a10ae996b67c622b9f3f6a4dfea5de4500a11
Author: Kumar Gala <galak@freescale.com>
Date:   Tue Sep 20 16:33:54 2005 -0500

    [PATCH] powerpc: merge include/asm-ppc*/spinlock_types.h into include/asm-powerpc/spinlock_types.h
    
    Signed-off-by: Kumar Gala <kumar.gala@freescale.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

commit fb1c8f93d869b34cacb8b8932e2b83d96a19d720
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Sep 10 00:25:56 2005 -0700

    [PATCH] spinlock consolidation
    
    This patch (written by me and also containing many suggestions of Arjan van
    de Ven) does a major cleanup of the spinlock code.  It does the following
    things:
    
     - consolidates and enhances the spinlock/rwlock debugging code
    
     - simplifies the asm/spinlock.h files
    
     - encapsulates the raw spinlock type and moves generic spinlock
       features (such as ->break_lock) into the generic code.
    
     - cleans up the spinlock code hierarchy to get rid of the spaghetti.
    
    Most notably there's now only a single variant of the debugging code,
    located in lib/spinlock_debug.c.  (previously we had one SMP debugging
    variant per architecture, plus a separate generic one for UP builds)
    
    Also, i've enhanced the rwlock debugging facility, it will now track
    write-owners.  There is new spinlock-owner/CPU-tracking on SMP builds too.
    All locks have lockup detection now, which will work for both soft and hard
    spin/rwlock lockups.
    
    The arch-level include files now only contain the minimally necessary
    subset of the spinlock code - all the rest that can be generalized now
    lives in the generic headers:
    
     include/asm-i386/spinlock_types.h       |   16
     include/asm-x86_64/spinlock_types.h     |   16
    
    I have also split up the various spinlock variants into separate files,
    making it easier to see which does what. The new layout is:
    
       SMP                         |  UP
       ----------------------------|-----------------------------------
       asm/spinlock_types_smp.h    |  linux/spinlock_types_up.h
       linux/spinlock_types.h      |  linux/spinlock_types.h
       asm/spinlock_smp.h          |  linux/spinlock_up.h
       linux/spinlock_api_smp.h    |  linux/spinlock_api_up.h
       linux/spinlock.h            |  linux/spinlock.h
    
    /*
     * here's the role of the various spinlock/rwlock related include files:
     *
     * on SMP builds:
     *
     *  asm/spinlock_types.h: contains the raw_spinlock_t/raw_rwlock_t and the
     *                        initializers
     *
     *  linux/spinlock_types.h:
     *                        defines the generic type and initializers
     *
     *  asm/spinlock.h:       contains the __raw_spin_*()/etc. lowlevel
     *                        implementations, mostly inline assembly code
     *
     *   (also included on UP-debug builds:)
     *
     *  linux/spinlock_api_smp.h:
     *                        contains the prototypes for the _spin_*() APIs.
     *
     *  linux/spinlock.h:     builds the final spin_*() APIs.
     *
     * on UP builds:
     *
     *  linux/spinlock_type_up.h:
     *                        contains the generic, simplified UP spinlock type.
     *                        (which is an empty structure on non-debug builds)
     *
     *  linux/spinlock_types.h:
     *                        defines the generic type and initializers
     *
     *  linux/spinlock_up.h:
     *                        contains the __raw_spin_*()/etc. version of UP
     *                        builds. (which are NOPs on non-debug, non-preempt
     *                        builds)
     *
     *   (included on UP-non-debug builds:)
     *
     *  linux/spinlock_api_up.h:
     *                        builds the _spin_*() APIs.
     *
     *  linux/spinlock.h:     builds the final spin_*() APIs.
     */
    
    All SMP and UP architectures are converted by this patch.
    
    arm, i386, ia64, ppc, ppc64, s390/s390x, x64 was build-tested via
    crosscompilers.  m32r, mips, sh, sparc, have not been tested yet, but should
    be mostly fine.
    
    From: Grant Grundler <grundler@parisc-linux.org>
    
      Booted and lightly tested on a500-44 (64-bit, SMP kernel, dual CPU).
      Builds 32-bit SMP kernel (not booted or tested).  I did not try to build
      non-SMP kernels.  That should be trivial to fix up later if necessary.
    
      I converted bit ops atomic_hash lock to raw_spinlock_t.  Doing so avoids
      some ugly nesting of linux/*.h and asm/*.h files.  Those particular locks
      are well tested and contained entirely inside arch specific code.  I do NOT
      expect any new issues to arise with them.
    
     If someone does ever need to use debug/metrics with them, then they will
      need to unravel this hairball between spinlocks, atomic ops, and bit ops
      that exist only because parisc has exactly one atomic instruction: LDCW
      (load and clear word).
    
    From: "Luck, Tony" <tony.luck@intel.com>
    
       ia64 fix
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjanv@infradead.org>
    Signed-off-by: Grant Grundler <grundler@parisc-linux.org>
    Cc: Matthew Wilcox <willy@debian.org>
    Signed-off-by: Hirokazu Takata <takata@linux-m32r.org>
    Signed-off-by: Mikael Pettersson <mikpe@csd.uu.se>
    Signed-off-by: Benoit Boissinot <benoit.boissinot@ens-lyon.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 5d337b9194b1ce3b6fd5f3cb2799455ed2f9a3d1
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Sep 3 15:54:41 2005 -0700

    [PATCH] swap: swap_lock replace list+device
    
    The idea of a swap_device_lock per device, and a swap_list_lock over them all,
    is appealing; but in practice almost every holder of swap_device_lock must
    already hold swap_list_lock, which defeats the purpose of the split.
    
    The only exceptions have been swap_duplicate, valid_swaphandles and an
    untrodden path in try_to_unuse (plus a few places added in this series).
    valid_swaphandles doesn't show up high in profiles, but swap_duplicate does
    demand attention.  However, with the hold time in get_swap_pages so much
    reduced, I've not yet found a load and set of swap device priorities to show
    even swap_duplicate benefitting from the split.  Certainly the split is mere
    overhead in the common case of a single swap device.
    
    So, replace swap_list_lock and swap_device_lock by spinlock_t swap_lock
    (generally we seem to prefer an _ in the name, and not hide in a macro).
    
    If someone can show a regression in swap_duplicate, then probably we should
    add a hashlock for the swap_map entries alone (shorts being anatomic), so as
    to help the case of the single swap device too.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 7fe4c1b16854f0440939c62b8102cbf5c75e7cdc
Author: Prarit Bhargava <prarit@sgi.com>
Date:   Wed Jul 6 15:30:25 2005 -0700

    [IA64] hotplug/ia64: SN Hotplug Driver - PREEMPT/pcibus_info fix
    
    This patch fixes an issue with the PROM and a kernel running with
    CONFIG_PREEMPT enabled.  When CONFIG_PREEMPT is enabled, the size of a
    spinlock_t changes -- resulting in the PROM writing to an incorrect location.
    
    Signed-off-by: Prarit Bhargava <prarit@sgi.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>

commit 55c888d6d09a0df236adfaf8ccf06ff5d0646775
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Thu Jun 23 00:08:56 2005 -0700

    [PATCH] timers fixes/improvements
    
    This patch tries to solve following problems:
    
    1. del_timer_sync() is racy. The timer can be fired again after
       del_timer_sync have checked all cpus and before it will recheck
       timer_pending().
    
    2. It has scalability problems. All cpus are scanned to determine
       if the timer is running on that cpu.
    
       With this patch del_timer_sync is O(1) and no slower than plain
       del_timer(pending_timer), unless it has to actually wait for
       completion of the currently running timer.
    
       The only restriction is that the recurring timer should not use
       add_timer_on().
    
    3. The timers are not serialized wrt to itself.
    
       If CPU_0 does mod_timer(jiffies+1) while the timer is currently
       running on CPU 1, it is quite possible that local interrupt on
       CPU_0 will start that timer before it finished on CPU_1.
    
    4. The timers locking is suboptimal. __mod_timer() takes 3 locks
       at once and still requires wmb() in del_timer/run_timers.
    
       The new implementation takes 2 locks sequentially and does not
       need memory barriers.
    
    Currently ->base != NULL means that the timer is pending. In that case
    ->base.lock is used to lock the timer. __mod_timer also takes timer->lock
    because ->base can be == NULL.
    
    This patch uses timer->entry.next != NULL as indication that the timer is
    pending. So it does __list_del(), entry->next = NULL instead of list_del()
    when the timer is deleted.
    
    The ->base field is used for hashed locking only, it is initialized
    in init_timer() which sets ->base = per_cpu(tvec_bases). When the
    tvec_bases.lock is locked, it means that all timers which are tied
    to this base via timer->base are locked, and the base itself is locked
    too.
    
    So __run_timers/migrate_timers can safely modify all timers which could
    be found on ->tvX lists (pending timers).
    
    When the timer's base is locked, and the timer removed from ->entry list
    (which means that _run_timers/migrate_timers can't see this timer), it is
    possible to set timer->base = NULL and drop the lock: the timer remains
    locked.
    
    This patch adds lock_timer_base() helper, which waits for ->base != NULL,
    locks the ->base, and checks it is still the same.
    
    __mod_timer() schedules the timer on the local CPU and changes it's base.
    However, it does not lock both old and new bases at once. It locks the
    timer via lock_timer_base(), deletes the timer, sets ->base = NULL, and
    unlocks old base. Then __mod_timer() locks new_base, sets ->base = new_base,
    and adds this timer. This simplifies the code, because AB-BA deadlock is not
    possible. __mod_timer() also ensures that the timer's base is not changed
    while the timer's handler is running on the old base.
    
    __run_timers(), del_timer() do not change ->base anymore, they only clear
    pending flag.
    
    So del_timer_sync() can test timer->base->running_timer == timer to detect
    whether it is running or not.
    
    We don't need timer_list->lock anymore, this patch kills it.
    
    We also don't need barriers. del_timer() and __run_timers() used smp_wmb()
    before clearing timer's pending flag. It was needed because __mod_timer()
    did not lock old_base if the timer is not pending, so __mod_timer()->list_add()
    could race with del_timer()->list_del(). With this patch these functions are
    serialized through base->lock.
    
    One problem. TIMER_INITIALIZER can't use per_cpu(tvec_bases). So this patch
    adds global
    
            struct timer_base_s {
                    spinlock_t lock;
                    struct timer_list *running_timer;
            } __init_timer_base;
    
    which is used by TIMER_INITIALIZER. The corresponding fields in tvec_t_base_s
    struct are replaced by struct timer_base_s t_base.
    
    It is indeed ugly. But this can't have scalability problems. The global
    __init_timer_base.lock is used only when __mod_timer() is called for the first
    time AND the timer was compile time initialized. After that the timer migrates
    to the local CPU.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Renaud Lienhart <renaud.lienhart@free.fr>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
