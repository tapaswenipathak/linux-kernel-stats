commit 3a415d59c1dbec9d772dbfab2d2520d98360caae
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Jan 13 16:48:49 2023 +0000

    net/sched: sch_taprio: fix possible use-after-free
    
    syzbot reported a nasty crash [1] in net_tx_action() which
    made little sense until we got a repro.
    
    This repro installs a taprio qdisc, but providing an
    invalid TCA_RATE attribute.
    
    qdisc_create() has to destroy the just initialized
    taprio qdisc, and taprio_destroy() is called.
    
    However, the hrtimer used by taprio had already fired,
    therefore advance_sched() called __netif_schedule().
    
    Then net_tx_action was trying to use a destroyed qdisc.
    
    We can not undo the __netif_schedule(), so we must wait
    until one cpu serviced the qdisc before we can proceed.
    
    Many thanks to Alexander Potapenko for his help.
    
    [1]
    BUG: KMSAN: uninit-value in queued_spin_trylock include/asm-generic/qspinlock.h:94 [inline]
    BUG: KMSAN: uninit-value in do_raw_spin_trylock include/linux/spinlock.h:191 [inline]
    BUG: KMSAN: uninit-value in __raw_spin_trylock include/linux/spinlock_api_smp.h:89 [inline]
    BUG: KMSAN: uninit-value in _raw_spin_trylock+0x92/0xa0 kernel/locking/spinlock.c:138
     queued_spin_trylock include/asm-generic/qspinlock.h:94 [inline]
     do_raw_spin_trylock include/linux/spinlock.h:191 [inline]
     __raw_spin_trylock include/linux/spinlock_api_smp.h:89 [inline]
     _raw_spin_trylock+0x92/0xa0 kernel/locking/spinlock.c:138
     spin_trylock include/linux/spinlock.h:359 [inline]
     qdisc_run_begin include/net/sch_generic.h:187 [inline]
     qdisc_run+0xee/0x540 include/net/pkt_sched.h:125
     net_tx_action+0x77c/0x9a0 net/core/dev.c:5086
     __do_softirq+0x1cc/0x7fb kernel/softirq.c:571
     run_ksoftirqd+0x2c/0x50 kernel/softirq.c:934
     smpboot_thread_fn+0x554/0x9f0 kernel/smpboot.c:164
     kthread+0x31b/0x430 kernel/kthread.c:376
     ret_from_fork+0x1f/0x30
    
    Uninit was created at:
     slab_post_alloc_hook mm/slab.h:732 [inline]
     slab_alloc_node mm/slub.c:3258 [inline]
     __kmalloc_node_track_caller+0x814/0x1250 mm/slub.c:4970
     kmalloc_reserve net/core/skbuff.c:358 [inline]
     __alloc_skb+0x346/0xcf0 net/core/skbuff.c:430
     alloc_skb include/linux/skbuff.h:1257 [inline]
     nlmsg_new include/net/netlink.h:953 [inline]
     netlink_ack+0x5f3/0x12b0 net/netlink/af_netlink.c:2436
     netlink_rcv_skb+0x55d/0x6c0 net/netlink/af_netlink.c:2507
     rtnetlink_rcv+0x30/0x40 net/core/rtnetlink.c:6108
     netlink_unicast_kernel net/netlink/af_netlink.c:1319 [inline]
     netlink_unicast+0xf3b/0x1270 net/netlink/af_netlink.c:1345
     netlink_sendmsg+0x1288/0x1440 net/netlink/af_netlink.c:1921
     sock_sendmsg_nosec net/socket.c:714 [inline]
     sock_sendmsg net/socket.c:734 [inline]
     ____sys_sendmsg+0xabc/0xe90 net/socket.c:2482
     ___sys_sendmsg+0x2a1/0x3f0 net/socket.c:2536
     __sys_sendmsg net/socket.c:2565 [inline]
     __do_sys_sendmsg net/socket.c:2574 [inline]
     __se_sys_sendmsg net/socket.c:2572 [inline]
     __x64_sys_sendmsg+0x367/0x540 net/socket.c:2572
     do_syscall_x64 arch/x86/entry/common.c:50 [inline]
     do_syscall_64+0x3d/0xb0 arch/x86/entry/common.c:80
     entry_SYSCALL_64_after_hwframe+0x63/0xcd
    
    CPU: 0 PID: 13 Comm: ksoftirqd/0 Not tainted 6.0.0-rc2-syzkaller-47461-gac3859c02d7f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 07/22/2022
    
    Fixes: 5a781ccbd19e ("tc: Add support for configuring the taprio scheduler")
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Vinicius Costa Gomes <vinicius.gomes@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 5f6e430f931d245da838db3e10e918681207029b
Merge: a6e3e6f13805 980411a4d1bb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 19 07:13:33 2022 -0600

    Merge tag 'powerpc-6.2-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
    
     - Add powerpc qspinlock implementation optimised for large system
       scalability and paravirt. See the merge message for more details
    
     - Enable objtool to be built on powerpc to generate mcount locations
    
     - Use a temporary mm for code patching with the Radix MMU, so the
       writable mapping is restricted to the patching CPU
    
     - Add an option to build the 64-bit big-endian kernel with the ELFv2
       ABI
    
     - Sanitise user registers on interrupt entry on 64-bit Book3S
    
     - Many other small features and fixes
    
    Thanks to Aboorva Devarajan, Angel Iglesias, Benjamin Gray, Bjorn
    Helgaas, Bo Liu, Chen Lifu, Christoph Hellwig, Christophe JAILLET,
    Christophe Leroy, Christopher M. Riedl, Colin Ian King, Deming Wang,
    Disha Goel, Dmitry Torokhov, Finn Thain, Geert Uytterhoeven, Gustavo A.
    R. Silva, Haowen Bai, Joel Stanley, Jordan Niethe, Julia Lawall, Kajol
    Jain, Laurent Dufour, Li zeming, Miaoqian Lin, Michael Jeanson, Nathan
    Lynch, Naveen N. Rao, Nayna Jain, Nicholas Miehlbradt, Nicholas Piggin,
    Pali Rohár, Randy Dunlap, Rohan McLure, Russell Currey, Sathvika
    Vasireddy, Shaomin Deng, Stephen Kitt, Stephen Rothwell, Thomas
    Weißschuh, Tiezhu Yang, Uwe Kleine-König, Xie Shaowen, Xiu Jianfeng,
    XueBing Chen, Yang Yingliang, Zhang Jiaming, ruanjinjie, Jessica Yu,
    and Wolfram Sang.
    
    * tag 'powerpc-6.2-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (181 commits)
      powerpc/code-patching: Fix oops with DEBUG_VM enabled
      powerpc/qspinlock: Fix 32-bit build
      powerpc/prom: Fix 32-bit build
      powerpc/rtas: mandate RTAS syscall filtering
      powerpc/rtas: define pr_fmt and convert printk call sites
      powerpc/rtas: clean up includes
      powerpc/rtas: clean up rtas_error_log_max initialization
      powerpc/pseries/eeh: use correct API for error log size
      powerpc/rtas: avoid scheduling in rtas_os_term()
      powerpc/rtas: avoid device tree lookups in rtas_os_term()
      powerpc/rtasd: use correct OF API for event scan rate
      powerpc/rtas: document rtas_call()
      powerpc/pseries: unregister VPA when hot unplugging a CPU
      powerpc/pseries: reset the RCU watchdogs after a LPM
      powerpc: Take in account addition CPU node when building kexec FDT
      powerpc: export the CPU node count
      powerpc/cpuidle: Set CPUIDLE_FLAG_POLLING for snooze state
      powerpc/dts/fsl: Fix pca954x i2c-mux node names
      cxl: Remove unnecessary cxl_pci_window_alignment()
      selftests/powerpc: Fix resource leaks
      ...

commit 13959373e9c9021cc80730c7bd1242e07b10b328
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Dec 8 22:32:25 2022 +1000

    powerpc/qspinlock: Fix 32-bit build
    
    Some 32-bit configurations don't pull in the spin_begin/end/relax
    definitions. Fix is to restore a lost include.
    
    Reported-by: kernel test robot <lkp@intel.com>
    Fixes: 84990b169557 ("powerpc/qspinlock: add mcs queueing for contended waiters")
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/oe-kbuild-all/202212050224.i7uh9fOh-lkp@intel.com
    Link: https://lore.kernel.org/r/20221208123225.1566113-1-npiggin@gmail.com

commit 22db71bcba826c607324a8ee1b21f5cf7ec71e8b
Merge: 94ba4f2c33f4 0b2199841a79
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri Dec 2 18:04:56 2022 +1100

    Merge branch 'topic/qspinlock' into next
    
    Merge Nick's powerpc qspinlock implementation. From his cover letter:
    
    This replaces the generic queued spinlock code (like s390 does) with our
    own implementation.
    
    Generic PV qspinlock code is causing latency / starvation regressions on
    large systems that are resulting in hard lockups reported (mostly in
    pathoogical cases). The generic qspinlock code has a number of issues
    important for powerpc hardware and hypervisors that aren't easily solved
    without changing code that would impact other architectures. Follow
    s390's lead and implement our own for now.
    
    Issues for powerpc using generic qspinlocks:
      - The previous lock value should not be loaded with simple loads, and
        need not be passed around from previous loads or cmpxchg results,
        because powerpc uses ll/sc-style atomics which can perform more
        complex operations that do not require this. powerpc implementations
        tend to prefer loads use larx for improved coherency performance.
      - The queueing process should absolutely minimise the number of stores
        to the lock word to reduce exclusive coherency probes, important for
        large system scalability. The pending logic is counter productive
        here.
      - Non-atomic unlock for paravirt locks is important (atomic
        instructions tend to still be more expensive than x86 CPUs).
      - Yielding to the lock owner is important in the oversubscribed
        paravirt case, which requires storing the owner CPU in the lock
        word.
      - More control of lock stealing for the paravirt case is important to
        keep latency down on large systems.
      - The lock acquisition operation should always be made with a special
        variant of atomic instructions with the lock hint bit set,
        including (especially) in the queueing paths. This is more a matter
        of adding more arch lock helpers so not an insurmountable problem
        for generic code.

commit 0b2199841a7952d01a717b465df028b40b2cf3e9
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:32 2022 +1000

    powerpc/qspinlock: add compile-time tuning adjustments
    
    This adds compile-time options that allow the EH lock hint bit to be
    enabled or disabled, and adds some new options that may or may not
    help matters. To help with experimentation and tuning.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-18-npiggin@gmail.com

commit 12b459a5ebf3308e718bc1dd48acb7c4cf7f1a75
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:31 2022 +1000

    powerpc/qspinlock: provide accounting and options for sleepy locks
    
    Finding the owner or a queued waiter on a lock with a preempted vcpu is
    indicative of an oversubscribed guest causing the lock to get into
    trouble. Provide some options to detect this situation and have new CPUs
    avoid queueing for a longer time (more steal iterations) to minimise the
    problems caused by vcpu preemption on the queue.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-17-npiggin@gmail.com

commit 39dfc73596b48bb50cf7e4f3f54e38427dda5b4e
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:30 2022 +1000

    powerpc/qspinlock: allow indefinite spinning on a preempted owner
    
    Provide an option that holds off queueing indefinitely while the lock
    owner is preempted. This could reduce queueing latencies for very
    overcommitted vcpu situations.
    
    This is disabled by default.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-16-npiggin@gmail.com

commit cc79701114154efe79663ba47d9e51aad2ed3c78
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:29 2022 +1000

    powerpc/qspinlock: reduce remote node steal spins
    
    Allow for a reduction in the number of times a CPU from a different
    node than the owner can attempt to steal the lock before queueing.
    This could bias the transfer behaviour of the lock across the
    machine and reduce NUMA crossings.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-15-npiggin@gmail.com

commit 71c235027ce7940434acd3f553602ad8b5d36469
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:28 2022 +1000

    powerpc/qspinlock: use spin_begin/end API
    
    Use the spin_begin/spin_cpu_relax/spin_end APIs in qspinlock, which helps
    to prevent threads issuing a lot of expensive priority nops which may not
    have much effect due to immediately executing low then medium priority.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-14-npiggin@gmail.com

commit f61ab43cc1a6146d6eef7e0713a452c3677ad13e
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:27 2022 +1000

    powerpc/qspinlock: allow lock stealing in trylock and lock fastpath
    
    This change allows trylock to steal the lock. It also allows the initial
    lock attempt to steal the lock rather than bailing out and going to the
    slow path.
    
    This gives trylock more strength: without this a continually-contended
    lock will never permit a trylock to succeed. With this change, the
    trylock has a small but non-zero chance.
    
    It also gives the lock fastpath most of the benefit of passing the
    reservation back through to the steal loop in the slow path without the
    complexity.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-13-npiggin@gmail.com

commit be742c573fdafcfa1752642ca1c7aaf08c258128
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:26 2022 +1000

    powerpc/qspinlock: add ability to prod new queue head CPU
    
    After the head of the queue acquires the lock, it releases the
    next waiter in the queue to become the new head. Add an option
    to prod the new head if its vCPU was preempted. This may only
    have an effect if queue waiters are yielding.
    
    Disable this option by default for now, i.e., no logical change.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-12-npiggin@gmail.com

commit 28db61e207ea3890d286cff3141c1ce67346074d
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:25 2022 +1000

    powerpc/qspinlock: allow propagation of yield CPU down the queue
    
    Having all CPUs poll the lock word for the owner CPU that should be
    yielded to defeats most of the purpose of using MCS queueing for
    scalability. Yet it may be desirable for queued waiters to yield to a
    preempted owner.
    
    With this change, queue waiters never sample the owner CPU directly from
    the lock word. The queue head (which is spinning on the lock) propagates
    the owner CPU back to the next waiter if it finds the owner has been
    preempted. That waiter then propagates the owner CPU back to the next
    waiter, and so on.
    
    s390 addresses this problem differenty, by having queued waiters sample
    the lock word to find the owner at a low frequency. That has the
    advantage of being simpler, the advantage of propagation is that the
    lock word never has to be accesed by queued waiters, and the transfer of
    cache lines to transmit the owner data is only required when lock holder
    vCPU preemption occurs.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-11-npiggin@gmail.com

commit b4c3cdc1a698a2f6168768d0bed4bf062723722e
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:24 2022 +1000

    powerpc/qspinlock: allow stealing when head of queue yields
    
    If the head of queue is preventing stealing but it finds the owner vCPU
    is preempted, it will yield its cycles to the owner which could cause it
    to become preempted. Add an option to re-allow stealers before yielding,
    and disallow them again after returning from the yield.
    
    Disable this option by default for now, i.e., no logical change.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-10-npiggin@gmail.com

commit bd48287b2cf4cd6e95576db3a94fd2a7cdf9832d
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:23 2022 +1000

    powerpc/qspinlock: implement option to yield to previous node
    
    Queued waiters which are not at the head of the queue don't spin on
    the lock word but their qnode lock word, waiting for the previous queued
    CPU to release them. Add an option which allows these waiters to yield
    to the previous CPU if its vCPU is preempted.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-9-npiggin@gmail.com

commit 085f03311bcede99550e08a1f7cad41bf758b460
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:22 2022 +1000

    powerpc/qspinlock: paravirt yield to lock owner
    
    Waiters spinning on the lock word should yield to the lock owner if the
    vCPU is preempted. This improves performance when the hypervisor has
    oversubscribed physical CPUs.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-8-npiggin@gmail.com

commit e1a31e7fd7130628cfd229253da2b4630e7a809c
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:21 2022 +1000

    powerpc/qspinlock: store owner CPU in lock word
    
    Store the owner CPU number in the lock word so it may be yielded to,
    as powerpc's paravirtualised simple spinlocks do.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-7-npiggin@gmail.com

commit 0944534ef4d5cf39c8133575524be0be3337dd62
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:20 2022 +1000

    powerpc/qspinlock: theft prevention to control latency
    
    Give the queue head the ability to stop stealers. After a number of
    spins without successfully acquiring the lock, the queue head sets
    this, which halts stealing and will assure it is the next owner.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-6-npiggin@gmail.com

commit 6aa42f883c438ea132a28801bef3f86f3883d14c
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:19 2022 +1000

    powerpc/qspinlock: allow new waiters to steal the lock before queueing
    
    Allow new waiters to "steal" the lock before queueing. That is, to
    acquire it while other CPUs have queued.
    
    This particularly helps paravirt performance when physical CPUs are
    oversubscribed, by keeping the lock from becoming a strict FIFO and
    vCPU preemption causing queue train wrecks.
    
    The new __queued_spin_trylock_steal() function is put in qspinlock.h
    to save having to move it, because it will be used there by a later
    change.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-5-npiggin@gmail.com

commit b3a73b7db2b6cb3b2e5bfda5518a0e92230ef673
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:18 2022 +1000

    powerpc/qspinlock: convert atomic operations to assembly
    
    This uses more optimal ll/sc style access patterns (rather than
    cmpxchg), and also sets the EH=1 lock hint on those operations
    which acquire ownership of the lock.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-4-npiggin@gmail.com

commit 4c93c2e4b9e8988511c06b9c042f23d4b8f593ad
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:17 2022 +1000

    powerpc/qspinlock: use a half-word store to unlock to avoid larx/stcx.
    
    The first 16 bits of the lock are only modified by the owner, and other
    modifications always use atomic operations on the entire 32 bits, so
    unlocks can use plain stores on the 16 bits. This is the same kind of
    optimisation done by core qspinlock code.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-3-npiggin@gmail.com

commit 84990b169557428c318df87b7836cd15f65b62dc
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Nov 26 19:59:16 2022 +1000

    powerpc/qspinlock: add mcs queueing for contended waiters
    
    This forms the basis of the qspinlock slow path.
    
    Like generic qspinlocks and unlike the vanilla MCS algorithm, the lock
    owner does not participate in the queue, only waiters. The first waiter
    spins on the lock word, then when the lock is released it takes
    ownership and unqueues the next waiter. This is how qspinlocks can be
    implemented with the spinlock API -- lock owners don't need a node, only
    waiters do.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221126095932.1234527-2-npiggin@gmail.com

commit 9f61521c7a284e799050cd2adacc9a611bd2b491
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Mon Nov 28 13:11:13 2022 +1000

    powerpc/qspinlock: powerpc qspinlock implementation
    
    Add a powerpc specific implementation of queued spinlocks. This is the
    build framework with a very simple (non-queued) spinlock implementation
    to begin with. Later changes add queueing, and other features and
    optimisations one-at-a-time. It is done this way to more easily see how
    the queued spinlocks are built, and to make performance and correctness
    bisects more useful.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Drop paravirt.h & processor.h changes to fix 32-bit build]
    [mpe: Fix 32-bit build of qspinlock.o & disallow GENERIC_LOCKBREAK per Nick]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/CONLLQB6DCJU.2ZPOS7T6S5GRR@bobo

commit 6a211a753d1c836856cf942afbf3b6bfdcf0a5f4
Merge: 712fb83dc3f6 23df39fc6a36
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 20 10:39:45 2022 -0800

    Merge tag 'locking_urgent_for_v6.1_rc6' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fix from Borislav Petkov:
    
     - Fix a build error with clang 11
    
    * tag 'locking_urgent_for_v6.1_rc6' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking: Fix qspinlock/x86 inline asm error

commit 23df39fc6a36183af5e6e4f47523f1ad2cdc1d30
Author: Guo Jin <guoj17@chinatelecom.cn>
Date:   Tue Nov 8 14:01:26 2022 +0800

    locking: Fix qspinlock/x86 inline asm error
    
    When compiling linux 6.1.0-rc3 configured with CONFIG_64BIT=y and
    CONFIG_PARAVIRT_SPINLOCKS=y on x86_64 using LLVM 11.0, an error:
    "<inline asm> error: changed section flags for .spinlock.text,
    expected:: 0x6" occurred.
    
    The reason is the .spinlock.text in kernel/locking/qspinlock.o
    is used many times, but its flags are omitted in subsequent use.
    
    LLVM 11.0 assembler didn't permit to
    leave out flags in subsequent uses of the same sections.
    
    So this patch adds the corresponding flags to avoid above error.
    
    Fixes: 501f7f69bca1 ("locking: Add __lockfunc to slow path functions")
    Signed-off-by: Guo Jin <guoj17@chinatelecom.cn>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Nathan Chancellor <nathan@kernel.org>
    Link: https://lore.kernel.org/r/20221108060126.2505-1-guoj17@chinatelecom.cn

commit 95b8b5953a315081eadbadf49200e57d7e05aae7
Merge: 60ac35bf6b98 2c8577f5e455
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 12 10:35:20 2022 -0700

    Merge tag 'loongarch-6.1' of git://git.kernel.org/pub/scm/linux/kernel/git/chenhuacai/linux-loongson
    
    Pull LoongArch updates from Huacai Chen:
    
     - Use EXPLICIT_RELOCS (ABIv2.0)
    
     - Use generic BUG() handler
    
     - Refactor TLB/Cache operations
    
     - Add qspinlock support
    
     - Add perf events support
    
     - Add kexec/kdump support
    
     - Add BPF JIT support
    
     - Add ACPI-based laptop driver
    
     - Update the default config file
    
    * tag 'loongarch-6.1' of git://git.kernel.org/pub/scm/linux/kernel/git/chenhuacai/linux-loongson: (25 commits)
      LoongArch: Update Loongson-3 default config file
      LoongArch: Add ACPI-based generic laptop driver
      LoongArch: Add BPF JIT support
      LoongArch: Add some instruction opcodes and formats
      LoongArch: Move {signed,unsigned}_imm_check() to inst.h
      LoongArch: Add kdump support
      LoongArch: Add kexec support
      LoongArch: Use generic BUG() handler
      LoongArch: Add SysRq-x (TLB Dump) support
      LoongArch: Add perf events support
      LoongArch: Add qspinlock support
      LoongArch: Use TLB for ioremap()
      LoongArch: Support access filter to /dev/mem interface
      LoongArch: Refactor cache probe and flush methods
      LoongArch: mm: Refactor TLB exception handlers
      LoongArch: Support R_LARCH_GOT_PC_{LO12,HI20} in modules
      LoongArch: Support PC-relative relocations in modules
      LoongArch: Define ELF relocation types added in ABIv2.0
      LoongArch: Adjust symbol addressing for AS_HAS_EXPLICIT_RELOCS
      LoongArch: Add Kconfig option AS_HAS_EXPLICIT_RELOCS
      ...

commit 5f1e001be579c2b7f37e7d5ff87c208c33e90fca
Author: Huacai Chen <chenhuacai@loongson.cn>
Date:   Wed Oct 12 16:36:14 2022 +0800

    LoongArch: Add qspinlock support
    
    On NUMA system, the performance of qspinlock is better than generic
    spinlock. Below is the UnixBench test results on a 8 nodes (4 cores
    per node, 32 cores in total) machine.
    
    A. With generic spinlock:
    
    System Benchmarks Index Values               BASELINE       RESULT    INDEX
    Dhrystone 2 using register variables         116700.0  449574022.5  38523.9
    Double-Precision Whetstone                       55.0      85190.4  15489.2
    Execl Throughput                                 43.0      14696.2   3417.7
    File Copy 1024 bufsize 2000 maxblocks          3960.0     143157.8    361.5
    File Copy 256 bufsize 500 maxblocks            1655.0      37631.8    227.4
    File Copy 4096 bufsize 8000 maxblocks          5800.0     444814.2    766.9
    Pipe Throughput                               12440.0    5047490.7   4057.5
    Pipe-based Context Switching                   4000.0    2021545.7   5053.9
    Process Creation                                126.0      23829.8   1891.3
    Shell Scripts (1 concurrent)                     42.4      33756.7   7961.5
    Shell Scripts (8 concurrent)                      6.0       4062.9   6771.5
    System Call Overhead                          15000.0    2479748.6   1653.2
                                                                       ========
    System Benchmarks Index Score                                        2955.6
    
    B. With qspinlock:
    
    System Benchmarks Index Values               BASELINE       RESULT    INDEX
    Dhrystone 2 using register variables         116700.0  449467876.9  38514.8
    Double-Precision Whetstone                       55.0      85174.6  15486.3
    Execl Throughput                                 43.0      14769.1   3434.7
    File Copy 1024 bufsize 2000 maxblocks          3960.0     146150.5    369.1
    File Copy 256 bufsize 500 maxblocks            1655.0      37496.8    226.6
    File Copy 4096 bufsize 8000 maxblocks          5800.0     447527.0    771.6
    Pipe Throughput                               12440.0    5175989.2   4160.8
    Pipe-based Context Switching                   4000.0    2207747.8   5519.4
    Process Creation                                126.0      25125.5   1994.1
    Shell Scripts (1 concurrent)                     42.4      33461.2   7891.8
    Shell Scripts (8 concurrent)                      6.0       4024.7   6707.8
    System Call Overhead                          15000.0    2917278.6   1944.9
                                                                       ========
    System Benchmarks Index Score                                        3040.1
    
    Signed-off-by: Rui Wang <wangrui@loongson.cn>
    Signed-off-by: Huacai Chen <chenhuacai@loongson.cn>

commit 720dc7ab252bbdf404cab7b909e26b31e602bf7e
Author: Huacai Chen <chenhuacai@loongson.cn>
Date:   Thu Aug 25 19:34:59 2022 +0800

    LoongArch: Add subword xchg/cmpxchg emulation
    
    LoongArch only support 32-bit/64-bit xchg/cmpxchg in native. But percpu
    operation, qspinlock and some drivers need 8-bit/16-bit xchg/cmpxchg. We
    add subword xchg/cmpxchg emulation in this patch because the emulation
    has better performance than the generic implementation (on NUMA system),
    and it can fix some build errors meanwhile [1].
    
    LoongArch's guarantee for forward progress (avoid many ll/sc happening
    at the same time and no one succeeds):
    
    We have the "exclusive access (with timeout) of ll" feature to avoid
    simultaneous ll (which also blocks other memory load/store on the same
    address), and the "random delay of sc" feature to avoid simultaneous
    sc. It is a mandatory requirement for multi-core LoongArch processors
    to implement such features, only except those single-core and dual-core
    processors (they also don't support multi-chip interconnection).
    
    Feature bits are introduced in CPUCFG3, bit 3 and bit 4 [2].
    
    [1] https://lore.kernel.org/loongarch/CAAhV-H6vvkuOzy8OemWdYK3taj5Jn3bFX0ZTwE=twM8ywpBUYA@mail.gmail.com/T/#t
    [2] https://loongson.github.io/LoongArch-Documentation/LoongArch-Vol1-EN.html#_cpucfg
    
    Reported-by: Sudip Mukherjee (Codethink) <sudipm.mukherjee@gmail.com>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Rui Wang <wangrui@loongson.cn>
    Signed-off-by: Huacai Chen <chenhuacai@loongson.cn>

commit 7df9075e232e09d99cf23b657b6cb04c9506e618
Merge: 25e6bed5a648 45fef4c4b9c9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Aug 4 15:27:20 2022 -0700

    Merge tag 'csky-for-linus-6.0-rc1' of https://github.com/c-sky/csky-linux
    
    Pull csky updates from Guo Ren:
    
     - Add jump-label implementation
    
     - Add qspinlock support
    
     - Enable ARCH_INLINE_READ*/WRITE*/SPIN*
    
     - Some fixups and a coding convention
    
    * tag 'csky-for-linus-6.0-rc1' of https://github.com/c-sky/csky-linux:
      csky: abiv1: Fixup compile error
      csky: cmpxchg: Coding convention for BUILD_BUG()
      csky: Enable ARCH_INLINE_READ*/WRITE*/SPIN*
      csky: Add qspinlock support
      csky: Add jump-label implementation
      csky: Move HEAD_TEXT_SECTION out of __init_begin-end
      csky: Correct position of _stext
      csky: Use the bitmap API to allocate bitmaps
      csky/kprobe: reclaim insn_slot on kprobe unregistration

commit 45e15c1a375ea380d55880be2f8182cb737b60ed
Author: Guo Ren <guoren@linux.alibaba.com>
Date:   Sat Jul 23 21:32:34 2022 -0400

    csky: Add qspinlock support
    
    Enable qspinlock by the requirements mentioned in a8ad07e5240c9
    ("asm-generic: qspinlock: Indicate the use of mixed-size atomics").
    
    C-SKY only has "ldex/stex" for all atomic operations. So csky give a
    strong forward guarantee for "ldex/stex." That means when ldex grabbed
    the cache line into $L1, it would block other cores from snooping the
    address with several cycles. The atomic_fetch_add & xchg16 has the same
    forward guarantee level in C-SKY.
    
    Qspinlock has better code size and performance in a fast path.
    
    Signed-off-by: Guo Ren <guoren@linux.alibaba.com>
    Signed-off-by: Guo Ren <guoren@kernel.org>

commit 16477cdfefdb494235a675cc80563d736991d833
Merge: ecf0aa5317b0 b2441b3bdce6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 26 10:50:30 2022 -0700

    Merge tag 'asm-generic-5.19' of git://git.kernel.org/pub/scm/linux/kernel/git/arnd/asm-generic
    
    Pull asm-generic updates from Arnd Bergmann:
     "The asm-generic tree contains three separate changes for linux-5.19:
    
       - The h8300 architecture is retired after it has been effectively
         unmaintained for a number of years. This is the last architecture
         we supported that has no MMU implementation, but there are still a
         few architectures (arm, m68k, riscv, sh and xtensa) that support
         CPUs with and without an MMU.
    
       - A series to add a generic ticket spinlock that can be shared by
         most architectures with a working cmpxchg or ll/sc type atomic,
         including the conversion of riscv, csky and openrisc. This series
         is also a prerequisite for the loongarch64 architecture port that
         will come as a separate pull request.
    
       - A cleanup of some exported uapi header files to ensure they can be
         included from user space without relying on other kernel headers"
    
    * tag 'asm-generic-5.19' of git://git.kernel.org/pub/scm/linux/kernel/git/arnd/asm-generic:
      h8300: remove stale bindings and symlink
      sparc: add asm/stat.h to UAPI compile-test coverage
      powerpc: add asm/stat.h to UAPI compile-test coverage
      mips: add asm/stat.h to UAPI compile-test coverage
      riscv: add linux/bpf_perf_event.h to UAPI compile-test coverage
      kbuild: prevent exported headers from including <stdlib.h>, <stdbool.h>
      agpgart.h: do not include <stdlib.h> from exported header
      csky: Move to generic ticket-spinlock
      RISC-V: Move to queued RW locks
      RISC-V: Move to generic spinlocks
      openrisc: Move to ticket-spinlock
      asm-generic: qrwlock: Document the spinlock fairness requirements
      asm-generic: qspinlock: Indicate the use of mixed-size atomics
      asm-generic: ticket-lock: New generic ticket-based spinlock
      remove the h8300 architecture

commit 19bc59bbeddf07360ef8bceb420f95712977a32f
Merge: 83a7a614ce58 9282d0996936
Author: Palmer Dabbelt <palmer@rivosinc.com>
Date:   Fri May 20 10:14:08 2022 -0700

    Merge tag 'generic-ticket-spinlocks-v6' into for-next
    
    asm-generic: New generic ticket-based spinlock
    
    This contains a new ticket-based spinlock that uses only generic
    atomics and doesn't require as much from the memory system as qspinlock
    does in order to be fair.  It also includes a bit of documentation about
    the qspinlock and qrwlock fairness requirements.
    
    This will soon be used by a handful of architectures that don't meet the
    qspinlock requirements.
    
    * tag 'generic-ticket-spinlocks-v6':
      csky: Move to generic ticket-spinlock
      RISC-V: Move to queued RW locks
      RISC-V: Move to generic spinlocks
      openrisc: Move to ticket-spinlock
      asm-generic: qrwlock: Document the spinlock fairness requirements
      asm-generic: qspinlock: Indicate the use of mixed-size atomics
      asm-generic: ticket-lock: New generic ticket-based spinlock

commit 03a679a1a4ec8fbe44119a6d06eeabdf7944883d
Merge: fba2689ee77e 9282d0996936
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed May 11 20:52:52 2022 +0200

    Merge tag 'generic-ticket-spinlocks-v6' of git://git.kernel.org/pub/scm/linux/kernel/git/palmer/linux into asm-generic
    
    asm-generic: New generic ticket-based spinlock
    
    This contains a new ticket-based spinlock that uses only generic
    atomics and doesn't require as much from the memory system as qspinlock
    does in order to be fair.  It also includes a bit of documentation about
    the qspinlock and qrwlock fairness requirements.
    
    This will soon be used by a handful of architectures that don't meet the
    qspinlock requirements.
    
    * tag 'generic-ticket-spinlocks-v6' of git://git.kernel.org/pub/scm/linux/kernel/git/palmer/linux:
      csky: Move to generic ticket-spinlock
      RISC-V: Move to queued RW locks
      RISC-V: Move to generic spinlocks
      openrisc: Move to ticket-spinlock
      asm-generic: qrwlock: Document the spinlock fairness requirements
      asm-generic: qspinlock: Indicate the use of mixed-size atomics
      asm-generic: ticket-lock: New generic ticket-based spinlock

commit 205bf39a3441fde4fcd3931a28f4720b20ca68f7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Mar 16 16:03:58 2022 -0700

    openrisc: Move to ticket-spinlock
    
    We have no indications that openrisc meets the qspinlock requirements,
    so move to ticket-spinlock as that is more likey to be correct.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Stafford Horne <shorne@gmail.com>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Palmer Dabbelt <palmer@rivosinc.com>

commit a8ad07e5240c9e78633270be2fa2356b7e0f0af5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Mar 16 15:48:29 2022 -0700

    asm-generic: qspinlock: Indicate the use of mixed-size atomics
    
    The qspinlock implementation depends on having well behaved mixed-size
    atomics.  This is true on the more widely-used platforms, but these
    requirements are somewhat subtle and may not be satisfied by all the
    platforms that qspinlock is used on.
    
    Document these requirements, so ports that use qspinlock can more easily
    determine if they meet these requirements.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Palmer Dabbelt <palmer@rivosinc.com>

commit 1bce11126d57dde90a02ecf9bfe98175ab4e729e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Mar 16 15:53:54 2022 -0700

    asm-generic: ticket-lock: New generic ticket-based spinlock
    
    This is a simple, fair spinlock.  Specifically it doesn't have all the
    subtle memory model dependencies that qspinlock has, which makes it more
    suitable for simple systems as it is more likely to be correct.  It is
    implemented entirely in terms of standard atomics and thus works fine
    without any arch-specific code.
    
    This replaces the existing asm-generic/spinlock.h, which just errored
    out on SMP systems.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Heiko Stuebner <heiko@sntech.de>
    Reviewed-by: Guo Ren <guoren@kernel.org>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Palmer Dabbelt <palmer@rivosinc.com>

commit ee042be16cb455116d0fe99b77c6bc8baf87c8c6
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Tue Mar 22 11:57:09 2022 -0700

    locking: Apply contention tracepoints in the slow path
    
    Adding the lock contention tracepoints in various lock function slow
    paths.  Note that each arch can define spinlock differently, I only
    added it only to the generic qspinlock for now.
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Hyeonggon Yoo <42.hyeyoo@gmail.com>
    Link: https://lkml.kernel.org/r/20220322185709.141236-3-namhyung@kernel.org

commit 4e68c4b475672592c71b4a7d1f1aff93c419e451
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Fri Feb 18 00:10:38 2022 -0800

    x86/kvm: Don't use pv tlb/ipi/sched_yield if on 1 vCPU
    
    [ Upstream commit ec756e40e271866f951d77c5e923d8deb6002b15 ]
    
    Inspired by commit 3553ae5690a (x86/kvm: Don't use pvqspinlock code if
    only 1 vCPU), on a VM with only 1 vCPU, there is no need to enable
    pv tlb/ipi/sched_yield and we can save the memory for __pv_cpu_mask.
    
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Message-Id: <1645171838-2855-1-git-send-email-wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 1280c8ae9745f6132cdd741d750c98b325f9536b
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Fri Feb 18 00:10:38 2022 -0800

    x86/kvm: Don't use pv tlb/ipi/sched_yield if on 1 vCPU
    
    [ Upstream commit ec756e40e271866f951d77c5e923d8deb6002b15 ]
    
    Inspired by commit 3553ae5690a (x86/kvm: Don't use pvqspinlock code if
    only 1 vCPU), on a VM with only 1 vCPU, there is no need to enable
    pv tlb/ipi/sched_yield and we can save the memory for __pv_cpu_mask.
    
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Message-Id: <1645171838-2855-1-git-send-email-wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 132de3a6cc19680bdd0d28cd5b79f91d0edb132f
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 15 15:53:05 2022 -0800

    net: sched: limit TC_ACT_REPEAT loops
    
    commit 5740d068909676d4bdb5c9c00c37a83df7728909 upstream.
    
    We have been living dangerously, at the mercy of malicious users,
    abusing TC_ACT_REPEAT, as shown by this syzpot report [1].
    
    Add an arbitrary limit (32) to the number of times an action can
    return TC_ACT_REPEAT.
    
    v2: switch the limit to 32 instead of 10.
        Use net_warn_ratelimited() instead of pr_err_once().
    
    [1] (C repro available on demand)
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    1-...!: (10500 ticks this GP) idle=021/1/0x4000000000000000 softirq=5592/5592 fqs=0
            (t=10502 jiffies g=5305 q=190)
    rcu: rcu_preempt kthread timer wakeup didn't happen for 10502 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402
    rcu:    Possible timer handling issue on cpu=0 timer-softirq=3527
    rcu: rcu_preempt kthread starved for 10505 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu:    Unless rcu_preempt kthread gets sufficient CPU time, OOM is now expected behavior.
    rcu: RCU grace-period kthread stack dump:
    task:rcu_preempt     state:I stack:29344 pid:   14 ppid:     2 flags:0x00004000
    Call Trace:
     <TASK>
     context_switch kernel/sched/core.c:4986 [inline]
     __schedule+0xab2/0x4db0 kernel/sched/core.c:6295
     schedule+0xd2/0x260 kernel/sched/core.c:6368
     schedule_timeout+0x14a/0x2a0 kernel/time/timer.c:1881
     rcu_gp_fqs_loop+0x186/0x810 kernel/rcu/tree.c:1963
     rcu_gp_kthread+0x1de/0x320 kernel/rcu/tree.c:2136
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    rcu: Stack dump where RCU GP kthread last ran:
    Sending NMI from CPU 1 to CPUs 0:
    NMI backtrace for cpu 0
    CPU: 0 PID: 3646 Comm: syz-executor358 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:rep_nop arch/x86/include/asm/vdso/processor.h:13 [inline]
    RIP: 0010:cpu_relax arch/x86/include/asm/vdso/processor.h:18 [inline]
    RIP: 0010:pv_wait_head_or_lock kernel/locking/qspinlock_paravirt.h:437 [inline]
    RIP: 0010:__pv_queued_spin_lock_slowpath+0x3b8/0xb40 kernel/locking/qspinlock.c:508
    Code: 48 89 eb c6 45 01 01 41 bc 00 80 00 00 48 c1 e9 03 83 e3 07 41 be 01 00 00 00 48 b8 00 00 00 00 00 fc ff df 4c 8d 2c 01 eb 0c <f3> 90 41 83 ec 01 0f 84 72 04 00 00 41 0f b6 45 00 38 d8 7f 08 84
    RSP: 0018:ffffc9000283f1b0 EFLAGS: 00000206
    RAX: 0000000000000003 RBX: 0000000000000000 RCX: 1ffff1100fc0071e
    RDX: 0000000000000001 RSI: 0000000000000201 RDI: 0000000000000000
    RBP: ffff88807e0038f0 R08: 0000000000000001 R09: ffffffff8ffbf9ff
    R10: 0000000000000001 R11: 0000000000000001 R12: 0000000000004c1e
    R13: ffffed100fc0071e R14: 0000000000000001 R15: ffff8880b9c3aa80
    FS:  00005555562bf300(0000) GS:ffff8880b9c00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007ffdbfef12b8 CR3: 00000000723c2000 CR4: 00000000003506f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     <TASK>
     pv_queued_spin_lock_slowpath arch/x86/include/asm/paravirt.h:591 [inline]
     queued_spin_lock_slowpath arch/x86/include/asm/qspinlock.h:51 [inline]
     queued_spin_lock include/asm-generic/qspinlock.h:85 [inline]
     do_raw_spin_lock+0x200/0x2b0 kernel/locking/spinlock_debug.c:115
     spin_lock_bh include/linux/spinlock.h:354 [inline]
     sch_tree_lock include/net/sch_generic.h:610 [inline]
     sch_tree_lock include/net/sch_generic.h:605 [inline]
     prio_tune+0x3b9/0xb50 net/sched/sch_prio.c:211
     prio_init+0x5c/0x80 net/sched/sch_prio.c:244
     qdisc_create.constprop.0+0x44a/0x10f0 net/sched/sch_api.c:1253
     tc_modify_qdisc+0x4c5/0x1980 net/sched/sch_api.c:1660
     rtnetlink_rcv_msg+0x413/0xb80 net/core/rtnetlink.c:5594
     netlink_rcv_skb+0x153/0x420 net/netlink/af_netlink.c:2494
     netlink_unicast_kernel net/netlink/af_netlink.c:1317 [inline]
     netlink_unicast+0x539/0x7e0 net/netlink/af_netlink.c:1343
     netlink_sendmsg+0x904/0xe00 net/netlink/af_netlink.c:1919
     sock_sendmsg_nosec net/socket.c:705 [inline]
     sock_sendmsg+0xcf/0x120 net/socket.c:725
     ____sys_sendmsg+0x6e8/0x810 net/socket.c:2413
     ___sys_sendmsg+0xf3/0x170 net/socket.c:2467
     __sys_sendmsg+0xe5/0x1b0 net/socket.c:2496
     do_syscall_x64 arch/x86/entry/common.c:50 [inline]
     do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80
     entry_SYSCALL_64_after_hwframe+0x44/0xae
    RIP: 0033:0x7f7ee98aae99
    Code: 28 00 00 00 75 05 48 83 c4 28 c3 e8 41 15 00 00 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 c0 ff ff ff f7 d8 64 89 01 48
    RSP: 002b:00007ffdbfef12d8 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
    RAX: ffffffffffffffda RBX: 00007ffdbfef1300 RCX: 00007f7ee98aae99
    RDX: 0000000000000000 RSI: 0000000020000000 RDI: 0000000000000003
    RBP: 0000000000000000 R08: 000000000000000d R09: 000000000000000d
    R10: 000000000000000d R11: 0000000000000246 R12: 00007ffdbfef12f0
    R13: 00000000000f4240 R14: 000000000004ca47 R15: 00007ffdbfef12e4
     </TASK>
    INFO: NMI handler (nmi_cpu_backtrace_handler) took too long to run: 2.293 msecs
    NMI backtrace for cpu 1
    CPU: 1 PID: 3260 Comm: kworker/1:3 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Workqueue: mld mld_ifc_work
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     nmi_cpu_backtrace.cold+0x47/0x144 lib/nmi_backtrace.c:111
     nmi_trigger_cpumask_backtrace+0x1b3/0x230 lib/nmi_backtrace.c:62
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x25e/0x3f0 kernel/rcu/tree_stall.h:343
     print_cpu_stall kernel/rcu/tree_stall.h:604 [inline]
     check_cpu_stall kernel/rcu/tree_stall.h:688 [inline]
     rcu_pending kernel/rcu/tree.c:3919 [inline]
     rcu_sched_clock_irq.cold+0x5c/0x759 kernel/rcu/tree.c:2617
     update_process_times+0x16d/0x200 kernel/time/timer.c:1785
     tick_sched_handle+0x9b/0x180 kernel/time/tick-sched.c:226
     tick_sched_timer+0x1b0/0x2d0 kernel/time/tick-sched.c:1428
     __run_hrtimer kernel/time/hrtimer.c:1685 [inline]
     __hrtimer_run_queues+0x1c0/0xe50 kernel/time/hrtimer.c:1749
     hrtimer_interrupt+0x31c/0x790 kernel/time/hrtimer.c:1811
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1086 [inline]
     __sysvec_apic_timer_interrupt+0x146/0x530 arch/x86/kernel/apic/apic.c:1103
     sysvec_apic_timer_interrupt+0x8e/0xc0 arch/x86/kernel/apic/apic.c:1097
     </IRQ>
     <TASK>
     asm_sysvec_apic_timer_interrupt+0x12/0x20 arch/x86/include/asm/idtentry.h:638
    RIP: 0010:__sanitizer_cov_trace_const_cmp4+0xc/0x70 kernel/kcov.c:286
    Code: 00 00 00 48 89 7c 30 e8 48 89 4c 30 f0 4c 89 54 d8 20 48 89 10 5b c3 0f 1f 80 00 00 00 00 41 89 f8 bf 03 00 00 00 4c 8b 14 24 <89> f1 65 48 8b 34 25 00 70 02 00 e8 14 f9 ff ff 84 c0 74 4b 48 8b
    RSP: 0018:ffffc90002c5eea8 EFLAGS: 00000246
    RAX: 0000000000000007 RBX: ffff88801c625800 RCX: 0000000000000000
    RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000003
    RBP: ffff8880137d3100 R08: 0000000000000000 R09: 0000000000000000
    R10: ffffffff874fcd88 R11: 0000000000000000 R12: ffff88801d692dc0
    R13: ffff8880137d3104 R14: 0000000000000000 R15: ffff88801d692de8
     tcf_police_act+0x358/0x11d0 net/sched/act_police.c:256
     tcf_action_exec net/sched/act_api.c:1049 [inline]
     tcf_action_exec+0x1a6/0x530 net/sched/act_api.c:1026
     tcf_exts_exec include/net/pkt_cls.h:326 [inline]
     route4_classify+0xef0/0x1400 net/sched/cls_route.c:179
     __tcf_classify net/sched/cls_api.c:1549 [inline]
     tcf_classify+0x3e8/0x9d0 net/sched/cls_api.c:1615
     prio_classify net/sched/sch_prio.c:42 [inline]
     prio_enqueue+0x3a7/0x790 net/sched/sch_prio.c:75
     dev_qdisc_enqueue+0x40/0x300 net/core/dev.c:3668
     __dev_xmit_skb net/core/dev.c:3756 [inline]
     __dev_queue_xmit+0x1f61/0x3660 net/core/dev.c:4081
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip_finish_output2+0x14dc/0x2170 net/ipv4/ip_output.c:228
     __ip_finish_output net/ipv4/ip_output.c:306 [inline]
     __ip_finish_output+0x396/0x650 net/ipv4/ip_output.c:288
     ip_finish_output+0x32/0x200 net/ipv4/ip_output.c:316
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip_output+0x196/0x310 net/ipv4/ip_output.c:430
     dst_output include/net/dst.h:451 [inline]
     ip_local_out+0xaf/0x1a0 net/ipv4/ip_output.c:126
     iptunnel_xmit+0x628/0xa50 net/ipv4/ip_tunnel_core.c:82
     geneve_xmit_skb drivers/net/geneve.c:966 [inline]
     geneve_xmit+0x10c8/0x3530 drivers/net/geneve.c:1077
     __netdev_start_xmit include/linux/netdevice.h:4683 [inline]
     netdev_start_xmit include/linux/netdevice.h:4697 [inline]
     xmit_one net/core/dev.c:3473 [inline]
     dev_hard_start_xmit+0x1eb/0x920 net/core/dev.c:3489
     __dev_queue_xmit+0x2985/0x3660 net/core/dev.c:4116
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip6_finish_output2+0xf7a/0x14f0 net/ipv6/ip6_output.c:126
     __ip6_finish_output net/ipv6/ip6_output.c:191 [inline]
     __ip6_finish_output+0x61e/0xe90 net/ipv6/ip6_output.c:170
     ip6_finish_output+0x32/0x200 net/ipv6/ip6_output.c:201
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip6_output+0x1e4/0x530 net/ipv6/ip6_output.c:224
     dst_output include/net/dst.h:451 [inline]
     NF_HOOK include/linux/netfilter.h:307 [inline]
     NF_HOOK include/linux/netfilter.h:301 [inline]
     mld_sendpack+0x9a3/0xe40 net/ipv6/mcast.c:1826
     mld_send_cr net/ipv6/mcast.c:2127 [inline]
     mld_ifc_work+0x71c/0xdc0 net/ipv6/mcast.c:2659
     process_one_work+0x9ac/0x1650 kernel/workqueue.c:2307
     worker_thread+0x657/0x1110 kernel/workqueue.c:2454
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    ----------------
    Code disassembly (best guess):
       0:   48 89 eb                mov    %rbp,%rbx
       3:   c6 45 01 01             movb   $0x1,0x1(%rbp)
       7:   41 bc 00 80 00 00       mov    $0x8000,%r12d
       d:   48 c1 e9 03             shr    $0x3,%rcx
      11:   83 e3 07                and    $0x7,%ebx
      14:   41 be 01 00 00 00       mov    $0x1,%r14d
      1a:   48 b8 00 00 00 00 00    movabs $0xdffffc0000000000,%rax
      21:   fc ff df
      24:   4c 8d 2c 01             lea    (%rcx,%rax,1),%r13
      28:   eb 0c                   jmp    0x36
    * 2a:   f3 90                   pause <-- trapping instruction
      2c:   41 83 ec 01             sub    $0x1,%r12d
      30:   0f 84 72 04 00 00       je     0x4a8
      36:   41 0f b6 45 00          movzbl 0x0(%r13),%eax
      3b:   38 d8                   cmp    %bl,%al
      3d:   7f 08                   jg     0x47
      3f:   84                      .byte 0x84
    
    Fixes: 1da177e4c3f4 ("Linux-2.6.12-rc2")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Jamal Hadi Salim <jhs@mojatatu.com>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Jiri Pirko <jiri@resnulli.us>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Link: https://lore.kernel.org/r/20220215235305.3272331-1-eric.dumazet@gmail.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f4a821b098c5ee2cdc294ae5fc3b97dfd884d3e4
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 15 15:53:05 2022 -0800

    net: sched: limit TC_ACT_REPEAT loops
    
    commit 5740d068909676d4bdb5c9c00c37a83df7728909 upstream.
    
    We have been living dangerously, at the mercy of malicious users,
    abusing TC_ACT_REPEAT, as shown by this syzpot report [1].
    
    Add an arbitrary limit (32) to the number of times an action can
    return TC_ACT_REPEAT.
    
    v2: switch the limit to 32 instead of 10.
        Use net_warn_ratelimited() instead of pr_err_once().
    
    [1] (C repro available on demand)
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    1-...!: (10500 ticks this GP) idle=021/1/0x4000000000000000 softirq=5592/5592 fqs=0
            (t=10502 jiffies g=5305 q=190)
    rcu: rcu_preempt kthread timer wakeup didn't happen for 10502 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402
    rcu:    Possible timer handling issue on cpu=0 timer-softirq=3527
    rcu: rcu_preempt kthread starved for 10505 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu:    Unless rcu_preempt kthread gets sufficient CPU time, OOM is now expected behavior.
    rcu: RCU grace-period kthread stack dump:
    task:rcu_preempt     state:I stack:29344 pid:   14 ppid:     2 flags:0x00004000
    Call Trace:
     <TASK>
     context_switch kernel/sched/core.c:4986 [inline]
     __schedule+0xab2/0x4db0 kernel/sched/core.c:6295
     schedule+0xd2/0x260 kernel/sched/core.c:6368
     schedule_timeout+0x14a/0x2a0 kernel/time/timer.c:1881
     rcu_gp_fqs_loop+0x186/0x810 kernel/rcu/tree.c:1963
     rcu_gp_kthread+0x1de/0x320 kernel/rcu/tree.c:2136
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    rcu: Stack dump where RCU GP kthread last ran:
    Sending NMI from CPU 1 to CPUs 0:
    NMI backtrace for cpu 0
    CPU: 0 PID: 3646 Comm: syz-executor358 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:rep_nop arch/x86/include/asm/vdso/processor.h:13 [inline]
    RIP: 0010:cpu_relax arch/x86/include/asm/vdso/processor.h:18 [inline]
    RIP: 0010:pv_wait_head_or_lock kernel/locking/qspinlock_paravirt.h:437 [inline]
    RIP: 0010:__pv_queued_spin_lock_slowpath+0x3b8/0xb40 kernel/locking/qspinlock.c:508
    Code: 48 89 eb c6 45 01 01 41 bc 00 80 00 00 48 c1 e9 03 83 e3 07 41 be 01 00 00 00 48 b8 00 00 00 00 00 fc ff df 4c 8d 2c 01 eb 0c <f3> 90 41 83 ec 01 0f 84 72 04 00 00 41 0f b6 45 00 38 d8 7f 08 84
    RSP: 0018:ffffc9000283f1b0 EFLAGS: 00000206
    RAX: 0000000000000003 RBX: 0000000000000000 RCX: 1ffff1100fc0071e
    RDX: 0000000000000001 RSI: 0000000000000201 RDI: 0000000000000000
    RBP: ffff88807e0038f0 R08: 0000000000000001 R09: ffffffff8ffbf9ff
    R10: 0000000000000001 R11: 0000000000000001 R12: 0000000000004c1e
    R13: ffffed100fc0071e R14: 0000000000000001 R15: ffff8880b9c3aa80
    FS:  00005555562bf300(0000) GS:ffff8880b9c00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007ffdbfef12b8 CR3: 00000000723c2000 CR4: 00000000003506f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     <TASK>
     pv_queued_spin_lock_slowpath arch/x86/include/asm/paravirt.h:591 [inline]
     queued_spin_lock_slowpath arch/x86/include/asm/qspinlock.h:51 [inline]
     queued_spin_lock include/asm-generic/qspinlock.h:85 [inline]
     do_raw_spin_lock+0x200/0x2b0 kernel/locking/spinlock_debug.c:115
     spin_lock_bh include/linux/spinlock.h:354 [inline]
     sch_tree_lock include/net/sch_generic.h:610 [inline]
     sch_tree_lock include/net/sch_generic.h:605 [inline]
     prio_tune+0x3b9/0xb50 net/sched/sch_prio.c:211
     prio_init+0x5c/0x80 net/sched/sch_prio.c:244
     qdisc_create.constprop.0+0x44a/0x10f0 net/sched/sch_api.c:1253
     tc_modify_qdisc+0x4c5/0x1980 net/sched/sch_api.c:1660
     rtnetlink_rcv_msg+0x413/0xb80 net/core/rtnetlink.c:5594
     netlink_rcv_skb+0x153/0x420 net/netlink/af_netlink.c:2494
     netlink_unicast_kernel net/netlink/af_netlink.c:1317 [inline]
     netlink_unicast+0x539/0x7e0 net/netlink/af_netlink.c:1343
     netlink_sendmsg+0x904/0xe00 net/netlink/af_netlink.c:1919
     sock_sendmsg_nosec net/socket.c:705 [inline]
     sock_sendmsg+0xcf/0x120 net/socket.c:725
     ____sys_sendmsg+0x6e8/0x810 net/socket.c:2413
     ___sys_sendmsg+0xf3/0x170 net/socket.c:2467
     __sys_sendmsg+0xe5/0x1b0 net/socket.c:2496
     do_syscall_x64 arch/x86/entry/common.c:50 [inline]
     do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80
     entry_SYSCALL_64_after_hwframe+0x44/0xae
    RIP: 0033:0x7f7ee98aae99
    Code: 28 00 00 00 75 05 48 83 c4 28 c3 e8 41 15 00 00 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 c0 ff ff ff f7 d8 64 89 01 48
    RSP: 002b:00007ffdbfef12d8 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
    RAX: ffffffffffffffda RBX: 00007ffdbfef1300 RCX: 00007f7ee98aae99
    RDX: 0000000000000000 RSI: 0000000020000000 RDI: 0000000000000003
    RBP: 0000000000000000 R08: 000000000000000d R09: 000000000000000d
    R10: 000000000000000d R11: 0000000000000246 R12: 00007ffdbfef12f0
    R13: 00000000000f4240 R14: 000000000004ca47 R15: 00007ffdbfef12e4
     </TASK>
    INFO: NMI handler (nmi_cpu_backtrace_handler) took too long to run: 2.293 msecs
    NMI backtrace for cpu 1
    CPU: 1 PID: 3260 Comm: kworker/1:3 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Workqueue: mld mld_ifc_work
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     nmi_cpu_backtrace.cold+0x47/0x144 lib/nmi_backtrace.c:111
     nmi_trigger_cpumask_backtrace+0x1b3/0x230 lib/nmi_backtrace.c:62
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x25e/0x3f0 kernel/rcu/tree_stall.h:343
     print_cpu_stall kernel/rcu/tree_stall.h:604 [inline]
     check_cpu_stall kernel/rcu/tree_stall.h:688 [inline]
     rcu_pending kernel/rcu/tree.c:3919 [inline]
     rcu_sched_clock_irq.cold+0x5c/0x759 kernel/rcu/tree.c:2617
     update_process_times+0x16d/0x200 kernel/time/timer.c:1785
     tick_sched_handle+0x9b/0x180 kernel/time/tick-sched.c:226
     tick_sched_timer+0x1b0/0x2d0 kernel/time/tick-sched.c:1428
     __run_hrtimer kernel/time/hrtimer.c:1685 [inline]
     __hrtimer_run_queues+0x1c0/0xe50 kernel/time/hrtimer.c:1749
     hrtimer_interrupt+0x31c/0x790 kernel/time/hrtimer.c:1811
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1086 [inline]
     __sysvec_apic_timer_interrupt+0x146/0x530 arch/x86/kernel/apic/apic.c:1103
     sysvec_apic_timer_interrupt+0x8e/0xc0 arch/x86/kernel/apic/apic.c:1097
     </IRQ>
     <TASK>
     asm_sysvec_apic_timer_interrupt+0x12/0x20 arch/x86/include/asm/idtentry.h:638
    RIP: 0010:__sanitizer_cov_trace_const_cmp4+0xc/0x70 kernel/kcov.c:286
    Code: 00 00 00 48 89 7c 30 e8 48 89 4c 30 f0 4c 89 54 d8 20 48 89 10 5b c3 0f 1f 80 00 00 00 00 41 89 f8 bf 03 00 00 00 4c 8b 14 24 <89> f1 65 48 8b 34 25 00 70 02 00 e8 14 f9 ff ff 84 c0 74 4b 48 8b
    RSP: 0018:ffffc90002c5eea8 EFLAGS: 00000246
    RAX: 0000000000000007 RBX: ffff88801c625800 RCX: 0000000000000000
    RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000003
    RBP: ffff8880137d3100 R08: 0000000000000000 R09: 0000000000000000
    R10: ffffffff874fcd88 R11: 0000000000000000 R12: ffff88801d692dc0
    R13: ffff8880137d3104 R14: 0000000000000000 R15: ffff88801d692de8
     tcf_police_act+0x358/0x11d0 net/sched/act_police.c:256
     tcf_action_exec net/sched/act_api.c:1049 [inline]
     tcf_action_exec+0x1a6/0x530 net/sched/act_api.c:1026
     tcf_exts_exec include/net/pkt_cls.h:326 [inline]
     route4_classify+0xef0/0x1400 net/sched/cls_route.c:179
     __tcf_classify net/sched/cls_api.c:1549 [inline]
     tcf_classify+0x3e8/0x9d0 net/sched/cls_api.c:1615
     prio_classify net/sched/sch_prio.c:42 [inline]
     prio_enqueue+0x3a7/0x790 net/sched/sch_prio.c:75
     dev_qdisc_enqueue+0x40/0x300 net/core/dev.c:3668
     __dev_xmit_skb net/core/dev.c:3756 [inline]
     __dev_queue_xmit+0x1f61/0x3660 net/core/dev.c:4081
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip_finish_output2+0x14dc/0x2170 net/ipv4/ip_output.c:228
     __ip_finish_output net/ipv4/ip_output.c:306 [inline]
     __ip_finish_output+0x396/0x650 net/ipv4/ip_output.c:288
     ip_finish_output+0x32/0x200 net/ipv4/ip_output.c:316
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip_output+0x196/0x310 net/ipv4/ip_output.c:430
     dst_output include/net/dst.h:451 [inline]
     ip_local_out+0xaf/0x1a0 net/ipv4/ip_output.c:126
     iptunnel_xmit+0x628/0xa50 net/ipv4/ip_tunnel_core.c:82
     geneve_xmit_skb drivers/net/geneve.c:966 [inline]
     geneve_xmit+0x10c8/0x3530 drivers/net/geneve.c:1077
     __netdev_start_xmit include/linux/netdevice.h:4683 [inline]
     netdev_start_xmit include/linux/netdevice.h:4697 [inline]
     xmit_one net/core/dev.c:3473 [inline]
     dev_hard_start_xmit+0x1eb/0x920 net/core/dev.c:3489
     __dev_queue_xmit+0x2985/0x3660 net/core/dev.c:4116
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip6_finish_output2+0xf7a/0x14f0 net/ipv6/ip6_output.c:126
     __ip6_finish_output net/ipv6/ip6_output.c:191 [inline]
     __ip6_finish_output+0x61e/0xe90 net/ipv6/ip6_output.c:170
     ip6_finish_output+0x32/0x200 net/ipv6/ip6_output.c:201
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip6_output+0x1e4/0x530 net/ipv6/ip6_output.c:224
     dst_output include/net/dst.h:451 [inline]
     NF_HOOK include/linux/netfilter.h:307 [inline]
     NF_HOOK include/linux/netfilter.h:301 [inline]
     mld_sendpack+0x9a3/0xe40 net/ipv6/mcast.c:1826
     mld_send_cr net/ipv6/mcast.c:2127 [inline]
     mld_ifc_work+0x71c/0xdc0 net/ipv6/mcast.c:2659
     process_one_work+0x9ac/0x1650 kernel/workqueue.c:2307
     worker_thread+0x657/0x1110 kernel/workqueue.c:2454
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    ----------------
    Code disassembly (best guess):
       0:   48 89 eb                mov    %rbp,%rbx
       3:   c6 45 01 01             movb   $0x1,0x1(%rbp)
       7:   41 bc 00 80 00 00       mov    $0x8000,%r12d
       d:   48 c1 e9 03             shr    $0x3,%rcx
      11:   83 e3 07                and    $0x7,%ebx
      14:   41 be 01 00 00 00       mov    $0x1,%r14d
      1a:   48 b8 00 00 00 00 00    movabs $0xdffffc0000000000,%rax
      21:   fc ff df
      24:   4c 8d 2c 01             lea    (%rcx,%rax,1),%r13
      28:   eb 0c                   jmp    0x36
    * 2a:   f3 90                   pause <-- trapping instruction
      2c:   41 83 ec 01             sub    $0x1,%r12d
      30:   0f 84 72 04 00 00       je     0x4a8
      36:   41 0f b6 45 00          movzbl 0x0(%r13),%eax
      3b:   38 d8                   cmp    %bl,%al
      3d:   7f 08                   jg     0x47
      3f:   84                      .byte 0x84
    
    Fixes: 1da177e4c3f4 ("Linux-2.6.12-rc2")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Jamal Hadi Salim <jhs@mojatatu.com>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Jiri Pirko <jiri@resnulli.us>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Link: https://lore.kernel.org/r/20220215235305.3272331-1-eric.dumazet@gmail.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 797b380f0756354b39f7487c362ea203cf3e3e80
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 15 15:53:05 2022 -0800

    net: sched: limit TC_ACT_REPEAT loops
    
    commit 5740d068909676d4bdb5c9c00c37a83df7728909 upstream.
    
    We have been living dangerously, at the mercy of malicious users,
    abusing TC_ACT_REPEAT, as shown by this syzpot report [1].
    
    Add an arbitrary limit (32) to the number of times an action can
    return TC_ACT_REPEAT.
    
    v2: switch the limit to 32 instead of 10.
        Use net_warn_ratelimited() instead of pr_err_once().
    
    [1] (C repro available on demand)
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    1-...!: (10500 ticks this GP) idle=021/1/0x4000000000000000 softirq=5592/5592 fqs=0
            (t=10502 jiffies g=5305 q=190)
    rcu: rcu_preempt kthread timer wakeup didn't happen for 10502 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402
    rcu:    Possible timer handling issue on cpu=0 timer-softirq=3527
    rcu: rcu_preempt kthread starved for 10505 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu:    Unless rcu_preempt kthread gets sufficient CPU time, OOM is now expected behavior.
    rcu: RCU grace-period kthread stack dump:
    task:rcu_preempt     state:I stack:29344 pid:   14 ppid:     2 flags:0x00004000
    Call Trace:
     <TASK>
     context_switch kernel/sched/core.c:4986 [inline]
     __schedule+0xab2/0x4db0 kernel/sched/core.c:6295
     schedule+0xd2/0x260 kernel/sched/core.c:6368
     schedule_timeout+0x14a/0x2a0 kernel/time/timer.c:1881
     rcu_gp_fqs_loop+0x186/0x810 kernel/rcu/tree.c:1963
     rcu_gp_kthread+0x1de/0x320 kernel/rcu/tree.c:2136
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    rcu: Stack dump where RCU GP kthread last ran:
    Sending NMI from CPU 1 to CPUs 0:
    NMI backtrace for cpu 0
    CPU: 0 PID: 3646 Comm: syz-executor358 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:rep_nop arch/x86/include/asm/vdso/processor.h:13 [inline]
    RIP: 0010:cpu_relax arch/x86/include/asm/vdso/processor.h:18 [inline]
    RIP: 0010:pv_wait_head_or_lock kernel/locking/qspinlock_paravirt.h:437 [inline]
    RIP: 0010:__pv_queued_spin_lock_slowpath+0x3b8/0xb40 kernel/locking/qspinlock.c:508
    Code: 48 89 eb c6 45 01 01 41 bc 00 80 00 00 48 c1 e9 03 83 e3 07 41 be 01 00 00 00 48 b8 00 00 00 00 00 fc ff df 4c 8d 2c 01 eb 0c <f3> 90 41 83 ec 01 0f 84 72 04 00 00 41 0f b6 45 00 38 d8 7f 08 84
    RSP: 0018:ffffc9000283f1b0 EFLAGS: 00000206
    RAX: 0000000000000003 RBX: 0000000000000000 RCX: 1ffff1100fc0071e
    RDX: 0000000000000001 RSI: 0000000000000201 RDI: 0000000000000000
    RBP: ffff88807e0038f0 R08: 0000000000000001 R09: ffffffff8ffbf9ff
    R10: 0000000000000001 R11: 0000000000000001 R12: 0000000000004c1e
    R13: ffffed100fc0071e R14: 0000000000000001 R15: ffff8880b9c3aa80
    FS:  00005555562bf300(0000) GS:ffff8880b9c00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007ffdbfef12b8 CR3: 00000000723c2000 CR4: 00000000003506f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     <TASK>
     pv_queued_spin_lock_slowpath arch/x86/include/asm/paravirt.h:591 [inline]
     queued_spin_lock_slowpath arch/x86/include/asm/qspinlock.h:51 [inline]
     queued_spin_lock include/asm-generic/qspinlock.h:85 [inline]
     do_raw_spin_lock+0x200/0x2b0 kernel/locking/spinlock_debug.c:115
     spin_lock_bh include/linux/spinlock.h:354 [inline]
     sch_tree_lock include/net/sch_generic.h:610 [inline]
     sch_tree_lock include/net/sch_generic.h:605 [inline]
     prio_tune+0x3b9/0xb50 net/sched/sch_prio.c:211
     prio_init+0x5c/0x80 net/sched/sch_prio.c:244
     qdisc_create.constprop.0+0x44a/0x10f0 net/sched/sch_api.c:1253
     tc_modify_qdisc+0x4c5/0x1980 net/sched/sch_api.c:1660
     rtnetlink_rcv_msg+0x413/0xb80 net/core/rtnetlink.c:5594
     netlink_rcv_skb+0x153/0x420 net/netlink/af_netlink.c:2494
     netlink_unicast_kernel net/netlink/af_netlink.c:1317 [inline]
     netlink_unicast+0x539/0x7e0 net/netlink/af_netlink.c:1343
     netlink_sendmsg+0x904/0xe00 net/netlink/af_netlink.c:1919
     sock_sendmsg_nosec net/socket.c:705 [inline]
     sock_sendmsg+0xcf/0x120 net/socket.c:725
     ____sys_sendmsg+0x6e8/0x810 net/socket.c:2413
     ___sys_sendmsg+0xf3/0x170 net/socket.c:2467
     __sys_sendmsg+0xe5/0x1b0 net/socket.c:2496
     do_syscall_x64 arch/x86/entry/common.c:50 [inline]
     do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80
     entry_SYSCALL_64_after_hwframe+0x44/0xae
    RIP: 0033:0x7f7ee98aae99
    Code: 28 00 00 00 75 05 48 83 c4 28 c3 e8 41 15 00 00 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 c0 ff ff ff f7 d8 64 89 01 48
    RSP: 002b:00007ffdbfef12d8 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
    RAX: ffffffffffffffda RBX: 00007ffdbfef1300 RCX: 00007f7ee98aae99
    RDX: 0000000000000000 RSI: 0000000020000000 RDI: 0000000000000003
    RBP: 0000000000000000 R08: 000000000000000d R09: 000000000000000d
    R10: 000000000000000d R11: 0000000000000246 R12: 00007ffdbfef12f0
    R13: 00000000000f4240 R14: 000000000004ca47 R15: 00007ffdbfef12e4
     </TASK>
    INFO: NMI handler (nmi_cpu_backtrace_handler) took too long to run: 2.293 msecs
    NMI backtrace for cpu 1
    CPU: 1 PID: 3260 Comm: kworker/1:3 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Workqueue: mld mld_ifc_work
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     nmi_cpu_backtrace.cold+0x47/0x144 lib/nmi_backtrace.c:111
     nmi_trigger_cpumask_backtrace+0x1b3/0x230 lib/nmi_backtrace.c:62
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x25e/0x3f0 kernel/rcu/tree_stall.h:343
     print_cpu_stall kernel/rcu/tree_stall.h:604 [inline]
     check_cpu_stall kernel/rcu/tree_stall.h:688 [inline]
     rcu_pending kernel/rcu/tree.c:3919 [inline]
     rcu_sched_clock_irq.cold+0x5c/0x759 kernel/rcu/tree.c:2617
     update_process_times+0x16d/0x200 kernel/time/timer.c:1785
     tick_sched_handle+0x9b/0x180 kernel/time/tick-sched.c:226
     tick_sched_timer+0x1b0/0x2d0 kernel/time/tick-sched.c:1428
     __run_hrtimer kernel/time/hrtimer.c:1685 [inline]
     __hrtimer_run_queues+0x1c0/0xe50 kernel/time/hrtimer.c:1749
     hrtimer_interrupt+0x31c/0x790 kernel/time/hrtimer.c:1811
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1086 [inline]
     __sysvec_apic_timer_interrupt+0x146/0x530 arch/x86/kernel/apic/apic.c:1103
     sysvec_apic_timer_interrupt+0x8e/0xc0 arch/x86/kernel/apic/apic.c:1097
     </IRQ>
     <TASK>
     asm_sysvec_apic_timer_interrupt+0x12/0x20 arch/x86/include/asm/idtentry.h:638
    RIP: 0010:__sanitizer_cov_trace_const_cmp4+0xc/0x70 kernel/kcov.c:286
    Code: 00 00 00 48 89 7c 30 e8 48 89 4c 30 f0 4c 89 54 d8 20 48 89 10 5b c3 0f 1f 80 00 00 00 00 41 89 f8 bf 03 00 00 00 4c 8b 14 24 <89> f1 65 48 8b 34 25 00 70 02 00 e8 14 f9 ff ff 84 c0 74 4b 48 8b
    RSP: 0018:ffffc90002c5eea8 EFLAGS: 00000246
    RAX: 0000000000000007 RBX: ffff88801c625800 RCX: 0000000000000000
    RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000003
    RBP: ffff8880137d3100 R08: 0000000000000000 R09: 0000000000000000
    R10: ffffffff874fcd88 R11: 0000000000000000 R12: ffff88801d692dc0
    R13: ffff8880137d3104 R14: 0000000000000000 R15: ffff88801d692de8
     tcf_police_act+0x358/0x11d0 net/sched/act_police.c:256
     tcf_action_exec net/sched/act_api.c:1049 [inline]
     tcf_action_exec+0x1a6/0x530 net/sched/act_api.c:1026
     tcf_exts_exec include/net/pkt_cls.h:326 [inline]
     route4_classify+0xef0/0x1400 net/sched/cls_route.c:179
     __tcf_classify net/sched/cls_api.c:1549 [inline]
     tcf_classify+0x3e8/0x9d0 net/sched/cls_api.c:1615
     prio_classify net/sched/sch_prio.c:42 [inline]
     prio_enqueue+0x3a7/0x790 net/sched/sch_prio.c:75
     dev_qdisc_enqueue+0x40/0x300 net/core/dev.c:3668
     __dev_xmit_skb net/core/dev.c:3756 [inline]
     __dev_queue_xmit+0x1f61/0x3660 net/core/dev.c:4081
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip_finish_output2+0x14dc/0x2170 net/ipv4/ip_output.c:228
     __ip_finish_output net/ipv4/ip_output.c:306 [inline]
     __ip_finish_output+0x396/0x650 net/ipv4/ip_output.c:288
     ip_finish_output+0x32/0x200 net/ipv4/ip_output.c:316
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip_output+0x196/0x310 net/ipv4/ip_output.c:430
     dst_output include/net/dst.h:451 [inline]
     ip_local_out+0xaf/0x1a0 net/ipv4/ip_output.c:126
     iptunnel_xmit+0x628/0xa50 net/ipv4/ip_tunnel_core.c:82
     geneve_xmit_skb drivers/net/geneve.c:966 [inline]
     geneve_xmit+0x10c8/0x3530 drivers/net/geneve.c:1077
     __netdev_start_xmit include/linux/netdevice.h:4683 [inline]
     netdev_start_xmit include/linux/netdevice.h:4697 [inline]
     xmit_one net/core/dev.c:3473 [inline]
     dev_hard_start_xmit+0x1eb/0x920 net/core/dev.c:3489
     __dev_queue_xmit+0x2985/0x3660 net/core/dev.c:4116
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip6_finish_output2+0xf7a/0x14f0 net/ipv6/ip6_output.c:126
     __ip6_finish_output net/ipv6/ip6_output.c:191 [inline]
     __ip6_finish_output+0x61e/0xe90 net/ipv6/ip6_output.c:170
     ip6_finish_output+0x32/0x200 net/ipv6/ip6_output.c:201
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip6_output+0x1e4/0x530 net/ipv6/ip6_output.c:224
     dst_output include/net/dst.h:451 [inline]
     NF_HOOK include/linux/netfilter.h:307 [inline]
     NF_HOOK include/linux/netfilter.h:301 [inline]
     mld_sendpack+0x9a3/0xe40 net/ipv6/mcast.c:1826
     mld_send_cr net/ipv6/mcast.c:2127 [inline]
     mld_ifc_work+0x71c/0xdc0 net/ipv6/mcast.c:2659
     process_one_work+0x9ac/0x1650 kernel/workqueue.c:2307
     worker_thread+0x657/0x1110 kernel/workqueue.c:2454
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    ----------------
    Code disassembly (best guess):
       0:   48 89 eb                mov    %rbp,%rbx
       3:   c6 45 01 01             movb   $0x1,0x1(%rbp)
       7:   41 bc 00 80 00 00       mov    $0x8000,%r12d
       d:   48 c1 e9 03             shr    $0x3,%rcx
      11:   83 e3 07                and    $0x7,%ebx
      14:   41 be 01 00 00 00       mov    $0x1,%r14d
      1a:   48 b8 00 00 00 00 00    movabs $0xdffffc0000000000,%rax
      21:   fc ff df
      24:   4c 8d 2c 01             lea    (%rcx,%rax,1),%r13
      28:   eb 0c                   jmp    0x36
    * 2a:   f3 90                   pause <-- trapping instruction
      2c:   41 83 ec 01             sub    $0x1,%r12d
      30:   0f 84 72 04 00 00       je     0x4a8
      36:   41 0f b6 45 00          movzbl 0x0(%r13),%eax
      3b:   38 d8                   cmp    %bl,%al
      3d:   7f 08                   jg     0x47
      3f:   84                      .byte 0x84
    
    Fixes: 1da177e4c3f4 ("Linux-2.6.12-rc2")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Jamal Hadi Salim <jhs@mojatatu.com>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Jiri Pirko <jiri@resnulli.us>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Link: https://lore.kernel.org/r/20220215235305.3272331-1-eric.dumazet@gmail.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c83049cb88177b932c03258ca37ac1e6b02d5826
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 15 15:53:05 2022 -0800

    net: sched: limit TC_ACT_REPEAT loops
    
    commit 5740d068909676d4bdb5c9c00c37a83df7728909 upstream.
    
    We have been living dangerously, at the mercy of malicious users,
    abusing TC_ACT_REPEAT, as shown by this syzpot report [1].
    
    Add an arbitrary limit (32) to the number of times an action can
    return TC_ACT_REPEAT.
    
    v2: switch the limit to 32 instead of 10.
        Use net_warn_ratelimited() instead of pr_err_once().
    
    [1] (C repro available on demand)
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    1-...!: (10500 ticks this GP) idle=021/1/0x4000000000000000 softirq=5592/5592 fqs=0
            (t=10502 jiffies g=5305 q=190)
    rcu: rcu_preempt kthread timer wakeup didn't happen for 10502 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402
    rcu:    Possible timer handling issue on cpu=0 timer-softirq=3527
    rcu: rcu_preempt kthread starved for 10505 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu:    Unless rcu_preempt kthread gets sufficient CPU time, OOM is now expected behavior.
    rcu: RCU grace-period kthread stack dump:
    task:rcu_preempt     state:I stack:29344 pid:   14 ppid:     2 flags:0x00004000
    Call Trace:
     <TASK>
     context_switch kernel/sched/core.c:4986 [inline]
     __schedule+0xab2/0x4db0 kernel/sched/core.c:6295
     schedule+0xd2/0x260 kernel/sched/core.c:6368
     schedule_timeout+0x14a/0x2a0 kernel/time/timer.c:1881
     rcu_gp_fqs_loop+0x186/0x810 kernel/rcu/tree.c:1963
     rcu_gp_kthread+0x1de/0x320 kernel/rcu/tree.c:2136
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    rcu: Stack dump where RCU GP kthread last ran:
    Sending NMI from CPU 1 to CPUs 0:
    NMI backtrace for cpu 0
    CPU: 0 PID: 3646 Comm: syz-executor358 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:rep_nop arch/x86/include/asm/vdso/processor.h:13 [inline]
    RIP: 0010:cpu_relax arch/x86/include/asm/vdso/processor.h:18 [inline]
    RIP: 0010:pv_wait_head_or_lock kernel/locking/qspinlock_paravirt.h:437 [inline]
    RIP: 0010:__pv_queued_spin_lock_slowpath+0x3b8/0xb40 kernel/locking/qspinlock.c:508
    Code: 48 89 eb c6 45 01 01 41 bc 00 80 00 00 48 c1 e9 03 83 e3 07 41 be 01 00 00 00 48 b8 00 00 00 00 00 fc ff df 4c 8d 2c 01 eb 0c <f3> 90 41 83 ec 01 0f 84 72 04 00 00 41 0f b6 45 00 38 d8 7f 08 84
    RSP: 0018:ffffc9000283f1b0 EFLAGS: 00000206
    RAX: 0000000000000003 RBX: 0000000000000000 RCX: 1ffff1100fc0071e
    RDX: 0000000000000001 RSI: 0000000000000201 RDI: 0000000000000000
    RBP: ffff88807e0038f0 R08: 0000000000000001 R09: ffffffff8ffbf9ff
    R10: 0000000000000001 R11: 0000000000000001 R12: 0000000000004c1e
    R13: ffffed100fc0071e R14: 0000000000000001 R15: ffff8880b9c3aa80
    FS:  00005555562bf300(0000) GS:ffff8880b9c00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007ffdbfef12b8 CR3: 00000000723c2000 CR4: 00000000003506f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     <TASK>
     pv_queued_spin_lock_slowpath arch/x86/include/asm/paravirt.h:591 [inline]
     queued_spin_lock_slowpath arch/x86/include/asm/qspinlock.h:51 [inline]
     queued_spin_lock include/asm-generic/qspinlock.h:85 [inline]
     do_raw_spin_lock+0x200/0x2b0 kernel/locking/spinlock_debug.c:115
     spin_lock_bh include/linux/spinlock.h:354 [inline]
     sch_tree_lock include/net/sch_generic.h:610 [inline]
     sch_tree_lock include/net/sch_generic.h:605 [inline]
     prio_tune+0x3b9/0xb50 net/sched/sch_prio.c:211
     prio_init+0x5c/0x80 net/sched/sch_prio.c:244
     qdisc_create.constprop.0+0x44a/0x10f0 net/sched/sch_api.c:1253
     tc_modify_qdisc+0x4c5/0x1980 net/sched/sch_api.c:1660
     rtnetlink_rcv_msg+0x413/0xb80 net/core/rtnetlink.c:5594
     netlink_rcv_skb+0x153/0x420 net/netlink/af_netlink.c:2494
     netlink_unicast_kernel net/netlink/af_netlink.c:1317 [inline]
     netlink_unicast+0x539/0x7e0 net/netlink/af_netlink.c:1343
     netlink_sendmsg+0x904/0xe00 net/netlink/af_netlink.c:1919
     sock_sendmsg_nosec net/socket.c:705 [inline]
     sock_sendmsg+0xcf/0x120 net/socket.c:725
     ____sys_sendmsg+0x6e8/0x810 net/socket.c:2413
     ___sys_sendmsg+0xf3/0x170 net/socket.c:2467
     __sys_sendmsg+0xe5/0x1b0 net/socket.c:2496
     do_syscall_x64 arch/x86/entry/common.c:50 [inline]
     do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80
     entry_SYSCALL_64_after_hwframe+0x44/0xae
    RIP: 0033:0x7f7ee98aae99
    Code: 28 00 00 00 75 05 48 83 c4 28 c3 e8 41 15 00 00 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 c0 ff ff ff f7 d8 64 89 01 48
    RSP: 002b:00007ffdbfef12d8 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
    RAX: ffffffffffffffda RBX: 00007ffdbfef1300 RCX: 00007f7ee98aae99
    RDX: 0000000000000000 RSI: 0000000020000000 RDI: 0000000000000003
    RBP: 0000000000000000 R08: 000000000000000d R09: 000000000000000d
    R10: 000000000000000d R11: 0000000000000246 R12: 00007ffdbfef12f0
    R13: 00000000000f4240 R14: 000000000004ca47 R15: 00007ffdbfef12e4
     </TASK>
    INFO: NMI handler (nmi_cpu_backtrace_handler) took too long to run: 2.293 msecs
    NMI backtrace for cpu 1
    CPU: 1 PID: 3260 Comm: kworker/1:3 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Workqueue: mld mld_ifc_work
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     nmi_cpu_backtrace.cold+0x47/0x144 lib/nmi_backtrace.c:111
     nmi_trigger_cpumask_backtrace+0x1b3/0x230 lib/nmi_backtrace.c:62
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x25e/0x3f0 kernel/rcu/tree_stall.h:343
     print_cpu_stall kernel/rcu/tree_stall.h:604 [inline]
     check_cpu_stall kernel/rcu/tree_stall.h:688 [inline]
     rcu_pending kernel/rcu/tree.c:3919 [inline]
     rcu_sched_clock_irq.cold+0x5c/0x759 kernel/rcu/tree.c:2617
     update_process_times+0x16d/0x200 kernel/time/timer.c:1785
     tick_sched_handle+0x9b/0x180 kernel/time/tick-sched.c:226
     tick_sched_timer+0x1b0/0x2d0 kernel/time/tick-sched.c:1428
     __run_hrtimer kernel/time/hrtimer.c:1685 [inline]
     __hrtimer_run_queues+0x1c0/0xe50 kernel/time/hrtimer.c:1749
     hrtimer_interrupt+0x31c/0x790 kernel/time/hrtimer.c:1811
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1086 [inline]
     __sysvec_apic_timer_interrupt+0x146/0x530 arch/x86/kernel/apic/apic.c:1103
     sysvec_apic_timer_interrupt+0x8e/0xc0 arch/x86/kernel/apic/apic.c:1097
     </IRQ>
     <TASK>
     asm_sysvec_apic_timer_interrupt+0x12/0x20 arch/x86/include/asm/idtentry.h:638
    RIP: 0010:__sanitizer_cov_trace_const_cmp4+0xc/0x70 kernel/kcov.c:286
    Code: 00 00 00 48 89 7c 30 e8 48 89 4c 30 f0 4c 89 54 d8 20 48 89 10 5b c3 0f 1f 80 00 00 00 00 41 89 f8 bf 03 00 00 00 4c 8b 14 24 <89> f1 65 48 8b 34 25 00 70 02 00 e8 14 f9 ff ff 84 c0 74 4b 48 8b
    RSP: 0018:ffffc90002c5eea8 EFLAGS: 00000246
    RAX: 0000000000000007 RBX: ffff88801c625800 RCX: 0000000000000000
    RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000003
    RBP: ffff8880137d3100 R08: 0000000000000000 R09: 0000000000000000
    R10: ffffffff874fcd88 R11: 0000000000000000 R12: ffff88801d692dc0
    R13: ffff8880137d3104 R14: 0000000000000000 R15: ffff88801d692de8
     tcf_police_act+0x358/0x11d0 net/sched/act_police.c:256
     tcf_action_exec net/sched/act_api.c:1049 [inline]
     tcf_action_exec+0x1a6/0x530 net/sched/act_api.c:1026
     tcf_exts_exec include/net/pkt_cls.h:326 [inline]
     route4_classify+0xef0/0x1400 net/sched/cls_route.c:179
     __tcf_classify net/sched/cls_api.c:1549 [inline]
     tcf_classify+0x3e8/0x9d0 net/sched/cls_api.c:1615
     prio_classify net/sched/sch_prio.c:42 [inline]
     prio_enqueue+0x3a7/0x790 net/sched/sch_prio.c:75
     dev_qdisc_enqueue+0x40/0x300 net/core/dev.c:3668
     __dev_xmit_skb net/core/dev.c:3756 [inline]
     __dev_queue_xmit+0x1f61/0x3660 net/core/dev.c:4081
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip_finish_output2+0x14dc/0x2170 net/ipv4/ip_output.c:228
     __ip_finish_output net/ipv4/ip_output.c:306 [inline]
     __ip_finish_output+0x396/0x650 net/ipv4/ip_output.c:288
     ip_finish_output+0x32/0x200 net/ipv4/ip_output.c:316
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip_output+0x196/0x310 net/ipv4/ip_output.c:430
     dst_output include/net/dst.h:451 [inline]
     ip_local_out+0xaf/0x1a0 net/ipv4/ip_output.c:126
     iptunnel_xmit+0x628/0xa50 net/ipv4/ip_tunnel_core.c:82
     geneve_xmit_skb drivers/net/geneve.c:966 [inline]
     geneve_xmit+0x10c8/0x3530 drivers/net/geneve.c:1077
     __netdev_start_xmit include/linux/netdevice.h:4683 [inline]
     netdev_start_xmit include/linux/netdevice.h:4697 [inline]
     xmit_one net/core/dev.c:3473 [inline]
     dev_hard_start_xmit+0x1eb/0x920 net/core/dev.c:3489
     __dev_queue_xmit+0x2985/0x3660 net/core/dev.c:4116
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip6_finish_output2+0xf7a/0x14f0 net/ipv6/ip6_output.c:126
     __ip6_finish_output net/ipv6/ip6_output.c:191 [inline]
     __ip6_finish_output+0x61e/0xe90 net/ipv6/ip6_output.c:170
     ip6_finish_output+0x32/0x200 net/ipv6/ip6_output.c:201
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip6_output+0x1e4/0x530 net/ipv6/ip6_output.c:224
     dst_output include/net/dst.h:451 [inline]
     NF_HOOK include/linux/netfilter.h:307 [inline]
     NF_HOOK include/linux/netfilter.h:301 [inline]
     mld_sendpack+0x9a3/0xe40 net/ipv6/mcast.c:1826
     mld_send_cr net/ipv6/mcast.c:2127 [inline]
     mld_ifc_work+0x71c/0xdc0 net/ipv6/mcast.c:2659
     process_one_work+0x9ac/0x1650 kernel/workqueue.c:2307
     worker_thread+0x657/0x1110 kernel/workqueue.c:2454
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    ----------------
    Code disassembly (best guess):
       0:   48 89 eb                mov    %rbp,%rbx
       3:   c6 45 01 01             movb   $0x1,0x1(%rbp)
       7:   41 bc 00 80 00 00       mov    $0x8000,%r12d
       d:   48 c1 e9 03             shr    $0x3,%rcx
      11:   83 e3 07                and    $0x7,%ebx
      14:   41 be 01 00 00 00       mov    $0x1,%r14d
      1a:   48 b8 00 00 00 00 00    movabs $0xdffffc0000000000,%rax
      21:   fc ff df
      24:   4c 8d 2c 01             lea    (%rcx,%rax,1),%r13
      28:   eb 0c                   jmp    0x36
    * 2a:   f3 90                   pause <-- trapping instruction
      2c:   41 83 ec 01             sub    $0x1,%r12d
      30:   0f 84 72 04 00 00       je     0x4a8
      36:   41 0f b6 45 00          movzbl 0x0(%r13),%eax
      3b:   38 d8                   cmp    %bl,%al
      3d:   7f 08                   jg     0x47
      3f:   84                      .byte 0x84
    
    Fixes: 1da177e4c3f4 ("Linux-2.6.12-rc2")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Jamal Hadi Salim <jhs@mojatatu.com>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Jiri Pirko <jiri@resnulli.us>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Link: https://lore.kernel.org/r/20220215235305.3272331-1-eric.dumazet@gmail.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f63c9fa36bd7548fd07792127057cccb68a7e274
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 15 15:53:05 2022 -0800

    net: sched: limit TC_ACT_REPEAT loops
    
    commit 5740d068909676d4bdb5c9c00c37a83df7728909 upstream.
    
    We have been living dangerously, at the mercy of malicious users,
    abusing TC_ACT_REPEAT, as shown by this syzpot report [1].
    
    Add an arbitrary limit (32) to the number of times an action can
    return TC_ACT_REPEAT.
    
    v2: switch the limit to 32 instead of 10.
        Use net_warn_ratelimited() instead of pr_err_once().
    
    [1] (C repro available on demand)
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    1-...!: (10500 ticks this GP) idle=021/1/0x4000000000000000 softirq=5592/5592 fqs=0
            (t=10502 jiffies g=5305 q=190)
    rcu: rcu_preempt kthread timer wakeup didn't happen for 10502 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402
    rcu:    Possible timer handling issue on cpu=0 timer-softirq=3527
    rcu: rcu_preempt kthread starved for 10505 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu:    Unless rcu_preempt kthread gets sufficient CPU time, OOM is now expected behavior.
    rcu: RCU grace-period kthread stack dump:
    task:rcu_preempt     state:I stack:29344 pid:   14 ppid:     2 flags:0x00004000
    Call Trace:
     <TASK>
     context_switch kernel/sched/core.c:4986 [inline]
     __schedule+0xab2/0x4db0 kernel/sched/core.c:6295
     schedule+0xd2/0x260 kernel/sched/core.c:6368
     schedule_timeout+0x14a/0x2a0 kernel/time/timer.c:1881
     rcu_gp_fqs_loop+0x186/0x810 kernel/rcu/tree.c:1963
     rcu_gp_kthread+0x1de/0x320 kernel/rcu/tree.c:2136
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    rcu: Stack dump where RCU GP kthread last ran:
    Sending NMI from CPU 1 to CPUs 0:
    NMI backtrace for cpu 0
    CPU: 0 PID: 3646 Comm: syz-executor358 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:rep_nop arch/x86/include/asm/vdso/processor.h:13 [inline]
    RIP: 0010:cpu_relax arch/x86/include/asm/vdso/processor.h:18 [inline]
    RIP: 0010:pv_wait_head_or_lock kernel/locking/qspinlock_paravirt.h:437 [inline]
    RIP: 0010:__pv_queued_spin_lock_slowpath+0x3b8/0xb40 kernel/locking/qspinlock.c:508
    Code: 48 89 eb c6 45 01 01 41 bc 00 80 00 00 48 c1 e9 03 83 e3 07 41 be 01 00 00 00 48 b8 00 00 00 00 00 fc ff df 4c 8d 2c 01 eb 0c <f3> 90 41 83 ec 01 0f 84 72 04 00 00 41 0f b6 45 00 38 d8 7f 08 84
    RSP: 0018:ffffc9000283f1b0 EFLAGS: 00000206
    RAX: 0000000000000003 RBX: 0000000000000000 RCX: 1ffff1100fc0071e
    RDX: 0000000000000001 RSI: 0000000000000201 RDI: 0000000000000000
    RBP: ffff88807e0038f0 R08: 0000000000000001 R09: ffffffff8ffbf9ff
    R10: 0000000000000001 R11: 0000000000000001 R12: 0000000000004c1e
    R13: ffffed100fc0071e R14: 0000000000000001 R15: ffff8880b9c3aa80
    FS:  00005555562bf300(0000) GS:ffff8880b9c00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007ffdbfef12b8 CR3: 00000000723c2000 CR4: 00000000003506f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     <TASK>
     pv_queued_spin_lock_slowpath arch/x86/include/asm/paravirt.h:591 [inline]
     queued_spin_lock_slowpath arch/x86/include/asm/qspinlock.h:51 [inline]
     queued_spin_lock include/asm-generic/qspinlock.h:85 [inline]
     do_raw_spin_lock+0x200/0x2b0 kernel/locking/spinlock_debug.c:115
     spin_lock_bh include/linux/spinlock.h:354 [inline]
     sch_tree_lock include/net/sch_generic.h:610 [inline]
     sch_tree_lock include/net/sch_generic.h:605 [inline]
     prio_tune+0x3b9/0xb50 net/sched/sch_prio.c:211
     prio_init+0x5c/0x80 net/sched/sch_prio.c:244
     qdisc_create.constprop.0+0x44a/0x10f0 net/sched/sch_api.c:1253
     tc_modify_qdisc+0x4c5/0x1980 net/sched/sch_api.c:1660
     rtnetlink_rcv_msg+0x413/0xb80 net/core/rtnetlink.c:5594
     netlink_rcv_skb+0x153/0x420 net/netlink/af_netlink.c:2494
     netlink_unicast_kernel net/netlink/af_netlink.c:1317 [inline]
     netlink_unicast+0x539/0x7e0 net/netlink/af_netlink.c:1343
     netlink_sendmsg+0x904/0xe00 net/netlink/af_netlink.c:1919
     sock_sendmsg_nosec net/socket.c:705 [inline]
     sock_sendmsg+0xcf/0x120 net/socket.c:725
     ____sys_sendmsg+0x6e8/0x810 net/socket.c:2413
     ___sys_sendmsg+0xf3/0x170 net/socket.c:2467
     __sys_sendmsg+0xe5/0x1b0 net/socket.c:2496
     do_syscall_x64 arch/x86/entry/common.c:50 [inline]
     do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80
     entry_SYSCALL_64_after_hwframe+0x44/0xae
    RIP: 0033:0x7f7ee98aae99
    Code: 28 00 00 00 75 05 48 83 c4 28 c3 e8 41 15 00 00 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 c0 ff ff ff f7 d8 64 89 01 48
    RSP: 002b:00007ffdbfef12d8 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
    RAX: ffffffffffffffda RBX: 00007ffdbfef1300 RCX: 00007f7ee98aae99
    RDX: 0000000000000000 RSI: 0000000020000000 RDI: 0000000000000003
    RBP: 0000000000000000 R08: 000000000000000d R09: 000000000000000d
    R10: 000000000000000d R11: 0000000000000246 R12: 00007ffdbfef12f0
    R13: 00000000000f4240 R14: 000000000004ca47 R15: 00007ffdbfef12e4
     </TASK>
    INFO: NMI handler (nmi_cpu_backtrace_handler) took too long to run: 2.293 msecs
    NMI backtrace for cpu 1
    CPU: 1 PID: 3260 Comm: kworker/1:3 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Workqueue: mld mld_ifc_work
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     nmi_cpu_backtrace.cold+0x47/0x144 lib/nmi_backtrace.c:111
     nmi_trigger_cpumask_backtrace+0x1b3/0x230 lib/nmi_backtrace.c:62
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x25e/0x3f0 kernel/rcu/tree_stall.h:343
     print_cpu_stall kernel/rcu/tree_stall.h:604 [inline]
     check_cpu_stall kernel/rcu/tree_stall.h:688 [inline]
     rcu_pending kernel/rcu/tree.c:3919 [inline]
     rcu_sched_clock_irq.cold+0x5c/0x759 kernel/rcu/tree.c:2617
     update_process_times+0x16d/0x200 kernel/time/timer.c:1785
     tick_sched_handle+0x9b/0x180 kernel/time/tick-sched.c:226
     tick_sched_timer+0x1b0/0x2d0 kernel/time/tick-sched.c:1428
     __run_hrtimer kernel/time/hrtimer.c:1685 [inline]
     __hrtimer_run_queues+0x1c0/0xe50 kernel/time/hrtimer.c:1749
     hrtimer_interrupt+0x31c/0x790 kernel/time/hrtimer.c:1811
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1086 [inline]
     __sysvec_apic_timer_interrupt+0x146/0x530 arch/x86/kernel/apic/apic.c:1103
     sysvec_apic_timer_interrupt+0x8e/0xc0 arch/x86/kernel/apic/apic.c:1097
     </IRQ>
     <TASK>
     asm_sysvec_apic_timer_interrupt+0x12/0x20 arch/x86/include/asm/idtentry.h:638
    RIP: 0010:__sanitizer_cov_trace_const_cmp4+0xc/0x70 kernel/kcov.c:286
    Code: 00 00 00 48 89 7c 30 e8 48 89 4c 30 f0 4c 89 54 d8 20 48 89 10 5b c3 0f 1f 80 00 00 00 00 41 89 f8 bf 03 00 00 00 4c 8b 14 24 <89> f1 65 48 8b 34 25 00 70 02 00 e8 14 f9 ff ff 84 c0 74 4b 48 8b
    RSP: 0018:ffffc90002c5eea8 EFLAGS: 00000246
    RAX: 0000000000000007 RBX: ffff88801c625800 RCX: 0000000000000000
    RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000003
    RBP: ffff8880137d3100 R08: 0000000000000000 R09: 0000000000000000
    R10: ffffffff874fcd88 R11: 0000000000000000 R12: ffff88801d692dc0
    R13: ffff8880137d3104 R14: 0000000000000000 R15: ffff88801d692de8
     tcf_police_act+0x358/0x11d0 net/sched/act_police.c:256
     tcf_action_exec net/sched/act_api.c:1049 [inline]
     tcf_action_exec+0x1a6/0x530 net/sched/act_api.c:1026
     tcf_exts_exec include/net/pkt_cls.h:326 [inline]
     route4_classify+0xef0/0x1400 net/sched/cls_route.c:179
     __tcf_classify net/sched/cls_api.c:1549 [inline]
     tcf_classify+0x3e8/0x9d0 net/sched/cls_api.c:1615
     prio_classify net/sched/sch_prio.c:42 [inline]
     prio_enqueue+0x3a7/0x790 net/sched/sch_prio.c:75
     dev_qdisc_enqueue+0x40/0x300 net/core/dev.c:3668
     __dev_xmit_skb net/core/dev.c:3756 [inline]
     __dev_queue_xmit+0x1f61/0x3660 net/core/dev.c:4081
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip_finish_output2+0x14dc/0x2170 net/ipv4/ip_output.c:228
     __ip_finish_output net/ipv4/ip_output.c:306 [inline]
     __ip_finish_output+0x396/0x650 net/ipv4/ip_output.c:288
     ip_finish_output+0x32/0x200 net/ipv4/ip_output.c:316
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip_output+0x196/0x310 net/ipv4/ip_output.c:430
     dst_output include/net/dst.h:451 [inline]
     ip_local_out+0xaf/0x1a0 net/ipv4/ip_output.c:126
     iptunnel_xmit+0x628/0xa50 net/ipv4/ip_tunnel_core.c:82
     geneve_xmit_skb drivers/net/geneve.c:966 [inline]
     geneve_xmit+0x10c8/0x3530 drivers/net/geneve.c:1077
     __netdev_start_xmit include/linux/netdevice.h:4683 [inline]
     netdev_start_xmit include/linux/netdevice.h:4697 [inline]
     xmit_one net/core/dev.c:3473 [inline]
     dev_hard_start_xmit+0x1eb/0x920 net/core/dev.c:3489
     __dev_queue_xmit+0x2985/0x3660 net/core/dev.c:4116
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip6_finish_output2+0xf7a/0x14f0 net/ipv6/ip6_output.c:126
     __ip6_finish_output net/ipv6/ip6_output.c:191 [inline]
     __ip6_finish_output+0x61e/0xe90 net/ipv6/ip6_output.c:170
     ip6_finish_output+0x32/0x200 net/ipv6/ip6_output.c:201
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip6_output+0x1e4/0x530 net/ipv6/ip6_output.c:224
     dst_output include/net/dst.h:451 [inline]
     NF_HOOK include/linux/netfilter.h:307 [inline]
     NF_HOOK include/linux/netfilter.h:301 [inline]
     mld_sendpack+0x9a3/0xe40 net/ipv6/mcast.c:1826
     mld_send_cr net/ipv6/mcast.c:2127 [inline]
     mld_ifc_work+0x71c/0xdc0 net/ipv6/mcast.c:2659
     process_one_work+0x9ac/0x1650 kernel/workqueue.c:2307
     worker_thread+0x657/0x1110 kernel/workqueue.c:2454
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    ----------------
    Code disassembly (best guess):
       0:   48 89 eb                mov    %rbp,%rbx
       3:   c6 45 01 01             movb   $0x1,0x1(%rbp)
       7:   41 bc 00 80 00 00       mov    $0x8000,%r12d
       d:   48 c1 e9 03             shr    $0x3,%rcx
      11:   83 e3 07                and    $0x7,%ebx
      14:   41 be 01 00 00 00       mov    $0x1,%r14d
      1a:   48 b8 00 00 00 00 00    movabs $0xdffffc0000000000,%rax
      21:   fc ff df
      24:   4c 8d 2c 01             lea    (%rcx,%rax,1),%r13
      28:   eb 0c                   jmp    0x36
    * 2a:   f3 90                   pause <-- trapping instruction
      2c:   41 83 ec 01             sub    $0x1,%r12d
      30:   0f 84 72 04 00 00       je     0x4a8
      36:   41 0f b6 45 00          movzbl 0x0(%r13),%eax
      3b:   38 d8                   cmp    %bl,%al
      3d:   7f 08                   jg     0x47
      3f:   84                      .byte 0x84
    
    Fixes: 1da177e4c3f4 ("Linux-2.6.12-rc2")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Jamal Hadi Salim <jhs@mojatatu.com>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Jiri Pirko <jiri@resnulli.us>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Link: https://lore.kernel.org/r/20220215235305.3272331-1-eric.dumazet@gmail.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2170a3d1c2d84ed8bd233fc085b25cced3da843f
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 15 15:53:05 2022 -0800

    net: sched: limit TC_ACT_REPEAT loops
    
    commit 5740d068909676d4bdb5c9c00c37a83df7728909 upstream.
    
    We have been living dangerously, at the mercy of malicious users,
    abusing TC_ACT_REPEAT, as shown by this syzpot report [1].
    
    Add an arbitrary limit (32) to the number of times an action can
    return TC_ACT_REPEAT.
    
    v2: switch the limit to 32 instead of 10.
        Use net_warn_ratelimited() instead of pr_err_once().
    
    [1] (C repro available on demand)
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    1-...!: (10500 ticks this GP) idle=021/1/0x4000000000000000 softirq=5592/5592 fqs=0
            (t=10502 jiffies g=5305 q=190)
    rcu: rcu_preempt kthread timer wakeup didn't happen for 10502 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402
    rcu:    Possible timer handling issue on cpu=0 timer-softirq=3527
    rcu: rcu_preempt kthread starved for 10505 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu:    Unless rcu_preempt kthread gets sufficient CPU time, OOM is now expected behavior.
    rcu: RCU grace-period kthread stack dump:
    task:rcu_preempt     state:I stack:29344 pid:   14 ppid:     2 flags:0x00004000
    Call Trace:
     <TASK>
     context_switch kernel/sched/core.c:4986 [inline]
     __schedule+0xab2/0x4db0 kernel/sched/core.c:6295
     schedule+0xd2/0x260 kernel/sched/core.c:6368
     schedule_timeout+0x14a/0x2a0 kernel/time/timer.c:1881
     rcu_gp_fqs_loop+0x186/0x810 kernel/rcu/tree.c:1963
     rcu_gp_kthread+0x1de/0x320 kernel/rcu/tree.c:2136
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    rcu: Stack dump where RCU GP kthread last ran:
    Sending NMI from CPU 1 to CPUs 0:
    NMI backtrace for cpu 0
    CPU: 0 PID: 3646 Comm: syz-executor358 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:rep_nop arch/x86/include/asm/vdso/processor.h:13 [inline]
    RIP: 0010:cpu_relax arch/x86/include/asm/vdso/processor.h:18 [inline]
    RIP: 0010:pv_wait_head_or_lock kernel/locking/qspinlock_paravirt.h:437 [inline]
    RIP: 0010:__pv_queued_spin_lock_slowpath+0x3b8/0xb40 kernel/locking/qspinlock.c:508
    Code: 48 89 eb c6 45 01 01 41 bc 00 80 00 00 48 c1 e9 03 83 e3 07 41 be 01 00 00 00 48 b8 00 00 00 00 00 fc ff df 4c 8d 2c 01 eb 0c <f3> 90 41 83 ec 01 0f 84 72 04 00 00 41 0f b6 45 00 38 d8 7f 08 84
    RSP: 0018:ffffc9000283f1b0 EFLAGS: 00000206
    RAX: 0000000000000003 RBX: 0000000000000000 RCX: 1ffff1100fc0071e
    RDX: 0000000000000001 RSI: 0000000000000201 RDI: 0000000000000000
    RBP: ffff88807e0038f0 R08: 0000000000000001 R09: ffffffff8ffbf9ff
    R10: 0000000000000001 R11: 0000000000000001 R12: 0000000000004c1e
    R13: ffffed100fc0071e R14: 0000000000000001 R15: ffff8880b9c3aa80
    FS:  00005555562bf300(0000) GS:ffff8880b9c00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007ffdbfef12b8 CR3: 00000000723c2000 CR4: 00000000003506f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     <TASK>
     pv_queued_spin_lock_slowpath arch/x86/include/asm/paravirt.h:591 [inline]
     queued_spin_lock_slowpath arch/x86/include/asm/qspinlock.h:51 [inline]
     queued_spin_lock include/asm-generic/qspinlock.h:85 [inline]
     do_raw_spin_lock+0x200/0x2b0 kernel/locking/spinlock_debug.c:115
     spin_lock_bh include/linux/spinlock.h:354 [inline]
     sch_tree_lock include/net/sch_generic.h:610 [inline]
     sch_tree_lock include/net/sch_generic.h:605 [inline]
     prio_tune+0x3b9/0xb50 net/sched/sch_prio.c:211
     prio_init+0x5c/0x80 net/sched/sch_prio.c:244
     qdisc_create.constprop.0+0x44a/0x10f0 net/sched/sch_api.c:1253
     tc_modify_qdisc+0x4c5/0x1980 net/sched/sch_api.c:1660
     rtnetlink_rcv_msg+0x413/0xb80 net/core/rtnetlink.c:5594
     netlink_rcv_skb+0x153/0x420 net/netlink/af_netlink.c:2494
     netlink_unicast_kernel net/netlink/af_netlink.c:1317 [inline]
     netlink_unicast+0x539/0x7e0 net/netlink/af_netlink.c:1343
     netlink_sendmsg+0x904/0xe00 net/netlink/af_netlink.c:1919
     sock_sendmsg_nosec net/socket.c:705 [inline]
     sock_sendmsg+0xcf/0x120 net/socket.c:725
     ____sys_sendmsg+0x6e8/0x810 net/socket.c:2413
     ___sys_sendmsg+0xf3/0x170 net/socket.c:2467
     __sys_sendmsg+0xe5/0x1b0 net/socket.c:2496
     do_syscall_x64 arch/x86/entry/common.c:50 [inline]
     do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80
     entry_SYSCALL_64_after_hwframe+0x44/0xae
    RIP: 0033:0x7f7ee98aae99
    Code: 28 00 00 00 75 05 48 83 c4 28 c3 e8 41 15 00 00 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 c0 ff ff ff f7 d8 64 89 01 48
    RSP: 002b:00007ffdbfef12d8 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
    RAX: ffffffffffffffda RBX: 00007ffdbfef1300 RCX: 00007f7ee98aae99
    RDX: 0000000000000000 RSI: 0000000020000000 RDI: 0000000000000003
    RBP: 0000000000000000 R08: 000000000000000d R09: 000000000000000d
    R10: 000000000000000d R11: 0000000000000246 R12: 00007ffdbfef12f0
    R13: 00000000000f4240 R14: 000000000004ca47 R15: 00007ffdbfef12e4
     </TASK>
    INFO: NMI handler (nmi_cpu_backtrace_handler) took too long to run: 2.293 msecs
    NMI backtrace for cpu 1
    CPU: 1 PID: 3260 Comm: kworker/1:3 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Workqueue: mld mld_ifc_work
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     nmi_cpu_backtrace.cold+0x47/0x144 lib/nmi_backtrace.c:111
     nmi_trigger_cpumask_backtrace+0x1b3/0x230 lib/nmi_backtrace.c:62
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x25e/0x3f0 kernel/rcu/tree_stall.h:343
     print_cpu_stall kernel/rcu/tree_stall.h:604 [inline]
     check_cpu_stall kernel/rcu/tree_stall.h:688 [inline]
     rcu_pending kernel/rcu/tree.c:3919 [inline]
     rcu_sched_clock_irq.cold+0x5c/0x759 kernel/rcu/tree.c:2617
     update_process_times+0x16d/0x200 kernel/time/timer.c:1785
     tick_sched_handle+0x9b/0x180 kernel/time/tick-sched.c:226
     tick_sched_timer+0x1b0/0x2d0 kernel/time/tick-sched.c:1428
     __run_hrtimer kernel/time/hrtimer.c:1685 [inline]
     __hrtimer_run_queues+0x1c0/0xe50 kernel/time/hrtimer.c:1749
     hrtimer_interrupt+0x31c/0x790 kernel/time/hrtimer.c:1811
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1086 [inline]
     __sysvec_apic_timer_interrupt+0x146/0x530 arch/x86/kernel/apic/apic.c:1103
     sysvec_apic_timer_interrupt+0x8e/0xc0 arch/x86/kernel/apic/apic.c:1097
     </IRQ>
     <TASK>
     asm_sysvec_apic_timer_interrupt+0x12/0x20 arch/x86/include/asm/idtentry.h:638
    RIP: 0010:__sanitizer_cov_trace_const_cmp4+0xc/0x70 kernel/kcov.c:286
    Code: 00 00 00 48 89 7c 30 e8 48 89 4c 30 f0 4c 89 54 d8 20 48 89 10 5b c3 0f 1f 80 00 00 00 00 41 89 f8 bf 03 00 00 00 4c 8b 14 24 <89> f1 65 48 8b 34 25 00 70 02 00 e8 14 f9 ff ff 84 c0 74 4b 48 8b
    RSP: 0018:ffffc90002c5eea8 EFLAGS: 00000246
    RAX: 0000000000000007 RBX: ffff88801c625800 RCX: 0000000000000000
    RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000003
    RBP: ffff8880137d3100 R08: 0000000000000000 R09: 0000000000000000
    R10: ffffffff874fcd88 R11: 0000000000000000 R12: ffff88801d692dc0
    R13: ffff8880137d3104 R14: 0000000000000000 R15: ffff88801d692de8
     tcf_police_act+0x358/0x11d0 net/sched/act_police.c:256
     tcf_action_exec net/sched/act_api.c:1049 [inline]
     tcf_action_exec+0x1a6/0x530 net/sched/act_api.c:1026
     tcf_exts_exec include/net/pkt_cls.h:326 [inline]
     route4_classify+0xef0/0x1400 net/sched/cls_route.c:179
     __tcf_classify net/sched/cls_api.c:1549 [inline]
     tcf_classify+0x3e8/0x9d0 net/sched/cls_api.c:1615
     prio_classify net/sched/sch_prio.c:42 [inline]
     prio_enqueue+0x3a7/0x790 net/sched/sch_prio.c:75
     dev_qdisc_enqueue+0x40/0x300 net/core/dev.c:3668
     __dev_xmit_skb net/core/dev.c:3756 [inline]
     __dev_queue_xmit+0x1f61/0x3660 net/core/dev.c:4081
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip_finish_output2+0x14dc/0x2170 net/ipv4/ip_output.c:228
     __ip_finish_output net/ipv4/ip_output.c:306 [inline]
     __ip_finish_output+0x396/0x650 net/ipv4/ip_output.c:288
     ip_finish_output+0x32/0x200 net/ipv4/ip_output.c:316
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip_output+0x196/0x310 net/ipv4/ip_output.c:430
     dst_output include/net/dst.h:451 [inline]
     ip_local_out+0xaf/0x1a0 net/ipv4/ip_output.c:126
     iptunnel_xmit+0x628/0xa50 net/ipv4/ip_tunnel_core.c:82
     geneve_xmit_skb drivers/net/geneve.c:966 [inline]
     geneve_xmit+0x10c8/0x3530 drivers/net/geneve.c:1077
     __netdev_start_xmit include/linux/netdevice.h:4683 [inline]
     netdev_start_xmit include/linux/netdevice.h:4697 [inline]
     xmit_one net/core/dev.c:3473 [inline]
     dev_hard_start_xmit+0x1eb/0x920 net/core/dev.c:3489
     __dev_queue_xmit+0x2985/0x3660 net/core/dev.c:4116
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip6_finish_output2+0xf7a/0x14f0 net/ipv6/ip6_output.c:126
     __ip6_finish_output net/ipv6/ip6_output.c:191 [inline]
     __ip6_finish_output+0x61e/0xe90 net/ipv6/ip6_output.c:170
     ip6_finish_output+0x32/0x200 net/ipv6/ip6_output.c:201
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip6_output+0x1e4/0x530 net/ipv6/ip6_output.c:224
     dst_output include/net/dst.h:451 [inline]
     NF_HOOK include/linux/netfilter.h:307 [inline]
     NF_HOOK include/linux/netfilter.h:301 [inline]
     mld_sendpack+0x9a3/0xe40 net/ipv6/mcast.c:1826
     mld_send_cr net/ipv6/mcast.c:2127 [inline]
     mld_ifc_work+0x71c/0xdc0 net/ipv6/mcast.c:2659
     process_one_work+0x9ac/0x1650 kernel/workqueue.c:2307
     worker_thread+0x657/0x1110 kernel/workqueue.c:2454
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    ----------------
    Code disassembly (best guess):
       0:   48 89 eb                mov    %rbp,%rbx
       3:   c6 45 01 01             movb   $0x1,0x1(%rbp)
       7:   41 bc 00 80 00 00       mov    $0x8000,%r12d
       d:   48 c1 e9 03             shr    $0x3,%rcx
      11:   83 e3 07                and    $0x7,%ebx
      14:   41 be 01 00 00 00       mov    $0x1,%r14d
      1a:   48 b8 00 00 00 00 00    movabs $0xdffffc0000000000,%rax
      21:   fc ff df
      24:   4c 8d 2c 01             lea    (%rcx,%rax,1),%r13
      28:   eb 0c                   jmp    0x36
    * 2a:   f3 90                   pause <-- trapping instruction
      2c:   41 83 ec 01             sub    $0x1,%r12d
      30:   0f 84 72 04 00 00       je     0x4a8
      36:   41 0f b6 45 00          movzbl 0x0(%r13),%eax
      3b:   38 d8                   cmp    %bl,%al
      3d:   7f 08                   jg     0x47
      3f:   84                      .byte 0x84
    
    Fixes: 1da177e4c3f4 ("Linux-2.6.12-rc2")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Jamal Hadi Salim <jhs@mojatatu.com>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Jiri Pirko <jiri@resnulli.us>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Link: https://lore.kernel.org/r/20220215235305.3272331-1-eric.dumazet@gmail.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ec756e40e271866f951d77c5e923d8deb6002b15
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Fri Feb 18 00:10:38 2022 -0800

    x86/kvm: Don't use pv tlb/ipi/sched_yield if on 1 vCPU
    
    Inspired by commit 3553ae5690a (x86/kvm: Don't use pvqspinlock code if
    only 1 vCPU), on a VM with only 1 vCPU, there is no need to enable
    pv tlb/ipi/sched_yield and we can save the memory for __pv_cpu_mask.
    
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Message-Id: <1645171838-2855-1-git-send-email-wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit 5740d068909676d4bdb5c9c00c37a83df7728909
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Feb 15 15:53:05 2022 -0800

    net: sched: limit TC_ACT_REPEAT loops
    
    We have been living dangerously, at the mercy of malicious users,
    abusing TC_ACT_REPEAT, as shown by this syzpot report [1].
    
    Add an arbitrary limit (32) to the number of times an action can
    return TC_ACT_REPEAT.
    
    v2: switch the limit to 32 instead of 10.
        Use net_warn_ratelimited() instead of pr_err_once().
    
    [1] (C repro available on demand)
    
    rcu: INFO: rcu_preempt self-detected stall on CPU
    rcu:    1-...!: (10500 ticks this GP) idle=021/1/0x4000000000000000 softirq=5592/5592 fqs=0
            (t=10502 jiffies g=5305 q=190)
    rcu: rcu_preempt kthread timer wakeup didn't happen for 10502 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402
    rcu:    Possible timer handling issue on cpu=0 timer-softirq=3527
    rcu: rcu_preempt kthread starved for 10505 jiffies! g5305 f0x0 RCU_GP_WAIT_FQS(5) ->state=0x402 ->cpu=0
    rcu:    Unless rcu_preempt kthread gets sufficient CPU time, OOM is now expected behavior.
    rcu: RCU grace-period kthread stack dump:
    task:rcu_preempt     state:I stack:29344 pid:   14 ppid:     2 flags:0x00004000
    Call Trace:
     <TASK>
     context_switch kernel/sched/core.c:4986 [inline]
     __schedule+0xab2/0x4db0 kernel/sched/core.c:6295
     schedule+0xd2/0x260 kernel/sched/core.c:6368
     schedule_timeout+0x14a/0x2a0 kernel/time/timer.c:1881
     rcu_gp_fqs_loop+0x186/0x810 kernel/rcu/tree.c:1963
     rcu_gp_kthread+0x1de/0x320 kernel/rcu/tree.c:2136
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    rcu: Stack dump where RCU GP kthread last ran:
    Sending NMI from CPU 1 to CPUs 0:
    NMI backtrace for cpu 0
    CPU: 0 PID: 3646 Comm: syz-executor358 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:rep_nop arch/x86/include/asm/vdso/processor.h:13 [inline]
    RIP: 0010:cpu_relax arch/x86/include/asm/vdso/processor.h:18 [inline]
    RIP: 0010:pv_wait_head_or_lock kernel/locking/qspinlock_paravirt.h:437 [inline]
    RIP: 0010:__pv_queued_spin_lock_slowpath+0x3b8/0xb40 kernel/locking/qspinlock.c:508
    Code: 48 89 eb c6 45 01 01 41 bc 00 80 00 00 48 c1 e9 03 83 e3 07 41 be 01 00 00 00 48 b8 00 00 00 00 00 fc ff df 4c 8d 2c 01 eb 0c <f3> 90 41 83 ec 01 0f 84 72 04 00 00 41 0f b6 45 00 38 d8 7f 08 84
    RSP: 0018:ffffc9000283f1b0 EFLAGS: 00000206
    RAX: 0000000000000003 RBX: 0000000000000000 RCX: 1ffff1100fc0071e
    RDX: 0000000000000001 RSI: 0000000000000201 RDI: 0000000000000000
    RBP: ffff88807e0038f0 R08: 0000000000000001 R09: ffffffff8ffbf9ff
    R10: 0000000000000001 R11: 0000000000000001 R12: 0000000000004c1e
    R13: ffffed100fc0071e R14: 0000000000000001 R15: ffff8880b9c3aa80
    FS:  00005555562bf300(0000) GS:ffff8880b9c00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007ffdbfef12b8 CR3: 00000000723c2000 CR4: 00000000003506f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     <TASK>
     pv_queued_spin_lock_slowpath arch/x86/include/asm/paravirt.h:591 [inline]
     queued_spin_lock_slowpath arch/x86/include/asm/qspinlock.h:51 [inline]
     queued_spin_lock include/asm-generic/qspinlock.h:85 [inline]
     do_raw_spin_lock+0x200/0x2b0 kernel/locking/spinlock_debug.c:115
     spin_lock_bh include/linux/spinlock.h:354 [inline]
     sch_tree_lock include/net/sch_generic.h:610 [inline]
     sch_tree_lock include/net/sch_generic.h:605 [inline]
     prio_tune+0x3b9/0xb50 net/sched/sch_prio.c:211
     prio_init+0x5c/0x80 net/sched/sch_prio.c:244
     qdisc_create.constprop.0+0x44a/0x10f0 net/sched/sch_api.c:1253
     tc_modify_qdisc+0x4c5/0x1980 net/sched/sch_api.c:1660
     rtnetlink_rcv_msg+0x413/0xb80 net/core/rtnetlink.c:5594
     netlink_rcv_skb+0x153/0x420 net/netlink/af_netlink.c:2494
     netlink_unicast_kernel net/netlink/af_netlink.c:1317 [inline]
     netlink_unicast+0x539/0x7e0 net/netlink/af_netlink.c:1343
     netlink_sendmsg+0x904/0xe00 net/netlink/af_netlink.c:1919
     sock_sendmsg_nosec net/socket.c:705 [inline]
     sock_sendmsg+0xcf/0x120 net/socket.c:725
     ____sys_sendmsg+0x6e8/0x810 net/socket.c:2413
     ___sys_sendmsg+0xf3/0x170 net/socket.c:2467
     __sys_sendmsg+0xe5/0x1b0 net/socket.c:2496
     do_syscall_x64 arch/x86/entry/common.c:50 [inline]
     do_syscall_64+0x35/0xb0 arch/x86/entry/common.c:80
     entry_SYSCALL_64_after_hwframe+0x44/0xae
    RIP: 0033:0x7f7ee98aae99
    Code: 28 00 00 00 75 05 48 83 c4 28 c3 e8 41 15 00 00 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 73 01 c3 48 c7 c1 c0 ff ff ff f7 d8 64 89 01 48
    RSP: 002b:00007ffdbfef12d8 EFLAGS: 00000246 ORIG_RAX: 000000000000002e
    RAX: ffffffffffffffda RBX: 00007ffdbfef1300 RCX: 00007f7ee98aae99
    RDX: 0000000000000000 RSI: 0000000020000000 RDI: 0000000000000003
    RBP: 0000000000000000 R08: 000000000000000d R09: 000000000000000d
    R10: 000000000000000d R11: 0000000000000246 R12: 00007ffdbfef12f0
    R13: 00000000000f4240 R14: 000000000004ca47 R15: 00007ffdbfef12e4
     </TASK>
    INFO: NMI handler (nmi_cpu_backtrace_handler) took too long to run: 2.293 msecs
    NMI backtrace for cpu 1
    CPU: 1 PID: 3260 Comm: kworker/1:3 Not tainted 5.17.0-rc3-syzkaller-00149-gbf8e59fd315f #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Workqueue: mld mld_ifc_work
    Call Trace:
     <IRQ>
     __dump_stack lib/dump_stack.c:88 [inline]
     dump_stack_lvl+0xcd/0x134 lib/dump_stack.c:106
     nmi_cpu_backtrace.cold+0x47/0x144 lib/nmi_backtrace.c:111
     nmi_trigger_cpumask_backtrace+0x1b3/0x230 lib/nmi_backtrace.c:62
     trigger_single_cpu_backtrace include/linux/nmi.h:164 [inline]
     rcu_dump_cpu_stacks+0x25e/0x3f0 kernel/rcu/tree_stall.h:343
     print_cpu_stall kernel/rcu/tree_stall.h:604 [inline]
     check_cpu_stall kernel/rcu/tree_stall.h:688 [inline]
     rcu_pending kernel/rcu/tree.c:3919 [inline]
     rcu_sched_clock_irq.cold+0x5c/0x759 kernel/rcu/tree.c:2617
     update_process_times+0x16d/0x200 kernel/time/timer.c:1785
     tick_sched_handle+0x9b/0x180 kernel/time/tick-sched.c:226
     tick_sched_timer+0x1b0/0x2d0 kernel/time/tick-sched.c:1428
     __run_hrtimer kernel/time/hrtimer.c:1685 [inline]
     __hrtimer_run_queues+0x1c0/0xe50 kernel/time/hrtimer.c:1749
     hrtimer_interrupt+0x31c/0x790 kernel/time/hrtimer.c:1811
     local_apic_timer_interrupt arch/x86/kernel/apic/apic.c:1086 [inline]
     __sysvec_apic_timer_interrupt+0x146/0x530 arch/x86/kernel/apic/apic.c:1103
     sysvec_apic_timer_interrupt+0x8e/0xc0 arch/x86/kernel/apic/apic.c:1097
     </IRQ>
     <TASK>
     asm_sysvec_apic_timer_interrupt+0x12/0x20 arch/x86/include/asm/idtentry.h:638
    RIP: 0010:__sanitizer_cov_trace_const_cmp4+0xc/0x70 kernel/kcov.c:286
    Code: 00 00 00 48 89 7c 30 e8 48 89 4c 30 f0 4c 89 54 d8 20 48 89 10 5b c3 0f 1f 80 00 00 00 00 41 89 f8 bf 03 00 00 00 4c 8b 14 24 <89> f1 65 48 8b 34 25 00 70 02 00 e8 14 f9 ff ff 84 c0 74 4b 48 8b
    RSP: 0018:ffffc90002c5eea8 EFLAGS: 00000246
    RAX: 0000000000000007 RBX: ffff88801c625800 RCX: 0000000000000000
    RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000003
    RBP: ffff8880137d3100 R08: 0000000000000000 R09: 0000000000000000
    R10: ffffffff874fcd88 R11: 0000000000000000 R12: ffff88801d692dc0
    R13: ffff8880137d3104 R14: 0000000000000000 R15: ffff88801d692de8
     tcf_police_act+0x358/0x11d0 net/sched/act_police.c:256
     tcf_action_exec net/sched/act_api.c:1049 [inline]
     tcf_action_exec+0x1a6/0x530 net/sched/act_api.c:1026
     tcf_exts_exec include/net/pkt_cls.h:326 [inline]
     route4_classify+0xef0/0x1400 net/sched/cls_route.c:179
     __tcf_classify net/sched/cls_api.c:1549 [inline]
     tcf_classify+0x3e8/0x9d0 net/sched/cls_api.c:1615
     prio_classify net/sched/sch_prio.c:42 [inline]
     prio_enqueue+0x3a7/0x790 net/sched/sch_prio.c:75
     dev_qdisc_enqueue+0x40/0x300 net/core/dev.c:3668
     __dev_xmit_skb net/core/dev.c:3756 [inline]
     __dev_queue_xmit+0x1f61/0x3660 net/core/dev.c:4081
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip_finish_output2+0x14dc/0x2170 net/ipv4/ip_output.c:228
     __ip_finish_output net/ipv4/ip_output.c:306 [inline]
     __ip_finish_output+0x396/0x650 net/ipv4/ip_output.c:288
     ip_finish_output+0x32/0x200 net/ipv4/ip_output.c:316
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip_output+0x196/0x310 net/ipv4/ip_output.c:430
     dst_output include/net/dst.h:451 [inline]
     ip_local_out+0xaf/0x1a0 net/ipv4/ip_output.c:126
     iptunnel_xmit+0x628/0xa50 net/ipv4/ip_tunnel_core.c:82
     geneve_xmit_skb drivers/net/geneve.c:966 [inline]
     geneve_xmit+0x10c8/0x3530 drivers/net/geneve.c:1077
     __netdev_start_xmit include/linux/netdevice.h:4683 [inline]
     netdev_start_xmit include/linux/netdevice.h:4697 [inline]
     xmit_one net/core/dev.c:3473 [inline]
     dev_hard_start_xmit+0x1eb/0x920 net/core/dev.c:3489
     __dev_queue_xmit+0x2985/0x3660 net/core/dev.c:4116
     neigh_hh_output include/net/neighbour.h:533 [inline]
     neigh_output include/net/neighbour.h:547 [inline]
     ip6_finish_output2+0xf7a/0x14f0 net/ipv6/ip6_output.c:126
     __ip6_finish_output net/ipv6/ip6_output.c:191 [inline]
     __ip6_finish_output+0x61e/0xe90 net/ipv6/ip6_output.c:170
     ip6_finish_output+0x32/0x200 net/ipv6/ip6_output.c:201
     NF_HOOK_COND include/linux/netfilter.h:296 [inline]
     ip6_output+0x1e4/0x530 net/ipv6/ip6_output.c:224
     dst_output include/net/dst.h:451 [inline]
     NF_HOOK include/linux/netfilter.h:307 [inline]
     NF_HOOK include/linux/netfilter.h:301 [inline]
     mld_sendpack+0x9a3/0xe40 net/ipv6/mcast.c:1826
     mld_send_cr net/ipv6/mcast.c:2127 [inline]
     mld_ifc_work+0x71c/0xdc0 net/ipv6/mcast.c:2659
     process_one_work+0x9ac/0x1650 kernel/workqueue.c:2307
     worker_thread+0x657/0x1110 kernel/workqueue.c:2454
     kthread+0x2e9/0x3a0 kernel/kthread.c:377
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:295
     </TASK>
    ----------------
    Code disassembly (best guess):
       0:   48 89 eb                mov    %rbp,%rbx
       3:   c6 45 01 01             movb   $0x1,0x1(%rbp)
       7:   41 bc 00 80 00 00       mov    $0x8000,%r12d
       d:   48 c1 e9 03             shr    $0x3,%rcx
      11:   83 e3 07                and    $0x7,%ebx
      14:   41 be 01 00 00 00       mov    $0x1,%r14d
      1a:   48 b8 00 00 00 00 00    movabs $0xdffffc0000000000,%rax
      21:   fc ff df
      24:   4c 8d 2c 01             lea    (%rcx,%rax,1),%r13
      28:   eb 0c                   jmp    0x36
    * 2a:   f3 90                   pause <-- trapping instruction
      2c:   41 83 ec 01             sub    $0x1,%r12d
      30:   0f 84 72 04 00 00       je     0x4a8
      36:   41 0f b6 45 00          movzbl 0x0(%r13),%eax
      3b:   38 d8                   cmp    %bl,%al
      3d:   7f 08                   jg     0x47
      3f:   84                      .byte 0x84
    
    Fixes: 1da177e4c3f4 ("Linux-2.6.12-rc2")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Acked-by: Jamal Hadi Salim <jhs@mojatatu.com>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Jiri Pirko <jiri@resnulli.us>
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Link: https://lore.kernel.org/r/20220215235305.3272331-1-eric.dumazet@gmail.com
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>

commit 1be5bdf8cd5a194d981e65687367b0828c839c37
Merge: 1c824bf768d6 b473a3891c46
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 11 09:51:26 2022 -0800

    Merge tag 'kcsan.2022.01.09a' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu
    
    Pull KCSAN updates from Paul McKenney:
     "This provides KCSAN fixes and also the ability to take memory barriers
      into account for weakly-ordered systems. This last can increase the
      probability of detecting certain types of data races"
    
    * tag 'kcsan.2022.01.09a' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu: (29 commits)
      kcsan: Only test clear_bit_unlock_is_negative_byte if arch defines it
      kcsan: Avoid nested contexts reading inconsistent reorder_access
      kcsan: Turn barrier instrumentation into macros
      kcsan: Make barrier tests compatible with lockdep
      kcsan: Support WEAK_MEMORY with Clang where no objtool support exists
      compiler_attributes.h: Add __disable_sanitizer_instrumentation
      objtool, kcsan: Remove memory barrier instrumentation from noinstr
      objtool, kcsan: Add memory barrier instrumentation to whitelist
      sched, kcsan: Enable memory barrier instrumentation
      mm, kcsan: Enable barrier instrumentation
      x86/qspinlock, kcsan: Instrument barrier of pv_queued_spin_unlock()
      x86/barriers, kcsan: Use generic instrumentation for non-smp barriers
      asm-generic/bitops, kcsan: Add instrumentation for barriers
      locking/atomics, kcsan: Add instrumentation for barriers
      locking/barriers, kcsan: Support generic instrumentation
      locking/barriers, kcsan: Add instrumentation for barriers
      kcsan: selftest: Add test case to check memory barrier instrumentation
      kcsan: Ignore GCC 11+ warnings about TSan runtime support
      kcsan: test: Add test cases for memory barrier instrumentation
      kcsan: test: Match reordered or normal accesses
      ...

commit e3d2b72bbf3c580b0c5c96385777c2f483a45ab5
Author: Marco Elver <elver@google.com>
Date:   Mon Dec 6 07:41:50 2021 +0100

    kcsan: Avoid nested contexts reading inconsistent reorder_access
    
    Nested contexts, such as nested interrupts or scheduler code, share the
    same kcsan_ctx. When such a nested context reads an inconsistent
    reorder_access due to an interrupt during set_reorder_access(), we can
    observe the following warning:
    
     | ------------[ cut here ]------------
     | Cannot find frame for torture_random kernel/torture.c:456 in stack trace
     | WARNING: CPU: 13 PID: 147 at kernel/kcsan/report.c:343 replace_stack_entry kernel/kcsan/report.c:343
     | ...
     | Call Trace:
     |  <TASK>
     |  sanitize_stack_entries kernel/kcsan/report.c:351 [inline]
     |  print_report kernel/kcsan/report.c:409
     |  kcsan_report_known_origin kernel/kcsan/report.c:693
     |  kcsan_setup_watchpoint kernel/kcsan/core.c:658
     |  rcutorture_one_extend kernel/rcu/rcutorture.c:1475
     |  rcutorture_loop_extend kernel/rcu/rcutorture.c:1558 [inline]
     |  ...
     |  </TASK>
     | ---[ end trace ee5299cb933115f5 ]---
     | ==================================================================
     | BUG: KCSAN: data-race in _raw_spin_lock_irqsave / rcutorture_one_extend
     |
     | write (reordered) to 0xffffffff8c93b300 of 8 bytes by task 154 on cpu 12:
     |  queued_spin_lock                include/asm-generic/qspinlock.h:80 [inline]
     |  do_raw_spin_lock                include/linux/spinlock.h:185 [inline]
     |  __raw_spin_lock_irqsave         include/linux/spinlock_api_smp.h:111 [inline]
     |  _raw_spin_lock_irqsave          kernel/locking/spinlock.c:162
     |  try_to_wake_up                  kernel/sched/core.c:4003
     |  sysvec_apic_timer_interrupt     arch/x86/kernel/apic/apic.c:1097
     |  asm_sysvec_apic_timer_interrupt arch/x86/include/asm/idtentry.h:638
     |  set_reorder_access              kernel/kcsan/core.c:416 [inline]    <-- inconsistent reorder_access
     |  kcsan_setup_watchpoint          kernel/kcsan/core.c:693
     |  rcutorture_one_extend           kernel/rcu/rcutorture.c:1475
     |  rcutorture_loop_extend          kernel/rcu/rcutorture.c:1558 [inline]
     |  rcu_torture_one_read            kernel/rcu/rcutorture.c:1600
     |  rcu_torture_reader              kernel/rcu/rcutorture.c:1692
     |  kthread                         kernel/kthread.c:327
     |  ret_from_fork                   arch/x86/entry/entry_64.S:295
     |
     | read to 0xffffffff8c93b300 of 8 bytes by task 147 on cpu 13:
     |  rcutorture_one_extend           kernel/rcu/rcutorture.c:1475
     |  rcutorture_loop_extend          kernel/rcu/rcutorture.c:1558 [inline]
     |  ...
    
    The warning is telling us that there was a data race which KCSAN wants
    to report, but the function where the original access (that is now
    reordered) happened cannot be found in the stack trace, which prevents
    KCSAN from generating the right stack trace. The stack trace of "write
    (reordered)" now only shows where the access was reordered to, but
    should instead show the stack trace of the original write, with a final
    line saying "reordered to".
    
    At the point where set_reorder_access() is interrupted, it just set
    reorder_access->ptr and size, at which point size is non-zero. This is
    sufficient (if ctx->disable_scoped is zero) for further accesses from
    nested contexts to perform checking of this reorder_access.
    
    That then happened in _raw_spin_lock_irqsave(), which is called by
    scheduler code. However, since reorder_access->ip is still stale (ptr
    and size belong to a different ip not yet set) this finally leads to
    replace_stack_entry() not finding the frame in reorder_access->ip and
    generating the above warning.
    
    Fix it by ensuring that a nested context cannot access reorder_access
    while we update it in set_reorder_access(): set ctx->disable_scoped for
    the duration that reorder_access is updated, which effectively locks
    reorder_access and prevents concurrent use by nested contexts. Note,
    set_reorder_access() can do the update only if disabled_scoped is zero
    on entry, and must therefore set disable_scoped back to non-zero after
    the initial check in set_reorder_access().
    
    Signed-off-by: Marco Elver <elver@google.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit d93414e37586691dd2684a7f8ed05fb9cd640f83
Author: Marco Elver <elver@google.com>
Date:   Tue Nov 30 12:44:27 2021 +0100

    x86/qspinlock, kcsan: Instrument barrier of pv_queued_spin_unlock()
    
    If CONFIG_PARAVIRT_SPINLOCKS=y, queued_spin_unlock() is implemented
    using pv_queued_spin_unlock() which is entirely inline asm based. As
    such, we do not receive any KCSAN barrier instrumentation via regular
    atomic operations.
    
    Add the missing KCSAN barrier instrumentation for the
    CONFIG_PARAVIRT_SPINLOCKS case.
    
    Signed-off-by: Marco Elver <elver@google.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit 748786564a358945922aa43a5b90710c81ed133e
Author: Lee Jones <lee.jones@linaro.org>
Date:   Tue Nov 2 15:49:30 2021 +0000

    Revert "io_uring: reinforce cancel on flush during exit"
    
    This reverts commit 88dbd085a51ec78c83dde79ad63bca8aa4272a9d.
    
    Causes the following Syzkaller reported issue:
    
    BUG: kernel NULL pointer dereference, address: 0000000000000010
    PGD 0 P4D 0
    Oops: 0002 [#1] PREEMPT SMP KASAN
    CPU: 1 PID: 546 Comm: syz-executor631 Tainted: G    B             5.10.76-syzkaller-01178-g4944ec82ebb9 #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    RIP: 0010:arch_atomic_try_cmpxchg syzkaller/managers/android-5-10/kernel/./arch/x86/include/asm/atomic.h:202 [inline]
    RIP: 0010:atomic_try_cmpxchg_acquire syzkaller/managers/android-5-10/kernel/./include/asm-generic/atomic-instrumented.h:707 [inline]
    RIP: 0010:queued_spin_lock syzkaller/managers/android-5-10/kernel/./include/asm-generic/qspinlock.h:82 [inline]
    RIP: 0010:do_raw_spin_lock_flags syzkaller/managers/android-5-10/kernel/./include/linux/spinlock.h:195 [inline]
    RIP: 0010:__raw_spin_lock_irqsave syzkaller/managers/android-5-10/kernel/./include/linux/spinlock_api_smp.h:119 [inline]
    RIP: 0010:_raw_spin_lock_irqsave+0x10d/0x210 syzkaller/managers/android-5-10/kernel/kernel/locking/spinlock.c:159
    Code: 00 00 00 e8 d5 29 09 fd 4c 89 e7 be 04 00 00 00 e8 c8 29 09 fd 42 8a 04 3b 84 c0 0f 85 be 00 00 00 8b 44 24 40 b9 01 00 00 00 <f0> 41 0f b1 4d 00 75 45 48 c7 44 24 20 0e 36 e0 45 4b c7 04 37 00
    RSP: 0018:ffffc90000f174e0 EFLAGS: 00010097
    RAX: 0000000000000000 RBX: 1ffff920001e2ea4 RCX: 0000000000000001
    RDX: 0000000000000001 RSI: 0000000000000004 RDI: ffffc90000f17520
    RBP: ffffc90000f175b0 R08: dffffc0000000000 R09: 0000000000000003
    R10: fffff520001e2ea5 R11: 0000000000000004 R12: ffffc90000f17520
    R13: 0000000000000010 R14: 1ffff920001e2ea0 R15: dffffc0000000000
    FS:  0000000000000000(0000) GS:ffff8881f7100000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000000000010 CR3: 000000000640f000 CR4: 00000000003506a0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
     prepare_to_wait+0x9c/0x290 syzkaller/managers/android-5-10/kernel/kernel/sched/wait.c:248
     io_uring_cancel_files syzkaller/managers/android-5-10/kernel/fs/io_uring.c:8690 [inline]
     io_uring_cancel_task_requests+0x16a9/0x1ed0 syzkaller/managers/android-5-10/kernel/fs/io_uring.c:8760
     io_uring_flush+0x170/0x6d0 syzkaller/managers/android-5-10/kernel/fs/io_uring.c:8923
     filp_close+0xb0/0x150 syzkaller/managers/android-5-10/kernel/fs/open.c:1319
     close_files syzkaller/managers/android-5-10/kernel/fs/file.c:401 [inline]
     put_files_struct+0x1d4/0x350 syzkaller/managers/android-5-10/kernel/fs/file.c:429
     exit_files+0x80/0xa0 syzkaller/managers/android-5-10/kernel/fs/file.c:458
     do_exit+0x6d9/0x23a0 syzkaller/managers/android-5-10/kernel/kernel/exit.c:808
     do_group_exit+0x16a/0x2d0 syzkaller/managers/android-5-10/kernel/kernel/exit.c:910
     get_signal+0x133e/0x1f80 syzkaller/managers/android-5-10/kernel/kernel/signal.c:2790
     arch_do_signal+0x8d/0x620 syzkaller/managers/android-5-10/kernel/arch/x86/kernel/signal.c:805
     exit_to_user_mode_loop syzkaller/managers/android-5-10/kernel/kernel/entry/common.c:161 [inline]
     exit_to_user_mode_prepare+0xaa/0xe0 syzkaller/managers/android-5-10/kernel/kernel/entry/common.c:191
     syscall_exit_to_user_mode+0x24/0x40 syzkaller/managers/android-5-10/kernel/kernel/entry/common.c:266
     do_syscall_64+0x3d/0x70 syzkaller/managers/android-5-10/kernel/arch/x86/entry/common.c:56
     entry_SYSCALL_64_after_hwframe+0x44/0xa9
    RIP: 0033:0x7fc6d1589a89
    Code: Unable to access opcode bytes at RIP 0x7fc6d1589a5f.
    RSP: 002b:00007ffd2b5da728 EFLAGS: 00000246 ORIG_RAX: 00000000000000ca
    RAX: fffffffffffffdfc RBX: 0000000000005193 RCX: 00007fc6d1589a89
    RDX: 0000000000000000 RSI: 0000000000000080 RDI: 00007fc6d161142c
    RBP: 0000000000000032 R08: 00007ffd2b5eb0b8 R09: 0000000000000000
    R10: 00007ffd2b5da750 R11: 0000000000000246 R12: 00007fc6d161142c
    R13: 00007ffd2b5da750 R14: 00007ffd2b5da770 R15: 0000000000000000
    Modules linked in:
    CR2: 0000000000000010
    ---[ end trace fe8044f7dc4d8d65 ]---
    RIP: 0010:arch_atomic_try_cmpxchg syzkaller/managers/android-5-10/kernel/./arch/x86/include/asm/atomic.h:202 [inline]
    RIP: 0010:atomic_try_cmpxchg_acquire syzkaller/managers/android-5-10/kernel/./include/asm-generic/atomic-instrumented.h:707 [inline]
    RIP: 0010:queued_spin_lock syzkaller/managers/android-5-10/kernel/./include/asm-generic/qspinlock.h:82 [inline]
    RIP: 0010:do_raw_spin_lock_flags syzkaller/managers/android-5-10/kernel/./include/linux/spinlock.h:195 [inline]
    RIP: 0010:__raw_spin_lock_irqsave syzkaller/managers/android-5-10/kernel/./include/linux/spinlock_api_smp.h:119 [inline]
    RIP: 0010:_raw_spin_lock_irqsave+0x10d/0x210 syzkaller/managers/android-5-10/kernel/kernel/locking/spinlock.c:159
    Code: 00 00 00 e8 d5 29 09 fd 4c 89 e7 be 04 00 00 00 e8 c8 29 09 fd 42 8a 04 3b 84 c0 0f 85 be 00 00 00 8b 44 24 40 b9 01 00 00 00 <f0> 41 0f b1 4d 00 75 45 48 c7 44 24 20 0e 36 e0 45 4b c7 04 37 00
    RSP: 0018:ffffc90000f174e0 EFLAGS: 00010097
    RAX: 0000000000000000 RBX: 1ffff920001e2ea4 RCX: 0000000000000001
    RDX: 0000000000000001 RSI: 0000000000000004 RDI: ffffc90000f17520
    RBP: ffffc90000f175b0 R08: dffffc0000000000 R09: 0000000000000003
    R10: fffff520001e2ea5 R11: 0000000000000004 R12: ffffc90000f17520
    R13: 0000000000000010 R14: 1ffff920001e2ea0 R15: dffffc0000000000
    FS:  0000000000000000(0000) GS:ffff8881f7100000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000000000010 CR3: 000000000640f000 CR4: 00000000003506a0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    ----------------
    Code disassembly (best guess), 1 bytes skipped:
       0:   00 00                   add    %al,(%rax)
       2:   e8 d5 29 09 fd          callq  0xfd0929dc
       7:   4c 89 e7                mov    %r12,%rdi
       a:   be 04 00 00 00          mov    $0x4,%esi
       f:   e8 c8 29 09 fd          callq  0xfd0929dc
      14:   42 8a 04 3b             mov    (%rbx,%r15,1),%al
      18:   84 c0                   test   %al,%al
      1a:   0f 85 be 00 00 00       jne    0xde
      20:   8b 44 24 40             mov    0x40(%rsp),%eax
      24:   b9 01 00 00 00          mov    $0x1,%ecx
    * 29:   f0 41 0f b1 4d 00       lock cmpxchg %ecx,0x0(%r13) <-- trapping instruction
      2f:   75 45                   jne    0x76
      31:   48 c7 44 24 20 0e 36    movq   $0x45e0360e,0x20(%rsp)
      38:   e0 45
      3a:   4b                      rex.WXB
      3b:   c7                      .byte 0xc7
      3c:   04 37                   add    $0x37,%al
    
    Link: https://syzkaller.appspot.com/bug?extid=b0003676644cf0d6acc4
    Reported-by: syzbot+b0003676644cf0d6acc4@syzkaller.appspotmail.com
    Signed-off-by: Lee Jones <lee.jones@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4414c8145485579a8f160b3af006f6f0f10bbbdf
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Apr 14 14:45:43 2021 +0200

    openrisc: Define memory barrier mb
    
    [ Upstream commit 8b549c18ae81dbc36fb11e4aa08b8378c599ca95 ]
    
    This came up in the discussion of the requirements of qspinlock on an
    architecture.  OpenRISC uses qspinlock, but it was noticed that the
    memmory barrier was not defined.
    
    Peter defined it in the mail thread writing:
    
        As near as I can tell this should do. The arch spec only lists
        this one instruction and the text makes it sound like a completion
        barrier.
    
    This is correct so applying this patch.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    [shorne@gmail.com:Turned the mail into a patch]
    Signed-off-by: Stafford Horne <shorne@gmail.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 71084e2a4d2fbe7fcb5ea455020318dd16520247
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Apr 14 14:45:43 2021 +0200

    openrisc: Define memory barrier mb
    
    [ Upstream commit 8b549c18ae81dbc36fb11e4aa08b8378c599ca95 ]
    
    This came up in the discussion of the requirements of qspinlock on an
    architecture.  OpenRISC uses qspinlock, but it was noticed that the
    memmory barrier was not defined.
    
    Peter defined it in the mail thread writing:
    
        As near as I can tell this should do. The arch spec only lists
        this one instruction and the text makes it sound like a completion
        barrier.
    
    This is correct so applying this patch.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    [shorne@gmail.com:Turned the mail into a patch]
    Signed-off-by: Stafford Horne <shorne@gmail.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit a103713429035da799fa05c7da113f969c0df992
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Apr 14 14:45:43 2021 +0200

    openrisc: Define memory barrier mb
    
    [ Upstream commit 8b549c18ae81dbc36fb11e4aa08b8378c599ca95 ]
    
    This came up in the discussion of the requirements of qspinlock on an
    architecture.  OpenRISC uses qspinlock, but it was noticed that the
    memmory barrier was not defined.
    
    Peter defined it in the mail thread writing:
    
        As near as I can tell this should do. The arch spec only lists
        this one instruction and the text makes it sound like a completion
        barrier.
    
    This is correct so applying this patch.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    [shorne@gmail.com:Turned the mail into a patch]
    Signed-off-by: Stafford Horne <shorne@gmail.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2191a85943ca848ec756ad79623774d2116a3be4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Apr 14 14:45:43 2021 +0200

    openrisc: Define memory barrier mb
    
    [ Upstream commit 8b549c18ae81dbc36fb11e4aa08b8378c599ca95 ]
    
    This came up in the discussion of the requirements of qspinlock on an
    architecture.  OpenRISC uses qspinlock, but it was noticed that the
    memmory barrier was not defined.
    
    Peter defined it in the mail thread writing:
    
        As near as I can tell this should do. The arch spec only lists
        this one instruction and the text makes it sound like a completion
        barrier.
    
    This is correct so applying this patch.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    [shorne@gmail.com:Turned the mail into a patch]
    Signed-off-by: Stafford Horne <shorne@gmail.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f1cff517d188485b93af2921c37c270df5c7da6d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Apr 14 14:45:43 2021 +0200

    openrisc: Define memory barrier mb
    
    [ Upstream commit 8b549c18ae81dbc36fb11e4aa08b8378c599ca95 ]
    
    This came up in the discussion of the requirements of qspinlock on an
    architecture.  OpenRISC uses qspinlock, but it was noticed that the
    memmory barrier was not defined.
    
    Peter defined it in the mail thread writing:
    
        As near as I can tell this should do. The arch spec only lists
        this one instruction and the text makes it sound like a completion
        barrier.
    
    This is correct so applying this patch.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    [shorne@gmail.com:Turned the mail into a patch]
    Signed-off-by: Stafford Horne <shorne@gmail.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit c156a75f428f602ede7b840a357cbc1aa33bded7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Apr 14 14:45:43 2021 +0200

    openrisc: Define memory barrier mb
    
    [ Upstream commit 8b549c18ae81dbc36fb11e4aa08b8378c599ca95 ]
    
    This came up in the discussion of the requirements of qspinlock on an
    architecture.  OpenRISC uses qspinlock, but it was noticed that the
    memmory barrier was not defined.
    
    Peter defined it in the mail thread writing:
    
        As near as I can tell this should do. The arch spec only lists
        this one instruction and the text makes it sound like a completion
        barrier.
    
    This is correct so applying this patch.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    [shorne@gmail.com:Turned the mail into a patch]
    Signed-off-by: Stafford Horne <shorne@gmail.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit ffe3b372c52920e0a4e12bc3162389b540b74693
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Apr 14 14:45:43 2021 +0200

    openrisc: Define memory barrier mb
    
    [ Upstream commit 8b549c18ae81dbc36fb11e4aa08b8378c599ca95 ]
    
    This came up in the discussion of the requirements of qspinlock on an
    architecture.  OpenRISC uses qspinlock, but it was noticed that the
    memmory barrier was not defined.
    
    Peter defined it in the mail thread writing:
    
        As near as I can tell this should do. The arch spec only lists
        this one instruction and the text makes it sound like a completion
        barrier.
    
    This is correct so applying this patch.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    [shorne@gmail.com:Turned the mail into a patch]
    Signed-off-by: Stafford Horne <shorne@gmail.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2ef3c76540c49167a0bc3d5f80d00fd1fc4586df
Author: Omar Sandoval <osandov@fb.com>
Date:   Mon May 10 17:05:35 2021 -0700

    kyber: fix out of bounds access when preempted
    
    [ Upstream commit efed9a3337e341bd0989161b97453b52567bc59d ]
    
    __blk_mq_sched_bio_merge() gets the ctx and hctx for the current CPU and
    passes the hctx to ->bio_merge(). kyber_bio_merge() then gets the ctx
    for the current CPU again and uses that to get the corresponding Kyber
    context in the passed hctx. However, the thread may be preempted between
    the two calls to blk_mq_get_ctx(), and the ctx returned the second time
    may no longer correspond to the passed hctx. This "works" accidentally
    most of the time, but it can cause us to read garbage if the second ctx
    came from an hctx with more ctx's than the first one (i.e., if
    ctx->index_hw[hctx->type] > hctx->nr_ctx).
    
    This manifested as this UBSAN array index out of bounds error reported
    by Jakub:
    
    UBSAN: array-index-out-of-bounds in ../kernel/locking/qspinlock.c:130:9
    index 13106 is out of range for type 'long unsigned int [128]'
    Call Trace:
     dump_stack+0xa4/0xe5
     ubsan_epilogue+0x5/0x40
     __ubsan_handle_out_of_bounds.cold.13+0x2a/0x34
     queued_spin_lock_slowpath+0x476/0x480
     do_raw_spin_lock+0x1c2/0x1d0
     kyber_bio_merge+0x112/0x180
     blk_mq_submit_bio+0x1f5/0x1100
     submit_bio_noacct+0x7b0/0x870
     submit_bio+0xc2/0x3a0
     btrfs_map_bio+0x4f0/0x9d0
     btrfs_submit_data_bio+0x24e/0x310
     submit_one_bio+0x7f/0xb0
     submit_extent_page+0xc4/0x440
     __extent_writepage_io+0x2b8/0x5e0
     __extent_writepage+0x28d/0x6e0
     extent_write_cache_pages+0x4d7/0x7a0
     extent_writepages+0xa2/0x110
     do_writepages+0x8f/0x180
     __writeback_single_inode+0x99/0x7f0
     writeback_sb_inodes+0x34e/0x790
     __writeback_inodes_wb+0x9e/0x120
     wb_writeback+0x4d2/0x660
     wb_workfn+0x64d/0xa10
     process_one_work+0x53a/0xa80
     worker_thread+0x69/0x5b0
     kthread+0x20b/0x240
     ret_from_fork+0x1f/0x30
    
    Only Kyber uses the hctx, so fix it by passing the request_queue to
    ->bio_merge() instead. BFQ and mq-deadline just use that, and Kyber can
    map the queues itself to avoid the mismatch.
    
    Fixes: a6088845c2bf ("block: kyber: make kyber more friendly with merging")
    Reported-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Link: https://lore.kernel.org/r/c7598605401a48d5cfeadebb678abd10af22b83f.1620691329.git.osandov@fb.com
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit a287cd84e047045f5a4d4da793414e848de627c6
Author: Omar Sandoval <osandov@fb.com>
Date:   Mon May 10 17:05:35 2021 -0700

    kyber: fix out of bounds access when preempted
    
    [ Upstream commit efed9a3337e341bd0989161b97453b52567bc59d ]
    
    __blk_mq_sched_bio_merge() gets the ctx and hctx for the current CPU and
    passes the hctx to ->bio_merge(). kyber_bio_merge() then gets the ctx
    for the current CPU again and uses that to get the corresponding Kyber
    context in the passed hctx. However, the thread may be preempted between
    the two calls to blk_mq_get_ctx(), and the ctx returned the second time
    may no longer correspond to the passed hctx. This "works" accidentally
    most of the time, but it can cause us to read garbage if the second ctx
    came from an hctx with more ctx's than the first one (i.e., if
    ctx->index_hw[hctx->type] > hctx->nr_ctx).
    
    This manifested as this UBSAN array index out of bounds error reported
    by Jakub:
    
    UBSAN: array-index-out-of-bounds in ../kernel/locking/qspinlock.c:130:9
    index 13106 is out of range for type 'long unsigned int [128]'
    Call Trace:
     dump_stack+0xa4/0xe5
     ubsan_epilogue+0x5/0x40
     __ubsan_handle_out_of_bounds.cold.13+0x2a/0x34
     queued_spin_lock_slowpath+0x476/0x480
     do_raw_spin_lock+0x1c2/0x1d0
     kyber_bio_merge+0x112/0x180
     blk_mq_submit_bio+0x1f5/0x1100
     submit_bio_noacct+0x7b0/0x870
     submit_bio+0xc2/0x3a0
     btrfs_map_bio+0x4f0/0x9d0
     btrfs_submit_data_bio+0x24e/0x310
     submit_one_bio+0x7f/0xb0
     submit_extent_page+0xc4/0x440
     __extent_writepage_io+0x2b8/0x5e0
     __extent_writepage+0x28d/0x6e0
     extent_write_cache_pages+0x4d7/0x7a0
     extent_writepages+0xa2/0x110
     do_writepages+0x8f/0x180
     __writeback_single_inode+0x99/0x7f0
     writeback_sb_inodes+0x34e/0x790
     __writeback_inodes_wb+0x9e/0x120
     wb_writeback+0x4d2/0x660
     wb_workfn+0x64d/0xa10
     process_one_work+0x53a/0xa80
     worker_thread+0x69/0x5b0
     kthread+0x20b/0x240
     ret_from_fork+0x1f/0x30
    
    Only Kyber uses the hctx, so fix it by passing the request_queue to
    ->bio_merge() instead. BFQ and mq-deadline just use that, and Kyber can
    map the queues itself to avoid the mismatch.
    
    Fixes: a6088845c2bf ("block: kyber: make kyber more friendly with merging")
    Reported-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Link: https://lore.kernel.org/r/c7598605401a48d5cfeadebb678abd10af22b83f.1620691329.git.osandov@fb.com
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 54dbe2d2c1fcabf650c7a8b747601da355cd7f9f
Author: Omar Sandoval <osandov@fb.com>
Date:   Mon May 10 17:05:35 2021 -0700

    kyber: fix out of bounds access when preempted
    
    [ Upstream commit efed9a3337e341bd0989161b97453b52567bc59d ]
    
    __blk_mq_sched_bio_merge() gets the ctx and hctx for the current CPU and
    passes the hctx to ->bio_merge(). kyber_bio_merge() then gets the ctx
    for the current CPU again and uses that to get the corresponding Kyber
    context in the passed hctx. However, the thread may be preempted between
    the two calls to blk_mq_get_ctx(), and the ctx returned the second time
    may no longer correspond to the passed hctx. This "works" accidentally
    most of the time, but it can cause us to read garbage if the second ctx
    came from an hctx with more ctx's than the first one (i.e., if
    ctx->index_hw[hctx->type] > hctx->nr_ctx).
    
    This manifested as this UBSAN array index out of bounds error reported
    by Jakub:
    
    UBSAN: array-index-out-of-bounds in ../kernel/locking/qspinlock.c:130:9
    index 13106 is out of range for type 'long unsigned int [128]'
    Call Trace:
     dump_stack+0xa4/0xe5
     ubsan_epilogue+0x5/0x40
     __ubsan_handle_out_of_bounds.cold.13+0x2a/0x34
     queued_spin_lock_slowpath+0x476/0x480
     do_raw_spin_lock+0x1c2/0x1d0
     kyber_bio_merge+0x112/0x180
     blk_mq_submit_bio+0x1f5/0x1100
     submit_bio_noacct+0x7b0/0x870
     submit_bio+0xc2/0x3a0
     btrfs_map_bio+0x4f0/0x9d0
     btrfs_submit_data_bio+0x24e/0x310
     submit_one_bio+0x7f/0xb0
     submit_extent_page+0xc4/0x440
     __extent_writepage_io+0x2b8/0x5e0
     __extent_writepage+0x28d/0x6e0
     extent_write_cache_pages+0x4d7/0x7a0
     extent_writepages+0xa2/0x110
     do_writepages+0x8f/0x180
     __writeback_single_inode+0x99/0x7f0
     writeback_sb_inodes+0x34e/0x790
     __writeback_inodes_wb+0x9e/0x120
     wb_writeback+0x4d2/0x660
     wb_workfn+0x64d/0xa10
     process_one_work+0x53a/0xa80
     worker_thread+0x69/0x5b0
     kthread+0x20b/0x240
     ret_from_fork+0x1f/0x30
    
    Only Kyber uses the hctx, so fix it by passing the request_queue to
    ->bio_merge() instead. BFQ and mq-deadline just use that, and Kyber can
    map the queues itself to avoid the mismatch.
    
    Fixes: a6088845c2bf ("block: kyber: make kyber more friendly with merging")
    Reported-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Link: https://lore.kernel.org/r/c7598605401a48d5cfeadebb678abd10af22b83f.1620691329.git.osandov@fb.com
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 0b6b4b90b74c27bea968c214d820ba4254b903a5
Author: Omar Sandoval <osandov@fb.com>
Date:   Mon May 10 17:05:35 2021 -0700

    kyber: fix out of bounds access when preempted
    
    [ Upstream commit efed9a3337e341bd0989161b97453b52567bc59d ]
    
    __blk_mq_sched_bio_merge() gets the ctx and hctx for the current CPU and
    passes the hctx to ->bio_merge(). kyber_bio_merge() then gets the ctx
    for the current CPU again and uses that to get the corresponding Kyber
    context in the passed hctx. However, the thread may be preempted between
    the two calls to blk_mq_get_ctx(), and the ctx returned the second time
    may no longer correspond to the passed hctx. This "works" accidentally
    most of the time, but it can cause us to read garbage if the second ctx
    came from an hctx with more ctx's than the first one (i.e., if
    ctx->index_hw[hctx->type] > hctx->nr_ctx).
    
    This manifested as this UBSAN array index out of bounds error reported
    by Jakub:
    
    UBSAN: array-index-out-of-bounds in ../kernel/locking/qspinlock.c:130:9
    index 13106 is out of range for type 'long unsigned int [128]'
    Call Trace:
     dump_stack+0xa4/0xe5
     ubsan_epilogue+0x5/0x40
     __ubsan_handle_out_of_bounds.cold.13+0x2a/0x34
     queued_spin_lock_slowpath+0x476/0x480
     do_raw_spin_lock+0x1c2/0x1d0
     kyber_bio_merge+0x112/0x180
     blk_mq_submit_bio+0x1f5/0x1100
     submit_bio_noacct+0x7b0/0x870
     submit_bio+0xc2/0x3a0
     btrfs_map_bio+0x4f0/0x9d0
     btrfs_submit_data_bio+0x24e/0x310
     submit_one_bio+0x7f/0xb0
     submit_extent_page+0xc4/0x440
     __extent_writepage_io+0x2b8/0x5e0
     __extent_writepage+0x28d/0x6e0
     extent_write_cache_pages+0x4d7/0x7a0
     extent_writepages+0xa2/0x110
     do_writepages+0x8f/0x180
     __writeback_single_inode+0x99/0x7f0
     writeback_sb_inodes+0x34e/0x790
     __writeback_inodes_wb+0x9e/0x120
     wb_writeback+0x4d2/0x660
     wb_workfn+0x64d/0xa10
     process_one_work+0x53a/0xa80
     worker_thread+0x69/0x5b0
     kthread+0x20b/0x240
     ret_from_fork+0x1f/0x30
    
    Only Kyber uses the hctx, so fix it by passing the request_queue to
    ->bio_merge() instead. BFQ and mq-deadline just use that, and Kyber can
    map the queues itself to avoid the mismatch.
    
    Fixes: a6088845c2bf ("block: kyber: make kyber more friendly with merging")
    Reported-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Link: https://lore.kernel.org/r/c7598605401a48d5cfeadebb678abd10af22b83f.1620691329.git.osandov@fb.com
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 8b549c18ae81dbc36fb11e4aa08b8378c599ca95
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Apr 14 14:45:43 2021 +0200

    openrisc: Define memory barrier mb
    
    This came up in the discussion of the requirements of qspinlock on an
    architecture.  OpenRISC uses qspinlock, but it was noticed that the
    memmory barrier was not defined.
    
    Peter defined it in the mail thread writing:
    
        As near as I can tell this should do. The arch spec only lists
        this one instruction and the text makes it sound like a completion
        barrier.
    
    This is correct so applying this patch.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    [shorne@gmail.com:Turned the mail into a patch]
    Signed-off-by: Stafford Horne <shorne@gmail.com>

commit efed9a3337e341bd0989161b97453b52567bc59d
Author: Omar Sandoval <osandov@fb.com>
Date:   Mon May 10 17:05:35 2021 -0700

    kyber: fix out of bounds access when preempted
    
    __blk_mq_sched_bio_merge() gets the ctx and hctx for the current CPU and
    passes the hctx to ->bio_merge(). kyber_bio_merge() then gets the ctx
    for the current CPU again and uses that to get the corresponding Kyber
    context in the passed hctx. However, the thread may be preempted between
    the two calls to blk_mq_get_ctx(), and the ctx returned the second time
    may no longer correspond to the passed hctx. This "works" accidentally
    most of the time, but it can cause us to read garbage if the second ctx
    came from an hctx with more ctx's than the first one (i.e., if
    ctx->index_hw[hctx->type] > hctx->nr_ctx).
    
    This manifested as this UBSAN array index out of bounds error reported
    by Jakub:
    
    UBSAN: array-index-out-of-bounds in ../kernel/locking/qspinlock.c:130:9
    index 13106 is out of range for type 'long unsigned int [128]'
    Call Trace:
     dump_stack+0xa4/0xe5
     ubsan_epilogue+0x5/0x40
     __ubsan_handle_out_of_bounds.cold.13+0x2a/0x34
     queued_spin_lock_slowpath+0x476/0x480
     do_raw_spin_lock+0x1c2/0x1d0
     kyber_bio_merge+0x112/0x180
     blk_mq_submit_bio+0x1f5/0x1100
     submit_bio_noacct+0x7b0/0x870
     submit_bio+0xc2/0x3a0
     btrfs_map_bio+0x4f0/0x9d0
     btrfs_submit_data_bio+0x24e/0x310
     submit_one_bio+0x7f/0xb0
     submit_extent_page+0xc4/0x440
     __extent_writepage_io+0x2b8/0x5e0
     __extent_writepage+0x28d/0x6e0
     extent_write_cache_pages+0x4d7/0x7a0
     extent_writepages+0xa2/0x110
     do_writepages+0x8f/0x180
     __writeback_single_inode+0x99/0x7f0
     writeback_sb_inodes+0x34e/0x790
     __writeback_inodes_wb+0x9e/0x120
     wb_writeback+0x4d2/0x660
     wb_workfn+0x64d/0xa10
     process_one_work+0x53a/0xa80
     worker_thread+0x69/0x5b0
     kthread+0x20b/0x240
     ret_from_fork+0x1f/0x30
    
    Only Kyber uses the hctx, so fix it by passing the request_queue to
    ->bio_merge() instead. BFQ and mq-deadline just use that, and Kyber can
    map the queues itself to avoid the mismatch.
    
    Fixes: a6088845c2bf ("block: kyber: make kyber more friendly with merging")
    Reported-by: Jakub Kicinski <kuba@kernel.org>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Link: https://lore.kernel.org/r/c7598605401a48d5cfeadebb678abd10af22b83f.1620691329.git.osandov@fb.com
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit deb9b13eb2571fbde164ae012c77985fd14f2f02
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Mon Mar 8 17:59:50 2021 -0800

    powerpc/qspinlock: Use generic smp_cond_load_relaxed
    
    49a7d46a06c3 (powerpc: Implement smp_cond_load_relaxed()) added
    busy-waiting pausing with a preferred SMT priority pattern, lowering
    the priority (reducing decode cycles) during the whole loop slowpath.
    
    However, data shows that while this pattern works well with simple
    spinlocks, queued spinlocks benefit more being kept in medium priority,
    with a cpu_relax() instead, being a low+medium combo on powerpc.
    
    Data is from three benchmarks on a Power9: 9008-22L 64 CPUs with
    2 sockets and 8 threads per core.
    
    1. locktorture.
    
    This is data for the lowest and most artificial/pathological level,
    with increasing thread counts pounding on the lock. Metrics are total
    ops/minute. Despite some small hits in the 4-8 range, scenarios are
    either neutral or favorable to this patch.
    
    +=========+==========+==========+=======+
    | # tasks | vanilla  | dirty    | %diff |
    +=========+==========+==========+=======+
    | 2       | 46718565 | 48751350 | 4.35  |
    +---------+----------+----------+-------+
    | 4       | 51740198 | 50369082 | -2.65 |
    +---------+----------+----------+-------+
    | 8       | 63756510 | 62568821 | -1.86 |
    +---------+----------+----------+-------+
    | 16      | 67824531 | 70966546 | 4.63  |
    +---------+----------+----------+-------+
    | 32      | 53843519 | 61155508 | 13.58 |
    +---------+----------+----------+-------+
    | 64      | 53005778 | 53104412 | 0.18  |
    +---------+----------+----------+-------+
    | 128     | 53331980 | 54606910 | 2.39  |
    +=========+==========+==========+=======+
    
    2. sockperf (tcp throughput)
    
    Here a client will do one-way throughput tests to a localhost server, with
    increasing message sizes, dealing with the sk_lock. This patch shows to put
    the performance of the qspinlock back to par with that of the simple lock:
    
                         simple-spinlock           vanilla                  dirty
    Hmean     14        73.50 (   0.00%)       54.44 * -25.93%*       73.45 * -0.07%*
    Hmean     100      654.47 (   0.00%)      385.61 * -41.08%*      771.43 * 17.87%*
    Hmean     300     2719.39 (   0.00%)     2181.67 * -19.77%*     2666.50 * -1.94%*
    Hmean     500     4400.59 (   0.00%)     3390.77 * -22.95%*     4322.14 * -1.78%*
    Hmean     850     6726.21 (   0.00%)     5264.03 * -21.74%*     6863.12 * 2.04%*
    
    3. dbench (tmpfs)
    
    Configured to run with up to ncpusx8 clients, it shows both latency and
    throughput metrics. For the latency, with the exception of the 64 case,
    there is really nothing to go by:
                                         vanilla                dirty
    Amean     latency-1          1.67 (   0.00%)        1.67 *   0.09%*
    Amean     latency-2          2.15 (   0.00%)        2.08 *   3.36%*
    Amean     latency-4          2.50 (   0.00%)        2.56 *  -2.27%*
    Amean     latency-8          2.49 (   0.00%)        2.48 *   0.31%*
    Amean     latency-16         2.69 (   0.00%)        2.72 *  -1.37%*
    Amean     latency-32         2.96 (   0.00%)        3.04 *  -2.60%*
    Amean     latency-64         7.78 (   0.00%)        8.17 *  -5.07%*
    Amean     latency-512      186.91 (   0.00%)      186.41 *   0.27%*
    
    For the dbench4 Throughput (misleading but traditional) there's a small
    but rather constant improvement:
    
                                 vanilla                dirty
    Hmean     1        849.13 (   0.00%)      851.51 *   0.28%*
    Hmean     2       1664.03 (   0.00%)     1663.94 *  -0.01%*
    Hmean     4       3073.70 (   0.00%)     3104.29 *   1.00%*
    Hmean     8       5624.02 (   0.00%)     5694.16 *   1.25%*
    Hmean     16      9169.49 (   0.00%)     9324.43 *   1.69%*
    Hmean     32     11969.37 (   0.00%)    12127.09 *   1.32%*
    Hmean     64     15021.12 (   0.00%)    15243.14 *   1.48%*
    Hmean     512    14891.27 (   0.00%)    15162.11 *   1.82%*
    
    Measuring the dbench4 Per-VFS Operation latency, shows some very minor
    differences within the noise level, around the 0-1% ranges.
    
    Fixes: 49a7d46a06c3 ("powerpc: Implement smp_cond_load_relaxed()")
    Acked-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20210318204702.71417-1-dave@stgolabs.net

commit 67c2d326332ee28079348e43cf4f17bbfe63b260
Author: Will Deacon <will@kernel.org>
Date:   Fri Mar 19 10:01:11 2021 +0000

    arm64: kvm: Add standalone ticket spinlock implementation for use at hyp
    
    We will soon need to synchronise multiple CPUs in the hyp text at EL2.
    The qspinlock-based locking used by the host is overkill for this purpose
    and relies on the kernel's "percpu" implementation for the MCS nodes.
    
    Implement a simple ticket locking scheme based heavily on the code removed
    by commit c11090474d70 ("arm64: locking: Replace ticket lock implementation
    with qspinlock").
    
    Signed-off-by: Will Deacon <will@kernel.org>
    Signed-off-by: Quentin Perret <qperret@google.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20210319100146.1149909-4-qperret@google.com

commit 3e10585335b7967326ca7b4118cada0d2d00a2ab
Merge: 9c5b80b795e9 8c6e67bec319
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 21 13:31:43 2021 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "x86:
    
       - Support for userspace to emulate Xen hypercalls
    
       - Raise the maximum number of user memslots
    
       - Scalability improvements for the new MMU.
    
         Instead of the complex "fast page fault" logic that is used in
         mmu.c, tdp_mmu.c uses an rwlock so that page faults are concurrent,
         but the code that can run against page faults is limited. Right now
         only page faults take the lock for reading; in the future this will
         be extended to some cases of page table destruction. I hope to
         switch the default MMU around 5.12-rc3 (some testing was delayed
         due to Chinese New Year).
    
       - Cleanups for MAXPHYADDR checks
    
       - Use static calls for vendor-specific callbacks
    
       - On AMD, use VMLOAD/VMSAVE to save and restore host state
    
       - Stop using deprecated jump label APIs
    
       - Workaround for AMD erratum that made nested virtualization
         unreliable
    
       - Support for LBR emulation in the guest
    
       - Support for communicating bus lock vmexits to userspace
    
       - Add support for SEV attestation command
    
       - Miscellaneous cleanups
    
      PPC:
    
       - Support for second data watchpoint on POWER10
    
       - Remove some complex workarounds for buggy early versions of POWER9
    
       - Guest entry/exit fixes
    
      ARM64:
    
       - Make the nVHE EL2 object relocatable
    
       - Cleanups for concurrent translation faults hitting the same page
    
       - Support for the standard TRNG hypervisor call
    
       - A bunch of small PMU/Debug fixes
    
       - Simplification of the early init hypercall handling
    
      Non-KVM changes (with acks):
    
       - Detection of contended rwlocks (implemented only for qrwlocks,
         because KVM only needs it for x86)
    
       - Allow __DISABLE_EXPORTS from assembly code
    
       - Provide a saner follow_pfn replacements for modules"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (192 commits)
      KVM: x86/xen: Explicitly pad struct compat_vcpu_info to 64 bytes
      KVM: selftests: Don't bother mapping GVA for Xen shinfo test
      KVM: selftests: Fix hex vs. decimal snafu in Xen test
      KVM: selftests: Fix size of memslots created by Xen tests
      KVM: selftests: Ignore recently added Xen tests' build output
      KVM: selftests: Add missing header file needed by xAPIC IPI tests
      KVM: selftests: Add operand to vmsave/vmload/vmrun in svm.c
      KVM: SVM: Make symbol 'svm_gp_erratum_intercept' static
      locking/arch: Move qrwlock.h include after qspinlock.h
      KVM: PPC: Book3S HV: Fix host radix SLB optimisation with hash guests
      KVM: PPC: Book3S HV: Ensure radix guest has no SLB entries
      KVM: PPC: Don't always report hash MMU capability for P9 < DD2.2
      KVM: PPC: Book3S HV: Save and restore FSCR in the P9 path
      KVM: PPC: remove unneeded semicolon
      KVM: PPC: Book3S HV: Use POWER9 SLBIA IH=6 variant to clear SLB
      KVM: PPC: Book3S HV: No need to clear radix host SLB before loading HPT guest
      KVM: PPC: Book3S HV: Fix radix guest SLB side channel
      KVM: PPC: Book3S HV: Remove support for running HPT guest on RPT host without mixed mode support
      KVM: PPC: Book3S HV: Introduce new capability for 2nd DAWR
      KVM: PPC: Book3S HV: Add infrastructure to support 2nd DAWR
      ...

commit d8d0da4eee5c4e86ea08abde6975848376b4ac13
Author: Waiman Long <longman@redhat.com>
Date:   Wed Feb 10 13:16:31 2021 -0500

    locking/arch: Move qrwlock.h include after qspinlock.h
    
    include/asm-generic/qrwlock.h was trying to get arch_spin_is_locked via
    asm-generic/qspinlock.h.  However, this does not work because architectures
    might be using queued rwlocks but not queued spinlocks (csky), or because they
    might be defining their own queued_* macros before including asm/qspinlock.h.
    
    To fix this, ensure that asm/spinlock.h always includes qrwlock.h after
    defining arch_spin_is_locked (either directly for csky, or via
    asm/qspinlock.h for other architectures).  The only inclusion elsewhere
    is in kernel/locking/qrwlock.c.  That one is really unnecessary because
    the file is only compiled in SMP configurations (config QUEUED_RWLOCKS
    depends on SMP) and in that case linux/spinlock.h already includes
    asm/qrwlock.h if needed, via asm/spinlock.h.
    
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Fixes: 26128cb6c7e6 ("locking/rwlocks: Add contention detection for rwlocks")
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Reviewed-by: Ben Gardon <bgardon@google.com>
    [Add arch/sparc and kernel/locking parts per discussion with Waiman. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit e2dc4957349a7a15f87ac2ea6367b129192769e1
Merge: f986e3508333 8d0dd23c6c78
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 15 23:41:19 2020 -0800

    Merge tag 'asm-generic-cleanup-5.11' of git://git.kernel.org/pub/scm/linux/kernel/git/arnd/asm-generic
    
    Pull asm-generic cleanups from Arnd Bergmann:
     "These are a couple of compiler warning fixes to make 'make W=2' less
      noisy, as well as some fixes to code comments in asm-generic"
    
    * tag 'asm-generic-cleanup-5.11' of git://git.kernel.org/pub/scm/linux/kernel/git/arnd/asm-generic:
      syscalls: Fix file comments for syscalls implemented in kernel/sys.c
      ctype.h: remove duplicate isdigit() helper
      qspinlock: use signed temporaries for cmpxchg
      asm-generic: fix ffs -Wshadow warning
      asm-generic: percpu: avoid Wshadow warning
      asm-generic/sembuf: Update architecture related information in comment

commit f44ca0871b7a98b075560711d48849914a102221
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Oct 19 09:09:21 2020 +0200

    qspinlock: use signed temporaries for cmpxchg
    
    When building with W=2, the build log is flooded with
    
    include/asm-generic/qrwlock.h:65:56: warning: pointer targets in passing argument 2 of 'atomic_try_cmpxchg_acquire' differ in signedness [-Wpointer-sign]
    include/asm-generic/qrwlock.h:92:53: warning: pointer targets in passing argument 2 of 'atomic_try_cmpxchg_acquire' differ in signedness [-Wpointer-sign]
    include/asm-generic/qspinlock.h:68:55: warning: pointer targets in passing argument 2 of 'atomic_try_cmpxchg_acquire' differ in signedness [-Wpointer-sign]
    include/asm-generic/qspinlock.h:82:52: warning: pointer targets in passing argument 2 of 'atomic_try_cmpxchg_acquire' differ in signedness [-Wpointer-sign]
    
    The atomics are built on top of signed integers, but the caller
    doesn't actually care. Just use signed types as well.
    
    Fixes: 27df89689e25 ("locking/spinlocks: Remove an instruction from spin and write locks")
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 9ba19ccd2d283a79dd29e8130819c59beca80f62
Merge: 8f0cb6660acb 992414a18cd4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 3 14:39:35 2020 -0700

    Merge tag 'locking-core-2020-08-03' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
    
     - LKMM updates: mostly documentation changes, but also some new litmus
       tests for atomic ops.
    
     - KCSAN updates: the most important change is that GCC 11 now has all
       fixes in place to support KCSAN, so GCC support can be enabled again.
       Also more annotations.
    
     - futex updates: minor cleanups and simplifications
    
     - seqlock updates: merge preparatory changes/cleanups for the
       'associated locks' facilities.
    
     - lockdep updates:
        - simplify IRQ trace event handling
        - add various new debug checks
        - simplify header dependencies, split out <linux/lockdep_types.h>,
          decouple lockdep from other low level headers some more
        - fix NMI handling
    
     - misc cleanups and smaller fixes
    
    * tag 'locking-core-2020-08-03' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (60 commits)
      kcsan: Improve IRQ state trace reporting
      lockdep: Refactor IRQ trace events fields into struct
      seqlock: lockdep assert non-preemptibility on seqcount_t write
      lockdep: Add preemption enabled/disabled assertion APIs
      seqlock: Implement raw_seqcount_begin() in terms of raw_read_seqcount()
      seqlock: Add kernel-doc for seqcount_t and seqlock_t APIs
      seqlock: Reorder seqcount_t and seqlock_t API definitions
      seqlock: seqcount_t latch: End read sections with read_seqcount_retry()
      seqlock: Properly format kernel-doc code samples
      Documentation: locking: Describe seqlock design and usage
      locking/qspinlock: Do not include atomic.h from qspinlock_types.h
      locking/atomic: Move ATOMIC_INIT into linux/types.h
      lockdep: Move list.h inclusion into lockdep.h
      locking/lockdep: Fix TRACE_IRQFLAGS vs. NMIs
      futex: Remove unused or redundant includes
      futex: Consistently use fshared as boolean
      futex: Remove needless goto's
      futex: Remove put_futex_key()
      rwsem: fix commas in initialisation
      docs: locking: Replace HTTP links with HTTPS ones
      ...

commit 459e39538e612b8dd130d34b93c9bfc89ecc836c
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Wed Jul 29 22:33:16 2020 +1000

    locking/qspinlock: Do not include atomic.h from qspinlock_types.h
    
    This patch breaks a header loop involving qspinlock_types.h.
    The issue is that qspinlock_types.h includes atomic.h, which then
    eventually includes kernel.h which could lead back to the original
    file via spinlock_types.h.
    
    As ATOMIC_INIT is now defined by linux/types.h, there is no longer
    any need to include atomic.h from qspinlock_types.h.  This also
    allows the CONFIG_PARAVIRT hack to be removed since it was trying
    to prevent exactly this loop.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Link: https://lkml.kernel.org/r/20200729123316.GC7047@gondor.apana.org.au

commit 2f6560e652dfdbdb59df28b45a3458bf36d3c580
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Jul 24 23:14:22 2020 +1000

    powerpc/qspinlock: Optimised atomic_try_cmpxchg_lock() that adds the lock hint
    
    This brings the behaviour of the uncontended fast path back to roughly
    equivalent to simple spinlocks -- a single atomic op with lock hint.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Acked-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200724131423.1362108-6-npiggin@gmail.com

commit 20c0e8269e9d515e677670902c7e1cc0209d6ad9
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Jul 24 23:14:21 2020 +1000

    powerpc/pseries: Implement paravirt qspinlocks for SPLPAR
    
    This implements the generic paravirt qspinlocks using H_PROD and
    H_CONFER to kick and wait.
    
    This uses an un-directed yield to any CPU rather than the directed
    yield to a pre-empted lock holder that paravirtualised simple
    spinlocks use, that requires no kick hcall. This is something that
    could be investigated and improved in future.
    
    Performance results can be found in the commit which added queued
    spinlocks.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200724131423.1362108-5-npiggin@gmail.com

commit 9a3c05e658d4d31b38ef03fe5c17bc2039402ff7
Author: Zhenzhong Duan <zhenzhong.duan@oracle.com>
Date:   Wed Oct 23 19:16:23 2019 +0800

    xen: Mark "xen_nopvspin" parameter obsolete
    
    Map "xen_nopvspin" to "nopvspin", fix stale description of "xen_nopvspin"
    as we use qspinlock now.
    
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Stefano Stabellini <sstabellini@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit 6f7818d90cc6b26d22d4375292ae74e767ee67f6
Author: Andrew Melnychenko <andrew@daynix.com>
Date:   Tue Apr 14 22:15:03 2020 +0300

    tty: hvc: fix buffer overflow during hvc_alloc().
    
    commit 9a9fc42b86c06120744555fea43fdcabe297c656 upstream.
    
    If there is a lot(more then 16) of virtio-console devices
    or virtio_console module is reloaded
    - buffers 'vtermnos' and 'cons_ops' are overflowed.
    In older kernels it overruns spinlock which leads to kernel freezing:
    https://bugzilla.redhat.com/show_bug.cgi?id=1786239
    
    To reproduce the issue, you can try simple script that
    loads/unloads module. Something like this:
    while [ 1 ]
    do
      modprobe virtio_console
      sleep 2
      modprobe -r virtio_console
      sleep 2
    done
    
    Description of problem:
    Guest get 'Call Trace' when loading module "virtio_console"
    and unloading it frequently - clearly reproduced on kernel-4.18.0:
    
    [   81.498208] ------------[ cut here ]------------
    [   81.499263] pvqspinlock: lock 0xffffffff92080020 has corrupted value 0xc0774ca0!
    [   81.501000] WARNING: CPU: 0 PID: 785 at kernel/locking/qspinlock_paravirt.h:500 __pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.503173] Modules linked in: virtio_console fuse xt_CHECKSUM ipt_MASQUERADE xt_conntrack ipt_REJECT nft_counter nf_nat_tftp nft_objref nf_conntrack_tftp tun bridge stp llc nft_fib_inet nft_fib_ipv4 nft_fib_ipv6 nft_fib nft_reject_inet nf_reject_ipv4 nf_reject_ipv6 nft_reject nft_ct nf_tables_set nft_chain_nat_ipv6 nf_conntrack_ipv6 nf_defrag_ipv6 nf_nat_ipv6 nft_chain_route_ipv6 nft_chain_nat_ipv4 nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack nft_chain_route_ipv4 ip6_tables nft_compat ip_set nf_tables nfnetlink sunrpc bochs_drm drm_vram_helper ttm drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops drm i2c_piix4 pcspkr crct10dif_pclmul crc32_pclmul joydev ghash_clmulni_intel ip_tables xfs libcrc32c sd_mod sg ata_generic ata_piix virtio_net libata crc32c_intel net_failover failover serio_raw virtio_scsi dm_mirror dm_region_hash dm_log dm_mod [last unloaded: virtio_console]
    [   81.517019] CPU: 0 PID: 785 Comm: kworker/0:2 Kdump: loaded Not tainted 4.18.0-167.el8.x86_64 #1
    [   81.518639] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   81.520205] Workqueue: events control_work_handler [virtio_console]
    [   81.521354] RIP: 0010:__pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.522450] Code: 07 00 48 63 7a 10 e8 bf 64 f5 ff 66 90 c3 8b 05 e6 cf d6 01 85 c0 74 01 c3 8b 17 48 89 fe 48 c7 c7 38 4b 29 91 e8 3a 6c fa ff <0f> 0b c3 0f 0b 90 90 90 90 90 90 90 90 90 90 90 0f 1f 44 00 00 48
    [   81.525830] RSP: 0018:ffffb51a01ffbd70 EFLAGS: 00010282
    [   81.526798] RAX: 0000000000000000 RBX: 0000000000000010 RCX: 0000000000000000
    [   81.528110] RDX: ffff9e66f1826480 RSI: ffff9e66f1816a08 RDI: ffff9e66f1816a08
    [   81.529437] RBP: ffffffff9153ff10 R08: 000000000000026c R09: 0000000000000053
    [   81.530732] R10: 0000000000000000 R11: ffffb51a01ffbc18 R12: ffff9e66cd682200
    [   81.532133] R13: ffffffff9153ff10 R14: ffff9e6685569500 R15: ffff9e66cd682000
    [   81.533442] FS:  0000000000000000(0000) GS:ffff9e66f1800000(0000) knlGS:0000000000000000
    [   81.534914] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   81.535971] CR2: 00005624c55b14d0 CR3: 00000003a023c000 CR4: 00000000003406f0
    [   81.537283] Call Trace:
    [   81.537763]  __raw_callee_save___pv_queued_spin_unlock_slowpath+0x11/0x20
    [   81.539011]  .slowpath+0x9/0xe
    [   81.539585]  hvc_alloc+0x25e/0x300
    [   81.540237]  init_port_console+0x28/0x100 [virtio_console]
    [   81.541251]  handle_control_message.constprop.27+0x1c4/0x310 [virtio_console]
    [   81.542546]  control_work_handler+0x70/0x10c [virtio_console]
    [   81.543601]  process_one_work+0x1a7/0x3b0
    [   81.544356]  worker_thread+0x30/0x390
    [   81.545025]  ? create_worker+0x1a0/0x1a0
    [   81.545749]  kthread+0x112/0x130
    [   81.546358]  ? kthread_flush_work_fn+0x10/0x10
    [   81.547183]  ret_from_fork+0x22/0x40
    [   81.547842] ---[ end trace aa97649bd16c8655 ]---
    [   83.546539] general protection fault: 0000 [#1] SMP NOPTI
    [   83.547422] CPU: 5 PID: 3225 Comm: modprobe Kdump: loaded Tainted: G        W        --------- -  - 4.18.0-167.el8.x86_64 #1
    [   83.549191] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   83.550544] RIP: 0010:__pv_queued_spin_lock_slowpath+0x19a/0x2a0
    [   83.551504] Code: c4 c1 ea 12 41 be 01 00 00 00 4c 8d 6d 14 41 83 e4 03 8d 42 ff 49 c1 e4 05 48 98 49 81 c4 40 a5 02 00 4c 03 24 c5 60 48 34 91 <49> 89 2c 24 b8 00 80 00 00 eb 15 84 c0 75 0a 41 0f b6 54 24 14 84
    [   83.554449] RSP: 0018:ffffb51a0323fdb0 EFLAGS: 00010202
    [   83.555290] RAX: 000000000000301c RBX: ffffffff92080020 RCX: 0000000000000001
    [   83.556426] RDX: 000000000000301d RSI: 0000000000000000 RDI: 0000000000000000
    [   83.557556] RBP: ffff9e66f196a540 R08: 000000000000028a R09: ffff9e66d2757788
    [   83.558688] R10: 0000000000000000 R11: 0000000000000000 R12: 646e61725f770b07
    [   83.559821] R13: ffff9e66f196a554 R14: 0000000000000001 R15: 0000000000180000
    [   83.560958] FS:  00007fd5032e8740(0000) GS:ffff9e66f1940000(0000) knlGS:0000000000000000
    [   83.562233] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   83.563149] CR2: 00007fd5022b0da0 CR3: 000000038c334000 CR4: 00000000003406e0
    
    Signed-off-by: Andrew Melnychenko <andrew@daynix.com>
    Cc: stable <stable@vger.kernel.org>
    Link: https://lore.kernel.org/r/20200414191503.3471783-1-andrew@daynix.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8ecdbc141a2c625223c65ca1bd6ff3cd622ef6a0
Author: Andrew Melnychenko <andrew@daynix.com>
Date:   Tue Apr 14 22:15:03 2020 +0300

    tty: hvc: fix buffer overflow during hvc_alloc().
    
    commit 9a9fc42b86c06120744555fea43fdcabe297c656 upstream.
    
    If there is a lot(more then 16) of virtio-console devices
    or virtio_console module is reloaded
    - buffers 'vtermnos' and 'cons_ops' are overflowed.
    In older kernels it overruns spinlock which leads to kernel freezing:
    https://bugzilla.redhat.com/show_bug.cgi?id=1786239
    
    To reproduce the issue, you can try simple script that
    loads/unloads module. Something like this:
    while [ 1 ]
    do
      modprobe virtio_console
      sleep 2
      modprobe -r virtio_console
      sleep 2
    done
    
    Description of problem:
    Guest get 'Call Trace' when loading module "virtio_console"
    and unloading it frequently - clearly reproduced on kernel-4.18.0:
    
    [   81.498208] ------------[ cut here ]------------
    [   81.499263] pvqspinlock: lock 0xffffffff92080020 has corrupted value 0xc0774ca0!
    [   81.501000] WARNING: CPU: 0 PID: 785 at kernel/locking/qspinlock_paravirt.h:500 __pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.503173] Modules linked in: virtio_console fuse xt_CHECKSUM ipt_MASQUERADE xt_conntrack ipt_REJECT nft_counter nf_nat_tftp nft_objref nf_conntrack_tftp tun bridge stp llc nft_fib_inet nft_fib_ipv4 nft_fib_ipv6 nft_fib nft_reject_inet nf_reject_ipv4 nf_reject_ipv6 nft_reject nft_ct nf_tables_set nft_chain_nat_ipv6 nf_conntrack_ipv6 nf_defrag_ipv6 nf_nat_ipv6 nft_chain_route_ipv6 nft_chain_nat_ipv4 nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack nft_chain_route_ipv4 ip6_tables nft_compat ip_set nf_tables nfnetlink sunrpc bochs_drm drm_vram_helper ttm drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops drm i2c_piix4 pcspkr crct10dif_pclmul crc32_pclmul joydev ghash_clmulni_intel ip_tables xfs libcrc32c sd_mod sg ata_generic ata_piix virtio_net libata crc32c_intel net_failover failover serio_raw virtio_scsi dm_mirror dm_region_hash dm_log dm_mod [last unloaded: virtio_console]
    [   81.517019] CPU: 0 PID: 785 Comm: kworker/0:2 Kdump: loaded Not tainted 4.18.0-167.el8.x86_64 #1
    [   81.518639] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   81.520205] Workqueue: events control_work_handler [virtio_console]
    [   81.521354] RIP: 0010:__pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.522450] Code: 07 00 48 63 7a 10 e8 bf 64 f5 ff 66 90 c3 8b 05 e6 cf d6 01 85 c0 74 01 c3 8b 17 48 89 fe 48 c7 c7 38 4b 29 91 e8 3a 6c fa ff <0f> 0b c3 0f 0b 90 90 90 90 90 90 90 90 90 90 90 0f 1f 44 00 00 48
    [   81.525830] RSP: 0018:ffffb51a01ffbd70 EFLAGS: 00010282
    [   81.526798] RAX: 0000000000000000 RBX: 0000000000000010 RCX: 0000000000000000
    [   81.528110] RDX: ffff9e66f1826480 RSI: ffff9e66f1816a08 RDI: ffff9e66f1816a08
    [   81.529437] RBP: ffffffff9153ff10 R08: 000000000000026c R09: 0000000000000053
    [   81.530732] R10: 0000000000000000 R11: ffffb51a01ffbc18 R12: ffff9e66cd682200
    [   81.532133] R13: ffffffff9153ff10 R14: ffff9e6685569500 R15: ffff9e66cd682000
    [   81.533442] FS:  0000000000000000(0000) GS:ffff9e66f1800000(0000) knlGS:0000000000000000
    [   81.534914] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   81.535971] CR2: 00005624c55b14d0 CR3: 00000003a023c000 CR4: 00000000003406f0
    [   81.537283] Call Trace:
    [   81.537763]  __raw_callee_save___pv_queued_spin_unlock_slowpath+0x11/0x20
    [   81.539011]  .slowpath+0x9/0xe
    [   81.539585]  hvc_alloc+0x25e/0x300
    [   81.540237]  init_port_console+0x28/0x100 [virtio_console]
    [   81.541251]  handle_control_message.constprop.27+0x1c4/0x310 [virtio_console]
    [   81.542546]  control_work_handler+0x70/0x10c [virtio_console]
    [   81.543601]  process_one_work+0x1a7/0x3b0
    [   81.544356]  worker_thread+0x30/0x390
    [   81.545025]  ? create_worker+0x1a0/0x1a0
    [   81.545749]  kthread+0x112/0x130
    [   81.546358]  ? kthread_flush_work_fn+0x10/0x10
    [   81.547183]  ret_from_fork+0x22/0x40
    [   81.547842] ---[ end trace aa97649bd16c8655 ]---
    [   83.546539] general protection fault: 0000 [#1] SMP NOPTI
    [   83.547422] CPU: 5 PID: 3225 Comm: modprobe Kdump: loaded Tainted: G        W        --------- -  - 4.18.0-167.el8.x86_64 #1
    [   83.549191] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   83.550544] RIP: 0010:__pv_queued_spin_lock_slowpath+0x19a/0x2a0
    [   83.551504] Code: c4 c1 ea 12 41 be 01 00 00 00 4c 8d 6d 14 41 83 e4 03 8d 42 ff 49 c1 e4 05 48 98 49 81 c4 40 a5 02 00 4c 03 24 c5 60 48 34 91 <49> 89 2c 24 b8 00 80 00 00 eb 15 84 c0 75 0a 41 0f b6 54 24 14 84
    [   83.554449] RSP: 0018:ffffb51a0323fdb0 EFLAGS: 00010202
    [   83.555290] RAX: 000000000000301c RBX: ffffffff92080020 RCX: 0000000000000001
    [   83.556426] RDX: 000000000000301d RSI: 0000000000000000 RDI: 0000000000000000
    [   83.557556] RBP: ffff9e66f196a540 R08: 000000000000028a R09: ffff9e66d2757788
    [   83.558688] R10: 0000000000000000 R11: 0000000000000000 R12: 646e61725f770b07
    [   83.559821] R13: ffff9e66f196a554 R14: 0000000000000001 R15: 0000000000180000
    [   83.560958] FS:  00007fd5032e8740(0000) GS:ffff9e66f1940000(0000) knlGS:0000000000000000
    [   83.562233] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   83.563149] CR2: 00007fd5022b0da0 CR3: 000000038c334000 CR4: 00000000003406e0
    
    Signed-off-by: Andrew Melnychenko <andrew@daynix.com>
    Cc: stable <stable@vger.kernel.org>
    Link: https://lore.kernel.org/r/20200414191503.3471783-1-andrew@daynix.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e2b89121b8964d3ab6521eb8acccb0bc82b4755f
Author: Andrew Melnychenko <andrew@daynix.com>
Date:   Tue Apr 14 22:15:03 2020 +0300

    tty: hvc: fix buffer overflow during hvc_alloc().
    
    commit 9a9fc42b86c06120744555fea43fdcabe297c656 upstream.
    
    If there is a lot(more then 16) of virtio-console devices
    or virtio_console module is reloaded
    - buffers 'vtermnos' and 'cons_ops' are overflowed.
    In older kernels it overruns spinlock which leads to kernel freezing:
    https://bugzilla.redhat.com/show_bug.cgi?id=1786239
    
    To reproduce the issue, you can try simple script that
    loads/unloads module. Something like this:
    while [ 1 ]
    do
      modprobe virtio_console
      sleep 2
      modprobe -r virtio_console
      sleep 2
    done
    
    Description of problem:
    Guest get 'Call Trace' when loading module "virtio_console"
    and unloading it frequently - clearly reproduced on kernel-4.18.0:
    
    [   81.498208] ------------[ cut here ]------------
    [   81.499263] pvqspinlock: lock 0xffffffff92080020 has corrupted value 0xc0774ca0!
    [   81.501000] WARNING: CPU: 0 PID: 785 at kernel/locking/qspinlock_paravirt.h:500 __pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.503173] Modules linked in: virtio_console fuse xt_CHECKSUM ipt_MASQUERADE xt_conntrack ipt_REJECT nft_counter nf_nat_tftp nft_objref nf_conntrack_tftp tun bridge stp llc nft_fib_inet nft_fib_ipv4 nft_fib_ipv6 nft_fib nft_reject_inet nf_reject_ipv4 nf_reject_ipv6 nft_reject nft_ct nf_tables_set nft_chain_nat_ipv6 nf_conntrack_ipv6 nf_defrag_ipv6 nf_nat_ipv6 nft_chain_route_ipv6 nft_chain_nat_ipv4 nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack nft_chain_route_ipv4 ip6_tables nft_compat ip_set nf_tables nfnetlink sunrpc bochs_drm drm_vram_helper ttm drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops drm i2c_piix4 pcspkr crct10dif_pclmul crc32_pclmul joydev ghash_clmulni_intel ip_tables xfs libcrc32c sd_mod sg ata_generic ata_piix virtio_net libata crc32c_intel net_failover failover serio_raw virtio_scsi dm_mirror dm_region_hash dm_log dm_mod [last unloaded: virtio_console]
    [   81.517019] CPU: 0 PID: 785 Comm: kworker/0:2 Kdump: loaded Not tainted 4.18.0-167.el8.x86_64 #1
    [   81.518639] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   81.520205] Workqueue: events control_work_handler [virtio_console]
    [   81.521354] RIP: 0010:__pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.522450] Code: 07 00 48 63 7a 10 e8 bf 64 f5 ff 66 90 c3 8b 05 e6 cf d6 01 85 c0 74 01 c3 8b 17 48 89 fe 48 c7 c7 38 4b 29 91 e8 3a 6c fa ff <0f> 0b c3 0f 0b 90 90 90 90 90 90 90 90 90 90 90 0f 1f 44 00 00 48
    [   81.525830] RSP: 0018:ffffb51a01ffbd70 EFLAGS: 00010282
    [   81.526798] RAX: 0000000000000000 RBX: 0000000000000010 RCX: 0000000000000000
    [   81.528110] RDX: ffff9e66f1826480 RSI: ffff9e66f1816a08 RDI: ffff9e66f1816a08
    [   81.529437] RBP: ffffffff9153ff10 R08: 000000000000026c R09: 0000000000000053
    [   81.530732] R10: 0000000000000000 R11: ffffb51a01ffbc18 R12: ffff9e66cd682200
    [   81.532133] R13: ffffffff9153ff10 R14: ffff9e6685569500 R15: ffff9e66cd682000
    [   81.533442] FS:  0000000000000000(0000) GS:ffff9e66f1800000(0000) knlGS:0000000000000000
    [   81.534914] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   81.535971] CR2: 00005624c55b14d0 CR3: 00000003a023c000 CR4: 00000000003406f0
    [   81.537283] Call Trace:
    [   81.537763]  __raw_callee_save___pv_queued_spin_unlock_slowpath+0x11/0x20
    [   81.539011]  .slowpath+0x9/0xe
    [   81.539585]  hvc_alloc+0x25e/0x300
    [   81.540237]  init_port_console+0x28/0x100 [virtio_console]
    [   81.541251]  handle_control_message.constprop.27+0x1c4/0x310 [virtio_console]
    [   81.542546]  control_work_handler+0x70/0x10c [virtio_console]
    [   81.543601]  process_one_work+0x1a7/0x3b0
    [   81.544356]  worker_thread+0x30/0x390
    [   81.545025]  ? create_worker+0x1a0/0x1a0
    [   81.545749]  kthread+0x112/0x130
    [   81.546358]  ? kthread_flush_work_fn+0x10/0x10
    [   81.547183]  ret_from_fork+0x22/0x40
    [   81.547842] ---[ end trace aa97649bd16c8655 ]---
    [   83.546539] general protection fault: 0000 [#1] SMP NOPTI
    [   83.547422] CPU: 5 PID: 3225 Comm: modprobe Kdump: loaded Tainted: G        W        --------- -  - 4.18.0-167.el8.x86_64 #1
    [   83.549191] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   83.550544] RIP: 0010:__pv_queued_spin_lock_slowpath+0x19a/0x2a0
    [   83.551504] Code: c4 c1 ea 12 41 be 01 00 00 00 4c 8d 6d 14 41 83 e4 03 8d 42 ff 49 c1 e4 05 48 98 49 81 c4 40 a5 02 00 4c 03 24 c5 60 48 34 91 <49> 89 2c 24 b8 00 80 00 00 eb 15 84 c0 75 0a 41 0f b6 54 24 14 84
    [   83.554449] RSP: 0018:ffffb51a0323fdb0 EFLAGS: 00010202
    [   83.555290] RAX: 000000000000301c RBX: ffffffff92080020 RCX: 0000000000000001
    [   83.556426] RDX: 000000000000301d RSI: 0000000000000000 RDI: 0000000000000000
    [   83.557556] RBP: ffff9e66f196a540 R08: 000000000000028a R09: ffff9e66d2757788
    [   83.558688] R10: 0000000000000000 R11: 0000000000000000 R12: 646e61725f770b07
    [   83.559821] R13: ffff9e66f196a554 R14: 0000000000000001 R15: 0000000000180000
    [   83.560958] FS:  00007fd5032e8740(0000) GS:ffff9e66f1940000(0000) knlGS:0000000000000000
    [   83.562233] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   83.563149] CR2: 00007fd5022b0da0 CR3: 000000038c334000 CR4: 00000000003406e0
    
    Signed-off-by: Andrew Melnychenko <andrew@daynix.com>
    Cc: stable <stable@vger.kernel.org>
    Link: https://lore.kernel.org/r/20200414191503.3471783-1-andrew@daynix.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8e8fb10143d2881b6eae27f551e304f8469560e5
Author: Andrew Melnychenko <andrew@daynix.com>
Date:   Tue Apr 14 22:15:03 2020 +0300

    tty: hvc: fix buffer overflow during hvc_alloc().
    
    commit 9a9fc42b86c06120744555fea43fdcabe297c656 upstream.
    
    If there is a lot(more then 16) of virtio-console devices
    or virtio_console module is reloaded
    - buffers 'vtermnos' and 'cons_ops' are overflowed.
    In older kernels it overruns spinlock which leads to kernel freezing:
    https://bugzilla.redhat.com/show_bug.cgi?id=1786239
    
    To reproduce the issue, you can try simple script that
    loads/unloads module. Something like this:
    while [ 1 ]
    do
      modprobe virtio_console
      sleep 2
      modprobe -r virtio_console
      sleep 2
    done
    
    Description of problem:
    Guest get 'Call Trace' when loading module "virtio_console"
    and unloading it frequently - clearly reproduced on kernel-4.18.0:
    
    [   81.498208] ------------[ cut here ]------------
    [   81.499263] pvqspinlock: lock 0xffffffff92080020 has corrupted value 0xc0774ca0!
    [   81.501000] WARNING: CPU: 0 PID: 785 at kernel/locking/qspinlock_paravirt.h:500 __pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.503173] Modules linked in: virtio_console fuse xt_CHECKSUM ipt_MASQUERADE xt_conntrack ipt_REJECT nft_counter nf_nat_tftp nft_objref nf_conntrack_tftp tun bridge stp llc nft_fib_inet nft_fib_ipv4 nft_fib_ipv6 nft_fib nft_reject_inet nf_reject_ipv4 nf_reject_ipv6 nft_reject nft_ct nf_tables_set nft_chain_nat_ipv6 nf_conntrack_ipv6 nf_defrag_ipv6 nf_nat_ipv6 nft_chain_route_ipv6 nft_chain_nat_ipv4 nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack nft_chain_route_ipv4 ip6_tables nft_compat ip_set nf_tables nfnetlink sunrpc bochs_drm drm_vram_helper ttm drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops drm i2c_piix4 pcspkr crct10dif_pclmul crc32_pclmul joydev ghash_clmulni_intel ip_tables xfs libcrc32c sd_mod sg ata_generic ata_piix virtio_net libata crc32c_intel net_failover failover serio_raw virtio_scsi dm_mirror dm_region_hash dm_log dm_mod [last unloaded: virtio_console]
    [   81.517019] CPU: 0 PID: 785 Comm: kworker/0:2 Kdump: loaded Not tainted 4.18.0-167.el8.x86_64 #1
    [   81.518639] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   81.520205] Workqueue: events control_work_handler [virtio_console]
    [   81.521354] RIP: 0010:__pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.522450] Code: 07 00 48 63 7a 10 e8 bf 64 f5 ff 66 90 c3 8b 05 e6 cf d6 01 85 c0 74 01 c3 8b 17 48 89 fe 48 c7 c7 38 4b 29 91 e8 3a 6c fa ff <0f> 0b c3 0f 0b 90 90 90 90 90 90 90 90 90 90 90 0f 1f 44 00 00 48
    [   81.525830] RSP: 0018:ffffb51a01ffbd70 EFLAGS: 00010282
    [   81.526798] RAX: 0000000000000000 RBX: 0000000000000010 RCX: 0000000000000000
    [   81.528110] RDX: ffff9e66f1826480 RSI: ffff9e66f1816a08 RDI: ffff9e66f1816a08
    [   81.529437] RBP: ffffffff9153ff10 R08: 000000000000026c R09: 0000000000000053
    [   81.530732] R10: 0000000000000000 R11: ffffb51a01ffbc18 R12: ffff9e66cd682200
    [   81.532133] R13: ffffffff9153ff10 R14: ffff9e6685569500 R15: ffff9e66cd682000
    [   81.533442] FS:  0000000000000000(0000) GS:ffff9e66f1800000(0000) knlGS:0000000000000000
    [   81.534914] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   81.535971] CR2: 00005624c55b14d0 CR3: 00000003a023c000 CR4: 00000000003406f0
    [   81.537283] Call Trace:
    [   81.537763]  __raw_callee_save___pv_queued_spin_unlock_slowpath+0x11/0x20
    [   81.539011]  .slowpath+0x9/0xe
    [   81.539585]  hvc_alloc+0x25e/0x300
    [   81.540237]  init_port_console+0x28/0x100 [virtio_console]
    [   81.541251]  handle_control_message.constprop.27+0x1c4/0x310 [virtio_console]
    [   81.542546]  control_work_handler+0x70/0x10c [virtio_console]
    [   81.543601]  process_one_work+0x1a7/0x3b0
    [   81.544356]  worker_thread+0x30/0x390
    [   81.545025]  ? create_worker+0x1a0/0x1a0
    [   81.545749]  kthread+0x112/0x130
    [   81.546358]  ? kthread_flush_work_fn+0x10/0x10
    [   81.547183]  ret_from_fork+0x22/0x40
    [   81.547842] ---[ end trace aa97649bd16c8655 ]---
    [   83.546539] general protection fault: 0000 [#1] SMP NOPTI
    [   83.547422] CPU: 5 PID: 3225 Comm: modprobe Kdump: loaded Tainted: G        W        --------- -  - 4.18.0-167.el8.x86_64 #1
    [   83.549191] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   83.550544] RIP: 0010:__pv_queued_spin_lock_slowpath+0x19a/0x2a0
    [   83.551504] Code: c4 c1 ea 12 41 be 01 00 00 00 4c 8d 6d 14 41 83 e4 03 8d 42 ff 49 c1 e4 05 48 98 49 81 c4 40 a5 02 00 4c 03 24 c5 60 48 34 91 <49> 89 2c 24 b8 00 80 00 00 eb 15 84 c0 75 0a 41 0f b6 54 24 14 84
    [   83.554449] RSP: 0018:ffffb51a0323fdb0 EFLAGS: 00010202
    [   83.555290] RAX: 000000000000301c RBX: ffffffff92080020 RCX: 0000000000000001
    [   83.556426] RDX: 000000000000301d RSI: 0000000000000000 RDI: 0000000000000000
    [   83.557556] RBP: ffff9e66f196a540 R08: 000000000000028a R09: ffff9e66d2757788
    [   83.558688] R10: 0000000000000000 R11: 0000000000000000 R12: 646e61725f770b07
    [   83.559821] R13: ffff9e66f196a554 R14: 0000000000000001 R15: 0000000000180000
    [   83.560958] FS:  00007fd5032e8740(0000) GS:ffff9e66f1940000(0000) knlGS:0000000000000000
    [   83.562233] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   83.563149] CR2: 00007fd5022b0da0 CR3: 000000038c334000 CR4: 00000000003406e0
    
    Signed-off-by: Andrew Melnychenko <andrew@daynix.com>
    Cc: stable <stable@vger.kernel.org>
    Link: https://lore.kernel.org/r/20200414191503.3471783-1-andrew@daynix.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f1c0d3243dbe18b2225723c3068eef992098186b
Author: Andrew Melnychenko <andrew@daynix.com>
Date:   Tue Apr 14 22:15:03 2020 +0300

    tty: hvc: fix buffer overflow during hvc_alloc().
    
    commit 9a9fc42b86c06120744555fea43fdcabe297c656 upstream.
    
    If there is a lot(more then 16) of virtio-console devices
    or virtio_console module is reloaded
    - buffers 'vtermnos' and 'cons_ops' are overflowed.
    In older kernels it overruns spinlock which leads to kernel freezing:
    https://bugzilla.redhat.com/show_bug.cgi?id=1786239
    
    To reproduce the issue, you can try simple script that
    loads/unloads module. Something like this:
    while [ 1 ]
    do
      modprobe virtio_console
      sleep 2
      modprobe -r virtio_console
      sleep 2
    done
    
    Description of problem:
    Guest get 'Call Trace' when loading module "virtio_console"
    and unloading it frequently - clearly reproduced on kernel-4.18.0:
    
    [   81.498208] ------------[ cut here ]------------
    [   81.499263] pvqspinlock: lock 0xffffffff92080020 has corrupted value 0xc0774ca0!
    [   81.501000] WARNING: CPU: 0 PID: 785 at kernel/locking/qspinlock_paravirt.h:500 __pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.503173] Modules linked in: virtio_console fuse xt_CHECKSUM ipt_MASQUERADE xt_conntrack ipt_REJECT nft_counter nf_nat_tftp nft_objref nf_conntrack_tftp tun bridge stp llc nft_fib_inet nft_fib_ipv4 nft_fib_ipv6 nft_fib nft_reject_inet nf_reject_ipv4 nf_reject_ipv6 nft_reject nft_ct nf_tables_set nft_chain_nat_ipv6 nf_conntrack_ipv6 nf_defrag_ipv6 nf_nat_ipv6 nft_chain_route_ipv6 nft_chain_nat_ipv4 nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack nft_chain_route_ipv4 ip6_tables nft_compat ip_set nf_tables nfnetlink sunrpc bochs_drm drm_vram_helper ttm drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops drm i2c_piix4 pcspkr crct10dif_pclmul crc32_pclmul joydev ghash_clmulni_intel ip_tables xfs libcrc32c sd_mod sg ata_generic ata_piix virtio_net libata crc32c_intel net_failover failover serio_raw virtio_scsi dm_mirror dm_region_hash dm_log dm_mod [last unloaded: virtio_console]
    [   81.517019] CPU: 0 PID: 785 Comm: kworker/0:2 Kdump: loaded Not tainted 4.18.0-167.el8.x86_64 #1
    [   81.518639] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   81.520205] Workqueue: events control_work_handler [virtio_console]
    [   81.521354] RIP: 0010:__pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.522450] Code: 07 00 48 63 7a 10 e8 bf 64 f5 ff 66 90 c3 8b 05 e6 cf d6 01 85 c0 74 01 c3 8b 17 48 89 fe 48 c7 c7 38 4b 29 91 e8 3a 6c fa ff <0f> 0b c3 0f 0b 90 90 90 90 90 90 90 90 90 90 90 0f 1f 44 00 00 48
    [   81.525830] RSP: 0018:ffffb51a01ffbd70 EFLAGS: 00010282
    [   81.526798] RAX: 0000000000000000 RBX: 0000000000000010 RCX: 0000000000000000
    [   81.528110] RDX: ffff9e66f1826480 RSI: ffff9e66f1816a08 RDI: ffff9e66f1816a08
    [   81.529437] RBP: ffffffff9153ff10 R08: 000000000000026c R09: 0000000000000053
    [   81.530732] R10: 0000000000000000 R11: ffffb51a01ffbc18 R12: ffff9e66cd682200
    [   81.532133] R13: ffffffff9153ff10 R14: ffff9e6685569500 R15: ffff9e66cd682000
    [   81.533442] FS:  0000000000000000(0000) GS:ffff9e66f1800000(0000) knlGS:0000000000000000
    [   81.534914] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   81.535971] CR2: 00005624c55b14d0 CR3: 00000003a023c000 CR4: 00000000003406f0
    [   81.537283] Call Trace:
    [   81.537763]  __raw_callee_save___pv_queued_spin_unlock_slowpath+0x11/0x20
    [   81.539011]  .slowpath+0x9/0xe
    [   81.539585]  hvc_alloc+0x25e/0x300
    [   81.540237]  init_port_console+0x28/0x100 [virtio_console]
    [   81.541251]  handle_control_message.constprop.27+0x1c4/0x310 [virtio_console]
    [   81.542546]  control_work_handler+0x70/0x10c [virtio_console]
    [   81.543601]  process_one_work+0x1a7/0x3b0
    [   81.544356]  worker_thread+0x30/0x390
    [   81.545025]  ? create_worker+0x1a0/0x1a0
    [   81.545749]  kthread+0x112/0x130
    [   81.546358]  ? kthread_flush_work_fn+0x10/0x10
    [   81.547183]  ret_from_fork+0x22/0x40
    [   81.547842] ---[ end trace aa97649bd16c8655 ]---
    [   83.546539] general protection fault: 0000 [#1] SMP NOPTI
    [   83.547422] CPU: 5 PID: 3225 Comm: modprobe Kdump: loaded Tainted: G        W        --------- -  - 4.18.0-167.el8.x86_64 #1
    [   83.549191] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   83.550544] RIP: 0010:__pv_queued_spin_lock_slowpath+0x19a/0x2a0
    [   83.551504] Code: c4 c1 ea 12 41 be 01 00 00 00 4c 8d 6d 14 41 83 e4 03 8d 42 ff 49 c1 e4 05 48 98 49 81 c4 40 a5 02 00 4c 03 24 c5 60 48 34 91 <49> 89 2c 24 b8 00 80 00 00 eb 15 84 c0 75 0a 41 0f b6 54 24 14 84
    [   83.554449] RSP: 0018:ffffb51a0323fdb0 EFLAGS: 00010202
    [   83.555290] RAX: 000000000000301c RBX: ffffffff92080020 RCX: 0000000000000001
    [   83.556426] RDX: 000000000000301d RSI: 0000000000000000 RDI: 0000000000000000
    [   83.557556] RBP: ffff9e66f196a540 R08: 000000000000028a R09: ffff9e66d2757788
    [   83.558688] R10: 0000000000000000 R11: 0000000000000000 R12: 646e61725f770b07
    [   83.559821] R13: ffff9e66f196a554 R14: 0000000000000001 R15: 0000000000180000
    [   83.560958] FS:  00007fd5032e8740(0000) GS:ffff9e66f1940000(0000) knlGS:0000000000000000
    [   83.562233] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   83.563149] CR2: 00007fd5022b0da0 CR3: 000000038c334000 CR4: 00000000003406e0
    
    Signed-off-by: Andrew Melnychenko <andrew@daynix.com>
    Cc: stable <stable@vger.kernel.org>
    Link: https://lore.kernel.org/r/20200414191503.3471783-1-andrew@daynix.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit cc919ba02b88fe9151baa806dbc469c2f254d5e7
Author: Andrew Melnychenko <andrew@daynix.com>
Date:   Tue Apr 14 22:15:03 2020 +0300

    tty: hvc: fix buffer overflow during hvc_alloc().
    
    commit 9a9fc42b86c06120744555fea43fdcabe297c656 upstream.
    
    If there is a lot(more then 16) of virtio-console devices
    or virtio_console module is reloaded
    - buffers 'vtermnos' and 'cons_ops' are overflowed.
    In older kernels it overruns spinlock which leads to kernel freezing:
    https://bugzilla.redhat.com/show_bug.cgi?id=1786239
    
    To reproduce the issue, you can try simple script that
    loads/unloads module. Something like this:
    while [ 1 ]
    do
      modprobe virtio_console
      sleep 2
      modprobe -r virtio_console
      sleep 2
    done
    
    Description of problem:
    Guest get 'Call Trace' when loading module "virtio_console"
    and unloading it frequently - clearly reproduced on kernel-4.18.0:
    
    [   81.498208] ------------[ cut here ]------------
    [   81.499263] pvqspinlock: lock 0xffffffff92080020 has corrupted value 0xc0774ca0!
    [   81.501000] WARNING: CPU: 0 PID: 785 at kernel/locking/qspinlock_paravirt.h:500 __pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.503173] Modules linked in: virtio_console fuse xt_CHECKSUM ipt_MASQUERADE xt_conntrack ipt_REJECT nft_counter nf_nat_tftp nft_objref nf_conntrack_tftp tun bridge stp llc nft_fib_inet nft_fib_ipv4 nft_fib_ipv6 nft_fib nft_reject_inet nf_reject_ipv4 nf_reject_ipv6 nft_reject nft_ct nf_tables_set nft_chain_nat_ipv6 nf_conntrack_ipv6 nf_defrag_ipv6 nf_nat_ipv6 nft_chain_route_ipv6 nft_chain_nat_ipv4 nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack nft_chain_route_ipv4 ip6_tables nft_compat ip_set nf_tables nfnetlink sunrpc bochs_drm drm_vram_helper ttm drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops drm i2c_piix4 pcspkr crct10dif_pclmul crc32_pclmul joydev ghash_clmulni_intel ip_tables xfs libcrc32c sd_mod sg ata_generic ata_piix virtio_net libata crc32c_intel net_failover failover serio_raw virtio_scsi dm_mirror dm_region_hash dm_log dm_mod [last unloaded: virtio_console]
    [   81.517019] CPU: 0 PID: 785 Comm: kworker/0:2 Kdump: loaded Not tainted 4.18.0-167.el8.x86_64 #1
    [   81.518639] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   81.520205] Workqueue: events control_work_handler [virtio_console]
    [   81.521354] RIP: 0010:__pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.522450] Code: 07 00 48 63 7a 10 e8 bf 64 f5 ff 66 90 c3 8b 05 e6 cf d6 01 85 c0 74 01 c3 8b 17 48 89 fe 48 c7 c7 38 4b 29 91 e8 3a 6c fa ff <0f> 0b c3 0f 0b 90 90 90 90 90 90 90 90 90 90 90 0f 1f 44 00 00 48
    [   81.525830] RSP: 0018:ffffb51a01ffbd70 EFLAGS: 00010282
    [   81.526798] RAX: 0000000000000000 RBX: 0000000000000010 RCX: 0000000000000000
    [   81.528110] RDX: ffff9e66f1826480 RSI: ffff9e66f1816a08 RDI: ffff9e66f1816a08
    [   81.529437] RBP: ffffffff9153ff10 R08: 000000000000026c R09: 0000000000000053
    [   81.530732] R10: 0000000000000000 R11: ffffb51a01ffbc18 R12: ffff9e66cd682200
    [   81.532133] R13: ffffffff9153ff10 R14: ffff9e6685569500 R15: ffff9e66cd682000
    [   81.533442] FS:  0000000000000000(0000) GS:ffff9e66f1800000(0000) knlGS:0000000000000000
    [   81.534914] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   81.535971] CR2: 00005624c55b14d0 CR3: 00000003a023c000 CR4: 00000000003406f0
    [   81.537283] Call Trace:
    [   81.537763]  __raw_callee_save___pv_queued_spin_unlock_slowpath+0x11/0x20
    [   81.539011]  .slowpath+0x9/0xe
    [   81.539585]  hvc_alloc+0x25e/0x300
    [   81.540237]  init_port_console+0x28/0x100 [virtio_console]
    [   81.541251]  handle_control_message.constprop.27+0x1c4/0x310 [virtio_console]
    [   81.542546]  control_work_handler+0x70/0x10c [virtio_console]
    [   81.543601]  process_one_work+0x1a7/0x3b0
    [   81.544356]  worker_thread+0x30/0x390
    [   81.545025]  ? create_worker+0x1a0/0x1a0
    [   81.545749]  kthread+0x112/0x130
    [   81.546358]  ? kthread_flush_work_fn+0x10/0x10
    [   81.547183]  ret_from_fork+0x22/0x40
    [   81.547842] ---[ end trace aa97649bd16c8655 ]---
    [   83.546539] general protection fault: 0000 [#1] SMP NOPTI
    [   83.547422] CPU: 5 PID: 3225 Comm: modprobe Kdump: loaded Tainted: G        W        --------- -  - 4.18.0-167.el8.x86_64 #1
    [   83.549191] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   83.550544] RIP: 0010:__pv_queued_spin_lock_slowpath+0x19a/0x2a0
    [   83.551504] Code: c4 c1 ea 12 41 be 01 00 00 00 4c 8d 6d 14 41 83 e4 03 8d 42 ff 49 c1 e4 05 48 98 49 81 c4 40 a5 02 00 4c 03 24 c5 60 48 34 91 <49> 89 2c 24 b8 00 80 00 00 eb 15 84 c0 75 0a 41 0f b6 54 24 14 84
    [   83.554449] RSP: 0018:ffffb51a0323fdb0 EFLAGS: 00010202
    [   83.555290] RAX: 000000000000301c RBX: ffffffff92080020 RCX: 0000000000000001
    [   83.556426] RDX: 000000000000301d RSI: 0000000000000000 RDI: 0000000000000000
    [   83.557556] RBP: ffff9e66f196a540 R08: 000000000000028a R09: ffff9e66d2757788
    [   83.558688] R10: 0000000000000000 R11: 0000000000000000 R12: 646e61725f770b07
    [   83.559821] R13: ffff9e66f196a554 R14: 0000000000000001 R15: 0000000000180000
    [   83.560958] FS:  00007fd5032e8740(0000) GS:ffff9e66f1940000(0000) knlGS:0000000000000000
    [   83.562233] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   83.563149] CR2: 00007fd5022b0da0 CR3: 000000038c334000 CR4: 00000000003406e0
    
    Signed-off-by: Andrew Melnychenko <andrew@daynix.com>
    Cc: stable <stable@vger.kernel.org>
    Link: https://lore.kernel.org/r/20200414191503.3471783-1-andrew@daynix.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9a9fc42b86c06120744555fea43fdcabe297c656
Author: Andrew Melnychenko <andrew@daynix.com>
Date:   Tue Apr 14 22:15:03 2020 +0300

    tty: hvc: fix buffer overflow during hvc_alloc().
    
    If there is a lot(more then 16) of virtio-console devices
    or virtio_console module is reloaded
    - buffers 'vtermnos' and 'cons_ops' are overflowed.
    In older kernels it overruns spinlock which leads to kernel freezing:
    https://bugzilla.redhat.com/show_bug.cgi?id=1786239
    
    To reproduce the issue, you can try simple script that
    loads/unloads module. Something like this:
    while [ 1 ]
    do
      modprobe virtio_console
      sleep 2
      modprobe -r virtio_console
      sleep 2
    done
    
    Description of problem:
    Guest get 'Call Trace' when loading module "virtio_console"
    and unloading it frequently - clearly reproduced on kernel-4.18.0:
    
    [   81.498208] ------------[ cut here ]------------
    [   81.499263] pvqspinlock: lock 0xffffffff92080020 has corrupted value 0xc0774ca0!
    [   81.501000] WARNING: CPU: 0 PID: 785 at kernel/locking/qspinlock_paravirt.h:500 __pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.503173] Modules linked in: virtio_console fuse xt_CHECKSUM ipt_MASQUERADE xt_conntrack ipt_REJECT nft_counter nf_nat_tftp nft_objref nf_conntrack_tftp tun bridge stp llc nft_fib_inet nft_fib_ipv4 nft_fib_ipv6 nft_fib nft_reject_inet nf_reject_ipv4 nf_reject_ipv6 nft_reject nft_ct nf_tables_set nft_chain_nat_ipv6 nf_conntrack_ipv6 nf_defrag_ipv6 nf_nat_ipv6 nft_chain_route_ipv6 nft_chain_nat_ipv4 nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat_ipv4 nf_nat nf_conntrack nft_chain_route_ipv4 ip6_tables nft_compat ip_set nf_tables nfnetlink sunrpc bochs_drm drm_vram_helper ttm drm_kms_helper syscopyarea sysfillrect sysimgblt fb_sys_fops drm i2c_piix4 pcspkr crct10dif_pclmul crc32_pclmul joydev ghash_clmulni_intel ip_tables xfs libcrc32c sd_mod sg ata_generic ata_piix virtio_net libata crc32c_intel net_failover failover serio_raw virtio_scsi dm_mirror dm_region_hash dm_log dm_mod [last unloaded: virtio_console]
    [   81.517019] CPU: 0 PID: 785 Comm: kworker/0:2 Kdump: loaded Not tainted 4.18.0-167.el8.x86_64 #1
    [   81.518639] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   81.520205] Workqueue: events control_work_handler [virtio_console]
    [   81.521354] RIP: 0010:__pv_queued_spin_unlock_slowpath+0xc0/0xd0
    [   81.522450] Code: 07 00 48 63 7a 10 e8 bf 64 f5 ff 66 90 c3 8b 05 e6 cf d6 01 85 c0 74 01 c3 8b 17 48 89 fe 48 c7 c7 38 4b 29 91 e8 3a 6c fa ff <0f> 0b c3 0f 0b 90 90 90 90 90 90 90 90 90 90 90 0f 1f 44 00 00 48
    [   81.525830] RSP: 0018:ffffb51a01ffbd70 EFLAGS: 00010282
    [   81.526798] RAX: 0000000000000000 RBX: 0000000000000010 RCX: 0000000000000000
    [   81.528110] RDX: ffff9e66f1826480 RSI: ffff9e66f1816a08 RDI: ffff9e66f1816a08
    [   81.529437] RBP: ffffffff9153ff10 R08: 000000000000026c R09: 0000000000000053
    [   81.530732] R10: 0000000000000000 R11: ffffb51a01ffbc18 R12: ffff9e66cd682200
    [   81.532133] R13: ffffffff9153ff10 R14: ffff9e6685569500 R15: ffff9e66cd682000
    [   81.533442] FS:  0000000000000000(0000) GS:ffff9e66f1800000(0000) knlGS:0000000000000000
    [   81.534914] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   81.535971] CR2: 00005624c55b14d0 CR3: 00000003a023c000 CR4: 00000000003406f0
    [   81.537283] Call Trace:
    [   81.537763]  __raw_callee_save___pv_queued_spin_unlock_slowpath+0x11/0x20
    [   81.539011]  .slowpath+0x9/0xe
    [   81.539585]  hvc_alloc+0x25e/0x300
    [   81.540237]  init_port_console+0x28/0x100 [virtio_console]
    [   81.541251]  handle_control_message.constprop.27+0x1c4/0x310 [virtio_console]
    [   81.542546]  control_work_handler+0x70/0x10c [virtio_console]
    [   81.543601]  process_one_work+0x1a7/0x3b0
    [   81.544356]  worker_thread+0x30/0x390
    [   81.545025]  ? create_worker+0x1a0/0x1a0
    [   81.545749]  kthread+0x112/0x130
    [   81.546358]  ? kthread_flush_work_fn+0x10/0x10
    [   81.547183]  ret_from_fork+0x22/0x40
    [   81.547842] ---[ end trace aa97649bd16c8655 ]---
    [   83.546539] general protection fault: 0000 [#1] SMP NOPTI
    [   83.547422] CPU: 5 PID: 3225 Comm: modprobe Kdump: loaded Tainted: G        W        --------- -  - 4.18.0-167.el8.x86_64 #1
    [   83.549191] Hardware name: Red Hat KVM, BIOS 1.12.0-5.scrmod+el8.2.0+5159+d8aa4d83 04/01/2014
    [   83.550544] RIP: 0010:__pv_queued_spin_lock_slowpath+0x19a/0x2a0
    [   83.551504] Code: c4 c1 ea 12 41 be 01 00 00 00 4c 8d 6d 14 41 83 e4 03 8d 42 ff 49 c1 e4 05 48 98 49 81 c4 40 a5 02 00 4c 03 24 c5 60 48 34 91 <49> 89 2c 24 b8 00 80 00 00 eb 15 84 c0 75 0a 41 0f b6 54 24 14 84
    [   83.554449] RSP: 0018:ffffb51a0323fdb0 EFLAGS: 00010202
    [   83.555290] RAX: 000000000000301c RBX: ffffffff92080020 RCX: 0000000000000001
    [   83.556426] RDX: 000000000000301d RSI: 0000000000000000 RDI: 0000000000000000
    [   83.557556] RBP: ffff9e66f196a540 R08: 000000000000028a R09: ffff9e66d2757788
    [   83.558688] R10: 0000000000000000 R11: 0000000000000000 R12: 646e61725f770b07
    [   83.559821] R13: ffff9e66f196a554 R14: 0000000000000001 R15: 0000000000180000
    [   83.560958] FS:  00007fd5032e8740(0000) GS:ffff9e66f1940000(0000) knlGS:0000000000000000
    [   83.562233] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [   83.563149] CR2: 00007fd5022b0da0 CR3: 000000038c334000 CR4: 00000000003406e0
    
    Signed-off-by: Andrew Melnychenko <andrew@daynix.com>
    Cc: stable <stable@vger.kernel.org>
    Link: https://lore.kernel.org/r/20200414191503.3471783-1-andrew@daynix.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0b4f33def7bbde1ce2fea05f116639270e7acdc7
Author: Florian Westphal <fw@strlen.de>
Date:   Thu Apr 2 13:44:51 2020 +0200

    mptcp: fix tcp fallback crash
    
    Christoph Paasch reports following crash:
    
    general protection fault [..]
    CPU: 0 PID: 2874 Comm: syz-executor072 Not tainted 5.6.0-rc5 #62
    RIP: 0010:__pv_queued_spin_lock_slowpath kernel/locking/qspinlock.c:471
    [..]
     queued_spin_lock_slowpath arch/x86/include/asm/qspinlock.h:50 [inline]
     do_raw_spin_lock include/linux/spinlock.h:181 [inline]
     spin_lock_bh include/linux/spinlock.h:343 [inline]
     __mptcp_flush_join_list+0x44/0xb0 net/mptcp/protocol.c:278
     mptcp_shutdown+0xb3/0x230 net/mptcp/protocol.c:1882
    [..]
    
    Problem is that mptcp_shutdown() socket isn't an mptcp socket,
    its a plain tcp_sk.  Thus, trying to access mptcp_sk specific
    members accesses garbage.
    
    Root cause is that accept() returns a fallback (tcp) socket, not an mptcp
    one.  There is code in getpeername to detect this and override the sockets
    stream_ops.  But this will only run when accept() caller provided a
    sockaddr struct.  "accept(fd, NULL, 0)" will therefore result in
    mptcp stream ops, but with sock->sk pointing at a tcp_sk.
    
    Update the existing fallback handling to detect this as well.
    
    Moreover, mptcp_shutdown did not have fallback handling, and
    mptcp_poll did it too late so add that there as well.
    
    Reported-by: Christoph Paasch <cpaasch@apple.com>
    Tested-by: Christoph Paasch <cpaasch@apple.com>
    Reviewed-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
    Signed-off-by: Matthieu Baerts <matthieu.baerts@tessares.net>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 2c22c06ce426a0d025a3dacd17cd8868f3dbe96b
Author: Florian Westphal <fw@strlen.de>
Date:   Tue Feb 4 18:12:30 2020 +0100

    mptcp: fix use-after-free on tcp fallback
    
    When an mptcp socket connects to a tcp peer or when a middlebox interferes
    with tcp options, mptcp needs to fall back to plain tcp.
    Problem is that mptcp is trying to be too clever in this case:
    
    It attempts to close the mptcp meta sk and transparently replace it with
    the (only) subflow tcp sk.
    
    Unfortunately, this is racy -- the socket is already exposed to userspace.
    Any parallel calls to send/recv/setsockopt etc. can cause use-after-free:
    
    BUG: KASAN: use-after-free in atomic_try_cmpxchg include/asm-generic/atomic-instrumented.h:693 [inline]
    CPU: 1 PID: 2083 Comm: syz-executor.1 Not tainted 5.5.0 #2
     atomic_try_cmpxchg include/asm-generic/atomic-instrumented.h:693 [inline]
     queued_spin_lock include/asm-generic/qspinlock.h:78 [inline]
     do_raw_spin_lock include/linux/spinlock.h:181 [inline]
     __raw_spin_lock_bh include/linux/spinlock_api_smp.h:136 [inline]
     _raw_spin_lock_bh+0x71/0xd0 kernel/locking/spinlock.c:175
     spin_lock_bh include/linux/spinlock.h:343 [inline]
     __lock_sock+0x105/0x190 net/core/sock.c:2414
     lock_sock_nested+0x10f/0x140 net/core/sock.c:2938
     lock_sock include/net/sock.h:1516 [inline]
     mptcp_setsockopt+0x2f/0x1f0 net/mptcp/protocol.c:800
     __sys_setsockopt+0x152/0x240 net/socket.c:2130
     __do_sys_setsockopt net/socket.c:2146 [inline]
     __se_sys_setsockopt net/socket.c:2143 [inline]
     __x64_sys_setsockopt+0xba/0x150 net/socket.c:2143
     do_syscall_64+0xb7/0x3d0 arch/x86/entry/common.c:294
     entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    While the use-after-free can be resolved, there is another problem:
    sock->ops and sock->sk assignments are not atomic, i.e. we may get calls
    into mptcp functions with sock->sk already pointing at the subflow socket,
    or calls into tcp functions with a mptcp meta sk.
    
    Remove the fallback code and call the relevant functions for the (only)
    subflow in case the mptcp socket is connected to tcp peer.
    
    Reported-by: Christoph Paasch <cpaasch@apple.com>
    Diagnosed-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Reviewed-by: Mat Martineau <mathew.j.martineau@linux.intel.com>
    Tested-by: Christoph Paasch <cpaasch@apple.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 2180f214f4a5d8e2d8b7138d9a59246ee05753b9
Merge: 634cd4b6afe1 f5bfdc8e3947
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 28 09:33:25 2020 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "Just a handful of changes in this cycle: an ARM64 performance
      optimization, a comment fix and a debug output fix"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/osq: Use optimized spinning loop for arm64
      locking/qspinlock: Fix inaccessible URL of MCS lock paper
      locking/lockdep: Fix lockdep_stats indentation problem

commit 57097124cbbd310cc2b5884189e22e60a3c20514
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jan 7 12:49:14 2020 -0500

    locking/qspinlock: Fix inaccessible URL of MCS lock paper
    
    It turns out that the URL of the MCS lock paper listed in the source
    code is no longer accessible. I did got question about where the paper
    was. This patch updates the URL to BZ 206115 which contains a copy of
    the paper from
    
      https://www.cs.rochester.edu/u/scott/papers/1991_TOCS_synch.pdf
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lkml.kernel.org/r/20200107174914.4187-1-longman@redhat.com

commit 69d1d977c53c73b9d5116bd56a11ba77417986ec
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Sep 9 09:40:28 2019 +0800

    Revert "locking/pvqspinlock: Don't wait if vCPU is preempted"
    
    commit 89340d0935c9296c7b8222b6eab30e67cb57ab82 upstream.
    
    This patch reverts commit 75437bb304b20 (locking/pvqspinlock: Don't
    wait if vCPU is preempted).  A large performance regression was caused
    by this commit.  on over-subscription scenarios.
    
    The test was run on a Xeon Skylake box, 2 sockets, 40 cores, 80 threads,
    with three VMs of 80 vCPUs each.  The score of ebizzy -M is reduced from
    13000-14000 records/s to 1700-1800 records/s:
    
              Host                Guest                score
    
    vanilla w/o kvm optimizations     upstream    1700-1800 records/s
    vanilla w/o kvm optimizations     revert      13000-14000 records/s
    vanilla w/ kvm optimizations      upstream    4500-5000 records/s
    vanilla w/ kvm optimizations      revert      14000-15500 records/s
    
    Exit from aggressive wait-early mechanism can result in premature yield
    and extra scheduling latency.
    
    Actually, only 6% of wait_early events are caused by vcpu_is_preempted()
    being true.  However, when one vCPU voluntarily releases its vCPU, all
    the subsequently waiters in the queue will do the same and the cascading
    effect leads to bad performance.
    
    kvm optimizations:
    [1] commit d73eb57b80b (KVM: Boost vCPUs that are delivering interrupts)
    [2] commit 266e85a5ec9 (KVM: X86: Boost queue head vCPU to mitigate lock waiter preemption)
    
    Tested-by: loobinliu@tencent.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: loobinliu@tencent.com
    Cc: stable@vger.kernel.org
    Fixes: 75437bb304b20 (locking/pvqspinlock: Don't wait if vCPU is preempted)
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e409b81d9ddb0dd2d70636ca24f4587f65571833
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Sep 9 09:40:28 2019 +0800

    Revert "locking/pvqspinlock: Don't wait if vCPU is preempted"
    
    commit 89340d0935c9296c7b8222b6eab30e67cb57ab82 upstream.
    
    This patch reverts commit 75437bb304b20 (locking/pvqspinlock: Don't
    wait if vCPU is preempted).  A large performance regression was caused
    by this commit.  on over-subscription scenarios.
    
    The test was run on a Xeon Skylake box, 2 sockets, 40 cores, 80 threads,
    with three VMs of 80 vCPUs each.  The score of ebizzy -M is reduced from
    13000-14000 records/s to 1700-1800 records/s:
    
              Host                Guest                score
    
    vanilla w/o kvm optimizations     upstream    1700-1800 records/s
    vanilla w/o kvm optimizations     revert      13000-14000 records/s
    vanilla w/ kvm optimizations      upstream    4500-5000 records/s
    vanilla w/ kvm optimizations      revert      14000-15500 records/s
    
    Exit from aggressive wait-early mechanism can result in premature yield
    and extra scheduling latency.
    
    Actually, only 6% of wait_early events are caused by vcpu_is_preempted()
    being true.  However, when one vCPU voluntarily releases its vCPU, all
    the subsequently waiters in the queue will do the same and the cascading
    effect leads to bad performance.
    
    kvm optimizations:
    [1] commit d73eb57b80b (KVM: Boost vCPUs that are delivering interrupts)
    [2] commit 266e85a5ec9 (KVM: X86: Boost queue head vCPU to mitigate lock waiter preemption)
    
    Tested-by: loobinliu@tencent.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: loobinliu@tencent.com
    Cc: stable@vger.kernel.org
    Fixes: 75437bb304b20 (locking/pvqspinlock: Don't wait if vCPU is preempted)
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 859cc323167050bcda865415c1716ec9518947f0
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Sep 9 09:40:28 2019 +0800

    Revert "locking/pvqspinlock: Don't wait if vCPU is preempted"
    
    commit 89340d0935c9296c7b8222b6eab30e67cb57ab82 upstream.
    
    This patch reverts commit 75437bb304b20 (locking/pvqspinlock: Don't
    wait if vCPU is preempted).  A large performance regression was caused
    by this commit.  on over-subscription scenarios.
    
    The test was run on a Xeon Skylake box, 2 sockets, 40 cores, 80 threads,
    with three VMs of 80 vCPUs each.  The score of ebizzy -M is reduced from
    13000-14000 records/s to 1700-1800 records/s:
    
              Host                Guest                score
    
    vanilla w/o kvm optimizations     upstream    1700-1800 records/s
    vanilla w/o kvm optimizations     revert      13000-14000 records/s
    vanilla w/ kvm optimizations      upstream    4500-5000 records/s
    vanilla w/ kvm optimizations      revert      14000-15500 records/s
    
    Exit from aggressive wait-early mechanism can result in premature yield
    and extra scheduling latency.
    
    Actually, only 6% of wait_early events are caused by vcpu_is_preempted()
    being true.  However, when one vCPU voluntarily releases its vCPU, all
    the subsequently waiters in the queue will do the same and the cascading
    effect leads to bad performance.
    
    kvm optimizations:
    [1] commit d73eb57b80b (KVM: Boost vCPUs that are delivering interrupts)
    [2] commit 266e85a5ec9 (KVM: X86: Boost queue head vCPU to mitigate lock waiter preemption)
    
    Tested-by: loobinliu@tencent.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: loobinliu@tencent.com
    Cc: stable@vger.kernel.org
    Fixes: 75437bb304b20 (locking/pvqspinlock: Don't wait if vCPU is preempted)
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8bbe0dec38e147a50e9dd5f585295f7e68e0f2d0
Merge: e37e3bc7e265 fd3edd4a9066
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 27 12:44:26 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull more KVM updates from Paolo Bonzini:
     "x86 KVM changes:
    
       - The usual accuracy improvements for nested virtualization
    
       - The usual round of code cleanups from Sean
    
       - Added back optimizations that were prematurely removed in 5.2 (the
         bare minimum needed to fix the regression was in 5.3-rc8, here
         comes the rest)
    
       - Support for UMWAIT/UMONITOR/TPAUSE
    
       - Direct L2->L0 TLB flushing when L0 is Hyper-V and L1 is KVM
    
       - Tell Windows guests if SMT is disabled on the host
    
       - More accurate detection of vmexit cost
    
       - Revert a pvqspinlock pessimization"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (56 commits)
      KVM: nVMX: cleanup and fix host 64-bit mode checks
      KVM: vmx: fix build warnings in hv_enable_direct_tlbflush() on i386
      KVM: x86: Don't check kvm_rebooting in __kvm_handle_fault_on_reboot()
      KVM: x86: Drop ____kvm_handle_fault_on_reboot()
      KVM: VMX: Add error handling to VMREAD helper
      KVM: VMX: Optimize VMX instruction error and fault handling
      KVM: x86: Check kvm_rebooting in kvm_spurious_fault()
      KVM: selftests: fix ucall on x86
      Revert "locking/pvqspinlock: Don't wait if vCPU is preempted"
      kvm: nvmx: limit atomic switch MSRs
      kvm: svm: Intercept RDPRU
      kvm: x86: Add "significant index" flag to a few CPUID leaves
      KVM: x86/mmu: Skip invalid pages during zapping iff root_count is zero
      KVM: x86/mmu: Explicitly track only a single invalid mmu generation
      KVM: x86/mmu: Revert "KVM: x86/mmu: Remove is_obsolete() call"
      KVM: x86/mmu: Revert "Revert "KVM: MMU: reclaim the zapped-obsolete page first""
      KVM: x86/mmu: Revert "Revert "KVM: MMU: collapse TLB flushes when zap all pages""
      KVM: x86/mmu: Revert "Revert "KVM: MMU: zap pages in batch""
      KVM: x86/mmu: Revert "Revert "KVM: MMU: add tracepoint for kvm_mmu_invalidate_all_pages""
      KVM: x86/mmu: Revert "Revert "KVM: MMU: show mmu_valid_gen in shadow page related tracepoints""
      ...

commit 89340d0935c9296c7b8222b6eab30e67cb57ab82
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Sep 9 09:40:28 2019 +0800

    Revert "locking/pvqspinlock: Don't wait if vCPU is preempted"
    
    This patch reverts commit 75437bb304b20 (locking/pvqspinlock: Don't
    wait if vCPU is preempted).  A large performance regression was caused
    by this commit.  on over-subscription scenarios.
    
    The test was run on a Xeon Skylake box, 2 sockets, 40 cores, 80 threads,
    with three VMs of 80 vCPUs each.  The score of ebizzy -M is reduced from
    13000-14000 records/s to 1700-1800 records/s:
    
              Host                Guest                score
    
    vanilla w/o kvm optimizations     upstream    1700-1800 records/s
    vanilla w/o kvm optimizations     revert      13000-14000 records/s
    vanilla w/ kvm optimizations      upstream    4500-5000 records/s
    vanilla w/ kvm optimizations      revert      14000-15500 records/s
    
    Exit from aggressive wait-early mechanism can result in premature yield
    and extra scheduling latency.
    
    Actually, only 6% of wait_early events are caused by vcpu_is_preempted()
    being true.  However, when one vCPU voluntarily releases its vCPU, all
    the subsequently waiters in the queue will do the same and the cascading
    effect leads to bad performance.
    
    kvm optimizations:
    [1] commit d73eb57b80b (KVM: Boost vCPUs that are delivering interrupts)
    [2] commit 266e85a5ec9 (KVM: X86: Boost queue head vCPU to mitigate lock waiter preemption)
    
    Tested-by: loobinliu@tencent.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Waiman Long <longman@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: loobinliu@tencent.com
    Cc: stable@vger.kernel.org
    Fixes: 75437bb304b20 (locking/pvqspinlock: Don't wait if vCPU is preempted)
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit c7eba51cfdf9cd1ca7ed4201b30be8b2bef15ff5
Merge: cc9b499a1f71 e57d143091f1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 16 16:49:55 2019 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
    
     - improve rwsem scalability
    
     - add uninitialized rwsem debugging check
    
     - reduce lockdep's stacktrace memory usage and add diagnostics
    
     - misc cleanups, code consolidation and constification
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      mutex: Fix up mutex_waiter usage
      locking/mutex: Use mutex flags macro instead of hard code
      locking/mutex: Make __mutex_owner static to mutex.c
      locking/qspinlock,x86: Clarify virt_spin_lock_key
      locking/rwsem: Check for operations on an uninitialized rwsem
      locking/rwsem: Make handoff writer optimistically spin on owner
      locking/lockdep: Report more stack trace statistics
      locking/lockdep: Reduce space occupied by stack traces
      stacktrace: Constify 'entries' arguments
      locking/lockdep: Make it clear that what lock_class::key points at is not modified

commit 24a376d65177009a4dd8d846543c5dc69f5c4ced
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Aug 1 15:30:28 2019 +0200

    locking/qspinlock,x86: Clarify virt_spin_lock_key
    
    Add a few comments to clarify how this is supposed to work.
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Juergen Gross <jgross@suse.com>

commit da422ade5c879355f7410859069ef4f957c303d4
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Jul 23 14:22:03 2019 +0100

    Documentation/features/locking: update lists
    
    The locking feature lists don't match reality as of v5.3-rc1:
    
    * arm64 moved to queued spinlocks in commit:
      c11090474d70590170cf5fa6afe85864ab494b37
      ("arm64: locking: Replace ticket lock implementation with qspinlock")
    
    * xtensa moved to queued spinlocks and rwlocks in commit:
      579afe866f52adcd921272a224ab36733051059c
      ("xtensa: use generic spinlock/rwlock implementation")
    
    * architecture-specific rwsem support was removed in commit:
      46ad0840b1584b92b5ff2cc3ed0b011dd6b8e0f1
      ("locking/rwsem: Remove arch specific rwsem files")
    
    So update the feature lists accordingly, and remove the now redundant
    rwsem-optimized list.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>

commit 266e85a5ec9100dcd9ae03601453bbc96fefee5d
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Wed Jul 24 17:43:13 2019 +0800

    KVM: X86: Boost queue head vCPU to mitigate lock waiter preemption
    
    Commit 11752adb (locking/pvqspinlock: Implement hybrid PV queued/unfair locks)
    introduces hybrid PV queued/unfair locks
     - queued mode (no starvation)
     - unfair mode (good performance on not heavily contended lock)
    The lock waiter goes into the unfair mode especially in VMs with over-commit
    vCPUs since increaing over-commitment increase the likehood that the queue
    head vCPU may have been preempted and not actively spinning.
    
    However, reschedule queue head vCPU timely to acquire the lock still can get
    better performance than just depending on lock stealing in over-subscribe
    scenario.
    
    Testing on 80 HT 2 socket Xeon Skylake server, with 80 vCPUs VM 80GB RAM:
    ebizzy -M
                 vanilla     boosting    improved
     1VM          23520        25040         6%
     2VM           8000        13600        70%
     3VM           3100         5400        74%
    
    The lock holder vCPU yields to the queue head vCPU when unlock, to boost queue
    head vCPU which is involuntary preemption or the one which is voluntary halt
    due to fail to acquire the lock after a short spin in the guest.
    
    Cc: Waiman Long <longman@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit 007dc78fea62610bf06829e38f1d8c69b6ea5af6
Merge: 2f1835dffa94 d671002be6bd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 13:50:15 2019 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "Here are the locking changes in this cycle:
    
       - rwsem unification and simpler micro-optimizations to prepare for
         more intrusive (and more lucrative) scalability improvements in
         v5.3 (Waiman Long)
    
       - Lockdep irq state tracking flag usage cleanups (Frederic
         Weisbecker)
    
       - static key improvements (Jakub Kicinski, Peter Zijlstra)
    
       - misc updates, cleanups and smaller fixes"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (26 commits)
      locking/lockdep: Remove unnecessary unlikely()
      locking/static_key: Don't take sleeping locks in __static_key_slow_dec_deferred()
      locking/static_key: Factor out the fast path of static_key_slow_dec()
      locking/static_key: Add support for deferred static branches
      locking/lockdep: Test all incompatible scenarios at once in check_irq_usage()
      locking/lockdep: Avoid bogus Clang warning
      locking/lockdep: Generate LOCKF_ bit composites
      locking/lockdep: Use expanded masks on find_usage_*() functions
      locking/lockdep: Map remaining magic numbers to lock usage mask names
      locking/lockdep: Move valid_state() inside CONFIG_TRACE_IRQFLAGS && CONFIG_PROVE_LOCKING
      locking/rwsem: Prevent unneeded warning during locking selftest
      locking/rwsem: Optimize rwsem structure for uncontended lock acquisition
      locking/rwsem: Enable lock event counting
      locking/lock_events: Don't show pvqspinlock events on bare metal
      locking/lock_events: Make lock_events available for all archs & other locks
      locking/qspinlock_stat: Introduce generic lockevent_*() counting APIs
      locking/rwsem: Enhance DEBUG_RWSEMS_WARN_ON() macro
      locking/rwsem: Add debug check for __down_read*()
      locking/rwsem: Micro-optimize rwsem_try_read_lock_unqueued()
      locking/rwsem: Move rwsem internal function declarations to rwsem-xadd.h
      ...

commit 02143c2931c3c0faf088c5859a10de6c2b4f2d96
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri Mar 29 17:47:40 2019 -0700

    x86/hyperv: Make hv_vcpu_is_preempted() visible
    
    This function is referrenced from assembler, so it needs to be marked
    visible for LTO.
    
    Fixes: 3a025de64bf8 ("x86/hyperv: Enable PV qspinlock for Hyper-V")
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Yi Sun <yi.y.sun@linux.intel.com>
    Cc: kys@microsoft.com
    Cc: haiyangz@microsoft.com
    Link: https://lkml.kernel.org/r/20190330004743.29541-6-andi@firstfloor.org

commit bf20616f46e536fe8affed6f138db4b3040b55a6
Author: Waiman Long <longman@redhat.com>
Date:   Thu Apr 4 13:43:18 2019 -0400

    locking/lock_events: Don't show pvqspinlock events on bare metal
    
    On bare metal, the pvqspinlock event counts will always be 0. So there
    is no point in showing their corresponding debugfs files. So they are
    skipped in this case.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/20190404174320.22416-10-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit fb346fd9fc081c3d978c3f3d26d39334527a2662
Author: Waiman Long <longman@redhat.com>
Date:   Thu Apr 4 13:43:17 2019 -0400

    locking/lock_events: Make lock_events available for all archs & other locks
    
    The QUEUED_LOCK_STAT option to report queued spinlocks event counts
    was previously allowed only on x86 architecture. To make the locking
    event counting code more useful, it is now renamed to a more generic
    LOCK_EVENT_COUNTS config option. This new option will be available to
    all the architectures that use qspinlock at the moment.
    
    Other locking code can now start to use the generic locking event
    counting code by including lock_events.h and put the new locking event
    names into the lock_events_list.h header file.
    
    My experience with lock event counting is that it gives valuable insight
    on how the locking code works and what can be done to make it better. I
    would like to extend this benefit to other locking code like mutex and
    rwsem in the near future.
    
    The PV qspinlock specific code will stay in qspinlock_stat.h. The
    locking event counters will now reside in the <debugfs>/lock_event_counts
    directory.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/20190404174320.22416-9-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit ad53fa10fa9e816067bbae7109845940f5e6df50
Author: Waiman Long <longman@redhat.com>
Date:   Thu Apr 4 13:43:16 2019 -0400

    locking/qspinlock_stat: Introduce generic lockevent_*() counting APIs
    
    The percpu event counts used by qspinlock code can be useful for
    other locking code as well. So a new set of lockevent_* counting APIs
    is introduced with the lock event names extracted out into the new
    lock_events_list.h header file for easier addition in the future.
    
    The existing qstat_inc() calls are replaced by either lockevent_inc() or
    lockevent_cond_inc() calls.
    
    The qstat_hop() call is renamed to lockevent_pv_hop(). The "reset_counters"
    debugfs file is also renamed to ".reset_counts".
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/20190404174320.22416-8-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 3478588b5136966c80c571cf0006f08e9e5b8f04
Merge: c8f5ed6ef972 28d49e282665
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 6 07:17:17 2019 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The biggest part of this tree is the new auto-generated atomics API
      wrappers by Mark Rutland.
    
      The primary motivation was to allow instrumentation without uglifying
      the primary source code.
    
      The linecount increase comes from adding the auto-generated files to
      the Git space as well:
    
        include/asm-generic/atomic-instrumented.h     | 1689 ++++++++++++++++--
        include/asm-generic/atomic-long.h             | 1174 ++++++++++---
        include/linux/atomic-fallback.h               | 2295 +++++++++++++++++++++++++
        include/linux/atomic.h                        | 1241 +------------
    
      I preferred this approach, so that the full call stack of the (already
      complex) locking APIs is still fully visible in 'git grep'.
    
      But if this is excessive we could certainly hide them.
    
      There's a separate build-time mechanism to determine whether the
      headers are out of date (they should never be stale if we do our job
      right).
    
      Anyway, nothing from this should be visible to regular kernel
      developers.
    
      Other changes:
    
       - Add support for dynamic keys, which removes a source of false
         positives in the workqueue code, among other things (Bart Van
         Assche)
    
       - Updates to tools/memory-model (Andrea Parri, Paul E. McKenney)
    
       - qspinlock, wake_q and lockdep micro-optimizations (Waiman Long)
    
       - misc other updates and enhancements"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (48 commits)
      locking/lockdep: Shrink struct lock_class_key
      locking/lockdep: Add module_param to enable consistency checks
      lockdep/lib/tests: Test dynamic key registration
      lockdep/lib/tests: Fix run_tests.sh
      kernel/workqueue: Use dynamic lockdep keys for workqueues
      locking/lockdep: Add support for dynamic keys
      locking/lockdep: Verify whether lock objects are small enough to be used as class keys
      locking/lockdep: Check data structure consistency
      locking/lockdep: Reuse lock chains that have been freed
      locking/lockdep: Fix a comment in add_chain_cache()
      locking/lockdep: Introduce lockdep_next_lockchain() and lock_chain_count()
      locking/lockdep: Reuse list entries that are no longer in use
      locking/lockdep: Free lock classes that are no longer in use
      locking/lockdep: Update two outdated comments
      locking/lockdep: Make it easy to detect whether or not inside a selftest
      locking/lockdep: Split lockdep_free_key_range() and lockdep_reset_lock()
      locking/lockdep: Initialize the locks_before and locks_after lists earlier
      locking/lockdep: Make zap_class() remove all matching lock order entries
      locking/lockdep: Reorder struct lock_class members
      locking/lockdep: Avoid that add_chain_cache() adds an invalid chain to the cache
      ...

commit 733000c7ffd9d9c8c4fdfd82f0d41956c8cf0537
Author: Waiman Long <longman@redhat.com>
Date:   Sun Feb 24 20:14:13 2019 -0500

    locking/qspinlock: Remove unnecessary BUG_ON() call
    
    With the > 4 nesting levels case handled by the commit:
    
      d682b596d993 ("locking/qspinlock: Handle > 4 slowpath nesting levels")
    
    the BUG_ON() call in encode_tail() will never actually be triggered.
    
    Remove it.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/1551057253-3231-1-git-send-email-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 412f34a82ccf7dd52f6b197f6450a33f03342523
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jan 29 22:53:46 2019 +0100

    locking/qspinlock_stat: Track the no MCS node available case
    
    Track the number of slowpath locking operations that are being done
    without any MCS node available as well renaming lock_index[123] to make
    them more descriptive.
    
    Using these stat counters is one way to find out if a code path is
    being exercised.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: SRINIVAS <srinivas.eeda@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Link: https://lkml.kernel.org/r/1548798828-16156-3-git-send-email-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d682b596d99345ef0000e7017db714ba7f29e017
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jan 29 22:53:45 2019 +0100

    locking/qspinlock: Handle > 4 slowpath nesting levels
    
    Four queue nodes per CPU are allocated to enable up to 4 nesting levels
    using the per-CPU nodes. Nested NMIs are possible in some architectures.
    Still it is very unlikely that we will ever hit more than 4 nested
    levels with contention in the slowpath.
    
    When that rare condition happens, however, it is likely that the system
    will hang or crash shortly after that. It is not good and we need to
    handle this exception case.
    
    This is done by spinning directly on the lock using repeated trylock.
    This alternative code path should only be used when there is nested
    NMIs. Assuming that the locks used by those NMI handlers will not be
    heavily contended, a simple TAS locking should work out.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: SRINIVAS <srinivas.eeda@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Link: https://lkml.kernel.org/r/1548798828-16156-2-git-send-email-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 2863debfbc6e881913eb8c2847bea963cde410b4
Merge: 1832f4ef5867 ba72a7b4badb
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Feb 1 20:55:40 2019 +0100

    Merge branch 'bpf-spinlocks'
    
    Alexei Starovoitov says:
    
    ====================
    Many algorithms need to read and modify several variables atomically.
    Until now it was hard to impossible to implement such algorithms in BPF.
    Hence introduce support for bpf_spin_lock.
    
    The api consists of 'struct bpf_spin_lock' that should be placed
    inside hash/array/cgroup_local_storage element
    and bpf_spin_lock/unlock() helper function.
    
    Example:
    struct hash_elem {
        int cnt;
        struct bpf_spin_lock lock;
    };
    struct hash_elem * val = bpf_map_lookup_elem(&hash_map, &key);
    if (val) {
        bpf_spin_lock(&val->lock);
        val->cnt++;
        bpf_spin_unlock(&val->lock);
    }
    
    and BPF_F_LOCK flag for lookup/update bpf syscall commands that
    allows user space to read/write map elements under lock.
    
    Together these primitives allow race free access to map elements
    from bpf programs and from user space.
    
    Key restriction: root only.
    Key requirement: maps must be annotated with BTF.
    
    This concept was discussed at Linux Plumbers Conference 2018.
    Thank you everyone who participated and helped to iron out details
    of api and implementation.
    
    Patch 1: bpf_spin_lock support in the verifier, BTF, hash, array.
    Patch 2: bpf_spin_lock in cgroup local storage.
    Patches 3,4,5: tests
    Patch 6: BPF_F_LOCK flag to lookup/update
    Patches 7,8,9: tests
    
    v6->v7:
    - fixed this_cpu->__this_cpu per Peter's suggestion and added Ack.
    - simplified bpf_spin_lock and load/store overlap check in the verifier
      as suggested by Andrii
    - rebase
    
    v5->v6:
    - adopted arch_spinlock approach suggested by Peter
    - switched to spin_lock_irqsave equivalent as the simplest way
      to avoid deadlocks in rare case of nested networking progs
      (cgroup-bpf prog in preempt_disable vs clsbpf in softirq sharing
      the same map with bpf_spin_lock)
      bpf_spin_lock is only allowed in networking progs that don't
      have arbitrary entry points unlike tracing progs.
    - rebase and split test_verifier tests
    
    v4->v5:
    - disallow bpf_spin_lock for tracing progs due to insufficient preemption checks
    - socket filter progs cannot use bpf_spin_lock due to missing preempt_disable
    - fix atomic_set_release. Spotted by Peter.
    - fixed hash_of_maps
    
    v3->v4:
    - fix BPF_EXIST | BPF_NOEXIST check patch 6. Spotted by Jakub. Thanks!
    - rebase
    
    v2->v3:
    - fixed build on ia64 and archs where qspinlock is not supported
    - fixed missing lock init during lookup w/o BPF_F_LOCK. Spotted by Martin
    
    v1->v2:
    - addressed several issues spotted by Daniel and Martin in patch 1
    - added test11 to patch 4 as suggested by Daniel
    ====================
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

commit 4b527f25a4ac64c0d17c882cdf8322be66f31611
Author: Dave Airlie <airlied@redhat.com>
Date:   Thu Jan 24 18:54:15 2019 +0000

    locking/qspinlock: Pull in asm/byteorder.h to ensure correct endianness
    
    This commit is not required upstream, but is required for the 4.9.y
    stable series.
    
    Upstream commit 101110f6271c ("Kbuild: always define endianess in
    kconfig.h") ensures that either __LITTLE_ENDIAN or __BIG_ENDIAN is
    defined to reflect the endianness of the target CPU architecture
    regardless of whether or not <asm/byteorder.h> has been #included. The
    upstream definition of 'struct qspinlock' relies on this property.
    
    Unfortunately, the 4.9.y stable series does not provide this guarantee,
    so the 'spin_unlock()' routine can erroneously treat the underlying
    lockword as big-endian on little-endian architectures using native
    qspinlock (i.e. x86_64 without PV) if the caller has not included
    <asm/byteorder.h>. This can lead to hangs such as the one in
    'i915_gem_request()' reported via bugzilla:
    
      https://bugzilla.kernel.org/show_bug.cgi?id=202063
    
    Fix the issue by ensuring that <asm/byteorder.h> is #included in
    <asm/qspinlock_types.h>, where 'struct qspinlock' is defined.
    
    Cc: <stable@vger.kernel.org> # 4.9
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    [will: wrote commit message]
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2658687568cd36cc1250106032d540454c0046c9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Dec 18 18:48:28 2018 +0100

    locking/qspinlock, x86: Provide liveness guarantee
    
    commit 7aa54be2976550f17c11a1c3e3630002dea39303 upstream.
    
    On x86 we cannot do fetch_or() with a single instruction and thus end up
    using a cmpxchg loop, this reduces determinism. Replace the fetch_or()
    with a composite operation: tas-pending + load.
    
    Using two instructions of course opens a window we previously did not
    have. Consider the scenario:
    
            CPU0            CPU1            CPU2
    
     1)     lock
              trylock -> (0,0,1)
    
     2)                     lock
                              trylock /* fail */
    
     3)     unlock -> (0,0,0)
    
     4)                                     lock
                                              trylock -> (0,0,1)
    
     5)                       tas-pending -> (0,1,1)
                              load-val <- (0,1,0) from 3
    
     6)                       clear-pending-set-locked -> (0,0,1)
    
                              FAIL: _2_ owners
    
    where 5) is our new composite operation. When we consider each part of
    the qspinlock state as a separate variable (as we can when
    _Q_PENDING_BITS == 8) then the above is entirely possible, because
    tas-pending will only RmW the pending byte, so the later load is able
    to observe prior tail and lock state (but not earlier than its own
    trylock, which operates on the whole word, due to coherence).
    
    To avoid this we need 2 things:
    
     - the load must come after the tas-pending (obviously, otherwise it
       can trivially observe prior state).
    
     - the tas-pending must be a full word RmW instruction, it cannot be an XCHGB for
       example, such that we cannot observe other state prior to setting
       pending.
    
    On x86 we can realize this by using "LOCK BTS m32, r32" for
    tas-pending followed by a regular load.
    
    Note that observing later state is not a problem:
    
     - if we fail to observe a later unlock, we'll simply spin-wait for
       that store to become visible.
    
     - if we observe a later xchg_tail(), there is no difference from that
       xchg_tail() having taken place before the tas-pending.
    
    Suggested-by: Will Deacon <will.deacon@arm.com>
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: andrea.parri@amarulasolutions.com
    Cc: longman@redhat.com
    Fixes: 59fb586b4a07 ("locking/qspinlock: Remove unbounded cmpxchg() loop from locking slowpath")
    Link: https://lkml.kernel.org/r/20181003130957.183726335@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    [bigeasy: GEN_BINARY_RMWcc macro redo]
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 150f038c9382e92de446caee1754d65c47127341
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Dec 18 18:48:27 2018 +0100

    locking/qspinlock: Re-order code
    
    commit 53bf57fab7321fb42b703056a4c80fc9d986d170 upstream.
    
    Flip the branch condition after atomic_fetch_or_acquire(_Q_PENDING_VAL)
    such that we loose the indent. This also result in a more natural code
    flow IMO.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: andrea.parri@amarulasolutions.com
    Cc: longman@redhat.com
    Link: https://lkml.kernel.org/r/20181003130257.156322446@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 1f972505011c27d09423a46707789192a039b66a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jun 21 20:35:26 2018 -0400

    locking/qspinlock: Fix build for anonymous union in older GCC compilers
    
    [ Upstream commit 6cc65be4f6f2a7186af8f3e09900787c7912dad2 ]
    
    One of my tests compiles the kernel with gcc 4.5.3, and I hit the
    following build error:
    
      include/linux/semaphore.h: In function 'sema_init':
      include/linux/semaphore.h:35:17: error: unknown field 'val' specified in initializer
      include/linux/semaphore.h:35:17: warning: missing braces around initializer
      include/linux/semaphore.h:35:17: warning: (near initialization for '(anonymous).raw_lock.<anonymous>.val')
    
    I bisected it down to:
    
     625e88be1f41 ("locking/qspinlock: Merge 'struct __qspinlock' into 'struct qspinlock'")
    
    ... which makes qspinlock have an anonymous union, which makes initializing it special
    for older compilers. By adding strategic brackets, it makes the build
    happy again.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Fixes: 625e88be1f41 ("locking/qspinlock: Merge 'struct __qspinlock' into 'struct qspinlock'")
    Link: http://lkml.kernel.org/r/20180621203526.172ab5c4@vmware.local.home
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 5d01e063296a80e93abe93803602e5cf37309865
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Dec 18 18:14:00 2018 +0100

    locking/qspinlock, x86: Provide liveness guarantee
    
    commit 7aa54be2976550f17c11a1c3e3630002dea39303 upstream.
    
    On x86 we cannot do fetch_or() with a single instruction and thus end up
    using a cmpxchg loop, this reduces determinism. Replace the fetch_or()
    with a composite operation: tas-pending + load.
    
    Using two instructions of course opens a window we previously did not
    have. Consider the scenario:
    
            CPU0            CPU1            CPU2
    
     1)     lock
              trylock -> (0,0,1)
    
     2)                     lock
                              trylock /* fail */
    
     3)     unlock -> (0,0,0)
    
     4)                                     lock
                                              trylock -> (0,0,1)
    
     5)                       tas-pending -> (0,1,1)
                              load-val <- (0,1,0) from 3
    
     6)                       clear-pending-set-locked -> (0,0,1)
    
                              FAIL: _2_ owners
    
    where 5) is our new composite operation. When we consider each part of
    the qspinlock state as a separate variable (as we can when
    _Q_PENDING_BITS == 8) then the above is entirely possible, because
    tas-pending will only RmW the pending byte, so the later load is able
    to observe prior tail and lock state (but not earlier than its own
    trylock, which operates on the whole word, due to coherence).
    
    To avoid this we need 2 things:
    
     - the load must come after the tas-pending (obviously, otherwise it
       can trivially observe prior state).
    
     - the tas-pending must be a full word RmW instruction, it cannot be an XCHGB for
       example, such that we cannot observe other state prior to setting
       pending.
    
    On x86 we can realize this by using "LOCK BTS m32, r32" for
    tas-pending followed by a regular load.
    
    Note that observing later state is not a problem:
    
     - if we fail to observe a later unlock, we'll simply spin-wait for
       that store to become visible.
    
     - if we observe a later xchg_tail(), there is no difference from that
       xchg_tail() having taken place before the tas-pending.
    
    Suggested-by: Will Deacon <will.deacon@arm.com>
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: andrea.parri@amarulasolutions.com
    Cc: longman@redhat.com
    Fixes: 59fb586b4a07 ("locking/qspinlock: Remove unbounded cmpxchg() loop from locking slowpath")
    Link: https://lkml.kernel.org/r/20181003130957.183726335@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    [bigeasy: GEN_BINARY_RMWcc macro redo]
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 4e21502d37a503d4311e2bdb46a66aef783a7cfe
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 18:13:59 2018 +0100

    locking/qspinlock/x86: Increase _Q_PENDING_LOOPS upper bound
    
    commit b247be3fe89b6aba928bf80f4453d1c4ba8d2063 upstream.
    
    On x86, atomic_cond_read_relaxed will busy-wait with a cpu_relax() loop,
    so it is desirable to increase the number of times we spin on the qspinlock
    lockword when it is found to be transitioning from pending to locked.
    
    According to Waiman Long:
    
     | Ideally, the spinning times should be at least a few times the typical
     | cacheline load time from memory which I think can be down to 100ns or
     | so for each cacheline load with the newest systems or up to several
     | hundreds ns for older systems.
    
    which in his benchmarking corresponded to 512 iterations.
    
    Suggested-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-5-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 49849a651b8456263391ae767be6f7d02af8ef26
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Dec 18 18:13:58 2018 +0100

    locking/qspinlock: Re-order code
    
    commit 53bf57fab7321fb42b703056a4c80fc9d986d170 upstream.
    
    Flip the branch condition after atomic_fetch_or_acquire(_Q_PENDING_VAL)
    such that we loose the indent. This also result in a more natural code
    flow IMO.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: andrea.parri@amarulasolutions.com
    Cc: longman@redhat.com
    Link: https://lkml.kernel.org/r/20181003130257.156322446@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f2f76a2c666e9b09fa1fb903bdb20c756dc90e21
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 18:13:57 2018 +0100

    locking/qspinlock: Kill cmpxchg() loop when claiming lock from head of queue
    
    commit c61da58d8a9ba9238250a548f00826eaf44af0f7 upstream.
    
    When a queued locker reaches the head of the queue, it claims the lock
    by setting _Q_LOCKED_VAL in the lockword. If there isn't contention, it
    must also clear the tail as part of this operation so that subsequent
    lockers can avoid taking the slowpath altogether.
    
    Currently this is expressed as a cmpxchg() loop that practically only
    runs up to two iterations. This is confusing to the reader and unhelpful
    to the compiler. Rewrite the cmpxchg() loop without the loop, so that a
    failed cmpxchg() implies that there is contention and we just need to
    write to _Q_LOCKED_VAL without considering the rest of the lockword.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-7-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 075703d79c3a0a603a69e09167f96c1a2502eabe
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 18:13:56 2018 +0100

    locking/qspinlock: Remove duplicate clear_pending() function from PV code
    
    commit 3bea9adc96842b8a7345c7fb202c16ae9c8d5b25 upstream.
    
    The native clear_pending() function is identical to the PV version, so the
    latter can simply be removed.
    
    This fixes the build for systems with >= 16K CPUs using the PV lock implementation.
    
    Reported-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/20180427101619.GB21705@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 7a617996cba577806960e87882e60d7444d01df4
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 18:13:55 2018 +0100

    locking/qspinlock: Remove unbounded cmpxchg() loop from locking slowpath
    
    commit 59fb586b4a07b4e1a0ee577140ab4842ba451acd upstream.
    
    The qspinlock locking slowpath utilises a "pending" bit as a simple form
    of an embedded test-and-set lock that can avoid the overhead of explicit
    queuing in cases where the lock is held but uncontended. This bit is
    managed using a cmpxchg() loop which tries to transition the uncontended
    lock word from (0,0,0) -> (0,0,1) or (0,0,1) -> (0,1,1).
    
    Unfortunately, the cmpxchg() loop is unbounded and lockers can be starved
    indefinitely if the lock word is seen to oscillate between unlocked
    (0,0,0) and locked (0,0,1). This could happen if concurrent lockers are
    able to take the lock in the cmpxchg() loop without queuing and pass it
    around amongst themselves.
    
    This patch fixes the problem by unconditionally setting _Q_PENDING_VAL
    using atomic_fetch_or, and then inspecting the old value to see whether
    we need to spin on the current lock owner, or whether we now effectively
    hold the lock. The tricky scenario is when concurrent lockers end up
    queuing on the lock and the lock becomes available, causing us to see
    a lockword of (n,0,0). With pending now set, simply queuing could lead
    to deadlock as the head of the queue may not have observed the pending
    flag being cleared. Conversely, if the head of the queue did observe
    pending being cleared, then it could transition the lock from (n,0,0) ->
    (0,0,1) meaning that any attempt to "undo" our setting of the pending
    bit could race with a concurrent locker trying to set it.
    
    We handle this race by preserving the pending bit when taking the lock
    after reaching the head of the queue and leaving the tail entry intact
    if we saw pending set, because we know that the tail is going to be
    updated shortly.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-6-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 5261ad70e2d673b9cfea7c81f309cabc847b7a27
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 18:13:54 2018 +0100

    locking/qspinlock: Merge 'struct __qspinlock' into 'struct qspinlock'
    
    commit 625e88be1f41b53cec55827c984e4a89ea8ee9f9 upstream.
    
    'struct __qspinlock' provides a handy union of fields so that
    subcomponents of the lockword can be accessed by name, without having to
    manage shifts and masks explicitly and take endianness into account.
    
    This is useful in qspinlock.h and also potentially in arch headers, so
    move the 'struct __qspinlock' into 'struct qspinlock' and kill the extra
    definition.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Acked-by: Boqun Feng <boqun.feng@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-3-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 3dab30f33814dc0b97c001921d3d724004c09b5f
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 18:13:53 2018 +0100

    locking/qspinlock: Bound spinning on pending->locked transition in slowpath
    
    commit 6512276d97b160d90b53285bd06f7f201459a7e3 upstream.
    
    If a locker taking the qspinlock slowpath reads a lock value indicating
    that only the pending bit is set, then it will spin whilst the
    concurrent pending->locked transition takes effect.
    
    Unfortunately, there is no guarantee that such a transition will ever be
    observed since concurrent lockers could continuously set pending and
    hand over the lock amongst themselves, leading to starvation. Whilst
    this would probably resolve in practice, it means that it is not
    possible to prove liveness properties about the lock and means that lock
    acquisition time is unbounded.
    
    Rather than removing the pending->locked spinning from the slowpath
    altogether (which has been shown to heavily penalise a 2-threaded
    locking stress test on x86), this patch replaces the explicit spinning
    with a call to atomic_cond_read_relaxed and allows the architecture to
    provide a bound on the number of spins. For architectures that can
    respond to changes in cacheline state in their smp_cond_load implementation,
    it should be sufficient to use the default bound of 1.
    
    Suggested-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-4-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 13f14c36323d71e2178d7d97dc65808c0e5c872e
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 18:13:52 2018 +0100

    locking/qspinlock: Ensure node is initialised before updating prev->next
    
    commit 95bcade33a8af38755c9b0636e36a36ad3789fe6 upstream.
    
    When a locker ends up queuing on the qspinlock locking slowpath, we
    initialise the relevant mcs node and publish it indirectly by updating
    the tail portion of the lock word using xchg_tail. If we find that there
    was a pre-existing locker in the queue, we subsequently update their
    ->next field to point at our node so that we are notified when it's our
    turn to take the lock.
    
    This can be roughly illustrated as follows:
    
      /* Initialise the fields in node and encode a pointer to node in tail */
      tail = initialise_node(node);
    
      /*
       * Exchange tail into the lockword using an atomic read-modify-write
       * operation with release semantics
       */
      old = xchg_tail(lock, tail);
    
      /* If there was a pre-existing waiter ... */
      if (old & _Q_TAIL_MASK) {
            prev = decode_tail(old);
            smp_read_barrier_depends();
    
            /* ... then update their ->next field to point to node.
            WRITE_ONCE(prev->next, node);
      }
    
    The conditional update of prev->next therefore relies on the address
    dependency from the result of xchg_tail ensuring order against the
    prior initialisation of node. However, since the release semantics of
    the xchg_tail operation apply only to the write portion of the RmW,
    then this ordering is not guaranteed and it is possible for the CPU
    to return old before the writes to node have been published, consequently
    allowing us to point prev->next to an uninitialised node.
    
    This patch fixes the problem by making the update of prev->next a RELEASE
    operation, which also removes the reliance on dependency ordering.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1518528177-19169-2-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit c6bcf40f769294a80c64213f9175ccd408d64532
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jun 21 20:35:26 2018 -0400

    locking/qspinlock: Fix build for anonymous union in older GCC compilers
    
    [ Upstream commit 6cc65be4f6f2a7186af8f3e09900787c7912dad2 ]
    
    One of my tests compiles the kernel with gcc 4.5.3, and I hit the
    following build error:
    
      include/linux/semaphore.h: In function 'sema_init':
      include/linux/semaphore.h:35:17: error: unknown field 'val' specified in initializer
      include/linux/semaphore.h:35:17: warning: missing braces around initializer
      include/linux/semaphore.h:35:17: warning: (near initialization for '(anonymous).raw_lock.<anonymous>.val')
    
    I bisected it down to:
    
     625e88be1f41 ("locking/qspinlock: Merge 'struct __qspinlock' into 'struct qspinlock'")
    
    ... which makes qspinlock have an anonymous union, which makes initializing it special
    for older compilers. By adding strategic brackets, it makes the build
    happy again.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Fixes: 625e88be1f41 ("locking/qspinlock: Merge 'struct __qspinlock' into 'struct qspinlock'")
    Link: http://lkml.kernel.org/r/20180621203526.172ab5c4@vmware.local.home
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 88ce30fb88a192d8bf3b789122030a85e6c3ce5a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Dec 18 23:10:49 2018 +0100

    locking/qspinlock, x86: Provide liveness guarantee
    
    commit 7aa54be2976550f17c11a1c3e3630002dea39303 upstream.
    
    On x86 we cannot do fetch_or() with a single instruction and thus end up
    using a cmpxchg loop, this reduces determinism. Replace the fetch_or()
    with a composite operation: tas-pending + load.
    
    Using two instructions of course opens a window we previously did not
    have. Consider the scenario:
    
            CPU0            CPU1            CPU2
    
     1)     lock
              trylock -> (0,0,1)
    
     2)                     lock
                              trylock /* fail */
    
     3)     unlock -> (0,0,0)
    
     4)                                     lock
                                              trylock -> (0,0,1)
    
     5)                       tas-pending -> (0,1,1)
                              load-val <- (0,1,0) from 3
    
     6)                       clear-pending-set-locked -> (0,0,1)
    
                              FAIL: _2_ owners
    
    where 5) is our new composite operation. When we consider each part of
    the qspinlock state as a separate variable (as we can when
    _Q_PENDING_BITS == 8) then the above is entirely possible, because
    tas-pending will only RmW the pending byte, so the later load is able
    to observe prior tail and lock state (but not earlier than its own
    trylock, which operates on the whole word, due to coherence).
    
    To avoid this we need 2 things:
    
     - the load must come after the tas-pending (obviously, otherwise it
       can trivially observe prior state).
    
     - the tas-pending must be a full word RmW instruction, it cannot be an XCHGB for
       example, such that we cannot observe other state prior to setting
       pending.
    
    On x86 we can realize this by using "LOCK BTS m32, r32" for
    tas-pending followed by a regular load.
    
    Note that observing later state is not a problem:
    
     - if we fail to observe a later unlock, we'll simply spin-wait for
       that store to become visible.
    
     - if we observe a later xchg_tail(), there is no difference from that
       xchg_tail() having taken place before the tas-pending.
    
    Suggested-by: Will Deacon <will.deacon@arm.com>
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: andrea.parri@amarulasolutions.com
    Cc: longman@redhat.com
    Fixes: 59fb586b4a07 ("locking/qspinlock: Remove unbounded cmpxchg() loop from locking slowpath")
    Link: https://lkml.kernel.org/r/20181003130957.183726335@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    [bigeasy: GEN_BINARY_RMWcc macro redo]
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 8ae5642df23de3d2fce8f176943c0739abbeb9da
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 23:10:48 2018 +0100

    locking/qspinlock/x86: Increase _Q_PENDING_LOOPS upper bound
    
    commit b247be3fe89b6aba928bf80f4453d1c4ba8d2063 upstream.
    
    On x86, atomic_cond_read_relaxed will busy-wait with a cpu_relax() loop,
    so it is desirable to increase the number of times we spin on the qspinlock
    lockword when it is found to be transitioning from pending to locked.
    
    According to Waiman Long:
    
     | Ideally, the spinning times should be at least a few times the typical
     | cacheline load time from memory which I think can be down to 100ns or
     | so for each cacheline load with the newest systems or up to several
     | hundreds ns for older systems.
    
    which in his benchmarking corresponded to 512 iterations.
    
    Suggested-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-5-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f650bdcabf560f530b74a2266a17549fdf084161
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Dec 18 23:10:47 2018 +0100

    locking/qspinlock: Re-order code
    
    commit 53bf57fab7321fb42b703056a4c80fc9d986d170 upstream.
    
    Flip the branch condition after atomic_fetch_or_acquire(_Q_PENDING_VAL)
    such that we loose the indent. This also result in a more natural code
    flow IMO.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: andrea.parri@amarulasolutions.com
    Cc: longman@redhat.com
    Link: https://lkml.kernel.org/r/20181003130257.156322446@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 0952e8f0e62456d7b6eb0abb3f71213b7630cf0f
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 23:10:46 2018 +0100

    locking/qspinlock: Kill cmpxchg() loop when claiming lock from head of queue
    
    commit c61da58d8a9ba9238250a548f00826eaf44af0f7 upstream.
    
    When a queued locker reaches the head of the queue, it claims the lock
    by setting _Q_LOCKED_VAL in the lockword. If there isn't contention, it
    must also clear the tail as part of this operation so that subsequent
    lockers can avoid taking the slowpath altogether.
    
    Currently this is expressed as a cmpxchg() loop that practically only
    runs up to two iterations. This is confusing to the reader and unhelpful
    to the compiler. Rewrite the cmpxchg() loop without the loop, so that a
    failed cmpxchg() implies that there is contention and we just need to
    write to _Q_LOCKED_VAL without considering the rest of the lockword.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-7-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 0f28d5f4ce393354e50de2da743dc04e7bcd8667
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 23:10:45 2018 +0100

    locking/qspinlock: Remove duplicate clear_pending() function from PV code
    
    commit 3bea9adc96842b8a7345c7fb202c16ae9c8d5b25 upstream.
    
    The native clear_pending() function is identical to the PV version, so the
    latter can simply be removed.
    
    This fixes the build for systems with >= 16K CPUs using the PV lock implementation.
    
    Reported-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/20180427101619.GB21705@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 9b5884372c792b0dade0cf22390a9516c870b7d7
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 23:10:44 2018 +0100

    locking/qspinlock: Remove unbounded cmpxchg() loop from locking slowpath
    
    commit 59fb586b4a07b4e1a0ee577140ab4842ba451acd upstream.
    
    The qspinlock locking slowpath utilises a "pending" bit as a simple form
    of an embedded test-and-set lock that can avoid the overhead of explicit
    queuing in cases where the lock is held but uncontended. This bit is
    managed using a cmpxchg() loop which tries to transition the uncontended
    lock word from (0,0,0) -> (0,0,1) or (0,0,1) -> (0,1,1).
    
    Unfortunately, the cmpxchg() loop is unbounded and lockers can be starved
    indefinitely if the lock word is seen to oscillate between unlocked
    (0,0,0) and locked (0,0,1). This could happen if concurrent lockers are
    able to take the lock in the cmpxchg() loop without queuing and pass it
    around amongst themselves.
    
    This patch fixes the problem by unconditionally setting _Q_PENDING_VAL
    using atomic_fetch_or, and then inspecting the old value to see whether
    we need to spin on the current lock owner, or whether we now effectively
    hold the lock. The tricky scenario is when concurrent lockers end up
    queuing on the lock and the lock becomes available, causing us to see
    a lockword of (n,0,0). With pending now set, simply queuing could lead
    to deadlock as the head of the queue may not have observed the pending
    flag being cleared. Conversely, if the head of the queue did observe
    pending being cleared, then it could transition the lock from (n,0,0) ->
    (0,0,1) meaning that any attempt to "undo" our setting of the pending
    bit could race with a concurrent locker trying to set it.
    
    We handle this race by preserving the pending bit when taking the lock
    after reaching the head of the queue and leaving the tail entry intact
    if we saw pending set, because we know that the tail is going to be
    updated shortly.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-6-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 60668f3cddf1b25a954b198cade0ce726a6853ab
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 23:10:43 2018 +0100

    locking/qspinlock: Merge 'struct __qspinlock' into 'struct qspinlock'
    
    commit 625e88be1f41b53cec55827c984e4a89ea8ee9f9 upstream.
    
    'struct __qspinlock' provides a handy union of fields so that
    subcomponents of the lockword can be accessed by name, without having to
    manage shifts and masks explicitly and take endianness into account.
    
    This is useful in qspinlock.h and also potentially in arch headers, so
    move the 'struct __qspinlock' into 'struct qspinlock' and kill the extra
    definition.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Acked-by: Boqun Feng <boqun.feng@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-3-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 8e5b3bcc5291092aaac4cadc0b5fb46182172ed3
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 23:10:42 2018 +0100

    locking/qspinlock: Bound spinning on pending->locked transition in slowpath
    
    commit 6512276d97b160d90b53285bd06f7f201459a7e3 upstream.
    
    If a locker taking the qspinlock slowpath reads a lock value indicating
    that only the pending bit is set, then it will spin whilst the
    concurrent pending->locked transition takes effect.
    
    Unfortunately, there is no guarantee that such a transition will ever be
    observed since concurrent lockers could continuously set pending and
    hand over the lock amongst themselves, leading to starvation. Whilst
    this would probably resolve in practice, it means that it is not
    possible to prove liveness properties about the lock and means that lock
    acquisition time is unbounded.
    
    Rather than removing the pending->locked spinning from the slowpath
    altogether (which has been shown to heavily penalise a 2-threaded
    locking stress test on x86), this patch replaces the explicit spinning
    with a call to atomic_cond_read_relaxed and allows the architecture to
    provide a bound on the number of spins. For architectures that can
    respond to changes in cacheline state in their smp_cond_load implementation,
    it should be sufficient to use the default bound of 1.
    
    Suggested-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-4-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 48c42d4dfec408760d15acc334d91208a6b2262e
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 23:10:41 2018 +0100

    locking/qspinlock: Ensure node is initialised before updating prev->next
    
    commit 95bcade33a8af38755c9b0636e36a36ad3789fe6 upstream.
    
    When a locker ends up queuing on the qspinlock locking slowpath, we
    initialise the relevant mcs node and publish it indirectly by updating
    the tail portion of the lock word using xchg_tail. If we find that there
    was a pre-existing locker in the queue, we subsequently update their
    ->next field to point at our node so that we are notified when it's our
    turn to take the lock.
    
    This can be roughly illustrated as follows:
    
      /* Initialise the fields in node and encode a pointer to node in tail */
      tail = initialise_node(node);
    
      /*
       * Exchange tail into the lockword using an atomic read-modify-write
       * operation with release semantics
       */
      old = xchg_tail(lock, tail);
    
      /* If there was a pre-existing waiter ... */
      if (old & _Q_TAIL_MASK) {
            prev = decode_tail(old);
            smp_read_barrier_depends();
    
            /* ... then update their ->next field to point to node.
            WRITE_ONCE(prev->next, node);
      }
    
    The conditional update of prev->next therefore relies on the address
    dependency from the result of xchg_tail ensuring order against the
    prior initialisation of node. However, since the release semantics of
    the xchg_tail operation apply only to the write portion of the RmW,
    then this ordering is not guaranteed and it is possible for the CPU
    to return old before the writes to node have been published, consequently
    allowing us to point prev->next to an uninitialised node.
    
    This patch fixes the problem by making the update of prev->next a RELEASE
    operation, which also removes the reliance on dependency ordering.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1518528177-19169-2-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d92bb9a9dc3865f222e0c934a893cb63ac14c372
Author: Huacai Chen <chenhc@lemote.com>
Date:   Fri Jul 13 15:37:57 2018 +0800

    MIPS: Change definition of cpu_relax() for Loongson-3
    
    commit a30718868915fbb991a9ae9e45594b059f28e9ae upstream.
    
    Linux expects that if a CPU modifies a memory location, then that
    modification will eventually become visible to other CPUs in the system.
    
    Loongson 3 CPUs include a Store Fill Buffer (SFB) which sits between a
    core & its L1 data cache, queueing memory accesses & allowing for faster
    forwarding of data from pending stores to younger loads from the core.
    Unfortunately the SFB prioritizes loads such that a continuous stream of
    loads may cause a pending write to be buffered indefinitely. This is
    problematic if we end up with 2 CPUs which each perform a store that the
    other polls for - one or both CPUs may end up with their stores buffered
    in the SFB, never reaching cache due to the continuous reads from the
    poll loop. Such a deadlock condition has been observed whilst running
    qspinlock code.
    
    This patch changes the definition of cpu_relax() to smp_mb() for
    Loongson-3, forcing a flush of the SFB on SMP systems which will cause
    any pending writes to make it as far as the L1 caches where they will
    become visible to other CPUs. If the kernel is not compiled for SMP
    support, this will expand to a barrier() as before.
    
    This workaround matches that currently implemented for ARM when
    CONFIG_ARM_ERRATA_754327=y, which was introduced by commit 534be1d5a2da
    ("ARM: 6194/1: change definition of cpu_relax() for ARM11MPCore").
    
    Although the workaround is only required when the Loongson 3 SFB
    functionality is enabled, and we only began explicitly enabling that
    functionality in v4.7 with commit 1e820da3c9af ("MIPS: Loongson-3:
    Introduce CONFIG_LOONGSON3_ENHANCEMENT"), existing or future firmware
    may enable the SFB which means we may need the workaround backported to
    earlier kernels too.
    
    [paul.burton@mips.com:
      - Reword commit message & comment.
      - Limit stable backport to v3.15+ where we support Loongson 3 CPUs.]
    
    Signed-off-by: Huacai Chen <chenhc@lemote.com>
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    References: 534be1d5a2da ("ARM: 6194/1: change definition of cpu_relax() for ARM11MPCore")
    References: 1e820da3c9af ("MIPS: Loongson-3: Introduce CONFIG_LOONGSON3_ENHANCEMENT")
    Patchwork: https://patchwork.linux-mips.org/patch/19830/
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: linux-mips@linux-mips.org
    Cc: Fuxin Zhang <zhangfx@lemote.com>
    Cc: Zhangjin Wu <wuzhangjin@gmail.com>
    Cc: Huacai Chen <chenhuacai@gmail.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 97b8ca659ab410c6955da052592959244d041fa8
Author: Daniel Wagner <daniel.wagner@siemens.com>
Date:   Wed Oct 31 09:14:58 2018 +0100

    x86/kconfig: Fall back to ticket spinlocks
    
    Sebastian writes:
    
    """
    We reproducibly observe cache line starvation on a Core2Duo E6850 (2
    cores), a i5-6400 SKL (4 cores) and on a NXP LS2044A ARM Cortex-A72 (4
    cores).
    
    The problem can be triggered with a v4.9-RT kernel by starting
    
        cyclictest -S -p98 -m  -i2000 -b 200
    
    and as "load"
    
        stress-ng --ptrace 4
    
    The reported maximal latency is usually less than 60us. If the problem
    triggers then values around 400us, 800us or even more are reported. The
    upperlimit is the -i parameter.
    
    Reproduction with 4.9-RT is almost immediate on Core2Duo, ARM64 and SKL,
    but it took 7.5 hours to trigger on v4.14-RT on the Core2Duo.
    
    Instrumentation show always the picture:
    
    CPU0                                         CPU1
    => do_syscall_64                              => do_syscall_64
    => SyS_ptrace                                   => syscall_slow_exit_work
    => ptrace_check_attach                          => ptrace_do_notify / rt_read_unlock
    => wait_task_inactive                              rt_spin_lock_slowunlock()
       -> while task_running()                         __rt_mutex_unlock_common()
      /   check_task_state()                           mark_wakeup_next_waiter()
     |     raw_spin_lock_irq(&p->pi_lock);             raw_spin_lock(&current->pi_lock);
     |     .                                               .
     |     raw_spin_unlock_irq(&p->pi_lock);               .
      \  cpu_relax()                                       .
       -                                                   .
        *IRQ*                                          <lock acquired>
    
    In the error case we observe that the while() loop is repeated more than
    5000 times which indicates that the pi_lock can be acquired. CPU1 on the
    other side does not make progress waiting for the same lock with interrupts
    disabled.
    
    This continues until an IRQ hits CPU0. Once CPU0 starts processing the IRQ
    the other CPU is able to acquire pi_lock and the situation relaxes.
    """
    
    This matches with the observeration for v4.4-rt on a Core2Duo E6850:
    
    CPU 0:
    
    - no progress for a very long time in rt_mutex_dequeue_pi):
    
    stress-n-1931    0d..11  5060.891219: function:             __try_to_take_rt_mutex
    stress-n-1931    0d..11  5060.891219: function:                rt_mutex_dequeue
    stress-n-1931    0d..21  5060.891220: function:                rt_mutex_enqueue_pi
    stress-n-1931    0....2  5060.891220: signal_generate:      sig=17 errno=0 code=262148 comm=stress-ng-ptrac pid=1928 grp=1 res=1
    stress-n-1931    0d..21  5060.894114: function:             rt_mutex_dequeue_pi
    stress-n-1931    0d.h11  5060.894115: local_timer_entry:    vector=239
    
    CPU 1:
    
    - IRQ at 5060.894114 on CPU 1 followed by the IRQ on CPU 0
    
    stress-n-1928    1....0  5060.891215: sys_enter:            NR 101 (18, 78b, 0, 0, 17, 788)
    stress-n-1928    1d..11  5060.891216: function:             __try_to_take_rt_mutex
    stress-n-1928    1d..21  5060.891216: function:                rt_mutex_enqueue_pi
    stress-n-1928    1d..21  5060.891217: function:             rt_mutex_dequeue_pi
    stress-n-1928    1....1  5060.891217: function:             rt_mutex_adjust_prio
    stress-n-1928    1d..11  5060.891218: function:                __rt_mutex_adjust_prio
    stress-n-1928    1d.h10  5060.894114: local_timer_entry:    vector=239
    
    Thomas writes:
    
    """
    This has nothing to do with RT. RT is merily exposing the
    problem in an observable way. The same issue happens with upstream, it's
    harder to trigger and it's harder to observe for obvious reasons.
    
    If you read through the discussions [see the links below] then you
    really see that there is an upstream issue with the x86 qrlock
    implementation and Peter has posted fixes which resolve it, both at
    the practical and the theoretical level.
    """
    
    Backporting all qspinlock related patches is very likely to introduce
    regressions on v4.4. Therefore, the recommended solution by Peter and
    Thomas is to drop back to ticket spinlocks for v4.4.
    
    Link :https://lkml.kernel.org/r/20180921120226.6xjgr4oiho22ex75@linutronix.de
    Link: https://lkml.kernel.org/r/20180926110117.405325143@infradead.org
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Daniel Wagner <daniel.wagner@siemens.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1acf93ca6c53144f5ffd408f1820ef5431656eb7
Merge: 0b002cdd5004 b987ffc18fb3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 11 16:18:10 2018 -0600

    Merge branch 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking build fix from Thomas Gleixner:
     "A single fix for a build fail with CONFIG_PROFILE_ALL_BRANCHES=y in
      the qspinlock code"
    
    * 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/qspinlock: Fix compile error

commit b987ffc18fb3b3b76b059aa9e372dbee26f7c4f2
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 2 14:26:53 2018 +0100

    x86/qspinlock: Fix compile error
    
    With a compiler that has asm-goto but not asm-cc-output and
    CONFIG_PROFILE_ALL_BRANCHES=y we get a compiler error:
    
      arch/x86/include/asm/rmwcc.h:23:17: error: jump into statement expression
    
    Fix this by writing the if() as a boolean multiplication instead.
    
    Reported-by: kbuild test robot <lkp@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-kernel@vger.kernel.org
    Fixes: 7aa54be29765 ("locking/qspinlock, x86: Provide liveness guarantee")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f682a7920baf7b721d01dd317f3b532265357cbb
Merge: 99792e0cea1e 3a025de64bf8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 17:54:58 2018 +0100

    Merge branch 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 paravirt updates from Ingo Molnar:
     "Two main changes:
    
       - Remove no longer used parts of the paravirt infrastructure and put
         large quantities of paravirt ops under a new config option
         PARAVIRT_XXL=y, which is selected by XEN_PV only. (Joergen Gross)
    
       - Enable PV spinlocks on Hyperv (Yi Sun)"
    
    * 'x86-paravirt-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/hyperv: Enable PV qspinlock for Hyper-V
      x86/hyperv: Add GUEST_IDLE_MSR support
      x86/paravirt: Clean up native_patch()
      x86/paravirt: Prevent redefinition of SAVE_FLAGS macro
      x86/xen: Make xen_reservation_lock static
      x86/paravirt: Remove unneeded mmu related paravirt ops bits
      x86/paravirt: Move the Xen-only pv_mmu_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move the pv_irq_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move the Xen-only pv_cpu_ops under the PARAVIRT_XXL umbrella
      x86/paravirt: Move items in pv_info under PARAVIRT_XXL umbrella
      x86/paravirt: Introduce new config option PARAVIRT_XXL
      x86/paravirt: Remove unused paravirt bits
      x86/paravirt: Use a single ops structure
      x86/paravirt: Remove clobbers from struct paravirt_patch_site
      x86/paravirt: Remove clobbers parameter from paravirt patch functions
      x86/paravirt: Make paravirt_patch_call() and paravirt_patch_jmp() static
      x86/xen: Add SPDX identifier in arch/x86/xen files
      x86/xen: Link platform-pci-unplug.o only if CONFIG_XEN_PVHVM
      x86/xen: Move pv specific parts of arch/x86/xen/mmu.c to mmu_pv.c
      x86/xen: Move pv irq related functions under CONFIG_XEN_PV umbrella

commit 0200fbdd431519d730b5d399a12840ec832b27cc
Merge: de3fbb2aa802 01a14bda11ad
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 13:08:53 2018 +0100

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking and misc x86 updates from Ingo Molnar:
     "Lots of changes in this cycle - in part because locking/core attracted
      a number of related x86 low level work which was easier to handle in a
      single tree:
    
       - Linux Kernel Memory Consistency Model updates (Alan Stern, Paul E.
         McKenney, Andrea Parri)
    
       - lockdep scalability improvements and micro-optimizations (Waiman
         Long)
    
       - rwsem improvements (Waiman Long)
    
       - spinlock micro-optimization (Matthew Wilcox)
    
       - qspinlocks: Provide a liveness guarantee (more fairness) on x86.
         (Peter Zijlstra)
    
       - Add support for relative references in jump tables on arm64, x86
         and s390 to optimize jump labels (Ard Biesheuvel, Heiko Carstens)
    
       - Be a lot less permissive on weird (kernel address) uaccess faults
         on x86: BUG() when uaccess helpers fault on kernel addresses (Jann
         Horn)
    
       - macrofy x86 asm statements to un-confuse the GCC inliner. (Nadav
         Amit)
    
       - ... and a handful of other smaller changes as well"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (57 commits)
      locking/lockdep: Make global debug_locks* variables read-mostly
      locking/lockdep: Fix debug_locks off performance problem
      locking/pvqspinlock: Extend node size when pvqspinlock is configured
      locking/qspinlock_stat: Count instances of nested lock slowpaths
      locking/qspinlock, x86: Provide liveness guarantee
      x86/asm: 'Simplify' GEN_*_RMWcc() macros
      locking/qspinlock: Rework some comments
      locking/qspinlock: Re-order code
      locking/lockdep: Remove duplicated 'lock_class_ops' percpu array
      x86/defconfig: Enable CONFIG_USB_XHCI_HCD=y
      futex: Replace spin_is_locked() with lockdep
      locking/lockdep: Make class->ops a percpu counter and move it under CONFIG_DEBUG_LOCKDEP=y
      x86/jump-labels: Macrofy inline assembly code to work around GCC inlining bugs
      x86/cpufeature: Macrofy inline assembly code to work around GCC inlining bugs
      x86/extable: Macrofy inline assembly code to work around GCC inlining bugs
      x86/paravirt: Work around GCC inlining bugs when compiling paravirt ops
      x86/bug: Macrofy the BUG table section handling, to work around GCC inlining bugs
      x86/alternatives: Macrofy lock prefixes to work around GCC inlining bugs
      x86/refcount: Work around GCC inlining bug
      x86/objtool: Use asm macros to work around GCC inlining bugs
      ...

commit 0fa809ca7f81c47bea6706bc689e941eb25d7e89
Author: Waiman Long <longman@redhat.com>
Date:   Tue Oct 16 09:45:07 2018 -0400

    locking/pvqspinlock: Extend node size when pvqspinlock is configured
    
    The qspinlock code supports up to 4 levels of slowpath nesting using
    four per-CPU mcs_spinlock structures. For 64-bit architectures, they
    fit nicely in one 64-byte cacheline.
    
    For para-virtualized (PV) qspinlocks it needs to store more information
    in the per-CPU node structure than there is space for. It uses a trick
    to use a second cacheline to hold the extra information that it needs.
    So PV qspinlock needs to access two extra cachelines for its information
    whereas the native qspinlock code only needs one extra cacheline.
    
    Freshly added counter profiling of the qspinlock code, however, revealed
    that it was very rare to use more than two levels of slowpath nesting.
    So it doesn't make sense to penalize PV qspinlock code in order to have
    four mcs_spinlock structures in the same cacheline to optimize for a case
    in the native qspinlock code that rarely happens.
    
    Extend the per-CPU node structure to have two more long words when PV
    qspinlock locks are configured to hold the extra data that it needs.
    
    As a result, the PV qspinlock code will enjoy the same benefit of using
    just one extra cacheline like the native counterpart, for most cases.
    
    [ mingo: Minor changelog edits. ]
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/1539697507-28084-2-git-send-email-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1222109a53637f96c581224198b86856d503f892
Author: Waiman Long <longman@redhat.com>
Date:   Tue Oct 16 09:45:06 2018 -0400

    locking/qspinlock_stat: Count instances of nested lock slowpaths
    
    Queued spinlock supports up to 4 levels of lock slowpath nesting -
    user context, soft IRQ, hard IRQ and NMI. However, we are not sure how
    often the nesting happens.
    
    So add 3 more per-CPU stat counters to track the number of instances where
    nesting index goes to 1, 2 and 3 respectively.
    
    On a dual-socket 64-core 128-thread Zen server, the following were the
    new stat counter values under different circumstances:
    
             State                         slowpath   index1   index2   index3
             -----                         --------   ------   ------   -------
      After bootup                         1,012,150    82       0        0
      After parallel build + perf-top    125,195,009    82       0        0
    
    So the chance of having more than 2 levels of nesting is extremely low.
    
    [ mingo: Minor changelog edits. ]
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/1539697507-28084-1-git-send-email-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 7aa54be2976550f17c11a1c3e3630002dea39303
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 26 13:01:20 2018 +0200

    locking/qspinlock, x86: Provide liveness guarantee
    
    On x86 we cannot do fetch_or() with a single instruction and thus end up
    using a cmpxchg loop, this reduces determinism. Replace the fetch_or()
    with a composite operation: tas-pending + load.
    
    Using two instructions of course opens a window we previously did not
    have. Consider the scenario:
    
            CPU0            CPU1            CPU2
    
     1)     lock
              trylock -> (0,0,1)
    
     2)                     lock
                              trylock /* fail */
    
     3)     unlock -> (0,0,0)
    
     4)                                     lock
                                              trylock -> (0,0,1)
    
     5)                       tas-pending -> (0,1,1)
                              load-val <- (0,1,0) from 3
    
     6)                       clear-pending-set-locked -> (0,0,1)
    
                              FAIL: _2_ owners
    
    where 5) is our new composite operation. When we consider each part of
    the qspinlock state as a separate variable (as we can when
    _Q_PENDING_BITS == 8) then the above is entirely possible, because
    tas-pending will only RmW the pending byte, so the later load is able
    to observe prior tail and lock state (but not earlier than its own
    trylock, which operates on the whole word, due to coherence).
    
    To avoid this we need 2 things:
    
     - the load must come after the tas-pending (obviously, otherwise it
       can trivially observe prior state).
    
     - the tas-pending must be a full word RmW instruction, it cannot be an XCHGB for
       example, such that we cannot observe other state prior to setting
       pending.
    
    On x86 we can realize this by using "LOCK BTS m32, r32" for
    tas-pending followed by a regular load.
    
    Note that observing later state is not a problem:
    
     - if we fail to observe a later unlock, we'll simply spin-wait for
       that store to become visible.
    
     - if we observe a later xchg_tail(), there is no difference from that
       xchg_tail() having taken place before the tas-pending.
    
    Suggested-by: Will Deacon <will.deacon@arm.com>
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: andrea.parri@amarulasolutions.com
    Cc: longman@redhat.com
    Fixes: 59fb586b4a07 ("locking/qspinlock: Remove unbounded cmpxchg() loop from locking slowpath")
    Link: https://lkml.kernel.org/r/20181003130957.183726335@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 756b1df4c2c82a1cdffeafa9d2aa76c92e7fb405
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 26 13:01:19 2018 +0200

    locking/qspinlock: Rework some comments
    
    While working my way through the code again; I felt the comments could
    use help.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: andrea.parri@amarulasolutions.com
    Cc: longman@redhat.com
    Link: https://lkml.kernel.org/r/20181003130257.156322446@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 53bf57fab7321fb42b703056a4c80fc9d986d170
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 26 13:01:18 2018 +0200

    locking/qspinlock: Re-order code
    
    Flip the branch condition after atomic_fetch_or_acquire(_Q_PENDING_VAL)
    such that we loose the indent. This also result in a more natural code
    flow IMO.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: andrea.parri@amarulasolutions.com
    Cc: longman@redhat.com
    Link: https://lkml.kernel.org/r/20181003130257.156322446@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 3a025de64bf89c84a79909069e3c24ad9e710d27
Author: Yi Sun <yi.y.sun@linux.intel.com>
Date:   Mon Oct 8 16:29:34 2018 +0800

    x86/hyperv: Enable PV qspinlock for Hyper-V
    
    Implement the required wait and kick callbacks to support PV spinlocks in
    Hyper-V guests.
    
    [ tglx: Document the requirement for disabling interrupts in the wait()
            callback. Remove goto and unnecessary includes. Add prototype
            for hv_vcpu_is_preempted(). Adapted to pending paravirt changes. ]
    
    Signed-off-by: Yi Sun <yi.y.sun@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Michael Kelley (EOSG) <Michael.H.Kelley@microsoft.com>
    Cc: chao.p.peng@intel.com
    Cc: chao.gao@intel.com
    Cc: isaku.yamahata@intel.com
    Cc: tianyu.lan@microsoft.com
    Link: https://lkml.kernel.org/r/1538987374-51217-3-git-send-email-yi.y.sun@linux.intel.com

commit 6e89e831a90172bc3d34ecbba52af5b9c4a447d1
Author: Alan Stern <stern@rowland.harvard.edu>
Date:   Wed Sep 26 11:29:17 2018 -0700

    tools/memory-model: Add extra ordering for locks and remove it for ordinary release/acquire
    
    More than one kernel developer has expressed the opinion that the LKMM
    should enforce ordering of writes by locking.  In other words, given
    the following code:
    
            WRITE_ONCE(x, 1);
            spin_unlock(&s):
            spin_lock(&s);
            WRITE_ONCE(y, 1);
    
    the stores to x and y should be propagated in order to all other CPUs,
    even though those other CPUs might not access the lock s.  In terms of
    the memory model, this means expanding the cumul-fence relation.
    
    Locks should also provide read-read (and read-write) ordering in a
    similar way.  Given:
    
            READ_ONCE(x);
            spin_unlock(&s);
            spin_lock(&s);
            READ_ONCE(y);           // or WRITE_ONCE(y, 1);
    
    the load of x should be executed before the load of (or store to) y.
    The LKMM already provides this ordering, but it provides it even in
    the case where the two accesses are separated by a release/acquire
    pair of fences rather than unlock/lock.  This would prevent
    architectures from using weakly ordered implementations of release and
    acquire, which seems like an unnecessary restriction.  The patch
    therefore removes the ordering requirement from the LKMM for that
    case.
    
    There are several arguments both for and against this change.  Let us
    refer to these enhanced ordering properties by saying that the LKMM
    would require locks to be RCtso (a bit of a misnomer, but analogous to
    RCpc and RCsc) and it would require ordinary acquire/release only to
    be RCpc.  (Note: In the following, the phrase "all supported
    architectures" is meant not to include RISC-V.  Although RISC-V is
    indeed supported by the kernel, the implementation is still somewhat
    in a state of flux and therefore statements about it would be
    premature.)
    
    Pros:
    
            The kernel already provides RCtso ordering for locks on all
            supported architectures, even though this is not stated
            explicitly anywhere.  Therefore the LKMM should formalize it.
    
            In theory, guaranteeing RCtso ordering would reduce the need
            for additional barrier-like constructs meant to increase the
            ordering strength of locks.
    
            Will Deacon and Peter Zijlstra are strongly in favor of
            formalizing the RCtso requirement.  Linus Torvalds and Will
            would like to go even further, requiring locks to have RCsc
            behavior (ordering preceding writes against later reads), but
            they recognize that this would incur a noticeable performance
            degradation on the POWER architecture.  Linus also points out
            that people have made the mistake, in the past, of assuming
            that locking has stronger ordering properties than is
            currently guaranteed, and this change would reduce the
            likelihood of such mistakes.
    
            Not requiring ordinary acquire/release to be any stronger than
            RCpc may prove advantageous for future architectures, allowing
            them to implement smp_load_acquire() and smp_store_release()
            with more efficient machine instructions than would be
            possible if the operations had to be RCtso.  Will and Linus
            approve this rationale, hypothetical though it is at the
            moment (it may end up affecting the RISC-V implementation).
            The same argument may or may not apply to RMW-acquire/release;
            see also the second Con entry below.
    
            Linus feels that locks should be easy for people to use
            without worrying about memory consistency issues, since they
            are so pervasive in the kernel, whereas acquire/release is
            much more of an "experts only" tool.  Requiring locks to be
            RCtso is a step in this direction.
    
    Cons:
    
            Andrea Parri and Luc Maranget think that locks should have the
            same ordering properties as ordinary acquire/release (indeed,
            Luc points out that the names "acquire" and "release" derive
            from the usage of locks).  Andrea points out that having
            different ordering properties for different forms of acquires
            and releases is not only unnecessary, it would also be
            confusing and unmaintainable.
    
            Locks are constructed from lower-level primitives, typically
            RMW-acquire (for locking) and ordinary release (for unlock).
            It is illogical to require stronger ordering properties from
            the high-level operations than from the low-level operations
            they comprise.  Thus, this change would make
    
                    while (cmpxchg_acquire(&s, 0, 1) != 0)
                            cpu_relax();
    
            an incorrect implementation of spin_lock(&s) as far as the
            LKMM is concerned.  In theory this weakness can be ameliorated
            by changing the LKMM even further, requiring
            RMW-acquire/release also to be RCtso (which it already is on
            all supported architectures).
    
            As far as I know, nobody has singled out any examples of code
            in the kernel that actually relies on locks being RCtso.
            (People mumble about RCU and the scheduler, but nobody has
            pointed to any actual code.  If there are any real cases,
            their number is likely quite small.)  If RCtso ordering is not
            needed, why require it?
    
            A handful of locking constructs (qspinlocks, qrwlocks, and
            mcs_spinlocks) are built on top of smp_cond_load_acquire()
            instead of an RMW-acquire instruction.  It currently provides
            only the ordinary acquire semantics, not the stronger ordering
            this patch would require of locks.  In theory this could be
            ameliorated by requiring smp_cond_load_acquire() in
            combination with ordinary release also to be RCtso (which is
            currently true on all supported architectures).
    
            On future weakly ordered architectures, people may be able to
            implement locks in a non-RCtso fashion with significant
            performance improvement.  Meeting the RCtso requirement would
            necessarily add run-time overhead.
    
    Overall, the technical aspects of these arguments seem relatively
    minor, and it appears mostly to boil down to a matter of opinion.
    Since the opinions of senior kernel maintainers such as Linus,
    Peter, and Will carry more weight than those of Luc and Andrea, this
    patch changes the model in accordance with the maintainers' wishes.
    
    Signed-off-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: akiyks@gmail.com
    Cc: boqun.feng@gmail.com
    Cc: dhowells@redhat.com
    Cc: j.alglave@ucl.ac.uk
    Cc: linux-arch@vger.kernel.org
    Cc: luc.maranget@inria.fr
    Cc: npiggin@gmail.com
    Cc: parri.andrea@gmail.com
    Link: http://lkml.kernel.org/r/20180926182920.27644-2-paulmck@linux.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 8f55e1f507d6e2436d876cf5c4f1a80b51a23432
Author: Huacai Chen <chenhc@lemote.com>
Date:   Fri Jul 13 15:37:57 2018 +0800

    MIPS: Change definition of cpu_relax() for Loongson-3
    
    commit a30718868915fbb991a9ae9e45594b059f28e9ae upstream.
    
    Linux expects that if a CPU modifies a memory location, then that
    modification will eventually become visible to other CPUs in the system.
    
    Loongson 3 CPUs include a Store Fill Buffer (SFB) which sits between a
    core & its L1 data cache, queueing memory accesses & allowing for faster
    forwarding of data from pending stores to younger loads from the core.
    Unfortunately the SFB prioritizes loads such that a continuous stream of
    loads may cause a pending write to be buffered indefinitely. This is
    problematic if we end up with 2 CPUs which each perform a store that the
    other polls for - one or both CPUs may end up with their stores buffered
    in the SFB, never reaching cache due to the continuous reads from the
    poll loop. Such a deadlock condition has been observed whilst running
    qspinlock code.
    
    This patch changes the definition of cpu_relax() to smp_mb() for
    Loongson-3, forcing a flush of the SFB on SMP systems which will cause
    any pending writes to make it as far as the L1 caches where they will
    become visible to other CPUs. If the kernel is not compiled for SMP
    support, this will expand to a barrier() as before.
    
    This workaround matches that currently implemented for ARM when
    CONFIG_ARM_ERRATA_754327=y, which was introduced by commit 534be1d5a2da
    ("ARM: 6194/1: change definition of cpu_relax() for ARM11MPCore").
    
    Although the workaround is only required when the Loongson 3 SFB
    functionality is enabled, and we only began explicitly enabling that
    functionality in v4.7 with commit 1e820da3c9af ("MIPS: Loongson-3:
    Introduce CONFIG_LOONGSON3_ENHANCEMENT"), existing or future firmware
    may enable the SFB which means we may need the workaround backported to
    earlier kernels too.
    
    [paul.burton@mips.com:
      - Reword commit message & comment.
      - Limit stable backport to v3.15+ where we support Loongson 3 CPUs.]
    
    Signed-off-by: Huacai Chen <chenhc@lemote.com>
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    References: 534be1d5a2da ("ARM: 6194/1: change definition of cpu_relax() for ARM11MPCore")
    References: 1e820da3c9af ("MIPS: Loongson-3: Introduce CONFIG_LOONGSON3_ENHANCEMENT")
    Patchwork: https://patchwork.linux-mips.org/patch/19830/
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: linux-mips@linux-mips.org
    Cc: Fuxin Zhang <zhangfx@lemote.com>
    Cc: Zhangjin Wu <wuzhangjin@gmail.com>
    Cc: Huacai Chen <chenhuacai@gmail.com>
    Cc: stable@vger.kernel.org # v3.15+
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1c40cd97ffe335101dfb738f7af138d49fb8b44f
Author: Huacai Chen <chenhc@lemote.com>
Date:   Fri Jul 13 15:37:57 2018 +0800

    MIPS: Change definition of cpu_relax() for Loongson-3
    
    commit a30718868915fbb991a9ae9e45594b059f28e9ae upstream.
    
    Linux expects that if a CPU modifies a memory location, then that
    modification will eventually become visible to other CPUs in the system.
    
    Loongson 3 CPUs include a Store Fill Buffer (SFB) which sits between a
    core & its L1 data cache, queueing memory accesses & allowing for faster
    forwarding of data from pending stores to younger loads from the core.
    Unfortunately the SFB prioritizes loads such that a continuous stream of
    loads may cause a pending write to be buffered indefinitely. This is
    problematic if we end up with 2 CPUs which each perform a store that the
    other polls for - one or both CPUs may end up with their stores buffered
    in the SFB, never reaching cache due to the continuous reads from the
    poll loop. Such a deadlock condition has been observed whilst running
    qspinlock code.
    
    This patch changes the definition of cpu_relax() to smp_mb() for
    Loongson-3, forcing a flush of the SFB on SMP systems which will cause
    any pending writes to make it as far as the L1 caches where they will
    become visible to other CPUs. If the kernel is not compiled for SMP
    support, this will expand to a barrier() as before.
    
    This workaround matches that currently implemented for ARM when
    CONFIG_ARM_ERRATA_754327=y, which was introduced by commit 534be1d5a2da
    ("ARM: 6194/1: change definition of cpu_relax() for ARM11MPCore").
    
    Although the workaround is only required when the Loongson 3 SFB
    functionality is enabled, and we only began explicitly enabling that
    functionality in v4.7 with commit 1e820da3c9af ("MIPS: Loongson-3:
    Introduce CONFIG_LOONGSON3_ENHANCEMENT"), existing or future firmware
    may enable the SFB which means we may need the workaround backported to
    earlier kernels too.
    
    [paul.burton@mips.com:
      - Reword commit message & comment.
      - Limit stable backport to v3.15+ where we support Loongson 3 CPUs.]
    
    Signed-off-by: Huacai Chen <chenhc@lemote.com>
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    References: 534be1d5a2da ("ARM: 6194/1: change definition of cpu_relax() for ARM11MPCore")
    References: 1e820da3c9af ("MIPS: Loongson-3: Introduce CONFIG_LOONGSON3_ENHANCEMENT")
    Patchwork: https://patchwork.linux-mips.org/patch/19830/
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: linux-mips@linux-mips.org
    Cc: Fuxin Zhang <zhangfx@lemote.com>
    Cc: Zhangjin Wu <wuzhangjin@gmail.com>
    Cc: Huacai Chen <chenhuacai@gmail.com>
    Cc: stable@vger.kernel.org # v3.15+
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 31130a16d459de809cd1c03eabc9567d094aae6a
Merge: 1202f4fdbcb6 3596924a233e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 16:54:22 2018 -0700

    Merge tag 'for-linus-4.19-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from Juergen Gross:
    
     - add dma-buf functionality to Xen grant table handling
    
     - fix for booting the kernel as Xen PVH dom0
    
     - fix for booting the kernel as a Xen PV guest with
       CONFIG_DEBUG_VIRTUAL enabled
    
     - other minor performance and style fixes
    
    * tag 'for-linus-4.19-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/balloon: fix balloon initialization for PVH Dom0
      xen: don't use privcmd_call() from xen_mc_flush()
      xen/pv: Call get_cpu_address_sizes to set x86_virt/phys_bits
      xen/biomerge: Use true and false for boolean values
      xen/gntdev: don't dereference a null gntdev_dmabuf on allocation failure
      xen/spinlock: Don't use pvqspinlock if only 1 vCPU
      xen/gntdev: Implement dma-buf import functionality
      xen/gntdev: Implement dma-buf export functionality
      xen/gntdev: Add initial support for dma-buf UAPI
      xen/gntdev: Make private routines/structures accessible
      xen/gntdev: Allow mappings for DMA buffers
      xen/grant-table: Allow allocating buffers suitable for DMA
      xen/balloon: Share common memory reservation routines
      xen/grant-table: Make set/clear page private code shared

commit 1202f4fdbcb6deeffd3eb39c94b8dc0cc8202b16
Merge: d0055f351e64 3c4d9137eefe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 16:39:13 2018 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "A bunch of good stuff in here. Worth noting is that we've pulled in
      the x86/mm branch from -tip so that we can make use of the core
      ioremap changes which allow us to put down huge mappings in the
      vmalloc area without screwing up the TLB. Much of the positive
      diffstat is because of the rseq selftest for arm64.
    
      Summary:
    
       - Wire up support for qspinlock, replacing our trusty ticket lock
         code
    
       - Add an IPI to flush_icache_range() to ensure that stale
         instructions fetched into the pipeline are discarded along with the
         I-cache lines
    
       - Support for the GCC "stackleak" plugin
    
       - Support for restartable sequences, plus an arm64 port for the
         selftest
    
       - Kexec/kdump support on systems booting with ACPI
    
       - Rewrite of our syscall entry code in C, which allows us to zero the
         GPRs on entry from userspace
    
       - Support for chained PMU counters, allowing 64-bit event counters to
         be constructed on current CPUs
    
       - Ensure scheduler topology information is kept up-to-date with CPU
         hotplug events
    
       - Re-enable support for huge vmalloc/IO mappings now that the core
         code has the correct hooks to use break-before-make sequences
    
       - Miscellaneous, non-critical fixes and cleanups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (90 commits)
      arm64: alternative: Use true and false for boolean values
      arm64: kexec: Add comment to explain use of __flush_icache_range()
      arm64: sdei: Mark sdei stack helper functions as static
      arm64, kaslr: export offset in VMCOREINFO ELF notes
      arm64: perf: Add cap_user_time aarch64
      efi/libstub: Only disable stackleak plugin for arm64
      arm64: drop unused kernel_neon_begin_partial() macro
      arm64: kexec: machine_kexec should call __flush_icache_range
      arm64: svc: Ensure hardirq tracing is updated before return
      arm64: mm: Export __sync_icache_dcache() for xen-privcmd
      drivers/perf: arm-ccn: Use devm_ioremap_resource() to map memory
      arm64: Add support for STACKLEAK gcc plugin
      arm64: Add stack information to on_accessible_stack
      drivers/perf: hisi: update the sccl_id/ccl_id when MT is supported
      arm64: fix ACPI dependencies
      rseq/selftests: Add support for arm64
      arm64: acpi: fix alignment fault in accessing ACPI
      efi/arm: map UEFI memory map even w/o runtime services enabled
      efi/arm: preserve early mapping of UEFI memory map longer for BGRT
      drivers: acpi: add dependency of EFI for arm64
      ...

commit 3553ae5690a84a5baae5baa329467b3df2d99f72
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jul 17 17:59:27 2018 -0400

    x86/kvm: Don't use pvqspinlock code if only 1 vCPU
    
    On a VM with only 1 vCPU, the locking fast path will always be
    successful. In this case, there is no need to use the the PV qspinlock
    code which has higher overhead on the unlock side than the native
    qspinlock code.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit fb20c03d3748fc6991dd58a3161c0d954485c2ce
Merge: d464b0314c79 c0dc373a780f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 30 11:37:16 2018 -0700

    Merge branch 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fixes from Ingo Molnar:
     "A paravirt UP-patching fix, and an I2C MUX driver lockdep warning fix"
    
    * 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/pvqspinlock/x86: Use LOCK_PREFIX in __pv_queued_spin_unlock() assembly code
      i2c/mux, locking/core: Annotate the nested rt_mutex usage
      locking/rtmutex: Allow specifying a subclass for nested locking

commit 47b428d14f06dbeab23dd5c7e424e15283841765
Author: Waiman Long <longman@redhat.com>
Date:   Thu Jul 19 17:39:57 2018 -0400

    xen/spinlock: Don't use pvqspinlock if only 1 vCPU
    
    On a VM with only 1 vCPU, the locking fast paths will always be
    successful. In this case, there is no need to use the the PV qspinlock
    code which has higher overhead on the unlock side than the native
    qspinlock code.
    
    The xen_pvspin veriable is also turned off in this 1 vCPU case to
    eliminate unneeded pvqspinlock initialization in xen_init_lock_cpu()
    which is run after xen_init_spinlocks().
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

commit c0dc373a780f4ec63e45a573b9551763abd8cd1a
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jul 17 16:16:00 2018 -0400

    locking/pvqspinlock/x86: Use LOCK_PREFIX in __pv_queued_spin_unlock() assembly code
    
    The LOCK_PREFIX macro should be used in the __raw_callee_save___pv_queued_spin_unlock()
    assembly code, so that the lock prefix can be patched out on UP systems.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Joe Mario <jmario@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/1531858560-21547-1-git-send-email-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a30718868915fbb991a9ae9e45594b059f28e9ae
Author: Huacai Chen <chenhc@lemote.com>
Date:   Fri Jul 13 15:37:57 2018 +0800

    MIPS: Change definition of cpu_relax() for Loongson-3
    
    Linux expects that if a CPU modifies a memory location, then that
    modification will eventually become visible to other CPUs in the system.
    
    Loongson 3 CPUs include a Store Fill Buffer (SFB) which sits between a
    core & its L1 data cache, queueing memory accesses & allowing for faster
    forwarding of data from pending stores to younger loads from the core.
    Unfortunately the SFB prioritizes loads such that a continuous stream of
    loads may cause a pending write to be buffered indefinitely. This is
    problematic if we end up with 2 CPUs which each perform a store that the
    other polls for - one or both CPUs may end up with their stores buffered
    in the SFB, never reaching cache due to the continuous reads from the
    poll loop. Such a deadlock condition has been observed whilst running
    qspinlock code.
    
    This patch changes the definition of cpu_relax() to smp_mb() for
    Loongson-3, forcing a flush of the SFB on SMP systems which will cause
    any pending writes to make it as far as the L1 caches where they will
    become visible to other CPUs. If the kernel is not compiled for SMP
    support, this will expand to a barrier() as before.
    
    This workaround matches that currently implemented for ARM when
    CONFIG_ARM_ERRATA_754327=y, which was introduced by commit 534be1d5a2da
    ("ARM: 6194/1: change definition of cpu_relax() for ARM11MPCore").
    
    Although the workaround is only required when the Loongson 3 SFB
    functionality is enabled, and we only began explicitly enabling that
    functionality in v4.7 with commit 1e820da3c9af ("MIPS: Loongson-3:
    Introduce CONFIG_LOONGSON3_ENHANCEMENT"), existing or future firmware
    may enable the SFB which means we may need the workaround backported to
    earlier kernels too.
    
    [paul.burton@mips.com:
      - Reword commit message & comment.
      - Limit stable backport to v3.15+ where we support Loongson 3 CPUs.]
    
    Signed-off-by: Huacai Chen <chenhc@lemote.com>
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    References: 534be1d5a2da ("ARM: 6194/1: change definition of cpu_relax() for ARM11MPCore")
    References: 1e820da3c9af ("MIPS: Loongson-3: Introduce CONFIG_LOONGSON3_ENHANCEMENT")
    Patchwork: https://patchwork.linux-mips.org/patch/19830/
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: linux-mips@linux-mips.org
    Cc: Fuxin Zhang <zhangfx@lemote.com>
    Cc: Zhangjin Wu <wuzhangjin@gmail.com>
    Cc: Huacai Chen <chenhuacai@gmail.com>
    Cc: stable@vger.kernel.org # v3.15+

commit c11090474d70590170cf5fa6afe85864ab494b37
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Mar 13 20:45:45 2018 +0000

    arm64: locking: Replace ticket lock implementation with qspinlock
    
    It's fair to say that our ticket lock has served us well over time, but
    it's time to bite the bullet and start using the generic qspinlock code
    so we can make use of explicit MCS queuing and potentially better PV
    performance in future.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 2da2ca24a38f0200111e3b8823c08d02cb59d362
Merge: a43de489934c 6cc65be4f6f2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jun 24 19:36:16 2018 +0800

    Merge branch 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fixes from Thomas Gleixner:
     "A set of fixes and updates for the locking code:
    
       - Prevent lockdep from updating irq state within its own code and
         thereby confusing itself.
    
       - Buid fix for older GCCs which mistreat anonymous unions
    
       - Add a missing lockdep annotation in down_read_non_onwer() which
         causes up_read_non_owner() to emit a lockdep splat
    
       - Remove the custom alpha dec_and_lock() implementation which is
         incorrect in terms of ordering and use the generic one.
    
      The remaining two commits are not strictly fixes. They provide irqsave
      variants of atomic_dec_and_lock() and refcount_dec_and_lock(). These
      are required to merge the relevant updates and cleanups into different
      maintainer trees for 4.19, so routing them into mainline without
      actual users is the sanest approach.
    
      They should have been in -rc1, but last weekend I took the liberty to
      just avoid computers in order to regain some mental sanity"
    
    * 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/qspinlock: Fix build for anonymous union in older GCC compilers
      locking/lockdep: Do not record IRQ state within lockdep code
      locking/rwsem: Fix up_read_non_owner() warning with DEBUG_RWSEMS
      locking/refcounts: Implement refcount_dec_and_lock_irqsave()
      atomic: Add irqsave variant of atomic_dec_and_lock()
      alpha: Remove custom dec_and_lock() implementation

commit 6cc65be4f6f2a7186af8f3e09900787c7912dad2
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jun 21 20:35:26 2018 -0400

    locking/qspinlock: Fix build for anonymous union in older GCC compilers
    
    One of my tests compiles the kernel with gcc 4.5.3, and I hit the
    following build error:
    
      include/linux/semaphore.h: In function 'sema_init':
      include/linux/semaphore.h:35:17: error: unknown field 'val' specified in initializer
      include/linux/semaphore.h:35:17: warning: missing braces around initializer
      include/linux/semaphore.h:35:17: warning: (near initialization for '(anonymous).raw_lock.<anonymous>.val')
    
    I bisected it down to:
    
     625e88be1f41 ("locking/qspinlock: Merge 'struct __qspinlock' into 'struct qspinlock'")
    
    ... which makes qspinlock have an anonymous union, which makes initializing it special
    for older compilers. By adding strategic brackets, it makes the build
    happy again.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Fixes: 625e88be1f41 ("locking/qspinlock: Merge 'struct __qspinlock' into 'struct qspinlock'")
    Link: http://lkml.kernel.org/r/20180621203526.172ab5c4@vmware.local.home
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 92400b8c8b42e53abb0fcb4ac75cb85d4177a891
Merge: 31a85cb35c82 1b22fc609cec
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 16:40:11 2018 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
    
     - Lots of tidying up changes all across the map for Linux's formal
       memory/locking-model tooling, by Alan Stern, Akira Yokosawa, Andrea
       Parri, Paul E. McKenney and SeongJae Park.
    
       Notable changes beyond an overall update in the tooling itself is the
       tidying up of spin_is_locked() semantics, which spills over into the
       kernel proper as well.
    
     - qspinlock improvements: the locking algorithm now guarantees forward
       progress whereas the previous implementation in mainline could starve
       threads indefinitely in cmpxchg() loops. Also other related cleanups
       to the qspinlock code (Will Deacon)
    
     - misc smaller improvements, cleanups and fixes all across the locking
       subsystem
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (51 commits)
      locking/rwsem: Simplify the is-owner-spinnable checks
      tools/memory-model: Add reference for 'Simplifying ARM concurrency'
      tools/memory-model: Update ASPLOS information
      MAINTAINERS, tools/memory-model: Update e-mail address for Andrea Parri
      tools/memory-model: Fix coding style in 'lock.cat'
      tools/memory-model: Remove out-of-date comments and code from lock.cat
      tools/memory-model: Improve mixed-access checking in lock.cat
      tools/memory-model: Improve comments in lock.cat
      tools/memory-model: Remove duplicated code from lock.cat
      tools/memory-model: Flag "cumulativity" and "propagation" tests
      tools/memory-model: Add model support for spin_is_locked()
      tools/memory-model: Add scripts to test memory model
      tools/memory-model: Fix coding style in 'linux-kernel.def'
      tools/memory-model: Model 'smp_store_mb()'
      tools/memory-order: Update the cheat-sheet to show that smp_mb__after_atomic() orders later RMW operations
      tools/memory-order: Improve key for SELF and SV
      tools/memory-model: Fix cheat sheet typo
      tools/memory-model: Update required version of herdtools7
      tools/memory-model: Redefine rb in terms of rcu-fence
      tools/memory-model: Rename link and rcu-path to rcu-link and rb
      ...

commit c8723ceed341db0e62570f5b996554bd60026af5
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 13 13:22:57 2018 +0000

    locking/qspinlock: Ensure node->count is updated before initialising node
    
    [ Upstream commit 11dc13224c975efcec96647a4768a6f1bb7a19a8 ]
    
    When queuing on the qspinlock, the count field for the current CPU's head
    node is incremented. This needn't be atomic because locking in e.g. IRQ
    context is balanced and so an IRQ will return with node->count as it
    found it.
    
    However, the compiler could in theory reorder the initialisation of
    node[idx] before the increment of the head node->count, causing an
    IRQ to overwrite the initialised node and potentially corrupt the lock
    state.
    
    Avoid the potential for this harmful compiler reordering by placing a
    barrier() between the increment of the head node->count and the subsequent
    node initialisation.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1518528177-19169-3-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit abd9138a1b0a987498296f93be174aab0d41b051
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 13 13:22:57 2018 +0000

    locking/qspinlock: Ensure node->count is updated before initialising node
    
    [ Upstream commit 11dc13224c975efcec96647a4768a6f1bb7a19a8 ]
    
    When queuing on the qspinlock, the count field for the current CPU's head
    node is incremented. This needn't be atomic because locking in e.g. IRQ
    context is balanced and so an IRQ will return with node->count as it
    found it.
    
    However, the compiler could in theory reorder the initialisation of
    node[idx] before the increment of the head node->count, causing an
    IRQ to overwrite the initialised node and potentially corrupt the lock
    state.
    
    Avoid the potential for this harmful compiler reordering by placing a
    barrier() between the increment of the head node->count and the subsequent
    node initialisation.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1518528177-19169-3-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3bea9adc96842b8a7345c7fb202c16ae9c8d5b25
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Apr 27 10:40:13 2018 +0100

    locking/qspinlock: Remove duplicate clear_pending() function from PV code
    
    The native clear_pending() function is identical to the PV version, so the
    latter can simply be removed.
    
    This fixes the build for systems with >= 16K CPUs using the PV lock implementation.
    
    Reported-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/20180427101619.GB21705@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit baa8c6ddf7be33f2b0ddeb68906d668caf646baa
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Apr 26 11:34:28 2018 +0100

    MAINTAINERS: Add myself as a co-maintainer for the locking subsystem
    
    I've been heavily involved with concurrency and memory ordering stuff
    (see ATOMIC INFRASTRUCTURE and LINUX KERNEL MEMORY CONSISTENCY MODEL)
    and with arm64 now using qrwlock with a view to using qspinlock in the
    near future, I'm going to continue being involved with the core locking
    primitives. Reflect this by adding myself as a co-maintainer alongside
    Ingo and Peter.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-15-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 81d3dc9a349b1e61d77106bbb05a6e6dd29b9d5e
Author: Waiman Long <longman@redhat.com>
Date:   Thu Apr 26 11:34:27 2018 +0100

    locking/qspinlock: Add stat tracking for pending vs. slowpath
    
    Currently, the qspinlock_stat code tracks only statistical counts in the
    PV qspinlock code. However, it may also be useful to track the number
    of locking operations done via the pending code vs. the MCS lock queue
    slowpath for the non-PV case.
    
    The qspinlock stat code is modified to do that. The stat counter
    pv_lock_slowpath is renamed to lock_slowpath so that it can be used by
    both the PV and non-PV cases.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Cc: will.deacon@arm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-14-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit ae75d9089ff7095d1d1a12c3cd86b21d3eaf3b15
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Apr 26 11:34:26 2018 +0100

    locking/qspinlock: Use try_cmpxchg() instead of cmpxchg() when locking
    
    When reaching the head of an uncontended queue on the qspinlock slow-path,
    using a try_cmpxchg() instead of a cmpxchg() operation to transition the
    lock work to _Q_LOCKED_VAL generates slightly better code for x86 and
    pretty much identical code for arm64.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-13-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9d4646d14d51d62b967a12452c30ea7edf8dd8fa
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Apr 26 11:34:25 2018 +0100

    locking/qspinlock: Elide back-to-back RELEASE operations with smp_wmb()
    
    The qspinlock slowpath must ensure that the MCS node is fully initialised
    before it can be reached by another other CPU. This is currently enforced
    by using a RELEASE operation when updating the tail and also when linking
    the node into the waitqueue, since the control dependency off xchg_tail
    is insufficient to enforce sufficient ordering, see:
    
      95bcade33a8a ("locking/qspinlock: Ensure node is initialised before updating prev->next")
    
    Back-to-back RELEASE operations may be expensive on some architectures,
    particularly those that implement them using fences under the hood. We
    can replace the two RELEASE operations with a single smp_wmb() fence and
    use RELAXED operations for the subsequent publishing of the node.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-12-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 626e5fbc14358901ddaa90ce510e0fbeab310432
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Apr 26 11:34:24 2018 +0100

    locking/qspinlock: Use smp_store_release() in queued_spin_unlock()
    
    A qspinlock can be unlocked simply by writing zero to the locked byte.
    This can be implemented in the generic code, so do that and remove the
    arch-specific override for x86 in the !PV case.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-11-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c131a198c497db436b558ac5e9a140cdcb91b304
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Apr 26 11:34:23 2018 +0100

    locking/qspinlock: Use smp_cond_load_relaxed() to wait for next node
    
    When a locker reaches the head of the queue and takes the lock, a
    concurrent locker may enqueue and force the lock holder to spin
    whilst its node->next field is initialised. Rather than open-code
    a READ_ONCE/cpu_relax() loop, this can be implemented using
    smp_cond_load_relaxed() instead.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-10-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 7f56b58a92aaf2cab049f32a19af7cc57a3972f2
Author: Jason Low <jason.low2@hp.com>
Date:   Thu Apr 26 11:34:22 2018 +0100

    locking/mcs: Use smp_cond_load_acquire() in MCS spin loop
    
    For qspinlocks on ARM64, we would like to use WFE instead
    of purely spinning. Qspinlocks internally have lock
    contenders spin on an MCS lock.
    
    Update arch_mcs_spin_lock_contended() such that it uses
    the new smp_cond_load_acquire() so that ARM64 can also
    override this spin loop with its own implementation using WFE.
    
    On x86, this can also be cheaper than spinning on
    smp_load_acquire().
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-9-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f9c811fac48cfbbfb452b08d1042386947868d07
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Apr 26 11:34:21 2018 +0100

    locking/qspinlock: Use atomic_cond_read_acquire()
    
    Rather than dig into the counter field of the atomic_t inside the
    qspinlock structure so that we can call smp_cond_load_acquire(), use
    atomic_cond_read_acquire() instead, which operates on the atomic_t
    directly.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-8-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c61da58d8a9ba9238250a548f00826eaf44af0f7
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Apr 26 11:34:20 2018 +0100

    locking/qspinlock: Kill cmpxchg() loop when claiming lock from head of queue
    
    When a queued locker reaches the head of the queue, it claims the lock
    by setting _Q_LOCKED_VAL in the lockword. If there isn't contention, it
    must also clear the tail as part of this operation so that subsequent
    lockers can avoid taking the slowpath altogether.
    
    Currently this is expressed as a cmpxchg() loop that practically only
    runs up to two iterations. This is confusing to the reader and unhelpful
    to the compiler. Rewrite the cmpxchg() loop without the loop, so that a
    failed cmpxchg() implies that there is contention and we just need to
    write to _Q_LOCKED_VAL without considering the rest of the lockword.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-7-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 59fb586b4a07b4e1a0ee577140ab4842ba451acd
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Apr 26 11:34:19 2018 +0100

    locking/qspinlock: Remove unbounded cmpxchg() loop from locking slowpath
    
    The qspinlock locking slowpath utilises a "pending" bit as a simple form
    of an embedded test-and-set lock that can avoid the overhead of explicit
    queuing in cases where the lock is held but uncontended. This bit is
    managed using a cmpxchg() loop which tries to transition the uncontended
    lock word from (0,0,0) -> (0,0,1) or (0,0,1) -> (0,1,1).
    
    Unfortunately, the cmpxchg() loop is unbounded and lockers can be starved
    indefinitely if the lock word is seen to oscillate between unlocked
    (0,0,0) and locked (0,0,1). This could happen if concurrent lockers are
    able to take the lock in the cmpxchg() loop without queuing and pass it
    around amongst themselves.
    
    This patch fixes the problem by unconditionally setting _Q_PENDING_VAL
    using atomic_fetch_or, and then inspecting the old value to see whether
    we need to spin on the current lock owner, or whether we now effectively
    hold the lock. The tricky scenario is when concurrent lockers end up
    queuing on the lock and the lock becomes available, causing us to see
    a lockword of (n,0,0). With pending now set, simply queuing could lead
    to deadlock as the head of the queue may not have observed the pending
    flag being cleared. Conversely, if the head of the queue did observe
    pending being cleared, then it could transition the lock from (n,0,0) ->
    (0,0,1) meaning that any attempt to "undo" our setting of the pending
    bit could race with a concurrent locker trying to set it.
    
    We handle this race by preserving the pending bit when taking the lock
    after reaching the head of the queue and leaving the tail entry intact
    if we saw pending set, because we know that the tail is going to be
    updated shortly.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-6-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b247be3fe89b6aba928bf80f4453d1c4ba8d2063
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Apr 26 11:34:18 2018 +0100

    locking/qspinlock/x86: Increase _Q_PENDING_LOOPS upper bound
    
    On x86, atomic_cond_read_relaxed will busy-wait with a cpu_relax() loop,
    so it is desirable to increase the number of times we spin on the qspinlock
    lockword when it is found to be transitioning from pending to locked.
    
    According to Waiman Long:
    
     | Ideally, the spinning times should be at least a few times the typical
     | cacheline load time from memory which I think can be down to 100ns or
     | so for each cacheline load with the newest systems or up to several
     | hundreds ns for older systems.
    
    which in his benchmarking corresponded to 512 iterations.
    
    Suggested-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-5-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6512276d97b160d90b53285bd06f7f201459a7e3
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Apr 26 11:34:17 2018 +0100

    locking/qspinlock: Bound spinning on pending->locked transition in slowpath
    
    If a locker taking the qspinlock slowpath reads a lock value indicating
    that only the pending bit is set, then it will spin whilst the
    concurrent pending->locked transition takes effect.
    
    Unfortunately, there is no guarantee that such a transition will ever be
    observed since concurrent lockers could continuously set pending and
    hand over the lock amongst themselves, leading to starvation. Whilst
    this would probably resolve in practice, it means that it is not
    possible to prove liveness properties about the lock and means that lock
    acquisition time is unbounded.
    
    Rather than removing the pending->locked spinning from the slowpath
    altogether (which has been shown to heavily penalise a 2-threaded
    locking stress test on x86), this patch replaces the explicit spinning
    with a call to atomic_cond_read_relaxed and allows the architecture to
    provide a bound on the number of spins. For architectures that can
    respond to changes in cacheline state in their smp_cond_load implementation,
    it should be sufficient to use the default bound of 1.
    
    Suggested-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: boqun.feng@gmail.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-4-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 625e88be1f41b53cec55827c984e4a89ea8ee9f9
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Apr 26 11:34:16 2018 +0100

    locking/qspinlock: Merge 'struct __qspinlock' into 'struct qspinlock'
    
    'struct __qspinlock' provides a handy union of fields so that
    subcomponents of the lockword can be accessed by name, without having to
    manage shifts and masks explicitly and take endianness into account.
    
    This is useful in qspinlock.h and also potentially in arch headers, so
    move the 'struct __qspinlock' into 'struct qspinlock' and kill the extra
    definition.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Acked-by: Boqun Feng <boqun.feng@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1524738868-31318-3-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c74e004c62739d6a6a002730d57fe851ff42f346
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 13 13:22:57 2018 +0000

    locking/qspinlock: Ensure node->count is updated before initialising node
    
    
    [ Upstream commit 11dc13224c975efcec96647a4768a6f1bb7a19a8 ]
    
    When queuing on the qspinlock, the count field for the current CPU's head
    node is incremented. This needn't be atomic because locking in e.g. IRQ
    context is balanced and so an IRQ will return with node->count as it
    found it.
    
    However, the compiler could in theory reorder the initialisation of
    node[idx] before the increment of the head node->count, causing an
    IRQ to overwrite the initialised node and potentially corrupt the lock
    state.
    
    Avoid the potential for this harmful compiler reordering by placing a
    barrier() between the increment of the head node->count and the subsequent
    node initialisation.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1518528177-19169-3-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 34226b6b70980a8f81fff3c09a2c889f77edeeff
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Sat Mar 24 21:17:24 2018 -0700

    KVM: X86: Fix setup the virt_spin_lock_key before static key get initialized
    
     static_key_disable_cpuslocked(): static key 'virt_spin_lock_key+0x0/0x20' used before call to jump_label_init()
     WARNING: CPU: 0 PID: 0 at kernel/jump_label.c:161 static_key_disable_cpuslocked+0x61/0x80
     RIP: 0010:static_key_disable_cpuslocked+0x61/0x80
     Call Trace:
      static_key_disable+0x16/0x20
      start_kernel+0x192/0x4b3
      secondary_startup_64+0xa5/0xb0
    
    Qspinlock will be choosed when dedicated pCPUs are available, however, the
    static virt_spin_lock_key is set in kvm_spinlock_init() before jump_label_init()
    has been called, which will result in a WARN(). This patch fixes it by delaying
    the virt_spin_lock_key setup to .smp_prepare_cpus().
    
    Reported-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Fixes: b2798ba0b876 ("KVM: X86: Choose qspinlock when dedicated physical CPUs are available")
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

commit b2798ba0b8769b42f00899b44a538b5fcecb480d
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Tue Feb 13 09:05:41 2018 +0800

    KVM: X86: Choose qspinlock when dedicated physical CPUs are available
    
    Waiman Long mentioned that:
    > Generally speaking, unfair lock performs well for VMs with a small
    > number of vCPUs. Native qspinlock may perform better than pvqspinlock
    > if there is vCPU pinning and there is no vCPU over-commitment.
    
    This patch uses the KVM_HINTS_DEDICATED performance hint, which is
    provided by the hypervisor admin, to choose the qspinlock algorithm
    when a dedicated physical CPU is available.
    
    PV_DEDICATED = 1, PV_UNHALT = anything: default is qspinlock
    PV_DEDICATED = 0, PV_UNHALT = 1: default is Hybrid PV queued/unfair lock
    PV_DEDICATED = 0, PV_UNHALT = 0: default is tas
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Eduardo Habkost <ehabkost@redhat.com>
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krčmář <rkrcmar@redhat.com>

commit e9e3b3002fc3c9ef665628bd85a8c1b5a3424f23
Merge: e525de3ab046 2dd6fd2e9997
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 15 09:05:26 2018 -0800

    Merge branch 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fixes from Ingo Molnar:
     "This contains two qspinlock fixes and three documentation and comment
      fixes"
    
    * 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/semaphore: Update the file path in documentation
      locking/atomic/bitops: Document and clarify ordering semantics for failed test_and_{}_bit()
      locking/qspinlock: Ensure node->count is updated before initialising node
      locking/qspinlock: Ensure node is initialised before updating prev->next
      Documentation/locking/mutex-design: Update to reflect latest changes

commit 11dc13224c975efcec96647a4768a6f1bb7a19a8
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 13 13:22:57 2018 +0000

    locking/qspinlock: Ensure node->count is updated before initialising node
    
    When queuing on the qspinlock, the count field for the current CPU's head
    node is incremented. This needn't be atomic because locking in e.g. IRQ
    context is balanced and so an IRQ will return with node->count as it
    found it.
    
    However, the compiler could in theory reorder the initialisation of
    node[idx] before the increment of the head node->count, causing an
    IRQ to overwrite the initialised node and potentially corrupt the lock
    state.
    
    Avoid the potential for this harmful compiler reordering by placing a
    barrier() between the increment of the head node->count and the subsequent
    node initialisation.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1518528177-19169-3-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 95bcade33a8af38755c9b0636e36a36ad3789fe6
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 13 13:22:56 2018 +0000

    locking/qspinlock: Ensure node is initialised before updating prev->next
    
    When a locker ends up queuing on the qspinlock locking slowpath, we
    initialise the relevant mcs node and publish it indirectly by updating
    the tail portion of the lock word using xchg_tail. If we find that there
    was a pre-existing locker in the queue, we subsequently update their
    ->next field to point at our node so that we are notified when it's our
    turn to take the lock.
    
    This can be roughly illustrated as follows:
    
      /* Initialise the fields in node and encode a pointer to node in tail */
      tail = initialise_node(node);
    
      /*
       * Exchange tail into the lockword using an atomic read-modify-write
       * operation with release semantics
       */
      old = xchg_tail(lock, tail);
    
      /* If there was a pre-existing waiter ... */
      if (old & _Q_TAIL_MASK) {
            prev = decode_tail(old);
            smp_read_barrier_depends();
    
            /* ... then update their ->next field to point to node.
            WRITE_ONCE(prev->next, node);
      }
    
    The conditional update of prev->next therefore relies on the address
    dependency from the result of xchg_tail ensuring order against the
    prior initialisation of node. However, since the release semantics of
    the xchg_tail operation apply only to the write portion of the RmW,
    then this ordering is not guaranteed and it is possible for the CPU
    to return old before the writes to node have been published, consequently
    allowing us to point prev->next to an uninitialised node.
    
    This patch fixes the problem by making the update of prev->next a RELEASE
    operation, which also removes the reliance on dependency ordering.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1518528177-19169-2-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 8e9a2dba8686187d8c8179e5b86640e653963889
Merge: 6098850e7e69 450cbdd0125c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 12:38:26 2017 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core locking updates from Ingo Molnar:
     "The main changes in this cycle are:
    
       - Another attempt at enabling cross-release lockdep dependency
         tracking (automatically part of CONFIG_PROVE_LOCKING=y), this time
         with better performance and fewer false positives. (Byungchul Park)
    
       - Introduce lockdep_assert_irqs_enabled()/disabled() and convert
         open-coded equivalents to lockdep variants. (Frederic Weisbecker)
    
       - Add down_read_killable() and use it in the VFS's iterate_dir()
         method. (Kirill Tkhai)
    
       - Convert remaining uses of ACCESS_ONCE() to
         READ_ONCE()/WRITE_ONCE(). Most of the conversion was Coccinelle
         driven. (Mark Rutland, Paul E. McKenney)
    
       - Get rid of lockless_dereference(), by strengthening Alpha atomics,
         strengthening READ_ONCE() with smp_read_barrier_depends() and thus
         being able to convert users of lockless_dereference() to
         READ_ONCE(). (Will Deacon)
    
       - Various micro-optimizations:
    
            - better PV qspinlocks (Waiman Long),
            - better x86 barriers (Michael S. Tsirkin)
            - better x86 refcounts (Kees Cook)
    
       - ... plus other fixes and enhancements. (Borislav Petkov, Juergen
         Gross, Miguel Bernal Marin)"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (70 commits)
      locking/x86: Use LOCK ADD for smp_mb() instead of MFENCE
      rcu: Use lockdep to assert IRQs are disabled/enabled
      netpoll: Use lockdep to assert IRQs are disabled/enabled
      timers/posix-cpu-timers: Use lockdep to assert IRQs are disabled/enabled
      sched/clock, sched/cputime: Use lockdep to assert IRQs are disabled/enabled
      irq_work: Use lockdep to assert IRQs are disabled/enabled
      irq/timings: Use lockdep to assert IRQs are disabled/enabled
      perf/core: Use lockdep to assert IRQs are disabled/enabled
      x86: Use lockdep to assert IRQs are disabled/enabled
      smp/core: Use lockdep to assert IRQs are disabled/enabled
      timers/hrtimer: Use lockdep to assert IRQs are disabled/enabled
      timers/nohz: Use lockdep to assert IRQs are disabled/enabled
      workqueue: Use lockdep to assert IRQs are disabled/enabled
      irq/softirqs: Use lockdep to assert IRQs are disabled/enabled
      locking/lockdep: Add IRQs disabled/enabled assertion APIs: lockdep_assert_irqs_enabled()/disabled()
      locking/pvqspinlock: Implement hybrid PV queued/unfair locks
      locking/rwlocks: Fix comments
      x86/paravirt: Set up the virt_spin_lock_key after static keys get initialized
      block, locking/lockdep: Assign a lock_class per gendisk used for wait_for_completion()
      workqueue: Remove now redundant lock acquisitions wrt. workqueue flushes
      ...

commit f3573b8f902c507c721999cc669fbb7e045081b8
Merge: 9e09d05cfe7d 610f01b9a88a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 12:12:00 2017 -0800

    Merge tag 'for-linus' of git://github.com/openrisc/linux
    
    Pull OpenRISC updates from Stafford Horne:
     "The OpenRISC work is a bit more interesting this time, adding SMP
      support and a few general cleanups.
    
      Small Things:
    
       - Move OpenRISC docs into Documentation and clean them up
    
       - Document previously undocumented devicetree bindings
    
       - Update the or1ksim dts to use stdout-path
    
      OpenRISC SMP support details:
    
       - First the "use shadow registers" and "define CPU_BIG_ENDIAN as
         true" get the architecture ready for SMP.
    
       - The "add 1 and 2 byte cmpxchg support" and "use qspinlocks and
         qrwlocks" add the SMP locking infrastructure as needed. Using the
         qspinlocks and qrwlocks as suggested by Peter Z while reviewing the
         original spinlocks implementation.
    
       - The "support for ompic" adds a new irqchip device which is used for
         IPI communication to support SMP.
    
       - The "initial SMP support" adds smp.c and makes changes to all of
         the necessary data-structures to be per-cpu.
    
      The remaining patches are bug fixes and debug helpers which I wanted
      to keep separate from the "initial SMP support" in order to allow them
      to be reviewed on their own. This includes:
    
       - add cacheflush support to fix icache aliasing
    
       - fix initial preempt state for secondary cpu tasks
    
       - sleep instead of spin on secondary wait
    
       - support framepointers and STACKTRACE_SUPPORT
    
       - enable LOCKDEP_SUPPORT and irqflags tracing
    
       - timer sync: Add tick timer sync logic
    
       - fix possible deadlock in timer sync, pointed out by mips guys
    
      Note: the irqchip patch was reviewed with Marc and we agreed to push
      it together with these patches"
    
    * tag 'for-linus' of git://github.com/openrisc/linux:
      openrisc: fix possible deadlock scenario during timer sync
      openrisc: pass endianness info to sparse
      openrisc: add tick timer multi-core sync logic
      openrisc: enable LOCKDEP_SUPPORT and irqflags tracing
      openrisc: support framepointers and STACKTRACE_SUPPORT
      openrisc: add simple_smp dts and defconfig for simulators
      openrisc: add cacheflush support to fix icache aliasing
      openrisc: sleep instead of spin on secondary wait
      openrisc: fix initial preempt state for secondary cpu tasks
      openrisc: initial SMP support
      irqchip: add initial support for ompic
      dt-bindings: add openrisc to vendor prefixes list
      openrisc: use qspinlocks and qrwlocks
      openrisc: add 1 and 2 byte cmpxchg support
      openrisc: use shadow registers to save regs on exception
      dt-bindings: openrisc: Add OpenRISC platform SoC
      Documentation: openrisc: Updates to README
      Documentation: Move OpenRISC docs out of arch/
      MAINTAINERS: Add OpenRISC pic maintainer
      openrisc: dts: or1ksim: Add stdout-path

commit 11752adb68a388724b1935d57bf543897c34d80b
Author: Waiman Long <longman@redhat.com>
Date:   Tue Nov 7 16:18:06 2017 -0500

    locking/pvqspinlock: Implement hybrid PV queued/unfair locks
    
    Currently, all the lock waiters entering the slowpath will do one
    lock stealing attempt to acquire the lock. That helps performance,
    especially in VMs with over-committed vCPUs. However, the current
    pvqspinlocks still don't perform as good as unfair locks in many cases.
    On the other hands, unfair locks do have the problem of lock starvation
    that pvqspinlocks don't have.
    
    This patch combines the best attributes of an unfair lock and a
    pvqspinlock into a hybrid lock with 2 modes - queued mode & unfair
    mode. A lock waiter goes into the unfair mode when there are waiters
    in the wait queue but the pending bit isn't set. Otherwise, it will
    go into the queued mode waiting in the queue for its turn.
    
    On a 2-socket 36-core E5-2699 v3 system (HT off), a kernel build
    (make -j<n>) was done in a VM with unpinned vCPUs 3 times with the
    best time selected and <n> is the number of vCPUs available. The build
    times of the original pvqspinlock, hybrid pvqspinlock and unfair lock
    with various number of vCPUs are as follows:
    
      vCPUs    pvqlock     hybrid pvqlock    unfair lock
      -----    -------     --------------    -----------
        30      342.1s         329.1s          329.1s
        36      314.1s         305.3s          307.3s
        45      345.0s         302.1s          306.6s
        54      365.4s         308.6s          307.8s
        72      358.9s         293.6s          303.9s
       108      343.0s         285.9s          304.2s
    
    The hybrid pvqspinlock performs better or comparable to the unfair
    lock.
    
    By turning on QUEUED_LOCK_STAT, the table below showed the number
    of lock acquisitions in unfair mode and queue mode after a kernel
    build with various number of vCPUs.
    
      vCPUs    queued mode  unfair mode
      -----    -----------  -----------
        30      9,130,518      294,954
        36     10,856,614      386,809
        45      8,467,264   11,475,373
        54      6,409,987   19,670,855
        72      4,782,063   25,712,180
    
    It can be seen that as the VM became more and more over-committed,
    the ratio of locks acquired in unfair mode increases. This is all
    done automatically to get the best overall performance as possible.
    
    Using a kernel locking microbenchmark with number of locking
    threads equals to the number of vCPUs available on the same machine,
    the minimum, average and maximum (min/avg/max) numbers of locking
    operations done per thread in a 5-second testing interval are shown
    below:
    
      vCPUs         hybrid pvqlock             unfair lock
      -----         --------------             -----------
        36     822,135/881,063/950,363    75,570/313,496/  690,465
        54     542,435/581,664/625,937    35,460/204,280/  457,172
        72     397,500/428,177/499,299    17,933/150,679/  708,001
       108     257,898/288,150/340,871     3,085/181,176/1,257,109
    
    It can be seen that the hybrid pvqspinlocks are more fair and
    performant than the unfair locks in this test.
    
    The table below shows the kernel build times on a smaller 2-socket
    16-core 32-thread E5-2620 v4 system.
    
      vCPUs    pvqlock     hybrid pvqlock    unfair lock
      -----    -------     --------------    -----------
        16      436.8s         433.4s          435.6s
        36      366.2s         364.8s          364.5s
        48      423.6s         376.3s          370.2s
        64      433.1s         376.6s          376.8s
    
    Again, the performance of the hybrid pvqspinlock was comparable to
    that of the unfair lock.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Eduardo Valentin <eduval@amazon.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1510089486-3466-1-git-send-email-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b5f8217615bb197af33c0cc71b14783b194fed6e
Author: Stafford Horne <shorne@gmail.com>
Date:   Fri Mar 24 07:13:03 2017 +0900

    openrisc: use qspinlocks and qrwlocks
    
    Enable OpenRISC to use qspinlocks and qrwlocks for upcoming SMP support.
    
    Signed-off-by: Stafford Horne <shorne@gmail.com>

commit e6fd28eb3522b31f79a938e24674f77268a120fd
Author: Juergen Gross <jgross@suse.com>
Date:   Wed Sep 6 19:36:25 2017 +0200

    locking/spinlocks, paravirt, xen: Correct the xen_nopvspin case
    
    With the boot parameter "xen_nopvspin" specified a Xen guest should not
    make use of paravirt spinlocks, but behave as if running on bare
    metal. This is not true, however, as the qspinlock code will fall back
    to a test-and-set scheme when it is detecting a hypervisor.
    
    In order to avoid this disable the virt_spin_lock_key.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akataria@vmware.com
    Cc: boris.ostrovsky@oracle.com
    Cc: chrisw@sous-sol.org
    Cc: hpa@zytor.com
    Cc: jeremy@goop.org
    Cc: rusty@rustcorp.com.au
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/20170906173625.18158-3-jgross@suse.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9043442b43b1fddf202591b84702863286700c1a
Author: Juergen Gross <jgross@suse.com>
Date:   Wed Sep 6 19:36:24 2017 +0200

    locking/paravirt: Use new static key for controlling call of virt_spin_lock()
    
    There are cases where a guest tries to switch spinlocks to bare metal
    behavior (e.g. by setting "xen_nopvspin" boot parameter). Today this
    has the downside of falling back to unfair test and set scheme for
    qspinlocks due to virt_spin_lock() detecting the virtualized
    environment.
    
    Add a static key controlling whether virt_spin_lock() should be
    called or not. When running on bare metal set the new key to false.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akataria@vmware.com
    Cc: boris.ostrovsky@oracle.com
    Cc: chrisw@sous-sol.org
    Cc: hpa@zytor.com
    Cc: jeremy@goop.org
    Cc: rusty@rustcorp.com.au
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/20170906173625.18158-2-jgross@suse.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b96f7d881ad94203e997cd2aa7112d4a06d121ef
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri Mar 24 17:25:02 2017 +0100

    s390/spinlock: introduce spinlock wait queueing
    
    The queued spinlock code for s390 follows the principles of the common
    code qspinlock implementation but with a few notable differences.
    
    The format of the spinlock_t locking word differs, s390 needs to store
    the logical CPU number of the lock holder in the spinlock_t to be able
    to use the diagnose 9c directed yield hypervisor call.
    
    The inline code sequences for spin_lock and spin_unlock are nice and
    short. The inline portion of a spin_lock now typically looks like this:
    
            lhi     %r0,0                   # 0 indicates an empty lock
            l       %r1,0x3a0               # CPU number + 1 from lowcore
            cs      %r0,%r1,<some_lock>     # lock operation
            jnz     call_wait               # on failure call wait function
    locked:
            ...
    call_wait:
            la      %r2,<some_lock>
            brasl   %r14,arch_spin_lock_wait
            j       locked
    
    A spin_unlock is as simple as before:
    
            lhi     %r0,0
            sth     %r0,2(%r2)              # unlock operation
    
    After a CPU has queued itself it may not enable interrupts again for the
    arch_spin_lock_flags() variant. The arch_spin_lock_wait_flags wait function
    is removed.
    
    To improve performance the code implements opportunistic lock stealing.
    If the wait function finds a spinlock_t that indicates that the lock is
    free but there are queued waiters, the CPU may steal the lock up to three
    times without queueing itself. The lock stealing update the steal counter
    in the lock word to prevent more than 3 steals. The counter is reset at
    the time the CPU next in the queue successfully takes the lock.
    
    While the queued spinlocks improve performance in a system with dedicated
    CPUs, in a virtualized environment with continuously overcommitted CPUs
    the queued spinlocks can have a negative effect on performance. This
    is due to the fact that a queued CPU that is preempted by the hypervisor
    will block the queue at some point even without holding the lock. With
    the classic spinlock it does not matter if a CPU is preempted that waits
    for the lock. Therefore use the queued spinlock code only if the system
    runs with dedicated CPUs and fall back to classic spinlocks when running
    with shared CPUs.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit 5f82e71a001d14824a7728ad9e49f6aea420f161
Merge: 6c51e67b64d1 edc2988c548d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 11:52:29 2017 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
    
     - Add 'cross-release' support to lockdep, which allows APIs like
       completions, where it's not the 'owner' who releases the lock, to be
       tracked. It's all activated automatically under
       CONFIG_PROVE_LOCKING=y.
    
     - Clean up (restructure) the x86 atomics op implementation to be more
       readable, in preparation of KASAN annotations. (Dmitry Vyukov)
    
     - Fix static keys (Paolo Bonzini)
    
     - Add killable versions of down_read() et al (Kirill Tkhai)
    
     - Rework and fix jump_label locking (Marc Zyngier, Paolo Bonzini)
    
     - Rework (and fix) tlb_flush_pending() barriers (Peter Zijlstra)
    
     - Remove smp_mb__before_spinlock() and convert its usages, introduce
       smp_mb__after_spinlock() (Peter Zijlstra)
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (56 commits)
      locking/lockdep/selftests: Fix mixed read-write ABBA tests
      sched/completion: Avoid unnecessary stack allocation for COMPLETION_INITIALIZER_ONSTACK()
      acpi/nfit: Fix COMPLETION_INITIALIZER_ONSTACK() abuse
      locking/pvqspinlock: Relax cmpxchg's to improve performance on some architectures
      smp: Avoid using two cache lines for struct call_single_data
      locking/lockdep: Untangle xhlock history save/restore from task independence
      locking/refcounts, x86/asm: Disable CONFIG_ARCH_HAS_REFCOUNT for the time being
      futex: Remove duplicated code and fix undefined behaviour
      Documentation/locking/atomic: Finish the document...
      locking/lockdep: Fix workqueue crossrelease annotation
      workqueue/lockdep: 'Fix' flush_work() annotation
      locking/lockdep/selftests: Add mixed read-write ABBA tests
      mm, locking/barriers: Clarify tlb_flush_pending() barriers
      locking/lockdep: Make CONFIG_LOCKDEP_CROSSRELEASE and CONFIG_LOCKDEP_COMPLETIONS truly non-interactive
      locking/lockdep: Explicitly initialize wq_barrier::done::map
      locking/lockdep: Rename CONFIG_LOCKDEP_COMPLETE to CONFIG_LOCKDEP_COMPLETIONS
      locking/lockdep: Reword title of LOCKDEP_CROSSRELEASE config
      locking/lockdep: Make CONFIG_LOCKDEP_CROSSRELEASE part of CONFIG_PROVE_LOCKING
      locking/refcounts, x86/asm: Implement fast refcount overflow protection
      locking/lockdep: Fix the rollback and overwrite detection logic in crossrelease
      ...

commit 34d54f3d6917f519693dbe873ee59cd06fb515ed
Author: Waiman Long <longman@redhat.com>
Date:   Mon Aug 14 16:07:02 2017 -0400

    locking/pvqspinlock: Relax cmpxchg's to improve performance on some architectures
    
    All the locking related cmpxchg's in the following functions are
    replaced with the _acquire variants:
    
     - pv_queued_spin_steal_lock()
     - trylock_clear_pending()
    
    This change should help performance on architectures that use LL/SC.
    
    The cmpxchg in pv_kick_node() is replaced with a relaxed version
    with explicit memory barrier to make sure that it is fully ordered
    in the writing of next->lock and the reading of pn->state whether
    the cmpxchg is a success or failure without affecting performance in
    non-LL/SC architectures.
    
    On a 2-socket 12-core 96-thread Power8 system with pvqspinlock
    explicitly enabled, the performance of a locking microbenchmark
    with and without this patch on a 4.13-rc4 kernel with Xinhui's PPC
    qspinlock patch were as follows:
    
      # of thread     w/o patch    with patch      % Change
      -----------     ---------    ----------      --------
           8         5054.8 Mop/s  5209.4 Mop/s     +3.1%
          16         3985.0 Mop/s  4015.0 Mop/s     +0.8%
          32         2378.2 Mop/s  2396.0 Mop/s     +0.7%
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrea Parri <parri.andrea@gmail.com>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pan Xinhui <xinhui@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/1502741222-24360-1-git-send-email-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 568d135d337d3114688fef9fdbce7fb6dbbd04c7
Merge: 4ecd4ff55ac5 d40e0d4fb561
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 15 10:59:54 2017 -0700

    Merge branch 'upstream' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus
    
    Pull MIPS updates from Ralf Baechle:
     "Boston platform support:
       - Document DT bindings
       - Add CLK driver for board clocks
    
      CM:
       - Avoid per-core locking with CM3 & higher
       - WARN on attempt to lock invalid VP, not BUG
    
      CPS:
       - Select CONFIG_SYS_SUPPORTS_SCHED_SMT for MIPSr6
       - Prevent multi-core with dcache aliasing
       - Handle cores not powering down more gracefully
       - Handle spurious VP starts more gracefully
    
      DSP:
       - Add lwx & lhx missaligned access support
    
      eBPF:
       - Add MIPS support along with many supporting change to add the
         required infrastructure
    
      Generic arch code:
       - Misc sysmips MIPS_ATOMIC_SET fixes
       - Drop duplicate HAVE_SYSCALL_TRACEPOINTS
       - Negate error syscall return in trace
       - Correct forced syscall errors
       - Traced negative syscalls should return -ENOSYS
       - Allow samples/bpf/tracex5 to access syscall arguments for sane
         traces
       - Cleanup from old Kconfig options in defconfigs
       - Fix PREF instruction usage by memcpy for MIPS R6
       - Fix various special cases in the FPU eulation
       - Fix some special cases in MIPS16e2 support
       - Fix MIPS I ISA /proc/cpuinfo reporting
       - Sort MIPS Kconfig alphabetically
       - Fix minimum alignment requirement of IRQ stack as required by
         ABI / GCC
       - Fix special cases in the module loader
       - Perform post-DMA cache flushes on systems with MAARs
       - Probe the I6500 CPU
       - Cleanup cmpxchg and add support for 1 and 2 byte operations
       - Use queued read/write locks (qrwlock)
       - Use queued spinlocks (qspinlock)
       - Add CPU shared FTLB feature detection
       - Handle tlbex-tlbp race condition
       - Allow storing pgd in C0_CONTEXT for MIPSr6
       - Use current_cpu_type() in m4kc_tlbp_war()
       - Support Boston in the generic kernel
    
      Generic platform:
       - yamon-dt: Pull YAMON DT shim code out of SEAD-3 board
       - yamon-dt: Support > 256MB of RAM
       - yamon-dt: Use serial* rather than uart* aliases
       - Abstract FDT fixup application
       - Set RTC_ALWAYS_BCD to 0
       - Add a MAINTAINERS entry
    
      core kernel:
       - qspinlock.c: include linux/prefetch.h
    
      Loongson 3:
       - Add support
    
      Perf:
       - Add I6500 support
    
      SEAD-3:
       - Remove GIC timer from DT
       - Set interrupt-parent per-device, not at root node
       - Fix GIC interrupt specifiers
    
      SMP:
       - Skip IPI setup if we only have a single CPU
    
      VDSO:
       - Make comment match reality
       - Improvements to time code in VDSO"
    
    * 'upstream' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus: (86 commits)
      locking/qspinlock: Include linux/prefetch.h
      MIPS: Fix MIPS I ISA /proc/cpuinfo reporting
      MIPS: Fix minimum alignment requirement of IRQ stack
      MIPS: generic: Support MIPS Boston development boards
      MIPS: DTS: img: Don't attempt to build-in all .dtb files
      clk: boston: Add a driver for MIPS Boston board clocks
      dt-bindings: Document img,boston-clock binding
      MIPS: Traced negative syscalls should return -ENOSYS
      MIPS: Correct forced syscall errors
      MIPS: Negate error syscall return in trace
      MIPS: Drop duplicate HAVE_SYSCALL_TRACEPOINTS select
      MIPS16e2: Provide feature overrides for non-MIPS16 systems
      MIPS: MIPS16e2: Report ASE presence in /proc/cpuinfo
      MIPS: MIPS16e2: Subdecode extended LWSP/SWSP instructions
      MIPS: MIPS16e2: Identify ASE presence
      MIPS: VDSO: Fix a mismatch between comment and preprocessor constant
      MIPS: VDSO: Add implementation of gettimeofday() fallback
      MIPS: VDSO: Add implementation of clock_gettime() fallback
      MIPS: VDSO: Fix conversions in do_monotonic()/do_monotonic_coarse()
      MIPS: Use current_cpu_type() in m4kc_tlbp_war()
      ...

commit d40e0d4fb5613099a58c95a9403f51b03e40e861
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Wed Jul 5 09:59:05 2017 -0700

    locking/qspinlock: Include linux/prefetch.h
    
    kernel/locking/qspinlock.c makes use of prefetchw() but hasn't included
    linux/prefetch.h up until now, instead relying upon indirect inclusion
    of some header that defines prefetchw().
    
    In the case of MIPS we generally obtain a definition of prefetchw() from
    asm/processor.h, included by way of linux/mutex.h, but only for
    configurations which select CONFIG_CPU_HAS_PREFETCH. For configurations
    which don't this leaves prefetchw() undefined leading to the following
    build failure:
    
      CC      kernel/locking/qspinlock.o
      kernel/locking/qspinlock.c: In function ‘queued_spin_lock_slowpath’:
      kernel/locking/qspinlock.c:549:4: error: implicit declaration of
          function ‘prefetchw’ [-Werror=implicit-function-declaration]
        prefetchw(next);
        ^~~~~~~~~
    
    Fix this by including linux/prefetch.h in order to ensure that we get a
    definition of prefetchw().
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit c8b2ba83fb01336f094226895087b644df0ec397
Merge: 7cb328c30a71 5671360f29c6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 9 10:47:50 2017 -0700

    Merge branch 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fixes from Thomas Gleixner:
    
     - Fix the EINTR logic in rwsem-spinlock to avoid double locking by a
       writer and a reader
    
     - Add a missing include to qspinlocks
    
    * 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/qspinlock: Explicitly include asm/prefetch.h
      locking/rwsem-spinlock: Fix EINTR branch in __down_write_common()

commit 5671360f29c68d9079914438f6a0109ef62f82a8
Author: Stafford Horne <shorne@gmail.com>
Date:   Sat Jul 8 04:56:58 2017 +0900

    locking/qspinlock: Explicitly include asm/prefetch.h
    
    In architectures that use qspinlock, like x86, prefetch is loaded
    indirectly via the asm/qspinlock.h include.  On other architectures, like
    OpenRISC, which may want to use asm-generic/qspinlock.h the built will
    fail without the asm/prefetch.h include.
    
    Fix this by including directly.
    
    Signed-off-by: Stafford Horne <shorne@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170707195658.23840-1-shorne@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 0b17c9670590148656645be57f62f279f0d3ad52
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Jun 9 17:26:43 2017 -0700

    MIPS: Use queued spinlocks (qspinlock)
    
    This patch switches MIPS to make use of generically implemented queued
    spinlocks, rather than the ticket spinlocks used previously. This allows
    us to drop a whole load of inline assembly, share more generic code, and
    is also a performance win.
    
    Results from running the AIM7 short workload on a MIPS Creator Ci40 (ie.
    2 core 2 thread interAptiv CPU clocked at 546MHz) with v4.12-rc4
    pistachio_defconfig, with ftrace disabled due to a current bug, and both
    with & without use of queued rwlocks & spinlocks:
    
      Forks | v4.12-rc4 | +qlocks  | Change
     -------|-----------|----------|--------
         10 | 52630.32  | 53316.31 | +1.01%
         20 | 51777.80  | 52623.15 | +1.02%
         30 | 51645.92  | 52517.26 | +1.02%
         40 | 51634.88  | 52419.89 | +1.02%
         50 | 51506.75  | 52307.81 | +1.02%
         60 | 51500.74  | 52322.72 | +1.02%
         70 | 51434.81  | 52288.60 | +1.02%
         80 | 51423.22  | 52434.85 | +1.02%
         90 | 51428.65  | 52410.10 | +1.02%
    
    The kernels used for these tests also had my "MIPS: Hardcode cpu_has_*
    where known at compile time due to ISA" patch applied, which allows the
    kernel_uses_llsc checks in cmpxchg() & xchg() to be optimised away at
    compile time.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/16358/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit b70eb30056dc84568f3d32440d9be6a558025843
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Jun 9 17:26:39 2017 -0700

    MIPS: cmpxchg: Implement 1 byte & 2 byte xchg()
    
    Implement 1 & 2 byte xchg() using read-modify-write atop a 4 byte
    cmpxchg(). This allows us to support these atomic operations despite the
    MIPS ISA only providing for 4 & 8 byte atomic operations.
    
    This is required in order to support queued spinlocks (qspinlock) in a
    later patch, since these make use of a 2 byte xchg() in their slow path.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/16354/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 60925ee97e2be4993fb7a2f7e70be0fbce08cf0f
Merge: be941bf2e6a3 145d97858597
Author: David S. Miller <davem@davemloft.net>
Date:   Thu May 25 12:07:08 2017 -0700

    Merge branch 'sparc64-queued-locks'
    
    Babu Moger says:
    
    ====================
    Enable queued rwlock and queued spinlock for SPARC
    
    This series of patches enables queued rwlock and queued spinlock support
    for SPARC. These features were introduced some time ago in upstream.
    Here are some of the earlier discussions.
    https://lwn.net/Articles/572765/
    https://lwn.net/Articles/582200/
    https://lwn.net/Articles/561775/
    https://lwn.net/Articles/590243/
    
    Tests: Ran AIM7 benchmark to verify the performance on various workloads.
    https://github.com/davidlohr/areaim. Same benchmark was used when this
    feature was introduced and enabled on x86. Here are the test results.
    
    Kernel                          4.11.0-rc6     4.11.0-rc6 +     Change
                                    baseline        queued locks
                                  (Avg No.of jobs) (Avg No.of jobs)
    Workload
    High systime 10-100 user         17290.48        17295.18       +0.02
    High systime 200-1000 users     109814.95       110248.87       +0.39
    High systime 1200-2000 users    107912.40       127923.16       +18.54
    
    Disk IO 10-100 users            168910.16       158834.17       -5.96
    Disk IO 200-1000 users          242781.74       281285.80       +15.85
    Disk IO 1200-2000 users         228518.23       218421.23       -4.41
    
    Disk IO 10-100 users            183933.77       207928.67       +13.04
    Disk IO 200-1000 users          491981.56       500162.33       +1.66
    Disk IO 1200-2000 users         463395.66       467312.70       +0.84
    
    fserver 10-100 users            254177.53       270283.08       +6.33
    fserver IO 200-1000 users       269017.35       324812.2        +20.74
    fserver IO 1200-2000 users      229538.87       284713.77       +24.03
    
    Disk I/O results are little bit in negative territory. But majority of the
    performance changes are in positive and it is significant in some cases.
    
    Changes:
    v3 -> v4:
     1. Took care of Geert Uytterhoeven's comment about patch #3(def_bool y)
     2. Working on separate patch sets to define CPU_BIG_ENDIAN for all the
        default big endian architectures based on feedback from Geert and Arnd.
    
    v2 -> v3:
     1. Rebased the patches on top of 4.12-rc2.
     2. Re-ordered the patch #1 and patch #2. That is the same order I have seen
        the issues. So, it should be addressed in the same order. Patch #1 removes
        the check __LINUX_SPINLOCK_TYPES_H. Patch #2 addreses the compile error
        with qrwlock.c. This addresses the comments from Dave Miller on v2.
    
    v1 -> v2:
    Addressed the comments from David Miller.
    1. Added CPU_BIG_ENDIAN for all SPARC
    2. Removed #ifndef __LINUX_SPINLOCK_TYPES_H guard from spinlock_types.h
    3. Removed check for CONFIG_QUEUED_RWLOCKS in SPARC64 as it is the
       default definition for SPARC64 now. Cleaned-up the previous arch_read_xxx
       and arch_write_xxx definitions as it is defined now in qrwlock.h.
    4. Removed check for CONFIG_QUEUED_SPINLOCKS in SPARC64 as it is the default
       definition now for SPARC64 now. Cleaned-up the previous arch_spin_xxx
       definitions as it is defined in qspinlock.h.
    
    v1: Initial version
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 145d978585977438ebb55079487827006c604e39
Author: Babu Moger <babu.moger@oracle.com>
Date:   Wed May 24 17:55:15 2017 -0600

    arch/sparc: Enable queued spinlock support for SPARC
    
    This patch makes the necessary changes in SPARC architecture to enable
    queued spinlock support. Here are some of the earlier discussions about
    this feature.
    https://lwn.net/Articles/561775/
    https://lwn.net/Articles/590243/
    
    Cleaned-up the spinlock_64.h. The definitions of arch_spin_xxx are
    replaced by the function in <asm-generic/qspinlock.h>
    
    Signed-off-by: Babu Moger <babu.moger@oracle.com>
    Reviewed-by: Håkon Bugge <haakon.bugge@oracle.com>
    Reviewed-by: Jane Chu <jane.chu@oracle.com>
    Reviewed-by: Shannon Nelson <shannon.nelson@oracle.com>
    Reviewed-by: Vijay Kumar <vijay.ac.kumar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e2d9577854f5a5469bcf7a3d1b17ca5e9b9ba673
Author: Ross Lagerwall <ross.lagerwall@citrix.com>
Date:   Fri Apr 22 13:05:31 2016 +0100

    xen/qspinlock: Don't kick CPU if IRQ is not initialized
    
    commit 707e59ba494372a90d245f18b0c78982caa88e48 upstream.
    
    The following commit:
    
      1fb3a8b2cfb2 ("xen/spinlock: Fix locking path engaging too soon under PVHVM.")
    
    ... moved the initalization of the kicker interrupt until after
    native_cpu_up() is called.
    
    However, when using qspinlocks, a CPU may try to kick another CPU that is
    spinning (because it has not yet initialized its kicker interrupt), resulting
    in the following crash during boot:
    
      kernel BUG at /build/linux-Ay7j_C/linux-4.4.0/drivers/xen/events/events_base.c:1210!
      invalid opcode: 0000 [#1] SMP
      ...
      RIP: 0010:[<ffffffff814c97c9>]  [<ffffffff814c97c9>] xen_send_IPI_one+0x59/0x60
      ...
      Call Trace:
       [<ffffffff8102be9e>] xen_qlock_kick+0xe/0x10
       [<ffffffff810cabc2>] __pv_queued_spin_unlock+0xb2/0xf0
       [<ffffffff810ca6d1>] ? __raw_callee_save___pv_queued_spin_unlock+0x11/0x20
       [<ffffffff81052936>] ? check_tsc_warp+0x76/0x150
       [<ffffffff81052aa6>] check_tsc_sync_source+0x96/0x160
       [<ffffffff81051e28>] native_cpu_up+0x3d8/0x9f0
       [<ffffffff8102b315>] xen_hvm_cpu_up+0x35/0x80
       [<ffffffff8108198c>] _cpu_up+0x13c/0x180
       [<ffffffff81081a4a>] cpu_up+0x7a/0xa0
       [<ffffffff81f80dfc>] smp_init+0x7f/0x81
       [<ffffffff81f5a121>] kernel_init_freeable+0xef/0x212
       [<ffffffff81817f30>] ? rest_init+0x80/0x80
       [<ffffffff81817f3e>] kernel_init+0xe/0xe0
       [<ffffffff8182488f>] ret_from_fork+0x3f/0x70
       [<ffffffff81817f30>] ? rest_init+0x80/0x80
    
    To fix this, only send the kick if the target CPU's interrupt has been
    initialized. This check isn't racy, because the target is waiting for
    the spinlock, so it won't have initialized the interrupt in the
    meantime.
    
    Signed-off-by: Ross Lagerwall <ross.lagerwall@citrix.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Cc: xen-devel@lists.xenproject.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 75437bb304b20a2b350b9a8e9f9238d5e24e12ba
Author: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
Date:   Tue Jan 10 02:56:46 2017 -0500

    locking/pvqspinlock: Don't wait if vCPU is preempted
    
    If prev node is not in running state or its vCPU is preempted, we can give
    up our vCPU slices in pv_wait_node() ASAP.
    
    Signed-off-by: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: longman@redhat.com
    Link: http://lkml.kernel.org/r/1484035006-6787-1-git-send-email-xinhui.pan@linux.vnet.ibm.com
    [ Fixed typos in the changelog, removed ugly linebreak from the code. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 00bcf5cdd6c0e2e92ce3dd852ca68a3b779fa4ec
Merge: de956b8f45b3 08645077b7f9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 12:15:00 2016 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - rwsem micro-optimizations (Davidlohr Bueso)
    
       - Improve the implementation and optimize the performance of
         percpu-rwsems. (Peter Zijlstra.)
    
       - Convert all lglock users to better facilities such as percpu-rwsems
         or percpu-spinlocks and remove lglocks. (Peter Zijlstra)
    
       - Remove the ticket (spin)lock implementation. (Peter Zijlstra)
    
       - Korean translation of memory-barriers.txt and related fixes to the
         English document. (SeongJae Park)
    
       - misc fixes and cleanups"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      x86/cmpxchg, locking/atomics: Remove superfluous definitions
      x86, locking/spinlocks: Remove ticket (spin)lock implementation
      locking/lglock: Remove lglock implementation
      stop_machine: Remove stop_cpus_lock and lg_double_lock/unlock()
      fs/locks: Use percpu_down_read_preempt_disable()
      locking/percpu-rwsem: Add down_read_preempt_disable()
      fs/locks: Replace lg_local with a per-cpu spinlock
      fs/locks: Replace lg_global with a percpu-rwsem
      locking/percpu-rwsem: Add DEFINE_STATIC_PERCPU_RWSEMand percpu_rwsem_assert_held()
      locking/pv-qspinlock: Use cmpxchg_release() in __pv_queued_spin_unlock()
      locking/rwsem, x86: Drop a bogus cc clobber
      futex: Add some more function commentry
      locking/hung_task: Show all locks
      locking/rwsem: Scan the wait_list for readers only once
      locking/rwsem: Remove a few useless comments
      locking/rwsem: Return void in __rwsem_mark_wake()
      locking, rcu, cgroup: Avoid synchronize_sched() in __cgroup_procs_write()
      locking/Documentation: Add Korean translation
      locking/Documentation: Fix a typo of example result
      locking/Documentation: Fix wrong section reference
      ...

commit b193049375b04df3ada8c3347b7083db95918bc3
Author: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
Date:   Mon Sep 19 05:23:52 2016 -0400

    locking/pv-qspinlock: Use cmpxchg_release() in __pv_queued_spin_unlock()
    
    cmpxchg_release() is more lighweight than cmpxchg() on some archs(e.g.
    PPC), moreover, in __pv_queued_spin_unlock() we only needs a RELEASE in
    the fast path(pairing with *_try_lock() or *_lock()). And the slow path
    has smp_store_release too. So it's safe to use cmpxchg_release here.
    
    Suggested-by:  Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: benh@kernel.crashing.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: mpe@ellerman.id.au
    Cc: paulmck@linux.vnet.ibm.com
    Cc: paulus@samba.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: waiman.long@hpe.com
    Link: http://lkml.kernel.org/r/1474277037-15200-2-git-send-email-xinhui.pan@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1f8083c640bf08aab8762a9e10326ce767c66492
Merge: 25db69188e03 c2ace36b884d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 12 12:46:37 2016 -0700

    Merge branch 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fixes from Ingo Molnar:
     "Misc fixes: lockstat fix, futex fix on !MMU systems, big endian fix
      for qrwlocks and a race fix for pvqspinlocks"
    
    * 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/pvqspinlock: Fix a bug in qstat_read()
      locking/pvqspinlock: Fix double hash race
      locking/qrwlock: Fix write unlock bug on big endian systems
      futex: Assume all mappings are private on !MMU systems

commit 08be8f63c40c030b5cf95b4368e314e563a86301
Author: Waiman Long <Waiman.Long@hpe.com>
Date:   Tue May 31 12:53:47 2016 -0400

    locking/pvstat: Separate wait_again and spurious wakeup stats
    
    Currently there are overlap in the pvqspinlock wait_again and
    spurious_wakeup stat counters. Because of lock stealing, it is
    no longer possible to accurately determine if spurious wakeup has
    happened in the queue head.  As they track both the queue node and
    queue head status, it is also hard to tell how many of those comes
    from the queue head and how many from the queue node.
    
    This patch changes the accounting rules so that spurious wakeup is
    only tracked in the queue node. The wait_again count, however, is
    only tracked in the queue head when the vCPU failed to acquire the
    lock after a vCPU kick. This should give a much better indication of
    the wait-kick dynamics in the queue node and the queue head.
    
    Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Douglas Hatch <doug.hatch@hpe.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pan Xinhui <xinhui@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1464713631-1066-2-git-send-email-Waiman.Long@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 64a5e3cb308028dba0676daae0a7821d770036fa
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jul 14 14:26:11 2016 +0200

    locking/qspinlock: Improve readability
    
    Restructure pv_queued_spin_steal_lock() as I found it hard to read.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <waiman.long@hpe.com
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c2ace36b884de9330c4149064ae8d212d2e0d9ee
Author: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
Date:   Wed Jul 13 18:23:34 2016 +0800

    locking/pvqspinlock: Fix a bug in qstat_read()
    
    It's obviously wrong to set stat to NULL. So lets remove it.
    Otherwise it is always zero when we check the latency of kick/wake.
    
    Signed-off-by: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Waiman Long <Waiman.Long@hpe.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1468405414-3700-1-git-send-email-xinhui.pan@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 229ce631574761870a2ac938845fadbd07f35caa
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Thu Jul 14 16:15:56 2016 +0800

    locking/pvqspinlock: Fix double hash race
    
    When the lock holder vCPU is racing with the queue head:
    
       CPU 0 (lock holder)    CPU1 (queue head)
       ===================    =================
       spin_lock();           spin_lock();
        pv_kick_node():        pv_wait_head_or_lock():
                                if (!lp) {
                                 lp = pv_hash(lock, pn);
                                 xchg(&l->locked, _Q_SLOW_VAL);
                                }
                                WRITE_ONCE(pn->state, vcpu_halted);
         cmpxchg(&pn->state,
          vcpu_halted, vcpu_hashed);
         WRITE_ONCE(l->locked, _Q_SLOW_VAL);
         (void)pv_hash(lock, pn);
    
    In this case, lock holder inserts the pv_node of queue head into the
    hash table and set _Q_SLOW_VAL unnecessary. This patch avoids it by
    restoring/setting vcpu_hashed state after failing adaptive locking
    spinning.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <Waiman.Long@hpe.com>
    Link: http://lkml.kernel.org/r/1468484156-4521-1-git-send-email-wanpeng.li@hotmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a39e660a55e8ce5ae35a7694835ad464b72666ca
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 8 10:19:51 2016 +0200

    locking/qspinlock: Fix spin_unlock_wait() some more
    
    commit 2c610022711675ee908b903d242f0b90e1db661f upstream.
    
    While this prior commit:
    
      54cf809b9512 ("locking,qspinlock: Fix spin_is_locked() and spin_unlock_wait()")
    
    ... fixes spin_is_locked() and spin_unlock_wait() for the usage
    in ipc/sem and netfilter, it does not in fact work right for the
    usage in task_work and futex.
    
    So while the 2 locks crossed problem:
    
            spin_lock(A)            spin_lock(B)
            if (!spin_is_locked(B)) spin_unlock_wait(A)
              foo()                 foo();
    
    ... works with the smp_mb() injected by both spin_is_locked() and
    spin_unlock_wait(), this is not sufficient for:
    
            flag = 1;
            smp_mb();               spin_lock()
            spin_unlock_wait()      if (!flag)
                                      // add to lockless list
            // iterate lockless list
    
    ... because in this scenario, the store from spin_lock() can be delayed
    past the load of flag, uncrossing the variables and loosing the
    guarantee.
    
    This patch reworks spin_is_locked() and spin_unlock_wait() to work in
    both cases by exploiting the observation that while the lock byte
    store can be delayed, the contender must have registered itself
    visibly in other state contained in the word.
    
    It also allows for architectures to override both functions, as PPC
    and ARM64 have an additional issue for which we currently have no
    generic solution.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Giovanni Gherdovich <ggherdovich@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <waiman.long@hpe.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Fixes: 54cf809b9512 ("locking,qspinlock: Fix spin_is_locked() and spin_unlock_wait()")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2f5367e5031cebf0513c96a4736bf7767fce4a7d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 8 10:19:51 2016 +0200

    locking/qspinlock: Fix spin_unlock_wait() some more
    
    commit 2c610022711675ee908b903d242f0b90e1db661f upstream.
    
    While this prior commit:
    
      54cf809b9512 ("locking,qspinlock: Fix spin_is_locked() and spin_unlock_wait()")
    
    ... fixes spin_is_locked() and spin_unlock_wait() for the usage
    in ipc/sem and netfilter, it does not in fact work right for the
    usage in task_work and futex.
    
    So while the 2 locks crossed problem:
    
            spin_lock(A)            spin_lock(B)
            if (!spin_is_locked(B)) spin_unlock_wait(A)
              foo()                 foo();
    
    ... works with the smp_mb() injected by both spin_is_locked() and
    spin_unlock_wait(), this is not sufficient for:
    
            flag = 1;
            smp_mb();               spin_lock()
            spin_unlock_wait()      if (!flag)
                                      // add to lockless list
            // iterate lockless list
    
    ... because in this scenario, the store from spin_lock() can be delayed
    past the load of flag, uncrossing the variables and loosing the
    guarantee.
    
    This patch reworks spin_is_locked() and spin_unlock_wait() to work in
    both cases by exploiting the observation that while the lock byte
    store can be delayed, the contender must have registered itself
    visibly in other state contained in the word.
    
    It also allows for architectures to override both functions, as PPC
    and ARM64 have an additional issue for which we currently have no
    generic solution.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Giovanni Gherdovich <ggherdovich@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <waiman.long@hpe.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Fixes: 54cf809b9512 ("locking,qspinlock: Fix spin_is_locked() and spin_unlock_wait()")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c86ad14d305d2429c3da19462440bac50c183def
Merge: a2303849a6b4 f06628638cf6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 12:41:29 2016 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The locking tree was busier in this cycle than the usual pattern - a
      couple of major projects happened to coincide.
    
      The main changes are:
    
       - implement the atomic_fetch_{add,sub,and,or,xor}() API natively
         across all SMP architectures (Peter Zijlstra)
    
       - add atomic_fetch_{inc/dec}() as well, using the generic primitives
         (Davidlohr Bueso)
    
       - optimize various aspects of rwsems (Jason Low, Davidlohr Bueso,
         Waiman Long)
    
       - optimize smp_cond_load_acquire() on arm64 and implement LSE based
         atomic{,64}_fetch_{add,sub,and,andnot,or,xor}{,_relaxed,_acquire,_release}()
         on arm64 (Will Deacon)
    
       - introduce smp_acquire__after_ctrl_dep() and fix various barrier
         mis-uses and bugs (Peter Zijlstra)
    
       - after discovering ancient spin_unlock_wait() barrier bugs in its
         implementation and usage, strengthen its semantics and update/fix
         usage sites (Peter Zijlstra)
    
       - optimize mutex_trylock() fastpath (Peter Zijlstra)
    
       - ... misc fixes and cleanups"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (67 commits)
      locking/atomic: Introduce inc/dec variants for the atomic_fetch_$op() API
      locking/barriers, arch/arm64: Implement LDXR+WFE based smp_cond_load_acquire()
      locking/static_keys: Fix non static symbol Sparse warning
      locking/qspinlock: Use __this_cpu_dec() instead of full-blown this_cpu_dec()
      locking/atomic, arch/tile: Fix tilepro build
      locking/atomic, arch/m68k: Remove comment
      locking/atomic, arch/arc: Fix build
      locking/Documentation: Clarify limited control-dependency scope
      locking/atomic, arch/rwsem: Employ atomic_long_fetch_add()
      locking/atomic, arch/qrwlock: Employ atomic_fetch_add_acquire()
      locking/atomic, arch/mips: Convert to _relaxed atomics
      locking/atomic, arch/alpha: Convert to _relaxed atomics
      locking/atomic: Remove the deprecated atomic_{set,clear}_mask() functions
      locking/atomic: Remove linux/atomic.h:atomic_fetch_or()
      locking/atomic: Implement atomic{,64,_long}_fetch_{add,sub,and,andnot,or,xor}{,_relaxed,_acquire,_release}()
      locking/atomic: Fix atomic64_relaxed() bits
      locking/atomic, arch/xtensa: Implement atomic_fetch_{add,sub,and,or,xor}()
      locking/atomic, arch/x86: Implement atomic{,64}_fetch_{add,sub,and,or,xor}()
      locking/atomic, arch/tile: Implement atomic{,64}_fetch_{add,sub,and,or,xor}()
      locking/atomic, arch/sparc: Implement atomic{,64}_fetch_{add,sub,and,or,xor}()
      ...

commit da677fe14364f7ac1f5f7085c58bbc2f7bb12da0
Author: Max Kellermann <max@duempel.org>
Date:   Mon Jul 4 14:08:45 2016 +0200

    [media] dvb-core/en50221: use kref to manage struct dvb_ca_private
    
    Don't free the object until the file handle has been closed.  Fixes
    use-after-free bug which occurs when I disconnect my DVB-S received
    while VDR is running.
    
    This is a crash dump of such a use-after-free:
    
        general protection fault: 0000 [#1] SMP
        CPU: 0 PID: 2541 Comm: CI adapter on d Not tainted 4.7.0-rc1-hosting+ #49
        Hardware name: Bochs Bochs, BIOS Bochs 01/01/2011
        task: ffff880027d7ce00 ti: ffff88003d8f8000 task.ti: ffff88003d8f8000
        RIP: 0010:[<ffffffff812f3d1f>]  [<ffffffff812f3d1f>] dvb_ca_en50221_io_read_condition.isra.7+0x6f/0x150
        RSP: 0018:ffff88003d8fba98  EFLAGS: 00010206
        RAX: 0000000059534255 RBX: 000000753d470f90 RCX: ffff88003c74d181
        RDX: 00000001bea04ba9 RSI: ffff88003d8fbaf4 RDI: 3a3030a56d763fc0
        RBP: ffff88003d8fbae0 R08: ffff88003c74d180 R09: 0000000000000000
        R10: 0000000000000001 R11: 0000000000000000 R12: ffff88003c480e00
        R13: 00000000ffffffff R14: 0000000059534255 R15: 0000000000000000
        FS:  00007fb4209b4700(0000) GS:ffff88003fc00000(0000) knlGS:0000000000000000
        CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
        CR2: 00007f06445f4078 CR3: 000000003c55b000 CR4: 00000000000006b0
        Stack:
         ffff88003d8fbaf4 000000003c2170c0 0000000000004000 0000000000000000
         ffff88003c480e00 ffff88003d8fbc80 ffff88003c74d180 ffff88003d8fbb8c
         0000000000000000 ffff88003d8fbb10 ffffffff812f3e37 ffff88003d8fbb00
        Call Trace:
         [<ffffffff812f3e37>] dvb_ca_en50221_io_poll+0x37/0xa0
         [<ffffffff8113109b>] do_sys_poll+0x2db/0x520
    
    This is a backtrace of the kernel attempting to lock a freed mutex:
    
        #0  0xffffffff81083d40 in rep_nop () at ./arch/x86/include/asm/processor.h:569
        #1  cpu_relax () at ./arch/x86/include/asm/processor.h:574
        #2  virt_spin_lock (lock=<optimized out>) at ./arch/x86/include/asm/qspinlock.h:57
        #3  native_queued_spin_lock_slowpath (lock=0xffff88003c480e90, val=761492029) at kernel/locking/qspinlock.c:304
        #4  0xffffffff810d1a06 in pv_queued_spin_lock_slowpath (val=<optimized out>, lock=<optimized out>) at ./arch/x86/include/asm/paravirt.h:669
        #5  queued_spin_lock_slowpath (val=<optimized out>, lock=<optimized out>) at ./arch/x86/include/asm/qspinlock.h:28
        #6  queued_spin_lock (lock=<optimized out>) at include/asm-generic/qspinlock.h:107
        #7  __mutex_lock_common (use_ww_ctx=<optimized out>, ww_ctx=<optimized out>, ip=<optimized out>, nest_lock=<optimized out>, subclass=<optimized out>,
            state=<optimized out>, lock=<optimized out>) at kernel/locking/mutex.c:526
        #8  mutex_lock_interruptible_nested (lock=0xffff88003c480e88, subclass=<optimized out>) at kernel/locking/mutex.c:647
        #9  0xffffffff812f49fe in dvb_ca_en50221_io_do_ioctl (file=<optimized out>, cmd=761492029, parg=0x1 <irq_stack_union+1>)
            at drivers/media/dvb-core/dvb_ca_en50221.c:1210
        #10 0xffffffff812ee660 in dvb_usercopy (file=<optimized out>, cmd=761492029, arg=<optimized out>, func=<optimized out>) at drivers/media/dvb-core/dvbdev.c:883
        #11 0xffffffff812f3410 in dvb_ca_en50221_io_ioctl (file=<optimized out>, cmd=<optimized out>, arg=<optimized out>) at drivers/media/dvb-core/dvb_ca_en50221.c:1284
        #12 0xffffffff8112eddd in vfs_ioctl (arg=<optimized out>, cmd=<optimized out>, filp=<optimized out>) at fs/ioctl.c:43
        #13 do_vfs_ioctl (filp=0xffff88003c480e90, fd=<optimized out>, cmd=<optimized out>, arg=<optimized out>) at fs/ioctl.c:674
        #14 0xffffffff8112f30c in SYSC_ioctl (arg=<optimized out>, cmd=<optimized out>, fd=<optimized out>) at fs/ioctl.c:689
        #15 SyS_ioctl (fd=6, cmd=2148298626, arg=140734533693696) at fs/ioctl.c:680
        #16 0xffffffff8103feb2 in entry_SYSCALL_64 () at arch/x86/entry/entry_64.S:207
    
    Signed-off-by: Max Kellermann <max@duempel.org>
    Signed-off-by: Mauro Carvalho Chehab <mchehab@s-opensource.com>

commit 0dceeaf599e6d9b8bd908ba4bd3dfee84aa26be2
Author: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
Date:   Tue Jun 14 14:37:27 2016 +0800

    locking/qspinlock: Use __this_cpu_dec() instead of full-blown this_cpu_dec()
    
    queued_spin_lock_slowpath() should not worry about another
    queued_spin_lock_slowpath() running in interrupt context and
    changing node->count by accident, because node->count keeps
    the same value every time we enter/leave queued_spin_lock_slowpath().
    
    On some architectures this_cpu_dec() will save/restore irq flags,
    which has high overhead. Use the much cheaper __this_cpu_dec() instead.
    
    Signed-off-by: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman.Long@hpe.com
    Link: http://lkml.kernel.org/r/1465886247-3773-1-git-send-email-xinhui.pan@linux.vnet.ibm.com
    [ Rewrote changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9cbbef4efb7cfb6962da57c9e17f5ce4280c14ca
Merge: 8c2561557415 0d15ef677839
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 17 07:19:13 2016 -1000

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes from Will Deacon:
     "The main things are getting kgdb up and running with upstream GDB
      after a protocol change was reverted and fixing our spin_unlock_wait
      and spin_is_locked implementations after doing some similar work with
      PeterZ on the qspinlock code last week.  Whilst we haven't seen any
      failures in practice, it's still worth getting this fixed.
    
      Summary:
    
       - Plug the ongoing spin_unlock_wait/spin_is_locked mess
       - KGDB protocol fix to sync w/ GDB
       - Fix MIDR-based PMU probing for old 32-bit SMP systems
         (OMAP4/Realview)
       - Minor tweaks to the fault handling path"
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: kgdb: Match pstate size with gdbserver protocol
      arm64: spinlock: Ensure forward-progress in spin_unlock_wait
      arm64: spinlock: fix spin_unlock_wait for LSE atomics
      arm64: spinlock: order spin_{is_locked,unlock_wait} against local locks
      arm: pmu: Fix non-devicetree probing
      arm64: mm: mark fault_info table const
      arm64: fix dump_instr when PAN and UAO are in use

commit 33ac279677dcc2441cb93d8cb9cf7a74df62814d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 24 13:17:12 2016 +0200

    locking/barriers: Introduce smp_acquire__after_ctrl_dep()
    
    Introduce smp_acquire__after_ctrl_dep(), this construct is not
    uncommon, but the lack of this barrier is.
    
    Use it to better express smp_rmb() uses in WRITE_ONCE(), the IPC
    semaphore code and the qspinlock code.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 02b07bde619e179bf7ac0e073d28e2e038dfab77
Merge: 606c17f4e9b7 077fa7aed17d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 10 10:53:46 2016 -0700

    Merge branch 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fixes from Ingo Molnar:
     "Misc fixes:
    
       - a file-based futex fix
       - one more spin_unlock_wait() fix
       - a ww-mutex deadlock detection improvement/fix
       - and a raw_read_seqcount_latch() barrier fix"
    
    * 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      futex: Calculate the futex key based on a tail page for file-based futexes
      locking/qspinlock: Fix spin_unlock_wait() some more
      locking/ww_mutex: Report recursive ww_mutex locking early
      locking/seqcount: Re-fix raw_read_seqcount_latch()

commit ca50e426f96c905e7d14a9c7a6bd4e0330516047
Author: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
Date:   Fri Jun 3 16:38:14 2016 +0800

    locking/qspinlock: Use atomic_sub_return_release() in queued_spin_unlock()
    
    The existing version uses a heavy barrier while only release semantics
    is required. So use atomic_sub_return_release() instead.
    
    Suggested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arnd@arndb.de
    Cc: waiman.long@hp.com
    Link: http://lkml.kernel.org/r/1464943094-3129-1-git-send-email-xinhui.pan@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 055ce0fd1b86c204430cbc0887165599d6e15090
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 8 10:36:53 2016 +0200

    locking/qspinlock: Add comments
    
    I figured we need to document the spin_is_locked() and
    spin_unlock_wait() constraints somwehere.
    
    Ideally 'someone' would rewrite Documentation/atomic_ops.txt and we
    could find a place in there. But currently that document is stale to
    the point of hardly being useful.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <waiman.long@hpe.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 8d53fa19041ae65c484d81d75179b4a577e6d8e4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 8 09:12:30 2016 +0200

    locking/qspinlock: Clarify xchg_tail() ordering
    
    While going over the code I noticed that xchg_tail() is a RELEASE but
    had no obvious pairing commented.
    
    It pairs with a somewhat unique address dependency through
    decode_tail().
    
    So the store-release of xchg_tail() is paired by the address
    dependency of the load of xchg_tail followed by the dereference from
    the pointer computed from that load.
    
    The @old -> @prev transformation itself is pure, and therefore does
    not depend on external state, so that is immaterial wrt. ordering.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <waiman.long@hpe.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 2c610022711675ee908b903d242f0b90e1db661f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 8 10:19:51 2016 +0200

    locking/qspinlock: Fix spin_unlock_wait() some more
    
    While this prior commit:
    
      54cf809b9512 ("locking,qspinlock: Fix spin_is_locked() and spin_unlock_wait()")
    
    ... fixes spin_is_locked() and spin_unlock_wait() for the usage
    in ipc/sem and netfilter, it does not in fact work right for the
    usage in task_work and futex.
    
    So while the 2 locks crossed problem:
    
            spin_lock(A)            spin_lock(B)
            if (!spin_is_locked(B)) spin_unlock_wait(A)
              foo()                 foo();
    
    ... works with the smp_mb() injected by both spin_is_locked() and
    spin_unlock_wait(), this is not sufficient for:
    
            flag = 1;
            smp_mb();               spin_lock()
            spin_unlock_wait()      if (!flag)
                                      // add to lockless list
            // iterate lockless list
    
    ... because in this scenario, the store from spin_lock() can be delayed
    past the load of flag, uncrossing the variables and loosing the
    guarantee.
    
    This patch reworks spin_is_locked() and spin_unlock_wait() to work in
    both cases by exploiting the observation that while the lock byte
    store can be delayed, the contender must have registered itself
    visibly in other state contained in the word.
    
    It also allows for architectures to override both functions, as PPC
    and ARM64 have an additional issue for which we currently have no
    generic solution.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Giovanni Gherdovich <ggherdovich@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <waiman.long@hpe.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: stable@vger.kernel.org # v4.2 and later
    Fixes: 54cf809b9512 ("locking,qspinlock: Fix spin_is_locked() and spin_unlock_wait()")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1267ee349ed8b3646c2f7f3154173ac0ef63ba5f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 20 18:04:36 2016 +0200

    locking,qspinlock: Fix spin_is_locked() and spin_unlock_wait()
    
    commit 54cf809b9512be95f53ed4a5e3b631d1ac42f0fa upstream.
    
    Similar to commits:
    
      51d7d5205d33 ("powerpc: Add smp_mb() to arch_spin_is_locked()")
      d86b8da04dfa ("arm64: spinlock: serialise spin_unlock_wait against concurrent lockers")
    
    qspinlock suffers from the fact that the _Q_LOCKED_VAL store is
    unordered inside the ACQUIRE of the lock.
    
    And while this is not a problem for the regular mutual exclusive
    critical section usage of spinlocks, it breaks creative locking like:
    
            spin_lock(A)                    spin_lock(B)
            spin_unlock_wait(B)             if (!spin_is_locked(A))
            do_something()                    do_something()
    
    In that both CPUs can end up running do_something at the same time,
    because our _Q_LOCKED_VAL store can drop past the spin_unlock_wait()
    spin_is_locked() loads (even on x86!!).
    
    To avoid making the normal case slower, add smp_mb()s to the less used
    spin_unlock_wait() / spin_is_locked() side of things to avoid this
    problem.
    
    Reported-and-tested-by: Davidlohr Bueso <dave@stgolabs.net>
    Reported-by: Giovanni Gherdovich <ggherdovich@suse.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit acc55cf5339a99389669139f83463ada2b1a6559
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 20 18:04:36 2016 +0200

    locking,qspinlock: Fix spin_is_locked() and spin_unlock_wait()
    
    commit 54cf809b9512be95f53ed4a5e3b631d1ac42f0fa upstream.
    
    Similar to commits:
    
      51d7d5205d33 ("powerpc: Add smp_mb() to arch_spin_is_locked()")
      d86b8da04dfa ("arm64: spinlock: serialise spin_unlock_wait against concurrent lockers")
    
    qspinlock suffers from the fact that the _Q_LOCKED_VAL store is
    unordered inside the ACQUIRE of the lock.
    
    And while this is not a problem for the regular mutual exclusive
    critical section usage of spinlocks, it breaks creative locking like:
    
            spin_lock(A)                    spin_lock(B)
            spin_unlock_wait(B)             if (!spin_is_locked(A))
            do_something()                    do_something()
    
    In that both CPUs can end up running do_something at the same time,
    because our _Q_LOCKED_VAL store can drop past the spin_unlock_wait()
    spin_is_locked() loads (even on x86!!).
    
    To avoid making the normal case slower, add smp_mb()s to the less used
    spin_unlock_wait() / spin_is_locked() side of things to avoid this
    problem.
    
    Reported-and-tested-by: Davidlohr Bueso <dave@stgolabs.net>
    Reported-by: Giovanni Gherdovich <ggherdovich@suse.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 035688290a740745adf9daff65bceac8b70e8732
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 20 18:04:36 2016 +0200

    locking,qspinlock: Fix spin_is_locked() and spin_unlock_wait()
    
    commit 54cf809b9512be95f53ed4a5e3b631d1ac42f0fa upstream.
    
    Similar to commits:
    
      51d7d5205d33 ("powerpc: Add smp_mb() to arch_spin_is_locked()")
      d86b8da04dfa ("arm64: spinlock: serialise spin_unlock_wait against concurrent lockers")
    
    qspinlock suffers from the fact that the _Q_LOCKED_VAL store is
    unordered inside the ACQUIRE of the lock.
    
    And while this is not a problem for the regular mutual exclusive
    critical section usage of spinlocks, it breaks creative locking like:
    
            spin_lock(A)                    spin_lock(B)
            spin_unlock_wait(B)             if (!spin_is_locked(A))
            do_something()                    do_something()
    
    In that both CPUs can end up running do_something at the same time,
    because our _Q_LOCKED_VAL store can drop past the spin_unlock_wait()
    spin_is_locked() loads (even on x86!!).
    
    To avoid making the normal case slower, add smp_mb()s to the less used
    spin_unlock_wait() / spin_is_locked() side of things to avoid this
    problem.
    
    Reported-and-tested-by: Davidlohr Bueso <dave@stgolabs.net>
    Reported-by: Giovanni Gherdovich <ggherdovich@suse.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 54cf809b9512be95f53ed4a5e3b631d1ac42f0fa
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 20 18:04:36 2016 +0200

    locking,qspinlock: Fix spin_is_locked() and spin_unlock_wait()
    
    Similar to commits:
    
      51d7d5205d33 ("powerpc: Add smp_mb() to arch_spin_is_locked()")
      d86b8da04dfa ("arm64: spinlock: serialise spin_unlock_wait against concurrent lockers")
    
    qspinlock suffers from the fact that the _Q_LOCKED_VAL store is
    unordered inside the ACQUIRE of the lock.
    
    And while this is not a problem for the regular mutual exclusive
    critical section usage of spinlocks, it breaks creative locking like:
    
            spin_lock(A)                    spin_lock(B)
            spin_unlock_wait(B)             if (!spin_is_locked(A))
            do_something()                    do_something()
    
    In that both CPUs can end up running do_something at the same time,
    because our _Q_LOCKED_VAL store can drop past the spin_unlock_wait()
    spin_is_locked() loads (even on x86!!).
    
    To avoid making the normal case slower, add smp_mb()s to the less used
    spin_unlock_wait() / spin_is_locked() side of things to avoid this
    problem.
    
    Reported-and-tested-by: Davidlohr Bueso <dave@stgolabs.net>
    Reported-by: Giovanni Gherdovich <ggherdovich@suse.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: stable@vger.kernel.org   # v4.2 and later
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 1c19b68a279c58d6da4379bf8b6d679a300a1daf
Merge: 49817c33433a a1cc5bcfcfca
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 13:17:56 2016 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking changes from Ingo Molnar:
     "The main changes in this cycle were:
    
       - pvqspinlock statistics fixes (Davidlohr Bueso)
    
       - flip atomic_fetch_or() arguments (Peter Zijlstra)
    
       - locktorture simplification (Paul E.  McKenney)
    
       - documentation updates (SeongJae Park, David Howells, Davidlohr
         Bueso, Paul E McKenney, Peter Zijlstra, Will Deacon)
    
       - various fixes"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/atomics: Flip atomic_fetch_or() arguments
      locking/pvqspinlock: Robustify init_qspinlock_stat()
      locking/pvqspinlock: Avoid double resetting of stats
      lcoking/locktorture: Simplify the torture_runnable computation
      locking/Documentation: Clarify that ACQUIRE applies to loads, RELEASE applies to stores
      locking/Documentation: State purpose of memory-barriers.txt
      locking/Documentation: Add disclaimer
      locking/Documentation/lockdep: Fix spelling mistakes
      locking/lockdep: Deinline register_lock_class(), save 2328 bytes
      locking/locktorture: Fix NULL pointer dereference for cleanup paths
      locking/locktorture: Fix deboosting NULL pointer dereference
      locking/Documentation: Mention smp_cond_acquire()
      locking/Documentation: Insert white spaces consistently
      locking/Documentation: Fix formatting inconsistencies
      locking/Documentation: Add missed subsection in TOC
      locking/Documentation: Fix missed s/lock/acquire renames
      locking/Documentation: Clarify relationship of barrier() to control dependencies

commit b96bbdde19cc56f288372d25fd5ea7af04fc1271
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Tue Apr 19 21:17:25 2016 -0700

    locking/pvqspinlock: Robustify init_qspinlock_stat()
    
    Specifically around the debugfs file creation calls,
    I have no idea if they could ever possibly fail, but
    this is core code (debug aside) so lets at least
    check the return value and inform anything fishy.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Waiman Long <Waiman.Long@hpe.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160420041725.GC3472@linux-uzut.site
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit dc209a3fd73ec96d4491bcc128c3b50b0a8e8017
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Sun Apr 17 23:31:42 2016 -0700

    locking/pvqspinlock: Avoid double resetting of stats
    
    ... remove the redundant second iteration, this is most
    likely a copy/past buglet.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dave@stgolabs.net
    Cc: waiman.long@hpe.com
    Link: http://lkml.kernel.org/r/1460961103-24953-2-git-send-email-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 92c19ea9535707701861b7533253a516c7d115c9
Merge: 814dd9481d20 1bdb8970392a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 28 20:24:27 2016 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar:
     "Two boot crash fixes and an IRQ handling crash fix"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/apic: Handle zero vector gracefully in clear_vector_irq()
      Revert "x86/mm/32: Set NX in __supported_pte_mask before enabling paging"
      xen/qspinlock: Don't kick CPU if IRQ is not initialized

commit 0e11d256512c5166e03c9e9f221f9371bd42911e
Merge: 16ecb4141013 fba7cd681b61
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Apr 23 11:39:48 2016 -0700

    Merge branch 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fixes from Ingo Molnar:
     "Misc fixes:
    
      pvqspinlocks:
       - an instrumentation fix
    
      futexes:
       - preempt-count vs pagefault_disable decouple corner case fix
       - futex requeue plist race window fix
       - futex UNLOCK_PI transaction fix for a corner case"
    
    * 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      asm-generic/futex: Re-enable preemption in futex_atomic_cmpxchg_inatomic()
      futex: Acknowledge a new waiter in counter before plist
      futex: Handle unlock_pi race gracefully
      locking/pvqspinlock: Fix division by zero in qstat_read()

commit 707e59ba494372a90d245f18b0c78982caa88e48
Author: Ross Lagerwall <ross.lagerwall@citrix.com>
Date:   Fri Apr 22 13:05:31 2016 +0100

    xen/qspinlock: Don't kick CPU if IRQ is not initialized
    
    The following commit:
    
      1fb3a8b2cfb2 ("xen/spinlock: Fix locking path engaging too soon under PVHVM.")
    
    ... moved the initalization of the kicker interrupt until after
    native_cpu_up() is called.
    
    However, when using qspinlocks, a CPU may try to kick another CPU that is
    spinning (because it has not yet initialized its kicker interrupt), resulting
    in the following crash during boot:
    
      kernel BUG at /build/linux-Ay7j_C/linux-4.4.0/drivers/xen/events/events_base.c:1210!
      invalid opcode: 0000 [#1] SMP
      ...
      RIP: 0010:[<ffffffff814c97c9>]  [<ffffffff814c97c9>] xen_send_IPI_one+0x59/0x60
      ...
      Call Trace:
       [<ffffffff8102be9e>] xen_qlock_kick+0xe/0x10
       [<ffffffff810cabc2>] __pv_queued_spin_unlock+0xb2/0xf0
       [<ffffffff810ca6d1>] ? __raw_callee_save___pv_queued_spin_unlock+0x11/0x20
       [<ffffffff81052936>] ? check_tsc_warp+0x76/0x150
       [<ffffffff81052aa6>] check_tsc_sync_source+0x96/0x160
       [<ffffffff81051e28>] native_cpu_up+0x3d8/0x9f0
       [<ffffffff8102b315>] xen_hvm_cpu_up+0x35/0x80
       [<ffffffff8108198c>] _cpu_up+0x13c/0x180
       [<ffffffff81081a4a>] cpu_up+0x7a/0xa0
       [<ffffffff81f80dfc>] smp_init+0x7f/0x81
       [<ffffffff81f5a121>] kernel_init_freeable+0xef/0x212
       [<ffffffff81817f30>] ? rest_init+0x80/0x80
       [<ffffffff81817f3e>] kernel_init+0xe/0xe0
       [<ffffffff8182488f>] ret_from_fork+0x3f/0x70
       [<ffffffff81817f30>] ? rest_init+0x80/0x80
    
    To fix this, only send the kick if the target CPU's interrupt has been
    initialized. This check isn't racy, because the target is waiting for
    the spinlock, so it won't have initialized the interrupt in the
    meantime.
    
    Signed-off-by: Ross Lagerwall <ross.lagerwall@citrix.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Cc: xen-devel@lists.xenproject.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6687659568e2ec5b3ac24b39c5d26ce8b9d90434
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Sun Apr 17 23:31:41 2016 -0700

    locking/pvqspinlock: Fix division by zero in qstat_read()
    
    While playing with the qstat statistics (in <debugfs>/qlockstat/) I ran into
    the following splat on a VM when opening pv_hash_hops:
    
      divide error: 0000 [#1] SMP
      ...
      RIP: 0010:[<ffffffff810b61fe>]  [<ffffffff810b61fe>] qstat_read+0x12e/0x1e0
      ...
      Call Trace:
        [<ffffffff811cad7c>] ? mem_cgroup_commit_charge+0x6c/0xd0
        [<ffffffff8119750c>] ? page_add_new_anon_rmap+0x8c/0xd0
        [<ffffffff8118d3b9>] ? handle_mm_fault+0x1439/0x1b40
        [<ffffffff811937a9>] ? do_mmap+0x449/0x550
        [<ffffffff811d3de3>] ? __vfs_read+0x23/0xd0
        [<ffffffff811d4ab2>] ? rw_verify_area+0x52/0xd0
        [<ffffffff811d4bb1>] ? vfs_read+0x81/0x120
        [<ffffffff811d5f12>] ? SyS_read+0x42/0xa0
        [<ffffffff815720f6>] ? entry_SYSCALL_64_fastpath+0x1e/0xa8
    
    Fix this by verifying that qstat_pv_kick_unlock is in fact non-zero,
    similarly to what the qstat_pv_latency_wake case does, as if nothing
    else, this can come from resetting the statistics, thus having 0 kicks
    should be quite valid in this context.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Reviewed-by: Waiman Long <Waiman.Long@hpe.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dave@stgolabs.net
    Cc: waiman.long@hpe.com
    Link: http://lkml.kernel.org/r/1460961103-24953-1-git-send-email-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit fbed0bc0915e2dec7452fc3e66ad03dd2b0c04c7
Merge: d37a14bb5fed 38460a2178d2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 14 15:50:44 2016 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking changes from Ingo Molnar:
     "Various updates:
    
       - Futex scalability improvements: remove page lock use for shared
         futex get_futex_key(), which speeds up 'perf bench futex hash'
         benchmarks by over 40% on a 60-core Westmere.  This makes anon-mem
         shared futexes perform close to private futexes.  (Mel Gorman)
    
       - lockdep hash collision detection and fix (Alfredo Alvarez
         Fernandez)
    
       - lockdep testing enhancements (Alfredo Alvarez Fernandez)
    
       - robustify lockdep init by using hlists (Andrew Morton, Andrey
         Ryabinin)
    
       - mutex and csd_lock micro-optimizations (Davidlohr Bueso)
    
       - small x86 barriers tweaks (Michael S Tsirkin)
    
       - qspinlock updates (Waiman Long)"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (25 commits)
      locking/csd_lock: Use smp_cond_acquire() in csd_lock_wait()
      locking/csd_lock: Explicitly inline csd_lock*() helpers
      futex: Replace barrier() in unqueue_me() with READ_ONCE()
      locking/lockdep: Detect chain_key collisions
      locking/lockdep: Prevent chain_key collisions
      tools/lib/lockdep: Fix link creation warning
      tools/lib/lockdep: Add tests for AA and ABBA locking
      tools/lib/lockdep: Add userspace version of READ_ONCE()
      tools/lib/lockdep: Fix the build on recent kernels
      locking/qspinlock: Move __ARCH_SPIN_LOCK_UNLOCKED to qspinlock_types.h
      locking/mutex: Allow next waiter lockless wakeup
      locking/pvqspinlock: Enable slowpath locking count tracking
      locking/qspinlock: Use smp_cond_acquire() in pending code
      locking/pvqspinlock: Move lock stealing count tracking code into pv_queued_spin_steal_lock()
      locking/mcs: Fix mcs_spin_lock() ordering
      futex: Remove requirement for lock_page() in get_futex_key()
      futex: Rename barrier references in ordering guarantees
      locking/atomics: Update comment about READ_ONCE() and structures
      locking/lockdep: Eliminate lockdep_init()
      locking/lockdep: Convert hash tables to hlists
      ...

commit b82e530290a0437522720becaf4abdf8ca4cb7d2
Author: Dan Streetman <dan.streetman@canonical.com>
Date:   Fri Feb 19 13:49:27 2016 -0500

    locking/qspinlock: Move __ARCH_SPIN_LOCK_UNLOCKED to qspinlock_types.h
    
    Move the __ARCH_SPIN_LOCK_UNLOCKED definition from qspinlock.h into
    qspinlock_types.h.
    
    The definition of __ARCH_SPIN_LOCK_UNLOCKED comes from the build arch's
    include files; but on x86 when CONFIG_QUEUED_SPINLOCKS=y, it just
    it's defined in asm-generic/qspinlock.h.  In most cases, this doesn't
    matter because linux/spinlock.h includes asm/spinlock.h, which for x86
    includes asm-generic/qspinlock.h.  However, any code that only includes
    linux/mutex.h will break, because it only includes asm/spinlock_types.h.
    
    For example, this breaks systemtap, which only includes mutex.h.
    
    Signed-off-by: Dan Streetman <dan.streetman@canonical.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Waiman Long <Waiman.Long@hpe.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dan Streetman <ddstreet@ieee.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1455907767-17821-1-git-send-email-dan.streetman@canonical.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 32d62510f949d3c8e83b9b3b844a84446611661b
Author: Waiman Long <Waiman.Long@hpe.com>
Date:   Thu Dec 10 15:17:45 2015 -0500

    locking/pvqspinlock: Enable slowpath locking count tracking
    
    This patch enables the tracking of the number of slowpath locking
    operations performed. This can be used to compare against the number
    of lock stealing operations to see what percentage of locks are stolen
    versus acquired via the regular slowpath.
    
    Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Douglas Hatch <doug.hatch@hpe.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1449778666-13593-2-git-send-email-Waiman.Long@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit cb037fdad6772df2d49fe61c97d7c0d8265bc918
Author: Waiman Long <Waiman.Long@hpe.com>
Date:   Thu Dec 10 15:17:44 2015 -0500

    locking/qspinlock: Use smp_cond_acquire() in pending code
    
    The newly introduced smp_cond_acquire() was used to replace the
    slowpath lock acquisition loop. Similarly, the new function can also
    be applied to the pending bit locking loop. This patch uses the new
    function in that loop.
    
    Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Douglas Hatch <doug.hatch@hpe.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1449778666-13593-1-git-send-email-Waiman.Long@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit eaff0e7003cca6c2748b67ead2d4b1a8ad858fc7
Author: Waiman Long <Waiman.Long@hpe.com>
Date:   Thu Dec 10 15:17:46 2015 -0500

    locking/pvqspinlock: Move lock stealing count tracking code into pv_queued_spin_steal_lock()
    
    This patch moves the lock stealing count tracking code into
    pv_queued_spin_steal_lock() instead of via a jacket function simplifying
    the code.
    
    Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Douglas Hatch <doug.hatch@hpe.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1449778666-13593-3-git-send-email-Waiman.Long@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 24af98c4cf5f5e69266e270c7f3fb34b82ff6656
Merge: 9061cbe62ade 337f13046ff0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 11 14:18:38 2016 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "So we have a laundry list of locking subsystem changes:
    
       - continuing barrier API and code improvements
    
       - futex enhancements
    
       - atomics API improvements
    
       - pvqspinlock enhancements: in particular lock stealing and adaptive
         spinning
    
       - qspinlock micro-enhancements"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      futex: Allow FUTEX_CLOCK_REALTIME with FUTEX_WAIT op
      futex: Cleanup the goto confusion in requeue_pi()
      futex: Remove pointless put_pi_state calls in requeue()
      futex: Document pi_state refcounting in requeue code
      futex: Rename free_pi_state() to put_pi_state()
      futex: Drop refcount if requeue_pi() acquired the rtmutex
      locking/barriers, arch: Remove ambiguous statement in the smp_store_mb() documentation
      lcoking/barriers, arch: Use smp barriers in smp_store_release()
      locking/cmpxchg, arch: Remove tas() definitions
      locking/pvqspinlock: Queue node adaptive spinning
      locking/pvqspinlock: Allow limited lock stealing
      locking/pvqspinlock: Collect slowpath lock statistics
      sched/core, locking: Document Program-Order guarantees
      locking, sched: Introduce smp_cond_acquire() and use it
      locking/pvqspinlock, x86: Optimize the PV unlock code path
      locking/qspinlock: Avoid redundant read of next pointer
      locking/qspinlock: Prefetch the next node cacheline
      locking/qspinlock: Use _acquire/_release() versions of cmpxchg() & xchg()
      atomics: Add test for atomic operations with _relaxed variants

commit cd0272fab785077c121aa91ec2401090965bbc37
Author: Waiman Long <Waiman.Long@hpe.com>
Date:   Mon Nov 9 19:09:27 2015 -0500

    locking/pvqspinlock: Queue node adaptive spinning
    
    In an overcommitted guest where some vCPUs have to be halted to make
    forward progress in other areas, it is highly likely that a vCPU later
    in the spinlock queue will be spinning while the ones earlier in the
    queue would have been halted. The spinning in the later vCPUs is then
    just a waste of precious CPU cycles because they are not going to
    get the lock soon as the earlier ones have to be woken up and take
    their turn to get the lock.
    
    This patch implements an adaptive spinning mechanism where the vCPU
    will call pv_wait() if the previous vCPU is not running.
    
    Linux kernel builds were run in KVM guest on an 8-socket, 4
    cores/socket Westmere-EX system and a 4-socket, 8 cores/socket
    Haswell-EX system. Both systems are configured to have 32 physical
    CPUs. The kernel build times before and after the patch were:
    
                        Westmere                    Haswell
      Patch         32 vCPUs    48 vCPUs    32 vCPUs    48 vCPUs
      -----         --------    --------    --------    --------
      Before patch   3m02.3s     5m00.2s     1m43.7s     3m03.5s
      After patch    3m03.0s     4m37.5s     1m43.0s     2m47.2s
    
    For 32 vCPUs, this patch doesn't cause any noticeable change in
    performance. For 48 vCPUs (over-committed), there is about 8%
    performance improvement.
    
    Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Douglas Hatch <doug.hatch@hpe.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447114167-47185-8-git-send-email-Waiman.Long@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1c4941fd53afb46ab15826628e4819866d008a28
Author: Waiman Long <Waiman.Long@hpe.com>
Date:   Tue Nov 10 16:18:56 2015 -0500

    locking/pvqspinlock: Allow limited lock stealing
    
    This patch allows one attempt for the lock waiter to steal the lock
    when entering the PV slowpath. To prevent lock starvation, the pending
    bit will be set by the queue head vCPU when it is in the active lock
    spinning loop to disable any lock stealing attempt.  This helps to
    reduce the performance penalty caused by lock waiter preemption while
    not having much of the downsides of a real unfair lock.
    
    The pv_wait_head() function was renamed as pv_wait_head_or_lock()
    as it was modified to acquire the lock before returning. This is
    necessary because of possible lock stealing attempts from other tasks.
    
    Linux kernel builds were run in KVM guest on an 8-socket, 4
    cores/socket Westmere-EX system and a 4-socket, 8 cores/socket
    Haswell-EX system. Both systems are configured to have 32 physical
    CPUs. The kernel build times before and after the patch were:
    
                        Westmere                    Haswell
      Patch         32 vCPUs    48 vCPUs    32 vCPUs    48 vCPUs
      -----         --------    --------    --------    --------
      Before patch   3m15.6s    10m56.1s     1m44.1s     5m29.1s
      After patch    3m02.3s     5m00.2s     1m43.7s     3m03.5s
    
    For the overcommited case (48 vCPUs), this patch is able to reduce
    kernel build time by more than 54% for Westmere and 44% for Haswell.
    
    Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Douglas Hatch <doug.hatch@hpe.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447190336-53317-1-git-send-email-Waiman.Long@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 45e898b735620f426eddf105fc886d2966593a58
Author: Waiman Long <Waiman.Long@hpe.com>
Date:   Mon Nov 9 19:09:25 2015 -0500

    locking/pvqspinlock: Collect slowpath lock statistics
    
    This patch enables the accumulation of kicking and waiting related
    PV qspinlock statistics when the new QUEUED_LOCK_STAT configuration
    option is selected. It also enables the collection of data which
    enable us to calculate the kicking and wakeup latencies which have
    a heavy dependency on the CPUs being used.
    
    The statistical counters are per-cpu variables to minimize the
    performance overhead in their updates. These counters are exported
    via the debugfs filesystem under the qlockstat directory.  When the
    corresponding debugfs files are read, summation and computing of the
    required data are then performed.
    
    The measured latencies for different CPUs are:
    
            CPU             Wakeup          Kicking
            ---             ------          -------
            Haswell-EX      63.6us           7.4us
            Westmere-EX     67.6us           9.3us
    
    The measured latencies varied a bit from run-to-run. The wakeup
    latency is much higher than the kicking latency.
    
    A sample of statistical counters after system bootup (with vCPU
    overcommit) was:
    
            pv_hash_hops=1.00
            pv_kick_unlock=1148
            pv_kick_wake=1146
            pv_latency_kick=11040
            pv_latency_wake=194840
            pv_spurious_wakeup=7
            pv_wait_again=4
            pv_wait_head=23
            pv_wait_node=1129
    
    Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Douglas Hatch <doug.hatch@hpe.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447114167-47185-6-git-send-email-Waiman.Long@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d78045306c41bd9334b956e4e7fa77cc72f06a40
Author: Waiman Long <Waiman.Long@hpe.com>
Date:   Mon Nov 9 19:09:24 2015 -0500

    locking/pvqspinlock, x86: Optimize the PV unlock code path
    
    The unlock function in queued spinlocks was optimized for better
    performance on bare metal systems at the expense of virtualized guests.
    
    For x86-64 systems, the unlock call needs to go through a
    PV_CALLEE_SAVE_REGS_THUNK() which saves and restores 8 64-bit
    registers before calling the real __pv_queued_spin_unlock()
    function. The thunk code may also be in a separate cacheline from
    __pv_queued_spin_unlock().
    
    This patch optimizes the PV unlock code path by:
    
     1) Moving the unlock slowpath code from the fastpath into a separate
        __pv_queued_spin_unlock_slowpath() function to make the fastpath
        as simple as possible..
    
     2) For x86-64, hand-coded an assembly function to combine the register
        saving thunk code with the fastpath code. Only registers that
        are used in the fastpath will be saved and restored. If the
        fastpath fails, the slowpath function will be called via another
        PV_CALLEE_SAVE_REGS_THUNK(). For 32-bit, it falls back to the C
        __pv_queued_spin_unlock() code as the thunk saves and restores
        only one 32-bit register.
    
    With a microbenchmark of 5M lock-unlock loop, the table below shows
    the execution times before and after the patch with different number
    of threads in a VM running on a 32-core Westmere-EX box with x86-64
    4.2-rc1 based kernels:
    
      Threads       Before patch    After patch     % Change
      -------       ------------    -----------     --------
         1             134.1 ms       119.3 ms        -11%
         2             1286  ms        953  ms        -26%
         3             3715  ms       3480  ms        -6.3%
         4             4092  ms       3764  ms        -8.0%
    
    Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Douglas Hatch <doug.hatch@hpe.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447114167-47185-5-git-send-email-Waiman.Long@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit aa68744f80bfb6f26fbe7f10e42876066f7dac1b
Author: Waiman Long <Waiman.Long@hpe.com>
Date:   Mon Nov 9 19:09:23 2015 -0500

    locking/qspinlock: Avoid redundant read of next pointer
    
    With optimistic prefetch of the next node cacheline, the next pointer
    may have been properly inititalized. As a result, the reading
    of node->next in the contended path may be redundant. This patch
    eliminates the redundant read if the next pointer value is not NULL.
    
    Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Douglas Hatch <doug.hatch@hpe.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447114167-47185-4-git-send-email-Waiman.Long@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 81b5598665a24083dd889fbd8cb08b0d8de4b8ad
Author: Waiman Long <Waiman.Long@hpe.com>
Date:   Mon Nov 9 19:09:22 2015 -0500

    locking/qspinlock: Prefetch the next node cacheline
    
    A queue head CPU, after acquiring the lock, will have to notify
    the next CPU in the wait queue that it has became the new queue
    head. This involves loading a new cacheline from the MCS node of the
    next CPU. That operation can be expensive and add to the latency of
    locking operation.
    
    This patch addes code to optmistically prefetch the next MCS node
    cacheline if the next pointer is defined and it has been spinning
    for the MCS lock for a while. This reduces the locking latency and
    improves the system throughput.
    
    The performance change will depend on whether the prefetch overhead
    can be hidden within the latency of the lock spin loop. On really
    short critical section, there may not be performance gain at all. With
    longer critical section, however, it was found to have a performance
    boost of 5-10% over a range of different queue depths with a spinlock
    loop microbenchmark.
    
    Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Douglas Hatch <doug.hatch@hpe.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447114167-47185-3-git-send-email-Waiman.Long@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 64d816cba06c67eeee455b8c78ebcda349d49c24
Author: Waiman Long <Waiman.Long@hpe.com>
Date:   Mon Nov 9 19:09:21 2015 -0500

    locking/qspinlock: Use _acquire/_release() versions of cmpxchg() & xchg()
    
    This patch replaces the cmpxchg() and xchg() calls in the native
    qspinlock code with the more relaxed _acquire or _release versions of
    those calls to enable other architectures to adopt queued spinlocks
    with less memory barrier performance overhead.
    
    Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Douglas Hatch <doug.hatch@hpe.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447114167-47185-2-git-send-email-Waiman.Long@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d63a9788650fcd999b34584316afee6bd4378f19
Merge: 281422869942 6e490b0106a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 3 16:10:43 2015 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking changes from Ingo Molnar:
     "The main changes in this cycle were:
    
       - More gradual enhancements to atomic ops: new atomic*_read_ctrl()
         ops, synchronize atomic_{read,set}() ordering requirements between
         architectures, add atomic_long_t bitops.  (Peter Zijlstra)
    
       - Add _{relaxed|acquire|release}() variants for inc/dec atomics and
         use them in various locking primitives: mutex, rtmutex, mcs, rwsem.
         This enables weakly ordered architectures (such as arm64) to make
         use of more locking related optimizations.  (Davidlohr Bueso)
    
       - Implement atomic[64]_{inc,dec}_relaxed() on ARM.  (Will Deacon)
    
       - Futex kernel data cache footprint micro-optimization.  (Rasmus
         Villemoes)
    
       - pvqspinlock runtime overhead micro-optimization.  (Waiman Long)
    
       - misc smaller fixlets"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      ARM, locking/atomics: Implement _relaxed variants of atomic[64]_{inc,dec}
      locking/rwsem: Use acquire/release semantics
      locking/mcs: Use acquire/release semantics
      locking/rtmutex: Use acquire/release semantics
      locking/mutex: Use acquire/release semantics
      locking/asm-generic: Add _{relaxed|acquire|release}() variants for inc/dec atomics
      atomic: Implement atomic_read_ctrl()
      atomic, arch: Audit atomic_{read,set}()
      atomic: Add atomic_long_t bitops
      futex: Force hot variables into a single cache line
      locking/pvqspinlock: Kick the PV CPU unconditionally when _Q_SLOW_VAL
      locking/osq: Relax atomic semantics
      locking/qrwlock: Rename ->lock to ->wait_lock
      locking/Documentation/lockstat: Fix typo - lokcing -> locking
      locking/atomics, cmpxchg: Privatize the inclusion of asm/cmpxchg.h

commit 75928be688c2ba28e41566ba1b9d942f906d90f4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Sep 5 16:55:05 2015 +0200

    locking/qspinlock/x86: Only emit the test-and-set fallback when building guest support
    
    commit a6b277857fd2c990bc208ca1958d3f34d26052f7 upstream.
    
    Only emit the test-and-set fallback for Hypervisors lacking
    PARAVIRT_SPINLOCKS support when building for guests.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 304b8b47da42152baf13fbf73f074db76d970bc9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Sep 4 17:25:23 2015 +0200

    locking/qspinlock/x86: Fix performance regression under unaccelerated VMs
    
    commit 43b3f02899f74ae9914a39547cc5492156f0027a upstream.
    
    Dave ran into horrible performance on a VM without PARAVIRT_SPINLOCKS
    set and Linus noted that the test-and-set implementation was retarded.
    
    One should spin on the variable with a load, not a RMW.
    
    While there, remove 'queued' from the name, as the lock isn't queued
    at all, but a simple test-and-set.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: Dave Chinner <david@fromorbit.com>
    Tested-by: Dave Chinner <david@fromorbit.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Link: http://lkml.kernel.org/r/20150904152523.GR18673@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 93edc8bd7750ff3cae088bfca453ea73dc9004a4
Author: Waiman Long <Waiman.Long@hpe.com>
Date:   Fri Sep 11 14:37:34 2015 -0400

    locking/pvqspinlock: Kick the PV CPU unconditionally when _Q_SLOW_VAL
    
    If _Q_SLOW_VAL has been set, the vCPU state must have been vcpu_hashed.
    The extra check at the end of __pv_queued_spin_unlock() is unnecessary
    and can be removed.
    
    Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1441996658-62854-3-git-send-email-Waiman.Long@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9786cff38a31c452e32fd2f9a479dd7d19d91712
Merge: 1b3dfde386b7 1975dbc276c6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 17 08:45:23 2015 -0700

    Merge branch 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fixes from Ingo Molnar:
     "Spinlock performance regression fix, plus documentation fixes"
    
    * 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/static_keys: Fix up the static keys documentation
      locking/qspinlock/x86: Only emit the test-and-set fallback when building guest support
      locking/qspinlock/x86: Fix performance regression under unaccelerated VMs
      locking/static_keys: Fix a silly typo

commit a6b277857fd2c990bc208ca1958d3f34d26052f7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Sep 5 16:55:05 2015 +0200

    locking/qspinlock/x86: Only emit the test-and-set fallback when building guest support
    
    Only emit the test-and-set fallback for Hypervisors lacking
    PARAVIRT_SPINLOCKS support when building for guests.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Cc: stable@vger.kernel.org # 4.2
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 43b3f02899f74ae9914a39547cc5492156f0027a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Sep 4 17:25:23 2015 +0200

    locking/qspinlock/x86: Fix performance regression under unaccelerated VMs
    
    Dave ran into horrible performance on a VM without PARAVIRT_SPINLOCKS
    set and Linus noted that the test-and-set implementation was retarded.
    
    One should spin on the variable with a load, not a RMW.
    
    While there, remove 'queued' from the name, as the lock isn't queued
    at all, but a simple test-and-set.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: Dave Chinner <david@fromorbit.com>
    Tested-by: Dave Chinner <david@fromorbit.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Cc: stable@vger.kernel.org # v4.2+
    Link: http://lkml.kernel.org/r/20150904152523.GR18673@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 5e5013c6b5150b8009d980d72495a55b79bf5134
Merge: c67976541670 cba77f03f2c7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 14 10:45:23 2015 -0700

    Merge branch 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking fix from Ingo Molnar:
     "A single fix for a locking self-test crash"
    
    * 'locking-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/pvqspinlock: Fix kernel panic in locking-selftest

commit 75d2270280686bff21b9ba66c7f3dd379c887981
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Sat Jul 11 16:36:52 2015 -0400

    locking/pvqspinlock: Only kick CPU at unlock time
    
    For an over-committed guest with more vCPUs than physical CPUs
    available, it is possible that a vCPU may be kicked twice before
    getting the lock - once before it becomes queue head and once again
    before it gets the lock. All these CPU kicking and halting (VMEXIT)
    can be expensive and slow down system performance.
    
    This patch adds a new vCPU state (vcpu_hashed) which enables the code
    to delay CPU kicking until at unlock time. Once this state is set,
    the new lock holder will set _Q_SLOW_VAL and fill in the hash table
    on behalf of the halted queue head vCPU. The original vcpu_halted
    state will be used by pv_wait_node() only to differentiate other
    queue nodes from the qeue head.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1436647018-49734-2-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 3b3fdf10a8add87ef0050138d51bfee9ab4983df
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jul 13 16:58:30 2015 +0100

    locking/pvqspinlock: Order pv_unhash() after cmpxchg() on unlock slowpath
    
    When we unlock in __pv_queued_spin_unlock(), a failed cmpxchg() on the lock
    value indicates that we need to take the slow-path and unhash the
    corresponding node blocked on the lock.
    
    Since a failed cmpxchg() does not provide any memory-ordering guarantees,
    it is possible that the node data could be read before the cmpxchg() on
    weakly-ordered architectures and therefore return a stale value, leading
    to hash corruption and/or a BUG().
    
    This patch adds an smb_rmb() following the failed cmpxchg operation, so
    that the unhashing is ordered after the lock has been checked.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    [ Added more comments]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by:  Waiman Long <Waiman.Long@hp.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Steve Capper <Steve.Capper@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20150713155830.GL2632@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 0b792bf519e68108d577fcec815ab50913787012
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jul 21 12:13:43 2015 +0200

    locking: Clean up pvqspinlock warning
    
     - Rename the on-stack variable to match the datastructure variable,
    
     - place the cmpxchg back under the comment that explains it,
    
     - clean up the WARN() statement to avoid superfluous conditionals
       and line-breaks.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit cba77f03f2c7b6cc0b0a44a3c679e0abade7da62
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Sat Jul 11 21:19:19 2015 -0400

    locking/pvqspinlock: Fix kernel panic in locking-selftest
    
    Enabling locking-selftest in a VM guest may cause the following
    kernel panic:
    
      kernel BUG at .../kernel/locking/qspinlock_paravirt.h:137!
    
    This is due to the fact that the pvqspinlock unlock function is
    expecting either a _Q_LOCKED_VAL or _Q_SLOW_VAL in the lock
    byte. This patch prevents that bug report by ignoring it when
    debug_locks_silent is set. Otherwise, a warning will be printed
    if it contains an unexpected value.
    
    With this patch applied, the kernel locking-selftest completed
    without any noise.
    
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1436663959-53092-1-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f7d71f2052555ae57b47322f2c2f6c29ff2438ae
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Fri Jun 19 11:50:00 2015 -0400

    locking/qrwlock: Rename functions to queued_*()
    
    To sync up with the naming convention used in qspinlock, all the
    qrwlock functions were renamed to started with "queued" instead of
    "queue".
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/1434729002-57724-2-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1bf7067c6e173dc10411704db48338ed69c05565
Merge: fc934d40178a 68722101ec3a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 14:54:22 2015 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes are:
    
       - 'qspinlock' support, enabled on x86: queued spinlocks - these are
         now the spinlock variant used by x86 as they outperform ticket
         spinlocks in every category.  (Waiman Long)
    
       - 'pvqspinlock' support on x86: paravirtualized variant of queued
         spinlocks.  (Waiman Long, Peter Zijlstra)
    
       - 'qrwlock' support, enabled on x86: queued rwlocks.  Similar to
         queued spinlocks, they are now the variant used by x86:
    
           CONFIG_ARCH_USE_QUEUED_SPINLOCKS=y
           CONFIG_QUEUED_SPINLOCKS=y
           CONFIG_ARCH_USE_QUEUED_RWLOCKS=y
           CONFIG_QUEUED_RWLOCKS=y
    
       - various lockdep fixlets
    
       - various locking primitives cleanups, further WRITE_ONCE()
         propagation"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      locking/lockdep: Remove hard coded array size dependency
      locking/qrwlock: Don't contend with readers when setting _QW_WAITING
      lockdep: Do not break user-visible string
      locking/arch: Rename set_mb() to smp_store_mb()
      locking/arch: Add WRITE_ONCE() to set_mb()
      rtmutex: Warn if trylock is called from hard/softirq context
      arch: Remove __ARCH_HAVE_CMPXCHG
      locking/rtmutex: Drop usage of __HAVE_ARCH_CMPXCHG
      locking/qrwlock: Rename QUEUE_RWLOCK to QUEUED_RWLOCKS
      locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS
      locking/pvqspinlock: Replace xchg() by the more descriptive set_mb()
      locking/pvqspinlock, x86: Enable PV qspinlock for Xen
      locking/pvqspinlock, x86: Enable PV qspinlock for KVM
      locking/pvqspinlock, x86: Implement the paravirt qspinlock call patching
      locking/pvqspinlock: Implement simple paravirt support for the qspinlock
      locking/qspinlock: Revert to test-and-set on hypervisors
      locking/qspinlock: Use a simple write to grab the lock
      locking/qspinlock: Optimize for smaller NR_CPUS
      locking/qspinlock: Extract out code snippets for the next patch
      locking/qspinlock: Add pending bit
      ...

commit 405963b6a57c60040bc1dad2597f7f4b897954d1
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Tue Jun 9 11:19:13 2015 -0400

    locking/qrwlock: Don't contend with readers when setting _QW_WAITING
    
    The current cmpxchg() loop in setting the _QW_WAITING flag for writers
    in queue_write_lock_slowpath() will contend with incoming readers
    causing possibly extra cmpxchg() operations that are wasteful. This
    patch changes the code to do a byte cmpxchg() to eliminate contention
    with new readers.
    
    A multithreaded microbenchmark running 5M read_lock/write_lock loop
    on a 8-socket 80-core Westmere-EX machine running 4.0 based kernel
    with the qspinlock patch have the following execution times (in ms)
    with and without the patch:
    
    With R:W ratio = 5:1
    
            Threads    w/o patch    with patch      % change
            -------    ---------    ----------      --------
               2         990            895           -9.6%
               3        2136           1912          -10.5%
               4        3166           2830          -10.6%
               5        3953           3629           -8.2%
               6        4628           4405           -4.8%
               7        5344           5197           -2.8%
               8        6065           6004           -1.0%
               9        6826           6811           -0.2%
              10        7599           7599            0.0%
              15        9757           9766           +0.1%
              20       13767          13817           +0.4%
    
    With small number of contending threads, this patch can improve
    locking performance by up to 10%. With more contending threads,
    however, the gain diminishes.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1433863153-30722-3-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 62c7a1e9ae54ef66658df9614bdbc09cbbdaa6f0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 11 09:47:23 2015 +0200

    locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS
    
    Valentin Rothberg reported that we use CONFIG_QUEUED_SPINLOCKS
    in arch/x86/kernel/paravirt_patch_32.c, while the symbol is
    called CONFIG_QUEUED_SPINLOCK. (Note the extra 'S')
    
    But the typo was natural: the proper English term for such
    a generic object would be 'queued spinlocks' - so rename
    this and related symbols accordingly to the plural form.
    
    Reported-by: Valentin Rothberg <valentinrothberg@gmail.com>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 52c9d2badd1ae4d11c29de57d4e964e48afd3cb4
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Sun May 10 21:17:10 2015 -0400

    locking/pvqspinlock: Replace xchg() by the more descriptive set_mb()
    
    The xchg() function was used in pv_wait_node() to set a certain
    value and provide a memory barrier which is what the set_mb()
    function is for.  This patch replaces the xchg() call by
    set_mb().
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e95e6f176c61dd0e7bd9fdfb4956df1f9bfe99d4
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Fri Apr 24 14:56:40 2015 -0400

    locking/pvqspinlock, x86: Enable PV qspinlock for Xen
    
    This patch adds the necessary Xen specific code to allow Xen to
    support the CPU halting and kicking operations needed by the queue
    spinlock PV code.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel J Blueman <daniel@numascale.com>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <paolo.bonzini@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1429901803-29771-12-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit bf0c7c34adc286bec3a5a38c00c773ba1b2d0396
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Fri Apr 24 14:56:39 2015 -0400

    locking/pvqspinlock, x86: Enable PV qspinlock for KVM
    
    This patch adds the necessary KVM specific code to allow KVM to
    support the CPU halting and kicking operations needed by the queue
    spinlock PV code.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel J Blueman <daniel@numascale.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <paolo.bonzini@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1429901803-29771-11-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f233f7f1581e78fd9b4023f2e7d8c1ed89020cc9
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Fri Apr 24 14:56:38 2015 -0400

    locking/pvqspinlock, x86: Implement the paravirt qspinlock call patching
    
    We use the regular paravirt call patching to switch between:
    
      native_queued_spin_lock_slowpath()    __pv_queued_spin_lock_slowpath()
      native_queued_spin_unlock()           __pv_queued_spin_unlock()
    
    We use a callee saved call for the unlock function which reduces the
    i-cache footprint and allows 'inlining' of SPIN_UNLOCK functions
    again.
    
    We further optimize the unlock path by patching the direct call with a
    "movb $0,%arg1" if we are indeed using the native unlock code. This
    makes the unlock code almost as fast as the !PARAVIRT case.
    
    This significantly lowers the overhead of having
    CONFIG_PARAVIRT_SPINLOCKS enabled, even for native code.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel J Blueman <daniel@numascale.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <paolo.bonzini@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1429901803-29771-10-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a23db284fe0d1879ca2002bf31077b5efa2fe2ca
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Fri Apr 24 14:56:37 2015 -0400

    locking/pvqspinlock: Implement simple paravirt support for the qspinlock
    
    Provide a separate (second) version of the spin_lock_slowpath for
    paravirt along with a special unlock path.
    
    The second slowpath is generated by adding a few pv hooks to the
    normal slowpath, but where those will compile away for the native
    case, they expand into special wait/wake code for the pv version.
    
    The actual MCS queue can use extra storage in the mcs_nodes[] array to
    keep track of state and therefore uses directed wakeups.
    
    The head contender has no such storage directly visible to the
    unlocker.  So the unlocker searches a hash table with open addressing
    using a simple binary Galois linear feedback shift register.
    
    Suggested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel J Blueman <daniel@numascale.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <paolo.bonzini@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1429901803-29771-9-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 2aa79af64263190eec610422b07f60e99a7d230a
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Fri Apr 24 14:56:36 2015 -0400

    locking/qspinlock: Revert to test-and-set on hypervisors
    
    When we detect a hypervisor (!paravirt, see qspinlock paravirt support
    patches), revert to a simple test-and-set lock to avoid the horrors
    of queue preemption.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel J Blueman <daniel@numascale.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <paolo.bonzini@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1429901803-29771-8-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 2c83e8e9492dc823be1d96d4c5ef75d16d3866a0
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Fri Apr 24 14:56:35 2015 -0400

    locking/qspinlock: Use a simple write to grab the lock
    
    Currently, atomic_cmpxchg() is used to get the lock. However, this
    is not really necessary if there is more than one task in the queue
    and the queue head don't need to reset the tail code. For that case,
    a simple write to set the lock bit is enough as the queue head will
    be the only one eligible to get the lock as long as it checks that
    both the lock and pending bits are not set. The current pending bit
    waiting code will ensure that the bit will not be set as soon as the
    tail code in the lock is set.
    
    With that change, the are some slight improvement in the performance
    of the queued spinlock in the 5M loop micro-benchmark run on a 4-socket
    Westere-EX machine as shown in the tables below.
    
                    [Standalone/Embedded - same node]
      # of tasks    Before patch    After patch     %Change
      ----------    -----------     ----------      -------
           3         2324/2321      2248/2265        -3%/-2%
           4         2890/2896      2819/2831        -2%/-2%
           5         3611/3595      3522/3512        -2%/-2%
           6         4281/4276      4173/4160        -3%/-3%
           7         5018/5001      4875/4861        -3%/-3%
           8         5759/5750      5563/5568        -3%/-3%
    
                    [Standalone/Embedded - different nodes]
      # of tasks    Before patch    After patch     %Change
      ----------    -----------     ----------      -------
           3        12242/12237     12087/12093      -1%/-1%
           4        10688/10696     10507/10521      -2%/-2%
    
    It was also found that this change produced a much bigger performance
    improvement in the newer IvyBridge-EX chip and was essentially to close
    the performance gap between the ticket spinlock and queued spinlock.
    
    The disk workload of the AIM7 benchmark was run on a 4-socket
    Westmere-EX machine with both ext4 and xfs RAM disks at 3000 users
    on a 3.14 based kernel. The results of the test runs were:
    
                    AIM7 XFS Disk Test
      kernel                 JPM    Real Time   Sys Time    Usr Time
      -----                  ---    ---------   --------    --------
      ticketlock            5678233    3.17       96.61       5.81
      qspinlock             5750799    3.13       94.83       5.97
    
                    AIM7 EXT4 Disk Test
      kernel                 JPM    Real Time   Sys Time    Usr Time
      -----                  ---    ---------   --------    --------
      ticketlock            1114551   16.15      509.72       7.11
      qspinlock             2184466    8.24      232.99       6.01
    
    The ext4 filesystem run had a much higher spinlock contention than
    the xfs filesystem run.
    
    The "ebizzy -m" test was also run with the following results:
    
      kernel               records/s  Real Time   Sys Time    Usr Time
      -----                ---------  ---------   --------    --------
      ticketlock             2075       10.00      216.35       3.49
      qspinlock              3023       10.00      198.20       4.80
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel J Blueman <daniel@numascale.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <paolo.bonzini@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1429901803-29771-7-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 69f9cae90907e09af95fb991ed384670cef8dd32
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Fri Apr 24 14:56:34 2015 -0400

    locking/qspinlock: Optimize for smaller NR_CPUS
    
    When we allow for a max NR_CPUS < 2^14 we can optimize the pending
    wait-acquire and the xchg_tail() operations.
    
    By growing the pending bit to a byte, we reduce the tail to 16bit.
    This means we can use xchg16 for the tail part and do away with all
    the repeated compxchg() operations.
    
    This in turn allows us to unconditionally acquire; the locked state
    as observed by the wait loops cannot change. And because both locked
    and pending are now a full byte we can use simple stores for the
    state transition, obviating one atomic operation entirely.
    
    This optimization is needed to make the qspinlock achieve performance
    parity with ticket spinlock at light load.
    
    All this is horribly broken on Alpha pre EV56 (and any other arch that
    cannot do single-copy atomic byte stores).
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel J Blueman <daniel@numascale.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <paolo.bonzini@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1429901803-29771-6-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6403bd7d0ea1878a487296114eccf78658d7dd7a
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Fri Apr 24 14:56:33 2015 -0400

    locking/qspinlock: Extract out code snippets for the next patch
    
    This is a preparatory patch that extracts out the following 2 code
    snippets to prepare for the next performance optimization patch.
    
     1) the logic for the exchange of new and previous tail code words
        into a new xchg_tail() function.
     2) the logic for clearing the pending bit and setting the locked bit
        into a new clear_pending_set_locked() function.
    
    This patch also simplifies the trylock operation before queuing by
    calling queued_spin_trylock() directly.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel J Blueman <daniel@numascale.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <paolo.bonzini@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1429901803-29771-5-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c1fb159db9f2e50e0f4025bed92a67a6a7bfa7b7
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Fri Apr 24 14:56:32 2015 -0400

    locking/qspinlock: Add pending bit
    
    Because the qspinlock needs to touch a second cacheline (the per-cpu
    mcs_nodes[]); add a pending bit and allow a single in-word spinner
    before we punt to the second cacheline.
    
    It is possible so observe the pending bit without the locked bit when
    the last owner has just released but the pending owner has not yet
    taken ownership.
    
    In this case we would normally queue -- because the pending bit is
    already taken. However, in this case the pending bit is guaranteed
    to be released 'soon', therefore wait for it and avoid queueing.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel J Blueman <daniel@numascale.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <paolo.bonzini@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1429901803-29771-4-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d73a33973f16ab6703e75ea00edee857afa3406e
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Fri Apr 24 14:56:31 2015 -0400

    locking/qspinlock, x86: Enable x86-64 to use queued spinlocks
    
    This patch makes the necessary changes at the x86 architecture
    specific layer to enable the use of queued spinlocks for x86-64. As
    x86-32 machines are typically not multi-socket. The benefit of queue
    spinlock may not be apparent. So queued spinlocks are not enabled.
    
    Currently, there is some incompatibilities between the para-virtualized
    spinlock code (which hard-codes the use of ticket spinlock) and the
    queued spinlocks. Therefore, the use of queued spinlocks is disabled
    when the para-virtualized spinlock is enabled.
    
    The arch/x86/include/asm/qspinlock.h header file includes some x86
    specific optimization which will make the queueds spinlock code
    perform better than the generic implementation.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel J Blueman <daniel@numascale.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <paolo.bonzini@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1429901803-29771-3-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a33fda35e3a7655fb7df756ed67822afb5ed5e8d
Author: Waiman Long <Waiman.Long@hp.com>
Date:   Fri Apr 24 14:56:30 2015 -0400

    locking/qspinlock: Introduce a simple generic 4-byte queued spinlock
    
    This patch introduces a new generic queued spinlock implementation that
    can serve as an alternative to the default ticket spinlock. Compared
    with the ticket spinlock, this queued spinlock should be almost as fair
    as the ticket spinlock. It has about the same speed in single-thread
    and it can be much faster in high contention situations especially when
    the spinlock is embedded within the data structure to be protected.
    
    Only in light to moderate contention where the average queue depth
    is around 1-3 will this queued spinlock be potentially a bit slower
    due to the higher slowpath overhead.
    
    This queued spinlock is especially suit to NUMA machines with a large
    number of cores as the chance of spinlock contention is much higher
    in those machines. The cost of contention is also higher because of
    slower inter-node memory traffic.
    
    Due to the fact that spinlocks are acquired with preemption disabled,
    the process will not be migrated to another CPU while it is trying
    to get a spinlock. Ignoring interrupt handling, a CPU can only be
    contending in one spinlock at any one time. Counting soft IRQ, hard
    IRQ and NMI, a CPU can only have a maximum of 4 concurrent lock waiting
    activities.  By allocating a set of per-cpu queue nodes and used them
    to form a waiting queue, we can encode the queue node address into a
    much smaller 24-bit size (including CPU number and queue node index)
    leaving one byte for the lock.
    
    Please note that the queue node is only needed when waiting for the
    lock. Once the lock is acquired, the queue node can be released to
    be used later.
    
    Signed-off-by: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Daniel J Blueman <daniel@numascale.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Douglas Hatch <doug.hatch@hp.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paolo Bonzini <paolo.bonzini@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Raghavendra K T <raghavendra.kt@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1429901803-29771-2-git-send-email-Waiman.Long@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 90631822c5d307b5410500806e8ac3e63928aa3e
Author: Jason Low <jason.low2@hp.com>
Date:   Mon Jul 14 10:27:49 2014 -0700

    locking/spinlocks/mcs: Convert osq lock to atomic_t to reduce overhead
    
    The cancellable MCS spinlock is currently used to queue threads that are
    doing optimistic spinning. It uses per-cpu nodes, where a thread obtaining
    the lock would access and queue the local node corresponding to the CPU that
    it's running on. Currently, the cancellable MCS lock is implemented by using
    pointers to these nodes.
    
    In this patch, instead of operating on pointers to the per-cpu nodes, we
    store the CPU numbers in which the per-cpu nodes correspond to in atomic_t.
    A similar concept is used with the qspinlock.
    
    By operating on the CPU # of the nodes using atomic_t instead of pointers
    to those nodes, this can reduce the overhead of the cancellable MCS spinlock
    by 32 bits (on 64 bit systems).
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott Norton <scott.norton@hp.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Waiman Long <waiman.long@hp.com>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Chris Mason <clm@fb.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Josef Bacik <jbacik@fusionio.com>
    Link: http://lkml.kernel.org/r/1405358872-3732-3-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
