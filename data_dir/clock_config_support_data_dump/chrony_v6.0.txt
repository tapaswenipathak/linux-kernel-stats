commit 82587c0e2735b04a3d9426bee1635f10dfe2829b
Author: Liam Howlett <liam.howlett@oracle.com>
Date:   Wed Jan 11 20:02:07 2023 +0000

    maple_tree: fix mas_empty_area_rev() lower bound validation
    
    commit 7327e8111adb315423035fb5233533016dfd3f2e upstream.
    
    mas_empty_area_rev() was not correctly validating the start of a gap
    against the lower limit.  This could lead to the range starting lower than
    the requested minimum.
    
    Fix the issue by better validating a gap once one is found.
    
    This commit also adds tests to the maple tree test suite for this issue
    and tests the mas_empty_area() function for similar bound checking.
    
    Link: https://lkml.kernel.org/r/20230111200136.1851322-1-Liam.Howlett@oracle.com
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=216911
    Fixes: 54a611b60590 ("Maple Tree: add new data structure")
    Signed-off-by: Liam R. Howlett <Liam.Howlett@oracle.com>
    Reported-by: <amanieu@gmail.com>
      Link: https://lore.kernel.org/linux-mm/0b9f5425-08d4-8013-aa4c-e620c3b10bb2@leemhuis.info/
    Tested-by: Holger Hoffsttte <holger@applied-asynchrony.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7327e8111adb315423035fb5233533016dfd3f2e
Author: Liam Howlett <liam.howlett@oracle.com>
Date:   Wed Jan 11 20:02:07 2023 +0000

    maple_tree: fix mas_empty_area_rev() lower bound validation
    
    mas_empty_area_rev() was not correctly validating the start of a gap
    against the lower limit.  This could lead to the range starting lower than
    the requested minimum.
    
    Fix the issue by better validating a gap once one is found.
    
    This commit also adds tests to the maple tree test suite for this issue
    and tests the mas_empty_area() function for similar bound checking.
    
    Link: https://lkml.kernel.org/r/20230111200136.1851322-1-Liam.Howlett@oracle.com
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=216911
    Fixes: 54a611b60590 ("Maple Tree: add new data structure")
    Signed-off-by: Liam R. Howlett <Liam.Howlett@oracle.com>
    Reported-by: <amanieu@gmail.com>
      Link: https://lore.kernel.org/linux-mm/0b9f5425-08d4-8013-aa4c-e620c3b10bb2@leemhuis.info/
    Tested-by: Holger Hoffsttte <holger@applied-asynchrony.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit 90ca7a874a9093e500cc6147cecd85ad2e6a2852
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Thu Jan 12 12:02:59 2023 +0100

    Linux 6.1.5
    
    Link: https://lore.kernel.org/r/20230110180018.288460217@linuxfoundation.org
    Tested-by: Ronald Warsow <rwarsow@gmx.de>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Florian Fainelli <f.fainelli@gmail.com>
    Tested-by: Shuah Khan <skhan@linuxfoundation.org>
    Tested-by: Takeshi Ogasawara <takeshi.ogasawara@futuring-girl.com>
    Tested-by: Fenil Jain <fkjainco@gmail.com>
    Tested-by: Rudi Heitbaum <rudi@heitbaum.com>
    Tested-by: Jon Hunter <jonathanh@nvidia.com>
    Tested-by: Ron Economos <re@w6rz.net>
    Tested-by: Sudip Mukherjee <sudip.mukherjee@codethink.co.uk>
    Tested-by: Salvatore Bonaccorso <carnil@debian.org>
    Tested-by: Linux Kernel Functional Testing <lkft@linaro.org>
    Tested-by: Allen Pais <apais@linux.microsoft.com>
    Tested-by: Justin M. Forbes <jforbes@fedoraproject.org>
    Tested-by: Conor Dooley <conor.dooley@microchip.com>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Tested-by: Bagas Sanjaya <bagasdotme@gmail.com>
    Tested-by: Kelsey Steele <kelseysteele@linux.microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e90fbe65c6b31ed48a6f13c232b0ca26688218d5
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Sat Oct 29 10:08:36 2022 +0200

    Linux 6.0.6
    
    Link: https://lore.kernel.org/r/20221027165057.208202132@linuxfoundation.org
    Tested-by: Luna Jernberg <droidbittin@gmail.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Justin M. Forbes <jforbes@fedoraproject.org>
    Tested-by: Ronald Warsow <rwarsow@gmx.de>
    Tested-by: Bagas Sanjaya <bagasdotme@gmail.com>=20
    Tested-by: Linux Kernel Functional Testing <lkft@linaro.org>
    Tested-by: Ron Economos <re@w6rz.net>
    Tested-by: Rudi Heitbaum <rudi@heitbaum.com>
    Tested-by: Sudip Mukherjee <sudip.mukherjee@codethink.co.uk>
    Tested-by: Jon Hunter <jonathanh@nvidia.com>
    Tested-by: Florian Fainelli <f.fainelli@gmail.com>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3642ef4d95699193c4a461862382e643ae3720f0
Author: Yu Kuai <yukuai3@huawei.com>
Date:   Wed Oct 19 20:15:16 2022 +0800

    blk-wbt: don't show valid wbt_lat_usec in sysfs while wbt is disabled
    
    Currently, if wbt is initialized and then disabled by
    wbt_disable_default(), sysfs will still show valid wbt_lat_usec, which
    will confuse users that wbt is still enabled.
    
    This patch shows wbt_lat_usec as zero if it's disabled.
    
    Signed-off-by: Yu Kuai <yukuai3@huawei.com>
    Reported-and-tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Link: https://lore.kernel.org/r/20221019121518.3865235-5-yukuai1@huaweicloud.com
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 58df6af8cea3c5377c0220d9fb47cbf85a216f54
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Sep 28 11:32:29 2022 +0200

    Linux 5.19.12
    
    Link: https://lore.kernel.org/r/20220926100806.522017616@linuxfoundation.org
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Ronald Warsow <rwarsow@gmx.de>
    Tested-by: Fenil Jain <fkjainco@gmail.com>
    Tested-by: Justin M. Forbes <jforbes@fedoraproject.org>
    Tested-by: Florian Fainelli <f.fainelli@gmail.com>
    Tested-by: Shuah Khan <skhan@linuxfoundation.org>
    Tested-by: Zan Aziz <zanaziz313@gmail.com>
    Tested-by: Ron Economos <re@w6rz.net>
    Tested-by: Bagas Sanjaya <bagasdotme@gmail.com>
    Tested-by: Linux Kernel Functional Testing <lkft@linaro.org>
    Tested-by: Sudip Mukherjee <sudip.mukherjee@codethink.co.uk>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Tested-by: Jiri Slaby <jirislaby@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8be976a0937a18118424dd2505925081d9192fd5
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 02:00:11 2022 -0600

    mm: multi-gen LRU: design doc
    
    Add a design doc.
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-15-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Barry Song <baohua@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit 07017acb06012d250fb68930e809257e6694d324
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 02:00:10 2022 -0600

    mm: multi-gen LRU: admin guide
    
    Add an admin guide.
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-14-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Acked-by: Mike Rapoport <rppt@linux.ibm.com>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Barry Song <baohua@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit d6c3af7d8a2ba5602c28841248c551a712ac50f5
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 02:00:09 2022 -0600

    mm: multi-gen LRU: debugfs interface
    
    Add /sys/kernel/debug/lru_gen for working set estimation and proactive
    reclaim.  These techniques are commonly used to optimize job scheduling
    (bin packing) in data centers [1][2].
    
    Compared with the page table-based approach and the PFN-based
    approach, this lruvec-based approach has the following advantages:
    1. It offers better choices because it is aware of memcgs, NUMA nodes,
       shared mappings and unmapped page cache.
    2. It is more scalable because it is O(nr_hot_pages), whereas the
       PFN-based approach is O(nr_total_pages).
    
    Add /sys/kernel/debug/lru_gen_full for debugging.
    
    [1] https://dl.acm.org/doi/10.1145/3297858.3304053
    [2] https://dl.acm.org/doi/10.1145/3503222.3507731
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-13-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Reviewed-by: Qi Zheng <zhengqi.arch@bytedance.com>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Barry Song <baohua@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit 1332a809d95a4fc763cabe5ecb6d4fb6a6d941b2
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 02:00:08 2022 -0600

    mm: multi-gen LRU: thrashing prevention
    
    Add /sys/kernel/mm/lru_gen/min_ttl_ms for thrashing prevention, as
    requested by many desktop users [1].
    
    When set to value N, it prevents the working set of N milliseconds from
    getting evicted.  The OOM killer is triggered if this working set cannot
    be kept in memory.  Based on the average human detectable lag (~100ms),
    N=1000 usually eliminates intolerable lags due to thrashing.  Larger
    values like N=3000 make lags less noticeable at the risk of premature OOM
    kills.
    
    Compared with the size-based approach [2], this time-based approach
    has the following advantages:
    
    1. It is easier to configure because it is agnostic to applications
       and memory sizes.
    2. It is more reliable because it is directly wired to the OOM killer.
    
    [1] https://lore.kernel.org/r/Ydza%2FzXKY9ATRoh6@google.com/
    [2] https://lore.kernel.org/r/20101028191523.GA14972@google.com/
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-12-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Barry Song <baohua@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit 354ed597442952fb680c9cafc7e4eb8a76f9514c
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 02:00:07 2022 -0600

    mm: multi-gen LRU: kill switch
    
    Add /sys/kernel/mm/lru_gen/enabled as a kill switch. Components that
    can be disabled include:
      0x0001: the multi-gen LRU core
      0x0002: walking page table, when arch_has_hw_pte_young() returns
              true
      0x0004: clearing the accessed bit in non-leaf PMD entries, when
              CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG=y
      [yYnN]: apply to all the components above
    E.g.,
      echo y >/sys/kernel/mm/lru_gen/enabled
      cat /sys/kernel/mm/lru_gen/enabled
      0x0007
      echo 5 >/sys/kernel/mm/lru_gen/enabled
      cat /sys/kernel/mm/lru_gen/enabled
      0x0005
    
    NB: the page table walks happen on the scale of seconds under heavy memory
    pressure, in which case the mmap_lock contention is a lesser concern,
    compared with the LRU lock contention and the I/O congestion.  So far the
    only well-known case of the mmap_lock contention happens on Android, due
    to Scudo [1] which allocates several thousand VMAs for merely a few
    hundred MBs.  The SPF and the Maple Tree also have provided their own
    assessments [2][3].  However, if walking page tables does worsen the
    mmap_lock contention, the kill switch can be used to disable it.  In this
    case the multi-gen LRU will suffer a minor performance degradation, as
    shown previously.
    
    Clearing the accessed bit in non-leaf PMD entries can also be disabled,
    since this behavior was not tested on x86 varieties other than Intel and
    AMD.
    
    [1] https://source.android.com/devices/tech/debug/scudo
    [2] https://lore.kernel.org/r/20220128131006.67712-1-michel@lespinasse.org/
    [3] https://lore.kernel.org/r/20220426150616.3937571-1-Liam.Howlett@oracle.com/
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-11-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Barry Song <baohua@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit f76c83378851f8e70f032848c4e61203f39480e4
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 02:00:06 2022 -0600

    mm: multi-gen LRU: optimize multiple memcgs
    
    When multiple memcgs are available, it is possible to use generations as a
    frame of reference to make better choices and improve overall performance
    under global memory pressure.  This patch adds a basic optimization to
    select memcgs that can drop single-use unmapped clean pages first.  Doing
    so reduces the chance of going into the aging path or swapping, which can
    be costly.
    
    A typical example that benefits from this optimization is a server running
    mixed types of workloads, e.g., heavy anon workload in one memcg and heavy
    buffered I/O workload in the other.
    
    Though this optimization can be applied to both kswapd and direct reclaim,
    it is only added to kswapd to keep the patchset manageable.  Later
    improvements may cover the direct reclaim path.
    
    While ensuring certain fairness to all eligible memcgs, proportional scans
    of individual memcgs also require proper backoff to avoid overshooting
    their aggregate reclaim target by too much.  Otherwise it can cause high
    direct reclaim latency.  The conditions for backoff are:
    
    1. At low priorities, for direct reclaim, if aging fairness or direct
       reclaim latency is at risk, i.e., aging one memcg multiple times or
       swapping after the target is met.
    2. At high priorities, for global reclaim, if per-zone free pages are
       above respective watermarks.
    
    Server benchmark results:
      Mixed workloads:
        fio (buffered I/O): +[19, 21]%
                    IOPS         BW
          patch1-8: 1880k        7343MiB/s
          patch1-9: 2252k        8796MiB/s
    
        memcached (anon): +[119, 123]%
                    Ops/sec      KB/sec
          patch1-8: 862768.65    33514.68
          patch1-9: 1911022.12   74234.54
    
      Mixed workloads:
        fio (buffered I/O): +[75, 77]%
                    IOPS         BW
          5.19-rc1: 1279k        4996MiB/s
          patch1-9: 2252k        8796MiB/s
    
        memcached (anon): +[13, 15]%
                    Ops/sec      KB/sec
          5.19-rc1: 1673524.04   65008.87
          patch1-9: 1911022.12   74234.54
    
      Configurations:
        (changes since patch 6)
    
        cat mixed.sh
        modprobe brd rd_nr=2 rd_size=56623104
    
        swapoff -a
        mkswap /dev/ram0
        swapon /dev/ram0
    
        mkfs.ext4 /dev/ram1
        mount -t ext4 /dev/ram1 /mnt
    
        memtier_benchmark -S /var/run/memcached/memcached.sock \
          -P memcache_binary -n allkeys --key-minimum=1 \
          --key-maximum=50000000 --key-pattern=P:P -c 1 -t 36 \
          --ratio 1:0 --pipeline 8 -d 2000
    
        fio -name=mglru --numjobs=36 --directory=/mnt --size=1408m \
          --buffered=1 --ioengine=io_uring --iodepth=128 \
          --iodepth_batch_submit=32 --iodepth_batch_complete=32 \
          --rw=randread --random_distribution=random --norandommap \
          --time_based --ramp_time=10m --runtime=90m --group_reporting &
        pid=$!
    
        sleep 200
    
        memtier_benchmark -S /var/run/memcached/memcached.sock \
          -P memcache_binary -n allkeys --key-minimum=1 \
          --key-maximum=50000000 --key-pattern=R:R -c 1 -t 36 \
          --ratio 0:1 --pipeline 8 --randomize --distinct-client-seed
    
        kill -INT $pid
        wait
    
    Client benchmark results:
      no change (CONFIG_MEMCG=n)
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-10-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Barry Song <baohua@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit bd74fdaea146029e4fa12c6de89adbe0779348a9
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 02:00:05 2022 -0600

    mm: multi-gen LRU: support page table walks
    
    To further exploit spatial locality, the aging prefers to walk page tables
    to search for young PTEs and promote hot pages.  A kill switch will be
    added in the next patch to disable this behavior.  When disabled, the
    aging relies on the rmap only.
    
    NB: this behavior has nothing similar with the page table scanning in the
    2.4 kernel [1], which searches page tables for old PTEs, adds cold pages
    to swapcache and unmaps them.
    
    To avoid confusion, the term "iteration" specifically means the traversal
    of an entire mm_struct list; the term "walk" will be applied to page
    tables and the rmap, as usual.
    
    An mm_struct list is maintained for each memcg, and an mm_struct follows
    its owner task to the new memcg when this task is migrated.  Given an
    lruvec, the aging iterates lruvec_memcg()->mm_list and calls
    walk_page_range() with each mm_struct on this list to promote hot pages
    before it increments max_seq.
    
    When multiple page table walkers iterate the same list, each of them gets
    a unique mm_struct; therefore they can run concurrently.  Page table
    walkers ignore any misplaced pages, e.g., if an mm_struct was migrated,
    pages it left in the previous memcg will not be promoted when its current
    memcg is under reclaim.  Similarly, page table walkers will not promote
    pages from nodes other than the one under reclaim.
    
    This patch uses the following optimizations when walking page tables:
    1. It tracks the usage of mm_struct's between context switches so that
       page table walkers can skip processes that have been sleeping since
       the last iteration.
    2. It uses generational Bloom filters to record populated branches so
       that page table walkers can reduce their search space based on the
       query results, e.g., to skip page tables containing mostly holes or
       misplaced pages.
    3. It takes advantage of the accessed bit in non-leaf PMD entries when
       CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG=y.
    4. It does not zigzag between a PGD table and the same PMD table
       spanning multiple VMAs. IOW, it finishes all the VMAs within the
       range of the same PMD table before it returns to a PGD table. This
       improves the cache performance for workloads that have large
       numbers of tiny VMAs [2], especially when CONFIG_PGTABLE_LEVELS=5.
    
    Server benchmark results:
      Single workload:
        fio (buffered I/O): no change
    
      Single workload:
        memcached (anon): +[8, 10]%
                    Ops/sec      KB/sec
          patch1-7: 1147696.57   44640.29
          patch1-8: 1245274.91   48435.66
    
      Configurations:
        no change
    
    Client benchmark results:
      kswapd profiles:
        patch1-7
          48.16%  lzo1x_1_do_compress (real work)
           8.20%  page_vma_mapped_walk (overhead)
           7.06%  _raw_spin_unlock_irq
           2.92%  ptep_clear_flush
           2.53%  __zram_bvec_write
           2.11%  do_raw_spin_lock
           2.02%  memmove
           1.93%  lru_gen_look_around
           1.56%  free_unref_page_list
           1.40%  memset
    
        patch1-8
          49.44%  lzo1x_1_do_compress (real work)
           6.19%  page_vma_mapped_walk (overhead)
           5.97%  _raw_spin_unlock_irq
           3.13%  get_pfn_folio
           2.85%  ptep_clear_flush
           2.42%  __zram_bvec_write
           2.08%  do_raw_spin_lock
           1.92%  memmove
           1.44%  alloc_zspage
           1.36%  memset
    
      Configurations:
        no change
    
    Thanks to the following developers for their efforts [3].
      kernel test robot <lkp@intel.com>
    
    [1] https://lwn.net/Articles/23732/
    [2] https://llvm.org/docs/ScudoHardenedAllocator.html
    [3] https://lore.kernel.org/r/202204160827.ekEARWQo-lkp@intel.com/
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-9-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Barry Song <baohua@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit 018ee47f14893d500131dfca2ff9f3ff8ebd4ed2
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 02:00:04 2022 -0600

    mm: multi-gen LRU: exploit locality in rmap
    
    Searching the rmap for PTEs mapping each page on an LRU list (to test and
    clear the accessed bit) can be expensive because pages from different VMAs
    (PA space) are not cache friendly to the rmap (VA space).  For workloads
    mostly using mapped pages, searching the rmap can incur the highest CPU
    cost in the reclaim path.
    
    This patch exploits spatial locality to reduce the trips into the rmap.
    When shrink_page_list() walks the rmap and finds a young PTE, a new
    function lru_gen_look_around() scans at most BITS_PER_LONG-1 adjacent
    PTEs.  On finding another young PTE, it clears the accessed bit and
    updates the gen counter of the page mapped by this PTE to
    (max_seq%MAX_NR_GENS)+1.
    
    Server benchmark results:
      Single workload:
        fio (buffered I/O): no change
    
      Single workload:
        memcached (anon): +[3, 5]%
                    Ops/sec      KB/sec
          patch1-6: 1106168.46   43025.04
          patch1-7: 1147696.57   44640.29
    
      Configurations:
        no change
    
    Client benchmark results:
      kswapd profiles:
        patch1-6
          39.03%  lzo1x_1_do_compress (real work)
          18.47%  page_vma_mapped_walk (overhead)
           6.74%  _raw_spin_unlock_irq
           3.97%  do_raw_spin_lock
           2.49%  ptep_clear_flush
           2.48%  anon_vma_interval_tree_iter_first
           1.92%  folio_referenced_one
           1.88%  __zram_bvec_write
           1.48%  memmove
           1.31%  vma_interval_tree_iter_next
    
        patch1-7
          48.16%  lzo1x_1_do_compress (real work)
           8.20%  page_vma_mapped_walk (overhead)
           7.06%  _raw_spin_unlock_irq
           2.92%  ptep_clear_flush
           2.53%  __zram_bvec_write
           2.11%  do_raw_spin_lock
           2.02%  memmove
           1.93%  lru_gen_look_around
           1.56%  free_unref_page_list
           1.40%  memset
    
      Configurations:
        no change
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-8-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Acked-by: Barry Song <baohua@kernel.org>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit ac35a490237446b71e3b4b782b1596967edd0aa8
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 02:00:03 2022 -0600

    mm: multi-gen LRU: minimal implementation
    
    To avoid confusion, the terms "promotion" and "demotion" will be applied
    to the multi-gen LRU, as a new convention; the terms "activation" and
    "deactivation" will be applied to the active/inactive LRU, as usual.
    
    The aging produces young generations.  Given an lruvec, it increments
    max_seq when max_seq-min_seq+1 approaches MIN_NR_GENS.  The aging promotes
    hot pages to the youngest generation when it finds them accessed through
    page tables; the demotion of cold pages happens consequently when it
    increments max_seq.  Promotion in the aging path does not involve any LRU
    list operations, only the updates of the gen counter and
    lrugen->nr_pages[]; demotion, unless as the result of the increment of
    max_seq, requires LRU list operations, e.g., lru_deactivate_fn().  The
    aging has the complexity O(nr_hot_pages), since it is only interested in
    hot pages.
    
    The eviction consumes old generations.  Given an lruvec, it increments
    min_seq when lrugen->lists[] indexed by min_seq%MAX_NR_GENS becomes empty.
    A feedback loop modeled after the PID controller monitors refaults over
    anon and file types and decides which type to evict when both types are
    available from the same generation.
    
    The protection of pages accessed multiple times through file descriptors
    takes place in the eviction path.  Each generation is divided into
    multiple tiers.  A page accessed N times through file descriptors is in
    tier order_base_2(N).  Tiers do not have dedicated lrugen->lists[], only
    bits in folio->flags.  The aforementioned feedback loop also monitors
    refaults over all tiers and decides when to protect pages in which tiers
    (N>1), using the first tier (N=0,1) as a baseline.  The first tier
    contains single-use unmapped clean pages, which are most likely the best
    choices.  In contrast to promotion in the aging path, the protection of a
    page in the eviction path is achieved by moving this page to the next
    generation, i.e., min_seq+1, if the feedback loop decides so.  This
    approach has the following advantages:
    
    1. It removes the cost of activation in the buffered access path by
       inferring whether pages accessed multiple times through file
       descriptors are statistically hot and thus worth protecting in the
       eviction path.
    2. It takes pages accessed through page tables into account and avoids
       overprotecting pages accessed multiple times through file
       descriptors. (Pages accessed through page tables are in the first
       tier, since N=0.)
    3. More tiers provide better protection for pages accessed more than
       twice through file descriptors, when under heavy buffered I/O
       workloads.
    
    Server benchmark results:
      Single workload:
        fio (buffered I/O): +[30, 32]%
                    IOPS         BW
          5.19-rc1: 2673k        10.2GiB/s
          patch1-6: 3491k        13.3GiB/s
    
      Single workload:
        memcached (anon): -[4, 6]%
                    Ops/sec      KB/sec
          5.19-rc1: 1161501.04   45177.25
          patch1-6: 1106168.46   43025.04
    
      Configurations:
        CPU: two Xeon 6154
        Mem: total 256G
    
        Node 1 was only used as a ram disk to reduce the variance in the
        results.
    
        patch drivers/block/brd.c <<EOF
        99,100c99,100
        <   gfp_flags = GFP_NOIO | __GFP_ZERO | __GFP_HIGHMEM;
        <   page = alloc_page(gfp_flags);
        ---
        >   gfp_flags = GFP_NOIO | __GFP_ZERO | __GFP_HIGHMEM | __GFP_THISNODE;
        >   page = alloc_pages_node(1, gfp_flags, 0);
        EOF
    
        cat >>/etc/systemd/system.conf <<EOF
        CPUAffinity=numa
        NUMAPolicy=bind
        NUMAMask=0
        EOF
    
        cat >>/etc/memcached.conf <<EOF
        -m 184320
        -s /var/run/memcached/memcached.sock
        -a 0766
        -t 36
        -B binary
        EOF
    
        cat fio.sh
        modprobe brd rd_nr=1 rd_size=113246208
        swapoff -a
        mkfs.ext4 /dev/ram0
        mount -t ext4 /dev/ram0 /mnt
    
        mkdir /sys/fs/cgroup/user.slice/test
        echo 38654705664 >/sys/fs/cgroup/user.slice/test/memory.max
        echo $$ >/sys/fs/cgroup/user.slice/test/cgroup.procs
        fio -name=mglru --numjobs=72 --directory=/mnt --size=1408m \
          --buffered=1 --ioengine=io_uring --iodepth=128 \
          --iodepth_batch_submit=32 --iodepth_batch_complete=32 \
          --rw=randread --random_distribution=random --norandommap \
          --time_based --ramp_time=10m --runtime=5m --group_reporting
    
        cat memcached.sh
        modprobe brd rd_nr=1 rd_size=113246208
        swapoff -a
        mkswap /dev/ram0
        swapon /dev/ram0
    
        memtier_benchmark -S /var/run/memcached/memcached.sock \
          -P memcache_binary -n allkeys --key-minimum=1 \
          --key-maximum=65000000 --key-pattern=P:P -c 1 -t 36 \
          --ratio 1:0 --pipeline 8 -d 2000
    
        memtier_benchmark -S /var/run/memcached/memcached.sock \
          -P memcache_binary -n allkeys --key-minimum=1 \
          --key-maximum=65000000 --key-pattern=R:R -c 1 -t 36 \
          --ratio 0:1 --pipeline 8 --randomize --distinct-client-seed
    
    Client benchmark results:
      kswapd profiles:
        5.19-rc1
          40.33%  page_vma_mapped_walk (overhead)
          21.80%  lzo1x_1_do_compress (real work)
           7.53%  do_raw_spin_lock
           3.95%  _raw_spin_unlock_irq
           2.52%  vma_interval_tree_iter_next
           2.37%  folio_referenced_one
           2.28%  vma_interval_tree_subtree_search
           1.97%  anon_vma_interval_tree_iter_first
           1.60%  ptep_clear_flush
           1.06%  __zram_bvec_write
    
        patch1-6
          39.03%  lzo1x_1_do_compress (real work)
          18.47%  page_vma_mapped_walk (overhead)
           6.74%  _raw_spin_unlock_irq
           3.97%  do_raw_spin_lock
           2.49%  ptep_clear_flush
           2.48%  anon_vma_interval_tree_iter_first
           1.92%  folio_referenced_one
           1.88%  __zram_bvec_write
           1.48%  memmove
           1.31%  vma_interval_tree_iter_next
    
      Configurations:
        CPU: single Snapdragon 7c
        Mem: total 4G
    
        ChromeOS MemoryPressure [1]
    
    [1] https://chromium.googlesource.com/chromiumos/platform/tast-tests/
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-7-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Barry Song <baohua@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit ec1c86b25f4bdd9dce6436c0539d2a6ae676e1c4
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 02:00:02 2022 -0600

    mm: multi-gen LRU: groundwork
    
    Evictable pages are divided into multiple generations for each lruvec.
    The youngest generation number is stored in lrugen->max_seq for both
    anon and file types as they are aged on an equal footing. The oldest
    generation numbers are stored in lrugen->min_seq[] separately for anon
    and file types as clean file pages can be evicted regardless of swap
    constraints. These three variables are monotonically increasing.
    
    Generation numbers are truncated into order_base_2(MAX_NR_GENS+1) bits
    in order to fit into the gen counter in folio->flags. Each truncated
    generation number is an index to lrugen->lists[]. The sliding window
    technique is used to track at least MIN_NR_GENS and at most
    MAX_NR_GENS generations. The gen counter stores a value within [1,
    MAX_NR_GENS] while a page is on one of lrugen->lists[]. Otherwise it
    stores 0.
    
    There are two conceptually independent procedures: "the aging", which
    produces young generations, and "the eviction", which consumes old
    generations.  They form a closed-loop system, i.e., "the page reclaim".
    Both procedures can be invoked from userspace for the purposes of working
    set estimation and proactive reclaim.  These techniques are commonly used
    to optimize job scheduling (bin packing) in data centers [1][2].
    
    To avoid confusion, the terms "hot" and "cold" will be applied to the
    multi-gen LRU, as a new convention; the terms "active" and "inactive" will
    be applied to the active/inactive LRU, as usual.
    
    The protection of hot pages and the selection of cold pages are based
    on page access channels and patterns. There are two access channels:
    one through page tables and the other through file descriptors. The
    protection of the former channel is by design stronger because:
    1. The uncertainty in determining the access patterns of the former
       channel is higher due to the approximation of the accessed bit.
    2. The cost of evicting the former channel is higher due to the TLB
       flushes required and the likelihood of encountering the dirty bit.
    3. The penalty of underprotecting the former channel is higher because
       applications usually do not prepare themselves for major page
       faults like they do for blocked I/O. E.g., GUI applications
       commonly use dedicated I/O threads to avoid blocking rendering
       threads.
    
    There are also two access patterns: one with temporal locality and the
    other without.  For the reasons listed above, the former channel is
    assumed to follow the former pattern unless VM_SEQ_READ or VM_RAND_READ is
    present; the latter channel is assumed to follow the latter pattern unless
    outlying refaults have been observed [3][4].
    
    The next patch will address the "outlying refaults".  Three macros, i.e.,
    LRU_REFS_WIDTH, LRU_REFS_PGOFF and LRU_REFS_MASK, used later are added in
    this patch to make the entire patchset less diffy.
    
    A page is added to the youngest generation on faulting.  The aging needs
    to check the accessed bit at least twice before handing this page over to
    the eviction.  The first check takes care of the accessed bit set on the
    initial fault; the second check makes sure this page has not been used
    since then.  This protocol, AKA second chance, requires a minimum of two
    generations, hence MIN_NR_GENS.
    
    [1] https://dl.acm.org/doi/10.1145/3297858.3304053
    [2] https://dl.acm.org/doi/10.1145/3503222.3507731
    [3] https://lwn.net/Articles/495543/
    [4] https://lwn.net/Articles/815342/
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-6-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Barry Song <baohua@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit aa1b67903a19e026d1749241fad177f6185c2d42
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 02:00:01 2022 -0600

    Revert "include/linux/mm_inline.h: fold __update_lru_size() into its sole caller"
    
    This patch undoes the following refactor: commit 289ccba18af4
    ("include/linux/mm_inline.h: fold __update_lru_size() into its sole
    caller")
    
    The upcoming changes to include/linux/mm_inline.h will reuse
    __update_lru_size().
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-5-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Reviewed-by: Miaohe Lin <linmiaohe@huawei.com>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Barry Song <baohua@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit f1e1a7be4718609042e3285bc2110d74825ad9d1
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 02:00:00 2022 -0600

    mm/vmscan.c: refactor shrink_node()
    
    This patch refactors shrink_node() to improve readability for the upcoming
    changes to mm/vmscan.c.
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-4-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Reviewed-by: Barry Song <baohua@kernel.org>
    Reviewed-by: Miaohe Lin <linmiaohe@huawei.com>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit eed9a328aa1ae6ac1edaa026957e6882f57de0dd
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 01:59:59 2022 -0600

    mm: x86: add CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG
    
    Some architectures support the accessed bit in non-leaf PMD entries, e.g.,
    x86 sets the accessed bit in a non-leaf PMD entry when using it as part of
    linear address translation [1].  Page table walkers that clear the
    accessed bit may use this capability to reduce their search space.
    
    Note that:
    1. Although an inline function is preferable, this capability is added
       as a configuration option for consistency with the existing macros.
    2. Due to the little interest in other varieties, this capability was
       only tested on Intel and AMD CPUs.
    
    Thanks to the following developers for their efforts [2][3].
      Randy Dunlap <rdunlap@infradead.org>
      Stephen Rothwell <sfr@canb.auug.org.au>
    
    [1]: Intel 64 and IA-32 Architectures Software Developer's Manual
         Volume 3 (June 2021), section 4.8
    [2] https://lore.kernel.org/r/bfdcc7c8-922f-61a9-aa15-7e7250f04af7@infradead.org/
    [3] https://lore.kernel.org/r/20220413151513.5a0d7a7e@canb.auug.org.au/
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-3-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Reviewed-by: Barry Song <baohua@kernel.org>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit e1fd09e3d1dd4a1a8b3b33bc1fd647eee9f4e475
Author: Yu Zhao <yuzhao@google.com>
Date:   Sun Sep 18 01:59:58 2022 -0600

    mm: x86, arm64: add arch_has_hw_pte_young()
    
    Patch series "Multi-Gen LRU Framework", v14.
    
    What's new
    ==========
    1. OpenWrt, in addition to Android, Arch Linux Zen, Armbian, ChromeOS,
       Liquorix, post-factum and XanMod, is now shipping MGLRU on 5.15.
    2. Fixed long-tailed direct reclaim latency seen on high-memory (TBs)
       machines. The old direct reclaim backoff, which tries to enforce a
       minimum fairness among all eligible memcgs, over-swapped by about
       (total_mem>>DEF_PRIORITY)-nr_to_reclaim. The new backoff, which
       pulls the plug on swapping once the target is met, trades some
       fairness for curtailed latency:
       https://lore.kernel.org/r/20220918080010.2920238-10-yuzhao@google.com/
    3. Fixed minior build warnings and conflicts. More comments and nits.
    
    TLDR
    ====
    The current page reclaim is too expensive in terms of CPU usage and it
    often makes poor choices about what to evict. This patchset offers an
    alternative solution that is performant, versatile and
    straightforward.
    
    Patchset overview
    =================
    The design and implementation overview is in patch 14:
    https://lore.kernel.org/r/20220918080010.2920238-15-yuzhao@google.com/
    
    01. mm: x86, arm64: add arch_has_hw_pte_young()
    02. mm: x86: add CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG
    Take advantage of hardware features when trying to clear the accessed
    bit in many PTEs.
    
    03. mm/vmscan.c: refactor shrink_node()
    04. Revert "include/linux/mm_inline.h: fold __update_lru_size() into
        its sole caller"
    Minor refactors to improve readability for the following patches.
    
    05. mm: multi-gen LRU: groundwork
    Adds the basic data structure and the functions that insert pages to
    and remove pages from the multi-gen LRU (MGLRU) lists.
    
    06. mm: multi-gen LRU: minimal implementation
    A minimal implementation without optimizations.
    
    07. mm: multi-gen LRU: exploit locality in rmap
    Exploits spatial locality to improve efficiency when using the rmap.
    
    08. mm: multi-gen LRU: support page table walks
    Further exploits spatial locality by optionally scanning page tables.
    
    09. mm: multi-gen LRU: optimize multiple memcgs
    Optimizes the overall performance for multiple memcgs running mixed
    types of workloads.
    
    10. mm: multi-gen LRU: kill switch
    Adds a kill switch to enable or disable MGLRU at runtime.
    
    11. mm: multi-gen LRU: thrashing prevention
    12. mm: multi-gen LRU: debugfs interface
    Provide userspace with features like thrashing prevention, working set
    estimation and proactive reclaim.
    
    13. mm: multi-gen LRU: admin guide
    14. mm: multi-gen LRU: design doc
    Add an admin guide and a design doc.
    
    Benchmark results
    =================
    Independent lab results
    -----------------------
    Based on the popularity of searches [01] and the memory usage in
    Google's public cloud, the most popular open-source memory-hungry
    applications, in alphabetical order, are:
          Apache Cassandra      Memcached
          Apache Hadoop         MongoDB
          Apache Spark          PostgreSQL
          MariaDB (MySQL)       Redis
    
    An independent lab evaluated MGLRU with the most widely used benchmark
    suites for the above applications. They posted 960 data points along
    with kernel metrics and perf profiles collected over more than 500
    hours of total benchmark time. Their final reports show that, with 95%
    confidence intervals (CIs), the above applications all performed
    significantly better for at least part of their benchmark matrices.
    
    On 5.14:
    1. Apache Spark [02] took 95% CIs [9.28, 11.19]% and [12.20, 14.93]%
       less wall time to sort three billion random integers, respectively,
       under the medium- and the high-concurrency conditions, when
       overcommitting memory. There were no statistically significant
       changes in wall time for the rest of the benchmark matrix.
    2. MariaDB [03] achieved 95% CIs [5.24, 10.71]% and [20.22, 25.97]%
       more transactions per minute (TPM), respectively, under the medium-
       and the high-concurrency conditions, when overcommitting memory.
       There were no statistically significant changes in TPM for the rest
       of the benchmark matrix.
    3. Memcached [04] achieved 95% CIs [23.54, 32.25]%, [20.76, 41.61]%
       and [21.59, 30.02]% more operations per second (OPS), respectively,
       for sequential access, random access and Gaussian (distribution)
       access, when THP=always; 95% CIs [13.85, 15.97]% and
       [23.94, 29.92]% more OPS, respectively, for random access and
       Gaussian access, when THP=never. There were no statistically
       significant changes in OPS for the rest of the benchmark matrix.
    4. MongoDB [05] achieved 95% CIs [2.23, 3.44]%, [6.97, 9.73]% and
       [2.16, 3.55]% more operations per second (OPS), respectively, for
       exponential (distribution) access, random access and Zipfian
       (distribution) access, when underutilizing memory; 95% CIs
       [8.83, 10.03]%, [21.12, 23.14]% and [5.53, 6.46]% more OPS,
       respectively, for exponential access, random access and Zipfian
       access, when overcommitting memory.
    
    On 5.15:
    5. Apache Cassandra [06] achieved 95% CIs [1.06, 4.10]%, [1.94, 5.43]%
       and [4.11, 7.50]% more operations per second (OPS), respectively,
       for exponential (distribution) access, random access and Zipfian
       (distribution) access, when swap was off; 95% CIs [0.50, 2.60]%,
       [6.51, 8.77]% and [3.29, 6.75]% more OPS, respectively, for
       exponential access, random access and Zipfian access, when swap was
       on.
    6. Apache Hadoop [07] took 95% CIs [5.31, 9.69]% and [2.02, 7.86]%
       less average wall time to finish twelve parallel TeraSort jobs,
       respectively, under the medium- and the high-concurrency
       conditions, when swap was on. There were no statistically
       significant changes in average wall time for the rest of the
       benchmark matrix.
    7. PostgreSQL [08] achieved 95% CI [1.75, 6.42]% more transactions per
       minute (TPM) under the high-concurrency condition, when swap was
       off; 95% CIs [12.82, 18.69]% and [22.70, 46.86]% more TPM,
       respectively, under the medium- and the high-concurrency
       conditions, when swap was on. There were no statistically
       significant changes in TPM for the rest of the benchmark matrix.
    8. Redis [09] achieved 95% CIs [0.58, 5.94]%, [6.55, 14.58]% and
       [11.47, 19.36]% more total operations per second (OPS),
       respectively, for sequential access, random access and Gaussian
       (distribution) access, when THP=always; 95% CIs [1.27, 3.54]%,
       [10.11, 14.81]% and [8.75, 13.64]% more total OPS, respectively,
       for sequential access, random access and Gaussian access, when
       THP=never.
    
    Our lab results
    ---------------
    To supplement the above results, we ran the following benchmark suites
    on 5.16-rc7 and found no regressions [10].
          fs_fio_bench_hdd_mq      pft
          fs_lmbench               pgsql-hammerdb
          fs_parallelio            redis
          fs_postmark              stream
          hackbench                sysbenchthread
          kernbench                tpcc_spark
          memcached                unixbench
          multichase               vm-scalability
          mutilate                 will-it-scale
          nginx
    
    [01] https://trends.google.com
    [02] https://lore.kernel.org/r/20211102002002.92051-1-bot@edi.works/
    [03] https://lore.kernel.org/r/20211009054315.47073-1-bot@edi.works/
    [04] https://lore.kernel.org/r/20211021194103.65648-1-bot@edi.works/
    [05] https://lore.kernel.org/r/20211109021346.50266-1-bot@edi.works/
    [06] https://lore.kernel.org/r/20211202062806.80365-1-bot@edi.works/
    [07] https://lore.kernel.org/r/20211209072416.33606-1-bot@edi.works/
    [08] https://lore.kernel.org/r/20211218071041.24077-1-bot@edi.works/
    [09] https://lore.kernel.org/r/20211122053248.57311-1-bot@edi.works/
    [10] https://lore.kernel.org/r/20220104202247.2903702-1-yuzhao@google.com/
    
    Read-world applications
    =======================
    Third-party testimonials
    ------------------------
    Konstantin reported [11]:
       I have Archlinux with 8G RAM + zswap + swap. While developing, I
       have lots of apps opened such as multiple LSP-servers for different
       langs, chats, two browsers, etc... Usually, my system gets quickly
       to a point of SWAP-storms, where I have to kill LSP-servers,
       restart browsers to free memory, etc, otherwise the system lags
       heavily and is barely usable.
    
       1.5 day ago I migrated from 5.11.15 kernel to 5.12 + the LRU
       patchset, and I started up by opening lots of apps to create memory
       pressure, and worked for a day like this. Till now I had not a
       single SWAP-storm, and mind you I got 3.4G in SWAP. I was never
       getting to the point of 3G in SWAP before without a single
       SWAP-storm.
    
    Vaibhav from IBM reported [12]:
       In a synthetic MongoDB Benchmark, seeing an average of ~19%
       throughput improvement on POWER10(Radix MMU + 64K Page Size) with
       MGLRU patches on top of 5.16 kernel for MongoDB + YCSB across
       three different request distributions, namely, Exponential, Uniform
       and Zipfan.
    
    Shuang from U of Rochester reported [13]:
       With the MGLRU, fio achieved 95% CIs [38.95, 40.26]%, [4.12, 6.64]%
       and [9.26, 10.36]% higher throughput, respectively, for random
       access, Zipfian (distribution) access and Gaussian (distribution)
       access, when the average number of jobs per CPU is 1; 95% CIs
       [42.32, 49.15]%, [9.44, 9.89]% and [20.99, 22.86]% higher
       throughput, respectively, for random access, Zipfian access and
       Gaussian access, when the average number of jobs per CPU is 2.
    
    Daniel from Michigan Tech reported [14]:
       With Memcached allocating ~100GB of byte-addressable Optante,
       performance improvement in terms of throughput (measured as queries
       per second) was about 10% for a series of workloads.
    
    Large-scale deployments
    -----------------------
    We've rolled out MGLRU to tens of millions of ChromeOS users and
    about a million Android users. Google's fleetwide profiling [15] shows
    an overall 40% decrease in kswapd CPU usage, in addition to
    improvements in other UX metrics, e.g., an 85% decrease in the number
    of low-memory kills at the 75th percentile and an 18% decrease in
    app launch time at the 50th percentile.
    
    The downstream kernels that have been using MGLRU include:
    1. Android [16]
    2. Arch Linux Zen [17]
    3. Armbian [18]
    4. ChromeOS [19]
    5. Liquorix [20]
    6. OpenWrt [21]
    7. post-factum [22]
    8. XanMod [23]
    
    [11] https://lore.kernel.org/r/140226722f2032c86301fbd326d91baefe3d7d23.camel@yandex.ru/
    [12] https://lore.kernel.org/r/87czj3mux0.fsf@vajain21.in.ibm.com/
    [13] https://lore.kernel.org/r/20220105024423.26409-1-szhai2@cs.rochester.edu/
    [14] https://lore.kernel.org/r/CA+4-3vksGvKd18FgRinxhqHetBS1hQekJE2gwco8Ja-bJWKtFw@mail.gmail.com/
    [15] https://dl.acm.org/doi/10.1145/2749469.2750392
    [16] https://android.com
    [17] https://archlinux.org
    [18] https://armbian.com
    [19] https://chromium.org
    [20] https://liquorix.net
    [21] https://openwrt.org
    [22] https://codeberg.org/pf-kernel
    [23] https://xanmod.org
    
    Summary
    =======
    The facts are:
    1. The independent lab results and the real-world applications
       indicate substantial improvements; there are no known regressions.
    2. Thrashing prevention, working set estimation and proactive reclaim
       work out of the box; there are no equivalent solutions.
    3. There is a lot of new code; no smaller changes have been
       demonstrated similar effects.
    
    Our options, accordingly, are:
    1. Given the amount of evidence, the reported improvements will likely
       materialize for a wide range of workloads.
    2. Gauging the interest from the past discussions, the new features
       will likely be put to use for both personal computers and data
       centers.
    3. Based on Google's track record, the new code will likely be well
       maintained in the long term. It'd be more difficult if not
       impossible to achieve similar effects with other approaches.
    
    
    This patch (of 14):
    
    Some architectures automatically set the accessed bit in PTEs, e.g., x86
    and arm64 v8.2.  On architectures that do not have this capability,
    clearing the accessed bit in a PTE usually triggers a page fault following
    the TLB miss of this PTE (to emulate the accessed bit).
    
    Being aware of this capability can help make better decisions, e.g.,
    whether to spread the work out over a period of time to reduce bursty page
    faults when trying to clear the accessed bit in many PTEs.
    
    Note that theoretically this capability can be unreliable, e.g.,
    hotplugged CPUs might be different from builtin ones.  Therefore it should
    not be used in architecture-independent code that involves correctness,
    e.g., to determine whether TLB flushes are required (in combination with
    the accessed bit).
    
    Link: https://lkml.kernel.org/r/20220918080010.2920238-1-yuzhao@google.com
    Link: https://lkml.kernel.org/r/20220918080010.2920238-2-yuzhao@google.com
    Signed-off-by: Yu Zhao <yuzhao@google.com>
    Reviewed-by: Barry Song <baohua@kernel.org>
    Acked-by: Brian Geffon <bgeffon@google.com>
    Acked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>
    Acked-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Acked-by: Steven Barrett <steven@liquorix.net>
    Acked-by: Suleiman Souhlal <suleiman@google.com>
    Acked-by: Will Deacon <will@kernel.org>
    Tested-by: Daniel Byrne <djbyrne@mtu.edu>
    Tested-by: Donald Carr <d@chaos-reins.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>
    Tested-by: Shuang Zhai <szhai2@cs.rochester.edu>
    Tested-by: Sofia Trinh <sofia.trinh@edi.works>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michael Larabel <Michael@MichaelLarabel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Miaohe Lin <linmiaohe@huawei.com>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit fb45f24264b94e8c37657789dbb1966a07933d12
Author: Manuel Ullmann <labre@posteo.de>
Date:   Wed May 4 21:30:44 2022 +0200

    net: atlantic: always deep reset on pm op, fixing up my null deref regression
    
    commit 1809c30b6e5a83a1de1435fe01aaa4de4d626a7c upstream.
    
    The impact of this regression is the same for resume that I saw on
    thaw: the kernel hangs and nothing except SysRq rebooting can be done.
    
    Fixes regression in commit cbe6c3a8f8f4 ("net: atlantic: invert deep
    par in pm functions, preventing null derefs"), where I disabled deep
    pm resets in suspend and resume, trying to make sense of the
    atl_resume_common() deep parameter in the first place.
    
    It turns out, that atlantic always has to deep reset on pm
    operations. Even though I expected that and tested resume, I screwed
    up by kexec-rebooting into an unpatched kernel, thus missing the
    breakage.
    
    This fixup obsoletes the deep parameter of atl_resume_common, but I
    leave the cleanup for the maintainers to post to mainline.
    
    Suspend and hibernation were successfully tested by the reporters.
    
    Fixes: cbe6c3a8f8f4 ("net: atlantic: invert deep par in pm functions, preventing null derefs")
    Link: https://lore.kernel.org/regressions/9-Ehc_xXSwdXcvZqKD5aSqsqeNj5Izco4MYEwnx5cySXVEc9-x_WC4C3kAoCqNTi-H38frroUK17iobNVnkLtW36V6VWGSQEOHXhmVMm5iQ=@protonmail.com/
    Reported-by: Jordan Leppert <jordanleppert@protonmail.com>
    Reported-by: Holger Hoffstaette <holger@applied-asynchrony.com>
    Tested-by: Jordan Leppert <jordanleppert@protonmail.com>
    Tested-by: Holger Hoffstaette <holger@applied-asynchrony.com>
    CC: <stable@vger.kernel.org> # 5.10+
    Signed-off-by: Manuel Ullmann <labre@posteo.de>
    Link: https://lore.kernel.org/r/87bkw8dfmp.fsf@posteo.de
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 890a5c6d19817a9db5c981174737d1bc9b6fb1a1
Author: Manuel Ullmann <labre@posteo.de>
Date:   Wed May 4 21:30:44 2022 +0200

    net: atlantic: always deep reset on pm op, fixing up my null deref regression
    
    commit 1809c30b6e5a83a1de1435fe01aaa4de4d626a7c upstream.
    
    The impact of this regression is the same for resume that I saw on
    thaw: the kernel hangs and nothing except SysRq rebooting can be done.
    
    Fixes regression in commit cbe6c3a8f8f4 ("net: atlantic: invert deep
    par in pm functions, preventing null derefs"), where I disabled deep
    pm resets in suspend and resume, trying to make sense of the
    atl_resume_common() deep parameter in the first place.
    
    It turns out, that atlantic always has to deep reset on pm
    operations. Even though I expected that and tested resume, I screwed
    up by kexec-rebooting into an unpatched kernel, thus missing the
    breakage.
    
    This fixup obsoletes the deep parameter of atl_resume_common, but I
    leave the cleanup for the maintainers to post to mainline.
    
    Suspend and hibernation were successfully tested by the reporters.
    
    Fixes: cbe6c3a8f8f4 ("net: atlantic: invert deep par in pm functions, preventing null derefs")
    Link: https://lore.kernel.org/regressions/9-Ehc_xXSwdXcvZqKD5aSqsqeNj5Izco4MYEwnx5cySXVEc9-x_WC4C3kAoCqNTi-H38frroUK17iobNVnkLtW36V6VWGSQEOHXhmVMm5iQ=@protonmail.com/
    Reported-by: Jordan Leppert <jordanleppert@protonmail.com>
    Reported-by: Holger Hoffstaette <holger@applied-asynchrony.com>
    Tested-by: Jordan Leppert <jordanleppert@protonmail.com>
    Tested-by: Holger Hoffstaette <holger@applied-asynchrony.com>
    CC: <stable@vger.kernel.org> # 5.10+
    Signed-off-by: Manuel Ullmann <labre@posteo.de>
    Link: https://lore.kernel.org/r/87bkw8dfmp.fsf@posteo.de
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6aa239d82e35b9e3eb8e8c7750fd5caa377e1e15
Author: Manuel Ullmann <labre@posteo.de>
Date:   Wed May 4 21:30:44 2022 +0200

    net: atlantic: always deep reset on pm op, fixing up my null deref regression
    
    commit 1809c30b6e5a83a1de1435fe01aaa4de4d626a7c upstream.
    
    The impact of this regression is the same for resume that I saw on
    thaw: the kernel hangs and nothing except SysRq rebooting can be done.
    
    Fixes regression in commit cbe6c3a8f8f4 ("net: atlantic: invert deep
    par in pm functions, preventing null derefs"), where I disabled deep
    pm resets in suspend and resume, trying to make sense of the
    atl_resume_common() deep parameter in the first place.
    
    It turns out, that atlantic always has to deep reset on pm
    operations. Even though I expected that and tested resume, I screwed
    up by kexec-rebooting into an unpatched kernel, thus missing the
    breakage.
    
    This fixup obsoletes the deep parameter of atl_resume_common, but I
    leave the cleanup for the maintainers to post to mainline.
    
    Suspend and hibernation were successfully tested by the reporters.
    
    Fixes: cbe6c3a8f8f4 ("net: atlantic: invert deep par in pm functions, preventing null derefs")
    Link: https://lore.kernel.org/regressions/9-Ehc_xXSwdXcvZqKD5aSqsqeNj5Izco4MYEwnx5cySXVEc9-x_WC4C3kAoCqNTi-H38frroUK17iobNVnkLtW36V6VWGSQEOHXhmVMm5iQ=@protonmail.com/
    Reported-by: Jordan Leppert <jordanleppert@protonmail.com>
    Reported-by: Holger Hoffstaette <holger@applied-asynchrony.com>
    Tested-by: Jordan Leppert <jordanleppert@protonmail.com>
    Tested-by: Holger Hoffstaette <holger@applied-asynchrony.com>
    CC: <stable@vger.kernel.org> # 5.10+
    Signed-off-by: Manuel Ullmann <labre@posteo.de>
    Link: https://lore.kernel.org/r/87bkw8dfmp.fsf@posteo.de
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1809c30b6e5a83a1de1435fe01aaa4de4d626a7c
Author: Manuel Ullmann <labre@posteo.de>
Date:   Wed May 4 21:30:44 2022 +0200

    net: atlantic: always deep reset on pm op, fixing up my null deref regression
    
    The impact of this regression is the same for resume that I saw on
    thaw: the kernel hangs and nothing except SysRq rebooting can be done.
    
    Fixes regression in commit cbe6c3a8f8f4 ("net: atlantic: invert deep
    par in pm functions, preventing null derefs"), where I disabled deep
    pm resets in suspend and resume, trying to make sense of the
    atl_resume_common() deep parameter in the first place.
    
    It turns out, that atlantic always has to deep reset on pm
    operations. Even though I expected that and tested resume, I screwed
    up by kexec-rebooting into an unpatched kernel, thus missing the
    breakage.
    
    This fixup obsoletes the deep parameter of atl_resume_common, but I
    leave the cleanup for the maintainers to post to mainline.
    
    Suspend and hibernation were successfully tested by the reporters.
    
    Fixes: cbe6c3a8f8f4 ("net: atlantic: invert deep par in pm functions, preventing null derefs")
    Link: https://lore.kernel.org/regressions/9-Ehc_xXSwdXcvZqKD5aSqsqeNj5Izco4MYEwnx5cySXVEc9-x_WC4C3kAoCqNTi-H38frroUK17iobNVnkLtW36V6VWGSQEOHXhmVMm5iQ=@protonmail.com/
    Reported-by: Jordan Leppert <jordanleppert@protonmail.com>
    Reported-by: Holger Hoffstaette <holger@applied-asynchrony.com>
    Tested-by: Jordan Leppert <jordanleppert@protonmail.com>
    Tested-by: Holger Hoffstaette <holger@applied-asynchrony.com>
    CC: <stable@vger.kernel.org> # 5.10+
    Signed-off-by: Manuel Ullmann <labre@posteo.de>
    Link: https://lore.kernel.org/r/87bkw8dfmp.fsf@posteo.de
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>

commit 931aff627469a75c77b9fd3823146d0575afffd6
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Thu Nov 25 19:15:10 2021 +0100

    Revert "Revert "block, bfq: honor already-setup queue merges""
    
    [ Upstream commit 15729ff8143f8135b03988a100a19e66d7cb7ecd ]
    
    A crash [1] happened to be triggered in conjunction with commit
    2d52c58b9c9b ("block, bfq: honor already-setup queue merges"). The
    latter was then reverted by commit ebc69e897e17 ("Revert "block, bfq:
    honor already-setup queue merges""). Yet, the reverted commit was not
    the one introducing the bug. In fact, it actually triggered a UAF
    introduced by a different commit, and now fixed by commit d29bd41428cf
    ("block, bfq: reset last_bfqq_created on group change").
    
    So, there is no point in keeping commit 2d52c58b9c9b ("block, bfq:
    honor already-setup queue merges") out. This commit restores it.
    
    [1] https://bugzilla.kernel.org/show_bug.cgi?id=214503
    
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Link: https://lore.kernel.org/r/20211125181510.15004-1-paolo.valente@linaro.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f990f0985eda59d4f29fc83fcf300c92b1225d39
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Thu Nov 25 19:15:10 2021 +0100

    Revert "Revert "block, bfq: honor already-setup queue merges""
    
    [ Upstream commit 15729ff8143f8135b03988a100a19e66d7cb7ecd ]
    
    A crash [1] happened to be triggered in conjunction with commit
    2d52c58b9c9b ("block, bfq: honor already-setup queue merges"). The
    latter was then reverted by commit ebc69e897e17 ("Revert "block, bfq:
    honor already-setup queue merges""). Yet, the reverted commit was not
    the one introducing the bug. In fact, it actually triggered a UAF
    introduced by a different commit, and now fixed by commit d29bd41428cf
    ("block, bfq: reset last_bfqq_created on group change").
    
    So, there is no point in keeping commit 2d52c58b9c9b ("block, bfq:
    honor already-setup queue merges") out. This commit restores it.
    
    [1] https://bugzilla.kernel.org/show_bug.cgi?id=214503
    
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Link: https://lore.kernel.org/r/20211125181510.15004-1-paolo.valente@linaro.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit cc051f497eac9d8a0d816cd4bffa3415f2724871
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Thu Nov 25 19:15:10 2021 +0100

    Revert "Revert "block, bfq: honor already-setup queue merges""
    
    [ Upstream commit 15729ff8143f8135b03988a100a19e66d7cb7ecd ]
    
    A crash [1] happened to be triggered in conjunction with commit
    2d52c58b9c9b ("block, bfq: honor already-setup queue merges"). The
    latter was then reverted by commit ebc69e897e17 ("Revert "block, bfq:
    honor already-setup queue merges""). Yet, the reverted commit was not
    the one introducing the bug. In fact, it actually triggered a UAF
    introduced by a different commit, and now fixed by commit d29bd41428cf
    ("block, bfq: reset last_bfqq_created on group change").
    
    So, there is no point in keeping commit 2d52c58b9c9b ("block, bfq:
    honor already-setup queue merges") out. This commit restores it.
    
    [1] https://bugzilla.kernel.org/show_bug.cgi?id=214503
    
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Link: https://lore.kernel.org/r/20211125181510.15004-1-paolo.valente@linaro.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 65d8a737452e88f251fe5d925371de6d606df613
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Thu Nov 25 19:15:10 2021 +0100

    Revert "Revert "block, bfq: honor already-setup queue merges""
    
    [ Upstream commit 15729ff8143f8135b03988a100a19e66d7cb7ecd ]
    
    A crash [1] happened to be triggered in conjunction with commit
    2d52c58b9c9b ("block, bfq: honor already-setup queue merges"). The
    latter was then reverted by commit ebc69e897e17 ("Revert "block, bfq:
    honor already-setup queue merges""). Yet, the reverted commit was not
    the one introducing the bug. In fact, it actually triggered a UAF
    introduced by a different commit, and now fixed by commit d29bd41428cf
    ("block, bfq: reset last_bfqq_created on group change").
    
    So, there is no point in keeping commit 2d52c58b9c9b ("block, bfq:
    honor already-setup queue merges") out. This commit restores it.
    
    [1] https://bugzilla.kernel.org/show_bug.cgi?id=214503
    
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Link: https://lore.kernel.org/r/20211125181510.15004-1-paolo.valente@linaro.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit abc2129e646af7b43025d90a071f83043f1ae76c
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Thu Nov 25 19:15:10 2021 +0100

    Revert "Revert "block, bfq: honor already-setup queue merges""
    
    [ Upstream commit 15729ff8143f8135b03988a100a19e66d7cb7ecd ]
    
    A crash [1] happened to be triggered in conjunction with commit
    2d52c58b9c9b ("block, bfq: honor already-setup queue merges"). The
    latter was then reverted by commit ebc69e897e17 ("Revert "block, bfq:
    honor already-setup queue merges""). Yet, the reverted commit was not
    the one introducing the bug. In fact, it actually triggered a UAF
    introduced by a different commit, and now fixed by commit d29bd41428cf
    ("block, bfq: reset last_bfqq_created on group change").
    
    So, there is no point in keeping commit 2d52c58b9c9b ("block, bfq:
    honor already-setup queue merges") out. This commit restores it.
    
    [1] https://bugzilla.kernel.org/show_bug.cgi?id=214503
    
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Link: https://lore.kernel.org/r/20211125181510.15004-1-paolo.valente@linaro.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 4083925bd6dc89216d156474a8076feec904e607
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Thu Nov 25 19:15:10 2021 +0100

    Revert "Revert "block, bfq: honor already-setup queue merges""
    
    [ Upstream commit 15729ff8143f8135b03988a100a19e66d7cb7ecd ]
    
    A crash [1] happened to be triggered in conjunction with commit
    2d52c58b9c9b ("block, bfq: honor already-setup queue merges"). The
    latter was then reverted by commit ebc69e897e17 ("Revert "block, bfq:
    honor already-setup queue merges""). Yet, the reverted commit was not
    the one introducing the bug. In fact, it actually triggered a UAF
    introduced by a different commit, and now fixed by commit d29bd41428cf
    ("block, bfq: reset last_bfqq_created on group change").
    
    So, there is no point in keeping commit 2d52c58b9c9b ("block, bfq:
    honor already-setup queue merges") out. This commit restores it.
    
    [1] https://bugzilla.kernel.org/show_bug.cgi?id=214503
    
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Link: https://lore.kernel.org/r/20211125181510.15004-1-paolo.valente@linaro.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 15729ff8143f8135b03988a100a19e66d7cb7ecd
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Thu Nov 25 19:15:10 2021 +0100

    Revert "Revert "block, bfq: honor already-setup queue merges""
    
    A crash [1] happened to be triggered in conjunction with commit
    2d52c58b9c9b ("block, bfq: honor already-setup queue merges"). The
    latter was then reverted by commit ebc69e897e17 ("Revert "block, bfq:
    honor already-setup queue merges""). Yet, the reverted commit was not
    the one introducing the bug. In fact, it actually triggered a UAF
    introduced by a different commit, and now fixed by commit d29bd41428cf
    ("block, bfq: reset last_bfqq_created on group change").
    
    So, there is no point in keeping commit 2d52c58b9c9b ("block, bfq:
    honor already-setup queue merges") out. This commit restores it.
    
    [1] https://bugzilla.kernel.org/show_bug.cgi?id=214503
    
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Link: https://lore.kernel.org/r/20211125181510.15004-1-paolo.valente@linaro.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 0dc636b3b757a6b747a156de613275f9d74a4a66
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 19 10:29:47 2021 +0100

    x86: Pin task-stack in __get_wchan()
    
    When commit 5d1ceb3969b6 ("x86: Fix __get_wchan() for !STACKTRACE")
    moved from stacktrace to native unwind_*() usage, the
    try_get_task_stack() got lost, leading to use-after-free issues for
    dying tasks.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Fixes: 5d1ceb3969b6 ("x86: Fix __get_wchan() for !STACKTRACE")
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=215031
    Link: https://lore.kernel.org/stable/YZV02RCRVHIa144u@fedora64.linuxtx.org/
    Reported-by: Justin Forbes <jmforbes@linuxtx.org>
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Cc: Qi Zheng <zhengqi.arch@bytedance.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit bb91de44693b1c10fe2f9e668506b01e88efed0e
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Mon Sep 6 11:57:46 2021 -0300

    perf beauty: Update copy of linux/socket.h with the kernel sources
    
    To pick the changes in:
    
    Fixes: d32f89da7fa8ccc8 ("net: add accept helper not installing fd")
    Fixes: bc49d8169aa72295 ("mctp: Add MCTP base")
    
    This automagically adds support for the AF_MCTP protocol domain:
    
      $ tools/perf/trace/beauty/socket.sh > before
      $ cp include/linux/socket.h tools/perf/trace/beauty/include/linux/socket.h
      $ tools/perf/trace/beauty/socket.sh > after
      $ diff -u before after
      --- before    2021-09-06 11:57:14.972747200 -0300
      +++ after     2021-09-06 11:57:30.541920222 -0300
      @@ -44,4 +44,5 @@
            [42] = "QIPCRTR",
            [43] = "SMC",
            [44] = "XDP",
      +     [45] = "MCTP",
       };
      $
    
    This will allow 'perf trace' to translate 45 into "MCTP" as is done with
    the other domains:
    
      # perf trace -e socket*
         0.000 chronyd/1029 socket(family: INET, type: DGRAM|CLOEXEC|NONBLOCK, protocol: IP) = 4
      ^C#
    
    This addresses this perf build warning:
    
      Warning: Kernel ABI header at 'tools/perf/trace/beauty/include/linux/socket.h' differs from latest version at 'include/linux/socket.h'
      diff -u tools/perf/trace/beauty/include/linux/socket.h include/linux/socket.h
    
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jeremy Kerr <jk@codeconstruct.com.au>
    Cc: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

commit 833cf2c80aa3142708f62a1cb7893d2bce57839e
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Sat Jun 19 16:09:46 2021 +0200

    block, bfq: avoid delayed merge of async queues
    
    [ Upstream commit bd3664b362381c4c1473753ebedf0ab242a60d1d ]
    
    Since commit 430a67f9d616 ("block, bfq: merge bursts of newly-created
    queues"), BFQ may schedule a merge between a newly created sync
    bfq_queue, say Q2, and the last sync bfq_queue created, say Q1. To this
    goal, BFQ stores the address of Q1 in the field bic->stable_merge_bfqq
    of the bic associated with Q2. So, when the time for the possible merge
    arrives, BFQ knows which bfq_queue to merge Q2 with. In particular,
    BFQ checks for possible merges on request arrivals.
    
    Yet the same bic may also be associated with an async bfq_queue, say
    Q3. So, if a request for Q3 arrives, then the above check may happen
    to be executed while the bfq_queue at hand is Q3, instead of Q2. In
    this case, Q1 happens to be merged with an async bfq_queue. This is
    not only a conceptual mistake, because async queues are to be kept out
    of queue merging, but also a bug that leads to inconsistent states.
    
    This commits simply filters async queues out of delayed merges.
    
    Fixes: 430a67f9d616 ("block, bfq: merge bursts of newly-created queues")
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Link: https://lore.kernel.org/r/20210619140948.98712-6-paolo.valente@linaro.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit bd3664b362381c4c1473753ebedf0ab242a60d1d
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Sat Jun 19 16:09:46 2021 +0200

    block, bfq: avoid delayed merge of async queues
    
    Since commit 430a67f9d616 ("block, bfq: merge bursts of newly-created
    queues"), BFQ may schedule a merge between a newly created sync
    bfq_queue, say Q2, and the last sync bfq_queue created, say Q1. To this
    goal, BFQ stores the address of Q1 in the field bic->stable_merge_bfqq
    of the bic associated with Q2. So, when the time for the possible merge
    arrives, BFQ knows which bfq_queue to merge Q2 with. In particular,
    BFQ checks for possible merges on request arrivals.
    
    Yet the same bic may also be associated with an async bfq_queue, say
    Q3. So, if a request for Q3 arrives, then the above check may happen
    to be executed while the bfq_queue at hand is Q3, instead of Q2. In
    this case, Q1 happens to be merged with an async bfq_queue. This is
    not only a conceptual mistake, because async queues are to be kept out
    of queue merging, but also a bug that leads to inconsistent states.
    
    This commits simply filters async queues out of delayed merges.
    
    Fixes: 430a67f9d616 ("block, bfq: merge bursts of newly-created queues")
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Link: https://lore.kernel.org/r/20210619140948.98712-6-paolo.valente@linaro.org
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 300bb1fe767183a1ca1dadf691409c53c4ecff4b
Author: Jianyong Wu <jianyong.wu@arm.com>
Date:   Wed Dec 9 14:09:30 2020 +0800

    ptp: arm/arm64: Enable ptp_kvm for arm/arm64
    
    Currently, there is no mechanism to keep time sync between guest and host
    in arm/arm64 virtualization environment. Time in guest will drift compared
    with host after boot up as they may both use third party time sources
    to correct their time respectively. The time deviation will be in order
    of milliseconds. But in some scenarios,like in cloud environment, we ask
    for higher time precision.
    
    kvm ptp clock, which chooses the host clock source as a reference
    clock to sync time between guest and host, has been adopted by x86
    which takes the time sync order from milliseconds to nanoseconds.
    
    This patch enables kvm ptp clock for arm/arm64 and improves clock sync precision
    significantly.
    
    Test result comparisons between with kvm ptp clock and without it in arm/arm64
    are as follows. This test derived from the result of command 'chronyc
    sources'. we should take more care of the last sample column which shows
    the offset between the local clock and the source at the last measurement.
    
    no kvm ptp in guest:
    MS Name/IP address   Stratum Poll Reach LastRx Last sample
    ========================================================================
    ^* dns1.synet.edu.cn      2   6   377    13  +1040us[+1581us] +/-   21ms
    ^* dns1.synet.edu.cn      2   6   377    21  +1040us[+1581us] +/-   21ms
    ^* dns1.synet.edu.cn      2   6   377    29  +1040us[+1581us] +/-   21ms
    ^* dns1.synet.edu.cn      2   6   377    37  +1040us[+1581us] +/-   21ms
    ^* dns1.synet.edu.cn      2   6   377    45  +1040us[+1581us] +/-   21ms
    ^* dns1.synet.edu.cn      2   6   377    53  +1040us[+1581us] +/-   21ms
    ^* dns1.synet.edu.cn      2   6   377    61  +1040us[+1581us] +/-   21ms
    ^* dns1.synet.edu.cn      2   6   377     4   -130us[ +796us] +/-   21ms
    ^* dns1.synet.edu.cn      2   6   377    12   -130us[ +796us] +/-   21ms
    ^* dns1.synet.edu.cn      2   6   377    20   -130us[ +796us] +/-   21ms
    
    in host:
    MS Name/IP address   Stratum Poll Reach LastRx Last sample
    ========================================================================
    ^* 120.25.115.20          2   7   377    72   -470us[ -603us] +/-   18ms
    ^* 120.25.115.20          2   7   377    92   -470us[ -603us] +/-   18ms
    ^* 120.25.115.20          2   7   377   112   -470us[ -603us] +/-   18ms
    ^* 120.25.115.20          2   7   377     2   +872ns[-6808ns] +/-   17ms
    ^* 120.25.115.20          2   7   377    22   +872ns[-6808ns] +/-   17ms
    ^* 120.25.115.20          2   7   377    43   +872ns[-6808ns] +/-   17ms
    ^* 120.25.115.20          2   7   377    63   +872ns[-6808ns] +/-   17ms
    ^* 120.25.115.20          2   7   377    83   +872ns[-6808ns] +/-   17ms
    ^* 120.25.115.20          2   7   377   103   +872ns[-6808ns] +/-   17ms
    ^* 120.25.115.20          2   7   377   123   +872ns[-6808ns] +/-   17ms
    
    The dns1.synet.edu.cn is the network reference clock for guest and
    120.25.115.20 is the network reference clock for host. we can't get the
    clock error between guest and host directly, but a roughly estimated value
    will be in order of hundreds of us to ms.
    
    with kvm ptp in guest:
    chrony has been disabled in host to remove the disturb by network clock.
    
    MS Name/IP address         Stratum Poll Reach LastRx Last sample
    ========================================================================
    * PHC0                    0   3   377     8     -7ns[   +1ns] +/-    3ns
    * PHC0                    0   3   377     8     +1ns[  +16ns] +/-    3ns
    * PHC0                    0   3   377     6     -4ns[   -0ns] +/-    6ns
    * PHC0                    0   3   377     6     -8ns[  -12ns] +/-    5ns
    * PHC0                    0   3   377     5     +2ns[   +4ns] +/-    4ns
    * PHC0                    0   3   377    13     +2ns[   +4ns] +/-    4ns
    * PHC0                    0   3   377    12     -4ns[   -6ns] +/-    4ns
    * PHC0                    0   3   377    11     -8ns[  -11ns] +/-    6ns
    * PHC0                    0   3   377    10    -14ns[  -20ns] +/-    4ns
    * PHC0                    0   3   377     8     +4ns[   +5ns] +/-    4ns
    
    The PHC0 is the ptp clock which choose the host clock as its source
    clock. So we can see that the clock difference between host and guest
    is in order of ns.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Richard Cochran <richardcochran@gmail.com>
    Signed-off-by: Jianyong Wu <jianyong.wu@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20201209060932.212364-8-jianyong.wu@arm.com

commit fc24cc8868a1be2819e8958c1bbaa6c51bb88efc
Author: Holger Hoffstätte <holger@applied-asynchrony.com>
Date:   Fri Mar 5 12:39:21 2021 +0100

    drm/amd/display: Fix nested FPU context in dcn21_validate_bandwidth()
    
    commit 15e8b95d5f7509e0b09289be8c422c459c9f0412 upstream.
    
    Commit 41401ac67791 added FPU wrappers to dcn21_validate_bandwidth(),
    which was correct. Unfortunately a nested function alredy contained
    DC_FP_START()/DC_FP_END() calls, which results in nested FPU context
    enter/exit and complaints by kernel_fpu_begin_mask().
    This can be observed e.g. with 5.10.20, which backported 41401ac67791
    and now emits the following warning on boot:
    
    WARNING: CPU: 6 PID: 858 at arch/x86/kernel/fpu/core.c:129 kernel_fpu_begin_mask+0xa5/0xc0
    Call Trace:
     dcn21_calculate_wm+0x47/0xa90 [amdgpu]
     dcn21_validate_bandwidth_fp+0x15d/0x2b0 [amdgpu]
     dcn21_validate_bandwidth+0x29/0x40 [amdgpu]
     dc_validate_global_state+0x3c7/0x4c0 [amdgpu]
    
    The warning is emitted due to the additional DC_FP_START/END calls in
    patch_bounding_box(), which is inlined into dcn21_calculate_wm(),
    its only caller. Removing the calls brings the code in line with
    dcn20 and makes the warning disappear.
    
    Fixes: 41401ac67791 ("drm/amd/display: Add FPU wrappers to dcn21_validate_bandwidth()")
    Signed-off-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4290476449535da46bbe7ac865c0fafa06ee17fd
Author: Holger Hoffstätte <holger@applied-asynchrony.com>
Date:   Fri Mar 5 15:23:18 2021 +0100

    drm/amdgpu/display: use GFP_ATOMIC in dcn21_validate_bandwidth_fp()
    
    commit 680174cfd1e1cea70a8f30ccb44d8fbdf996018e upstream.
    
    After fixing nested FPU contexts caused by 41401ac67791 we're still seeing
    complaints about spurious kernel_fpu_end(). As it turns out this was
    already fixed for dcn20 in commit f41ed88cbd ("drm/amdgpu/display:
    use GFP_ATOMIC in dcn20_validate_bandwidth_internal") but never moved
    forward to dcn21.
    
    Signed-off-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6b9900263a31c8ee40d4457aed045c34c3b676de
Author: Holger Hoffstätte <holger@applied-asynchrony.com>
Date:   Fri Mar 5 12:39:21 2021 +0100

    drm/amd/display: Fix nested FPU context in dcn21_validate_bandwidth()
    
    commit 15e8b95d5f7509e0b09289be8c422c459c9f0412 upstream.
    
    Commit 41401ac67791 added FPU wrappers to dcn21_validate_bandwidth(),
    which was correct. Unfortunately a nested function alredy contained
    DC_FP_START()/DC_FP_END() calls, which results in nested FPU context
    enter/exit and complaints by kernel_fpu_begin_mask().
    This can be observed e.g. with 5.10.20, which backported 41401ac67791
    and now emits the following warning on boot:
    
    WARNING: CPU: 6 PID: 858 at arch/x86/kernel/fpu/core.c:129 kernel_fpu_begin_mask+0xa5/0xc0
    Call Trace:
     dcn21_calculate_wm+0x47/0xa90 [amdgpu]
     dcn21_validate_bandwidth_fp+0x15d/0x2b0 [amdgpu]
     dcn21_validate_bandwidth+0x29/0x40 [amdgpu]
     dc_validate_global_state+0x3c7/0x4c0 [amdgpu]
    
    The warning is emitted due to the additional DC_FP_START/END calls in
    patch_bounding_box(), which is inlined into dcn21_calculate_wm(),
    its only caller. Removing the calls brings the code in line with
    dcn20 and makes the warning disappear.
    
    Fixes: 41401ac67791 ("drm/amd/display: Add FPU wrappers to dcn21_validate_bandwidth()")
    Signed-off-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b40528bcc10bc3d4e4f5d765898e64f570d61a19
Author: Holger Hoffstätte <holger@applied-asynchrony.com>
Date:   Fri Mar 5 15:23:18 2021 +0100

    drm/amdgpu/display: use GFP_ATOMIC in dcn21_validate_bandwidth_fp()
    
    commit 680174cfd1e1cea70a8f30ccb44d8fbdf996018e upstream.
    
    After fixing nested FPU contexts caused by 41401ac67791 we're still seeing
    complaints about spurious kernel_fpu_end(). As it turns out this was
    already fixed for dcn20 in commit f41ed88cbd ("drm/amdgpu/display:
    use GFP_ATOMIC in dcn20_validate_bandwidth_internal") but never moved
    forward to dcn21.
    
    Signed-off-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 680174cfd1e1cea70a8f30ccb44d8fbdf996018e
Author: Holger Hoffstätte <holger@applied-asynchrony.com>
Date:   Fri Mar 5 15:23:18 2021 +0100

    drm/amdgpu/display: use GFP_ATOMIC in dcn21_validate_bandwidth_fp()
    
    After fixing nested FPU contexts caused by 41401ac67791 we're still seeing
    complaints about spurious kernel_fpu_end(). As it turns out this was
    already fixed for dcn20 in commit f41ed88cbd ("drm/amdgpu/display:
    use GFP_ATOMIC in dcn20_validate_bandwidth_internal") but never moved
    forward to dcn21.
    
    Signed-off-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
    Cc: stable@vger.kernel.org

commit 15e8b95d5f7509e0b09289be8c422c459c9f0412
Author: Holger Hoffstätte <holger@applied-asynchrony.com>
Date:   Fri Mar 5 12:39:21 2021 +0100

    drm/amd/display: Fix nested FPU context in dcn21_validate_bandwidth()
    
    Commit 41401ac67791 added FPU wrappers to dcn21_validate_bandwidth(),
    which was correct. Unfortunately a nested function alredy contained
    DC_FP_START()/DC_FP_END() calls, which results in nested FPU context
    enter/exit and complaints by kernel_fpu_begin_mask().
    This can be observed e.g. with 5.10.20, which backported 41401ac67791
    and now emits the following warning on boot:
    
    WARNING: CPU: 6 PID: 858 at arch/x86/kernel/fpu/core.c:129 kernel_fpu_begin_mask+0xa5/0xc0
    Call Trace:
     dcn21_calculate_wm+0x47/0xa90 [amdgpu]
     dcn21_validate_bandwidth_fp+0x15d/0x2b0 [amdgpu]
     dcn21_validate_bandwidth+0x29/0x40 [amdgpu]
     dc_validate_global_state+0x3c7/0x4c0 [amdgpu]
    
    The warning is emitted due to the additional DC_FP_START/END calls in
    patch_bounding_box(), which is inlined into dcn21_calculate_wm(),
    its only caller. Removing the calls brings the code in line with
    dcn20 and makes the warning disappear.
    
    Fixes: 41401ac67791 ("drm/amd/display: Add FPU wrappers to dcn21_validate_bandwidth()")
    Signed-off-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>
    Cc: stable@vger.kernel.org

commit 37ba52c6bd13a31fa35008dc0b5790a1b57de7eb
Author: Holger Hoffstätte <holger@applied-asynchrony.com>
Date:   Fri Mar 5 15:23:18 2021 +0100

    drm/amdgpu/display: use GFP_ATOMIC in dcn21_validate_bandwidth_fp()
    
    After fixing nested FPU contexts caused by 41401ac67791 we're still seeing
    complaints about spurious kernel_fpu_end(). As it turns out this was
    already fixed for dcn20 in commit f41ed88cbd ("drm/amdgpu/display:
    use GFP_ATOMIC in dcn20_validate_bandwidth_internal") but never moved
    forward to dcn21.
    
    Signed-off-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

commit b42c68fac891d8c23c81cdfd66f82864c2353d7b
Author: Holger Hoffstätte <holger@applied-asynchrony.com>
Date:   Fri Mar 5 12:39:21 2021 +0100

    drm/amd/display: Fix nested FPU context in dcn21_validate_bandwidth()
    
    Commit 41401ac67791 added FPU wrappers to dcn21_validate_bandwidth(),
    which was correct. Unfortunately a nested function alredy contained
    DC_FP_START()/DC_FP_END() calls, which results in nested FPU context
    enter/exit and complaints by kernel_fpu_begin_mask().
    This can be observed e.g. with 5.10.20, which backported 41401ac67791
    and now emits the following warning on boot:
    
    WARNING: CPU: 6 PID: 858 at arch/x86/kernel/fpu/core.c:129 kernel_fpu_begin_mask+0xa5/0xc0
    Call Trace:
     dcn21_calculate_wm+0x47/0xa90 [amdgpu]
     dcn21_validate_bandwidth_fp+0x15d/0x2b0 [amdgpu]
     dcn21_validate_bandwidth+0x29/0x40 [amdgpu]
     dc_validate_global_state+0x3c7/0x4c0 [amdgpu]
    
    The warning is emitted due to the additional DC_FP_START/END calls in
    patch_bounding_box(), which is inlined into dcn21_calculate_wm(),
    its only caller. Removing the calls brings the code in line with
    dcn20 and makes the warning disappear.
    
    Fixes: 41401ac67791 ("drm/amd/display: Add FPU wrappers to dcn21_validate_bandwidth()")
    Signed-off-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Alex Deucher <alexander.deucher@amd.com>

commit 162e0a16b7d93629c8325ea32d99b38d17bc03eb
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Tue Jul 21 10:48:46 2020 -0400

    btrfs: if we're restriping, use the target restripe profile
    
    Previously we depended on some weird behavior in our chunk allocator to
    force the allocation of new stripes, so by the time we got to doing the
    reduce we would usually already have a chunk with the proper target.
    
    However that behavior causes other problems and needs to be removed.
    First however we need to remove this check to only restripe if we
    already have those available profiles, because if we're allocating our
    first chunk it obviously will not be available.  Simply use the target
    as specified, and if that fails it'll be because we're out of space.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 349e120ecebeb984376c8edb89bf0bfb85bc16f7
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Tue Jul 21 10:48:45 2020 -0400

    btrfs: don't adjust bg flags and use default allocation profiles
    
    btrfs/061 has been failing consistently for me recently with a
    transaction abort.  We run out of space in the system chunk array, which
    means we've allocated way too many system chunks than we need.
    
    Chris added this a long time ago for balance as a poor mans restriping.
    If you had a single disk and then added another disk and then did a
    balance, update_block_group_flags would then figure out which RAID level
    you needed.
    
    Fast forward to today and we have restriping behavior, so we can
    explicitly tell the fs that we're trying to change the raid level.  This
    is accomplished through the normal get_alloc_profile path.
    
    Furthermore this code actually causes btrfs/061 to fail, because we do
    things like mkfs -m dup -d single with multiple devices.  This trips
    this check
    
    alloc_flags = update_block_group_flags(fs_info, cache->flags);
    if (alloc_flags != cache->flags) {
            ret = btrfs_chunk_alloc(trans, alloc_flags, CHUNK_ALLOC_FORCE);
    
    in btrfs_inc_block_group_ro.  Because we're balancing and scrubbing, but
    not actually restriping, we keep forcing chunk allocation of RAID1
    chunks.  This eventually causes us to run out of system space and the
    file system aborts and flips read only.
    
    We don't need this poor mans restriping any more, simply use the normal
    get_alloc_profile helper, which will get the correct alloc_flags and
    thus make the right decision for chunk allocation.  This keeps us from
    allocating a billion system chunks and falling over.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reviewed-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 91ffb9d3823098e8efea7520ad8082a4bfaee739
Author: Daniel Drown <dan-netdev@drown.org>
Date:   Fri Jul 3 01:22:34 2020 -0500

    net/xen-netfront: add kernel TX timestamps
    
    This adds kernel TX timestamps to the xen-netfront driver.  Tested with chrony
    on an AWS EC2 instance.
    
    Signed-off-by: Daniel Drown <dan-netdev@drown.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e99be292f041a022b99bd75a95f46bd016fc9c46
Author: Guenter Roeck <linux@roeck-us.net>
Date:   Wed Apr 8 20:37:30 2020 -0700

    hwmon: (drivetemp) Return -ENODATA for invalid temperatures
    
    commit ed08ebb7124e90a99420bb913d602907d377d03d upstream.
    
    Holger Hoffstätte observed that Samsung 850 Pro may return invalid
    temperatures for a short period of time after resume. Return -ENODATA
    to userspace if this is observed.
    
    Fixes:  5b46903d8bf3 ("hwmon: Driver for disk and solid state drives with temperature sensors")
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Cc: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ed08ebb7124e90a99420bb913d602907d377d03d
Author: Guenter Roeck <linux@roeck-us.net>
Date:   Wed Apr 8 20:37:30 2020 -0700

    hwmon: (drivetemp) Return -ENODATA for invalid temperatures
    
    Holger Hoffstätte observed that Samsung 850 Pro may return invalid
    temperatures for a short period of time after resume. Return -ENODATA
    to userspace if this is observed.
    
    Fixes:  5b46903d8bf3 ("hwmon: Driver for disk and solid state drives with temperature sensors")
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Cc: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>

commit 60cbcb2d493ffecafc4f6c8b1dd528bb965ca631
Author: Igor Russkikh <Igor.Russkikh@aquantia.com>
Date:   Fri Oct 11 13:45:19 2019 +0000

    net: aquantia: temperature retrieval fix
    
    [ Upstream commit 06b0d7fe7e5ff3ba4c7e265ef41135e8bcc232bb ]
    
    Chip temperature is a two byte word, colocated internally with cable
    length data. We do all readouts from HW memory by dwords, thus
    we should clear extra high bytes, otherwise temperature output
    gets weird as soon as we attach a cable to the NIC.
    
    Fixes: 8f8940118654 ("net: aquantia: add infrastructure to readout chip temperature")
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Igor Russkikh <igor.russkikh@aquantia.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 06b0d7fe7e5ff3ba4c7e265ef41135e8bcc232bb
Author: Igor Russkikh <Igor.Russkikh@aquantia.com>
Date:   Fri Oct 11 13:45:19 2019 +0000

    net: aquantia: temperature retrieval fix
    
    Chip temperature is a two byte word, colocated internally with cable
    length data. We do all readouts from HW memory by dwords, thus
    we should clear extra high bytes, otherwise temperature output
    gets weird as soon as we attach a cable to the NIC.
    
    Fixes: 8f8940118654 ("net: aquantia: add infrastructure to readout chip temperature")
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Igor Russkikh <igor.russkikh@aquantia.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 4773f9bdb476501df565b610adc0edf1932f2c6e
Author: Heiner Kallweit <hkallweit1@gmail.com>
Date:   Mon Aug 12 20:47:40 2019 +0200

    r8169: fix sporadic transmit timeout issue
    
    Holger reported sporadic transmit timeouts and it turned out that one
    path misses ringing the doorbell. Fix was suggested by Eric.
    
    Fixes: ef14358546b1 ("r8169: make use of xmit_more")
    Suggested-by: Eric Dumazet <edumazet@google.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Heiner Kallweit <hkallweit1@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a7eb6a4f2560d5ae64bfac98d79d11378ca2de6c
Author: Holger Hoffstätte <holger@applied-asynchrony.com>
Date:   Fri Aug 9 00:02:40 2019 +0200

    r8169: fix performance issue on RTL8168evl
    
    Disabling TSO but leaving SG active results is a significant
    performance drop. Therefore disable also SG on RTL8168evl.
    This restores the original performance.
    
    Fixes: 93681cd7d94f ("r8169: enable HW csum and TSO")
    Signed-off-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Heiner Kallweit <hkallweit1@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit fdd098e78482b81f2dc2adde41e8917405d4ac42
Author: Guilherme G. Piccoli <kernel@gpiccoli.net>
Date:   Thu Jun 27 13:31:33 2019 -0300

    bnx2x: Prevent ptp_task to be rescheduled indefinitely
    
    [ Upstream commit 3c91f25c2f72ba6001775a5932857c1d2131c531 ]
    
    Currently bnx2x ptp worker tries to read a register with timestamp
    information in case of TX packet timestamping and in case it fails,
    the routine reschedules itself indefinitely. This was reported as a
    kworker always at 100% of CPU usage, which was narrowed down to be
    bnx2x ptp_task.
    
    By following the ioctl handler, we could narrow down the problem to
    an NTP tool (chrony) requesting HW timestamping from bnx2x NIC with
    RX filter zeroed; this isn't reproducible for example with ptp4l
    (from linuxptp) since this tool requests a supported RX filter.
    It seems NIC FW timestamp mechanism cannot work well with
    RX_FILTER_NONE - driver's PTP filter init routine skips a register
    write to the adapter if there's not a supported filter request.
    
    This patch addresses the problem of bnx2x ptp thread's everlasting
    reschedule by retrying the register read 10 times; between the read
    attempts the thread sleeps for an increasing amount of time starting
    in 1ms to give FW some time to perform the timestamping. If it still
    fails after all retries, we bail out in order to prevent an unbound
    resource consumption from bnx2x.
    
    The patch also adds an ethtool statistic for accounting the skipped
    TX timestamp packets and it reduces the priority of timestamping
    error messages to prevent log flooding. The code was tested using
    both linuxptp and chrony.
    
    Reported-and-tested-by: Przemyslaw Hausman <przemyslaw.hausman@canonical.com>
    Suggested-by: Sudarsana Reddy Kalluru <skalluru@marvell.com>
    Signed-off-by: Guilherme G. Piccoli <gpiccoli@canonical.com>
    Acked-by: Sudarsana Reddy Kalluru <skalluru@marvell.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9dd7343d3c85fbcf2b70ec3616f55df4b013b6c0
Author: Guilherme G. Piccoli <kernel@gpiccoli.net>
Date:   Thu Jun 27 13:31:33 2019 -0300

    bnx2x: Prevent ptp_task to be rescheduled indefinitely
    
    [ Upstream commit 3c91f25c2f72ba6001775a5932857c1d2131c531 ]
    
    Currently bnx2x ptp worker tries to read a register with timestamp
    information in case of TX packet timestamping and in case it fails,
    the routine reschedules itself indefinitely. This was reported as a
    kworker always at 100% of CPU usage, which was narrowed down to be
    bnx2x ptp_task.
    
    By following the ioctl handler, we could narrow down the problem to
    an NTP tool (chrony) requesting HW timestamping from bnx2x NIC with
    RX filter zeroed; this isn't reproducible for example with ptp4l
    (from linuxptp) since this tool requests a supported RX filter.
    It seems NIC FW timestamp mechanism cannot work well with
    RX_FILTER_NONE - driver's PTP filter init routine skips a register
    write to the adapter if there's not a supported filter request.
    
    This patch addresses the problem of bnx2x ptp thread's everlasting
    reschedule by retrying the register read 10 times; between the read
    attempts the thread sleeps for an increasing amount of time starting
    in 1ms to give FW some time to perform the timestamping. If it still
    fails after all retries, we bail out in order to prevent an unbound
    resource consumption from bnx2x.
    
    The patch also adds an ethtool statistic for accounting the skipped
    TX timestamp packets and it reduces the priority of timestamping
    error messages to prevent log flooding. The code was tested using
    both linuxptp and chrony.
    
    Reported-and-tested-by: Przemyslaw Hausman <przemyslaw.hausman@canonical.com>
    Suggested-by: Sudarsana Reddy Kalluru <skalluru@marvell.com>
    Signed-off-by: Guilherme G. Piccoli <gpiccoli@canonical.com>
    Acked-by: Sudarsana Reddy Kalluru <skalluru@marvell.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 51216937c3196b61ede2ec86098af50c65ab321d
Author: Guilherme G. Piccoli <kernel@gpiccoli.net>
Date:   Thu Jun 27 13:31:33 2019 -0300

    bnx2x: Prevent ptp_task to be rescheduled indefinitely
    
    [ Upstream commit 3c91f25c2f72ba6001775a5932857c1d2131c531 ]
    
    Currently bnx2x ptp worker tries to read a register with timestamp
    information in case of TX packet timestamping and in case it fails,
    the routine reschedules itself indefinitely. This was reported as a
    kworker always at 100% of CPU usage, which was narrowed down to be
    bnx2x ptp_task.
    
    By following the ioctl handler, we could narrow down the problem to
    an NTP tool (chrony) requesting HW timestamping from bnx2x NIC with
    RX filter zeroed; this isn't reproducible for example with ptp4l
    (from linuxptp) since this tool requests a supported RX filter.
    It seems NIC FW timestamp mechanism cannot work well with
    RX_FILTER_NONE - driver's PTP filter init routine skips a register
    write to the adapter if there's not a supported filter request.
    
    This patch addresses the problem of bnx2x ptp thread's everlasting
    reschedule by retrying the register read 10 times; between the read
    attempts the thread sleeps for an increasing amount of time starting
    in 1ms to give FW some time to perform the timestamping. If it still
    fails after all retries, we bail out in order to prevent an unbound
    resource consumption from bnx2x.
    
    The patch also adds an ethtool statistic for accounting the skipped
    TX timestamp packets and it reduces the priority of timestamping
    error messages to prevent log flooding. The code was tested using
    both linuxptp and chrony.
    
    Reported-and-tested-by: Przemyslaw Hausman <przemyslaw.hausman@canonical.com>
    Suggested-by: Sudarsana Reddy Kalluru <skalluru@marvell.com>
    Signed-off-by: Guilherme G. Piccoli <gpiccoli@canonical.com>
    Acked-by: Sudarsana Reddy Kalluru <skalluru@marvell.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 74d03f7e9017e6c0301817094f0437e6ff710ae5
Author: Guilherme G. Piccoli <kernel@gpiccoli.net>
Date:   Thu Jun 27 13:31:33 2019 -0300

    bnx2x: Prevent ptp_task to be rescheduled indefinitely
    
    [ Upstream commit 3c91f25c2f72ba6001775a5932857c1d2131c531 ]
    
    Currently bnx2x ptp worker tries to read a register with timestamp
    information in case of TX packet timestamping and in case it fails,
    the routine reschedules itself indefinitely. This was reported as a
    kworker always at 100% of CPU usage, which was narrowed down to be
    bnx2x ptp_task.
    
    By following the ioctl handler, we could narrow down the problem to
    an NTP tool (chrony) requesting HW timestamping from bnx2x NIC with
    RX filter zeroed; this isn't reproducible for example with ptp4l
    (from linuxptp) since this tool requests a supported RX filter.
    It seems NIC FW timestamp mechanism cannot work well with
    RX_FILTER_NONE - driver's PTP filter init routine skips a register
    write to the adapter if there's not a supported filter request.
    
    This patch addresses the problem of bnx2x ptp thread's everlasting
    reschedule by retrying the register read 10 times; between the read
    attempts the thread sleeps for an increasing amount of time starting
    in 1ms to give FW some time to perform the timestamping. If it still
    fails after all retries, we bail out in order to prevent an unbound
    resource consumption from bnx2x.
    
    The patch also adds an ethtool statistic for accounting the skipped
    TX timestamp packets and it reduces the priority of timestamping
    error messages to prevent log flooding. The code was tested using
    both linuxptp and chrony.
    
    Reported-and-tested-by: Przemyslaw Hausman <przemyslaw.hausman@canonical.com>
    Suggested-by: Sudarsana Reddy Kalluru <skalluru@marvell.com>
    Signed-off-by: Guilherme G. Piccoli <gpiccoli@canonical.com>
    Acked-by: Sudarsana Reddy Kalluru <skalluru@marvell.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 846027be71489ffb092996791b9a081db5cda2b6
Author: Guilherme G. Piccoli <kernel@gpiccoli.net>
Date:   Thu Jun 27 13:31:33 2019 -0300

    bnx2x: Prevent ptp_task to be rescheduled indefinitely
    
    [ Upstream commit 3c91f25c2f72ba6001775a5932857c1d2131c531 ]
    
    Currently bnx2x ptp worker tries to read a register with timestamp
    information in case of TX packet timestamping and in case it fails,
    the routine reschedules itself indefinitely. This was reported as a
    kworker always at 100% of CPU usage, which was narrowed down to be
    bnx2x ptp_task.
    
    By following the ioctl handler, we could narrow down the problem to
    an NTP tool (chrony) requesting HW timestamping from bnx2x NIC with
    RX filter zeroed; this isn't reproducible for example with ptp4l
    (from linuxptp) since this tool requests a supported RX filter.
    It seems NIC FW timestamp mechanism cannot work well with
    RX_FILTER_NONE - driver's PTP filter init routine skips a register
    write to the adapter if there's not a supported filter request.
    
    This patch addresses the problem of bnx2x ptp thread's everlasting
    reschedule by retrying the register read 10 times; between the read
    attempts the thread sleeps for an increasing amount of time starting
    in 1ms to give FW some time to perform the timestamping. If it still
    fails after all retries, we bail out in order to prevent an unbound
    resource consumption from bnx2x.
    
    The patch also adds an ethtool statistic for accounting the skipped
    TX timestamp packets and it reduces the priority of timestamping
    error messages to prevent log flooding. The code was tested using
    both linuxptp and chrony.
    
    Reported-and-tested-by: Przemyslaw Hausman <przemyslaw.hausman@canonical.com>
    Suggested-by: Sudarsana Reddy Kalluru <skalluru@marvell.com>
    Signed-off-by: Guilherme G. Piccoli <gpiccoli@canonical.com>
    Acked-by: Sudarsana Reddy Kalluru <skalluru@marvell.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 3c91f25c2f72ba6001775a5932857c1d2131c531
Author: Guilherme G. Piccoli <kernel@gpiccoli.net>
Date:   Thu Jun 27 13:31:33 2019 -0300

    bnx2x: Prevent ptp_task to be rescheduled indefinitely
    
    Currently bnx2x ptp worker tries to read a register with timestamp
    information in case of TX packet timestamping and in case it fails,
    the routine reschedules itself indefinitely. This was reported as a
    kworker always at 100% of CPU usage, which was narrowed down to be
    bnx2x ptp_task.
    
    By following the ioctl handler, we could narrow down the problem to
    an NTP tool (chrony) requesting HW timestamping from bnx2x NIC with
    RX filter zeroed; this isn't reproducible for example with ptp4l
    (from linuxptp) since this tool requests a supported RX filter.
    It seems NIC FW timestamp mechanism cannot work well with
    RX_FILTER_NONE - driver's PTP filter init routine skips a register
    write to the adapter if there's not a supported filter request.
    
    This patch addresses the problem of bnx2x ptp thread's everlasting
    reschedule by retrying the register read 10 times; between the read
    attempts the thread sleeps for an increasing amount of time starting
    in 1ms to give FW some time to perform the timestamping. If it still
    fails after all retries, we bail out in order to prevent an unbound
    resource consumption from bnx2x.
    
    The patch also adds an ethtool statistic for accounting the skipped
    TX timestamp packets and it reduces the priority of timestamping
    error messages to prevent log flooding. The code was tested using
    both linuxptp and chrony.
    
    Reported-and-tested-by: Przemyslaw Hausman <przemyslaw.hausman@canonical.com>
    Suggested-by: Sudarsana Reddy Kalluru <skalluru@marvell.com>
    Signed-off-by: Guilherme G. Piccoli <gpiccoli@canonical.com>
    Acked-by: Sudarsana Reddy Kalluru <skalluru@marvell.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit f88e587c64b73c8e6ace2281b74ae4499bcb502b
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Tue Mar 12 09:59:27 2019 +0100

    block, bfq: increase idling for weight-raised queues
    
    [ Upstream commit 778c02a236a8728bb992de10ed1f12c0be5b7b0e ]
    
    If a sync bfq_queue has a higher weight than some other queue, and
    remains temporarily empty while in service, then, to preserve the
    bandwidth share of the queue, it is necessary to plug I/O dispatching
    until a new request arrives for the queue. In addition, a timeout
    needs to be set, to avoid waiting for ever if the process associated
    with the queue has actually finished its I/O.
    
    Even with the above timeout, the device is however not fed with new
    I/O for a while, if the process has finished its I/O. If this happens
    often, then throughput drops and latencies grow. For this reason, the
    timeout is kept rather low: 8 ms is the current default.
    
    Unfortunately, such a low value may cause, on the opposite end, a
    violation of bandwidth guarantees for a process that happens to issue
    new I/O too late. The higher the system load, the higher the
    probability that this happens to some process. This is a problem in
    scenarios where service guarantees matter more than throughput. One
    important case are weight-raised queues, which need to be granted a
    very high fraction of the bandwidth.
    
    To address this issue, this commit lower-bounds the plugging timeout
    for weight-raised queues to 20 ms. This simple change provides
    relevant benefits. For example, on a PLEXTOR PX-256M5S, with which
    gnome-terminal starts in 0.6 seconds if there is no other I/O in
    progress, the same applications starts in
    - 0.8 seconds, instead of 1.2 seconds, if ten files are being read
      sequentially in parallel
    - 1 second, instead of 2 seconds, if, in parallel, five files are
      being read sequentially, and five more files are being written
      sequentially
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit b5a185ee30d7ffe936c9a713779e7e7f05df441c
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Tue Mar 12 09:59:27 2019 +0100

    block, bfq: increase idling for weight-raised queues
    
    [ Upstream commit 778c02a236a8728bb992de10ed1f12c0be5b7b0e ]
    
    If a sync bfq_queue has a higher weight than some other queue, and
    remains temporarily empty while in service, then, to preserve the
    bandwidth share of the queue, it is necessary to plug I/O dispatching
    until a new request arrives for the queue. In addition, a timeout
    needs to be set, to avoid waiting for ever if the process associated
    with the queue has actually finished its I/O.
    
    Even with the above timeout, the device is however not fed with new
    I/O for a while, if the process has finished its I/O. If this happens
    often, then throughput drops and latencies grow. For this reason, the
    timeout is kept rather low: 8 ms is the current default.
    
    Unfortunately, such a low value may cause, on the opposite end, a
    violation of bandwidth guarantees for a process that happens to issue
    new I/O too late. The higher the system load, the higher the
    probability that this happens to some process. This is a problem in
    scenarios where service guarantees matter more than throughput. One
    important case are weight-raised queues, which need to be granted a
    very high fraction of the bandwidth.
    
    To address this issue, this commit lower-bounds the plugging timeout
    for weight-raised queues to 20 ms. This simple change provides
    relevant benefits. For example, on a PLEXTOR PX-256M5S, with which
    gnome-terminal starts in 0.6 seconds if there is no other I/O in
    progress, the same applications starts in
    - 0.8 seconds, instead of 1.2 seconds, if ten files are being read
      sequentially in parallel
    - 1 second, instead of 2 seconds, if, in parallel, five files are
      being read sequentially, and five more files are being written
      sequentially
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 404ee275baec931a5c405c99dedc5abf6bb1d342
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Tue Mar 12 09:59:27 2019 +0100

    block, bfq: increase idling for weight-raised queues
    
    [ Upstream commit 778c02a236a8728bb992de10ed1f12c0be5b7b0e ]
    
    If a sync bfq_queue has a higher weight than some other queue, and
    remains temporarily empty while in service, then, to preserve the
    bandwidth share of the queue, it is necessary to plug I/O dispatching
    until a new request arrives for the queue. In addition, a timeout
    needs to be set, to avoid waiting for ever if the process associated
    with the queue has actually finished its I/O.
    
    Even with the above timeout, the device is however not fed with new
    I/O for a while, if the process has finished its I/O. If this happens
    often, then throughput drops and latencies grow. For this reason, the
    timeout is kept rather low: 8 ms is the current default.
    
    Unfortunately, such a low value may cause, on the opposite end, a
    violation of bandwidth guarantees for a process that happens to issue
    new I/O too late. The higher the system load, the higher the
    probability that this happens to some process. This is a problem in
    scenarios where service guarantees matter more than throughput. One
    important case are weight-raised queues, which need to be granted a
    very high fraction of the bandwidth.
    
    To address this issue, this commit lower-bounds the plugging timeout
    for weight-raised queues to 20 ms. This simple change provides
    relevant benefits. For example, on a PLEXTOR PX-256M5S, with which
    gnome-terminal starts in 0.6 seconds if there is no other I/O in
    progress, the same applications starts in
    - 0.8 seconds, instead of 1.2 seconds, if ten files are being read
      sequentially in parallel
    - 1 second, instead of 2 seconds, if, in parallel, five files are
      being read sequentially, and five more files are being written
      sequentially
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit dc8d8f83ea52967e5a74c871c2b3f1ba93b82228
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Mar 1 15:45:35 2019 -0300

    perf beauty msg_flags: Add missing %s lost when adding prefix suppression logic
    
    [ Upstream commit c3b81a500f35241a4c16febe0a015e572cf2c492 ]
    
    When the prefix suppresion/enabling logic was added, I forgot to add an
    extra %, which ended up chopping off the strings:
    
    Before:
    
      # perf trace -e *mmsg --map-dump syscalls
      [299] = 1,
      [307] = 1,
      DNS Res~ver #3/14587 sendmmsg(106<socket:[3462393]>, 0x7f252b0fcaf0, 2, MSG_) = 2
      chronyd/1053 recvmmsg(4, 0x558542ca5740, 4, MSG_, NULL) = 1
      DNS Res~ver #2/14445 sendmmsg(106<socket:[3461475]>, 0x7f252ab09af0, 2, MSG_) = 2
      DNS Res~ver #2/14444 sendmmsg(146<socket:[3457863]>, 0x7f2521a7aaf0, 2, MSG_) = 2
      DNS Res~ver #2/14445 sendmmsg(106<socket:[3461475]>, 0x7f252ab09af0, 2, MSG_) = 2
      DNS Res~ver #3/14587 sendmmsg(148<socket:[3460636]>, 0x7f252b0fcaf0, 2, MSG_) = 2
      DNS Res~ver #2/14444 sendmmsg(146<socket:[3457863]>, 0x7f2521a7aaf0, 2, MSG_) = 2
      ^C#
    
    After:
    
      # perf trace -e *mmsg --map-dump syscalls
      [299] = 1,
      [307] = 1,
      NetworkManager/17467 sendmmsg(22<socket:[3466493]>, 0x7f28927f9bb0, 2, MSG_NOSIGNAL) = 2
      pool/17478 sendmmsg(10<socket:[3466523]>, 0x7f2769f95e90, 2, MSG_NOSIGNAL) = 2
      DNS Res~ver #3/14587 sendmmsg(121<socket:[3466132]>, 0x7f252b0fcaf0, 2, MSG_NOSIGNAL) = 2
      chronyd/1053 recvmmsg(4, 0x558542ca5740, 4, MSG_DONTWAIT, NULL) = 1
      Socket Thread/17433 sendmmsg(121<socket:[3460903]>, 0x7f252668baf0, 2, MSG_NOSIGNAL) = 2
      ^C#
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Luis Cláudio Gonçalves <lclaudio@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Fixes: c65c83ffe904 ("perf trace: Allow asking for not suppressing common string prefixes")
    Link: https://lkml.kernel.org/n/tip-t2eu1rqx710k6jr4814mlzg7@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 56a85fd8376ef32458efb6ea97a820754e12f6bb
Author: Holger Hoffstätte <holger.hoffstaette@googlemail.com>
Date:   Tue Feb 12 15:54:24 2019 -0700

    loop: properly observe rotational flag of underlying device
    
    The loop driver always declares the rotational flag of its device as
    rotational, even when the device of the mapped file is nonrotational,
    as is the case with SSDs or on tmpfs. This can confuse filesystem tools
    which are SSD-aware; in my case I frequently forget to tell mkfs.btrfs
    that my loop device on tmpfs is nonrotational, and that I really don't
    need any automatic metadata redundancy.
    
    The attached patch fixes this by introspecting the rotational flag of the
    mapped file's underlying block device, if it exists. If the mapped file's
    filesystem has no associated block device - as is the case on e.g. tmpfs -
    we assume nonrotational storage. If there is a better way to identify such
    non-devices I'd love to hear them.
    
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: linux-block@vger.kernel.org
    Cc: holger@applied-asynchrony.com
    Signed-off-by: Holger Hoffstätte <holger.hoffstaette@googlemail.com>
    Signed-off-by: Gwendal Grignou <gwendal@chromium.org>
    Signed-off-by: Benjamin Gordon <bmgordon@chromium.org>
    Reviewed-by: Guenter Roeck <groeck@chromium.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit fffca087d587b03d0d0dca2e86bf8e688fbf2c18
Author: Francesco Pollicino <fra.fra.800@gmail.com>
Date:   Tue Mar 12 09:59:34 2019 +0100

    block, bfq: save & resume weight on a queue merge/split
    
    bfq saves the state of a queue each time a merge occurs, to be
    able to resume such a state when the queue is associated again
    with its original process, on a split.
    
    Unfortunately bfq does not save & restore also the weight of the
    queue. If the weight is not correctly resumed when the queue is
    recycled, then the weight of the recycled queue could differ
    from the weight of the original queue.
    
    This commit adds the missing save & resume of the weight.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: Francesco Pollicino <fra.fra.800@gmail.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 1e66413c4f68e2a61a210e4f5ff5df7a2ab86a5b
Author: Francesco Pollicino <fra.fra.800@gmail.com>
Date:   Tue Mar 12 09:59:33 2019 +0100

    block, bfq: print SHARED instead of pid for shared queues in logs
    
    The function "bfq_log_bfqq" prints the pid of the process
    associated with the queue passed as input.
    
    Unfortunately, if the queue is shared, then more than one process
    is associated with the queue. The pid that gets printed in this
    case is the pid of one of the associated processes.
    Which process gets printed depends on the exact sequence of merge
    events the queue underwent. So printing such a pid is rather
    useless and above all is often rather confusing because it
    reports a random pid between those of the associated processes.
    
    This commit addresses this issue by printing SHARED instead of a pid
    if the queue is shared.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: Francesco Pollicino <fra.fra.800@gmail.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 84a746891e1d8364485c0a37533fe6c1380270d4
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Tue Mar 12 09:59:32 2019 +0100

    block, bfq: always protect newly-created queues from existing active queues
    
    If many bfq_queues belonging to the same group happen to be created
    shortly after each other, then the processes associated with these
    queues have typically a common goal. In particular, bursts of queue
    creations are usually caused by services or applications that spawn
    many parallel threads/processes. Examples are systemd during boot, or
    git grep. If there are no other active queues, then, to help these
    processes get their job done as soon as possible, the best thing to do
    is to reach a high throughput. To this goal, it is usually better to
    not grant either weight-raising or device idling to the queues
    associated with these processes. And this is exactly what BFQ
    currently does.
    
    There is however a drawback: if, in contrast, some other queues are
    already active, then the newly created queues must be protected from
    the I/O flowing through the already existing queues. In this case, the
    best thing to do is the opposite as in the other case: it is much
    better to grant weight-raising and device idling to the newly-created
    queues, if they deserve it. This commit addresses this issue by doing
    so if there are already other active queues.
    
    This change also helps eliminating false positives, which occur when
    the newly-created queues do not belong to an actual large burst of
    creations, but some background task (e.g., a service) happens to
    trigger the creation of new queues in the middle, i.e., very close to
    when the victim queues are created. These false positive may cause
    total loss of control on process latencies.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 7074f076ff153021f408229b0ce63063dde9a400
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Tue Mar 12 09:59:31 2019 +0100

    block, bfq: do not tag totally seeky queues as soft rt
    
    Sync random I/O is likely to be confused with soft real-time I/O,
    because it is characterized by limited throughput and apparently
    isochronous arrival pattern. To avoid false positives, this commits
    prevents bfq_queues containing only random (seeky) I/O from being
    tagged as soft real-time.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 8cacc5ab3eacf5284bc9b0d7d5b85b748a338104
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Tue Mar 12 09:59:30 2019 +0100

    block, bfq: do not merge queues on flash storage with queueing
    
    To boost throughput with a set of processes doing interleaved I/O
    (i.e., a set of processes whose individual I/O is random, but whose
    merged cumulative I/O is sequential), BFQ merges the queues associated
    with these processes, i.e., redirects the I/O of these processes into a
    common, shared queue. In the shared queue, I/O requests are ordered by
    their position on the medium, thus sequential I/O gets dispatched to
    the device when the shared queue is served.
    
    Queue merging costs execution time, because, to detect which queues to
    merge, BFQ must maintain a list of the head I/O requests of active
    queues, ordered by request positions. Measurements showed that this
    costs about 10% of BFQ's total per-request processing time.
    
    Request processing time becomes more and more critical as the speed of
    the underlying storage device grows. Yet, fortunately, queue merging
    is basically useless on the very devices that are so fast to make
    request processing time critical. To reach a high throughput, these
    devices must have many requests queued at the same time. But, in this
    configuration, the internal scheduling algorithms of these devices do
    also the job of queue merging: they reorder requests so as to obtain
    as much as possible a sequential I/O pattern. As a consequence, with
    processes doing interleaved I/O, the throughput reached by one such
    device is likely to be the same, with and without queue merging.
    
    In view of this fact, this commit disables queue merging, and all
    related housekeeping, for non-rotational devices with internal
    queueing. The total, single-lock-protected, per-request processing
    time of BFQ drops to, e.g., 1.9 us on an Intel Core i7-2760QM@2.40GHz
    (time measured with simple code instrumentation, and using the
    throughput-sync.sh script of the S suite [1], in performance-profiling
    mode). To put this result into context, the total,
    single-lock-protected, per-request execution time of the lightest I/O
    scheduler available in blk-mq, mq-deadline, is 0.7 us (mq-deadline is
    ~800 LOC, against ~10500 LOC for BFQ).
    
    Disabling merging provides a further, remarkable benefit in terms of
    throughput. Merging tends to make many workloads artificially more
    uneven, mainly because of shared queues remaining non empty for
    incomparably more time than normal queues. So, if, e.g., one of the
    queues in a set of merged queues has a higher weight than a normal
    queue, then the shared queue may inherit such a high weight and, by
    staying almost always active, may force BFQ to perform I/O plugging
    most of the time. This evidently makes it harder for BFQ to let the
    device reach a high throughput.
    
    As a practical example of this problem, and of the benefits of this
    commit, we measured again the throughput in the nasty scenario
    considered in previous commit messages: dbench test (in the Phoronix
    suite), with 6 clients, on a filesystem with journaling, and with the
    journaling daemon enjoying a higher weight than normal processes. With
    this commit, the throughput grows from ~150 MB/s to ~200 MB/s on a
    PLEXTOR PX-256M5 SSD. This is the same peak throughput reached by any
    of the other I/O schedulers. As such, this is also likely to be the
    maximum possible throughput reachable with this workload on this
    device, because I/O is mostly random, and the other schedulers
    basically just pass I/O requests to the drive as fast as possible.
    
    [1] https://github.com/Algodev-github/S
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Tested-by: Francesco Pollicino <fra.fra.800@gmail.com>
    Signed-off-by: Alessio Masola <alessio.masola@gmail.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 2341d662e9a2a5751ff8ac4ffa640fb493b0ee84
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Tue Mar 12 09:59:29 2019 +0100

    block, bfq: tune service injection basing on request service times
    
    The processes associated with a bfq_queue, say Q, may happen to
    generate their cumulative I/O at a lower rate than the rate at which
    the device could serve the same I/O. This is rather probable, e.g., if
    only one process is associated with Q and the device is an SSD. It
    results in Q becoming often empty while in service. If BFQ is not
    allowed to switch to another queue when Q becomes empty, then, during
    the service of Q, there will be frequent "service holes", i.e., time
    intervals during which Q gets empty and the device can only consume
    the I/O already queued in its hardware queues. This easily causes
    considerable losses of throughput.
    
    To counter this problem, BFQ implements a request injection mechanism,
    which tries to fill the above service holes with I/O requests taken
    from other bfq_queues. The hard part in this mechanism is finding the
    right amount of I/O to inject, so as to both boost throughput and not
    break Q's bandwidth and latency guarantees. To this goal, the current
    version of this mechanism measures the bandwidth enjoyed by Q while it
    is being served, and tries to inject the maximum possible amount of
    extra service that does not cause Q's bandwidth to decrease too
    much.
    
    This solution has an important shortcoming. For bandwidth measurements
    to be stable and reliable, Q must remain in service for a much longer
    time than that needed to serve a single I/O request. Unfortunately,
    this does not hold with many workloads. This commit addresses this
    issue by changing the way the amount of injection allowed is
    dynamically computed. It tunes injection as a function of the service
    times of single I/O requests of Q, instead of Q's
    bandwidth. Single-request service times are evidently meaningful even
    if Q gets very few I/O requests completed while it is in service.
    
    As a testbed for this new solution, we measured the throughput reached
    by BFQ for one of the nastiest workloads and configurations for this
    scheduler: the workload generated by the dbench test (in the Phoronix
    suite), with 6 clients, on a filesystem with journaling, and with the
    journaling daemon enjoying a higher weight than normal processes.
    With this commit, the throughput grows from ~100 MB/s to ~150 MB/s on
    a PLEXTOR PX-256M5.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Tested-by: Francesco Pollicino <fra.fra.800@gmail.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit fb53ac6cd0269987b1b77f957db453b3ec7bf7e4
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Tue Mar 12 09:59:28 2019 +0100

    block, bfq: do not idle for lowest-weight queues
    
    In most cases, it is detrimental for throughput to plug I/O dispatch
    when the in-service bfq_queue becomes temporarily empty (plugging is
    performed to wait for the possible arrival, soon, of new I/O from the
    in-service queue). There is however a case where plugging is needed
    for service guarantees. If a bfq_queue, say Q, has a higher weight
    than some other active bfq_queue, and is sync, i.e., contains sync
    I/O, then, to guarantee that Q does receive a higher share of the
    throughput than other lower-weight queues, it is necessary to plug I/O
    dispatch when Q remains temporarily empty while being served.
    
    For this reason, BFQ performs I/O plugging when some active bfq_queue
    has a higher weight than some other active bfq_queue. But this is
    unnecessarily overkill. In fact, if the in-service bfq_queue actually
    has a weight lower than or equal to the other queues, then the queue
    *must not* be guaranteed a higher share of the throughput than the
    other queues. So, not plugging I/O cannot cause any harm to the
    queue. And can boost throughput.
    
    Taking advantage of this fact, this commit does not plug I/O for sync
    bfq_queues with a weight lower than or equal to the weights of the
    other queues. Here is an example of the resulting throughput boost
    with the dbench workload, which is particularly nasty for BFQ. With
    the dbench test in the Phoronix suite, BFQ reaches its lowest total
    throughput with 6 clients on a filesystem with journaling, in case the
    journaling daemon has a higher weight than normal processes. Before
    this commit, the total throughput was ~80 MB/sec on a PLEXTOR PX-256M5,
    after this commit it is ~100 MB/sec.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 778c02a236a8728bb992de10ed1f12c0be5b7b0e
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Tue Mar 12 09:59:27 2019 +0100

    block, bfq: increase idling for weight-raised queues
    
    If a sync bfq_queue has a higher weight than some other queue, and
    remains temporarily empty while in service, then, to preserve the
    bandwidth share of the queue, it is necessary to plug I/O dispatching
    until a new request arrives for the queue. In addition, a timeout
    needs to be set, to avoid waiting for ever if the process associated
    with the queue has actually finished its I/O.
    
    Even with the above timeout, the device is however not fed with new
    I/O for a while, if the process has finished its I/O. If this happens
    often, then throughput drops and latencies grow. For this reason, the
    timeout is kept rather low: 8 ms is the current default.
    
    Unfortunately, such a low value may cause, on the opposite end, a
    violation of bandwidth guarantees for a process that happens to issue
    new I/O too late. The higher the system load, the higher the
    probability that this happens to some process. This is a problem in
    scenarios where service guarantees matter more than throughput. One
    important case are weight-raised queues, which need to be granted a
    very high fraction of the bandwidth.
    
    To address this issue, this commit lower-bounds the plugging timeout
    for weight-raised queues to 20 ms. This simple change provides
    relevant benefits. For example, on a PLEXTOR PX-256M5S, with which
    gnome-terminal starts in 0.6 seconds if there is no other I/O in
    progress, the same applications starts in
    - 0.8 seconds, instead of 1.2 seconds, if ten files are being read
      sequentially in parallel
    - 1 second, instead of 2 seconds, if, in parallel, five files are
      being read sequentially, and five more files are being written
      sequentially
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 42b1bd33dcdef4ffd98f695e188bab82f9fa46d8
Author: Konstantin Khlebnikov <koct9i@gmail.com>
Date:   Fri Mar 29 17:01:18 2019 +0300

    block/bfq: fix ifdef for CONFIG_BFQ_GROUP_IOSCHED=y
    
    Replace BFQ_GROUP_IOSCHED_ENABLED with CONFIG_BFQ_GROUP_IOSCHED.
    Code under these ifdefs never worked, something might be broken.
    
    Fixes: 0471559c2fbd ("block, bfq: add/remove entity weights correctly")
    Fixes: 73d58118498b ("block, bfq: consider also ioprio classes in symmetry detection")
    Reviewed-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 4f661542a40217713f2cee0bb6678fbb30d9d367
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Mar 26 08:34:55 2019 -0700

    tcp: fix zerocopy and notsent_lowat issues
    
    My recent patch had at least three problems :
    
    1) TX zerocopy wants notification when skb is acknowledged,
       thus we need to call skb_zcopy_clear() if the skb is
       cached into sk->sk_tx_skb_cache
    
    2) Some applications might expect precise EPOLLOUT
       notifications, so we need to update sk->sk_wmem_queued
       and call sk_mem_uncharge() from sk_wmem_free_skb()
       in all cases. The SOCK_QUEUE_SHRUNK flag must also be set.
    
    3) Reuse of saved skb should have used skb_cloned() instead
      of simply checking if the fast clone has been freed.
    
    Fixes: 472c2e07eef0 ("tcp: add one skb cache for tx")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Willem de Bruijn <willemb@google.com>
    Cc: Soheil Hassas Yeganeh <soheil@google.com>
    Acked-by: Soheil Hassas Yeganeh <soheil@google.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c3b81a500f35241a4c16febe0a015e572cf2c492
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Mar 1 15:45:35 2019 -0300

    perf beauty msg_flags: Add missing %s lost when adding prefix suppression logic
    
    When the prefix suppresion/enabling logic was added, I forgot to add an
    extra %, which ended up chopping off the strings:
    
    Before:
    
      # perf trace -e *mmsg --map-dump syscalls
      [299] = 1,
      [307] = 1,
      DNS Res~ver #3/14587 sendmmsg(106<socket:[3462393]>, 0x7f252b0fcaf0, 2, MSG_) = 2
      chronyd/1053 recvmmsg(4, 0x558542ca5740, 4, MSG_, NULL) = 1
      DNS Res~ver #2/14445 sendmmsg(106<socket:[3461475]>, 0x7f252ab09af0, 2, MSG_) = 2
      DNS Res~ver #2/14444 sendmmsg(146<socket:[3457863]>, 0x7f2521a7aaf0, 2, MSG_) = 2
      DNS Res~ver #2/14445 sendmmsg(106<socket:[3461475]>, 0x7f252ab09af0, 2, MSG_) = 2
      DNS Res~ver #3/14587 sendmmsg(148<socket:[3460636]>, 0x7f252b0fcaf0, 2, MSG_) = 2
      DNS Res~ver #2/14444 sendmmsg(146<socket:[3457863]>, 0x7f2521a7aaf0, 2, MSG_) = 2
      ^C#
    
    After:
    
      # perf trace -e *mmsg --map-dump syscalls
      [299] = 1,
      [307] = 1,
      NetworkManager/17467 sendmmsg(22<socket:[3466493]>, 0x7f28927f9bb0, 2, MSG_NOSIGNAL) = 2
      pool/17478 sendmmsg(10<socket:[3466523]>, 0x7f2769f95e90, 2, MSG_NOSIGNAL) = 2
      DNS Res~ver #3/14587 sendmmsg(121<socket:[3466132]>, 0x7f252b0fcaf0, 2, MSG_NOSIGNAL) = 2
      chronyd/1053 recvmmsg(4, 0x558542ca5740, 4, MSG_DONTWAIT, NULL) = 1
      Socket Thread/17433 sendmmsg(121<socket:[3460903]>, 0x7f252668baf0, 2, MSG_NOSIGNAL) = 2
      ^C#
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Luis Cláudio Gonçalves <lclaudio@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Fixes: c65c83ffe904 ("perf trace: Allow asking for not suppressing common string prefixes")
    Link: https://lkml.kernel.org/n/tip-t2eu1rqx710k6jr4814mlzg7@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

commit c87de86948832df11de0696e00ad2f668e933655
Author: Holger Hoffstätte <holger@applied-asynchrony.com>
Date:   Sun Nov 4 19:02:42 2018 +0100

    net: phy: realtek: fix RTL8201F sysfs name
    
    [ Upstream commit 0432e833191ad4d17b7fc2364941f91dad51db1a ]
    
    Since 4.19 the following error in sysfs has appeared when using the
    r8169 NIC driver:
    
    $cd /sys/module/realtek/drivers
    $ls -l
    ls: cannot access 'mdio_bus:RTL8201F 10/100Mbps Ethernet': No such file or directory
    [..garbled dir entries follow..]
    
    Apparently the forward slash in "10/100Mbps Ethernet" is interpreted
    as directory separator that leads nowhere, and was introduced in commit
    513588dd44b ("net: phy: realtek: add RTL8201F phy-id and functions").
    
    Fix this by removing the offending slash in the driver name.
    
    Other drivers in net/phy seem to have the same problem, but I cannot
    test/verify them.
    
    Fixes: 513588dd44b ("net: phy: realtek: add RTL8201F phy-id and functions")
    Signed-off-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reviewed-by: Andrew Lunn <andrew@lunn.ch>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2537f9adf5119d895df5d4623cec84c0aa675be7
Author: Parav Pandit <parav@mellanox.com>
Date:   Sun Oct 7 12:12:40 2018 +0300

    RDMA/core: Do not expose unsupported counters
    
    [ Upstream commit 0f6ef65d1c6ec8deb5d0f11f86631ec4cfe8f22e ]
    
    If the provider driver (such as rdma_rxe) doesn't support pma counters,
    avoid exposing its directory similar to optional hw_counters directory.
    If core fails to read the PMA counter, return an error so that user can
    retry later if needed.
    
    Fixes: 35c4cbb17811 ("IB/core: Create get_perf_mad function in sysfs.c")
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 670ebdf0c41d366ca024f4d1016248e166f8b046
Author: Parav Pandit <parav@mellanox.com>
Date:   Sun Oct 7 12:12:40 2018 +0300

    RDMA/core: Do not expose unsupported counters
    
    [ Upstream commit 0f6ef65d1c6ec8deb5d0f11f86631ec4cfe8f22e ]
    
    If the provider driver (such as rdma_rxe) doesn't support pma counters,
    avoid exposing its directory similar to optional hw_counters directory.
    If core fails to read the PMA counter, return an error so that user can
    retry later if needed.
    
    Fixes: 35c4cbb17811 ("IB/core: Create get_perf_mad function in sysfs.c")
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 086b1e14c5256893bc41e246eee004cf01437756
Author: Parav Pandit <parav@mellanox.com>
Date:   Sun Oct 7 12:12:40 2018 +0300

    RDMA/core: Do not expose unsupported counters
    
    [ Upstream commit 0f6ef65d1c6ec8deb5d0f11f86631ec4cfe8f22e ]
    
    If the provider driver (such as rdma_rxe) doesn't support pma counters,
    avoid exposing its directory similar to optional hw_counters directory.
    If core fails to read the PMA counter, return an error so that user can
    retry later if needed.
    
    Fixes: 35c4cbb17811 ("IB/core: Create get_perf_mad function in sysfs.c")
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bc280cf5299d619feff74fd03f8e4ef3345c251f
Author: Parav Pandit <parav@mellanox.com>
Date:   Sun Oct 7 12:12:40 2018 +0300

    RDMA/core: Do not expose unsupported counters
    
    [ Upstream commit 0f6ef65d1c6ec8deb5d0f11f86631ec4cfe8f22e ]
    
    If the provider driver (such as rdma_rxe) doesn't support pma counters,
    avoid exposing its directory similar to optional hw_counters directory.
    If core fails to read the PMA counter, return an error so that user can
    retry later if needed.
    
    Fixes: 35c4cbb17811 ("IB/core: Create get_perf_mad function in sysfs.c")
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0432e833191ad4d17b7fc2364941f91dad51db1a
Author: Holger Hoffstätte <holger@applied-asynchrony.com>
Date:   Sun Nov 4 19:02:42 2018 +0100

    net: phy: realtek: fix RTL8201F sysfs name
    
    Since 4.19 the following error in sysfs has appeared when using the
    r8169 NIC driver:
    
    $cd /sys/module/realtek/drivers
    $ls -l
    ls: cannot access 'mdio_bus:RTL8201F 10/100Mbps Ethernet': No such file or directory
    [..garbled dir entries follow..]
    
    Apparently the forward slash in "10/100Mbps Ethernet" is interpreted
    as directory separator that leads nowhere, and was introduced in commit
    513588dd44b ("net: phy: realtek: add RTL8201F phy-id and functions").
    
    Fix this by removing the offending slash in the driver name.
    
    Other drivers in net/phy seem to have the same problem, but I cannot
    test/verify them.
    
    Fixes: 513588dd44b ("net: phy: realtek: add RTL8201F phy-id and functions")
    Signed-off-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reviewed-by: Andrew Lunn <andrew@lunn.ch>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit d92060bc69233a8175a0c2bfa0d2bce123cace2d
Author: Florian Westphal <fw@strlen.de>
Date:   Sat Oct 20 12:25:27 2018 +0200

    r8169: add support for Byte Queue Limits
    
    This patch is basically a resubmit of 1e918876853a ("r8169: add support
    for Byte Queue Limits") which was reverted later. The problems causing
    the revert seem to have been fixed in the meantime.
    Only change to the original patch is that the call to
    netdev_reset_queue was moved to rtl8169_tx_clear.
    
    The Tested-by refers to a system using the RTL8168evl chip version.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Heiner Kallweit <hkallweit1@gmail.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 0f6ef65d1c6ec8deb5d0f11f86631ec4cfe8f22e
Author: Parav Pandit <parav@mellanox.com>
Date:   Sun Oct 7 12:12:40 2018 +0300

    RDMA/core: Do not expose unsupported counters
    
    If the provider driver (such as rdma_rxe) doesn't support pma counters,
    avoid exposing its directory similar to optional hw_counters directory.
    If core fails to read the PMA counter, return an error so that user can
    retry later if needed.
    
    Fixes: 35c4cbb17811 ("IB/core: Create get_perf_mad function in sysfs.c")
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

commit ecf160b424ee648a14116079ff72d7d1241e377d
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Thu Aug 23 03:51:53 2018 +0800

    Btrfs: preftree: use rb_first_cached
    
    rb_first_cached() trades an extra pointer "leftmost" for doing the same
    job as rb_first() but in O(1).
    
    While resolving indirect refs and missing refs, it always looks for the
    first rb entry in a while loop, it's helpful to use rb_first_cached
    instead.
    
    For more details about the optimization see patch "Btrfs: delayed-refs:
    use rb_first_cached for href_root".
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 07e1ce096db3605f3e0c98695df66a51e2be9f05
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Thu Aug 23 03:51:52 2018 +0800

    Btrfs: extent_map: use rb_first_cached
    
    rb_first_cached() trades an extra pointer "leftmost" for doing the
    same job as rb_first() but in O(1).
    
    As evict_inode_truncate_pages() removes all extent mapping by always
    looking for the first rb entry, it's helpful to use rb_first_cached
    instead.
    
    For more details about the optimization see patch "Btrfs: delayed-refs:
    use rb_first_cached for href_root".
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 03a1d4c891634dd5b98da865fb783e8b22d4d027
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Thu Aug 23 03:51:51 2018 +0800

    Btrfs: delayed-inode: use rb_first_cached for ins_root and del_root
    
    rb_first_cached() trades an extra pointer "leftmost" for doing the same job as
    rb_first() but in O(1).
    
    Functions manipulating delayed_item need to get the first entry, this converts
    it to use rb_first_cached().
    
    For more details about the optimization see patch "Btrfs: delayed-refs:
    use rb_first_cached for href_root".
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit e3d039656384288bbe952413d8d404b3035fe7d7
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Thu Aug 23 03:51:50 2018 +0800

    Btrfs: delayed-refs: use rb_first_cached for ref_tree
    
    rb_first_cached() trades an extra pointer "leftmost" for doing the same
    job as rb_first() but in O(1).
    
    Functions manipulating href->ref_tree need to get the first entry, this
    converts href->ref_tree to use rb_first_cached().
    
    For more details about the optimization see patch "Btrfs: delayed-refs:
    use rb_first_cached for href_root".
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 5c9d028b3b174e5cf3678a7b0c14e21e51665793
Author: Liu Bo <bo.liu@linux.alibaba.com>
Date:   Thu Aug 23 03:51:49 2018 +0800

    Btrfs: delayed-refs: use rb_first_cached for href_root
    
    rb_first_cached() trades an extra pointer "leftmost" for doing the same
    job as rb_first() but in O(1).
    
    Functions manipulating href_root need to get the first entry, this
    converts href_root to use rb_first_cached().
    
    This patch is first in the sequenct of similar updates to other rbtrees
    and this is analysis of the expected behaviour and improvements.
    
    There's a common pattern:
    
    while (node = rb_first) {
            entry = rb_entry(node)
            next = rb_next(node)
            rb_erase(node)
            cleanup(entry)
    }
    
    rb_first needs to traverse the tree up to logN depth, rb_erase can
    completely reshuffle the tree. With the caching we'll skip the traversal
    in rb_first.  That's a cached memory access vs looped pointer
    dereference trade-off that IMHO has a clear winner.
    
    Measurements show there's not much difference in a sample tree with
    10000 nodes: 4.5s / rb_first and 4.8s / rb_first_cached. Real effects of
    caching and pointer chasing are unpredictable though.
    
    Further optimzations can be done to avoid the expensive rb_erase step.
    In some cases it's ok to process the nodes in any order, so the tree can
    be traversed in post-order, not rebalancing the children nodes and just
    calling free. Care must be taken regarding the next node.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Liu Bo <bo.liu@linux.alibaba.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ update changelog from mail discussions ]
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 9ab5aadebeddc77fccfdf94a048259315ce95fe1
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Tue Aug 21 15:02:09 2018 -0300

    perf trace: Add a etcsnoop.c augmented syscalls eBPF utility
    
    We need to put common stuff into a separate header in tools/perf/include/bpf/
    for these augmented syscalls, but I couldn't resist adding a etcsnoop.c tool,
    combining augmented syscalls + filtering, that in the future will be passed
    from 'perf trace''s command line, to use in building the eBPF program to do
    that specific filtering at the source, inside the kernel:
    
      Running system wide: (hope there isn't any embarassing stuff here...  ;-) )
    
      # perf trace -e tools/perf/examples/bpf/etcsnoop.c
           0.000 sed/21878 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
        1741.473 cat/21883 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
        1741.892 cat/21883 openat(dfd: CWD, filename: /etc/passwd)
        1748.948 sed/21886 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
        1777.136 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1777.738 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1778.158 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1778.528 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1778.595 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1778.901 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1778.939 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1778.966 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1778.992 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.019 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.045 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.071 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.095 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.121 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.148 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.175 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.202 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.229 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.254 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.279 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.309 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.336 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.363 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.388 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.414 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.442 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.470 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.500 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.529 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.557 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.586 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.617 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.648 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.679 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.706 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.739 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.769 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.798 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.823 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.844 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.862 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.880 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.911 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.942 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1779.972 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1780.004 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
        1780.035 gvfs-udisks2-v/2302 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
       13059.154 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13060.739 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13061.990 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13063.177 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13064.265 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13065.483 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13067.383 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13068.902 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13069.922 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13070.915 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13072.612 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13074.816 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13077.343 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13078.731 NetworkManager/1237 open(filename: /etc/passwd, flags: CLOEXEC)
       13559.064 DNS Res~er #22/21054 open(filename: /etc/hosts, flags: CLOEXEC)
       22419.522 sed/21896 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       24473.313 git/21900 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       24491.988 less/21901 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       24493.793 git/21901 openat(dfd: CWD, filename: /etc/sysless)
       24565.772 sed/21924 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       25878.752 git/21928 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       26075.666 git/21928 open(filename: /etc/localtime, flags: CLOEXEC)
       26075.565 less/21929 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       26076.060 less/21929 openat(dfd: CWD, filename: /etc/sysless)
       26346.395 sed/21932 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       26483.583 sed/21938 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       26954.890 sed/21944 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       27016.165 gsd-color/1762 openat(dfd: CWD, filename: /etc/localtime)
       27016.414 gsd-color/1762 openat(dfd: CWD, filename: /etc/localtime)
       27712.313 gsd-color/2408 openat(dfd: CWD, filename: /etc/localtime)
       27712.616 gsd-color/2408 openat(dfd: CWD, filename: /etc/localtime)
       27829.035 gnome-shell/2125 openat(dfd: CWD, filename: /etc/localtime)
       27829.368 gnome-shell/2125 openat(dfd: CWD, filename: /etc/localtime)
       27829.584 gnome-shell/2125 openat(dfd: CWD, filename: /etc/localtime)
       27829.800 gnome-shell/2125 openat(dfd: CWD, filename: /etc/localtime)
       27830.107 gnome-shell/2125 openat(dfd: CWD, filename: /etc/localtime)
       27830.521 gnome-shell/2125 openat(dfd: CWD, filename: /etc/localtime)
       27961.516 git/21948 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       27987.568 less/21949 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       27988.948 bash/21949 openat(dfd: CWD, filename: /etc/sysless)
       28043.536 sed/21972 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       28736.008 sed/21978 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       34882.664 git/21991 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       34882.664 sort/21990 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       34884.441 uniq/21992 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       35593.098 git/21997 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       35638.839 git/21997 openat(dfd: CWD, filename: /etc/gitattributes)
       35702.851 sed/22000 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       36076.039 sed/22006 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       37569.049 git/22014 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       37673.712 git/22014 open(filename: /etc/localtime, flags: CLOEXEC)
       37781.710 vim/22040 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       37783.667 git/22040 openat(dfd: CWD, filename: /etc/vimrc)
       37792.394 git/22040 open(filename: /etc/nsswitch.conf, flags: CLOEXEC)
       37792.436 git/22040 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       37792.580 git/22040 open(filename: /etc/passwd, flags: CLOEXEC)
       43893.625 DNS Res~er #23/21365 open(filename: /etc/hosts, flags: CLOEXEC)
       48060.409 nm-dhcp-helper/22044 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48071.745 systemd/1 openat(dfd: CWD, filename: /etc/systemd/system/dbus-org.freedesktop.nm-dispatcher.service, flags: CLOEXEC|NOFOLLOW|NOCTTY)
       48082.780 nm-dispatcher/22049 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48111.418 systemd/22049 open(filename: /etc/NetworkManager/dispatcher.d, flags: CLOEXEC|DIRECTORY|NONBLOCK)
       48111.904 systemd/22049 open(filename: /etc/localtime, flags: CLOEXEC)
       48118.357 00-netreport/22052 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48119.668 systemd/22052 open(filename: /etc/nsswitch.conf, flags: CLOEXEC)
       48119.762 systemd/22052 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48119.887 systemd/22052 open(filename: /etc/passwd, flags: CLOEXEC)
       48120.025 systemd/22052 openat(dfd: CWD, filename: /etc/NetworkManager/dispatcher.d/00-netreport)
       48124.144 hostname/22054 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48125.492 systemd/22052 openat(dfd: CWD, filename: /etc/init.d/functions)
       48127.253 systemd/22052 openat(dfd: CWD, filename: /etc/profile.d/lang.sh)
       48127.388 systemd/22052 openat(dfd: CWD, filename: /etc/locale.conf)
       48137.749 cat/22056 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48143.519 04-iscsi/22058 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48144.438 04-iscsi/22058 open(filename: /etc/nsswitch.conf, flags: CLOEXEC)
       48144.478 04-iscsi/22058 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48144.577 04-iscsi/22058 open(filename: /etc/passwd, flags: CLOEXEC)
       48144.819 04-iscsi/22058 openat(dfd: CWD, filename: /etc/NetworkManager/dispatcher.d/04-iscsi)
       48145.620 10-ifcfg-rh-ro/22059 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48146.169 systemd/22059 open(filename: /etc/nsswitch.conf, flags: CLOEXEC)
       48146.207 systemd/22059 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48146.287 systemd/22059 open(filename: /etc/passwd, flags: CLOEXEC)
       48146.387 systemd/22059 openat(dfd: CWD, filename: /etc/NetworkManager/dispatcher.d/10-ifcfg-rh-routes.sh)
       48147.215 11-dhclient/22060 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48147.787 11-dhclient/22060 open(filename: /etc/nsswitch.conf, flags: CLOEXEC)
       48147.813 11-dhclient/22060 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48147.929 11-dhclient/22060 open(filename: /etc/passwd, flags: CLOEXEC)
       48148.016 11-dhclient/22060 openat(dfd: CWD, filename: /etc/NetworkManager/dispatcher.d/11-dhclient)
       48148.906 grep/22063 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48151.165 11-dhclient/22060 openat(dfd: CWD, filename: /etc/sysconfig/network)
       48151.560 11-dhclient/22060 open(filename: /etc/dhcp/dhclient.d/, flags: CLOEXEC|DIRECTORY|NONBLOCK)
       48151.704 11-dhclient/22060 openat(dfd: CWD, filename: /etc/dhcp/dhclient.d/chrony.sh)
       48153.593 20-chrony/22065 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48154.695 20-chrony/22065 open(filename: /etc/nsswitch.conf, flags: CLOEXEC)
       48154.756 20-chrony/22065 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48154.914 20-chrony/22065 open(filename: /etc/passwd, flags: CLOEXEC)
       48155.067 20-chrony/22065 openat(dfd: CWD, filename: /etc/NetworkManager/dispatcher.d/20-chrony)
       48156.962 25-polipo/22066 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48157.824 systemd/22066 open(filename: /etc/nsswitch.conf, flags: CLOEXEC)
       48157.866 systemd/22066 openat(dfd: CWD, filename: /etc/ld.so.cache, flags: CLOEXEC)
       48157.981 systemd/22066 open(filename: /etc/passwd, flags: CLOEXEC)
       48158.090 systemd/22066 openat(dfd: CWD, filename: /etc/NetworkManager/dispatcher.d/25-polipo)
       48533.616 gsd-housekeepi/2412 openat(dfd: CWD, filename: /etc/fstab, flags: CLOEXEC)
       87122.021 gsd-color/1762 openat(dfd: CWD, filename: /etc/localtime)
       87122.146 gsd-color/1762 openat(dfd: CWD, filename: /etc/localtime)
       87825.582 gsd-color/2408 openat(dfd: CWD, filename: /etc/localtime)
       87825.844 gsd-color/2408 openat(dfd: CWD, filename: /etc/localtime)
       87829.524 gnome-shell/2125 openat(dfd: CWD, filename: /etc/localtime)
       87830.531 gnome-shell/2125 openat(dfd: CWD, filename: /etc/localtime)
       87831.288 gnome-shell/2125 openat(dfd: CWD, filename: /etc/localtime)
       87832.011 gnome-shell/2125 openat(dfd: CWD, filename: /etc/localtime)
       87832.672 gnome-shell/2125 openat(dfd: CWD, filename: /etc/localtime)
       87833.276 gnome-shell/2125 openat(dfd: CWD, filename: /etc/localtime)
       ^C#
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: https://lkml.kernel.org/n/tip-0o770jvdcy04ee6vhv6v471m@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

commit 277a4a9b56cde0f3d53ea8abc0e43ff636820007
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Mon Jun 25 21:55:37 2018 +0200

    block, bfq: give a better name to bfq_bfqq_may_idle
    
    The actual goal of the function bfq_bfqq_may_idle is to tell whether
    it is better to perform device idling (more precisely: I/O-dispatch
    plugging) for the input bfq_queue, either to boost throughput or to
    preserve service guarantees. This commit improves the name of the
    function accordingly.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 9fae8dd59ff3d9c19570cbddf12e87d7bb66c8a2
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Mon Jun 25 21:55:36 2018 +0200

    block, bfq: fix service being wrongly set to zero in case of preemption
    
    If
    - a bfq_queue Q preempts another queue, because one request of Q
    arrives in time,
    - but, after this preemption, Q is not the queue that is set in service,
    then Q->entity.service is set to 0 when Q is eventually set in
    service. But Q should have continued receiving service with its old
    budget (which is why preemption has occurred) and its old service.
    
    This commit addresses this issue by resetting service on queue real
    expiration.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 4420b095cc474759f6fbdb6351648c7ff9833a54
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Mon Jun 25 21:55:35 2018 +0200

    block, bfq: do not expire a queue that will deserve dispatch plugging
    
    For some bfq_queues, BFQ plugs I/O dispatching when the queue becomes
    idle, and keeps the plug until a new request of the queue arrives, or
    a timeout fires. BFQ does so either to boost throughput or to preserve
    service guarantees for the queue.
    
    More precisely, for such a queue, plugging starts when the queue
    happens to have either no request enqueued, or no request in flight,
    that is, no request already dispatched but not yet completed.
    
    On the opposite end, BFQ may happen to expire a queue with no request
    enqueued, without doing any plugging, if the queue still has some
    request in flight. Unfortunately, such a premature expiration causes
    the queue to lose its chance to enjoy dispatch plugging a moment
    later, i.e., when its in-flight requests finally get completed. This
    breaks service guarantees for the queue.
    
    This commit prevents BFQ from expiring an empty queue if the latter
    still has in-flight requests.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 0471559c2fbd2c19d183fc0f51ce88aefa0a13c8
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Mon Jun 25 21:55:34 2018 +0200

    block, bfq: add/remove entity weights correctly
    
    To keep I/O throughput high as often as possible, BFQ performs
    I/O-dispatch plugging (aka device idling) only when beneficial exactly
    for throughput, or when needed for service guarantees (low latency,
    fairness). An important case where the latter condition holds is when
    the scenario is 'asymmetric' in terms of weights: i.e., when some
    bfq_queue or whole group of queues has a higher weight, and thus has
    to receive more service, than other queues or groups. Without dispatch
    plugging, lower-weight queues/groups may unjustly steal bandwidth to
    higher-weight queues/groups.
    
    To detect asymmetric scenarios, BFQ checks some sufficient
    conditions. One of these conditions is that active groups have
    different weights. BFQ controls this condition by maintaining a
    special set of unique weights of active groups
    (group_weights_tree). To this purpose, in the function
    bfq_active_insert/bfq_active_extract BFQ adds/removes the weight of a
    group to/from this set.
    
    Unfortunately, the function bfq_active_extract may happen to be
    invoked also for a group that is still active (to preserve the correct
    update of the next queue to serve, see comments in function
    bfq_no_longer_next_in_service() for details). In this case, removing
    the weight of the group makes the set group_weights_tree
    inconsistent. Service-guarantee violations follow.
    
    This commit addresses this issue by moving group_weights_tree
    insertions from their previous location (in bfq_active_insert) into
    the function __bfq_activate_entity, and by moving group_weights_tree
    extractions from bfq_active_extract to when the entity that represents
    a group remains throughly idle, i.e., with no request either enqueued
    or dispatched.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit d0345e21ec98dfd6660ec00f118799140209dea2
Author: Colin Ian King <colin.i.king@gmail.com>
Date:   Thu Jun 7 17:54:37 2018 -0400

    net: aquantia: fix unsigned numvecs comparison with less than zero
    
    commit 58d813afbe89658a5972747460a5fe19dec4dbcb upstream.
    
    This was originally mistakenly submitted to net-next. Resubmitting to net.
    
    The comparison of numvecs < 0 is always false because numvecs is a u32
    and hence the error return from a failed call to pci_alloc_irq_vectores
    is never detected.  Fix this by using the signed int ret to handle the
    error return and assign numvecs to err.
    
    Detected by CoverityScan, CID#1468650 ("Unsigned compared against 0")
    
    Fixes: a09bd81b5413 ("net: aquantia: Limit number of vectors to actually allocated irqs")
    Signed-off-by: Colin Ian King <colin.king@canonical.com>
    Signed-off-by: Igor Russkikh <igor.russkikh@aquantia.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Cc: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1a33c67f5681b9efcfcf5f81260e214a4ae9f2d6
Author: Colin Ian King <colin.i.king@gmail.com>
Date:   Thu Jun 7 17:54:37 2018 -0400

    net: aquantia: fix unsigned numvecs comparison with less than zero
    
    commit 58d813afbe89658a5972747460a5fe19dec4dbcb upstream.
    
    This was originally mistakenly submitted to net-next. Resubmitting to net.
    
    The comparison of numvecs < 0 is always false because numvecs is a u32
    and hence the error return from a failed call to pci_alloc_irq_vectores
    is never detected.  Fix this by using the signed int ret to handle the
    error return and assign numvecs to err.
    
    Detected by CoverityScan, CID#1468650 ("Unsigned compared against 0")
    
    Fixes: a09bd81b5413 ("net: aquantia: Limit number of vectors to actually allocated irqs")
    Signed-off-by: Colin Ian King <colin.king@canonical.com>
    Signed-off-by: Igor Russkikh <igor.russkikh@aquantia.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Cc: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a6088845c2bf754d6cb2572b484180680b037804
Author: Jianchao Wang <jianchao.w.wang@oracle.com>
Date:   Wed May 30 10:47:40 2018 -0600

    block: kyber: make kyber more friendly with merging
    
    Currently, kyber is very unfriendly with merging. kyber depends
    on ctx rq_list to do merging, however, most of time, it will not
    leave any requests in ctx rq_list. This is because even if tokens
    of one domain is used up, kyber will try to dispatch requests
    from other domain and flush the rq_list there.
    
    To improve this, we setup kyber_ctx_queue (kcq) which is similar
    with ctx, but it has rq_lists for different domain and build same
    mapping between kcq and khd as the ctx & hctx. Then we could merge,
    insert and dispatch for different domains separately. At the same
    time, only flush the rq_list of kcq when get domain token successfully.
    Then if one domain token is used up, the requests could be left in
    the rq_list of that domain and maybe merged with following io.
    
    Following is my test result on machine with 8 cores and NVMe card
    INTEL SSDPEKKR128G7
    
    fio size=256m ioengine=libaio iodepth=64 direct=1 numjobs=8
    seq/random
    +------+---------------------------------------------------------------+
    |patch?| bw(MB/s) |   iops    | slat(usec) |    clat(usec)   |  merge  |
    +----------------------------------------------------------------------+
    | w/o  |  606/612 | 151k/153k |  6.89/7.03 | 3349.21/3305.40 |   0/0   |
    +----------------------------------------------------------------------+
    | w/   | 1083/616 | 277k/154k |  4.93/6.95 | 1830.62/3279.95 | 223k/3k |
    +----------------------------------------------------------------------+
    When set numjobs to 16, the bw and iops could reach 1662MB/s and 425k
    on my platform.
    
    Signed-off-by: Jianchao Wang <jianchao.w.wang@oracle.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit effbffc91da3c9520e9b2f15fbc099f257d7cf03
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Tue Jan 9 10:27:58 2018 +0100

    block, bfq: put async queues for root bfq groups too
    
    
    [ Upstream commit 52257ffbfcaf58d247b13fb148e27ed17c33e526 ]
    
    For each pair [device for which bfq is selected as I/O scheduler,
    group in blkio/io], bfq maintains a corresponding bfq group. Each such
    bfq group contains a set of async queues, with each async queue
    created on demand, i.e., when some I/O request arrives for it.  On
    creation, an async queue gets an extra reference, to make sure that
    the queue is not freed as long as its bfq group exists.  Accordingly,
    to allow the queue to be freed after the group exited, this extra
    reference must released on group exit.
    
    The above holds also for a bfq root group, i.e., for the bfq group
    corresponding to the root blkio/io root for a given device. Yet, by
    mistake, the references to the existing async queues of a root group
    are not released when the latter exits. This causes a memory leak when
    the instance of bfq for a given device exits. In a similar vein,
    bfqg_stats_xfer_dead is not executed for a root group.
    
    This commit fixes bfq_pd_offline so that the latter executes the above
    missing operations for a root group too.
    
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reported-by: Guoqing Jiang <gqjiang@suse.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Davide Ferrari <davideferrari8@gmail.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3af99e83a27866531976965c0f9c25c65698c35c
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Tue Jan 9 10:27:58 2018 +0100

    block, bfq: put async queues for root bfq groups too
    
    
    [ Upstream commit 52257ffbfcaf58d247b13fb148e27ed17c33e526 ]
    
    For each pair [device for which bfq is selected as I/O scheduler,
    group in blkio/io], bfq maintains a corresponding bfq group. Each such
    bfq group contains a set of async queues, with each async queue
    created on demand, i.e., when some I/O request arrives for it.  On
    creation, an async queue gets an extra reference, to make sure that
    the queue is not freed as long as its bfq group exists.  Accordingly,
    to allow the queue to be freed after the group exited, this extra
    reference must released on group exit.
    
    The above holds also for a bfq root group, i.e., for the bfq group
    corresponding to the root blkio/io root for a given device. Yet, by
    mistake, the references to the existing async queues of a root group
    are not released when the latter exits. This causes a memory leak when
    the instance of bfq for a given device exits. In a similar vein,
    bfqg_stats_xfer_dead is not executed for a root group.
    
    This commit fixes bfq_pd_offline so that the latter executes the above
    missing operations for a root group too.
    
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reported-by: Guoqing Jiang <gqjiang@suse.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Davide Ferrari <davideferrari8@gmail.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bdf19237e16f5495dab1edb0ec5eb60049d02a60
Author: Markus Trippelsdorf <markus@trippelsdorf.de>
Date:   Wed Oct 11 07:01:31 2017 +0200

    VFS: Handle lazytime in do_mount()
    
    commit d7ee946942bdd12394809305e3df05aa4c8b7b8f upstream.
    
    Since commit e462ec50cb5fa ("VFS: Differentiate mount flags (MS_*) from
    internal superblock flags") the lazytime mount option doesn't get passed
    on anymore.
    
    Fix the issue by handling the option in do_mount().
    
    Reviewed-by: Lukas Czerner <lczerner@redhat.com>
    Signed-off-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 50c1c6cc09dcea608ace4bd258714d01909fed45
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Fri Jan 12 16:53:17 2018 -0800

    tools/objtool/Makefile: don't assume sync-check.sh is executable
    
    commit 0f908ccbeca99ddf0ad60afa710e72aded4a5ea7 upstream.
    
    patch(1) loses the x bit.  So if a user follows our patching
    instructions in Documentation/admin-guide/README.rst, their kernel will
    not compile.
    
    Fixes: 3bd51c5a371de ("objtool: Move kernel headers/code sync check to a script")
    Reported-by: Nicolas Bock <nicolasbock@gentoo.org>
    Reported-by Joakim Tjernlund <Joakim.Tjernlund@infinera.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 872523233d640c21ce13ea51269c5c031ebb2f78
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Jan 18 13:07:00 2018 -0300

    perf bpf: Don't warn about unavailability of builtin clang, just fallback
    
    When clang is not linked with 'perf' we should just add a debug message
    about that before doing the fallback to calling the external compiler.
    
    I.e. just the "-95" warning below gets turned into a debug message:
    
      # cat sys_enter_open.c
      #include "bpf.h"
    
      SEC("syscalls:sys_enter_open")
      int func(void *ctx)
      {
            struct {
                    char *ptr;
                    char path[256];
            } filename = {
                    .ptr = *((char **)(ctx + 16)),
            };
            int len = bpf_probe_read_str(filename.path, sizeof(filename.path), filename.ptr);
            if (len > 0) {
                    if (len == 1)
                            perf_event_output(ctx, &__bpf_stdout__, BPF_F_CURRENT_CPU, &filename, len + sizeof(filename.ptr));
                    else if (len < 256)
                            perf_event_output(ctx, &__bpf_stdout__, BPF_F_CURRENT_CPU, &filename, len + sizeof(filename.ptr));
            }
            return 0;
      }
      # trace -e open,sys_enter_open.c
      bpf: builtin compilation failed: -95, try external compiler
         0.000 (         ): __bpf_stdout__:@......./proc/self/task/11160/comm..)
         0.014 ( 0.116 ms): qemu-system-x8/6721 open(filename: /proc/self/task/11160/comm, flags: RDWR) = 91
      2335.411 (         ): __bpf_stdout__:FB..~.../etc/resolv.conf....)
      2335.421 ( 0.030 ms): chronyd/883 open(filename: /etc/resolv.conf, flags: CLOEXEC) = 5
    ^C#
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Wang Nan <wangnan0@huawei.com>
    Link: https://lkml.kernel.org/n/tip-z5aak9oay448ffj37giz94yr@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

commit 8a8747dc01cee6f92a52c03ba686e9f60cb46c87
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Sat Jan 13 12:05:18 2018 +0100

    block, bfq: limit sectors served with interactive weight raising
    
    To maximise responsiveness, BFQ raises the weight, and performs device
    idling, for bfq_queues associated with processes deemed as
    interactive. In particular, weight raising has a maximum duration,
    equal to the time needed to start a large application. If a
    weight-raised process goes on doing I/O beyond this maximum duration,
    it loses weight-raising.
    
    This mechanism is evidently vulnerable to the following false
    positives: I/O-bound applications that will go on doing I/O for much
    longer than the duration of weight-raising. These applications have
    basically no benefit from being weight-raised at the beginning of
    their I/O. On the opposite end, while being weight-raised, these
    applications
    a) unjustly steal throughput to applications that may truly need
    low latency;
    b) make BFQ uselessly perform device idling; device idling results
    in loss of device throughput with most flash-based storage, and may
    increase latencies when used purposelessly.
    
    This commit adds a countermeasure to reduce both the above
    problems. To introduce this countermeasure, we provide the following
    extra piece of information (full details in the comments added by this
    commit). During the start-up of the large application used as a
    reference to set the duration of weight-raising, involved processes
    transfer at most ~110K sectors each. Accordingly, a process initially
    deemed as interactive has no right to be weight-raised any longer,
    once transferred 110K sectors or more.
    
    Basing on this consideration, this commit early-ends weight-raising
    for a bfq_queue if the latter happens to have received an amount of
    service at least equal to 110K sectors (actually, a little bit more,
    to keep a safety margin). I/O-bound applications that reach a high
    throughput, such as file copy, get to this threshold much before the
    allowed weight-raising period finishes. Thus this early ending of
    weight-raising reduces the amount of time during which these
    applications cause the problems described above.
    
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit a52a69ea89dc12e6f4572f554940789c1ab23c7a
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Sat Jan 13 12:05:17 2018 +0100

    block, bfq: limit tags for writes and async I/O
    
    Asynchronous I/O can easily starve synchronous I/O (both sync reads
    and sync writes), by consuming all request tags. Similarly, storms of
    synchronous writes, such as those that sync(2) may trigger, can starve
    synchronous reads. In their turn, these two problems may also cause
    BFQ to loose control on latency for interactive and soft real-time
    applications. For example, on a PLEXTOR PX-256M5S SSD, LibreOffice
    Writer takes 0.6 seconds to start if the device is idle, but it takes
    more than 45 seconds (!) if there are sequential writes in the
    background.
    
    This commit addresses this issue by limiting the maximum percentage of
    tags that asynchronous I/O requests and synchronous write requests can
    consume. In particular, this commit grants a higher threshold to
    synchronous writes, to prevent the latter from being starved by
    asynchronous I/O.
    
    According to the above test, LibreOffice Writer now starts in about
    1.2 seconds on average, regardless of the background workload, and
    apart from some rare outlier. To check this improvement, run, e.g.,
    sudo ./comm_startup_lat.sh bfq 5 5 seq 10 "lowriter --terminate_after_init"
    for the comm_startup_lat benchmark in the S suite [1].
    
    [1] https://github.com/Algodev-github/S
    
    Tested-by: Oleksandr Natalenko <oleksandr@natalenko.name>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 0d52af590552473666da5b6111e7182d6cd23f92
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Tue Jan 9 10:27:59 2018 +0100

    block, bfq: release oom-queue ref to root group on exit
    
    On scheduler init, a reference to the root group, and a reference to
    its corresponding blkg are taken for the oom queue. Yet these
    references are not released on scheduler exit, which prevents these
    objects from be freed. This commit adds the missing reference
    releases.
    
    Reported-by: Davide Ferrari <davideferrari8@gmail.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 52257ffbfcaf58d247b13fb148e27ed17c33e526
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Tue Jan 9 10:27:58 2018 +0100

    block, bfq: put async queues for root bfq groups too
    
    For each pair [device for which bfq is selected as I/O scheduler,
    group in blkio/io], bfq maintains a corresponding bfq group. Each such
    bfq group contains a set of async queues, with each async queue
    created on demand, i.e., when some I/O request arrives for it.  On
    creation, an async queue gets an extra reference, to make sure that
    the queue is not freed as long as its bfq group exists.  Accordingly,
    to allow the queue to be freed after the group exited, this extra
    reference must released on group exit.
    
    The above holds also for a bfq root group, i.e., for the bfq group
    corresponding to the root blkio/io root for a given device. Yet, by
    mistake, the references to the existing async queues of a root group
    are not released when the latter exits. This causes a memory leak when
    the instance of bfq for a given device exits. In a similar vein,
    bfqg_stats_xfer_dead is not executed for a root group.
    
    This commit fixes bfq_pd_offline so that the latter executes the above
    missing operations for a root group too.
    
    Reported-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reported-by: Guoqing Jiang <gqjiang@suse.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Davide Ferrari <davideferrari8@gmail.com>
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit c7203f55d53f9237bda0c0b6cf02f22f18262a5b
Author: Tobias Jakobi <tjakobi@math.uni-bielefeld.de>
Date:   Tue Nov 21 16:15:57 2017 +0100

    net: realtek: r8169: implement set_link_ksettings()
    
    
    [ Upstream commit 9e77d7a5549dc4d4999a60676373ab3fd1dae4db ]
    
    Commit 6fa1ba61520576cf1346c4ff09a056f2950cb3bf partially
    implemented the new ethtool API, by replacing get_settings()
    with get_link_ksettings(). This breaks ethtool, since the
    userspace tool (according to the new API specs) never tries
    the legacy set() call, when the new get() call succeeds.
    
    All attempts to chance some setting from userspace result in:
    > Cannot set new settings: Operation not supported
    
    Implement the missing set() call.
    
    Signed-off-by: Tobias Jakobi <tjakobi@math.uni-bielefeld.de>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reviewed-by: Andrew Lunn <andrew@lunn.ch>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7e70aa789d4a0c89dbfbd2c8a974a4df717475ec
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Dec 5 15:52:56 2017 +0800

    scsi: core: run queue if SCSI device queue isn't ready and queue is idle
    
    Before commit 0df21c86bdbf ("scsi: implement .get_budget and .put_budget
    for blk-mq"), we run queue after 3ms if queue is idle and SCSI device
    queue isn't ready, which is done in handling BLK_STS_RESOURCE. After
    commit 0df21c86bdbf is introduced, queue won't be run any more under
    this situation.
    
    IO hang is observed when timeout happened, and this patch fixes the IO
    hang issue by running queue after delay in scsi_dev_queue_ready, just
    like non-mq. This issue can be triggered by the following script[1].
    
    There is another issue which can be covered by running idle queue: when
    .get_budget() is called on request coming from hctx->dispatch_list, if
    one request just completes during .get_budget(), we can't depend on
    SCSI's restart to make progress any more. This patch fixes the race too.
    
    With this patch, we basically recover to previous behaviour (before
    commit 0df21c86bdbf) of handling idle queue when running out of
    resource.
    
    [1] script for test/verify SCSI timeout
    rmmod scsi_debug
    modprobe scsi_debug max_queue=1
    
    DEVICE=`ls -d /sys/bus/pseudo/drivers/scsi_debug/adapter*/host*/target*/*/block/* | head -1 | xargs basename`
    DISK_DIR=`ls -d /sys/block/$DEVICE/device/scsi_disk/*`
    
    echo "using scsi device $DEVICE"
    echo "-1" >/sys/bus/pseudo/drivers/scsi_debug/every_nth
    echo "temporary write through" >$DISK_DIR/cache_type
    echo "128" >/sys/bus/pseudo/drivers/scsi_debug/opts
    echo none > /sys/block/$DEVICE/queue/scheduler
    dd if=/dev/$DEVICE of=/dev/null bs=1M iflag=direct count=1 &
    sleep 5
    echo "0" >/sys/bus/pseudo/drivers/scsi_debug/opts
    wait
    echo "SUCCESS"
    
    Fixes: 0df21c86bdbf ("scsi: implement .get_budget and .put_budget for blk-mq")
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reviewed-by: Bart Van Assche <bart.vanassche@wdc.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit 9e77d7a5549dc4d4999a60676373ab3fd1dae4db
Author: Tobias Jakobi <tjakobi@math.uni-bielefeld.de>
Date:   Tue Nov 21 16:15:57 2017 +0100

    net: realtek: r8169: implement set_link_ksettings()
    
    Commit 6fa1ba61520576cf1346c4ff09a056f2950cb3bf partially
    implemented the new ethtool API, by replacing get_settings()
    with get_link_ksettings(). This breaks ethtool, since the
    userspace tool (according to the new API specs) never tries
    the legacy set() call, when the new get() call succeeds.
    
    All attempts to chance some setting from userspace result in:
    > Cannot set new settings: Operation not supported
    
    Implement the missing set() call.
    
    Signed-off-by: Tobias Jakobi <tjakobi@math.uni-bielefeld.de>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reviewed-by: Andrew Lunn <andrew@lunn.ch>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 162f50e6914ff75f11d15e52507aa88875878fd8
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jul 23 08:36:15 2017 -0400

    workqueue: implicit ordered attribute should be overridable
    
    commit 0a94efb5acbb6980d7c9ab604372d93cd507e4d8 upstream.
    
    5c0338c68706 ("workqueue: restore WQ_UNBOUND/max_active==1 to be
    ordered") automatically enabled ordered attribute for unbound
    workqueues w/ max_active == 1.  Because ordered workqueues reject
    max_active and some attribute changes, this implicit ordered mode
    broke cases where the user creates an unbound workqueue w/ max_active
    == 1 and later explicitly changes the related attributes.
    
    This patch distinguishes explicit and implicit ordered setting and
    overrides from attribute changes if implict.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Fixes: 5c0338c68706 ("workqueue: restore WQ_UNBOUND/max_active==1 to be ordered")
    Cc: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    Signed-off-by: Willy Tarreau <w@1wt.eu>

commit 509708310cf917a05fbceb41ad67da1416b81bd0
Author: Francois Romieu <romieu@fr.zoreil.com>
Date:   Fri Oct 27 13:24:49 2017 +0300

    r8169: Add support for interrupt coalesce tuning (ethtool -C)
    
    Kirr: In particular with
    
            ethtool -C <ifname> rx-usecs 0 rx-frames 0
    
    now it is possible to disable RX delays when NIC usage requires low-latency.
    
    See this thread for context:
    
            https://www.spinics.net/lists/netdev/msg217665.html
    
    My specific case is that:
    
    We have many computers with gigabit Realtek NICs. For 2 such computers
    connected to a gigabit store-and-forward switch the minimum round-trip
    time for small pings (`ping -i 0 -w 3 -s 56 -q peer`) is ~ 30μs.
    
    However it turned out that when Ethernet frame length transitions 127 ->
    128 bytes (`ping -i 0 -w 3 -s {81 -> 82} -q peer`) the lowest RTT
    transitions step-wise to ~ 270μs.
    
    As David Light said this is RX interrupt mitigation done by NIC which creates
    the latency. For workloads when low-latency is required with e.g. Intel,
    BCM etc NIC drivers one just uses `ethtool -C rx-usecs ...` to reduce
    the time NIC delays before interrupting CPU, but it turned out
    `ethtool -C` is not supported by r8169 driver.
    
    Like Stéphane ANCELOT I've traced the problem down to IntrMitigate being
    hardcoded to != 0 for our chips (we have 8168 based NICs):
    
    https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/drivers/net/ethernet/realtek/r8169.c#n5460
    static void rtl_hw_start_8169(struct net_device *dev) {
            ...
            /*
             * Undocumented corner. Supposedly:
             * (TxTimer << 12) | (TxPackets << 8) | (RxTimer << 4) | RxPackets
             */
            RTL_W16(IntrMitigate, 0x0000);
    
    https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/drivers/net/ethernet/realtek/r8169.c#n6346
    static void rtl_hw_start_8168(struct net_device *dev) {
            ...
            RTL_W16(IntrMitigate, 0x5151);
    
    and then I've also found
    
            https://www.spinics.net/lists/netdev/msg217665.html
    
    and original Francois' patch:
    
            https://www.spinics.net/lists/netdev/msg217984.html
            https://www.spinics.net/lists/netdev/msg218207.html
    
    So could we please finally get support for tuning r8169 interrupt
    coalescing in tree? (so that next poor soul who hits the problem does
    not need to go all the way to dig into driver sources and internet
    wildly and finally patch locally
    
            -RTL_W16(IntrMitigate, 0x5151);
            +RTL_W16(IntrMitigate, 0x5100);
    
    guessing whether it is right or not and also having to care to deploy
    the patch everywhere it needs to be used, etc...).
    
    To do so I've took original Francois's patch from 2012 and reworked it a bit:
    
    - updated to latest net-next.git;
    - adjusted scaling setup based on feedback from Hayes to pick up scaling
      vector depending not only on link speed but also on CPlusCmd[0:1] and to
      adjust CPlusCmd[0:1] correspondingly when setting timings;
    - improved a bit (I think so) error handling.
    
    I've tested the patch on "RTL8168d/8111d" (XID 083000c0) and with it and
    `ethtool -C rx-usecs 0 rx-frames 0` on both ends it improves:
    
    - minimum RTT latency:
    
            ~270μs ->  ~30μs (small packet),
            ~330μs -> ~110μs (full 1.5K ethernet frame)
    
    - average RTT latency:
    
            ~480μs ->  ~50μs (small packet),
            ~560μs -> ~125μs (full 1.5K ethernet frame)
    
    ( before:
    
            root@neo1:# ping -i 0 -w 3 -s 82 -q neo2
            PING neo2.kirr.nexedi.com (192.168.102.21) 82(110) bytes of data.
    
            --- neo2.kirr.nexedi.com ping statistics ---
            5906 packets transmitted, 5905 received, 0% packet loss, time 2999ms
            rtt min/avg/max/mdev = 0.274/0.485/0.607/0.026 ms, ipg/ewma 0.508/0.489 ms
    
            root@neo1:# ping -i 0 -w 3 -s 1472 -q neo2
            PING neo2.kirr.nexedi.com (192.168.102.21) 1472(1500) bytes of data.
    
            --- neo2.kirr.nexedi.com ping statistics ---
            5073 packets transmitted, 5073 received, 0% packet loss, time 2999ms
            rtt min/avg/max/mdev = 0.330/0.566/0.710/0.028 ms, ipg/ewma 0.591/0.544 ms
    
      after:
    
            root@neo1# ping -i 0 -w 3 -s 82 -q neo2
            PING neo2.kirr.nexedi.com (192.168.102.21) 82(110) bytes of data.
    
            --- neo2.kirr.nexedi.com ping statistics ---
            45815 packets transmitted, 45815 received, 0% packet loss, time 3000ms
            rtt min/avg/max/mdev = 0.036/0.051/0.368/0.010 ms, ipg/ewma 0.065/0.053 ms
    
            root@neo1:# ping -i 0 -w 3 -s 1472 -q neo2
            PING neo2.kirr.nexedi.com (192.168.102.21) 1472(1500) bytes of data.
    
            --- neo2.kirr.nexedi.com ping statistics ---
            21250 packets transmitted, 21250 received, 0% packet loss, time 3000ms
            rtt min/avg/max/mdev = 0.112/0.125/0.390/0.007 ms, ipg/ewma 0.141/0.125 ms
    
      the small -> 1.5K latency growth is understandable as it takes ~15μs
      to transmit 1.5K on 1Gbps on the wire and with 2 hosts and 1 switch
      and ICMP ECHO + ECHO reply the packet has to travel 4 ethernet
      segments which is already 60μs;
    
      probably something a bit else is also there as e.g. on Linux, even
      with `cpupower frequency-set -g performance`, on some computers I've
      noticed the kernel can be spending more time in software-only mode
      when incoming packets go in less frequently. E.g. this program can
      demonstrate the effect for ICMP ECHO processing:
    
      https://lab.nexedi.com/kirr/bcc/blob/43cfc13b/tools/pinglat.py
    
      (later this was found to be partly due to C-states exit latencies) )
    
    We have this patch running in our testing setup for 1 months already
    without any issues observed.
    
    It remains to be clarified whether RX and TX timers use the same base.
    For now I've set them equally, but Francois's original patch version
    suggests it could be not the same.
    
    I've got no feedback at all to my original posting of this patch and questions
    
            https://www.spinics.net/lists/netdev/msg457173.html
    
    neither from Francois, nor from any people from Realtek during one month.
    
    So I suggest we simply apply it to net-next.git now.
    
    Cc: Francois Romieu <romieu@fr.zoreil.com>
    Cc: Hayes Wang <hayeswang@realtek.com>
    Cc: Realtek linux nic maintainers <nic_swsd@realtek.com>
    Cc: David Laight <David.Laight@ACULAB.COM>
    Cc: Stéphane ANCELOT <sancelot@free.fr>
    Cc: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Kirill Smelkov <kirr@nexedi.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit d5ad330817a3d6d3e781bd2d0edc72c72fed9e5c
Author: Qu Wenruo <quwenruo@cn.fujitsu.com>
Date:   Thu Jun 22 10:01:21 2017 +0800

    btrfs: Remove false alert when fiemap range is smaller than on-disk extent
    
    commit 848c23b78fafdcd3270b06a30737f8dbd70c347f upstream.
    
    Commit 4751832da990 ("btrfs: fiemap: Cache and merge fiemap extent before
    submit it to user") introduced a warning to catch unemitted cached
    fiemap extent.
    
    However such warning doesn't take the following case into consideration:
    
    0                       4K                      8K
    |<---- fiemap range --->|
    |<----------- On-disk extent ------------------>|
    
    In this case, the whole 0~8K is cached, and since it's larger than
    fiemap range, it break the fiemap extent emit loop.
    This leaves the fiemap extent cached but not emitted, and caught by the
    final fiemap extent sanity check, causing kernel warning.
    
    This patch removes the kernel warning and renames the sanity check to
    emit_last_fiemap_cache() since it's possible and valid to have cached
    fiemap extent.
    
    Reported-by: David Sterba <dsterba@suse.cz>
    Reported-by: Adam Borowski <kilobyte@angband.pl>
    Fixes: 4751832da990 ("btrfs: fiemap: Cache and merge fiemap extent ...")
    Signed-off-by: Qu Wenruo <quwenruo@cn.fujitsu.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Cc: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 06114f074b92f87eeaf12e2560362a2105a1d5b3
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jul 23 08:36:15 2017 -0400

    workqueue: implicit ordered attribute should be overridable
    
    commit 0a94efb5acbb6980d7c9ab604372d93cd507e4d8 upstream.
    
    5c0338c68706 ("workqueue: restore WQ_UNBOUND/max_active==1 to be
    ordered") automatically enabled ordered attribute for unbound
    workqueues w/ max_active == 1.  Because ordered workqueues reject
    max_active and some attribute changes, this implicit ordered mode
    broke cases where the user creates an unbound workqueue w/ max_active
    == 1 and later explicitly changes the related attributes.
    
    This patch distinguishes explicit and implicit ordered setting and
    overrides from attribute changes if implict.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Fixes: 5c0338c68706 ("workqueue: restore WQ_UNBOUND/max_active==1 to be ordered")
    Cc: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 34a08ae493f1970d5ce80dd3812b8dba4e5cbe22
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jul 23 08:36:15 2017 -0400

    workqueue: implicit ordered attribute should be overridable
    
    commit 0a94efb5acbb6980d7c9ab604372d93cd507e4d8 upstream.
    
    5c0338c68706 ("workqueue: restore WQ_UNBOUND/max_active==1 to be
    ordered") automatically enabled ordered attribute for unbound
    workqueues w/ max_active == 1.  Because ordered workqueues reject
    max_active and some attribute changes, this implicit ordered mode
    broke cases where the user creates an unbound workqueue w/ max_active
    == 1 and later explicitly changes the related attributes.
    
    This patch distinguishes explicit and implicit ordered setting and
    overrides from attribute changes if implict.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Fixes: 5c0338c68706 ("workqueue: restore WQ_UNBOUND/max_active==1 to be ordered")
    Cc: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f9636c9bdd5828f29cdfaf620e3a424a5f8cc221
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jul 23 08:36:15 2017 -0400

    workqueue: implicit ordered attribute should be overridable
    
    commit 0a94efb5acbb6980d7c9ab604372d93cd507e4d8 upstream.
    
    5c0338c68706 ("workqueue: restore WQ_UNBOUND/max_active==1 to be
    ordered") automatically enabled ordered attribute for unbound
    workqueues w/ max_active == 1.  Because ordered workqueues reject
    max_active and some attribute changes, this implicit ordered mode
    broke cases where the user creates an unbound workqueue w/ max_active
    == 1 and later explicitly changes the related attributes.
    
    This patch distinguishes explicit and implicit ordered setting and
    overrides from attribute changes if implict.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Fixes: 5c0338c68706 ("workqueue: restore WQ_UNBOUND/max_active==1 to be ordered")
    Cc: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a799f35e52b5749b5fb7b4314b42dcd7315a908d
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Jul 23 08:36:15 2017 -0400

    workqueue: implicit ordered attribute should be overridable
    
    commit 0a94efb5acbb6980d7c9ab604372d93cd507e4d8 upstream.
    
    5c0338c68706 ("workqueue: restore WQ_UNBOUND/max_active==1 to be
    ordered") automatically enabled ordered attribute for unbound
    workqueues w/ max_active == 1.  Because ordered workqueues reject
    max_active and some attribute changes, this implicit ordered mode
    broke cases where the user creates an unbound workqueue w/ max_active
    == 1 and later explicitly changes the related attributes.
    
    This patch distinguishes explicit and implicit ordered setting and
    overrides from attribute changes if implict.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Fixes: 5c0338c68706 ("workqueue: restore WQ_UNBOUND/max_active==1 to be ordered")
    Cc: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d7eae3403f46646889a9d172476e61a7aa822cc7
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Jun 6 16:45:31 2017 -0700

    Btrfs: rework delayed ref total_bytes_pinned accounting
    
    The total_bytes_pinned counter is completely broken when accounting
    delayed refs:
    
    - If two drops for the same extent are merged, we will decrement
      total_bytes_pinned twice but only increment it once.
    - If an add is merged into a drop or vice versa, we will decrement the
      total_bytes_pinned counter but never increment it.
    - If multiple references to an extent are dropped, we will account it
      multiple times, potentially vastly over-estimating the number of bytes
      that will be freed by a commit and doing unnecessary work when we're
      close to ENOSPC.
    
    The last issue is relatively minor, but the first two make the
    total_bytes_pinned counter leak or underflow very often. These
    accounting issues were introduced in b150a4f10d87 ("Btrfs: use a percpu
    to keep track of possibly pinned bytes"), but they were papered over by
    zeroing out the counter on every commit until d288db5dc011 ("Btrfs: fix
    race of using total_bytes_pinned").
    
    We need to make sure that an extent is accounted as pinned exactly once
    if and only if we will drop references to it when when the transaction
    is committed. Ideally we would only add to total_bytes_pinned when the
    *last* reference is dropped, but this information isn't readily
    available for data extents. Again, this over-estimation can lead to
    extra commits when we're close to ENOSPC, but it's not as bad as before.
    
    The fix implemented here is to increment total_bytes_pinned when the
    total refmod count for an extent goes negative and decrement it if the
    refmod count goes back to non-negative or after we've run all of the
    delayed refs for that extent.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 7be07912b32d103d9789082f27dd54b47c89c744
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Jun 6 16:45:30 2017 -0700

    Btrfs: return old and new total ref mods when adding delayed refs
    
    We need this to decide when to account pinned bytes.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 0a16c7d7aecfae8987197e50116ebfc338cbe0a2
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Jun 6 16:45:29 2017 -0700

    Btrfs: always account pinned bytes when dropping a tree block ref
    
    Currently, we only increment total_bytes_pinned in
    btrfs_free_tree_block() when dropping the last reference on the block.
    However, when the delayed ref is run later, we will decrement
    total_bytes_pinned regardless of whether it was the last reference or
    not. This causes the counter to underflow when the reference we dropped
    was not the last reference. Fix it by incrementing the counter
    unconditionally, which is what btrfs_free_extent() does. This makes
    total_bytes_pinned an overestimate when references to shared extents are
    dropped, but in the worst case this will just make us try to commit the
    transaction to try to free up space and find we didn't free enough.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 4da8b76d347bcae951f89522a040c36d9fc9f3b3
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Jun 6 16:45:28 2017 -0700

    Btrfs: update total_bytes_pinned when pinning down extents
    
    The extents marked in pin_down_extent() will be unpinned later in
    unpin_extent_range(), which decrements total_bytes_pinned.
    pin_down_extent() must increment the counter to avoid underflowing it.
    Also adjust btrfs_free_tree_block() to avoid accounting for the same
    extent twice.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 55e8196a57cfe603ce3480a66c15dde3a13fe218
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Jun 6 16:45:27 2017 -0700

    Btrfs: make BUG_ON() in add_pinned_bytes() an ASSERT()
    
    The value of flags is one of DATA/METADATA/SYSTEM, they must exist at
    when add_pinned_bytes is called.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ added changelog ]
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 0d9f824df35d11215016785213f9c0fd06a72fc0
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Jun 6 16:45:26 2017 -0700

    Btrfs: make add_pinned_bytes() take an s64 num_bytes instead of u64
    
    There are a few places where we pass in a negative num_bytes, so make it
    signed for clarity. Also move it up in the file since later patches will
    need it there.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 77b7dcead36d15d7af9159f2a5f91149c5887634
Author: Paolo Valente <paolo.valente@linaro.org>
Date:   Wed Apr 12 18:23:13 2017 +0200

    block, bfq: reduce I/O latency for soft real-time applications
    
    To guarantee a low latency also to the I/O requests issued by soft
    real-time applications, this patch introduces a further heuristic,
    which weight-raises (in the sense explained in the previous patch)
    also the queues associated to applications deemed as soft real-time.
    
    To be deemed as soft real-time, an application must meet two
    requirements.  First, the application must not require an average
    bandwidth higher than the approximate bandwidth required to playback
    or record a compressed high-definition video. Second, the request
    pattern of the application must be isochronous, i.e., after issuing a
    request or a batch of requests, the application must stop issuing new
    requests until all its pending requests have been completed. After
    that, the application may issue a new batch, and so on.
    
    As for the second requirement, it is critical to require also that,
    after all the pending requests of the application have been completed,
    an adequate minimum amount of time elapses before the application
    starts issuing new requests. This prevents also greedy (i.e.,
    I/O-bound) applications from being incorrectly deemed, occasionally,
    as soft real-time. In fact, if *any amount of time* is fine, then even
    a greedy application may, paradoxically, meet both the above
    requirements, if: (1) the application performs random I/O and/or the
    device is slow, and (2) the CPU load is high. The reason is the
    following.  First, if condition (1) is true, then, during the service
    of the application, the throughput may be low enough to let the
    application meet the bandwidth requirement.  Second, if condition (2)
    is true as well, then the application may occasionally behave in an
    apparently isochronous way, because it may simply stop issuing
    requests while the CPUs are busy serving other processes.
    
    To address this issue, the heuristic leverages the simple fact that
    greedy applications issue *all* their requests as quickly as they can,
    whereas soft real-time applications spend some time processing data
    after each batch of requests is completed. In particular, the
    heuristic works as follows. First, according to the above isochrony
    requirement, the heuristic checks whether an application may be soft
    real-time, thereby giving to the application the opportunity to be
    deemed as such, only when both the following two conditions happen to
    hold: 1) the queue associated with the application has expired and is
    empty, 2) there is no outstanding request of the application.
    
    Suppose that both conditions hold at time, say, t_c and that the
    application issues its next request at time, say, t_i. At time t_c the
    heuristic computes the next time instant, called soft_rt_next_start in
    the code, such that, only if t_i >= soft_rt_next_start, then both the
    next conditions will hold when the application issues its next
    request: 1) the application will meet the above bandwidth requirement,
    2) a given minimum time interval, say Delta, will have elapsed from
    time t_c (so as to filter out greedy application).
    
    The current value of Delta is a little bit higher than the value that
    we have found, experimentally, to be adequate on a real,
    general-purpose machine. In particular we had to increase Delta to
    make the filter quite precise also in slower, embedded systems, and in
    KVM/QEMU virtual machines (details in the comments on the code).
    
    If the application actually issues its next request after time
    soft_rt_next_start, then its associated queue will be weight-raised
    for a relatively short time interval. If, during this time interval,
    the application proves again to meet the bandwidth and isochrony
    requirements, then the end of the weight-raising period for the queue
    is moved forward, and so on. Note that an application whose associated
    queue never happens to be empty when it expires will never have the
    opportunity to be deemed as soft real-time.
    
    Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Arianna Avanzini <avanzini.arianna@gmail.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit 3716a49a81ba19dda7202633a68b28564ba95eb5
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Sat Feb 4 09:57:14 2017 -0700

    hv_utils: implement Hyper-V PTP source
    
    With TimeSync version 4 protocol support we started updating system time
    continuously through the whole lifetime of Hyper-V guests. Every 5 seconds
    there is a time sample from the host which triggers do_settimeofday[64]().
    While the time from the host is very accurate such adjustments may cause
    issues:
    - Time is jumping forward and backward, some applications may misbehave.
    - In case an NTP server runs in parallel and uses something else for time
      sync (network, PTP,...) system time will never converge.
    - Systemd starts annoying you by printing "Time has been changed" every 5
      seconds to the system log.
    
    Instead of doing in-kernel time adjustments offload the work to an
    NTP client by exposing TimeSync messages as a PTP device. Users may now
    decide what they want to use as a source.
    
    I tested the solution with chrony, the config was:
    
     refclock PHC /dev/ptp0 poll 3 dpoll -2 offset 0
    
    The result I'm seeing is accurate enough, the time delta between the guest
    and the host is almost always within [-10us, +10us], the in-kernel solution
    was giving us comparable results.
    
    I also tried implementing PPS device instead of PTP by using not currently
    used Hyper-V synthetic timers (we use only one of four for clockevent) but
    with PPS source only chrony wasn't able to give me the required accuracy,
    the delta often more that 100us.
    
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: K. Y. Srinivasan <kys@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a0e136d436ded817c0aade72efdefa56a00b4e5e
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Jan 24 15:09:42 2017 -0200

    PTP: add kvm PTP driver
    
    Add a driver with gettime method returning hosts realtime clock.
    This allows Chrony to synchronize host and guest clocks with
    high precision (see results below).
    
    chronyc> sources
    MS Name/IP address         Stratum Poll Reach LastRx Last sample
    ===============================================================================
    
    To configure Chronyd to use PHC refclock, add the
    following line to its configuration file:
    
    refclock PHC /dev/ptpX poll 3 dpoll -2 offset 0
    
    Where /dev/ptpX is the kvmclock PTP clock.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Acked-by: Richard Cochran <richardcochran@gmail.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit 1d57ee941692d0cc928526e21a1557b2ae3e11db
Author: Wang Xiaoguang <wangxg.fnst@cn.fujitsu.com>
Date:   Wed Oct 26 18:07:33 2016 +0800

    btrfs: improve delayed refs iterations
    
    This issue was found when I tried to delete a heavily reflinked file,
    when deleting such files, other transaction operation will not have a
    chance to make progress, for example, start_transaction() will blocked
    in wait_current_trans(root) for long time, sometimes it even triggers
    soft lockups, and the time taken to delete such heavily reflinked file
    is also very large, often hundreds of seconds. Using perf top, it reports
    that:
    
    PerfTop:    7416 irqs/sec  kernel:99.8%  exact:  0.0% [4000Hz cpu-clock],  (all, 4 CPUs)
    ---------------------------------------------------------------------------------------
        84.37%  [btrfs]             [k] __btrfs_run_delayed_refs.constprop.80
        11.02%  [kernel]            [k] delay_tsc
         0.79%  [kernel]            [k] _raw_spin_unlock_irq
         0.78%  [kernel]            [k] _raw_spin_unlock_irqrestore
         0.45%  [kernel]            [k] do_raw_spin_lock
         0.18%  [kernel]            [k] __slab_alloc
    It seems __btrfs_run_delayed_refs() took most cpu time, after some debug
    work, I found it's select_delayed_ref() causing this issue, for a delayed
    head, in our case, it'll be full of BTRFS_DROP_DELAYED_REF nodes, but
    select_delayed_ref() will firstly try to iterate node list to find
    BTRFS_ADD_DELAYED_REF nodes, obviously it's a disaster in this case, and
    waste much time.
    
    To fix this issue, we introduce a new ref_add_list in struct btrfs_delayed_ref_head,
    then in select_delayed_ref(), if this list is not empty, we can directly use
    nodes in this list. With this patch, it just took about 10~15 seconds to
    delte the same file. Now using perf top, it reports that:
    
    PerfTop:    2734 irqs/sec  kernel:99.5%  exact:  0.0% [4000Hz cpu-clock],  (all, 4 CPUs)
    ----------------------------------------------------------------------------------------
    
        20.74%  [kernel]          [k] _raw_spin_unlock_irqrestore
        16.33%  [kernel]          [k] __slab_alloc
         5.41%  [kernel]          [k] lock_acquired
         4.42%  [kernel]          [k] lock_acquire
         4.05%  [kernel]          [k] lock_release
         3.37%  [kernel]          [k] _raw_spin_unlock_irq
    
    For normal files, this patch also gives help, at least we do not need to
    iterate whole list to found BTRFS_ADD_DELAYED_REF nodes.
    
    Signed-off-by: Wang Xiaoguang <wangxg.fnst@cn.fujitsu.com>
    Reviewed-by: Liu Bo <bo.li.liu@oracle.com>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit ebce0e01b930bfde74391f998d77720b2478a603
Author: Adam Borowski <kilobyte@angband.pl>
Date:   Mon Nov 14 18:44:34 2016 +0100

    btrfs: make block group flags in balance printks human-readable
    
    They're not even documented anywhere, letting users with no recourse but
    to RTFS.  It's no big burden to output the bitfield as words.
    
    Also, display unknown flags as hex.
    
    Signed-off-by: Adam Borowski <kilobyte@angband.pl>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit dd4b857aabbce57518f2d32d9805365d3ff34fc6
Author: Wang Xiaoguang <wangxg.fnst@cn.fujitsu.com>
Date:   Tue Oct 18 15:56:13 2016 +0800

    btrfs: pass correct args to btrfs_async_run_delayed_refs()
    
    In btrfs_truncate_inode_items()->btrfs_async_run_delayed_refs(), we
    swap the arg2 and arg3 wrongly, fix this.
    
    This bug just impacts asynchronous delayed refs handle when we truncate inodes.
    In delayed_ref_async_start(), there is such codes:
    
        trans = btrfs_join_transaction(async->root);
        if (trans->transid > async->transid)
            goto end;
        ret = btrfs_run_delayed_refs(trans, async->root, async->count);
    
    From this codes, we can see that this just influence whether can we handle
    delayed refs or the number of delayed refs to handle, this may impact
    performance, but will not result in missing delayed refs, all delayed refs will
    be handled in btrfs_commit_transaction().
    
    Signed-off-by: Wang Xiaoguang <wangxg.fnst@cn.fujitsu.com>
    Reviewed-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 1092e303fc9cbe71e3ba5f47d570495abfc07ffb
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu Sep 22 17:24:22 2016 -0700

    Btrfs: catch invalid free space trees
    
    commit 6675df311db87aa2107a04ef97e19420953cbace upstream.
    
    There are two separate issues that can lead to corrupted free space
    trees.
    
    1. The free space tree bitmaps had an endianness issue on big-endian
       systems which is fixed by an earlier patch in this series.
    2. btrfs-progs before v4.7.3 modified filesystems without updating the
       free space tree.
    
    To catch both of these issues at once, we need to force the free space
    tree to be rebuilt. To do so, add a FREE_SPACE_TREE_VALID compat_ro bit.
    If the bit isn't set, we know that it was either produced by a broken
    big-endian kernel or may have been corrupted by btrfs-progs.
    
    This also provides us with a way to add rudimentary read-write support
    for the free space tree to btrfs-progs: it can just clear this bit and
    have the kernel rebuild the free space tree.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6691ebee5a2eecb7a4157dc3cdbad48971015dad
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu Sep 22 17:24:21 2016 -0700

    Btrfs: fix mount -o clear_cache,space_cache=v2
    
    commit f8d468a15c22b954b379aa0c74914d5068448fb1 upstream.
    
    We moved the code for creating the free space tree the first time that
    it's enabled, but didn't move the clearing code along with it. This
    breaks my (undocumented) intention that `mount -o
    clear_cache,space_cache=v2` would clear the free space tree and then
    recreate it.
    
    Fixes: 511711af91f2 ("btrfs: don't run delayed references while we are creating the free space tree")
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1ff6341b5d92dd6b68b12508c143e174ec2caabe
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu Sep 22 17:24:20 2016 -0700

    Btrfs: fix free space tree bitmaps on big-endian systems
    
    commit 2fe1d55134fce05c17ea118a2e37a4af771887bc upstream.
    
    In convert_free_space_to_{bitmaps,extents}(), we buffer the free space
    bitmaps in memory and copy them directly to/from the extent buffers with
    {read,write}_extent_buffer(). The extent buffer bitmap helpers use byte
    granularity, which is equivalent to a little-endian bitmap. This means
    that on big-endian systems, the in-memory bitmaps will be written to
    disk byte-swapped. To fix this, use byte-granularity for the bitmaps in
    memory.
    
    Fixes: a5ed91828518 ("Btrfs: implement the free space B-tree")
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 56a17dd9eab1ea323f86adcbb05672369f63f8fa
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu Sep 22 17:24:22 2016 -0700

    Btrfs: catch invalid free space trees
    
    commit 6675df311db87aa2107a04ef97e19420953cbace upstream.
    
    There are two separate issues that can lead to corrupted free space
    trees.
    
    1. The free space tree bitmaps had an endianness issue on big-endian
       systems which is fixed by an earlier patch in this series.
    2. btrfs-progs before v4.7.3 modified filesystems without updating the
       free space tree.
    
    To catch both of these issues at once, we need to force the free space
    tree to be rebuilt. To do so, add a FREE_SPACE_TREE_VALID compat_ro bit.
    If the bit isn't set, we know that it was either produced by a broken
    big-endian kernel or may have been corrupted by btrfs-progs.
    
    This also provides us with a way to add rudimentary read-write support
    for the free space tree to btrfs-progs: it can just clear this bit and
    have the kernel rebuild the free space tree.
    
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 520f16abf003952def5d6b175c56c2b7d5d5b90f
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu Sep 22 17:24:20 2016 -0700

    Btrfs: fix free space tree bitmaps on big-endian systems
    
    commit 2fe1d55134fce05c17ea118a2e37a4af771887bc upstream.
    
    In convert_free_space_to_{bitmaps,extents}(), we buffer the free space
    bitmaps in memory and copy them directly to/from the extent buffers with
    {read,write}_extent_buffer(). The extent buffer bitmap helpers use byte
    granularity, which is equivalent to a little-endian bitmap. This means
    that on big-endian systems, the in-memory bitmaps will be written to
    disk byte-swapped. To fix this, use byte-granularity for the bitmaps in
    memory.
    
    Fixes: a5ed91828518 ("Btrfs: implement the free space B-tree")
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6675df311db87aa2107a04ef97e19420953cbace
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu Sep 22 17:24:22 2016 -0700

    Btrfs: catch invalid free space trees
    
    There are two separate issues that can lead to corrupted free space
    trees.
    
    1. The free space tree bitmaps had an endianness issue on big-endian
       systems which is fixed by an earlier patch in this series.
    2. btrfs-progs before v4.7.3 modified filesystems without updating the
       free space tree.
    
    To catch both of these issues at once, we need to force the free space
    tree to be rebuilt. To do so, add a FREE_SPACE_TREE_VALID compat_ro bit.
    If the bit isn't set, we know that it was either produced by a broken
    big-endian kernel or may have been corrupted by btrfs-progs.
    
    This also provides us with a way to add rudimentary read-write support
    for the free space tree to btrfs-progs: it can just clear this bit and
    have the kernel rebuild the free space tree.
    
    Cc: stable@vger.kernel.org # 4.5+
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit f8d468a15c22b954b379aa0c74914d5068448fb1
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu Sep 22 17:24:21 2016 -0700

    Btrfs: fix mount -o clear_cache,space_cache=v2
    
    We moved the code for creating the free space tree the first time that
    it's enabled, but didn't move the clearing code along with it. This
    breaks my (undocumented) intention that `mount -o
    clear_cache,space_cache=v2` would clear the free space tree and then
    recreate it.
    
    Fixes: 511711af91f2 ("btrfs: don't run delayed references while we are creating the free space tree")
    Cc: stable@vger.kernel.org # 4.5+
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 2fe1d55134fce05c17ea118a2e37a4af771887bc
Author: Omar Sandoval <osandov@fb.com>
Date:   Thu Sep 22 17:24:20 2016 -0700

    Btrfs: fix free space tree bitmaps on big-endian systems
    
    In convert_free_space_to_{bitmaps,extents}(), we buffer the free space
    bitmaps in memory and copy them directly to/from the extent buffers with
    {read,write}_extent_buffer(). The extent buffer bitmap helpers use byte
    granularity, which is equivalent to a little-endian bitmap. This means
    that on big-endian systems, the in-memory bitmaps will be written to
    disk byte-swapped. To fix this, use byte-granularity for the bitmaps in
    memory.
    
    Fixes: a5ed91828518 ("Btrfs: implement the free space B-tree")
    Cc: stable@vger.kernel.org # 4.5+
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>
    Tested-by: Chandan Rajendra <chandan@linux.vnet.ibm.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit c867a11289ee74c4594a96ccccac16f4cc29519e
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Aug 22 11:31:10 2016 -0700

    tcp: properly scale window in tcp_v[46]_reqsk_send_ack()
    
    [ Upstream commit 20a2b49fc538540819a0c552877086548cff8d8d ]
    
    When sending an ack in SYN_RECV state, we must scale the offered
    window if wscale option was negotiated and accepted.
    
    Tested:
     Following packetdrill test demonstrates the issue :
    
    0.000 socket(..., SOCK_STREAM, IPPROTO_TCP) = 3
    +0 setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
    
    +0 bind(3, ..., ...) = 0
    +0 listen(3, 1) = 0
    
    // Establish a connection.
    +0 < S 0:0(0) win 20000 <mss 1000,sackOK,wscale 7, nop, TS val 100 ecr 0>
    +0 > S. 0:0(0) ack 1 win 28960 <mss 1460,sackOK, TS val 100 ecr 100, nop, wscale 7>
    
    +0 < . 1:11(10) ack 1 win 156 <nop,nop,TS val 99 ecr 100>
    // check that window is properly scaled !
    +0 > . 1:1(0) ack 1 win 226 <nop,nop,TS val 200 ecr 100>
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Acked-by: Yuchung Cheng <ycheng@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>

commit 0f55fa7541d7ff34a6690438bb00b78521b98b54
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Aug 17 05:56:26 2016 -0700

    tcp: fix use after free in tcp_xmit_retransmit_queue()
    
    [ Upstream commit bb1fceca22492109be12640d49f5ea5a544c6bb4 ]
    
    When tcp_sendmsg() allocates a fresh and empty skb, it puts it at the
    tail of the write queue using tcp_add_write_queue_tail()
    
    Then it attempts to copy user data into this fresh skb.
    
    If the copy fails, we undo the work and remove the fresh skb.
    
    Unfortunately, this undo lacks the change done to tp->highest_sack and
    we can leave a dangling pointer (to a freed skb)
    
    Later, tcp_xmit_retransmit_queue() can dereference this pointer and
    access freed memory. For regular kernels where memory is not unmapped,
    this might cause SACK bugs because tcp_highest_sack_seq() is buggy,
    returning garbage instead of tp->snd_nxt, but with various debug
    features like CONFIG_DEBUG_PAGEALLOC, this can crash the kernel.
    
    This bug was found by Marco Grassi thanks to syzkaller.
    
    Fixes: 6859d49475d4 ("[TCP]: Abstract tp->highest_sack accessing & point to next skb")
    Reported-by: Marco Grassi <marco.gra@gmail.com>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Ilpo Järvinen <ilpo.jarvinen@helsinki.fi>
    Cc: Yuchung Cheng <ycheng@google.com>
    Cc: Neal Cardwell <ncardwell@google.com>
    Acked-by: Neal Cardwell <ncardwell@google.com>
    Reviewed-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>

commit 98418550e124ec047918df8b3bb587e60bd39a8a
Author: Artem Germanov <agermanov@anchorfree.com>
Date:   Wed Sep 7 10:49:36 2016 -0700

    tcp: cwnd does not increase in TCP YeAH
    
    [ Upstream commit db7196a0d0984b933ccf2cd6a60e26abf466e8a3 ]
    
    Commit 76174004a0f19785a328f40388e87e982bbf69b9
    (tcp: do not slow start when cwnd equals ssthresh )
    introduced regression in TCP YeAH. Using 100ms delay 1% loss virtual
    ethernet link kernel 4.2 shows bandwidth ~500KB/s for single TCP
    connection and kernel 4.3 and above (including 4.8-rc4) shows bandwidth
    ~100KB/s.
       That is caused by stalled cwnd when cwnd equals ssthresh. This patch
    fixes it by proper increasing cwnd in this case.
    
    Signed-off-by: Artem Germanov <agermanov@anchorfree.com>
    Acked-by: Dmitry Adamushko <d.adamushko@anchorfree.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: Holger Hoffstätte <holger@applied-asynchrony.com>

commit ea7dd213c1e3be9eeb6786affb7766aeca232439
Author: Dave Jones <davej@codemonkey.org.uk>
Date:   Fri Sep 2 14:39:50 2016 -0400

    ipv6: release dst in ping_v6_sendmsg
    
    [ Upstream commit 03c2778a938aaba0893f6d6cdc29511d91a79848 ]
    
    Neither the failure or success paths of ping_v6_sendmsg release
    the dst it acquires.  This leads to a flood of warnings from
    "net/core/dst.c:288 dst_release" on older kernels that
    don't have 8bf4ada2e21378816b28205427ee6b0e1ca4c5f1 backported.
    
    That patch optimistically hoped this had been fixed post 3.10, but
    it seems at least one case wasn't, where I've seen this triggered
    a lot from machines doing unprivileged icmp sockets.
    
    Cc: Martin Lau <kafai@fb.com>
    Signed-off-by: Dave Jones <davej@codemonkey.org.uk>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>

commit 6b8076b8a76bc7f3d51e51b9fb5b35853cc74138
Author: David Forster <dforster@brocade.com>
Date:   Wed Aug 3 15:13:01 2016 +0100

    ipv4: panic in leaf_walk_rcu due to stale node pointer
    
    [ Upstream commit 94d9f1c5906b20053efe375b6d66610bca4b8b64 ]
    
    Panic occurs when issuing "cat /proc/net/route" whilst
    populating FIB with > 1M routes.
    
    Use of cached node pointer in fib_route_get_idx is unsafe.
    
     BUG: unable to handle kernel paging request at ffffc90001630024
     IP: [<ffffffff814cf6a0>] leaf_walk_rcu+0x10/0xe0
     PGD 11b08d067 PUD 11b08e067 PMD dac4b067 PTE 0
     Oops: 0000 [#1] SMP
     Modules linked in: nfsd auth_rpcgss oid_registry nfs_acl nfs lockd grace fscac
     snd_hda_codec_generic snd_hda_intel snd_hda_codec snd_hda_core snd_hwdep virti
     acpi_cpufreq button parport_pc ppdev lp parport autofs4 ext4 crc16 mbcache jbd
    tio_ring virtio floppy uhci_hcd ehci_hcd usbcore usb_common libata scsi_mod
     CPU: 1 PID: 785 Comm: cat Not tainted 4.2.0-rc8+ #4
     Hardware name: Bochs Bochs, BIOS Bochs 01/01/2007
     task: ffff8800da1c0bc0 ti: ffff88011a05c000 task.ti: ffff88011a05c000
     RIP: 0010:[<ffffffff814cf6a0>]  [<ffffffff814cf6a0>] leaf_walk_rcu+0x10/0xe0
     RSP: 0018:ffff88011a05fda0  EFLAGS: 00010202
     RAX: ffff8800d8a40c00 RBX: ffff8800da4af940 RCX: ffff88011a05ff20
     RDX: ffffc90001630020 RSI: 0000000001013531 RDI: ffff8800da4af950
     RBP: 0000000000000000 R08: ffff8800da1f9a00 R09: 0000000000000000
     R10: ffff8800db45b7e4 R11: 0000000000000246 R12: ffff8800da4af950
     R13: ffff8800d97a74c0 R14: 0000000000000000 R15: ffff8800d97a7480
     FS:  00007fd3970e0700(0000) GS:ffff88011fd00000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
     CR2: ffffc90001630024 CR3: 000000011a7e4000 CR4: 00000000000006e0
     Stack:
      ffffffff814d00d3 0000000000000000 ffff88011a05ff20 ffff8800da1f9a00
      ffffffff811dd8b9 0000000000000800 0000000000020000 00007fd396f35000
      ffffffff811f8714 0000000000003431 ffffffff8138dce0 0000000000000f80
     Call Trace:
      [<ffffffff814d00d3>] ? fib_route_seq_start+0x93/0xc0
      [<ffffffff811dd8b9>] ? seq_read+0x149/0x380
      [<ffffffff811f8714>] ? fsnotify+0x3b4/0x500
      [<ffffffff8138dce0>] ? process_echoes+0x70/0x70
      [<ffffffff8121cfa7>] ? proc_reg_read+0x47/0x70
      [<ffffffff811bb823>] ? __vfs_read+0x23/0xd0
      [<ffffffff811bbd42>] ? rw_verify_area+0x52/0xf0
      [<ffffffff811bbe61>] ? vfs_read+0x81/0x120
      [<ffffffff811bcbc2>] ? SyS_read+0x42/0xa0
      [<ffffffff81549ab2>] ? entry_SYSCALL_64_fastpath+0x16/0x75
     Code: 48 85 c0 75 d8 f3 c3 31 c0 c3 f3 c3 66 66 66 66 66 66 2e 0f 1f 84 00 00
    a 04 89 f0 33 02 44 89 c9 48 d3 e8 0f b6 4a 05 49 89
     RIP  [<ffffffff814cf6a0>] leaf_walk_rcu+0x10/0xe0
      RSP <ffff88011a05fda0>
     CR2: ffffc90001630024
    
    Signed-off-by: Dave Forster <dforster@brocade.com>
    Acked-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Tested-by: Holger Hoffstätte <holger@applied-asynchrony.com>

commit 9a46b04f16a032c26bbf0ece61d6cd1e7ba9f627
Author: Brian Foster <bfoster@redhat.com>
Date:   Tue Jul 26 15:21:53 2016 -0700

    fs/fs-writeback.c: inode writeback list tracking tracepoints
    
    The per-sb inode writeback list tracks inodes currently under writeback
    to facilitate efficient sync processing.  In particular, it ensures that
    sync only needs to walk through a list of inodes that were cleaned by
    the sync.
    
    Add a couple tracepoints to help identify when inodes are added/removed
    to and from the writeback lists.  Piggyback off of the writeback
    lazytime tracepoint template as it already tracks the relevant inode
    information.
    
    Link: http://lkml.kernel.org/r/1466594593-6757-3-git-send-email-bfoster@redhat.com
    Signed-off-by: Brian Foster <bfoster@redhat.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Dave Chinner <dchinner@redhat.com>
    cc: Josef Bacik <jbacik@fb.com>
    Cc: Holger Hoffstätte <holger.hoffstaette@applied-asynchrony.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 6c60d2b5746cf23025ffe71bd7ff9075048fc90c
Author: Dave Chinner <dchinner@redhat.com>
Date:   Tue Jul 26 15:21:50 2016 -0700

    fs/fs-writeback.c: add a new writeback list for sync
    
    wait_sb_inodes() currently does a walk of all inodes in the filesystem
    to find dirty one to wait on during sync.  This is highly inefficient
    and wastes a lot of CPU when there are lots of clean cached inodes that
    we don't need to wait on.
    
    To avoid this "all inode" walk, we need to track inodes that are
    currently under writeback that we need to wait for.  We do this by
    adding inodes to a writeback list on the sb when the mapping is first
    tagged as having pages under writeback.  wait_sb_inodes() can then walk
    this list of "inodes under IO" and wait specifically just for the inodes
    that the current sync(2) needs to wait for.
    
    Define a couple helpers to add/remove an inode from the writeback list
    and call them when the overall mapping is tagged for or cleared from
    writeback.  Update wait_sb_inodes() to walk only the inodes under
    writeback due to the sync.
    
    With this change, filesystem sync times are significantly reduced for
    fs' with largely populated inode caches and otherwise no other work to
    do.  For example, on a 16xcpu 2GHz x86-64 server, 10TB XFS filesystem
    with a ~10m entry inode cache, sync times are reduced from ~7.3s to less
    than 0.1s when the filesystem is fully clean.
    
    Link: http://lkml.kernel.org/r/1466594593-6757-2-git-send-email-bfoster@redhat.com
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Signed-off-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Brian Foster <bfoster@redhat.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Tested-by: Holger Hoffstätte <holger.hoffstaette@applied-asynchrony.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 063b0dc8b46e56efa480bc2640fe01024f709482
Author: Eric Sandeen <sandeen@redhat.com>
Date:   Mon Jan 4 16:10:19 2016 +1100

    xfs: print name of verifier if it fails
    
    commit 233135b763db7c64d07b728a9c66745fb0376275 upstream.
    
    This adds a name to each buf_ops structure, so that if
    a verifier fails we can print the type of verifier that
    failed it.  Should be a slight debugging aid, I hope.
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Reviewed-by: Brian Foster <bfoster@redhat.com>
    Signed-off-by: Dave Chinner <david@fromorbit.com>
    Cc: Holger Hoffstätte <holger@applied-asynchrony.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ec02b076ceab63f99e5b3d80fd223d777266c236
Author: John Stultz <john.stultz@linaro.org>
Date:   Thu Dec 3 10:23:30 2015 -0800

    timekeeping: Cap adjustments so they don't exceed the maxadj value
    
    Thus its been occasionally noted that users have seen
    confusing warnings like:
    
        Adjusting tsc more than 11% (5941981 vs 7759439)
    
    We try to limit the maximum total adjustment to 11% (10% tick
    adjustment + 0.5% frequency adjustment). But this is done by
    bounding the requested adjustment values, and the internal
    steering that is done by tracking the error from what was
    requested and what was applied, does not have any such limits.
    
    This is usually not problematic, but in some cases has a risk
    that an adjustment could cause the clocksource mult value to
    overflow, so its an indication things are outside of what is
    expected.
    
    It ends up most of the reports of this 11% warning are on systems
    using chrony, which utilizes the adjtimex() ADJ_TICK interface
    (which allows a +-10% adjustment). The original rational for
    ADJ_TICK unclear to me but my assumption it was originally added
    to allow broken systems to get a big constant correction at boot
    (see adjtimex userspace package for an example) which would allow
    the system to work w/ ntpd's 0.5% adjustment limit.
    
    Chrony uses ADJ_TICK to make very aggressive short term corrections
    (usually right at startup). Which push us close enough to the max
    bound that a few late ticks can cause the internal steering to push
    past the max adjust value (tripping the warning).
    
    Thus this patch adds some extra logic to enforce the max adjustment
    cap in the internal steering.
    
    Note: This has the potential to slow corrections when the ADJ_TICK
    value is furthest away from the default value. So it would be good to
    get some testing from folks using chrony, to make sure we don't
    cause any troubles there.
    
    Cc: Miroslav Lichvar <mlichvar@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Tested-by: Miroslav Lichvar <mlichvar@redhat.com>
    Reported-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

commit 19867b6186f3fd38be65d86e20ef6f49d0caaa0b
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Sat Jul 4 12:44:59 2015 -0300

    perf trace: Use event filters for the event qualifier list
    
    We use raw_syscalls:sys_{enter,exit} events to show the syscalls, but were
    using a rather lazy/inneficient way to implement our 'strace -e' equivalent:
    filter out after reading the events in the ring buffer.
    
    Deflect more work to the kernel by appending a filter expression for that,
    that, together with the pid list, that is always present, if only to filter the
    tracer itself, reduces pressure on the ring buffer and otherwise use
    infrastructure already in place in the kernel to do early filtering.
    
    If we use it with -v we can see the filter passed to the kernel,
    for instance, for this contrieved case:
    
      # trace -v -e \!open,close,write,poll,recvfrom,select,recvmsg,writev,sendmsg,read,futex,epoll_wait,ioctl,eventfd --filter-pids 2189,2566,1398,2692,4475,4532
    <SNIP>
      (common_pid != 2514 && common_pid != 1398 && common_pid != 2189 && common_pid != 2566 && common_pid != 2692 && common_pid != 4475 && common_pid != 4532) && (id != 3 && id != 232 && id != 284 && id != 202 && id != 16 && id != 2 && id != 7 && id != 0 && id != 45 && id != 47 && id != 23 && id != 46 && id != 1 && id != 20)
         0.011 (0.011 ms): caribou/2295 eventfd2(flags: CLOEXEC|NONBLOCK) = 18
        16.946 (0.019 ms): caribou/2295 eventfd2(flags: CLOEXEC|NONBLOCK) = 18
        38.598 (0.167 ms): chronyd/794 socket(family: INET, type: DGRAM ) = 4
        38.603 (0.002 ms): chronyd/794 fcntl(fd: 4<socket:[239307]>, cmd: GETFD) = 0
        38.605 (0.001 ms): chronyd/794 fcntl(fd: 4<socket:[239307]>, cmd: SETFD, arg: 1) = 0
    ^C
     #
    
    Cc: Adrian Hunter <adrian.hunter@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: David Ahern <dsahern@gmail.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Stephane Eranian <eranian@google.com>
    Link: http://lkml.kernel.org/n/tip-ti2tg18atproqpguc2moinp6@git.kernel.org
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

commit f8cce9e338db5d455e62a4b9abbbeda3d5a0e203
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Feb 9 23:30:36 2015 -0800

    ntp: Fixup adjtimex freq validation on 32-bit systems
    
    commit 29183a70b0b828500816bd794b3fe192fce89f73 upstream.
    
    Additional validation of adjtimex freq values to avoid
    potential multiplication overflows were added in commit
    5e5aeb4367b (time: adjtimex: Validate the ADJ_FREQUENCY values)
    
    Unfortunately the patch used LONG_MAX/MIN instead of
    LLONG_MAX/MIN, which was fine on 64-bit systems, but being
    much smaller on 32-bit systems caused false positives
    resulting in most direct frequency adjustments to fail w/
    EINVAL.
    
    ntpd only does direct frequency adjustments at startup, so
    the issue was not as easily observed there, but other time
    sync applications like ptpd and chrony were more effected by
    the bug.
    
    See bugs:
    
      https://bugzilla.kernel.org/show_bug.cgi?id=92481
      https://bugzilla.redhat.com/show_bug.cgi?id=1188074
    
    This patch changes the checks to use LLONG_MAX for
    clarity, and additionally the checks are disabled
    on 32-bit systems since LLONG_MAX/PPM_SCALE is always
    larger then the 32-bit long freq value, so multiplication
    overflows aren't possible there.
    
    Reported-by: Josh Boyer <jwboyer@fedoraproject.org>
    Reported-by: George Joseph <george.joseph@fairview5.com>
    Tested-by: George Joseph <george.joseph@fairview5.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Link: http://lkml.kernel.org/r/1423553436-29747-1-git-send-email-john.stultz@linaro.org
    [ Prettified the changelog and the comments a bit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Zefan Li <lizefan@huawei.com>

commit 8d30871bfd929e5d931f51caf831b3c318ffcabe
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Feb 9 23:30:36 2015 -0800

    ntp: Fixup adjtimex freq validation on 32-bit systems
    
    commit 29183a70b0b828500816bd794b3fe192fce89f73 upstream.
    
    Additional validation of adjtimex freq values to avoid
    potential multiplication overflows were added in commit
    5e5aeb4367b (time: adjtimex: Validate the ADJ_FREQUENCY values)
    
    Unfortunately the patch used LONG_MAX/MIN instead of
    LLONG_MAX/MIN, which was fine on 64-bit systems, but being
    much smaller on 32-bit systems caused false positives
    resulting in most direct frequency adjustments to fail w/
    EINVAL.
    
    ntpd only does direct frequency adjustments at startup, so
    the issue was not as easily observed there, but other time
    sync applications like ptpd and chrony were more effected by
    the bug.
    
    See bugs:
    
      https://bugzilla.kernel.org/show_bug.cgi?id=92481
      https://bugzilla.redhat.com/show_bug.cgi?id=1188074
    
    This patch changes the checks to use LLONG_MAX for
    clarity, and additionally the checks are disabled
    on 32-bit systems since LLONG_MAX/PPM_SCALE is always
    larger then the 32-bit long freq value, so multiplication
    overflows aren't possible there.
    
    Reported-by: Josh Boyer <jwboyer@fedoraproject.org>
    Reported-by: George Joseph <george.joseph@fairview5.com>
    Tested-by: George Joseph <george.joseph@fairview5.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Link: http://lkml.kernel.org/r/1423553436-29747-1-git-send-email-john.stultz@linaro.org
    [ Prettified the changelog and the comments a bit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Cc: Christian Riesch <christian.riesch@omicron.at>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit 0bcbb5825c03329bcbe8a8c0b4a0aa20cec8a86a
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Feb 9 23:30:36 2015 -0800

    ntp: Fixup adjtimex freq validation on 32-bit systems
    
    commit 29183a70b0b828500816bd794b3fe192fce89f73 upstream.
    
    Additional validation of adjtimex freq values to avoid
    potential multiplication overflows were added in commit
    5e5aeb4367b (time: adjtimex: Validate the ADJ_FREQUENCY values)
    
    Unfortunately the patch used LONG_MAX/MIN instead of
    LLONG_MAX/MIN, which was fine on 64-bit systems, but being
    much smaller on 32-bit systems caused false positives
    resulting in most direct frequency adjustments to fail w/
    EINVAL.
    
    ntpd only does direct frequency adjustments at startup, so
    the issue was not as easily observed there, but other time
    sync applications like ptpd and chrony were more effected by
    the bug.
    
    See bugs:
    
      https://bugzilla.kernel.org/show_bug.cgi?id=92481
      https://bugzilla.redhat.com/show_bug.cgi?id=1188074
    
    This patch changes the checks to use LLONG_MAX for
    clarity, and additionally the checks are disabled
    on 32-bit systems since LLONG_MAX/PPM_SCALE is always
    larger then the 32-bit long freq value, so multiplication
    overflows aren't possible there.
    
    Reported-by: Josh Boyer <jwboyer@fedoraproject.org>
    Reported-by: George Joseph <george.joseph@fairview5.com>
    Tested-by: George Joseph <george.joseph@fairview5.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Link: http://lkml.kernel.org/r/1423553436-29747-1-git-send-email-john.stultz@linaro.org
    [ Prettified the changelog and the comments a bit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 20dcda8d40db528865b1aa5053b39e0a803df693
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Feb 9 23:30:36 2015 -0800

    ntp: Fixup adjtimex freq validation on 32-bit systems
    
    commit 29183a70b0b828500816bd794b3fe192fce89f73 upstream.
    
    Additional validation of adjtimex freq values to avoid
    potential multiplication overflows were added in commit
    5e5aeb4367b (time: adjtimex: Validate the ADJ_FREQUENCY values)
    
    Unfortunately the patch used LONG_MAX/MIN instead of
    LLONG_MAX/MIN, which was fine on 64-bit systems, but being
    much smaller on 32-bit systems caused false positives
    resulting in most direct frequency adjustments to fail w/
    EINVAL.
    
    ntpd only does direct frequency adjustments at startup, so
    the issue was not as easily observed there, but other time
    sync applications like ptpd and chrony were more effected by
    the bug.
    
    See bugs:
    
      https://bugzilla.kernel.org/show_bug.cgi?id=92481
      https://bugzilla.redhat.com/show_bug.cgi?id=1188074
    
    This patch changes the checks to use LLONG_MAX for
    clarity, and additionally the checks are disabled
    on 32-bit systems since LLONG_MAX/PPM_SCALE is always
    larger then the 32-bit long freq value, so multiplication
    overflows aren't possible there.
    
    Reported-by: Josh Boyer <jwboyer@fedoraproject.org>
    Reported-by: George Joseph <george.joseph@fairview5.com>
    Tested-by: George Joseph <george.joseph@fairview5.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Link: http://lkml.kernel.org/r/1423553436-29747-1-git-send-email-john.stultz@linaro.org
    [ Prettified the changelog and the comments a bit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5e2b0b3f502ec060479d3f266b2cf9ff40ce8d2a
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Feb 9 23:30:36 2015 -0800

    ntp: Fixup adjtimex freq validation on 32-bit systems
    
    commit 29183a70b0b828500816bd794b3fe192fce89f73 upstream.
    
    Additional validation of adjtimex freq values to avoid
    potential multiplication overflows were added in commit
    5e5aeb4367b (time: adjtimex: Validate the ADJ_FREQUENCY values)
    
    Unfortunately the patch used LONG_MAX/MIN instead of
    LLONG_MAX/MIN, which was fine on 64-bit systems, but being
    much smaller on 32-bit systems caused false positives
    resulting in most direct frequency adjustments to fail w/
    EINVAL.
    
    ntpd only does direct frequency adjustments at startup, so
    the issue was not as easily observed there, but other time
    sync applications like ptpd and chrony were more effected by
    the bug.
    
    See bugs:
    
      https://bugzilla.kernel.org/show_bug.cgi?id=92481
      https://bugzilla.redhat.com/show_bug.cgi?id=1188074
    
    This patch changes the checks to use LLONG_MAX for
    clarity, and additionally the checks are disabled
    on 32-bit systems since LLONG_MAX/PPM_SCALE is always
    larger then the 32-bit long freq value, so multiplication
    overflows aren't possible there.
    
    Reported-by: Josh Boyer <jwboyer@fedoraproject.org>
    Reported-by: George Joseph <george.joseph@fairview5.com>
    Tested-by: George Joseph <george.joseph@fairview5.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Link: http://lkml.kernel.org/r/1423553436-29747-1-git-send-email-john.stultz@linaro.org
    [ Prettified the changelog and the comments a bit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 72e2d609260a5a7515d4c9d2d709759fc282e552
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Feb 9 23:30:36 2015 -0800

    ntp: Fixup adjtimex freq validation on 32-bit systems
    
    commit 29183a70b0b828500816bd794b3fe192fce89f73 upstream.
    
    Additional validation of adjtimex freq values to avoid
    potential multiplication overflows were added in commit
    5e5aeb4367b (time: adjtimex: Validate the ADJ_FREQUENCY values)
    
    Unfortunately the patch used LONG_MAX/MIN instead of
    LLONG_MAX/MIN, which was fine on 64-bit systems, but being
    much smaller on 32-bit systems caused false positives
    resulting in most direct frequency adjustments to fail w/
    EINVAL.
    
    ntpd only does direct frequency adjustments at startup, so
    the issue was not as easily observed there, but other time
    sync applications like ptpd and chrony were more effected by
    the bug.
    
    See bugs:
    
      https://bugzilla.kernel.org/show_bug.cgi?id=92481
      https://bugzilla.redhat.com/show_bug.cgi?id=1188074
    
    This patch changes the checks to use LLONG_MAX for
    clarity, and additionally the checks are disabled
    on 32-bit systems since LLONG_MAX/PPM_SCALE is always
    larger then the 32-bit long freq value, so multiplication
    overflows aren't possible there.
    
    Reported-by: Josh Boyer <jwboyer@fedoraproject.org>
    Reported-by: George Joseph <george.joseph@fairview5.com>
    Tested-by: George Joseph <george.joseph@fairview5.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Link: http://lkml.kernel.org/r/1423553436-29747-1-git-send-email-john.stultz@linaro.org
    [ Prettified the changelog and the comments a bit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b2fcc089bcb64b2f1667eb3046cb72f9529ec2bc
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Feb 9 23:30:36 2015 -0800

    ntp: Fixup adjtimex freq validation on 32-bit systems
    
    commit 29183a70b0b828500816bd794b3fe192fce89f73 upstream.
    
    Additional validation of adjtimex freq values to avoid
    potential multiplication overflows were added in commit
    5e5aeb4367b (time: adjtimex: Validate the ADJ_FREQUENCY values)
    
    Unfortunately the patch used LONG_MAX/MIN instead of
    LLONG_MAX/MIN, which was fine on 64-bit systems, but being
    much smaller on 32-bit systems caused false positives
    resulting in most direct frequency adjustments to fail w/
    EINVAL.
    
    ntpd only does direct frequency adjustments at startup, so
    the issue was not as easily observed there, but other time
    sync applications like ptpd and chrony were more effected by
    the bug.
    
    See bugs:
    
      https://bugzilla.kernel.org/show_bug.cgi?id=92481
      https://bugzilla.redhat.com/show_bug.cgi?id=1188074
    
    This patch changes the checks to use LLONG_MAX for
    clarity, and additionally the checks are disabled
    on 32-bit systems since LLONG_MAX/PPM_SCALE is always
    larger then the 32-bit long freq value, so multiplication
    overflows aren't possible there.
    
    Reported-by: Josh Boyer <jwboyer@fedoraproject.org>
    Reported-by: George Joseph <george.joseph@fairview5.com>
    Tested-by: George Joseph <george.joseph@fairview5.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Link: http://lkml.kernel.org/r/1423553436-29747-1-git-send-email-john.stultz@linaro.org
    [ Prettified the changelog and the comments a bit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 06246b44e252dfa2f6b7d91cf8e603d31987ea9c
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Feb 9 23:30:36 2015 -0800

    ntp: Fixup adjtimex freq validation on 32-bit systems
    
    commit 29183a70b0b828500816bd794b3fe192fce89f73 upstream.
    
    Additional validation of adjtimex freq values to avoid
    potential multiplication overflows were added in commit
    5e5aeb4367b (time: adjtimex: Validate the ADJ_FREQUENCY values)
    
    Unfortunately the patch used LONG_MAX/MIN instead of
    LLONG_MAX/MIN, which was fine on 64-bit systems, but being
    much smaller on 32-bit systems caused false positives
    resulting in most direct frequency adjustments to fail w/
    EINVAL.
    
    ntpd only does direct frequency adjustments at startup, so
    the issue was not as easily observed there, but other time
    sync applications like ptpd and chrony were more effected by
    the bug.
    
    See bugs:
    
      https://bugzilla.kernel.org/show_bug.cgi?id=92481
      https://bugzilla.redhat.com/show_bug.cgi?id=1188074
    
    This patch changes the checks to use LLONG_MAX for
    clarity, and additionally the checks are disabled
    on 32-bit systems since LLONG_MAX/PPM_SCALE is always
    larger then the 32-bit long freq value, so multiplication
    overflows aren't possible there.
    
    Reported-by: Josh Boyer <jwboyer@fedoraproject.org>
    Reported-by: George Joseph <george.joseph@fairview5.com>
    Tested-by: George Joseph <george.joseph@fairview5.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Link: http://lkml.kernel.org/r/1423553436-29747-1-git-send-email-john.stultz@linaro.org
    [ Prettified the changelog and the comments a bit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit 29183a70b0b828500816bd794b3fe192fce89f73
Author: John Stultz <john.stultz@linaro.org>
Date:   Mon Feb 9 23:30:36 2015 -0800

    ntp: Fixup adjtimex freq validation on 32-bit systems
    
    Additional validation of adjtimex freq values to avoid
    potential multiplication overflows were added in commit
    5e5aeb4367b (time: adjtimex: Validate the ADJ_FREQUENCY values)
    
    Unfortunately the patch used LONG_MAX/MIN instead of
    LLONG_MAX/MIN, which was fine on 64-bit systems, but being
    much smaller on 32-bit systems caused false positives
    resulting in most direct frequency adjustments to fail w/
    EINVAL.
    
    ntpd only does direct frequency adjustments at startup, so
    the issue was not as easily observed there, but other time
    sync applications like ptpd and chrony were more effected by
    the bug.
    
    See bugs:
    
      https://bugzilla.kernel.org/show_bug.cgi?id=92481
      https://bugzilla.redhat.com/show_bug.cgi?id=1188074
    
    This patch changes the checks to use LLONG_MAX for
    clarity, and additionally the checks are disabled
    on 32-bit systems since LLONG_MAX/PPM_SCALE is always
    larger then the 32-bit long freq value, so multiplication
    overflows aren't possible there.
    
    Reported-by: Josh Boyer <jwboyer@fedoraproject.org>
    Reported-by: George Joseph <george.joseph@fairview5.com>
    Tested-by: George Joseph <george.joseph@fairview5.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <stable@vger.kernel.org> # v3.19+
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Link: http://lkml.kernel.org/r/1423553436-29747-1-git-send-email-john.stultz@linaro.org
    [ Prettified the changelog and the comments a bit. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 795f18956b547f0cfbcaa6d907d2bb05726a6a3e
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Fri Feb 3 22:31:13 2012 +0530

    perf evsel: Fix an issue where perf report fails to show the proper percentage
    
    commit a4a03fc7ef89020baca4f19174e6a43767c6d78a upstream.
    
    This patch fixes an issue where perf report shows nan% for certain
    perf.data files. The below is from a report for a do_fork probe:
    
       -nan%           sshd  [kernel.kallsyms]  [k] do_fork
       -nan%    packagekitd  [kernel.kallsyms]  [k] do_fork
       -nan%    dbus-daemon  [kernel.kallsyms]  [k] do_fork
       -nan%           bash  [kernel.kallsyms]  [k] do_fork
    
    A git bisect shows commit f3bda2c as the cause. However, looking back
    through the git history, I saw commit 640c03c which seems to have
    removed the required initialization for perf_sample->period. The problem
    only started showing after commit f3bda2c. The below patch re-introduces
    the initialization and it fixes the problem for me.
    
    With the below patch, for the same perf.data:
    
      73.08%             bash  [kernel.kallsyms]  [k] do_fork
       8.97%      11-dhclient  [kernel.kallsyms]  [k] do_fork
       6.41%             sshd  [kernel.kallsyms]  [k] do_fork
       3.85%        20-chrony  [kernel.kallsyms]  [k] do_fork
       2.56%         sendmail  [kernel.kallsyms]  [k] do_fork
    
    This patch applies over current linux-tip commit 9949284.
    
    Problem introduced in:
    
    $ git describe 640c03c
    v2.6.37-rc3-83-g640c03c
    
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20120203170113.5190.25558.stgit@localhost6.localdomain6
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 838d7aabe2a5c93aa7116f1a74758731ba1a0264
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Fri Feb 3 22:31:13 2012 +0530

    perf evsel: Fix an issue where perf report fails to show the proper percentage
    
    commit a4a03fc7ef89020baca4f19174e6a43767c6d78a upstream.
    
    This patch fixes an issue where perf report shows nan% for certain
    perf.data files. The below is from a report for a do_fork probe:
    
       -nan%           sshd  [kernel.kallsyms]  [k] do_fork
       -nan%    packagekitd  [kernel.kallsyms]  [k] do_fork
       -nan%    dbus-daemon  [kernel.kallsyms]  [k] do_fork
       -nan%           bash  [kernel.kallsyms]  [k] do_fork
    
    A git bisect shows commit f3bda2c as the cause. However, looking back
    through the git history, I saw commit 640c03c which seems to have
    removed the required initialization for perf_sample->period. The problem
    only started showing after commit f3bda2c. The below patch re-introduces
    the initialization and it fixes the problem for me.
    
    With the below patch, for the same perf.data:
    
      73.08%             bash  [kernel.kallsyms]  [k] do_fork
       8.97%      11-dhclient  [kernel.kallsyms]  [k] do_fork
       6.41%             sshd  [kernel.kallsyms]  [k] do_fork
       3.85%        20-chrony  [kernel.kallsyms]  [k] do_fork
       2.56%         sendmail  [kernel.kallsyms]  [k] do_fork
    
    This patch applies over current linux-tip commit 9949284.
    
    Problem introduced in:
    
    $ git describe 640c03c
    v2.6.37-rc3-83-g640c03c
    
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20120203170113.5190.25558.stgit@localhost6.localdomain6
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a4a03fc7ef89020baca4f19174e6a43767c6d78a
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Fri Feb 3 22:31:13 2012 +0530

    perf evsel: Fix an issue where perf report fails to show the proper percentage
    
    This patch fixes an issue where perf report shows nan% for certain
    perf.data files. The below is from a report for a do_fork probe:
    
       -nan%           sshd  [kernel.kallsyms]  [k] do_fork
       -nan%    packagekitd  [kernel.kallsyms]  [k] do_fork
       -nan%    dbus-daemon  [kernel.kallsyms]  [k] do_fork
       -nan%           bash  [kernel.kallsyms]  [k] do_fork
    
    A git bisect shows commit f3bda2c as the cause. However, looking back
    through the git history, I saw commit 640c03c which seems to have
    removed the required initialization for perf_sample->period. The problem
    only started showing after commit f3bda2c. The below patch re-introduces
    the initialization and it fixes the problem for me.
    
    With the below patch, for the same perf.data:
    
      73.08%             bash  [kernel.kallsyms]  [k] do_fork
       8.97%      11-dhclient  [kernel.kallsyms]  [k] do_fork
       6.41%             sshd  [kernel.kallsyms]  [k] do_fork
       3.85%        20-chrony  [kernel.kallsyms]  [k] do_fork
       2.56%         sendmail  [kernel.kallsyms]  [k] do_fork
    
    This patch applies over current linux-tip commit 9949284.
    
    Problem introduced in:
    
    $ git describe 640c03c
    v2.6.37-rc3-83-g640c03c
    
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: stable@kernel.org
    Link: http://lkml.kernel.org/r/20120203170113.5190.25558.stgit@localhost6.localdomain6
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

commit bc0fb201b34b12e2d16e8cbd5bb078c1db936304
Author: Chuck Lever <cel@netapp.com>
Date:   Mon Mar 20 13:44:31 2006 -0500

    NFS: create common routine for waiting for direct I/O to complete
    
    We're about to add asynchrony to the NFS direct write path.  Begin by
    abstracting out the common pieces in the read path.
    
    The first piece is nfs_direct_read_wait, which works the same whether the
    process is waiting for a read or a write.
    
    Test plan:
    Compile kernel with CONFIG_NFS and CONFIG_NFS_DIRECTIO enabled.
    
    Signed-off-by: Chuck Lever <cel@netapp.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>
