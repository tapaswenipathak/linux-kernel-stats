commit 05382ed9142cf8a8a3fb662224477eecc415778b
Author: Arun Easi <aeasi@marvell.com>
Date:   Tue Nov 29 01:26:34 2022 -0800

    scsi: qla2xxx: Fix crash when I/O abort times out
    
    commit 68ad83188d782b2ecef2e41ac245d27e0710fe8e upstream.
    
    While performing CPU hotplug, a crash with the following stack was seen:
    
    Call Trace:
         qla24xx_process_response_queue+0x42a/0x970 [qla2xxx]
         qla2x00_start_nvme_mq+0x3a2/0x4b0 [qla2xxx]
         qla_nvme_post_cmd+0x166/0x240 [qla2xxx]
         nvme_fc_start_fcp_op.part.0+0x119/0x2e0 [nvme_fc]
         blk_mq_dispatch_rq_list+0x17b/0x610
         __blk_mq_sched_dispatch_requests+0xb0/0x140
         blk_mq_sched_dispatch_requests+0x30/0x60
         __blk_mq_run_hw_queue+0x35/0x90
         __blk_mq_delay_run_hw_queue+0x161/0x180
         blk_execute_rq+0xbe/0x160
         __nvme_submit_sync_cmd+0x16f/0x220 [nvme_core]
         nvmf_connect_admin_queue+0x11a/0x170 [nvme_fabrics]
         nvme_fc_create_association.cold+0x50/0x3dc [nvme_fc]
         nvme_fc_connect_ctrl_work+0x19/0x30 [nvme_fc]
         process_one_work+0x1e8/0x3c0
    
    On abort timeout, completion was called without checking if the I/O was
    already completed.
    
    Verify that I/O and abort request are indeed outstanding before attempting
    completion.
    
    Fixes: 71c80b75ce8f ("scsi: qla2xxx: Do command completion on abort timeout")
    Reported-by: Marco Patalano <mpatalan@redhat.com>
    Tested-by: Marco Patalano <mpatalan@redhat.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Arun Easi <aeasi@marvell.com>
    Signed-off-by: Nilesh Javali <njavali@marvell.com>
    Link: https://lore.kernel.org/r/20221129092634.15347-1-njavali@marvell.com
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit cb4dff498468b62e8c520568559b3a9007e104d7
Author: Arun Easi <aeasi@marvell.com>
Date:   Tue Nov 29 01:26:34 2022 -0800

    scsi: qla2xxx: Fix crash when I/O abort times out
    
    commit 68ad83188d782b2ecef2e41ac245d27e0710fe8e upstream.
    
    While performing CPU hotplug, a crash with the following stack was seen:
    
    Call Trace:
         qla24xx_process_response_queue+0x42a/0x970 [qla2xxx]
         qla2x00_start_nvme_mq+0x3a2/0x4b0 [qla2xxx]
         qla_nvme_post_cmd+0x166/0x240 [qla2xxx]
         nvme_fc_start_fcp_op.part.0+0x119/0x2e0 [nvme_fc]
         blk_mq_dispatch_rq_list+0x17b/0x610
         __blk_mq_sched_dispatch_requests+0xb0/0x140
         blk_mq_sched_dispatch_requests+0x30/0x60
         __blk_mq_run_hw_queue+0x35/0x90
         __blk_mq_delay_run_hw_queue+0x161/0x180
         blk_execute_rq+0xbe/0x160
         __nvme_submit_sync_cmd+0x16f/0x220 [nvme_core]
         nvmf_connect_admin_queue+0x11a/0x170 [nvme_fabrics]
         nvme_fc_create_association.cold+0x50/0x3dc [nvme_fc]
         nvme_fc_connect_ctrl_work+0x19/0x30 [nvme_fc]
         process_one_work+0x1e8/0x3c0
    
    On abort timeout, completion was called without checking if the I/O was
    already completed.
    
    Verify that I/O and abort request are indeed outstanding before attempting
    completion.
    
    Fixes: 71c80b75ce8f ("scsi: qla2xxx: Do command completion on abort timeout")
    Reported-by: Marco Patalano <mpatalan@redhat.com>
    Tested-by: Marco Patalano <mpatalan@redhat.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Arun Easi <aeasi@marvell.com>
    Signed-off-by: Nilesh Javali <njavali@marvell.com>
    Link: https://lore.kernel.org/r/20221129092634.15347-1-njavali@marvell.com
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d3871af13aa03fbbe7fbb812eaf140501229a72e
Author: Arun Easi <aeasi@marvell.com>
Date:   Tue Nov 29 01:26:34 2022 -0800

    scsi: qla2xxx: Fix crash when I/O abort times out
    
    commit 68ad83188d782b2ecef2e41ac245d27e0710fe8e upstream.
    
    While performing CPU hotplug, a crash with the following stack was seen:
    
    Call Trace:
         qla24xx_process_response_queue+0x42a/0x970 [qla2xxx]
         qla2x00_start_nvme_mq+0x3a2/0x4b0 [qla2xxx]
         qla_nvme_post_cmd+0x166/0x240 [qla2xxx]
         nvme_fc_start_fcp_op.part.0+0x119/0x2e0 [nvme_fc]
         blk_mq_dispatch_rq_list+0x17b/0x610
         __blk_mq_sched_dispatch_requests+0xb0/0x140
         blk_mq_sched_dispatch_requests+0x30/0x60
         __blk_mq_run_hw_queue+0x35/0x90
         __blk_mq_delay_run_hw_queue+0x161/0x180
         blk_execute_rq+0xbe/0x160
         __nvme_submit_sync_cmd+0x16f/0x220 [nvme_core]
         nvmf_connect_admin_queue+0x11a/0x170 [nvme_fabrics]
         nvme_fc_create_association.cold+0x50/0x3dc [nvme_fc]
         nvme_fc_connect_ctrl_work+0x19/0x30 [nvme_fc]
         process_one_work+0x1e8/0x3c0
    
    On abort timeout, completion was called without checking if the I/O was
    already completed.
    
    Verify that I/O and abort request are indeed outstanding before attempting
    completion.
    
    Fixes: 71c80b75ce8f ("scsi: qla2xxx: Do command completion on abort timeout")
    Reported-by: Marco Patalano <mpatalan@redhat.com>
    Tested-by: Marco Patalano <mpatalan@redhat.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Arun Easi <aeasi@marvell.com>
    Signed-off-by: Nilesh Javali <njavali@marvell.com>
    Link: https://lore.kernel.org/r/20221129092634.15347-1-njavali@marvell.com
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bf57ae2165bad1cb273095a5c09708ab503cd874
Merge: add769595757 d6962c4fe8f9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 15:33:42 2022 -0800

    Merge tag 'sched-core-2022-12-12' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - Implement persistent user-requested affinity: introduce
       affinity_context::user_mask and unconditionally preserve the
       user-requested CPU affinity masks, for long-lived tasks to better
       interact with cpusets & CPU hotplug events over longer timespans,
       without destroying the original affinity intent if the underlying
       topology changes.
    
     - Uclamp updates: fix relationship between uclamp and fits_capacity()
    
     - PSI fixes
    
     - Misc fixes & updates
    
    * tag 'sched-core-2022-12-12' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Clear ttwu_pending after enqueue_task()
      sched/psi: Use task->psi_flags to clear in CPU migration
      sched/psi: Stop relying on timer_pending() for poll_work rescheduling
      sched/psi: Fix avgs_work re-arm in psi_avgs_work()
      sched/psi: Fix possible missing or delayed pending event
      sched: Always clear user_cpus_ptr in do_set_cpus_allowed()
      sched: Enforce user requested affinity
      sched: Always preserve the user requested cpumask
      sched: Introduce affinity_context
      sched: Add __releases annotations to affine_move_task()
      sched/fair: Check if prev_cpu has highest spare cap in feec()
      sched/fair: Consider capacity inversion in util_fits_cpu()
      sched/fair: Detect capacity inversion
      sched/uclamp: Cater for uclamp in find_energy_efficient_cpu()'s early exit condition
      sched/uclamp: Make cpu_overutilized() use util_fits_cpu()
      sched/uclamp: Make asym_fits_capacity() use util_fits_cpu()
      sched/uclamp: Make select_idle_capacity() use util_fits_cpu()
      sched/uclamp: Fix fits_capacity() check in feec()
      sched/uclamp: Make task_fits_capacity() use util_fits_cpu()
      sched/uclamp: Fix relationship between uclamp and migration margin

commit 08d72bd299d08ae019a8a999b95f3e72643a11a4
Merge: 9d33edb20f7e 6f855b39e460
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 11:56:59 2022 -0800

    Merge tag 'smp-core-2022-12-10' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug updates from Thomas Gleixner:
     "A small set of updates for CPU hotplug:
    
       - Prevent stale CPU hotplug state in the cpu_down() path which was
         detected by stress testing the sysfs interface
    
       - Ensure that the target CPU hotplug state for the boot CPU is
         CPUHP_ONLINE instead of the compile time init value CPUHP_OFFLINE.
    
       - Switch back to the original behaviour of warning when a CPU hotplug
         callback in the DYING/STARTING section returns an error code.
    
         Otherwise a buggy callback can leave the CPUs in an non recoverable
         state"
    
    * tag 'smp-core-2022-12-10' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Do not bail-out in DYING/STARTING sections
      cpu/hotplug: Set cpuhp target for boot cpu
      cpu/hotplug: Make target_store() a nop when target == state

commit 9581f8a00777a073fdd8146659a51ca007cae8d6
Author: Nathan Lynch <nathanl@linux.ibm.com>
Date:   Fri Nov 18 09:07:45 2022 -0600

    powerpc/rtas: clean up includes
    
    rtas.c used to host complex code related to pseries-specific guest
    migration and suspend, which used atomics, completions, hcalls, and
    CPU hotplug APIs. That's all been deleted or moved, so remove the
    include directives that have been rendered unnecessary. Sort the
    remainder (with linux/ before asm/) to impose some order on where
    future additions go.
    
    Signed-off-by: Nathan Lynch <nathanl@linux.ibm.com>
    Reviewed-by: Andrew Donnellan <ajd@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221118150751.469393-8-nathanl@linux.ibm.com

commit e13d23a404f2e6dfaf8b1ef7d161a0836fce4fa5
Author: Laurent Dufour <ldufour@linux.ibm.com>
Date:   Thu Nov 10 19:06:18 2022 +0100

    powerpc: export the CPU node count
    
    At boot time, the FDT is parsed to compute the number of CPUs.
    In addition count the number of CPU nodes and export it.
    
    This is useful when building the FDT for a kexeced kernel since we need to
    take in account the CPU node added since the boot time during CPU hotplug
    operations.
    
    Signed-off-by: Laurent Dufour <ldufour@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221110180619.15796-2-ldufour@linux.ibm.com

commit 31dd08b9efe7dfdc62ce0d871065ef5126b642d8
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Thu Nov 3 20:06:01 2022 +0100

    x86/hyperv: Restore VP assist page after cpu offlining/onlining
    
    [ Upstream commit ee6815416380bc069b7dcbdff0682d4c53617527 ]
    
    Commit e5d9b714fe40 ("x86/hyperv: fix root partition faults when writing
    to VP assist page MSR") moved 'wrmsrl(HV_X64_MSR_VP_ASSIST_PAGE)' under
    'if (*hvp)' condition. This works for root partition as hv_cpu_die()
    does memunmap() and sets 'hv_vp_assist_page[cpu]' to NULL but breaks
    non-root partitions as hv_cpu_die() doesn't free 'hv_vp_assist_page[cpu]'
    for them. This causes VP assist page to remain unset after CPU
    offline/online cycle:
    
    $ rdmsr -p 24 0x40000073
      10212f001
    $ echo 0 > /sys/devices/system/cpu/cpu24/online
    $ echo 1 > /sys/devices/system/cpu/cpu24/online
    $ rdmsr -p 24 0x40000073
      0
    
    Fix the issue by always writing to HV_X64_MSR_VP_ASSIST_PAGE in
    hv_cpu_init(). Note, checking 'if (!*hvp)', for root partition is
    pointless as hv_cpu_die() always sets 'hv_vp_assist_page[cpu]' to
    NULL (and it's also NULL initially).
    
    Note: the fact that 'hv_vp_assist_page[cpu]' is reset to NULL may
    present a (potential) issue for KVM. While Hyper-V uses
    CPUHP_AP_ONLINE_DYN stage in CPU hotplug, KVM uses CPUHP_AP_KVM_STARTING
    which comes earlier in CPU teardown sequence. It is theoretically
    possible that Enlightened VMCS is still in use. It is unclear if the
    issue is real and if using KVM with Hyper-V root partition is even
    possible.
    
    While on it, drop the unneeded smp_processor_id() call from hv_cpu_init().
    
    Fixes: e5d9b714fe40 ("x86/hyperv: fix root partition faults when writing to VP assist page MSR")
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com>
    Link: https://lore.kernel.org/r/20221103190601.399343-1-vkuznets@redhat.com
    Signed-off-by: Wei Liu <wei.liu@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit c2153fe2d0c6d9a518efb8ca50e56bebbf6cbc9d
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Thu Nov 3 20:06:01 2022 +0100

    x86/hyperv: Restore VP assist page after cpu offlining/onlining
    
    [ Upstream commit ee6815416380bc069b7dcbdff0682d4c53617527 ]
    
    Commit e5d9b714fe40 ("x86/hyperv: fix root partition faults when writing
    to VP assist page MSR") moved 'wrmsrl(HV_X64_MSR_VP_ASSIST_PAGE)' under
    'if (*hvp)' condition. This works for root partition as hv_cpu_die()
    does memunmap() and sets 'hv_vp_assist_page[cpu]' to NULL but breaks
    non-root partitions as hv_cpu_die() doesn't free 'hv_vp_assist_page[cpu]'
    for them. This causes VP assist page to remain unset after CPU
    offline/online cycle:
    
    $ rdmsr -p 24 0x40000073
      10212f001
    $ echo 0 > /sys/devices/system/cpu/cpu24/online
    $ echo 1 > /sys/devices/system/cpu/cpu24/online
    $ rdmsr -p 24 0x40000073
      0
    
    Fix the issue by always writing to HV_X64_MSR_VP_ASSIST_PAGE in
    hv_cpu_init(). Note, checking 'if (!*hvp)', for root partition is
    pointless as hv_cpu_die() always sets 'hv_vp_assist_page[cpu]' to
    NULL (and it's also NULL initially).
    
    Note: the fact that 'hv_vp_assist_page[cpu]' is reset to NULL may
    present a (potential) issue for KVM. While Hyper-V uses
    CPUHP_AP_ONLINE_DYN stage in CPU hotplug, KVM uses CPUHP_AP_KVM_STARTING
    which comes earlier in CPU teardown sequence. It is theoretically
    possible that Enlightened VMCS is still in use. It is unclear if the
    issue is real and if using KVM with Hyper-V root partition is even
    possible.
    
    While on it, drop the unneeded smp_processor_id() call from hv_cpu_init().
    
    Fixes: e5d9b714fe40 ("x86/hyperv: fix root partition faults when writing to VP assist page MSR")
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com>
    Link: https://lore.kernel.org/r/20221103190601.399343-1-vkuznets@redhat.com
    Signed-off-by: Wei Liu <wei.liu@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 68ad83188d782b2ecef2e41ac245d27e0710fe8e
Author: Arun Easi <aeasi@marvell.com>
Date:   Tue Nov 29 01:26:34 2022 -0800

    scsi: qla2xxx: Fix crash when I/O abort times out
    
    While performing CPU hotplug, a crash with the following stack was seen:
    
    Call Trace:
         qla24xx_process_response_queue+0x42a/0x970 [qla2xxx]
         qla2x00_start_nvme_mq+0x3a2/0x4b0 [qla2xxx]
         qla_nvme_post_cmd+0x166/0x240 [qla2xxx]
         nvme_fc_start_fcp_op.part.0+0x119/0x2e0 [nvme_fc]
         blk_mq_dispatch_rq_list+0x17b/0x610
         __blk_mq_sched_dispatch_requests+0xb0/0x140
         blk_mq_sched_dispatch_requests+0x30/0x60
         __blk_mq_run_hw_queue+0x35/0x90
         __blk_mq_delay_run_hw_queue+0x161/0x180
         blk_execute_rq+0xbe/0x160
         __nvme_submit_sync_cmd+0x16f/0x220 [nvme_core]
         nvmf_connect_admin_queue+0x11a/0x170 [nvme_fabrics]
         nvme_fc_create_association.cold+0x50/0x3dc [nvme_fc]
         nvme_fc_connect_ctrl_work+0x19/0x30 [nvme_fc]
         process_one_work+0x1e8/0x3c0
    
    On abort timeout, completion was called without checking if the I/O was
    already completed.
    
    Verify that I/O and abort request are indeed outstanding before attempting
    completion.
    
    Fixes: 71c80b75ce8f ("scsi: qla2xxx: Do command completion on abort timeout")
    Reported-by: Marco Patalano <mpatalan@redhat.com>
    Tested-by: Marco Patalano <mpatalan@redhat.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Arun Easi <aeasi@marvell.com>
    Signed-off-by: Nilesh Javali <njavali@marvell.com>
    Link: https://lore.kernel.org/r/20221129092634.15347-1-njavali@marvell.com
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit c5527c1787e84533f7b43fadb2d050e1ed115a50
Merge: ae27e8869fdb c767c3474013
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Tue Nov 29 13:22:45 2022 +0100

    Merge tag 'coresight-next-v6.2' of git://git.kernel.org/pub/scm/linux/kernel/git/coresight/linux into char-misc-next
    
    Suzuki writes:
    
    coresight: Update for v6.2
    
    CoreSight updatesfor v6.2 includes :
    
     - Support for ETMv4 probing on hotplugged CPUs
     - Fix TRBE driver for cpuhp state refcounting
     - Fix CTI driver NULL pointer dereferencing
     - Fix comment for repeated word
    
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    
    * tag 'coresight-next-v6.2' of git://git.kernel.org/pub/scm/linux/kernel/git/coresight/linux:
      coresight: etm4x: fix repeated words in comments
      coresight: cti: Fix null pointer error on CTI init before ETM
      coresight: trbe: remove cpuhp instance node before remove cpuhp state
      coresight: etm4x: add CPU hotplug support for probing

commit ee6815416380bc069b7dcbdff0682d4c53617527
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Thu Nov 3 20:06:01 2022 +0100

    x86/hyperv: Restore VP assist page after cpu offlining/onlining
    
    Commit e5d9b714fe40 ("x86/hyperv: fix root partition faults when writing
    to VP assist page MSR") moved 'wrmsrl(HV_X64_MSR_VP_ASSIST_PAGE)' under
    'if (*hvp)' condition. This works for root partition as hv_cpu_die()
    does memunmap() and sets 'hv_vp_assist_page[cpu]' to NULL but breaks
    non-root partitions as hv_cpu_die() doesn't free 'hv_vp_assist_page[cpu]'
    for them. This causes VP assist page to remain unset after CPU
    offline/online cycle:
    
    $ rdmsr -p 24 0x40000073
      10212f001
    $ echo 0 > /sys/devices/system/cpu/cpu24/online
    $ echo 1 > /sys/devices/system/cpu/cpu24/online
    $ rdmsr -p 24 0x40000073
      0
    
    Fix the issue by always writing to HV_X64_MSR_VP_ASSIST_PAGE in
    hv_cpu_init(). Note, checking 'if (!*hvp)', for root partition is
    pointless as hv_cpu_die() always sets 'hv_vp_assist_page[cpu]' to
    NULL (and it's also NULL initially).
    
    Note: the fact that 'hv_vp_assist_page[cpu]' is reset to NULL may
    present a (potential) issue for KVM. While Hyper-V uses
    CPUHP_AP_ONLINE_DYN stage in CPU hotplug, KVM uses CPUHP_AP_KVM_STARTING
    which comes earlier in CPU teardown sequence. It is theoretically
    possible that Enlightened VMCS is still in use. It is unclear if the
    issue is real and if using KVM with Hyper-V root partition is even
    possible.
    
    While on it, drop the unneeded smp_processor_id() call from hv_cpu_init().
    
    Fixes: e5d9b714fe40 ("x86/hyperv: fix root partition faults when writing to VP assist page MSR")
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com>
    Link: https://lore.kernel.org/r/20221103190601.399343-1-vkuznets@redhat.com
    Signed-off-by: Wei Liu <wei.liu@kernel.org>

commit 30f89e524becdbaa483b34902b079c9d4dfaa4a3
Author: Juergen Gross <jgross@suse.com>
Date:   Wed Nov 2 08:47:11 2022 +0100

    x86/cacheinfo: Switch cache_ap_init() to hotplug callback
    
    Instead of explicitly calling cache_ap_init() in
    identify_secondary_cpu() use a CPU hotplug callback instead. By
    registering the callback only after having started the non-boot CPUs
    and initializing cache_aps_delayed_init with "true", calling
    set_cache_aps_delayed_init() at boot time can be dropped.
    
    It should be noted that this change results in cache_ap_init() being
    called a little bit later when hotplugging CPUs. By using a new
    hotplug slot right at the start of the low level bringup this is not
    problematic, as no operations requiring a specific caching mode are
    performed that early in CPU initialization.
    
    Suggested-by: Borislav Petkov <bp@alien8.de>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lore.kernel.org/r/20221102074713.21493-15-jgross@suse.com
    Signed-off-by: Borislav Petkov <bp@suse.de>

commit 13fdbc8b8da6a2325cad3359c9a70504b0ff2f93
Author: Stuart Hayes <stuart.w.hayes@gmail.com>
Date:   Wed Nov 2 14:59:57 2022 -0500

    cpufreq: ACPI: Defer setting boost MSRs
    
    When acpi-cpufreq is loaded, boost is enabled on every CPU (by setting an
    MSR) before the driver is registered with cpufreq.  This can be very time
    consuming, because it is done with a CPU hotplug startup callback, and
    cpuhp_setup_state() schedules the callback (cpufreq_boost_online()) to run
    on each CPU one at a time, waiting for each to run before calling the next.
    
    If cpufreq_register_driver() fails--if, for example, there are no ACPI
    P-states present--this is wasted time.
    
    Since cpufreq already sets up a CPU hotplug startup callback if and when
    acpi-cpufreq is registered, set the boost MSRs in acpi_cpufreq_cpu_init(),
    which is called by the cpufreq cpuhp callback.  This allows acpi-cpufreq to
    exit quickly if it is loaded but not needed.
    
    On one system with 192 CPUs, this patch speeds up boot by about 30 seconds.
    
    Signed-off-by: Stuart Hayes <stuart.w.hayes@gmail.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit b6f86689d5b740f2cc3ac3a1032c7374b24381cc
Author: Borislav Petkov <bp@suse.de>
Date:   Wed Oct 19 18:13:06 2022 +0200

    x86/microcode: Rip out the subsys interface gunk
    
    This is a left-over from the old days when CPU hotplug wasn't as robust
    as it is now. Currently, microcode gets loaded early on the CPU init
    path and there's no need to attempt to load it again, which that subsys
    interface callback is doing.
    
    The only other thing that the subsys interface init path was doing is
    adding the
    
      /sys/devices/system/cpu/cpu*/microcode/
    
    hierarchy.
    
    So add a function which gets called on each CPU after all the necessary
    driver setup has happened. Use schedule_on_each_cpu() which can block
    because the sysfs creating code does kmem_cache_zalloc() which can block
    too and the initial version of this where it did that setup in an IPI
    handler of on_each_cpu() can cause a deadlock of the sort:
    
      lock(fs_reclaim);
      <Interrupt>
        lock(fs_reclaim);
    
    as the IPI handler runs in IRQ context.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Ashok Raj <ashok.raj@intel.com>
    Link: https://lore.kernel.org/r/20221028142638.28498-2-bp@alien8.de

commit 3c728e079d83f581a1f8b7755f6e26087b15c4fb
Author: Tamas Zsoldos <tamas.zsoldos@arm.com>
Date:   Tue Jul 5 16:59:35 2022 +0200

    coresight: etm4x: add CPU hotplug support for probing
    
    etm4x devices cannot be successfully probed when their CPU is offline.
    For example, when booting with maxcpus=n, ETM probing will fail on
    CPUs >n, and the probing won't be reattempted once the CPUs come
    online. This will leave those CPUs unable to make use of ETM.
    
    This change adds a mechanism to delay the probing if the corresponding
    CPU is offline, and to try it again when the CPU comes online.
    
    Signed-off-by: Tamas Zsoldos <tamas.zsoldos@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Link: https://lore.kernel.org/r/20220705145935.24679-1-tamas.zsoldos@arm.com

commit 2b2095f3a6b43ec36ff890febc588df1ec32e826
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Oct 14 01:16:46 2022 +1000

    powerpc/64s: Fix hash__change_memory_range preemption warning
    
    stop_machine_cpuslocked takes a mutex so it must be called in a
    preemptible context, so it can't simply be fixed by disabling
    preemption.
    
    This is not a bug, because CPU hotplug is locked, so this processor will
    call in to the stop machine function. So raw_smp_processor_id() could be
    used. This leaves a small chance that this thread will be migrated to
    another CPU, so the master work would be done by a CPU from a different
    context. Better for test coverage to make that a common case by just
    having the first CPU to call in become the master.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20221013151647.1857994-2-npiggin@gmail.com

commit 6c542ab75714fe90dae292aeb3e91ac53f5ff599
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Thu Aug 18 18:40:37 2022 +0530

    mm/demotion: build demotion targets based on explicit memory tiers
    
    This patch switch the demotion target building logic to use memory tiers
    instead of NUMA distance.  All N_MEMORY NUMA nodes will be placed in the
    default memory tier and additional memory tiers will be added by drivers
    like dax kmem.
    
    This patch builds the demotion target for a NUMA node by looking at all
    memory tiers below the tier to which the NUMA node belongs.  The closest
    node in the immediately following memory tier is used as a demotion
    target.
    
    Since we are now only building demotion target for N_MEMORY NUMA nodes the
    CPU hotplug calls are removed in this patch.
    
    Link: https://lkml.kernel.org/r/20220818131042.113280-6-aneesh.kumar@linux.ibm.com
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Reviewed-by: "Huang, Ying" <ying.huang@intel.com>
    Acked-by: Wei Xu <weixugc@google.com>
    Cc: Alistair Popple <apopple@nvidia.com>
    Cc: Bharata B Rao <bharata@amd.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Hesham Almatary <hesham.almatary@huawei.com>
    Cc: Jagdish Gediya <jvgediya.oss@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Jonathan Cameron <Jonathan.Cameron@huawei.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Tim Chen <tim.c.chen@intel.com>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: SeongJae Park <sj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

commit 798fd4b9ac37fec571f55fb8592497b0dd5f7a73
Author: James Morse <james.morse@arm.com>
Date:   Fri Sep 2 15:48:13 2022 +0000

    x86/resctrl: Add domain offline callback for resctrl work
    
    Because domains are exposed to user-space via resctrl, the filesystem
    must update its state when CPU hotplug callbacks are triggered.
    
    Some of this work is common to any architecture that would support
    resctrl, but the work is tied up with the architecture code to
    free the memory.
    
    Move the monitor subdir removal and the cancelling of the mbm/limbo
    works into a new resctrl_offline_domain() call. These bits are not
    specific to the architecture. Grouping them in one function allows
    that code to be moved to /fs/ and re-used by another architecture.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Jamie Iles <quic_jiles@quicinc.com>
    Reviewed-by: Shaopeng Tan <tan.shaopeng@fujitsu.com>
    Reviewed-by: Reinette Chatre <reinette.chatre@intel.com>
    Tested-by: Xin Hao <xhao@linux.alibaba.com>
    Tested-by: Shaopeng Tan <tan.shaopeng@fujitsu.com>
    Tested-by: Cristian Marussi <cristian.marussi@arm.com>
    Link: https://lore.kernel.org/r/20220902154829.30399-6-james.morse@arm.com

commit 3a7232cdf19e39e7f24c493117b373788b348af2
Author: James Morse <james.morse@arm.com>
Date:   Fri Sep 2 15:48:11 2022 +0000

    x86/resctrl: Add domain online callback for resctrl work
    
    Because domains are exposed to user-space via resctrl, the filesystem
    must update its state when CPU hotplug callbacks are triggered.
    
    Some of this work is common to any architecture that would support
    resctrl, but the work is tied up with the architecture code to
    allocate the memory.
    
    Move domain_setup_mon_state(), the monitor subdir creation call and the
    mbm/limbo workers into a new resctrl_online_domain() call. These bits
    are not specific to the architecture. Grouping them in one function
    allows that code to be moved to /fs/ and re-used by another architecture.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Jamie Iles <quic_jiles@quicinc.com>
    Reviewed-by: Shaopeng Tan <tan.shaopeng@fujitsu.com>
    Reviewed-by: Reinette Chatre <reinette.chatre@intel.com>
    Tested-by: Xin Hao <xhao@linux.alibaba.com>
    Tested-by: Shaopeng Tan <tan.shaopeng@fujitsu.com>
    Tested-by: Cristian Marussi <cristian.marussi@arm.com>
    Link: https://lore.kernel.org/r/20220902154829.30399-4-james.morse@arm.com

commit 59c6902a96b4439e07c25ef86a4593bea5481c3b
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 15 13:27:38 2022 -1000

    cgroup: Fix threadgroup_rwsem <-> cpus_read_lock() deadlock
    
    [ Upstream commit 4f7e7236435ca0abe005c674ebd6892c6e83aeb3 ]
    
    Bringing up a CPU may involve creating and destroying tasks which requires
    read-locking threadgroup_rwsem, so threadgroup_rwsem nests inside
    cpus_read_lock(). However, cpuset's ->attach(), which may be called with
    thredagroup_rwsem write-locked, also wants to disable CPU hotplug and
    acquires cpus_read_lock(), leading to a deadlock.
    
    Fix it by guaranteeing that ->attach() is always called with CPU hotplug
    disabled and removing cpus_read_lock() call from cpuset_attach().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-and-tested-by: Imran Khan <imran.f.khan@oracle.com>
    Reported-and-tested-by: Xuewen Yan <xuewen.yan@unisoc.com>
    Fixes: 05c7b7a92cc8 ("cgroup/cpuset: Fix a race between cpuset_attach() and cpu hotplug")
    Cc: stable@vger.kernel.org # v5.17+
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit dee1e2b18cf5426eed985512ccc6636ec69dbdd6
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 15 13:27:38 2022 -1000

    cgroup: Fix threadgroup_rwsem <-> cpus_read_lock() deadlock
    
    [ Upstream commit 4f7e7236435ca0abe005c674ebd6892c6e83aeb3 ]
    
    Bringing up a CPU may involve creating and destroying tasks which requires
    read-locking threadgroup_rwsem, so threadgroup_rwsem nests inside
    cpus_read_lock(). However, cpuset's ->attach(), which may be called with
    thredagroup_rwsem write-locked, also wants to disable CPU hotplug and
    acquires cpus_read_lock(), leading to a deadlock.
    
    Fix it by guaranteeing that ->attach() is always called with CPU hotplug
    disabled and removing cpus_read_lock() call from cpuset_attach().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-and-tested-by: Imran Khan <imran.f.khan@oracle.com>
    Reported-and-tested-by: Xuewen Yan <xuewen.yan@unisoc.com>
    Fixes: 05c7b7a92cc8 ("cgroup/cpuset: Fix a race between cpuset_attach() and cpu hotplug")
    Cc: stable@vger.kernel.org # v5.17+
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 3bf4bf54069f9b62a54988e5d085023c17a66c90
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 15 13:27:38 2022 -1000

    cgroup: Fix threadgroup_rwsem <-> cpus_read_lock() deadlock
    
    [ Upstream commit 4f7e7236435ca0abe005c674ebd6892c6e83aeb3 ]
    
    Bringing up a CPU may involve creating and destroying tasks which requires
    read-locking threadgroup_rwsem, so threadgroup_rwsem nests inside
    cpus_read_lock(). However, cpuset's ->attach(), which may be called with
    thredagroup_rwsem write-locked, also wants to disable CPU hotplug and
    acquires cpus_read_lock(), leading to a deadlock.
    
    Fix it by guaranteeing that ->attach() is always called with CPU hotplug
    disabled and removing cpus_read_lock() call from cpuset_attach().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-and-tested-by: Imran Khan <imran.f.khan@oracle.com>
    Reported-and-tested-by: Xuewen Yan <xuewen.yan@unisoc.com>
    Fixes: 05c7b7a92cc8 ("cgroup/cpuset: Fix a race between cpuset_attach() and cpu hotplug")
    Cc: stable@vger.kernel.org # v5.17+
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit c0deb027c99c099aa6b831e326bfba802b25e774
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 15 13:27:38 2022 -1000

    cgroup: Fix threadgroup_rwsem <-> cpus_read_lock() deadlock
    
    [ Upstream commit 4f7e7236435ca0abe005c674ebd6892c6e83aeb3 ]
    
    Bringing up a CPU may involve creating and destroying tasks which requires
    read-locking threadgroup_rwsem, so threadgroup_rwsem nests inside
    cpus_read_lock(). However, cpuset's ->attach(), which may be called with
    thredagroup_rwsem write-locked, also wants to disable CPU hotplug and
    acquires cpus_read_lock(), leading to a deadlock.
    
    Fix it by guaranteeing that ->attach() is always called with CPU hotplug
    disabled and removing cpus_read_lock() call from cpuset_attach().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-and-tested-by: Imran Khan <imran.f.khan@oracle.com>
    Reported-and-tested-by: Xuewen Yan <xuewen.yan@unisoc.com>
    Fixes: 05c7b7a92cc8 ("cgroup/cpuset: Fix a race between cpuset_attach() and cpu hotplug")
    Cc: stable@vger.kernel.org # v5.17+
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 9b03e79300100bcd36e77c8ce94ee7f47cd2f528
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Fri Aug 5 16:07:36 2022 -0700

    arch_topology: Silence early cacheinfo errors when non-existent
    
    Architectures which do not have cacheinfo such as ARM 32-bit would spit
    out the following during boot:
    
     Early cacheinfo failed, ret = -2
    
    Treat -ENOENT specifically to silence this error since it means that the
    platform does not support reporting its cache information.
    
    Fixes: 3fcbf1c77d08 ("arch_topology: Fix cache attributes detection in the CPU hotplug path")
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Tested-by: Michael Walle <michael@walle.cc>
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Reviewed-by: Conor Dooley <conor.dooley@microchip.com>
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Link: https://lore.kernel.org/r/20220805230736.1562801-1-f.fainelli@gmail.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f03d253ba71994b196f342a7acad448a56812a8c
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed Jul 20 13:55:39 2022 +0100

    ACPI: PPTT: Leave the table mapped for the runtime usage
    
    [ Upstream commit 0c80f9e165f8f9cca743d7b6cbdb54362da297e0 ]
    
    Currently, everytime an information needs to be fetched from the PPTT,
    the table is mapped via acpi_get_table() and unmapped after the use via
    acpi_put_table() which is fine. However we do this at runtime especially
    when the CPU is hotplugged out and plugged in back since we re-populate
    the cache topology and other information.
    
    However, with the support to fetch LLC information from the PPTT in the
    cpuhotplug path which is executed in the atomic context, it is preferred
    to avoid mapping and unmapping of the PPTT for every single use as the
    acpi_get_table() might sleep waiting for a mutex.
    
    In order to avoid the same, the table is needs to just mapped once on
    the boot CPU and is never unmapped allowing it to be used at runtime
    with out the hassle of mapping and unmapping the table.
    
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Cc: Rafael J. Wysocki <rafael@kernel.org>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    
    --
    
    Hi Rafael,
    
    Sorry to bother you again on this PPTT changes. Guenter reported an issue
    with lockdep enabled in -next that include my cacheinfo/arch_topology changes
    to utilise LLC from PPTT in the CPU hotplug path.
    
    Please ack the change once you are happy so that I can get it merged with
    other fixes via Greg's tree.
    
    Regards,
    Sudeep
    
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Link: https://lore.kernel.org/r/20220720-arch_topo_fixes-v3-2-43d696288e84@arm.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 4f7e7236435ca0abe005c674ebd6892c6e83aeb3
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Aug 15 13:27:38 2022 -1000

    cgroup: Fix threadgroup_rwsem <-> cpus_read_lock() deadlock
    
    Bringing up a CPU may involve creating and destroying tasks which requires
    read-locking threadgroup_rwsem, so threadgroup_rwsem nests inside
    cpus_read_lock(). However, cpuset's ->attach(), which may be called with
    thredagroup_rwsem write-locked, also wants to disable CPU hotplug and
    acquires cpus_read_lock(), leading to a deadlock.
    
    Fix it by guaranteeing that ->attach() is always called with CPU hotplug
    disabled and removing cpus_read_lock() call from cpuset_attach().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-and-tested-by: Imran Khan <imran.f.khan@oracle.com>
    Reported-and-tested-by: Xuewen Yan <xuewen.yan@unisoc.com>
    Fixes: 05c7b7a92cc8 ("cgroup/cpuset: Fix a race between cpuset_attach() and cpu hotplug")
    Cc: stable@vger.kernel.org # v5.17+

commit cfeafd94668910334a77c9437a18212baf9f5610
Merge: 228dfe98a313 273aaa24369c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Aug 4 11:31:20 2022 -0700

    Merge tag 'driver-core-6.0-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core
    
    Pull driver core / kernfs updates from Greg KH:
     "Here is the set of driver core and kernfs changes for 6.0-rc1.
    
      The "biggest" thing in here is some scalability improvements for
      kernfs for large systems. Other than that, included in here are:
    
       - arch topology and cache info changes that have been reviewed and
         discussed a lot.
    
       - potential error path cleanup fixes
    
       - deferred driver probe cleanups
    
       - firmware loader cleanups and tweaks
    
       - documentation updates
    
       - other small things
    
      All of these have been in the linux-next tree for a while with no
      reported problems"
    
    * tag 'driver-core-6.0-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core: (63 commits)
      docs: embargoed-hardware-issues: fix invalid AMD contact email
      firmware_loader: Replace kmap() with kmap_local_page()
      sysfs docs: ABI: Fix typo in comment
      kobject: fix Kconfig.debug "its" grammar
      kernfs: Fix typo 'the the' in comment
      docs: driver-api: firmware: add driver firmware guidelines. (v3)
      arch_topology: Fix cache attributes detection in the CPU hotplug path
      ACPI: PPTT: Leave the table mapped for the runtime usage
      cacheinfo: Use atomic allocation for percpu cache attributes
      drivers/base: fix userspace break from using bin_attributes for cpumap and cpulist
      MAINTAINERS: Change mentions of mpm to olivia
      docs: ABI: sysfs-devices-soc: Update Lee Jones' email address
      docs: ABI: sysfs-class-pwm: Update Lee Jones' email address
      Documentation/process: Add embargoed HW contact for LLVM
      Revert "kernfs: Change kernfs_notify_list to llist."
      ACPI: Remove the unused find_acpi_cpu_cache_topology()
      arch_topology: Warn that topology for nested clusters is not supported
      arch_topology: Add support for parsing sockets in /cpu-map
      arch_topology: Set cluster identifier in each core/thread from /cpu-map
      arch_topology: Limit span of cpu_clustergroup_mask()
      ...

commit a771ea6413c00cf4af0570745f2e27084d7e2376
Merge: 8fa0db3a9b8e aa727b7b4b67
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 2 11:17:00 2022 -0700

    Merge tag 'pm-5.20-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "These are mostly minor improvements all over including new CPU IDs for
      the Intel RAPL driver, an Energy Model rework to use micro-Watt as the
      power unit, cpufreq fixes and cleanus, cpuidle updates, devfreq
      updates, documentation cleanups and a new version of the pm-graph
      suite of utilities.
    
      Specifics:
    
       - Make cpufreq_show_cpus() more straightforward (Viresh Kumar).
    
       - Drop unnecessary CPU hotplug locking from store() used by cpufreq
         sysfs attributes (Viresh Kumar).
    
       - Make the ACPI cpufreq driver support the boost control interface on
         Zhaoxin/Centaur processors (Tony W Wang-oc).
    
       - Print a warning message on attempts to free an active cpufreq
         policy which should never happen (Viresh Kumar).
    
       - Fix grammar in the Kconfig help text for the loongson2 cpufreq
         driver (Randy Dunlap).
    
       - Use cpumask_var_t for an on-stack CPU mask in the ondemand cpufreq
         governor (Zhao Liu).
    
       - Add trace points for guest_halt_poll_ns grow/shrink to the haltpoll
         cpuidle driver (Eiichi Tsukata).
    
       - Modify intel_idle to treat C1 and C1E as independent idle states on
         Sapphire Rapids (Artem Bityutskiy).
    
       - Extend support for wakeirq to callback wrappers used during system
         suspend and resume (Ulf Hansson).
    
       - Defer waiting for device probe before loading a hibernation image
         till the first actual device access to avoid possible deadlocks
         reported by syzbot (Tetsuo Handa).
    
       - Unify device_init_wakeup() for PM_SLEEP and !PM_SLEEP (Bjorn
         Helgaas).
    
       - Add Raptor Lake-P to the list of processors supported by the Intel
         RAPL driver (George D Sworo).
    
       - Add Alder Lake-N and Raptor Lake-P to the list of processors for
         which Power Limit4 is supported in the Intel RAPL driver (Sumeet
         Pawnikar).
    
       - Make pm_genpd_remove() check genpd_debugfs_dir against NULL before
         attempting to remove it (Hsin-Yi Wang).
    
       - Change the Energy Model code to represent power in micro-Watts and
         adjust its users accordingly (Lukasz Luba).
    
       - Add new devfreq driver for Mediatek CCI (Cache Coherent
         Interconnect) (Johnson Wang).
    
       - Convert the Samsung Exynos SoC Bus bindings to DT schema of
         exynos-bus.c (Krzysztof Kozlowski).
    
       - Address kernel-doc warnings by adding the description for unused
         function parameters in devfreq core (Mauro Carvalho Chehab).
    
       - Use NULL to pass a null pointer rather than zero according to the
         function propotype in imx-bus.c (Colin Ian King).
    
       - Print error message instead of error interger value in
         tegra30-devfreq.c (Dmitry Osipenko).
    
       - Add checks to prevent setting negative frequency QoS limits for
         CPUs (Shivnandan Kumar).
    
       - Update the pm-graph suite of utilities to the latest revision 5.9
         including multiple improvements (Todd Brandt).
    
       - Drop pme_interrupt reference from the PCI power management
         documentation (Mario Limonciello)"
    
    * tag 'pm-5.20-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (27 commits)
      powercap: RAPL: Add Power Limit4 support for Alder Lake-N and Raptor Lake-P
      PM: QoS: Add check to make sure CPU freq is non-negative
      PM: hibernate: defer device probing when resuming from hibernation
      intel_idle: make SPR C1 and C1E be independent
      cpufreq: ondemand: Use cpumask_var_t for on-stack cpu mask
      cpufreq: loongson2: fix Kconfig "its" grammar
      pm-graph v5.9
      cpufreq: Warn users while freeing active policy
      cpufreq: scmi: Support the power scale in micro-Watts in SCMI v3.1
      firmware: arm_scmi: Get detailed power scale from perf
      Documentation: EM: Switch to micro-Watts scale
      PM: EM: convert power field to micro-Watts precision and align drivers
      PM / devfreq: tegra30: Add error message for devm_devfreq_add_device()
      PM / devfreq: imx-bus: use NULL to pass a null pointer rather than zero
      PM / devfreq: shut up kernel-doc warnings
      dt-bindings: interconnect: samsung,exynos-bus: convert to dtschema
      PM / devfreq: mediatek: Introduce MediaTek CCI devfreq driver
      dt-bindings: interconnect: Add MediaTek CCI dt-bindings
      PM: domains: Ensure genpd_debugfs_dir exists before remove
      PM: runtime: Extend support for wakeirq for force_suspend|resume
      ...

commit 82b6c2e7df79eb01ef561f8bf7a881f7db466c7a
Merge: 3e5c04f97c87 f611b33af2a8
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Jul 29 19:19:23 2022 +0200

    Merge branches 'pm-cpufreq' and 'pm-cpuidle'
    
    Merge processor power management changes for v5.20-rc1:
    
     - Make cpufreq_show_cpus() more straightforward (Viresh Kumar).
    
     - Drop unnecessary CPU hotplug locking from store() used by cpufreq
       sysfs attributes (Viresh Kumar).
    
     - Make the ACPI cpufreq driver support the boost control interface on
       Zhaoxin/Centaur processors (Tony W Wang-oc).
    
     - Print a warning message on attempts to free an active cpufreq policy
       which should never happen (Viresh Kumar).
    
     - Fix grammar in the Kconfig help text for the loongson2 cpufreq
       driver (Randy Dunlap).
    
     - Use cpumask_var_t for an on-stack CPU mask in the ondemand cpufreq
       governor (Zhao Liu).
    
     - Add trace points for guest_halt_poll_ns grow/shrink to the haltpoll
       cpuidle driver (Eiichi Tsukata).
    
     - Modify intel_idle to treat C1 and C1E as independent idle states on
       Sapphire Rapids (Artem Bityutskiy).
    
    * pm-cpufreq:
      cpufreq: ondemand: Use cpumask_var_t for on-stack cpu mask
      cpufreq: loongson2: fix Kconfig "its" grammar
      cpufreq: Warn users while freeing active policy
      cpufreq: ACPI: Add Zhaoxin/Centaur turbo boost control interface support
      cpufreq: Drop unnecessary cpus locking from store()
      cpufreq: Optimize cpufreq_show_cpus()
    
    * pm-cpuidle:
      intel_idle: make SPR C1 and C1E be independent
      cpuidle: haltpoll: Add trace points for guest_halt_poll_ns grow/shrink

commit 3fcbf1c77d089fcf0331fd8f3cbbe6c436a3edbd
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed Jul 20 13:55:40 2022 +0100

    arch_topology: Fix cache attributes detection in the CPU hotplug path
    
    init_cpu_topology() is called only once at the boot and all the cache
    attributes are detected early for all the possible CPUs. However when
    the CPUs are hotplugged out, the cacheinfo gets removed. While the
    attributes are added back when the CPUs are hotplugged back in as part
    of CPU hotplug state machine, it ends up called quite late after the
    update_siblings_masks() are called in the secondary_start_kernel()
    resulting in wrong llc_sibling_masks.
    
    Move the call to detect_cache_attributes() inside update_siblings_masks()
    to ensure the cacheinfo is updated before the LLC sibling masks are
    updated. This will fix the incorrect LLC sibling masks generated when
    the CPUs are hotplugged out and hotplugged back in again.
    
    Reported-by: Ionela Voinescu <ionela.voinescu@arm.com>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Tested-by: Ionela Voinescu <ionela.voinescu@arm.com>
    Reviewed-by: Conor Dooley <conor.dooley@microchip.com>
    Reviewed-by: Ionela Voinescu <ionela.voinescu@arm.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Link: https://lore.kernel.org/r/20220720-arch_topo_fixes-v3-3-43d696288e84@arm.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0c80f9e165f8f9cca743d7b6cbdb54362da297e0
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed Jul 20 13:55:39 2022 +0100

    ACPI: PPTT: Leave the table mapped for the runtime usage
    
    Currently, everytime an information needs to be fetched from the PPTT,
    the table is mapped via acpi_get_table() and unmapped after the use via
    acpi_put_table() which is fine. However we do this at runtime especially
    when the CPU is hotplugged out and plugged in back since we re-populate
    the cache topology and other information.
    
    However, with the support to fetch LLC information from the PPTT in the
    cpuhotplug path which is executed in the atomic context, it is preferred
    to avoid mapping and unmapping of the PPTT for every single use as the
    acpi_get_table() might sleep waiting for a mutex.
    
    In order to avoid the same, the table is needs to just mapped once on
    the boot CPU and is never unmapped allowing it to be used at runtime
    with out the hassle of mapping and unmapping the table.
    
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Cc: Rafael J. Wysocki <rafael@kernel.org>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    
    --
    
    Hi Rafael,
    
    Sorry to bother you again on this PPTT changes. Guenter reported an issue
    with lockdep enabled in -next that include my cacheinfo/arch_topology changes
    to utilise LLC from PPTT in the CPU hotplug path.
    
    Please ack the change once you are happy so that I can get it merged with
    other fixes via Greg's tree.
    
    Regards,
    Sudeep
    
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Link: https://lore.kernel.org/r/20220720-arch_topo_fixes-v3-2-43d696288e84@arm.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 11969d698f8cda31bd176ec346833ef97ea7c67e
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed Jul 20 13:55:38 2022 +0100

    cacheinfo: Use atomic allocation for percpu cache attributes
    
    On couple of architectures like RISC-V and ARM64, we need to detect
    cache attribues quite early during the boot when the secondary CPUs
    start. So we will call detect_cache_attributes in the atomic context
    and since use of normal allocation can sleep, we will end up getting
    "sleeping in the atomic context" bug splat.
    
    In order avoid that, move the allocation to use atomic version in
    preparation to move the actual detection of cache attributes in the
    CPU hotplug path which is atomic.
    
    Cc: Ionela Voinescu <ionela.voinescu@arm.com>
    Tested-by: Conor Dooley <conor.dooley@microchip.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Link: https://lore.kernel.org/r/20220720-arch_topo_fixes-v3-1-43d696288e84@arm.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 24fe0195bc19306b2769b43b3e22bd35bd6fb061
Author: Pierre Morel <pmorel@linux.ibm.com>
Date:   Thu Jul 14 12:18:23 2022 +0200

    KVM: s390: guest support for topology function
    
    We report a topology change to the guest for any CPU hotplug.
    
    The reporting to the guest is done using the Multiprocessor
    Topology-Change-Report (MTCR) bit of the utility entry in the guest's
    SCA which will be cleared during the interpretation of PTF.
    
    On every vCPU creation we set the MCTR bit to let the guest know the
    next time it uses the PTF with command 2 instruction that the
    topology changed and that it should use the STSI(15.1.x) instruction
    to get the topology details.
    
    STSI(15.1.x) gives information on the CPU configuration topology.
    Let's accept the interception of STSI with the function code 15 and
    let the userland part of the hypervisor handle it when userland
    supports the CPU Topology facility.
    
    Signed-off-by: Pierre Morel <pmorel@linux.ibm.com>
    Reviewed-by: Nico Boehr <nrb@linux.ibm.com>
    Reviewed-by: Janis Schoetterl-Glausch <scgl@linux.ibm.com>
    Reviewed-by: Janosch Frank <frankja@linux.ibm.com>
    Link: https://lore.kernel.org/r/20220714101824.101601-2-pmorel@linux.ibm.com
    Message-Id: <20220714101824.101601-2-pmorel@linux.ibm.com>
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>

commit 9b340131a4bcf6d0a282a2bdcd8ca268a74da709
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Tue Jun 21 10:04:12 2022 +0100

    sched/fair: Use the same cpumask per-PD throughout find_energy_efficient_cpu()
    
    The Perf Domain (PD) cpumask (struct em_perf_domain.cpus) stays
    invariant after Energy Model creation, i.e. it is not updated after
    CPU hotplug operations.
    
    That's why the PD mask is used in conjunction with the cpu_online_mask
    (or Sched Domain cpumask). Thereby the cpu_online_mask is fetched
    multiple times (in compute_energy()) during a run-queue selection
    for a task.
    
    cpu_online_mask may change during this time which can lead to wrong
    energy calculations.
    
    To be able to avoid this, use the select_rq_mask per-cpu cpumask to
    create a cpumask out of PD cpumask and cpu_online_mask and pass it
    through the function calls of the EAS run-queue selection path.
    
    The PD cpumask for max_spare_cap_cpu/compute_prev_delta selection
    (find_energy_efficient_cpu()) is now ANDed not only with the SD mask
    but also with the cpu_online_mask. This is fine since this cpumask
    has to be in syc with the one used for energy computation
    (compute_energy()).
    An exclusive cpuset setup with at least one asymmetric CPU capacity
    island (hence the additional AND with the SD cpumask) is the obvious
    exception here.
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Vincent Guittot <vincent.guittot@linaro.org>
    Tested-by: Lukasz Luba <lukasz.luba@arm.com>
    Link: https://lkml.kernel.org/r/20220621090414.433602-6-vdonnefort@google.com

commit 07cdb3a73e3e3834c0dbf06a3ce4abee52c0f33c
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Fri Feb 4 16:15:46 2022 +0100

    random: defer fast pool mixing to worker
    
    commit 58340f8e952b613e0ead0bed58b97b05bf4743c5 upstream.
    
    On PREEMPT_RT, it's problematic to take spinlocks from hard irq
    handlers. We can fix this by deferring to a workqueue the dumping of
    the fast pool into the input pool.
    
    We accomplish this with some careful rules on fast_pool->count:
    
      - When it's incremented to >= 64, we schedule the work.
      - If the top bit is set, we never schedule the work, even if >= 64.
      - The worker is responsible for setting it back to 0 when it's done.
    
    There are two small issues around using workqueues for this purpose that
    we work around.
    
    The first issue is that mix_interrupt_randomness() might be migrated to
    another CPU during CPU hotplug. This issue is rectified by checking that
    it hasn't been migrated (after disabling irqs). If it has been migrated,
    then we set the count to zero, so that when the CPU comes online again,
    it can requeue the work. As part of this, we switch to using an
    atomic_t, so that the increment in the irq handler doesn't wipe out the
    zeroing if the CPU comes back online while this worker is running.
    
    The second issue is that, though relatively minor in effect, we probably
    want to make sure we get a consistent view of the pool onto the stack,
    in case it's interrupted by an irq while reading. To do this, we don't
    reenable irqs until after the copy. There are only 18 instructions
    between the cli and sti, so this is a pretty tiny window.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Jonathan Neuschfer <j.neuschaefer@gmx.net>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Sultan Alsawaf <sultan@kerneltoast.com>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 80adfc1fa6910c5e90720b7fb9c7a1686b8ccd58
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Fri Feb 4 16:15:46 2022 +0100

    random: defer fast pool mixing to worker
    
    commit 58340f8e952b613e0ead0bed58b97b05bf4743c5 upstream.
    
    On PREEMPT_RT, it's problematic to take spinlocks from hard irq
    handlers. We can fix this by deferring to a workqueue the dumping of
    the fast pool into the input pool.
    
    We accomplish this with some careful rules on fast_pool->count:
    
      - When it's incremented to >= 64, we schedule the work.
      - If the top bit is set, we never schedule the work, even if >= 64.
      - The worker is responsible for setting it back to 0 when it's done.
    
    There are two small issues around using workqueues for this purpose that
    we work around.
    
    The first issue is that mix_interrupt_randomness() might be migrated to
    another CPU during CPU hotplug. This issue is rectified by checking that
    it hasn't been migrated (after disabling irqs). If it has been migrated,
    then we set the count to zero, so that when the CPU comes online again,
    it can requeue the work. As part of this, we switch to using an
    atomic_t, so that the increment in the irq handler doesn't wipe out the
    zeroing if the CPU comes back online while this worker is running.
    
    The second issue is that, though relatively minor in effect, we probably
    want to make sure we get a consistent view of the pool onto the stack,
    in case it's interrupted by an irq while reading. To do this, we don't
    reenable irqs until after the copy. There are only 18 instructions
    between the cli and sti, so this is a pretty tiny window.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Jonathan Neuschfer <j.neuschaefer@gmx.net>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Sultan Alsawaf <sultan@kerneltoast.com>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4c22f2c589d98d2b1dadc262ab6dcee94482b5e2
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Fri Feb 4 16:15:46 2022 +0100

    random: defer fast pool mixing to worker
    
    commit 58340f8e952b613e0ead0bed58b97b05bf4743c5 upstream.
    
    On PREEMPT_RT, it's problematic to take spinlocks from hard irq
    handlers. We can fix this by deferring to a workqueue the dumping of
    the fast pool into the input pool.
    
    We accomplish this with some careful rules on fast_pool->count:
    
      - When it's incremented to >= 64, we schedule the work.
      - If the top bit is set, we never schedule the work, even if >= 64.
      - The worker is responsible for setting it back to 0 when it's done.
    
    There are two small issues around using workqueues for this purpose that
    we work around.
    
    The first issue is that mix_interrupt_randomness() might be migrated to
    another CPU during CPU hotplug. This issue is rectified by checking that
    it hasn't been migrated (after disabling irqs). If it has been migrated,
    then we set the count to zero, so that when the CPU comes online again,
    it can requeue the work. As part of this, we switch to using an
    atomic_t, so that the increment in the irq handler doesn't wipe out the
    zeroing if the CPU comes back online while this worker is running.
    
    The second issue is that, though relatively minor in effect, we probably
    want to make sure we get a consistent view of the pool onto the stack,
    in case it's interrupted by an irq while reading. To do this, we don't
    reenable irqs until after the copy. There are only 18 instructions
    between the cli and sti, so this is a pretty tiny window.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Jonathan Neuschfer <j.neuschaefer@gmx.net>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Sultan Alsawaf <sultan@kerneltoast.com>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f886aab8291cb488ce995f8743dd5cbd1da6e9ad
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 7 22:41:55 2022 +0200

    sched: Fix balance_push() vs __sched_setscheduler()
    
    [ Upstream commit 04193d590b390ec7a0592630f46d559ec6564ba1 ]
    
    The purpose of balance_push() is to act as a filter on task selection
    in the case of CPU hotplug, specifically when taking the CPU out.
    
    It does this by (ab)using the balance callback infrastructure, with
    the express purpose of keeping all the unlikely/odd cases in a single
    place.
    
    In order to serve its purpose, the balance_push_callback needs to be
    (exclusively) on the callback list at all times (noting that the
    callback always places itself back on the list the moment it runs,
    also noting that when the CPU goes down, regular balancing concerns
    are moot, so ignoring them is fine).
    
    And here-in lies the problem, __sched_setscheduler()'s use of
    splice_balance_callbacks() takes the callbacks off the list across a
    lock-break, making it possible for, an interleaving, __schedule() to
    see an empty list and not get filtered.
    
    Fixes: ae7927023243 ("sched: Optimize finish_lock_switch()")
    Reported-by: Jing-Ting Wu <jing-ting.wu@mediatek.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Jing-Ting Wu <jing-ting.wu@mediatek.com>
    Link: https://lkml.kernel.org/r/20220519134706.GH2578@worktop.programming.kicks-ass.net
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit a863e9198077f012a6a0311bc1e6838a9e44d2bf
Author: Saurabh Sengar <ssengar@linux.microsoft.com>
Date:   Thu Jun 9 10:16:36 2022 -0700

    Drivers: hv: vmbus: Release cpu lock in error case
    
    [ Upstream commit 656c5ba50b7172a0ea25dc1b37606bd51d01fe8d ]
    
    In case of invalid sub channel, release cpu lock before returning.
    
    Fixes: a949e86c0d780 ("Drivers: hv: vmbus: Resolve race between init_vp_index() and CPU hotplug")
    Signed-off-by: Saurabh Sengar <ssengar@linux.microsoft.com>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com>
    Link: https://lore.kernel.org/r/1654794996-13244-1-git-send-email-ssengar@linux.microsoft.com
    Signed-off-by: Wei Liu <wei.liu@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 668a1f5e75d5d9ee3a8f7d844411e8ee81245e8b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 7 22:41:55 2022 +0200

    sched: Fix balance_push() vs __sched_setscheduler()
    
    [ Upstream commit 04193d590b390ec7a0592630f46d559ec6564ba1 ]
    
    The purpose of balance_push() is to act as a filter on task selection
    in the case of CPU hotplug, specifically when taking the CPU out.
    
    It does this by (ab)using the balance callback infrastructure, with
    the express purpose of keeping all the unlikely/odd cases in a single
    place.
    
    In order to serve its purpose, the balance_push_callback needs to be
    (exclusively) on the callback list at all times (noting that the
    callback always places itself back on the list the moment it runs,
    also noting that when the CPU goes down, regular balancing concerns
    are moot, so ignoring them is fine).
    
    And here-in lies the problem, __sched_setscheduler()'s use of
    splice_balance_callbacks() takes the callbacks off the list across a
    lock-break, making it possible for, an interleaving, __schedule() to
    see an empty list and not get filtered.
    
    Fixes: ae7927023243 ("sched: Optimize finish_lock_switch()")
    Reported-by: Jing-Ting Wu <jing-ting.wu@mediatek.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Jing-Ting Wu <jing-ting.wu@mediatek.com>
    Link: https://lkml.kernel.org/r/20220519134706.GH2578@worktop.programming.kicks-ass.net
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2cd5117ce64e03aa53d15ac957cc9ab040b02ff9
Author: Saurabh Sengar <ssengar@linux.microsoft.com>
Date:   Thu Jun 9 10:16:36 2022 -0700

    Drivers: hv: vmbus: Release cpu lock in error case
    
    [ Upstream commit 656c5ba50b7172a0ea25dc1b37606bd51d01fe8d ]
    
    In case of invalid sub channel, release cpu lock before returning.
    
    Fixes: a949e86c0d780 ("Drivers: hv: vmbus: Resolve race between init_vp_index() and CPU hotplug")
    Signed-off-by: Saurabh Sengar <ssengar@linux.microsoft.com>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com>
    Link: https://lore.kernel.org/r/1654794996-13244-1-git-send-email-ssengar@linux.microsoft.com
    Signed-off-by: Wei Liu <wei.liu@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 5334455067d51ea043d302a1d052db9b8af4832e
Author: Saurabh Sengar <ssengar@linux.microsoft.com>
Date:   Thu Jun 9 10:16:36 2022 -0700

    Drivers: hv: vmbus: Release cpu lock in error case
    
    [ Upstream commit 656c5ba50b7172a0ea25dc1b37606bd51d01fe8d ]
    
    In case of invalid sub channel, release cpu lock before returning.
    
    Fixes: a949e86c0d780 ("Drivers: hv: vmbus: Resolve race between init_vp_index() and CPU hotplug")
    Signed-off-by: Saurabh Sengar <ssengar@linux.microsoft.com>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com>
    Link: https://lore.kernel.org/r/1654794996-13244-1-git-send-email-ssengar@linux.microsoft.com
    Signed-off-by: Wei Liu <wei.liu@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 4a61bf7f9b189f0378fd8dbcf5cc6ea6abb205f7
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Fri Feb 4 16:15:46 2022 +0100

    random: defer fast pool mixing to worker
    
    commit 58340f8e952b613e0ead0bed58b97b05bf4743c5 upstream.
    
    On PREEMPT_RT, it's problematic to take spinlocks from hard irq
    handlers. We can fix this by deferring to a workqueue the dumping of
    the fast pool into the input pool.
    
    We accomplish this with some careful rules on fast_pool->count:
    
      - When it's incremented to >= 64, we schedule the work.
      - If the top bit is set, we never schedule the work, even if >= 64.
      - The worker is responsible for setting it back to 0 when it's done.
    
    There are two small issues around using workqueues for this purpose that
    we work around.
    
    The first issue is that mix_interrupt_randomness() might be migrated to
    another CPU during CPU hotplug. This issue is rectified by checking that
    it hasn't been migrated (after disabling irqs). If it has been migrated,
    then we set the count to zero, so that when the CPU comes online again,
    it can requeue the work. As part of this, we switch to using an
    atomic_t, so that the increment in the irq handler doesn't wipe out the
    zeroing if the CPU comes back online while this worker is running.
    
    The second issue is that, though relatively minor in effect, we probably
    want to make sure we get a consistent view of the pool onto the stack,
    in case it's interrupted by an irq while reading. To do this, we don't
    reenable irqs until after the copy. There are only 18 instructions
    between the cli and sti, so this is a pretty tiny window.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Jonathan Neuschfer <j.neuschaefer@gmx.net>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Sultan Alsawaf <sultan@kerneltoast.com>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 56096ecd5b04148b6d292e3847c23d4a2a454e94
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Tue Jun 7 17:25:03 2022 -0700

    rcu-tasks: Disable and enable CPU hotplug in same function
    
    The rcu_tasks_trace_pregp_step() function invokes cpus_read_lock() to
    disable CPU hotplug, and a later call to the rcu_tasks_trace_postscan()
    function invokes cpus_read_unlock() to re-enable it.  This was absolutely
    necessary in the past in order to protect the intervening scan of the full
    tasks list, but there is no longer such a scan.  This commit therefore
    improves readability by moving the cpus_read_unlock() call to the end
    of the rcu_tasks_trace_pregp_step() function.  This commit is a pure
    code-motion commit without any (intended) change in functionality.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
    Cc: Neeraj Upadhyay <quic_neeraju@quicinc.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Andrii Nakryiko <andrii@kernel.org>
    Cc: Martin KaFai Lau <kafai@fb.com>
    Cc: KP Singh <kpsingh@kernel.org>

commit 9ab9b9d3fb9231cdcfda8e0fb3d9c24a2f95ed26
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Thu May 26 17:21:21 2022 +0530

    cpufreq: Drop unnecessary cpus locking from store()
    
    This change was introduced long back by commit 4f750c930822 ("cpufreq:
    Synchronize the cpufreq store_*() routines with CPU hotplug").
    
    Since then, both cpufreq and hotplug core have been reworked and have
    much better locking in place. The race mentioned in commit 4f750c930822
    isn't possible anymore.
    
    Drop the unnecessary locking.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 04193d590b390ec7a0592630f46d559ec6564ba1
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 7 22:41:55 2022 +0200

    sched: Fix balance_push() vs __sched_setscheduler()
    
    The purpose of balance_push() is to act as a filter on task selection
    in the case of CPU hotplug, specifically when taking the CPU out.
    
    It does this by (ab)using the balance callback infrastructure, with
    the express purpose of keeping all the unlikely/odd cases in a single
    place.
    
    In order to serve its purpose, the balance_push_callback needs to be
    (exclusively) on the callback list at all times (noting that the
    callback always places itself back on the list the moment it runs,
    also noting that when the CPU goes down, regular balancing concerns
    are moot, so ignoring them is fine).
    
    And here-in lies the problem, __sched_setscheduler()'s use of
    splice_balance_callbacks() takes the callbacks off the list across a
    lock-break, making it possible for, an interleaving, __schedule() to
    see an empty list and not get filtered.
    
    Fixes: ae7927023243 ("sched: Optimize finish_lock_switch()")
    Reported-by: Jing-Ting Wu <jing-ting.wu@mediatek.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Jing-Ting Wu <jing-ting.wu@mediatek.com>
    Link: https://lkml.kernel.org/r/20220519134706.GH2578@worktop.programming.kicks-ass.net

commit 656c5ba50b7172a0ea25dc1b37606bd51d01fe8d
Author: Saurabh Sengar <ssengar@linux.microsoft.com>
Date:   Thu Jun 9 10:16:36 2022 -0700

    Drivers: hv: vmbus: Release cpu lock in error case
    
    In case of invalid sub channel, release cpu lock before returning.
    
    Fixes: a949e86c0d780 ("Drivers: hv: vmbus: Resolve race between init_vp_index() and CPU hotplug")
    Signed-off-by: Saurabh Sengar <ssengar@linux.microsoft.com>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com>
    Link: https://lore.kernel.org/r/1654794996-13244-1-git-send-email-ssengar@linux.microsoft.com
    Signed-off-by: Wei Liu <wei.liu@kernel.org>

commit 396b8e7ab2a99ddac57d3522b3da5e58cb608d37
Author: Ammar Faizi <ammarfaizi2@gnuweeb.org>
Date:   Tue Mar 29 17:47:05 2022 +0700

    x86/MCE/AMD: Fix memory leak when threshold_create_bank() fails
    
    commit e5f28623ceb103e13fc3d7bd45edf9818b227fd0 upstream.
    
    In mce_threshold_create_device(), if threshold_create_bank() fails, the
    previously allocated threshold banks array @bp will be leaked because
    the call to mce_threshold_remove_device() will not free it.
    
    This happens because mce_threshold_remove_device() fetches the pointer
    through the threshold_banks per-CPU variable but bp is written there
    only after the bank creation is successful, and not before, when
    threshold_create_bank() fails.
    
    Add a helper which unwinds all the bank creation work previously done
    and pass into it the previously allocated threshold banks array for
    freeing.
    
      [ bp: Massage. ]
    
    Fixes: 6458de97fc15 ("x86/mce/amd: Straighten CPU hotplug path")
    Co-developed-by: Alviro Iskandar Setiawan <alviro.iskandar@gnuweeb.org>
    Signed-off-by: Alviro Iskandar Setiawan <alviro.iskandar@gnuweeb.org>
    Co-developed-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Ammar Faizi <ammarfaizi2@gnuweeb.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org>
    Link: https://lore.kernel.org/r/20220329104705.65256-3-ammarfaizi2@gnuweeb.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9708f1956eeb70c86943e0bc62fa3b0101b59616
Author: Ammar Faizi <ammarfaizi2@gnuweeb.org>
Date:   Tue Mar 29 17:47:05 2022 +0700

    x86/MCE/AMD: Fix memory leak when threshold_create_bank() fails
    
    commit e5f28623ceb103e13fc3d7bd45edf9818b227fd0 upstream.
    
    In mce_threshold_create_device(), if threshold_create_bank() fails, the
    previously allocated threshold banks array @bp will be leaked because
    the call to mce_threshold_remove_device() will not free it.
    
    This happens because mce_threshold_remove_device() fetches the pointer
    through the threshold_banks per-CPU variable but bp is written there
    only after the bank creation is successful, and not before, when
    threshold_create_bank() fails.
    
    Add a helper which unwinds all the bank creation work previously done
    and pass into it the previously allocated threshold banks array for
    freeing.
    
      [ bp: Massage. ]
    
    Fixes: 6458de97fc15 ("x86/mce/amd: Straighten CPU hotplug path")
    Co-developed-by: Alviro Iskandar Setiawan <alviro.iskandar@gnuweeb.org>
    Signed-off-by: Alviro Iskandar Setiawan <alviro.iskandar@gnuweeb.org>
    Co-developed-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Ammar Faizi <ammarfaizi2@gnuweeb.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org>
    Link: https://lore.kernel.org/r/20220329104705.65256-3-ammarfaizi2@gnuweeb.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit cc0dd4456f9573bf8af9b4d8754433918e809e1e
Author: Ammar Faizi <ammarfaizi2@gnuweeb.org>
Date:   Tue Mar 29 17:47:05 2022 +0700

    x86/MCE/AMD: Fix memory leak when threshold_create_bank() fails
    
    commit e5f28623ceb103e13fc3d7bd45edf9818b227fd0 upstream.
    
    In mce_threshold_create_device(), if threshold_create_bank() fails, the
    previously allocated threshold banks array @bp will be leaked because
    the call to mce_threshold_remove_device() will not free it.
    
    This happens because mce_threshold_remove_device() fetches the pointer
    through the threshold_banks per-CPU variable but bp is written there
    only after the bank creation is successful, and not before, when
    threshold_create_bank() fails.
    
    Add a helper which unwinds all the bank creation work previously done
    and pass into it the previously allocated threshold banks array for
    freeing.
    
      [ bp: Massage. ]
    
    Fixes: 6458de97fc15 ("x86/mce/amd: Straighten CPU hotplug path")
    Co-developed-by: Alviro Iskandar Setiawan <alviro.iskandar@gnuweeb.org>
    Signed-off-by: Alviro Iskandar Setiawan <alviro.iskandar@gnuweeb.org>
    Co-developed-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Ammar Faizi <ammarfaizi2@gnuweeb.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org>
    Link: https://lore.kernel.org/r/20220329104705.65256-3-ammarfaizi2@gnuweeb.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b4acb8e7f1594607bc9017ef0aacb40b24a003d6
Author: Ammar Faizi <ammarfaizi2@gnuweeb.org>
Date:   Tue Mar 29 17:47:05 2022 +0700

    x86/MCE/AMD: Fix memory leak when threshold_create_bank() fails
    
    commit e5f28623ceb103e13fc3d7bd45edf9818b227fd0 upstream.
    
    In mce_threshold_create_device(), if threshold_create_bank() fails, the
    previously allocated threshold banks array @bp will be leaked because
    the call to mce_threshold_remove_device() will not free it.
    
    This happens because mce_threshold_remove_device() fetches the pointer
    through the threshold_banks per-CPU variable but bp is written there
    only after the bank creation is successful, and not before, when
    threshold_create_bank() fails.
    
    Add a helper which unwinds all the bank creation work previously done
    and pass into it the previously allocated threshold banks array for
    freeing.
    
      [ bp: Massage. ]
    
    Fixes: 6458de97fc15 ("x86/mce/amd: Straighten CPU hotplug path")
    Co-developed-by: Alviro Iskandar Setiawan <alviro.iskandar@gnuweeb.org>
    Signed-off-by: Alviro Iskandar Setiawan <alviro.iskandar@gnuweeb.org>
    Co-developed-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Ammar Faizi <ammarfaizi2@gnuweeb.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org>
    Link: https://lore.kernel.org/r/20220329104705.65256-3-ammarfaizi2@gnuweeb.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5d73e69a5dd41bbad0d6bdf2f00e3f0739124657
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Fri Feb 4 16:15:46 2022 +0100

    random: defer fast pool mixing to worker
    
    commit 58340f8e952b613e0ead0bed58b97b05bf4743c5 upstream.
    
    On PREEMPT_RT, it's problematic to take spinlocks from hard irq
    handlers. We can fix this by deferring to a workqueue the dumping of
    the fast pool into the input pool.
    
    We accomplish this with some careful rules on fast_pool->count:
    
      - When it's incremented to >= 64, we schedule the work.
      - If the top bit is set, we never schedule the work, even if >= 64.
      - The worker is responsible for setting it back to 0 when it's done.
    
    There are two small issues around using workqueues for this purpose that
    we work around.
    
    The first issue is that mix_interrupt_randomness() might be migrated to
    another CPU during CPU hotplug. This issue is rectified by checking that
    it hasn't been migrated (after disabling irqs). If it has been migrated,
    then we set the count to zero, so that when the CPU comes online again,
    it can requeue the work. As part of this, we switch to using an
    atomic_t, so that the increment in the irq handler doesn't wipe out the
    zeroing if the CPU comes back online while this worker is running.
    
    The second issue is that, though relatively minor in effect, we probably
    want to make sure we get a consistent view of the pool onto the stack,
    in case it's interrupted by an irq while reading. To do this, we don't
    reenable irqs until after the copy. There are only 18 instructions
    between the cli and sti, so this is a pretty tiny window.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Jonathan Neuschfer <j.neuschaefer@gmx.net>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Sultan Alsawaf <sultan@kerneltoast.com>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3c48a2da32ef8ebfa8b71233b790f45d48704b2a
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Fri Feb 4 16:15:46 2022 +0100

    random: defer fast pool mixing to worker
    
    commit 58340f8e952b613e0ead0bed58b97b05bf4743c5 upstream.
    
    On PREEMPT_RT, it's problematic to take spinlocks from hard irq
    handlers. We can fix this by deferring to a workqueue the dumping of
    the fast pool into the input pool.
    
    We accomplish this with some careful rules on fast_pool->count:
    
      - When it's incremented to >= 64, we schedule the work.
      - If the top bit is set, we never schedule the work, even if >= 64.
      - The worker is responsible for setting it back to 0 when it's done.
    
    There are two small issues around using workqueues for this purpose that
    we work around.
    
    The first issue is that mix_interrupt_randomness() might be migrated to
    another CPU during CPU hotplug. This issue is rectified by checking that
    it hasn't been migrated (after disabling irqs). If it has been migrated,
    then we set the count to zero, so that when the CPU comes online again,
    it can requeue the work. As part of this, we switch to using an
    atomic_t, so that the increment in the irq handler doesn't wipe out the
    zeroing if the CPU comes back online while this worker is running.
    
    The second issue is that, though relatively minor in effect, we probably
    want to make sure we get a consistent view of the pool onto the stack,
    in case it's interrupted by an irq while reading. To do this, we don't
    reenable irqs until after the copy. There are only 18 instructions
    between the cli and sti, so this is a pretty tiny window.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Jonathan Neuschfer <j.neuschaefer@gmx.net>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Sultan Alsawaf <sultan@kerneltoast.com>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a4217ab7175ed8e37a9bd8530e6fac8108b5d9c7
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Fri Feb 4 16:15:46 2022 +0100

    random: defer fast pool mixing to worker
    
    commit 58340f8e952b613e0ead0bed58b97b05bf4743c5 upstream.
    
    On PREEMPT_RT, it's problematic to take spinlocks from hard irq
    handlers. We can fix this by deferring to a workqueue the dumping of
    the fast pool into the input pool.
    
    We accomplish this with some careful rules on fast_pool->count:
    
      - When it's incremented to >= 64, we schedule the work.
      - If the top bit is set, we never schedule the work, even if >= 64.
      - The worker is responsible for setting it back to 0 when it's done.
    
    There are two small issues around using workqueues for this purpose that
    we work around.
    
    The first issue is that mix_interrupt_randomness() might be migrated to
    another CPU during CPU hotplug. This issue is rectified by checking that
    it hasn't been migrated (after disabling irqs). If it has been migrated,
    then we set the count to zero, so that when the CPU comes online again,
    it can requeue the work. As part of this, we switch to using an
    atomic_t, so that the increment in the irq handler doesn't wipe out the
    zeroing if the CPU comes back online while this worker is running.
    
    The second issue is that, though relatively minor in effect, we probably
    want to make sure we get a consistent view of the pool onto the stack,
    in case it's interrupted by an irq while reading. To do this, we don't
    reenable irqs until after the copy. There are only 18 instructions
    between the cli and sti, so this is a pretty tiny window.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Jonathan Neuschfer <j.neuschaefer@gmx.net>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Sultan Alsawaf <sultan@kerneltoast.com>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 28c8f9fe94c4e0b0c27383d48da3c85b0dc17081
Merge: 985564eb3e3c d308077e5e4d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 23 16:55:36 2022 -0700

    Merge tag 'smp-core-2022-05-23' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug updates from Thomas Gleixner:
    
     - Initialize the per-CPU structures during early boot so that the state
       is consistent from the very beginning.
    
     - Make the virtualization hotplug state handling more robust and let
       the core bringup CPUs which timed out in an earlier attempt again.
    
     - Make the x86/xen CPU state tracking consistent on a failed online
       attempt, so a consecutive bringup does not fall over the inconsistent
       state.
    
    * tag 'smp-core-2022-05-23' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Initialise all cpuhp_cpu_state structs earlier
      cpu/hotplug: Allow the CPU in CPU_UP_PREPARE state to be brought up again.
      x86/xen: Allow to retry if cpu_initialize_context() failed.

commit a956f4e281fe548a541a4970a6e8a0ec283b0d6d
Merge: 3d7285a335ed 1d0cb4c8864a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 20 08:09:00 2022 -1000

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes from Will Deacon:
     "Three arm64 fixes for -rc8/final.
    
      The MTE and stolen time fixes have been doing the rounds for a little
      while, but review and testing feedback was ongoing until earlier this
      week. The kexec fix showed up on Monday and addresses a failure
      observed under Qemu.
    
      Summary:
    
       - Add missing write barrier to publish MTE tags before a pte update
    
       - Fix kexec relocation clobbering its own data structures
    
       - Fix stolen time crash if a timer IRQ fires during CPU hotplug"
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: mte: Ensure the cleared tags are visible before setting the PTE
      arm64: kexec: load from kimage prior to clobbering
      arm64: paravirt: Use RCU read locks to guard stolen_time

commit 841b51e4a3590866d17fa2663c64688c25b891b1
Author: Borislav Petkov <bp@suse.de>
Date:   Mon May 16 17:48:38 2022 +0200

    perf/x86/amd: Run AMD BRS code only on supported hw
    
    This fires on a Fam16h machine here:
    
     unchecked MSR access error: WRMSR to 0xc000010f (tried to write 0x0000000000000018) \
        at rIP: 0xffffffff81007db1 (amd_brs_reset+0x11/0x50)
     Call Trace:
      <TASK>
      amd_pmu_cpu_starting
      ? x86_pmu_dead_cpu
      x86_pmu_starting_cpu
      cpuhp_invoke_callback
      ? x86_pmu_starting_cpu
      ? x86_pmu_dead_cpu
      cpuhp_issue_call
      ? x86_pmu_starting_cpu
      __cpuhp_setup_state_cpuslocked
      ? x86_pmu_dead_cpu
      ? x86_pmu_starting_cpu
      __cpuhp_setup_state
      ? map_vsyscall
      init_hw_perf_events
      ? map_vsyscall
      do_one_initcall
      ? _raw_spin_unlock_irqrestore
      ? try_to_wake_up
      kernel_init_freeable
      ? rest_init
      kernel_init
      ret_from_fork
    
    because that CPU hotplug callback gets executed on any AMD CPU - not
    only on the BRS-enabled ones. Check the BRS feature bit properly.
    
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-By: Stephane Eranian <eranian@google.com>
    Link: https://lkml.kernel.org/r/20220516154838.7044-1-bp@alien8.de

commit 4419da5d5d4b1788568b7bf22c083ba2832891df
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Thu Apr 28 14:38:58 2022 +0530

    tty: hvc: dcc: Bind driver to CPU core0 for reads and writes
    
    Some external debuggers do not handle reads/writes from/to DCC
    on secondary cores. Each core has its own DCC device registers,
    so when a core reads or writes from/to DCC, it only accesses
    its own DCC device. Since kernel code can run on any core,
    every time the kernel wants to write to the console, it might
    write to a different DCC.
    
    In SMP mode, external debugger creates multiple windows, and
    each window shows the DCC output only from that core's DCC.
    The result is that console output is either lost or scattered
    across windows.
    
    Selecting this debug option will enable code that serializes all
    console input and output to core 0. The DCC driver will create
    input and output FIFOs that all cores will use. Reads and writes
    from/to DCC are handled by a workqueue that runs only core 0.
    
    This is a debug feature to be used only in early stage development
    where debug serial console support would not be present. It disables
    PM feature like CPU hotplug and is not suitable for production
    environment.
    
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    Acked-by: Adam Wallis <awallis@codeaurora.org>
    Signed-off-by: Timur Tabi <timur@codeaurora.org>
    Signed-off-by: Elliot Berman <eberman@codeaurora.org>
    Signed-off-by: Sai Prakash Ranjan <quic_saipraka@quicinc.com>
    Link: https://lore.kernel.org/r/20220428090858.14489-1-quic_saipraka@quicinc.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8a58bcd00e2e8d46afce468adc09fcd7968f514c
Author: Mark Brown <broonie@kernel.org>
Date:   Wed Apr 27 14:08:28 2022 +0100

    arm64/sme: Add ID_AA64SMFR0_EL1 to __read_sysreg_by_encoding()
    
    We need to explicitly enumerate all the ID registers which we rely on
    for CPU capabilities in __read_sysreg_by_encoding(), ID_AA64SMFR0_EL1 was
    missed from this list so we trip a BUG() in paths which rely on that
    function such as CPU hotplug. Add the register.
    
    Reported-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Tested-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Link: https://lore.kernel.org/r/20220427130828.162615-1-broonie@kernel.org
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

commit b041b525dab95352fbd666b14dc73ab898df465f
Author: Tony Luck <tony.luck@intel.com>
Date:   Thu Mar 10 12:48:53 2022 -0800

    x86/split_lock: Make life miserable for split lockers
    
    In https://lore.kernel.org/all/87y22uujkm.ffs@tglx/ Thomas
    said:
    
      Its's simply wishful thinking that stuff gets fixed because of a
      WARN_ONCE(). This has never worked. The only thing which works is to
      make stuff fail hard or slow it down in a way which makes it annoying
      enough to users to complain.
    
    He was talking about WBINVD. But it made me think about how we use the
    split lock detection feature in Linux.
    
    Existing code has three options for applications:
    
     1) Don't enable split lock detection (allow arbitrary split locks)
     2) Warn once when a process uses split lock, but let the process
        keep running with split lock detection disabled
     3) Kill process that use split locks
    
    Option 2 falls into the "wishful thinking" territory that Thomas warns does
    nothing. But option 3 might not be viable in a situation with legacy
    applications that need to run.
    
    Hence make option 2 much stricter to "slow it down in a way which makes
    it annoying".
    
    Primary reason for this change is to provide better quality of service to
    the rest of the applications running on the system. Internal testing shows
    that even with many processes splitting locks, performance for the rest of
    the system is much more responsive.
    
    The new "warn" mode operates like this.  When an application tries to
    execute a bus lock the #AC handler.
    
     1) Delays (interruptibly) 10 ms before moving to next step.
    
     2) Blocks (interruptibly) until it can get the semaphore
            If interrupted, just return. Assume the signal will either
            kill the task, or direct execution away from the instruction
            that is trying to get the bus lock.
     3) Disables split lock detection for the current core
     4) Schedules a work queue to re-enable split lock detect in 2 jiffies
     5) Returns
    
    The work queue that re-enables split lock detection also releases the
    semaphore.
    
    There is a corner case where a CPU may be taken offline while split lock
    detection is disabled. A CPU hotplug handler handles this case.
    
    Old behaviour was to only print the split lock warning on the first
    occurrence of a split lock from a task. Preserve that by adding a flag to
    the task structure that suppresses subsequent split lock messages from that
    task.
    
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20220310204854.31752-2-tony.luck@intel.com

commit 177e7aceef185afd88932c16d0039aacbe9b7cb7
Author: Nadav Amit <namit@vmware.com>
Date:   Sat Mar 19 00:20:15 2022 -0700

    smp: Fix offline cpu check in flush_smp_call_function_queue()
    
    commit 9e949a3886356fe9112c6f6f34a6e23d1d35407f upstream.
    
    The check in flush_smp_call_function_queue() for callbacks that are sent
    to offline CPUs currently checks whether the queue is empty.
    
    However, flush_smp_call_function_queue() has just deleted all the
    callbacks from the queue and moved all the entries into a local list.
    This checks would only be positive if some callbacks were added in the
    short time after llist_del_all() was called. This does not seem to be
    the intention of this check.
    
    Change the check to look at the local list to which the entries were
    moved instead of the queue from which all the callbacks were just
    removed.
    
    Fixes: 8d056c48e4862 ("CPU hotplug, smp: flush any pending IPI callbacks before CPU offline")
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20220319072015.1495036-1-namit@vmware.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 44981e4cde6867167f0997e0bdcc76e61e2c9229
Author: Nadav Amit <namit@vmware.com>
Date:   Sat Mar 19 00:20:15 2022 -0700

    smp: Fix offline cpu check in flush_smp_call_function_queue()
    
    commit 9e949a3886356fe9112c6f6f34a6e23d1d35407f upstream.
    
    The check in flush_smp_call_function_queue() for callbacks that are sent
    to offline CPUs currently checks whether the queue is empty.
    
    However, flush_smp_call_function_queue() has just deleted all the
    callbacks from the queue and moved all the entries into a local list.
    This checks would only be positive if some callbacks were added in the
    short time after llist_del_all() was called. This does not seem to be
    the intention of this check.
    
    Change the check to look at the local list to which the entries were
    moved instead of the queue from which all the callbacks were just
    removed.
    
    Fixes: 8d056c48e4862 ("CPU hotplug, smp: flush any pending IPI callbacks before CPU offline")
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20220319072015.1495036-1-namit@vmware.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 89496d80bf847b49cb1349a7028b8b3c6e595d95
Author: Nadav Amit <namit@vmware.com>
Date:   Sat Mar 19 00:20:15 2022 -0700

    smp: Fix offline cpu check in flush_smp_call_function_queue()
    
    commit 9e949a3886356fe9112c6f6f34a6e23d1d35407f upstream.
    
    The check in flush_smp_call_function_queue() for callbacks that are sent
    to offline CPUs currently checks whether the queue is empty.
    
    However, flush_smp_call_function_queue() has just deleted all the
    callbacks from the queue and moved all the entries into a local list.
    This checks would only be positive if some callbacks were added in the
    short time after llist_del_all() was called. This does not seem to be
    the intention of this check.
    
    Change the check to look at the local list to which the entries were
    moved instead of the queue from which all the callbacks were just
    removed.
    
    Fixes: 8d056c48e4862 ("CPU hotplug, smp: flush any pending IPI callbacks before CPU offline")
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20220319072015.1495036-1-namit@vmware.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 659855c62c34eb2b401000aec702ea45d6ef9cdf
Author: Nadav Amit <namit@vmware.com>
Date:   Sat Mar 19 00:20:15 2022 -0700

    smp: Fix offline cpu check in flush_smp_call_function_queue()
    
    commit 9e949a3886356fe9112c6f6f34a6e23d1d35407f upstream.
    
    The check in flush_smp_call_function_queue() for callbacks that are sent
    to offline CPUs currently checks whether the queue is empty.
    
    However, flush_smp_call_function_queue() has just deleted all the
    callbacks from the queue and moved all the entries into a local list.
    This checks would only be positive if some callbacks were added in the
    short time after llist_del_all() was called. This does not seem to be
    the intention of this check.
    
    Change the check to look at the local list to which the entries were
    moved instead of the queue from which all the callbacks were just
    removed.
    
    Fixes: 8d056c48e4862 ("CPU hotplug, smp: flush any pending IPI callbacks before CPU offline")
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20220319072015.1495036-1-namit@vmware.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bd4ad32e44ea472c83c7c21e5bee29a1bb1c24e8
Author: Nadav Amit <namit@vmware.com>
Date:   Sat Mar 19 00:20:15 2022 -0700

    smp: Fix offline cpu check in flush_smp_call_function_queue()
    
    commit 9e949a3886356fe9112c6f6f34a6e23d1d35407f upstream.
    
    The check in flush_smp_call_function_queue() for callbacks that are sent
    to offline CPUs currently checks whether the queue is empty.
    
    However, flush_smp_call_function_queue() has just deleted all the
    callbacks from the queue and moved all the entries into a local list.
    This checks would only be positive if some callbacks were added in the
    short time after llist_del_all() was called. This does not seem to be
    the intention of this check.
    
    Change the check to look at the local list to which the entries were
    moved instead of the queue from which all the callbacks were just
    removed.
    
    Fixes: 8d056c48e4862 ("CPU hotplug, smp: flush any pending IPI callbacks before CPU offline")
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20220319072015.1495036-1-namit@vmware.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0f088623b6db0d533fe7fa787d0283b58a275b18
Author: Nadav Amit <namit@vmware.com>
Date:   Sat Mar 19 00:20:15 2022 -0700

    smp: Fix offline cpu check in flush_smp_call_function_queue()
    
    commit 9e949a3886356fe9112c6f6f34a6e23d1d35407f upstream.
    
    The check in flush_smp_call_function_queue() for callbacks that are sent
    to offline CPUs currently checks whether the queue is empty.
    
    However, flush_smp_call_function_queue() has just deleted all the
    callbacks from the queue and moved all the entries into a local list.
    This checks would only be positive if some callbacks were added in the
    short time after llist_del_all() was called. This does not seem to be
    the intention of this check.
    
    Change the check to look at the local list to which the entries were
    moved instead of the queue from which all the callbacks were just
    removed.
    
    Fixes: 8d056c48e4862 ("CPU hotplug, smp: flush any pending IPI callbacks before CPU offline")
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20220319072015.1495036-1-namit@vmware.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d9324898e53b84979e2c10ad1541f1f3dcfb24ae
Author: Nadav Amit <namit@vmware.com>
Date:   Sat Mar 19 00:20:15 2022 -0700

    smp: Fix offline cpu check in flush_smp_call_function_queue()
    
    commit 9e949a3886356fe9112c6f6f34a6e23d1d35407f upstream.
    
    The check in flush_smp_call_function_queue() for callbacks that are sent
    to offline CPUs currently checks whether the queue is empty.
    
    However, flush_smp_call_function_queue() has just deleted all the
    callbacks from the queue and moved all the entries into a local list.
    This checks would only be positive if some callbacks were added in the
    short time after llist_del_all() was called. This does not seem to be
    the intention of this check.
    
    Change the check to look at the local list to which the entries were
    moved instead of the queue from which all the callbacks were just
    removed.
    
    Fixes: 8d056c48e4862 ("CPU hotplug, smp: flush any pending IPI callbacks before CPU offline")
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20220319072015.1495036-1-namit@vmware.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0e59732ed61a24458b6875c162660dc0758b678f
Merge: 7e1777f5ec17 b7ba6d8dc356
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 17 09:46:15 2022 -0700

    Merge tag 'smp-urgent-2022-04-17' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull SMP fixes from Thomas Gleixner:
     "Two fixes for the SMP core:
    
       - Make the warning condition in flush_smp_call_function_queue()
         correct, which checked a just emptied list head for being empty
         instead of validating that there was no pending entry on the
         offlined CPU at all.
    
       - The @cpu member of struct cpuhp_cpu_state is initialized when the
         CPU hotplug thread for the upcoming CPU is created. That's too late
         because the creation of the thread can fail and then the following
         rollback operates on CPU0. Get rid of the CPU member and hand the
         CPU number to the involved functions directly"
    
    * tag 'smp-urgent-2022-04-17' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Remove the 'cpu' member of cpuhp_cpu_state
      smp: Fix offline cpu check in flush_smp_call_function_queue()

commit 9e949a3886356fe9112c6f6f34a6e23d1d35407f
Author: Nadav Amit <namit@vmware.com>
Date:   Sat Mar 19 00:20:15 2022 -0700

    smp: Fix offline cpu check in flush_smp_call_function_queue()
    
    The check in flush_smp_call_function_queue() for callbacks that are sent
    to offline CPUs currently checks whether the queue is empty.
    
    However, flush_smp_call_function_queue() has just deleted all the
    callbacks from the queue and moved all the entries into a local list.
    This checks would only be positive if some callbacks were added in the
    short time after llist_del_all() was called. This does not seem to be
    the intention of this check.
    
    Change the check to look at the local list to which the entries were
    moved instead of the queue from which all the callbacks were just
    removed.
    
    Fixes: 8d056c48e4862 ("CPU hotplug, smp: flush any pending IPI callbacks before CPU offline")
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20220319072015.1495036-1-namit@vmware.com

commit bae1a962ac2c5e6be08319ff3f7d6df542584fce
Author: Kuppuswamy Sathyanarayanan <sathyanarayanan.kuppuswamy@linux.intel.com>
Date:   Wed Apr 6 02:29:33 2022 +0300

    x86/topology: Disable CPU online/offline control for TDX guests
    
    Unlike regular VMs, TDX guests use the firmware hand-off wakeup method
    to wake up the APs during the boot process. This wakeup model uses a
    mailbox to communicate with firmware to bring up the APs. As per the
    design, this mailbox can only be used once for the given AP, which means
    after the APs are booted, the same mailbox cannot be used to
    offline/online the given AP. More details about this requirement can be
    found in Intel TDX Virtual Firmware Design Guide, sec titled "AP
    initialization in OS" and in sec titled "Hotplug Device".
    
    Since the architecture does not support any method of offlining the
    CPUs, disable CPU hotplug support in the kernel.
    
    Since this hotplug disable feature can be re-used by other VM guests,
    add a new CC attribute CC_ATTR_HOTPLUG_DISABLED and use it to disable
    the hotplug support.
    
    Attempt to offline CPU will fail with -EOPNOTSUPP.
    
    Signed-off-by: Kuppuswamy Sathyanarayanan <sathyanarayanan.kuppuswamy@linux.intel.com>
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20220405232939.73860-25-kirill.shutemov@linux.intel.com

commit e5f28623ceb103e13fc3d7bd45edf9818b227fd0
Author: Ammar Faizi <ammarfaizi2@gnuweeb.org>
Date:   Tue Mar 29 17:47:05 2022 +0700

    x86/MCE/AMD: Fix memory leak when threshold_create_bank() fails
    
    In mce_threshold_create_device(), if threshold_create_bank() fails, the
    previously allocated threshold banks array @bp will be leaked because
    the call to mce_threshold_remove_device() will not free it.
    
    This happens because mce_threshold_remove_device() fetches the pointer
    through the threshold_banks per-CPU variable but bp is written there
    only after the bank creation is successful, and not before, when
    threshold_create_bank() fails.
    
    Add a helper which unwinds all the bank creation work previously done
    and pass into it the previously allocated threshold banks array for
    freeing.
    
      [ bp: Massage. ]
    
    Fixes: 6458de97fc15 ("x86/mce/amd: Straighten CPU hotplug path")
    Co-developed-by: Alviro Iskandar Setiawan <alviro.iskandar@gnuweeb.org>
    Signed-off-by: Alviro Iskandar Setiawan <alviro.iskandar@gnuweeb.org>
    Co-developed-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Yazen Ghannam <yazen.ghannam@amd.com>
    Signed-off-by: Ammar Faizi <ammarfaizi2@gnuweeb.org>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org>
    Link: https://lore.kernel.org/r/20220329104705.65256-3-ammarfaizi2@gnuweeb.org

commit d5fd43bac8396c9b213faf14cd4560d73b30f618
Merge: 57c06b6e1e74 a9fe7fa7d874
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 30 15:11:26 2022 -0700

    Merge tag 'for-5.18/parisc-2' of git://git.kernel.org/pub/scm/linux/kernel/git/deller/parisc-linux
    
    Pull more parisc architecture updates from Helge Deller:
    
     - Revert a patch to the invalidate/flush vmap routines which broke
       kernel patching functions on older PA-RISC machines.
    
     - Fix the kernel patching code wrt locking and flushing. Works now on
       B160L machine as well.
    
     - Fix CPU IRQ affinity for LASI, WAX and Dino chips
    
     - Add CPU hotplug support
    
     - Detect the hppa-suse-linux-gcc compiler when cross-compiling
    
    * tag 'for-5.18/parisc-2' of git://git.kernel.org/pub/scm/linux/kernel/git/deller/parisc-linux:
      parisc: Fix patch code locking and flushing
      parisc: Find a new timesync master if current CPU is removed
      parisc: Move common_stext into .text section when CONFIG_HOTPLUG_CPU=y
      parisc: Rewrite arch_cpu_idle_dead() for CPU hotplugging
      parisc: Implement __cpu_die() and __cpu_disable() for CPU hotplugging
      parisc: Add PDC locking functions for rendezvous code
      parisc: Move disable_sr_hashing_asm() into .text section
      parisc: Move CPU startup-related functions into .text section
      parisc: Move store_cpu_topology() into text section
      parisc: Switch from GENERIC_CPU_DEVICES to GENERIC_ARCH_TOPOLOGY
      parisc: Ensure set_firmware_width() is called only once
      parisc: Add constants for control registers and clean up mfctl()
      parisc: Detect hppa-suse-linux-gcc compiler for cross-building
      parisc: Clean up cpu_check_affinity() and drop cpu_set_affinity_irq()
      parisc: Fix CPU affinity for Lasi, WAX and Dino chips
      Revert "parisc: Fix invalidate/flush vmap routines"

commit 1afde47d082c92c4fd3d9b322d944f8d87469834
Author: Helge Deller <deller@gmx.de>
Date:   Sun Mar 27 15:03:53 2022 +0200

    parisc: Find a new timesync master if current CPU is removed
    
    When CPU hotplugging is enabled, the user may want to remove the
    current CPU which is providing the timer ticks. If this happens
    we need to find a new timesync master.
    
    Signed-off-by: Helge Deller <deller@gmx.de>

commit 98903688e6106d9ca68e44c7d218e61336d54631
Author: Helge Deller <deller@gmx.de>
Date:   Fri Mar 25 14:27:21 2022 +0100

    parisc: Rewrite arch_cpu_idle_dead() for CPU hotplugging
    
    Let the PDC firmware put the CPU into firmware idle loop with the
    pdc_cpu_rendezvous() function.
    
    Signed-off-by: Helge Deller <deller@gmx.de>

commit 88b3aac6228baaac6a3bcc0808845083b9d9f08f
Author: Helge Deller <deller@gmx.de>
Date:   Fri Mar 25 14:31:08 2022 +0100

    parisc: Implement __cpu_die() and __cpu_disable() for CPU hotplugging
    
    Add relevant code to __cpu_die() and __cpu_disable() to finally enable
    the CPU hotplugging features. Reset the irq count values in smp_callin()
    to zero before bringing up the CPU.
    
    It seems that the firmware may need up to 8 seconds to fully stop a CPU
    in which no other PDC calls are allowed to be made. Use a timeout
    __cpu_die() to accommodate for this.
    
    Use "chcpu -d 1" to bring CPU1 down, and "chcpu -e 1" to bring it up.
    
    Signed-off-by: Helge Deller <deller@gmx.de>

commit 62773112acc55d29727465d075fc61ed08a0a532
Author: Helge Deller <deller@gmx.de>
Date:   Thu Mar 24 19:46:50 2022 +0100

    parisc: Switch from GENERIC_CPU_DEVICES to GENERIC_ARCH_TOPOLOGY
    
    Switch away from the own cpu topology code to common code which is used
    by ARM64 and RISCV. That will allow us to enable CPU hotplug later on.
    
    Signed-off-by: Helge Deller <deller@gmx.de>

commit 0790ed623847bbdd440ae29cc01da81c99834ea5
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Mar 22 14:40:50 2022 -0700

    mm/memcg: disable migration instead of preemption in drain_all_stock().
    
    Before the for-each-CPU loop, preemption is disabled so that so that
    drain_local_stock() can be invoked directly instead of scheduling a
    worker.  Ensuring that drain_local_stock() completed on the local CPU is
    not correctness problem.  It _could_ be that the charging path will be
    forced to reclaim memory because cached charges are still waiting for
    their draining.
    
    Disabling preemption before invoking drain_local_stock() is problematic
    on PREEMPT_RT due to the sleeping locks involved.  To ensure that no CPU
    migrations happens across for_each_online_cpu() it is enouhg to use
    migrate_disable() which disables migration and keeps context preemptible
    to a sleeping lock can be acquired.  A race with CPU hotplug is not a
    problem because pcp data is not going away.  In the worst case we just
    schedule draining of an empty stock.
    
    Use migrate_disable() instead of get_cpu() around the
    for_each_online_cpu() loop.
    
    Link: https://lkml.kernel.org/r/20220226204144.1008339-7-bigeasy@linutronix.de
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: kernel test robot <oliver.sang@intel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Michal Koutn <mkoutny@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Roman Gushchin <roman.gushchin@linux.dev>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Waiman Long <longman@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 31035f3e20af4ede5f1c8162068327ea0b35a96e
Merge: 2d6fc1455f3f 2045d38a6546
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Mar 18 19:00:26 2022 +0100

    Merge branch 'thermal-hfi'
    
    Merge Intel Hardware Feedback Interface (HFI) thermal driver for
    5.18-rc1 and update the intel-speed-select utility to support that
    driver.
    
    * thermal-hfi:
      tools/power/x86/intel-speed-select: v1.12 release
      tools/power/x86/intel-speed-select: HFI support
      tools/power/x86/intel-speed-select: OOB daemon mode
      thermal: intel: hfi: INTEL_HFI_THERMAL depends on NET
      thermal: netlink: Fix parameter type of thermal_genl_cpu_capability_event() stub
      thermal: intel: hfi: Notify user space for HFI events
      thermal: netlink: Add a new event to notify CPU capabilities change
      thermal: intel: hfi: Enable notification interrupt
      thermal: intel: hfi: Handle CPU hotplug events
      thermal: intel: hfi: Minimally initialize the Hardware Feedback Interface
      x86/cpu: Add definitions for the Intel Hardware Feedback Interface
      x86/Documentation: Describe the Intel Hardware Feedback Interface

commit f90205b95368ee2b56fc523abda6c4d514901d9b
Author: Marc Zyngier <maz@kernel.org>
Date:   Wed Mar 9 18:06:00 2022 +0000

    arm64: Add cavium_erratum_23154_cpus missing sentinel
    
    Qian Cai reported that playing with CPU hotplug resulted in a
    out-of-bound access due to cavium_erratum_23154_cpus missing
    a sentinel indicating the end of the array.
    
    Add it in order to restore peace and harmony in the world
    of broken HW.
    
    Reported-by: Qian Cai <quic_qiancai@quicinc.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Fixes: 24a147bcef8c ("irqchip/gic-v3: Workaround Marvell erratum 38545 when reading IAR")
    Link: https://lore.kernel.org/r/YijmkXp1VG7e8lDx@qian
    Cc: Linu Cherian <lcherian@marvell.com>
    Cc: Will Deacon <will@kernel.org>
    Link: https://lore.kernel.org/r/20220309180600.3990874-1-maz@kernel.org
    Signed-off-by: Will Deacon <will@kernel.org>

commit d5578190bed3d110203e3b6b29c5a7a39d51c6c0
Merge: 63c564da11cb 6a2c1d450a6a 5ae0f1b58b28 00a8b4b54cd6 c9515875850f 9c0f1c7fd7c6 8ea7a53daf3c
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Thu Feb 24 09:38:46 2022 -0800

    Merge branches 'exp.2022.02.24a', 'fixes.2022.02.14a', 'rcu_barrier.2022.02.08a', 'rcu-tasks.2022.02.08a', 'rt.2022.02.01b', 'torture.2022.02.01b' and 'torturescript.2022.02.08a' into HEAD
    
    exp.2022.02.24a: Expedited grace-period updates.
    fixes.2022.02.14a: Miscellaneous fixes.
    rcu_barrier.2022.02.08a: Make rcu_barrier() no longer exclude CPU hotplug.
    rcu-tasks.2022.02.08a: RCU-tasks updates.
    rt.2022.02.01b: Real-time-related updates.
    torture.2022.02.01b: Torture-test updates.
    torturescript.2022.02.08a: Torture-test scripting updates.

commit 58340f8e952b613e0ead0bed58b97b05bf4743c5
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Fri Feb 4 16:15:46 2022 +0100

    random: defer fast pool mixing to worker
    
    On PREEMPT_RT, it's problematic to take spinlocks from hard irq
    handlers. We can fix this by deferring to a workqueue the dumping of
    the fast pool into the input pool.
    
    We accomplish this with some careful rules on fast_pool->count:
    
      - When it's incremented to >= 64, we schedule the work.
      - If the top bit is set, we never schedule the work, even if >= 64.
      - The worker is responsible for setting it back to 0 when it's done.
    
    There are two small issues around using workqueues for this purpose that
    we work around.
    
    The first issue is that mix_interrupt_randomness() might be migrated to
    another CPU during CPU hotplug. This issue is rectified by checking that
    it hasn't been migrated (after disabling irqs). If it has been migrated,
    then we set the count to zero, so that when the CPU comes online again,
    it can requeue the work. As part of this, we switch to using an
    atomic_t, so that the increment in the irq handler doesn't wipe out the
    zeroing if the CPU comes back online while this worker is running.
    
    The second issue is that, though relatively minor in effect, we probably
    want to make sure we get a consistent view of the pool onto the stack,
    in case it's interrupted by an irq while reading. To do this, we don't
    reenable irqs until after the copy. There are only 18 instructions
    between the cli and sti, so this is a pretty tiny window.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Jonathan Neuschfer <j.neuschaefer@gmx.net>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Sultan Alsawaf <sultan@kerneltoast.com>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>

commit 0fb3978b0aac3a5c08637aed03cc2d65f793508f
Author: Huang Ying <ying.huang@intel.com>
Date:   Mon Feb 14 20:15:52 2022 +0800

    sched/numa: Fix NUMA topology for systems with CPU-less nodes
    
    The NUMA topology parameters (sched_numa_topology_type,
    sched_domains_numa_levels, and sched_max_numa_distance, etc.)
    identified by scheduler may be wrong for systems with CPU-less nodes.
    
    For example, the ACPI SLIT of a system with CPU-less persistent
    memory (Intel Optane DCPMM) nodes is as follows,
    
    [000h 0000   4]                    Signature : "SLIT"    [System Locality Information Table]
    [004h 0004   4]                 Table Length : 0000042C
    [008h 0008   1]                     Revision : 01
    [009h 0009   1]                     Checksum : 59
    [00Ah 0010   6]                       Oem ID : "XXXX"
    [010h 0016   8]                 Oem Table ID : "XXXXXXX"
    [018h 0024   4]                 Oem Revision : 00000001
    [01Ch 0028   4]              Asl Compiler ID : "INTL"
    [020h 0032   4]        Asl Compiler Revision : 20091013
    
    [024h 0036   8]                   Localities : 0000000000000004
    [02Ch 0044   4]                 Locality   0 : 0A 15 11 1C
    [030h 0048   4]                 Locality   1 : 15 0A 1C 11
    [034h 0052   4]                 Locality   2 : 11 1C 0A 1C
    [038h 0056   4]                 Locality   3 : 1C 11 1C 0A
    
    While the `numactl -H` output is as follows,
    
    available: 4 nodes (0-3)
    node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
    node 0 size: 64136 MB
    node 0 free: 5981 MB
    node 1 cpus: 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95
    node 1 size: 64466 MB
    node 1 free: 10415 MB
    node 2 cpus:
    node 2 size: 253952 MB
    node 2 free: 253920 MB
    node 3 cpus:
    node 3 size: 253952 MB
    node 3 free: 253951 MB
    node distances:
    node   0   1   2   3
      0:  10  21  17  28
      1:  21  10  28  17
      2:  17  28  10  28
      3:  28  17  28  10
    
    In this system, there are only 2 sockets.  In each memory controller,
    both DRAM and PMEM DIMMs are installed.  Although the physical NUMA
    topology is simple, the logical NUMA topology becomes a little
    complex.  Because both the distance(0, 1) and distance (1, 3) are less
    than the distance (0, 3), it appears that node 1 sits between node 0
    and node 3.  And the whole system appears to be a glueless mesh NUMA
    topology type.  But it's definitely not, there is even no CPU in node 3.
    
    This isn't a practical problem now yet.  Because the PMEM nodes (node
    2 and node 3 in example system) are offlined by default during system
    boot.  So init_numa_topology_type() called during system boot will
    ignore them and set sched_numa_topology_type to NUMA_DIRECT.  And
    init_numa_topology_type() is only called at runtime when a CPU of a
    never-onlined-before node gets plugged in.  And there's no CPU in the
    PMEM nodes.  But it appears better to fix this to make the code more
    robust.
    
    To test the potential problem.  We have used a debug patch to call
    init_numa_topology_type() when the PMEM node is onlined (in
    __set_migration_target_nodes()).  With that, the NUMA parameters
    identified by scheduler is as follows,
    
    sched_numa_topology_type:       NUMA_GLUELESS_MESH
    sched_domains_numa_levels:      4
    sched_max_numa_distance:        28
    
    To fix the issue, the CPU-less nodes are ignored when the NUMA topology
    parameters are identified.  Because a node may become CPU-less or not
    at run time because of CPU hotplug, the NUMA topology parameters need
    to be re-initialized at runtime for CPU hotplug too.
    
    With the patch, the NUMA parameters identified for the example system
    above is as follows,
    
    sched_numa_topology_type:       NUMA_DIRECT
    sched_domains_numa_levels:      2
    sched_max_numa_distance:        21
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20220214121553.582248-1-ying.huang@intel.com

commit 1d41d2e82623b40ee27811fe9ea38bafe2e722e9
Merge: e47ca4032626 6df2a016c0c8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 11 12:02:09 2022 -0800

    Merge tag 'riscv-for-linus-5.17-rc4' of git://git.kernel.org/pub/scm/linux/kernel/git/riscv/linux
    
    Pull RISC-V fixes from Palmer Dabbelt:
    
     - A fix to avoid undefined behavior when stack backtracing, which
       manifests in GCC as incorrect stack addresses
    
     - A few fixes for the XIP kernels
    
     - A fix to tracking NUMA state on CPU hotplug
    
     - Support for the recently relesaed binutils-2.38, which changed the
       default ISA version to one without CSRs or fence.i in 'I' extension
    
    * tag 'riscv-for-linus-5.17-rc4' of git://git.kernel.org/pub/scm/linux/kernel/git/riscv/linux:
      riscv: fix build with binutils 2.38
      riscv: cpu-hotplug: clear cpu from numa map when teardown
      riscv: extable: fix err reg writing in dedicated uaccess handler
      riscv/mm: Add XIP_FIXUP for riscv_pfn_base
      riscv/mm: Add XIP_FIXUP for phys_ram_base
      riscv: Fix XIP_FIXUP_FLASH_OFFSET
      riscv: eliminate unreliable __builtin_frame_address(1)

commit 2d74e6319abe278981e79166b6c2d0c3ed39b1ae
Author: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
Date:   Thu Jan 27 11:34:51 2022 -0800

    thermal: intel: hfi: Handle CPU hotplug events
    
    All CPUs in a package are represented in an HFI table. There exists an
    HFI table per package. Thus, CPUs in a package need to coordinate to
    initialize and access the table. Do such coordination during CPU hotplug.
    Use the first CPU to come online in a package to initialize the HFI
    instance and the data structure representing it. Other CPUs in the same
    package need only to register or unregister themselves in that data
    structure.
    
    The HFI depends on both the package-level thermal management and the local
    APIC thermal local vector. Thus, to ensure that a CPU coming online has an
    associated HFI instance when the hardware issues an HFI event, enable the
    HFI only after having enabled the local APIC thermal vector. The thermal
    throttle driver takes care of the needed package-level initialization.
    
    Reviewed-by: Len Brown <len.brown@intel.com>
    Signed-off-by: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 24e7590c60aa9487b8e43583dc9885f62f8216c1
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon Jan 24 14:05:32 2022 +0100

    KVM: x86: Partially allow KVM_SET_CPUID{,2} after KVM_RUN
    
    commit c6617c61e8fe44b9e9fdfede921f61cac6b5149d upstream.
    
    Commit feb627e8d6f6 ("KVM: x86: Forbid KVM_SET_CPUID{,2} after KVM_RUN")
    forbade changing CPUID altogether but unfortunately this is not fully
    compatible with existing VMMs. In particular, QEMU reuses vCPU fds for
    CPU hotplug after unplug and it calls KVM_SET_CPUID2. Instead of full ban,
    check whether the supplied CPUID data is equal to what was previously set.
    
    Reported-by: Igor Mammedov <imammedo@redhat.com>
    Fixes: feb627e8d6f6 ("KVM: x86: Forbid KVM_SET_CPUID{,2} after KVM_RUN")
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Message-Id: <20220117150542.2176196-3-vkuznets@redhat.com>
    Cc: stable@vger.kernel.org
    [Do not call kvm_find_cpuid_entry repeatedly. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c3dce8ccca7ac468e4cb6d8ccb24014d1b89f74b
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Fri Dec 3 11:44:50 2021 +0000

    perf/arm-cmn: Fix CPU hotplug unregistration
    
    [ Upstream commit 56c7c6eaf3eb8ac1ec40d56096c0f2b27250da5f ]
    
    Attempting to migrate the PMU context after we've unregistered the PMU
    device, or especially if we never successfully registered it in the
    first place, is a woefully bad idea. It's also fundamentally pointless
    anyway. Make sure to unregister an instance from the hotplug handler
    *without* invoking the teardown callback.
    
    Fixes: 0ba64770a2f2 ("perf: Add Arm CMN-600 PMU driver")
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Link: https://lore.kernel.org/r/2c221d745544774e4b07583b65b5d4d94f7e0fe4.1638530442.git.robin.murphy@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f99f6ea71b55a09d344be9fb1d71a92560d6cb7b
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Fri Dec 3 11:44:50 2021 +0000

    perf/arm-cmn: Fix CPU hotplug unregistration
    
    [ Upstream commit 56c7c6eaf3eb8ac1ec40d56096c0f2b27250da5f ]
    
    Attempting to migrate the PMU context after we've unregistered the PMU
    device, or especially if we never successfully registered it in the
    first place, is a woefully bad idea. It's also fundamentally pointless
    anyway. Make sure to unregister an instance from the hotplug handler
    *without* invoking the teardown callback.
    
    Fixes: 0ba64770a2f2 ("perf: Add Arm CMN-600 PMU driver")
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Link: https://lore.kernel.org/r/2c221d745544774e4b07583b65b5d4d94f7e0fe4.1638530442.git.robin.murphy@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit c6617c61e8fe44b9e9fdfede921f61cac6b5149d
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon Jan 17 16:05:40 2022 +0100

    KVM: x86: Partially allow KVM_SET_CPUID{,2} after KVM_RUN
    
    Commit feb627e8d6f6 ("KVM: x86: Forbid KVM_SET_CPUID{,2} after KVM_RUN")
    forbade changing CPUID altogether but unfortunately this is not fully
    compatible with existing VMMs. In particular, QEMU reuses vCPU fds for
    CPU hotplug after unplug and it calls KVM_SET_CPUID2. Instead of full ban,
    check whether the supplied CPUID data is equal to what was previously set.
    
    Reported-by: Igor Mammedov <imammedo@redhat.com>
    Fixes: feb627e8d6f6 ("KVM: x86: Forbid KVM_SET_CPUID{,2} after KVM_RUN")
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Message-Id: <20220117150542.2176196-3-vkuznets@redhat.com>
    Cc: stable@vger.kernel.org
    [Do not call kvm_find_cpuid_entry repeatedly. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit 5512c5eaf533a98d33a8dc0dcf415e72773184c8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Dec 6 23:51:13 2021 +0100

    PCI/MSI: Protect MSI operations
    
    To prepare for dynamic extension of MSI-X vectors, protect the MSI
    operations for MSI and MSI-X. This requires to move the invocation of
    irq_create_affinity_masks() out of the descriptor lock section to avoid
    reverse lock ordering vs. CPU hotplug lock as some callers of the PCI/MSI
    allocation interfaces already hold it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Michael Kelley <mikelley@microsoft.com>
    Tested-by: Nishanth Menon <nm@ti.com>
    Reviewed-by: Jason Gunthorpe <jgg@nvidia.com>
    Acked-by: Bjorn Helgaas <bhelgaas@google.com>
    Link: https://lore.kernel.org/r/20211206210747.982292705@linutronix.de

commit 3b54c71537d7beaaca8be9c57a81045e2b641655
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Nov 5 23:29:23 2021 +1000

    powerpc/pseries: use slab context cpumask allocation in CPU hotplug init
    
    Slab is up at this point, using the bootmem allocator triggers a
    warning. Switch to using the regular cpumask allocator.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Tested-by: Sachin Sant <sachinp@linux.vnet.ibm.com>
    Reviewed-by: Nathan Lynch <nathanl@linux.ibm.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20211105132923.1582514-1-npiggin@gmail.com

commit fc369f925f5c15a531313617fb22641aa6af874c
Merge: d58071a8a76d a88fa6c28b86
Author: Will Deacon <will@kernel.org>
Date:   Tue Dec 14 13:41:42 2021 +0000

    Merge branch 'for-next/perf-cmn' into for-next/perf
    
    * for-next/perf-cmn:
      perf/arm-cmn: Add debugfs topology info
      perf/arm-cmn: Add CI-700 Support
      dt-bindings: perf: arm-cmn: Add CI-700
      perf/arm-cmn: Support new IP features
      perf/arm-cmn: Demarcate CMN-600 specifics
      perf/arm-cmn: Move group validation data off-stack
      perf/arm-cmn: Optimise DTC counter accesses
      perf/arm-cmn: Optimise DTM counter reads
      perf/arm-cmn: Refactor DTM handling
      perf/arm-cmn: Streamline node iteration
      perf/arm-cmn: Refactor node ID handling
      perf/arm-cmn: Drop compile-test restriction
      perf/arm-cmn: Account for NUMA affinity
      perf/arm-cmn: Fix CPU hotplug unregistration

commit 56c7c6eaf3eb8ac1ec40d56096c0f2b27250da5f
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Fri Dec 3 11:44:50 2021 +0000

    perf/arm-cmn: Fix CPU hotplug unregistration
    
    Attempting to migrate the PMU context after we've unregistered the PMU
    device, or especially if we never successfully registered it in the
    first place, is a woefully bad idea. It's also fundamentally pointless
    anyway. Make sure to unregister an instance from the hotplug handler
    *without* invoking the teardown callback.
    
    Fixes: 0ba64770a2f2 ("perf: Add Arm CMN-600 PMU driver")
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Link: https://lore.kernel.org/r/2c221d745544774e4b07583b65b5d4d94f7e0fe4.1638530442.git.robin.murphy@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

commit 32f71f36808612dd829bc20957b802dd698c2e27
Author: Thomas Richter <tmricht@linux.ibm.com>
Date:   Wed Nov 3 13:13:04 2021 +0100

    s390/cpumf: cpum_cf PMU displays invalid value after hotplug remove
    
    commit 9d48c7afedf91a02d03295837ec76b2fb5e7d3fe upstream.
    
    When a CPU is hotplugged while the perf stat -e cycles command is
    running, a wrong (very large) value is displayed immediately after the
    CPU removal:
    
      Check the values, shouldn't be too high as in
                time             counts unit events
         1.001101919           29261846      cycles
         2.002454499           17523405      cycles
         3.003659292           24361161      cycles
         4.004816983 18446744073638406144      cycles
         5.005671647      <not counted>      cycles
         ...
    
    The CPU hotplug off took place after 3 seconds.
    The issue is the read of the event count value after 4 seconds when
    the CPU is not available and the read of the counter returns an
    error. This is treated as a counter value of zero. This results
    in a very large value (0 - previous_value).
    
    Fix this by detecting the hotplugged off CPU and report 0 instead
    of a very large number.
    
    Cc: stable@vger.kernel.org
    Fixes: a029a4eab39e ("s390/cpumf: Allow concurrent access for CPU Measurement Counter Facility")
    Reported-by: Sumanth Korikkar <sumanthk@linux.ibm.com>
    Signed-off-by: Thomas Richter <tmricht@linux.ibm.com>
    Reviewed-by: Sumanth Korikkar <sumanthk@linux.ibm.com>
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2ea358c095adf2ce63bf2ad3a680a85b725f8bfb
Author: Thomas Richter <tmricht@linux.ibm.com>
Date:   Wed Nov 3 13:13:04 2021 +0100

    s390/cpumf: cpum_cf PMU displays invalid value after hotplug remove
    
    commit 9d48c7afedf91a02d03295837ec76b2fb5e7d3fe upstream.
    
    When a CPU is hotplugged while the perf stat -e cycles command is
    running, a wrong (very large) value is displayed immediately after the
    CPU removal:
    
      Check the values, shouldn't be too high as in
                time             counts unit events
         1.001101919           29261846      cycles
         2.002454499           17523405      cycles
         3.003659292           24361161      cycles
         4.004816983 18446744073638406144      cycles
         5.005671647      <not counted>      cycles
         ...
    
    The CPU hotplug off took place after 3 seconds.
    The issue is the read of the event count value after 4 seconds when
    the CPU is not available and the read of the counter returns an
    error. This is treated as a counter value of zero. This results
    in a very large value (0 - previous_value).
    
    Fix this by detecting the hotplugged off CPU and report 0 instead
    of a very large number.
    
    Cc: stable@vger.kernel.org
    Fixes: a029a4eab39e ("s390/cpumf: Allow concurrent access for CPU Measurement Counter Facility")
    Reported-by: Sumanth Korikkar <sumanthk@linux.ibm.com>
    Signed-off-by: Thomas Richter <tmricht@linux.ibm.com>
    Reviewed-by: Sumanth Korikkar <sumanthk@linux.ibm.com>
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9d48c7afedf91a02d03295837ec76b2fb5e7d3fe
Author: Thomas Richter <tmricht@linux.ibm.com>
Date:   Wed Nov 3 13:13:04 2021 +0100

    s390/cpumf: cpum_cf PMU displays invalid value after hotplug remove
    
    When a CPU is hotplugged while the perf stat -e cycles command is
    running, a wrong (very large) value is displayed immediately after the
    CPU removal:
    
      Check the values, shouldn't be too high as in
                time             counts unit events
         1.001101919           29261846      cycles
         2.002454499           17523405      cycles
         3.003659292           24361161      cycles
         4.004816983 18446744073638406144      cycles
         5.005671647      <not counted>      cycles
         ...
    
    The CPU hotplug off took place after 3 seconds.
    The issue is the read of the event count value after 4 seconds when
    the CPU is not available and the read of the counter returns an
    error. This is treated as a counter value of zero. This results
    in a very large value (0 - previous_value).
    
    Fix this by detecting the hotplugged off CPU and report 0 instead
    of a very large number.
    
    Cc: stable@vger.kernel.org
    Fixes: a029a4eab39e ("s390/cpumf: Allow concurrent access for CPU Measurement Counter Facility")
    Reported-by: Sumanth Korikkar <sumanthk@linux.ibm.com>
    Signed-off-by: Thomas Richter <tmricht@linux.ibm.com>
    Reviewed-by: Sumanth Korikkar <sumanthk@linux.ibm.com>
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>

commit 9c25cbfcb38462803a3d68f5d88e66a587f5f045
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Nov 5 13:40:52 2021 -0700

    mm: page_alloc: use migrate_disable() in drain_local_pages_wq()
    
    drain_local_pages_wq() disables preemption to avoid CPU migration during
    CPU hotplug and can't use cpus_read_lock().
    
    Using migrate_disable() works here, too.  The scheduler won't take the
    CPU offline until the task left the migrate-disable section.  The
    problem with disabled preemption here is that drain_local_pages()
    acquires locks which are turned into sleeping locks on PREEMPT_RT and
    can't be acquired with disabled preemption.
    
    Use migrate_disable() in drain_local_pages_wq().
    
    Link: https://lkml.kernel.org/r/20211015210933.viw6rjvo64qtqxn4@linutronix.de
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit d9abdee5fd5abffd0e763e52fbfa3116de167822
Merge: 519d81956ee2 362d5dfc483c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 19 05:41:36 2021 -1000

    Merge branch 'akpm' (patches from Andrew)
    
    Merge misc fixes from Andrew Morton:
     "19 patches.
    
      Subsystems affected by this patch series: mm (userfaultfd, migration,
      memblock, mempolicy, slub, secretmem, and thp), ocfs2, binfmt, vfs,
      and misc"
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>:
      mailmap: add Andrej Shadura
      mm/thp: decrease nr_thps in file's mapping on THP split
      mm/secretmem: fix NULL page->mapping dereference in page_is_secretmem()
      vfs: check fd has read access in kernel_read_file_from_fd()
      elfcore: correct reference to CONFIG_UML
      mm, slub: fix incorrect memcg slab count for bulk free
      mm, slub: fix potential use-after-free in slab_debugfs_fops
      mm, slub: fix potential memoryleak in kmem_cache_open()
      mm, slub: fix mismatch between reconstructed freelist depth and cnt
      mm, slub: fix two bugs in slab_debug_trace_open()
      mm/mempolicy: do not allow illegal MPOL_F_NUMA_BALANCING | MPOL_LOCAL in mbind()
      memblock: check memory total_size
      ocfs2: mount fails with buffer overflow in strlen
      ocfs2: fix data corruption after conversion from inline format
      mm/migrate: fix CPUHP state to update node demotion order
      mm/migrate: add CPU hotplug to demotion #ifdef
      mm/migrate: optimize hotplug-time demotion order updates
      userfaultfd: fix a race between writeprotect and exit_mmap()
      mm/userfaultfd: selftests: fix memory corruption with thp enabled

commit a6a0251c6fce496744121b4e08c899f45270dbcc
Author: Huang Ying <ying.huang@intel.com>
Date:   Mon Oct 18 15:15:35 2021 -0700

    mm/migrate: fix CPUHP state to update node demotion order
    
    The node demotion order needs to be updated during CPU hotplug.  Because
    whether a NUMA node has CPU may influence the demotion order.  The
    update function should be called during CPU online/offline after the
    node_states[N_CPU] has been updated.  That is done in
    CPUHP_AP_ONLINE_DYN during CPU online and in CPUHP_MM_VMSTAT_DEAD during
    CPU offline.  But in commit 884a6e5d1f93 ("mm/migrate: update node
    demotion order on hotplug events"), the function to update node demotion
    order is called in CPUHP_AP_ONLINE_DYN during CPU online/offline.  This
    doesn't satisfy the order requirement.
    
    For example, there are 4 CPUs (P0, P1, P2, P3) in 2 sockets (P0, P1 in S0
    and P2, P3 in S1), the demotion order is
    
     - S0 -> NUMA_NO_NODE
     - S1 -> NUMA_NO_NODE
    
    After P2 and P3 is offlined, because S1 has no CPU now, the demotion
    order should have been changed to
    
     - S0 -> S1
     - S1 -> NO_NODE
    
    but it isn't changed, because the order updating callback for CPU
    hotplug doesn't see the new nodemask.  After that, if P1 is offlined,
    the demotion order is changed to the expected order as above.
    
    So in this patch, we added CPUHP_AP_MM_DEMOTION_ONLINE and
    CPUHP_MM_DEMOTION_DEAD to be called after CPUHP_AP_ONLINE_DYN and
    CPUHP_MM_VMSTAT_DEAD during CPU online and offline, and register the
    update function on them.
    
    Link: https://lkml.kernel.org/r/20210929060351.7293-1-ying.huang@intel.com
    Fixes: 884a6e5d1f93 ("mm/migrate: update node demotion order on hotplug events")
    Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Yang Shi <shy828301@gmail.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Wei Xu <weixugc@google.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Keith Busch <kbusch@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 76af6a054da4055305ddb28c5eb151b9ee4f74f9
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Mon Oct 18 15:15:32 2021 -0700

    mm/migrate: add CPU hotplug to demotion #ifdef
    
    Once upon a time, the node demotion updates were driven solely by memory
    hotplug events.  But now, there are handlers for both CPU and memory
    hotplug.
    
    However, the #ifdef around the code checks only memory hotplug.  A
    system that has HOTPLUG_CPU=y but MEMORY_HOTPLUG=n would miss CPU
    hotplug events.
    
    Update the #ifdef around the common code.  Add memory and CPU-specific
    #ifdefs for their handlers.  These memory/CPU #ifdefs avoid unused
    function warnings when their Kconfig option is off.
    
    [arnd@arndb.de: rework hotplug_memory_notifier() stub]
      Link: https://lkml.kernel.org/r/20211013144029.2154629-1-arnd@kernel.org
    
    Link: https://lkml.kernel.org/r/20210924161255.E5FE8F7E@davehans-spike.ostc.intel.com
    Fixes: 884a6e5d1f93 ("mm/migrate: update node demotion order on hotplug events")
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: "Huang, Ying" <ying.huang@intel.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Wei Xu <weixugc@google.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Yang Shi <yang.shi@linux.alibaba.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 295be91f7ef0027fca2f2e4788e99731aa931834
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Mon Oct 18 15:15:29 2021 -0700

    mm/migrate: optimize hotplug-time demotion order updates
    
    Patch series "mm/migrate: 5.15 fixes for automatic demotion", v2.
    
    This contains two fixes for the "automatic demotion" code which was
    merged into 5.15:
    
     * Fix memory hotplug performance regression by watching
       suppressing any real action on irrelevant hotplug events.
    
     * Ensure CPU hotplug handler is registered when memory hotplug
       is disabled.
    
    This patch (of 2):
    
    == tl;dr ==
    
    Automatic demotion opted for a simple, lazy approach to handling hotplug
    events.  This noticeably slows down memory hotplug[1].  Optimize away
    updates to the demotion order when memory hotplug events should have no
    effect.
    
    This has no effect on CPU hotplug.  There is no known problem on the CPU
    side and any work there will be in a separate series.
    
    == Background ==
    
    Automatic demotion is a memory migration strategy to ensure that new
    allocations have room in faster memory tiers on tiered memory systems.
    The kernel maintains an array (node_demotion[]) to drive these
    migrations.
    
    The node_demotion[] path is calculated by starting at nodes with CPUs
    and then "walking" to nodes with memory.  Only hotplug events which
    online or offline a node with memory (N_ONLINE) or CPUs (N_CPU) will
    actually affect the migration order.
    
    == Problem ==
    
    However, the current code is lazy.  It completely regenerates the
    migration order on *any* CPU or memory hotplug event.  The logic was
    that these events are extremely rare and that the overhead from
    indiscriminate order regeneration is minimal.
    
    Part of the update logic involves a synchronize_rcu(), which is a pretty
    big hammer.  Its overhead was large enough to be detected by some 0day
    tests that watch memory hotplug performance[1].
    
    == Solution ==
    
    Add a new helper (node_demotion_topo_changed()) which can differentiate
    between superfluous and impactful hotplug events.  Skip the expensive
    update operation for superfluous events.
    
    == Aside: Locking ==
    
    It took me a few moments to declare the locking to be safe enough for
    node_demotion_topo_changed() to work.  It all hinges on the memory
    hotplug lock:
    
    During memory hotplug events, 'mem_hotplug_lock' is held for write.
    This ensures that two memory hotplug events can not be called
    simultaneously.
    
    CPU hotplug has a similar lock (cpuhp_state_mutex) which also provides
    mutual exclusion between CPU hotplug events.  In addition, the demotion
    code acquire and hold the mem_hotplug_lock for read during its CPU
    hotplug handlers.  This provides mutual exclusion between the demotion
    memory hotplug callbacks and the CPU hotplug callbacks.
    
    This effectively allows treating the migration target generation code to
    act as if it is single-threaded.
    
    1. https://lore.kernel.org/all/20210905135932.GE15026@xsang-OptiPlex-9020/
    
    Link: https://lkml.kernel.org/r/20210924161251.093CCD06@davehans-spike.ostc.intel.com
    Link: https://lkml.kernel.org/r/20210924161253.D7673E31@davehans-spike.ostc.intel.com
    Fixes: 884a6e5d1f93 ("mm/migrate: update node demotion order on hotplug events")
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reported-by: kernel test robot <oliver.sang@intel.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Cc: "Huang, Ying" <ying.huang@intel.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Wei Xu <weixugc@google.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Yang Shi <yang.shi@linux.alibaba.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 0a1b8623d10c91959b53238e99529eaedb605f6e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Aug 31 13:48:34 2021 +0200

    drivers: base: cacheinfo: Get rid of DEFINE_SMP_CALL_CACHE_FUNCTION()
    
    [ Upstream commit 4b92d4add5f6dcf21275185c997d6ecb800054cd ]
    
    DEFINE_SMP_CALL_CACHE_FUNCTION() was usefel before the CPU hotplug rework
    to ensure that the cache related functions are called on the upcoming CPU
    because the notifier itself could run on any online CPU.
    
    The hotplug state machine guarantees that the callbacks are invoked on the
    upcoming CPU. So there is no need to have this SMP function call
    obfuscation. That indirection was missed when the hotplug notifiers were
    converted.
    
    This also solves the problem of ARM64 init_cache_level() invoking ACPI
    functions which take a semaphore in that context. That's invalid as SMP
    function calls run with interrupts disabled. Running it just from the
    callback in context of the CPU hotplug thread solves this.
    
    Fixes: 8571890e1513 ("arm64: Add support for ACPI based firmware tables")
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lore.kernel.org/r/871r69ersb.ffs@tglx
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit b9a1526d51744075a6245d3f3a5544b10a5405c9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Aug 31 13:48:34 2021 +0200

    drivers: base: cacheinfo: Get rid of DEFINE_SMP_CALL_CACHE_FUNCTION()
    
    [ Upstream commit 4b92d4add5f6dcf21275185c997d6ecb800054cd ]
    
    DEFINE_SMP_CALL_CACHE_FUNCTION() was usefel before the CPU hotplug rework
    to ensure that the cache related functions are called on the upcoming CPU
    because the notifier itself could run on any online CPU.
    
    The hotplug state machine guarantees that the callbacks are invoked on the
    upcoming CPU. So there is no need to have this SMP function call
    obfuscation. That indirection was missed when the hotplug notifiers were
    converted.
    
    This also solves the problem of ARM64 init_cache_level() invoking ACPI
    functions which take a semaphore in that context. That's invalid as SMP
    function calls run with interrupts disabled. Running it just from the
    callback in context of the CPU hotplug thread solves this.
    
    Fixes: 8571890e1513 ("arm64: Add support for ACPI based firmware tables")
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lore.kernel.org/r/871r69ersb.ffs@tglx
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2f7bfc07e38662077f802abe56715b5e92663364
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Aug 31 13:48:34 2021 +0200

    drivers: base: cacheinfo: Get rid of DEFINE_SMP_CALL_CACHE_FUNCTION()
    
    [ Upstream commit 4b92d4add5f6dcf21275185c997d6ecb800054cd ]
    
    DEFINE_SMP_CALL_CACHE_FUNCTION() was usefel before the CPU hotplug rework
    to ensure that the cache related functions are called on the upcoming CPU
    because the notifier itself could run on any online CPU.
    
    The hotplug state machine guarantees that the callbacks are invoked on the
    upcoming CPU. So there is no need to have this SMP function call
    obfuscation. That indirection was missed when the hotplug notifiers were
    converted.
    
    This also solves the problem of ARM64 init_cache_level() invoking ACPI
    functions which take a semaphore in that context. That's invalid as SMP
    function calls run with interrupts disabled. Running it just from the
    callback in context of the CPU hotplug thread solves this.
    
    Fixes: 8571890e1513 ("arm64: Add support for ACPI based firmware tables")
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lore.kernel.org/r/871r69ersb.ffs@tglx
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 6bd98f8259ac7e0c6f7cbf505b06504b18106623
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Aug 31 13:48:34 2021 +0200

    drivers: base: cacheinfo: Get rid of DEFINE_SMP_CALL_CACHE_FUNCTION()
    
    [ Upstream commit 4b92d4add5f6dcf21275185c997d6ecb800054cd ]
    
    DEFINE_SMP_CALL_CACHE_FUNCTION() was usefel before the CPU hotplug rework
    to ensure that the cache related functions are called on the upcoming CPU
    because the notifier itself could run on any online CPU.
    
    The hotplug state machine guarantees that the callbacks are invoked on the
    upcoming CPU. So there is no need to have this SMP function call
    obfuscation. That indirection was missed when the hotplug notifiers were
    converted.
    
    This also solves the problem of ARM64 init_cache_level() invoking ACPI
    functions which take a semaphore in that context. That's invalid as SMP
    function calls run with interrupts disabled. Running it just from the
    callback in context of the CPU hotplug thread solves this.
    
    Fixes: 8571890e1513 ("arm64: Add support for ACPI based firmware tables")
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lore.kernel.org/r/871r69ersb.ffs@tglx
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 46aa886c483f57ef13cd5ea0a85e70b93eb1d381
Author: Neeraj Upadhyay <neeraju@codeaurora.org>
Date:   Fri Aug 27 13:43:35 2021 +0530

    rcu-tasks: Fix IPI failure handling in trc_wait_for_one_reader
    
    The trc_wait_for_one_reader() function is called at multiple stages
    of trace rcu-tasks GP function, rcu_tasks_wait_gp():
    
    - First, it is called as part of per task function -
      rcu_tasks_trace_pertask(), for all non-idle tasks. As part of per task
      processing, this function add the task in the holdout list and if the
      task is currently running on a CPU, it sends IPI to the task's CPU.
      The IPI handler takes action depending on whether task is in trace
      rcu-tasks read side critical section or not:
    
      - a. If the task is in trace rcu-tasks read side critical section
           (t->trc_reader_nesting != 0), the IPI handler sets the task's
           ->trc_reader_special.b.need_qs, so that this task notifies exit
           from its outermost read side critical section (by decrementing
           trc_n_readers_need_end) to the GP handling function.
           trc_wait_for_one_reader() also increments trc_n_readers_need_end,
           so that the trace rcu-tasks GP handler function waits for this
           task's read side exit notification. The IPI handler also sets
           t->trc_reader_checked to true, and no further IPIs are sent for
           this task, for this trace rcu-tasks grace period and this
           task can be removed from holdout list.
    
      - b. If the task is in the process of exiting its trace rcu-tasks
           read side critical section, (t->trc_reader_nesting < 0), defer
           this task's processing to future calls to trc_wait_for_one_reader().
    
      - c. If task is not in rcu-task read side critical section,
           t->trc_reader_nesting == 0, ->trc_reader_checked is set for this
           task, so that this task is removed from holdout list.
    
    - Second, trc_wait_for_one_reader() is called as part of post scan, in
      function rcu_tasks_trace_postscan(), for all idle tasks.
    
    - Third, in function check_all_holdout_tasks_trace(), this function is
      called for each task in the holdout list, but only if there isn't
      a pending IPI for the task (->trc_ipi_to_cpu == -1). This function
      removed the task from holdout list, if IPI handler has completed the
      required work, to ensure that the current trace rcu-tasks grace period
      either waits for this task, or this task is not in a trace rcu-tasks
      read side critical section.
    
    Now, considering the scenario where smp_call_function_single() fails in
    first case, inside rcu_tasks_trace_pertask(). In this case,
    ->trc_ipi_to_cpu is set to the current CPU for that task. This will
    result in trc_wait_for_one_reader() getting skipped in third case,
    inside check_all_holdout_tasks_trace(), for this task. This further
    results in ->trc_reader_checked never getting set for this task,
    and the task not getting removed from holdout list. This can cause
    the current trace rcu-tasks grace period to stall.
    
    Fix the above problem, by resetting ->trc_ipi_to_cpu to -1, on
    smp_call_function_single() failure, so that future IPI calls can
    be send for this task.
    
    Note that all three of the trc_wait_for_one_reader() function's
    callers (rcu_tasks_trace_pertask(), rcu_tasks_trace_postscan(),
    check_all_holdout_tasks_trace()) hold cpu_read_lock().  This means
    that smp_call_function_single() cannot race with CPU hotplug, and thus
    should never fail.  Therefore, also add a warning in order to report
    any such failure in case smp_call_function_single() grows some other
    reason for failure.
    
    Signed-off-by: Neeraj Upadhyay <neeraju@codeaurora.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit b3d3890ed59ead6c95b4586e32a6adad01716013
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jul 20 10:18:28 2021 -0400

    cgroup/cpuset: Fix violation of cpuset locking rule
    
    [ Upstream commit 6ba34d3c73674e46d9e126e4f0cee79e5ef2481c ]
    
    The cpuset fields that manage partition root state do not strictly
    follow the cpuset locking rule that update to cpuset has to be done
    with both the callback_lock and cpuset_mutex held. This is now fixed
    by making sure that the locking rule is upheld.
    
    Fixes: 3881b86128d0 ("cpuset: Add an error state to cpuset.sched.partition")
    Fixes: 4b842da276a8 ("cpuset: Make CPU hotplug work with partition")
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 5e99b869007b05cf9e71b323929ff02b44e84e90
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jul 20 10:18:27 2021 -0400

    cgroup/cpuset: Fix a partition bug with hotplug
    
    [ Upstream commit 15d428e6fe77fffc3f4fff923336036f5496ef17 ]
    
    In cpuset_hotplug_workfn(), the detection of whether the cpu list
    has been changed is done by comparing the effective cpus of the top
    cpuset with the cpu_active_mask. However, in the rare case that just
    all the CPUs in the subparts_cpus are offlined, the detection fails
    and the partition states are not updated correctly. Fix it by forcing
    the cpus_updated flag to true in this particular case.
    
    Fixes: 4b842da276a8 ("cpuset: Make CPU hotplug work with partition")
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 5e31c9a94e92578cf5c15c58f0481bcb37a2d506
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jul 20 10:18:28 2021 -0400

    cgroup/cpuset: Fix violation of cpuset locking rule
    
    [ Upstream commit 6ba34d3c73674e46d9e126e4f0cee79e5ef2481c ]
    
    The cpuset fields that manage partition root state do not strictly
    follow the cpuset locking rule that update to cpuset has to be done
    with both the callback_lock and cpuset_mutex held. This is now fixed
    by making sure that the locking rule is upheld.
    
    Fixes: 3881b86128d0 ("cpuset: Add an error state to cpuset.sched.partition")
    Fixes: 4b842da276a8 ("cpuset: Make CPU hotplug work with partition")
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 1c58376ca37f636f69e66dd020278b46f22ecd36
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jul 20 10:18:27 2021 -0400

    cgroup/cpuset: Fix a partition bug with hotplug
    
    [ Upstream commit 15d428e6fe77fffc3f4fff923336036f5496ef17 ]
    
    In cpuset_hotplug_workfn(), the detection of whether the cpu list
    has been changed is done by comparing the effective cpus of the top
    cpuset with the cpu_active_mask. However, in the rare case that just
    all the CPUs in the subparts_cpus are offlined, the detection fails
    and the partition states are not updated correctly. Fix it by forcing
    the cpus_updated flag to true in this particular case.
    
    Fixes: 4b842da276a8 ("cpuset: Make CPU hotplug work with partition")
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 10dfcfda5c6f532726caf3b0e63a6d705592942b
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jul 20 10:18:28 2021 -0400

    cgroup/cpuset: Fix violation of cpuset locking rule
    
    [ Upstream commit 6ba34d3c73674e46d9e126e4f0cee79e5ef2481c ]
    
    The cpuset fields that manage partition root state do not strictly
    follow the cpuset locking rule that update to cpuset has to be done
    with both the callback_lock and cpuset_mutex held. This is now fixed
    by making sure that the locking rule is upheld.
    
    Fixes: 3881b86128d0 ("cpuset: Add an error state to cpuset.sched.partition")
    Fixes: 4b842da276a8 ("cpuset: Make CPU hotplug work with partition")
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit e0f3de1573fd00cfcff5252ebc66d70df92ce717
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jul 20 10:18:27 2021 -0400

    cgroup/cpuset: Fix a partition bug with hotplug
    
    [ Upstream commit 15d428e6fe77fffc3f4fff923336036f5496ef17 ]
    
    In cpuset_hotplug_workfn(), the detection of whether the cpu list
    has been changed is done by comparing the effective cpus of the top
    cpuset with the cpu_active_mask. However, in the rare case that just
    all the CPUs in the subparts_cpus are offlined, the detection fails
    and the partition states are not updated correctly. Fix it by forcing
    the cpus_updated flag to true in this particular case.
    
    Fixes: 4b842da276a8 ("cpuset: Make CPU hotplug work with partition")
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 9fdac650c41314ae02095bdebc5f7bc6ce8a4452
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jul 20 10:18:27 2021 -0400

    cgroup/cpuset: Fix a partition bug with hotplug
    
    [ Upstream commit 15d428e6fe77fffc3f4fff923336036f5496ef17 ]
    
    In cpuset_hotplug_workfn(), the detection of whether the cpu list
    has been changed is done by comparing the effective cpus of the top
    cpuset with the cpu_active_mask. However, in the rare case that just
    all the CPUs in the subparts_cpus are offlined, the detection fails
    and the partition states are not updated correctly. Fix it by forcing
    the cpus_updated flag to true in this particular case.
    
    Fixes: 4b842da276a8 ("cpuset: Make CPU hotplug work with partition")
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f306b90c69ce3994bb8046b54374a90a27f66be6
Merge: d8e988b62f94 c9871c800f65
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Sep 12 12:42:51 2021 -0700

    Merge tag 'smp-urgent-2021-09-12' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug updates from Thomas Gleixner:
     "Updates for the SMP and CPU hotplug:
    
       - Remove DEFINE_SMP_CALL_CACHE_FUNCTION() which is a left over of the
         original hotplug code and now causing trouble with the ARM64 cache
         topology setup due to the pointless SMP function call.
    
         It's not longer required as the hotplug callbacks are guaranteed to
         be invoked on the upcoming CPU.
    
       - Remove the deprecated and now unused CPU hotplug functions
    
       - Rewrite the CPU hotplug API documentation"
    
    * tag 'smp-urgent-2021-09-12' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      Documentation: core-api/cpuhotplug: Rewrite the API section
      cpu/hotplug: Remove deprecated CPU-hotplug functions.
      thermal: Replace deprecated CPU-hotplug functions.
      drivers: base: cacheinfo: Get rid of DEFINE_SMP_CALL_CACHE_FUNCTION()

commit c122358ea1e510d3def876abb7872f1b2b7365c9
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:02 2021 +0200

    thermal: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-20-bigeasy@linutronix.de

commit 884a6e5d1f93b5032e5d6dd2a183f8b3f008416b
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Sep 2 14:59:09 2021 -0700

    mm/migrate: update node demotion order on hotplug events
    
    Reclaim-based migration is attempting to optimize data placement in memory
    based on the system topology.  If the system changes, so must the
    migration ordering.
    
    The implementation is conceptually simple and entirely unoptimized.  On
    any memory or CPU hotplug events, assume that a node was added or removed
    and recalculate all migration targets.  This ensures that the
    node_demotion[] array is always ready to be used in case the new reclaim
    mode is enabled.
    
    This recalculation is far from optimal, most glaringly that it does not
    even attempt to figure out the hotplug event would have some *actual*
    effect on the demotion order.  But, given the expected paucity of hotplug
    events, this should be fine.
    
    Link: https://lkml.kernel.org/r/20210721063926.3024591-2-ying.huang@intel.com
    Link: https://lkml.kernel.org/r/20210715055145.195411-3-ying.huang@intel.com
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Signed-off-by: "Huang, Ying" <ying.huang@intel.com>
    Reviewed-by: Yang Shi <shy828301@gmail.com>
    Reviewed-by: Zi Yan <ziy@nvidia.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Wei Xu <weixugc@google.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Keith Busch <kbusch@kernel.org>
    Cc: Yang Shi <yang.shi@linux.alibaba.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 4b92d4add5f6dcf21275185c997d6ecb800054cd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Aug 31 13:48:34 2021 +0200

    drivers: base: cacheinfo: Get rid of DEFINE_SMP_CALL_CACHE_FUNCTION()
    
    DEFINE_SMP_CALL_CACHE_FUNCTION() was usefel before the CPU hotplug rework
    to ensure that the cache related functions are called on the upcoming CPU
    because the notifier itself could run on any online CPU.
    
    The hotplug state machine guarantees that the callbacks are invoked on the
    upcoming CPU. So there is no need to have this SMP function call
    obfuscation. That indirection was missed when the hotplug notifiers were
    converted.
    
    This also solves the problem of ARM64 init_cache_level() invoking ACPI
    functions which take a semaphore in that context. That's invalid as SMP
    function calls run with interrupts disabled. Running it just from the
    callback in context of the CPU hotplug thread solves this.
    
    Fixes: 8571890e1513 ("arm64: Add support for ACPI based firmware tables")
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lore.kernel.org/r/871r69ersb.ffs@tglx

commit 08403e2174c4ac8b23922b5b7abe670129f8acb5
Merge: e4c3562e1bc7 7625eccd1852
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 30 14:10:07 2021 -0700

    Merge tag 'smp-core-2021-08-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull SMP core updates from Thomas Gleixner:
    
     - Replace get/put_online_cpus() in various places. The final removal
       will happen shortly before v5.15-rc1 when the rest of the patches
       have been merged.
    
     - Add debug code to help the analysis of CPU hotplug failures
    
     - A set of kernel doc updates
    
    * tag 'smp-core-2021-08-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      mm: Replace deprecated CPU-hotplug functions.
      md/raid5: Replace deprecated CPU-hotplug functions.
      Documentation: Replace deprecated CPU-hotplug functions.
      smp: Fix all kernel-doc warnings
      cpu/hotplug: Add debug printks for hotplug callback failures
      cpu/hotplug: Use DEVICE_ATTR_*() macro
      cpu/hotplug: Eliminate all kernel-doc warnings
      cpu/hotplug: Fix kernel doc warnings for __cpuhp_setup_state_cpuslocked()
      cpu/hotplug: Fix comment typo
      smpboot: Replace deprecated CPU-hotplug functions.

commit 7625eccd1852ac84d3aa6a06ffc2f710e683b3fe
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:03 2021 +0200

    mm: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-21-bigeasy@linutronix.de

commit 252034e03f04e54acfb5f5924dd26ae638e3215e
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:58 2021 +0200

    md/raid5: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Song Liu <song@kernel.org>
    Link: https://lore.kernel.org/r/20210803141621.780504-16-bigeasy@linutronix.de

commit c7483d823ee0da31e42d32e51a752f667a059735
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:44 2021 +0200

    Documentation: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Update the documentation accordingly.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-2-bigeasy@linutronix.de

commit ffec09f9c7d7b21b0aff29dd5c3972f4631c0b6b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:54 2021 +0200

    perf/hw_breakpoint: Replace deprecated CPU-hotplug functions
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20210803141621.780504-12-bigeasy@linutronix.de

commit eda8a2c599d1ff874a63de7684b430740e747dea
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:53 2021 +0200

    perf/x86/intel: Replace deprecated CPU-hotplug functions
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20210803141621.780504-11-bigeasy@linutronix.de

commit 1daf08a066cfe500587affd3fa3be8c13b8ff007
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:09 2021 +0200

    livepatch: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Joe Lawrence <joe.lawrence@redhat.com>
    Cc: live-patching@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Petr Mladek <pmladek@suse.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

commit 5353dd72f99207e8118a766847df8d60bb559940
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Aug 18 13:40:22 2021 -0600

    coresight: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Link: https://lore.kernel.org/r/20210803141621.780504-15-bigeasy@linutronix.de
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Mike Leach <mike.leach@linaro.org>
    Cc: Leo Yan <leo.yan@linaro.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: coresight@lists.linaro.org
    Cc: linux-arm-kernel@lists.infradead.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Link: https://lore.kernel.org/r/20210818194022.379573-12-mathieu.poirier@linaro.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e104d530f3734c7666edf86bbf1b83b1bfafe6ee
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:56 2021 +0200

    hwmon: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: Jean Delvare <jdelvare@suse.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: linux-hwmon@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-14-bigeasy@linutronix.de
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>

commit 99c37d1a63eafcd3673302a7953df760b46d0f6f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:19 2021 +0200

    tracing: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Link: https://lkml.kernel.org/r/20210803141621.780504-37-bigeasy@linutronix.de
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Acked-by: Daniel Bristot de Oliveira <bristot@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

commit ecf93431963a95c0f475921101bedc0dd62ec96d
Merge: c4f14eac2246 cbc06f051c52
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Aug 15 06:57:43 2021 -1000

    Merge tag 'powerpc-5.14-5' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc fixes from Michael Ellerman:
    
     - Fix crashes coming out of nap on 32-bit Book3s (eg. powerbooks).
    
     - Fix critical and debug interrupts on BookE, seen as crashes when
       using ptrace.
    
     - Fix an oops when running an SMP kernel on a UP system.
    
     - Update pseries LPAR security flavor after partition migration.
    
     - Fix an oops when using kprobes on BookE.
    
     - Fix oops on 32-bit pmac by not calling do_IRQ() from
       timer_interrupt().
    
     - Fix softlockups on CPU hotplug into a CPU-less node with xive (P9).
    
    Thanks to Cdric Le Goater, Christophe Leroy, Finn Thain, Geetika
    Moolchandani, Laurent Dufour, Laurent Vivier, Nicholas Piggin, Pu Lehui,
    Radu Rendec, Srikar Dronamraju, and Stan Johnson.
    
    * tag 'powerpc-5.14-5' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux:
      powerpc/xive: Do not skip CPU-less nodes when creating the IPIs
      powerpc/interrupt: Do not call single_step_exception() from other exceptions
      powerpc/interrupt: Fix OOPS by not calling do_IRQ() from timer_interrupt()
      powerpc/kprobes: Fix kprobe Oops happens in booke
      powerpc/pseries: Fix update of LPAR security flavor after LPM
      powerpc/smp: Fix OOPS in topology_init()
      powerpc/32: Fix critical and debug interrupts on BOOKE
      powerpc/32s: Fix napping restore in data storage interrupt (DSI)

commit d31eb7c1a2288f61df75558f59328be01a264300
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:02 2021 +0200

    thermal/drivers/intel_powerclamp: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: Zhang Rui <rui.zhang@intel.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Amit Kucheria <amitk@kernel.org>
    Cc: linux-pm@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Link: https://lore.kernel.org/r/20210803141621.780504-20-bigeasy@linutronix.de

commit 80771c8228029daff4b3402e00883cde06e07d46
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:10 2021 +0200

    padata: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: linux-crypto@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit d01a9f7009c3812a8955b7ae5798470cd6ab3590
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:55 2021 +0200

    crypto: virtio - Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: Gonglei <arei.gonglei@huawei.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Jason Wang <jasowang@redhat.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: virtualization@lists.linux-foundation.org
    Cc: linux-crypto@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 560c71d4250f5f32b7952c290ddaf3cd4548a3ec
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:00 2021 +0200

    platform/x86: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: Stuart Hayes <stuart.w.hayes@gmail.com>
    Cc: Hans de Goede <hdegoede@redhat.com>
    Cc: Mark Gross <mgross@linux.intel.com>
    Cc: platform-driver-x86@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-18-bigeasy@linutronix.de
    Signed-off-by: Hans de Goede <hdegoede@redhat.com>

commit ed4fa2442e87bf9143d608473df117589e4bfc70
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:18 2021 +0200

    torture: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: "Paul E. McKenney" <paulmck@kernel.org>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit d3dd95a8853f1d588e38e9d9d7c8cc2da412cc36
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:14 2021 +0200

    rcu: Replace deprecated CPU-hotplug functions
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: "Paul E. McKenney" <paulmck@kernel.org>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Joel Fernandes <joel@joelfernandes.org>
    Cc: rcu@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit ebca71a8c96f0af2ba482489ecc64d88979cd825
Author: Dongli Zhang <dongli.zhang@oracle.com>
Date:   Thu Apr 8 22:53:16 2021 -0700

    cpu/hotplug: Add debug printks for hotplug callback failures
    
    CPU hotplug callbacks can fail and cause a rollback to the previous
    state. These failures are silent and therefore hard to debug.
    
    Add pr_debug() to the up and down paths which provide information about the
    error code, the CPU and the failed state. The debug printks can be enabled
    via kernel command line or sysfs.
    
    [ tglx: Adopt to current mainline, massage printk and changelog ]
    
    Signed-off-by: Dongli Zhang <dongli.zhang@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Qais Yousef <qais.yousef@arm.com>
    Link: https://lore.kernel.org/r/20210409055316.1709-1-dongli.zhang@oracle.com

commit 5ae36401ca4ea2737d779ce7c267444b16530001
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:46 2021 +0200

    powerpc: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20210803141621.780504-4-bigeasy@linutronix.de

commit 844d87871b6e0ac3ceb177535dcdf6e6a9f1fd4b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:16 2021 +0200

    smpboot: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-34-bigeasy@linutronix.de

commit 698429f9d0e54ce3964151adff886ee5fc59714b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:17 2021 +0200

    clocksource: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-35-bigeasy@linutronix.de

commit 746f5ea9c4283d98353c1cd41864aec475e0edbd
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:15 2021 +0200

    sched: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-33-bigeasy@linutronix.de

commit 428e211641ed808b55cdc7d880a0ee349eff354b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:08 2021 +0200

    genirq/affinity: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-26-bigeasy@linutronix.de

commit 8ae9e3f63865bc067c144817da9df025dbb667f2
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:52 2021 +0200

    x86/mce/inject: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-10-bigeasy@linutronix.de

commit 2089f34f8c5b91f7235023ec72e71e3247261ecc
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:51 2021 +0200

    x86/microcode: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-9-bigeasy@linutronix.de

commit 1a351eefd4acc97145903b1c07e4d8b626854b82
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:50 2021 +0200

    x86/mtrr: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-8-bigeasy@linutronix.de

commit 77ad320cfb2ac172eeba32a77a388281b003ec17
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:49 2021 +0200

    x86/mmiotrace: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Karol Herbst <kherbst@redhat.com>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Link: https://lore.kernel.org/r/20210803141621.780504-7-bigeasy@linutronix.de

commit c5c63b9a6a2e53757b598485b8adbafa56d6d9ee
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:07 2021 +0200

    cgroup: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: Zefan Li <lizefan.x@bytedance.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: cgroups@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 6ba34d3c73674e46d9e126e4f0cee79e5ef2481c
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jul 20 10:18:28 2021 -0400

    cgroup/cpuset: Fix violation of cpuset locking rule
    
    The cpuset fields that manage partition root state do not strictly
    follow the cpuset locking rule that update to cpuset has to be done
    with both the callback_lock and cpuset_mutex held. This is now fixed
    by making sure that the locking rule is upheld.
    
    Fixes: 3881b86128d0 ("cpuset: Add an error state to cpuset.sched.partition")
    Fixes: 4b842da276a8 ("cpuset: Make CPU hotplug work with partition")
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit ffd8bea81fbb5abe6518bce8d6297a149b935cf7
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:20 2021 +0200

    workqueue: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <jiangshanlai@gmail.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 8c39ed4876d4e541e2044f313c56b1eb20810fe1
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Aug 9 10:30:50 2021 +0200

    net/iucv: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: Julian Wiedmann <jwi@linux.ibm.com>
    Cc: Karsten Graul <kgraul@linux.ibm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Jakub Kicinski <kuba@kernel.org>
    Cc: linux-s390@vger.kernel.org
    Cc: netdev@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Julian Wiedmann <jwi@linux.ibm.com>
    Signed-off-by: Karsten Graul <kgraul@linux.ibm.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 2b1207801c393a5e9af2fbac2dd8b0377d8ae63a
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Tue Dec 3 14:31:10 2019 -0500

    padata: validate cpumask without removed CPU during offline
    
    commit 894c9ef9780c5cf2f143415e867ee39a33ecb75d upstream.
    
    Configuring an instance's parallel mask without any online CPUs...
    
      echo 2 > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
      echo 0 > /sys/devices/system/cpu/cpu1/online
    
    ...makes tcrypt mode=215 crash like this:
    
      divide error: 0000 [#1] SMP PTI
      CPU: 4 PID: 283 Comm: modprobe Not tainted 5.4.0-rc8-padata-doc-v2+ #2
      Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS ?-20191013_105130-anatol 04/01/2014
      RIP: 0010:padata_do_parallel+0x114/0x300
      Call Trace:
       pcrypt_aead_encrypt+0xc0/0xd0 [pcrypt]
       crypto_aead_encrypt+0x1f/0x30
       do_mult_aead_op+0x4e/0xdf [tcrypt]
       test_mb_aead_speed.constprop.0.cold+0x226/0x564 [tcrypt]
       do_test+0x28c2/0x4d49 [tcrypt]
       tcrypt_mod_init+0x55/0x1000 [tcrypt]
       ...
    
    cpumask_weight() in padata_cpu_hash() returns 0 because the mask has no
    CPUs.  The problem is __padata_remove_cpu() checks for valid masks too
    early and so doesn't mark the instance PADATA_INVALID as expected, which
    would have made padata_do_parallel() return error before doing the
    division.
    
    Fix by introducing a second padata CPU hotplug state before
    CPUHP_BRINGUP_CPU so that __padata_remove_cpu() sees the online mask
    without @cpu.  No need for the second argument to padata_replace() since
    @cpu is now already missing from the online mask.
    
    Fixes: 33e54450683c ("padata: Handle empty padata cpumasks")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Eric Biggers <ebiggers@kernel.org>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Yang Yingliang <yangyingliang@huawei.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f1653c2e2831e9db6cd68473bbec581782df03a5
Author: Dave Chinner <dchinner@redhat.com>
Date:   Fri Aug 6 11:05:37 2021 -0700

    xfs: introduce CPU hotplug infrastructure
    
    We need to move to per-cpu state for both deferred inode
    inactivation and CIL tracking, but to do that we
    need to handle CPUs being removed from the system by the hot-plug
    code. Introduce generic XFS infrastructure to handle CPU hotplug
    events that is set up at module init time and torn down at module
    exit time.
    
    Initially, we only need CPU dead notifications, so we only set
    up a callback for these notifications. The infrastructure can be
    updated in future for other CPU hotplug state machine notifications
    easily if ever needed.
    
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    [djwong: rearrange some macros, fix function prototypes]
    Reviewed-by: Darrick J. Wong <djwong@kernel.org>
    Signed-off-by: Darrick J. Wong <djwong@kernel.org>

commit 52b6defae7de31aaa960e78e506f882c12b4af53
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:48 2021 +0200

    s390/sclp: replace deprecated CPU-hotplug functions
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-6-bigeasy@linutronix.de
    Signed-off-by: Heiko Carstens <hca@linux.ibm.com>

commit a73de29320287d0e72b9e158879cb047e226ec2b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:47 2021 +0200

    s390: replace deprecated CPU-hotplug functions
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-5-bigeasy@linutronix.de
    Signed-off-by: Heiko Carstens <hca@linux.ibm.com>

commit 730d070ae9f12fff5d44fe8fb0547ae37d100da8
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:45 2021 +0200

    MIPS: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: linux-mips@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Bogendoerfer <tsbogend@alpha.franken.de>

commit 372bbdd5bb3fc454d9c280dc0914486a3c7419d5
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:06 2021 +0200

    net: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>

commit a0d1d0f47e3193d6188869ae6bcf08a792f63cf6
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:04 2021 +0200

    virtio_net: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Jason Wang <jasowang@redhat.com>
    Cc: virtualization@lists.linux-foundation.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Jakub Kicinski <kuba@kernel.org>

commit 95ac706744de78a93a7ec98d603c35fb21de8400
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:12 2021 +0200

    ACPI: processor: Replace deprecated CPU-hotplug functions
    
    The functions cpu_hotplug_begin, cpu_hotplug_done, get_online_cpus() and
    put_online_cpus() have been deprecated during the CPU hotplug rework. They map
    directly to cpus_write_lock(), cpus_write_unlock, cpus_read_lock() and
    cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit d2c8cce647f3022d5960a3bf2b50a2da341d9c8b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:13 2021 +0200

    PM: sleep: s2idle: Replace deprecated CPU-hotplug functions
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 09681a0772f773dddffd3c2d1796c87bd0d903b9
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:11 2021 +0200

    cpufreq: Replace deprecated CPU-hotplug functions
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 5d4c779cb62e676aedc278de910b4bb8ef65a5cc
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:16:01 2021 +0200

    powercap: intel_rapl: Replace deprecated CPU-hotplug functions
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit e67adaa1754d5383583c35a703518507e457482b
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 3 16:15:59 2021 +0200

    sgi-xpc: Replace deprecated CPU-hotplug functions.
    
    The functions get_online_cpus() and put_online_cpus() have been
    deprecated during the CPU hotplug rework. They map directly to
    cpus_read_lock() and cpus_read_unlock().
    
    Replace deprecated CPU-hotplug functions with the official version.
    The behavior remains unchanged.
    
    Cc: Robin Holt <robinmholt@gmail.com>
    Cc: Steve Wahl <steve.wahl@hpe.com>
    Cc: Mike Travis <mike.travis@hpe.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Link: https://lore.kernel.org/r/20210803141621.780504-17-bigeasy@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 15d428e6fe77fffc3f4fff923336036f5496ef17
Author: Waiman Long <longman@redhat.com>
Date:   Tue Jul 20 10:18:27 2021 -0400

    cgroup/cpuset: Fix a partition bug with hotplug
    
    In cpuset_hotplug_workfn(), the detection of whether the cpu list
    has been changed is done by comparing the effective cpus of the top
    cpuset with the cpu_active_mask. However, in the rare case that just
    all the CPUs in the subparts_cpus are offlined, the detection fails
    and the partition states are not updated correctly. Fix it by forcing
    the cpus_updated flag to true in this particular case.
    
    Fixes: 4b842da276a8 ("cpuset: Make CPU hotplug work with partition")
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit d5caec394a78617cbc0eea0870da22aa019c346d
Author: Yang Xu <xuyang2018.jy@fujitsu.com>
Date:   Wed Jun 23 09:37:48 2021 +0800

    admin-guide/cputopology.rst: Remove non-existed cpu-hotplug.txt
    
    Since kernel commit ff58fa7f556c ("Documentation: Update CPU hotplug and move it to core-api"),
    cpu_hotplug.txt has been removed. We should update it in here.
    
    Signed-off-by: Yang Xu <xuyang2018.jy@fujitsu.com>
    Link: https://lore.kernel.org/r/1624412269-13155-1-git-send-email-xuyang2018.jy@fujitsu.com
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>

commit 952da0c9ab5b047665442dc239cee36d5c9edb98
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Wed Jun 23 11:31:14 2021 +0530

    cpufreq: powernv: Migrate to ->exit() callback instead of ->stop_cpu()
    
    Commit 367dc4aa932b ("cpufreq: Add stop CPU callback to cpufreq_driver
    interface") added the ->stop_cpu() callback to allow the drivers to do
    clean up before the CPU is completely down and its state can't be
    modified.
    
    At that time the CPU hotplug framework used to call the cpufreq core's
    registered notifier for different events like CPU_DOWN_PREPARE and
    CPU_POST_DEAD. The ->stop_cpu() callback was called during the
    CPU_DOWN_PREPARE event.
    
    This is no longer the case, cpuhp_cpufreq_offline() is called only
    once by the CPU hotplug core now and we don't really need two
    separate callbacks for cpufreq drivers, i.e. ->stop_cpu() and
    ->exit(), as everything can be done from the ->exit() callback
    itself.
    
    Migrate to using the ->exit() callback instead of ->stop_cpu().
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    [ rjw: Minor changelog edits ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 9357a380f90a89a168d505561d11f68272e0e768
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Wed Jun 23 09:54:39 2021 +0530

    cpufreq: CPPC: Migrate to ->exit() callback instead of ->stop_cpu()
    
    Commit 367dc4aa932b ("cpufreq: Add stop CPU callback to cpufreq_driver
    interface") added the ->stop_cpu() callback to allow the drivers to do
    clean up before the CPU is completely down and its state can't be
    modified.
    
    At that time the CPU hotplug framework used to call the cpufreq core's
    registered notifier for different events like CPU_DOWN_PREPARE and
    CPU_POST_DEAD. The ->stop_cpu() callback was called during the
    CPU_DOWN_PREPARE event.
    
    This is no longer the case, cpuhp_cpufreq_offline() is called only
    once by the CPU hotplug core now and we don't really need two
    separate callbacks for cpufreq drivers, i.e. ->stop_cpu() and
    -<exit(), as everything can be done from the ->exit() callback
    itself.
    
    Migrate to using the ->exit() callback instead of ->stop_cpu().
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    [ rjw: Minor edits in the changelog and subject ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 62180152e0944e815ebbfd0ffd822d2b0e2cd8e7
Merge: 371fb85457c8 b22afcdf04c9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 29 12:23:02 2021 -0700

    Merge tag 'smp-urgent-2021-06-29' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug fix from Thomas Gleixner:
     "A fix for the CPU hotplug and cpusets interaction:
    
      cpusets delegate the hotplug work to a workqueue to prevent a lock
      order inversion vs. the CPU hotplug lock. The work is not flushed
      before the hotplug operation returns which creates user visible
      inconsistent state. Prevent this by flushing the work after dropping
      CPU hotplug lock and before releasing the outer mutex which serializes
      the CPU hotplug related sysfs interface operations"
    
    * tag 'smp-urgent-2021-06-29' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Cure the cpusets trainwreck

commit 371fb85457c857eeac1611d3661ee8e637f6548c
Merge: e563592c3e42 130708331bc6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 29 12:21:21 2021 -0700

    Merge tag 'smp-core-2021-06-29' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug cleanup from Thomas Gleixner:
     "A simple cleanup for the CPU hotplug code to avoid per_cpu_ptr()
      reevaluation"
    
    * tag 'smp-core-2021-06-29' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Simplify access to percpu cpuhp_state

commit 04f8cfeaed0849e702278378bce3867577ca45fb
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Mon Jun 28 19:42:15 2021 -0700

    mm/page_alloc: adjust pcp->high after CPU hotplug events
    
    The PCP high watermark is based on the number of online CPUs so the
    watermarks must be adjusted during CPU hotplug.  At the time of
    hot-remove, the number of online CPUs is already adjusted but during
    hot-add, a delta needs to be applied to update PCP to the correct value.
    After this patch is applied, the high watermarks are adjusted correctly.
    
      # grep high: /proc/zoneinfo  | tail -1
                  high:  649
      # echo 0 > /sys/devices/system/cpu/cpu4/online
      # grep high: /proc/zoneinfo  | tail -1
                  high:  664
      # echo 1 > /sys/devices/system/cpu/cpu4/online
      # grep high: /proc/zoneinfo  | tail -1
                  high:  649
    
    Link: https://lkml.kernel.org/r/20210525080119.5455-4-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b92ca18e8ca596f4f3d80c1fe833bc57a1b2458c
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Mon Jun 28 19:42:12 2021 -0700

    mm/page_alloc: disassociate the pcp->high from pcp->batch
    
    The pcp high watermark is based on the batch size but there is no
    relationship between them other than it is convenient to use early in
    boot.
    
    This patch takes the first step and bases pcp->high on the zone low
    watermark split across the number of CPUs local to a zone while the batch
    size remains the same to avoid increasing allocation latencies.  The
    intent behind the default pcp->high is "set the number of PCP pages such
    that if they are all full that background reclaim is not started
    prematurely".
    
    Note that in this patch the pcp->high values are adjusted after memory
    hotplug events, min_free_kbytes adjustments and watermark scale factor
    adjustments but not CPU hotplug events which is handled later in the
    series.
    
    On a test KVM instance;
    
    Before grep -E "high:|batch" /proc/zoneinfo | tail -2
                  high:  378
                  batch: 63
    
    After grep -E "high:|batch" /proc/zoneinfo | tail -2
                  high:  649
                  batch: 63
    
    [mgorman@techsingularity.net:  fix __setup_per_zone_wmarks for parallel memory
    hotplug]
      Link: https://lkml.kernel.org/r/20210528105925.GN30378@techsingularity.net
    
    Link: https://lkml.kernel.org/r/20210525080119.5455-3-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit c8895e271f7994a3ecb13b8a280e39aa53879545
Author: Daniel Bristot de Oliveira <bristot@redhat.com>
Date:   Tue Jun 22 16:42:32 2021 +0200

    trace/osnoise: Support hotplug operations
    
    Enable and disable osnoise/timerlat thread during on CPU hotplug online
    and offline operations respectivelly.
    
    Link: https://lore.kernel.org/linux-doc/20210621134636.5b332226@oasis.local.home/
    Link: https://lkml.kernel.org/r/39f98590b3caeb3c32f09526214058efe0e9272a.1624372313.git.bristot@redhat.com
    
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Kate Carcia <kcarcia@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Alexandre Chartre <alexandre.chartre@oracle.com>
    Cc: Clark Willaims <williams@redhat.com>
    Cc: John Kacur <jkacur@redhat.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: x86@kernel.org
    Cc: linux-doc@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Suggested-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

commit 4e8e4313cf81add679e1c57677d689c02e382a67
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 23 14:01:31 2021 +0200

    x86/fpu: Make xfeatures_mask_all __ro_after_init
    
    Nothing has to modify this after init.
    
    But of course there is code which unconditionally masks
    xfeatures_mask_all on CPU hotplug. This goes unnoticed during boot
    hotplug because at that point the variable is still RW mapped.
    
    This is broken in several ways:
    
      1) Masking this in post init CPU hotplug means that any
         modification of this state goes unnoticed until actual hotplug
         happens.
    
      2) If that ever happens then these bogus feature bits are already
         populated all over the place and the system is in inconsistent state
         vs. the compacted XSTATE offsets. If at all then this has to panic the
         machine because the inconsistency cannot be undone anymore.
    
    Make this a one-time paranoia check in xstate init code and disable
    xsave when this happens.
    
    Reported-by: Kan Liang <kan.liang@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20210623121451.712803952@linutronix.de

commit 771fac5e26c17845de8c679e6a947a4371e86ffc
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Fri Jun 11 08:48:02 2021 +0530

    Revert "cpufreq: CPPC: Add support for frequency invariance"
    
    This reverts commit 4c38f2df71c8e33c0b64865992d693f5022eeaad.
    
    There are few races in the frequency invariance support for CPPC driver,
    namely the driver doesn't stop the kthread_work and irq_work on policy
    exit during suspend/resume or CPU hotplug.
    
    A proper fix won't be possible for the 5.13-rc, as it requires a lot of
    changes. Lets revert the patch instead for now.
    
    Fixes: 4c38f2df71c8 ("cpufreq: CPPC: Add support for frequency invariance")
    Reported-by: Qian Cai <quic_qiancai@quicinc.com>
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit e34645f45805d8308866de7b69f117f554605bb6
Author: Anson Huang <b20788@freescale.com>
Date:   Tue May 25 21:14:16 2021 -0300

    ARM: imx: add smp support for imx7d
    
    Add SMP support for i.MX7D, including CPU hotplug support, for
    systems where TFA is not present.
    
    The motivation for bringing up the second i.MX7D core inside the kernel
    is that legacy vendor bootloaders usually do not implement PSCI support.
    
    This is a significant blocker for systems in the field that are running old
    bootloader versions to upgrade to a modern mainline kernel version, as only
    one CPU of the i.MX7D would be brought up.
    
    Bring up the second i.MX7D core inside the kernel to make the migration
    path to mainline kernel easier for the existing iMX7D users.
    
    Signed-off-by: Anson Huang <b20788@freescale.com>
    Signed-off-by: Arulpandiyan Vadivel <arulpandiyan_vadivel@mentor.com> # Fix merge conflicts
    Signed-off-by: Leonard Crestez <leonard.crestez@nxp.com>
    Signed-off-by: Marek Vasut <marex@denx.de> # heavy cleanup
    Signed-off-by: Fabio Estevam <festevam@denx.de>
    Signed-off-by: Shawn Guo <shawnguo@kernel.org>

commit af6d4954f9d43def013d3621d944ea2770bbd08b
Author: Otavio Pontes <otavio.pontes@intel.com>
Date:   Fri Mar 19 09:55:15 2021 -0700

    x86/microcode: Check for offline CPUs before requesting new microcode
    
    [ Upstream commit 7189b3c11903667808029ec9766a6e96de5012a5 ]
    
    Currently, the late microcode loading mechanism checks whether any CPUs
    are offlined, and, in such a case, aborts the load attempt.
    
    However, this must be done before the kernel caches new microcode from
    the filesystem. Otherwise, when offlined CPUs are onlined later, those
    cores are going to be updated through the CPU hotplug notifier callback
    with the new microcode, while CPUs previously onine will continue to run
    with the older microcode.
    
    For example:
    
    Turn off one core (2 threads):
    
      echo 0 > /sys/devices/system/cpu/cpu3/online
      echo 0 > /sys/devices/system/cpu/cpu1/online
    
    Install the ucode fails because a primary SMT thread is offline:
    
      cp intel-ucode/06-8e-09 /lib/firmware/intel-ucode/
      echo 1 > /sys/devices/system/cpu/microcode/reload
      bash: echo: write error: Invalid argument
    
    Turn the core back on
    
      echo 1 > /sys/devices/system/cpu/cpu3/online
      echo 1 > /sys/devices/system/cpu/cpu1/online
      cat /proc/cpuinfo |grep microcode
      microcode : 0x30
      microcode : 0xde
      microcode : 0x30
      microcode : 0xde
    
    The rationale for why the update is aborted when at least one primary
    thread is offline is because even if that thread is soft-offlined
    and idle, it will still have to participate in broadcasted MCE's
    synchronization dance or enter SMM, and in both examples it will execute
    instructions so it better have the same microcode revision as the other
    cores.
    
     [ bp: Heavily edit and extend commit message with the reasoning behind all
       this. ]
    
    Fixes: 30ec26da9967 ("x86/microcode: Do not upload microcode if CPUs are offline")
    Signed-off-by: Otavio Pontes <otavio.pontes@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Ashok Raj <ashok.raj@intel.com>
    Link: https://lkml.kernel.org/r/20210319165515.9240-2-otavio.pontes@intel.com
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 51474bd67074f7cf6712ea8f9bbf4882ee60645e
Author: Otavio Pontes <otavio.pontes@intel.com>
Date:   Fri Mar 19 09:55:15 2021 -0700

    x86/microcode: Check for offline CPUs before requesting new microcode
    
    [ Upstream commit 7189b3c11903667808029ec9766a6e96de5012a5 ]
    
    Currently, the late microcode loading mechanism checks whether any CPUs
    are offlined, and, in such a case, aborts the load attempt.
    
    However, this must be done before the kernel caches new microcode from
    the filesystem. Otherwise, when offlined CPUs are onlined later, those
    cores are going to be updated through the CPU hotplug notifier callback
    with the new microcode, while CPUs previously onine will continue to run
    with the older microcode.
    
    For example:
    
    Turn off one core (2 threads):
    
      echo 0 > /sys/devices/system/cpu/cpu3/online
      echo 0 > /sys/devices/system/cpu/cpu1/online
    
    Install the ucode fails because a primary SMT thread is offline:
    
      cp intel-ucode/06-8e-09 /lib/firmware/intel-ucode/
      echo 1 > /sys/devices/system/cpu/microcode/reload
      bash: echo: write error: Invalid argument
    
    Turn the core back on
    
      echo 1 > /sys/devices/system/cpu/cpu3/online
      echo 1 > /sys/devices/system/cpu/cpu1/online
      cat /proc/cpuinfo |grep microcode
      microcode : 0x30
      microcode : 0xde
      microcode : 0x30
      microcode : 0xde
    
    The rationale for why the update is aborted when at least one primary
    thread is offline is because even if that thread is soft-offlined
    and idle, it will still have to participate in broadcasted MCE's
    synchronization dance or enter SMM, and in both examples it will execute
    instructions so it better have the same microcode revision as the other
    cores.
    
     [ bp: Heavily edit and extend commit message with the reasoning behind all
       this. ]
    
    Fixes: 30ec26da9967 ("x86/microcode: Do not upload microcode if CPUs are offline")
    Signed-off-by: Otavio Pontes <otavio.pontes@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Ashok Raj <ashok.raj@intel.com>
    Link: https://lkml.kernel.org/r/20210319165515.9240-2-otavio.pontes@intel.com
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 5a3672a6edcbaf7fc3d79d0f1d518c72868da854
Author: Otavio Pontes <otavio.pontes@intel.com>
Date:   Fri Mar 19 09:55:15 2021 -0700

    x86/microcode: Check for offline CPUs before requesting new microcode
    
    [ Upstream commit 7189b3c11903667808029ec9766a6e96de5012a5 ]
    
    Currently, the late microcode loading mechanism checks whether any CPUs
    are offlined, and, in such a case, aborts the load attempt.
    
    However, this must be done before the kernel caches new microcode from
    the filesystem. Otherwise, when offlined CPUs are onlined later, those
    cores are going to be updated through the CPU hotplug notifier callback
    with the new microcode, while CPUs previously onine will continue to run
    with the older microcode.
    
    For example:
    
    Turn off one core (2 threads):
    
      echo 0 > /sys/devices/system/cpu/cpu3/online
      echo 0 > /sys/devices/system/cpu/cpu1/online
    
    Install the ucode fails because a primary SMT thread is offline:
    
      cp intel-ucode/06-8e-09 /lib/firmware/intel-ucode/
      echo 1 > /sys/devices/system/cpu/microcode/reload
      bash: echo: write error: Invalid argument
    
    Turn the core back on
    
      echo 1 > /sys/devices/system/cpu/cpu3/online
      echo 1 > /sys/devices/system/cpu/cpu1/online
      cat /proc/cpuinfo |grep microcode
      microcode : 0x30
      microcode : 0xde
      microcode : 0x30
      microcode : 0xde
    
    The rationale for why the update is aborted when at least one primary
    thread is offline is because even if that thread is soft-offlined
    and idle, it will still have to participate in broadcasted MCE's
    synchronization dance or enter SMM, and in both examples it will execute
    instructions so it better have the same microcode revision as the other
    cores.
    
     [ bp: Heavily edit and extend commit message with the reasoning behind all
       this. ]
    
    Fixes: 30ec26da9967 ("x86/microcode: Do not upload microcode if CPUs are offline")
    Signed-off-by: Otavio Pontes <otavio.pontes@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Ashok Raj <ashok.raj@intel.com>
    Link: https://lkml.kernel.org/r/20210319165515.9240-2-otavio.pontes@intel.com
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit b4f60d0c22383550e8ab28e659d2f5213f7072d3
Author: Otavio Pontes <otavio.pontes@intel.com>
Date:   Fri Mar 19 09:55:15 2021 -0700

    x86/microcode: Check for offline CPUs before requesting new microcode
    
    [ Upstream commit 7189b3c11903667808029ec9766a6e96de5012a5 ]
    
    Currently, the late microcode loading mechanism checks whether any CPUs
    are offlined, and, in such a case, aborts the load attempt.
    
    However, this must be done before the kernel caches new microcode from
    the filesystem. Otherwise, when offlined CPUs are onlined later, those
    cores are going to be updated through the CPU hotplug notifier callback
    with the new microcode, while CPUs previously onine will continue to run
    with the older microcode.
    
    For example:
    
    Turn off one core (2 threads):
    
      echo 0 > /sys/devices/system/cpu/cpu3/online
      echo 0 > /sys/devices/system/cpu/cpu1/online
    
    Install the ucode fails because a primary SMT thread is offline:
    
      cp intel-ucode/06-8e-09 /lib/firmware/intel-ucode/
      echo 1 > /sys/devices/system/cpu/microcode/reload
      bash: echo: write error: Invalid argument
    
    Turn the core back on
    
      echo 1 > /sys/devices/system/cpu/cpu3/online
      echo 1 > /sys/devices/system/cpu/cpu1/online
      cat /proc/cpuinfo |grep microcode
      microcode : 0x30
      microcode : 0xde
      microcode : 0x30
      microcode : 0xde
    
    The rationale for why the update is aborted when at least one primary
    thread is offline is because even if that thread is soft-offlined
    and idle, it will still have to participate in broadcasted MCE's
    synchronization dance or enter SMM, and in both examples it will execute
    instructions so it better have the same microcode revision as the other
    cores.
    
     [ bp: Heavily edit and extend commit message with the reasoning behind all
       this. ]
    
    Fixes: 30ec26da9967 ("x86/microcode: Do not upload microcode if CPUs are offline")
    Signed-off-by: Otavio Pontes <otavio.pontes@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Ashok Raj <ashok.raj@intel.com>
    Link: https://lkml.kernel.org/r/20210319165515.9240-2-otavio.pontes@intel.com
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit bac20313210a2f743dda97da1b7082522894aeff
Author: Otavio Pontes <otavio.pontes@intel.com>
Date:   Fri Mar 19 09:55:15 2021 -0700

    x86/microcode: Check for offline CPUs before requesting new microcode
    
    [ Upstream commit 7189b3c11903667808029ec9766a6e96de5012a5 ]
    
    Currently, the late microcode loading mechanism checks whether any CPUs
    are offlined, and, in such a case, aborts the load attempt.
    
    However, this must be done before the kernel caches new microcode from
    the filesystem. Otherwise, when offlined CPUs are onlined later, those
    cores are going to be updated through the CPU hotplug notifier callback
    with the new microcode, while CPUs previously onine will continue to run
    with the older microcode.
    
    For example:
    
    Turn off one core (2 threads):
    
      echo 0 > /sys/devices/system/cpu/cpu3/online
      echo 0 > /sys/devices/system/cpu/cpu1/online
    
    Install the ucode fails because a primary SMT thread is offline:
    
      cp intel-ucode/06-8e-09 /lib/firmware/intel-ucode/
      echo 1 > /sys/devices/system/cpu/microcode/reload
      bash: echo: write error: Invalid argument
    
    Turn the core back on
    
      echo 1 > /sys/devices/system/cpu/cpu3/online
      echo 1 > /sys/devices/system/cpu/cpu1/online
      cat /proc/cpuinfo |grep microcode
      microcode : 0x30
      microcode : 0xde
      microcode : 0x30
      microcode : 0xde
    
    The rationale for why the update is aborted when at least one primary
    thread is offline is because even if that thread is soft-offlined
    and idle, it will still have to participate in broadcasted MCE's
    synchronization dance or enter SMM, and in both examples it will execute
    instructions so it better have the same microcode revision as the other
    cores.
    
     [ bp: Heavily edit and extend commit message with the reasoning behind all
       this. ]
    
    Fixes: 30ec26da9967 ("x86/microcode: Do not upload microcode if CPUs are offline")
    Signed-off-by: Otavio Pontes <otavio.pontes@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Ashok Raj <ashok.raj@intel.com>
    Link: https://lkml.kernel.org/r/20210319165515.9240-2-otavio.pontes@intel.com
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 369428a646b7d43cf413f59e7e2ac5539fe98138
Author: Otavio Pontes <otavio.pontes@intel.com>
Date:   Fri Mar 19 09:55:15 2021 -0700

    x86/microcode: Check for offline CPUs before requesting new microcode
    
    [ Upstream commit 7189b3c11903667808029ec9766a6e96de5012a5 ]
    
    Currently, the late microcode loading mechanism checks whether any CPUs
    are offlined, and, in such a case, aborts the load attempt.
    
    However, this must be done before the kernel caches new microcode from
    the filesystem. Otherwise, when offlined CPUs are onlined later, those
    cores are going to be updated through the CPU hotplug notifier callback
    with the new microcode, while CPUs previously onine will continue to run
    with the older microcode.
    
    For example:
    
    Turn off one core (2 threads):
    
      echo 0 > /sys/devices/system/cpu/cpu3/online
      echo 0 > /sys/devices/system/cpu/cpu1/online
    
    Install the ucode fails because a primary SMT thread is offline:
    
      cp intel-ucode/06-8e-09 /lib/firmware/intel-ucode/
      echo 1 > /sys/devices/system/cpu/microcode/reload
      bash: echo: write error: Invalid argument
    
    Turn the core back on
    
      echo 1 > /sys/devices/system/cpu/cpu3/online
      echo 1 > /sys/devices/system/cpu/cpu1/online
      cat /proc/cpuinfo |grep microcode
      microcode : 0x30
      microcode : 0xde
      microcode : 0x30
      microcode : 0xde
    
    The rationale for why the update is aborted when at least one primary
    thread is offline is because even if that thread is soft-offlined
    and idle, it will still have to participate in broadcasted MCE's
    synchronization dance or enter SMM, and in both examples it will execute
    instructions so it better have the same microcode revision as the other
    cores.
    
     [ bp: Heavily edit and extend commit message with the reasoning behind all
       this. ]
    
    Fixes: 30ec26da9967 ("x86/microcode: Do not upload microcode if CPUs are offline")
    Signed-off-by: Otavio Pontes <otavio.pontes@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Ashok Raj <ashok.raj@intel.com>
    Link: https://lkml.kernel.org/r/20210319165515.9240-2-otavio.pontes@intel.com
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 179141865d08d9b9ebdbef8775b2450dc6f98a14
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Fri Mar 5 13:15:31 2021 -0800

    rcuscale: Allow CPU hotplug to be enabled
    
    It is no longer possible to disable CPU hotplug in many configurations,
    which means that the CONFIG_HOTPLUG_CPU=n lines in rcuscale's Kconfig
    options are just a source of useless diagnostics.  In addition, rcuscale
    doesn't do CPU-hotplug operations in any case.  This commit therefore
    changes these lines to read CONFIG_HOTPLUG_CPU=y.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit 68d415f91ff2284828211e937f12a3f6d9a18cb9
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Fri Mar 5 13:12:36 2021 -0800

    refscale: Allow CPU hotplug to be enabled
    
    It is no longer possible to disable CPU hotplug in many configurations,
    which means that the CONFIG_HOTPLUG_CPU=n lines in refscale's Kconfig
    options are just a source of useless diagnostics.  In addition, refscale
    doesn't do CPU-hotplug operations in any case.  This commit therefore
    changes these lines to read CONFIG_HOTPLUG_CPU=y.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit 16b3d0cf5bad844daaf436ad2e9061de0fe36e5c
Merge: 42dec9a936e7 2ea46c6fc945
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 28 13:33:57 2021 -0700

    Merge tag 'sched-core-2021-04-28' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - Clean up SCHED_DEBUG: move the decades old mess of sysctl, procfs and
       debugfs interfaces to a unified debugfs interface.
    
     - Signals: Allow caching one sigqueue object per task, to improve
       performance & latencies.
    
     - Improve newidle_balance() irq-off latencies on systems with a large
       number of CPU cgroups.
    
     - Improve energy-aware scheduling
    
     - Improve the PELT metrics for certain workloads
    
     - Reintroduce select_idle_smt() to improve load-balancing locality -
       but without the previous regressions
    
     - Add 'scheduler latency debugging': warn after long periods of pending
       need_resched. This is an opt-in feature that requires the enabling of
       the LATENCY_WARN scheduler feature, or the use of the
       resched_latency_warn_ms=xx boot parameter.
    
     - CPU hotplug fixes for HP-rollback, and for the 'fail' interface. Fix
       remaining balance_push() vs. hotplug holes/races
    
     - PSI fixes, plus allow /proc/pressure/ files to be written by
       CAP_SYS_RESOURCE tasks as well
    
     - Fix/improve various load-balancing corner cases vs. capacity margins
    
     - Fix sched topology on systems with NUMA diameter of 3 or above
    
     - Fix PF_KTHREAD vs to_kthread() race
    
     - Minor rseq optimizations
    
     - Misc cleanups, optimizations, fixes and smaller updates
    
    * tag 'sched-core-2021-04-28' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (61 commits)
      cpumask/hotplug: Fix cpu_dying() state tracking
      kthread: Fix PF_KTHREAD vs to_kthread() race
      sched/debug: Fix cgroup_path[] serialization
      sched,psi: Handle potential task count underflow bugs more gracefully
      sched: Warn on long periods of pending need_resched
      sched/fair: Move update_nohz_stats() to the CONFIG_NO_HZ_COMMON block to simplify the code & fix an unused function warning
      sched/debug: Rename the sched_debug parameter to sched_verbose
      sched,fair: Alternative sched_slice()
      sched: Move /proc/sched_debug to debugfs
      sched,debug: Convert sysctl sched_domains to debugfs
      debugfs: Implement debugfs_create_str()
      sched,preempt: Move preempt_dynamic to debug.c
      sched: Move SCHED_DEBUG sysctl to debugfs
      sched: Don't make LATENCYTOP select SCHED_DEBUG
      sched: Remove sched_schedstats sysctl out from under SCHED_DEBUG
      sched/numa: Allow runtime enabling/disabling of NUMA balance without SCHED_DEBUG
      sched: Use cpu_dying() to fix balance_push vs hotplug-rollback
      cpumask: Introduce DYING mask
      cpumask: Make cpu_{online,possible,present,active}() inline
      rseq: Optimise rseq_get_rseq_cs() and clear_rseq_cs()
      ...

commit b5c4477366fb5e6a2f0f38742c33acd666c07698
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jan 21 16:09:32 2021 +0100

    sched: Use cpu_dying() to fix balance_push vs hotplug-rollback
    
    Use the new cpu_dying() state to simplify and fix the balance_push()
    vs CPU hotplug rollback state.
    
    Specifically, we currently rely on notifiers sched_cpu_dying() /
    sched_cpu_activate() to terminate balance_push, however if the
    cpu_down() fails when we're past sched_cpu_deactivate(), it should
    terminate balance_push at that point and not wait until we hit
    sched_cpu_activate().
    
    Similarly, when cpu_up() fails and we're going back down, balance_push
    should be active, where it currently is not.
    
    So instead, make sure balance_push is enabled below SCHED_AP_ACTIVE
    (when !cpu_active()), and gate it's utility with cpu_dying().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
    Link: https://lkml.kernel.org/r/YHgAYef83VQhKdC2@hirez.programming.kicks-ass.net

commit e40f74c535b8a0ecf3ef0388b51a34cdadb34fb5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jan 19 18:43:45 2021 +0100

    cpumask: Introduce DYING mask
    
    Introduce a cpumask that indicates (for each CPU) what direction the
    CPU hotplug is currently going. Notably, it tracks rollbacks. Eg. when
    an up fails and we do a roll-back down, it will accurately reflect the
    direction.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
    Link: https://lkml.kernel.org/r/20210310150109.151441252@infradead.org

commit f598a497bc7dfbec60270bca8b8408db3d23ac07
Author: John Garry <john.garry@huawei.com>
Date:   Thu Mar 25 20:29:58 2021 +0800

    iova: Add CPU hotplug handler to flush rcaches
    
    Like the Intel IOMMU driver already does, flush the per-IOVA domain
    CPU rcache when a CPU goes offline - there's no point in keeping it.
    
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: John Garry <john.garry@huawei.com>
    Link: https://lore.kernel.org/r/1616675401-151997-2-git-send-email-john.garry@huawei.com
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

commit 7189b3c11903667808029ec9766a6e96de5012a5
Author: Otavio Pontes <otavio.pontes@intel.com>
Date:   Fri Mar 19 09:55:15 2021 -0700

    x86/microcode: Check for offline CPUs before requesting new microcode
    
    Currently, the late microcode loading mechanism checks whether any CPUs
    are offlined, and, in such a case, aborts the load attempt.
    
    However, this must be done before the kernel caches new microcode from
    the filesystem. Otherwise, when offlined CPUs are onlined later, those
    cores are going to be updated through the CPU hotplug notifier callback
    with the new microcode, while CPUs previously onine will continue to run
    with the older microcode.
    
    For example:
    
    Turn off one core (2 threads):
    
      echo 0 > /sys/devices/system/cpu/cpu3/online
      echo 0 > /sys/devices/system/cpu/cpu1/online
    
    Install the ucode fails because a primary SMT thread is offline:
    
      cp intel-ucode/06-8e-09 /lib/firmware/intel-ucode/
      echo 1 > /sys/devices/system/cpu/microcode/reload
      bash: echo: write error: Invalid argument
    
    Turn the core back on
    
      echo 1 > /sys/devices/system/cpu/cpu3/online
      echo 1 > /sys/devices/system/cpu/cpu1/online
      cat /proc/cpuinfo |grep microcode
      microcode : 0x30
      microcode : 0xde
      microcode : 0x30
      microcode : 0xde
    
    The rationale for why the update is aborted when at least one primary
    thread is offline is because even if that thread is soft-offlined
    and idle, it will still have to participate in broadcasted MCE's
    synchronization dance or enter SMM, and in both examples it will execute
    instructions so it better have the same microcode revision as the other
    cores.
    
     [ bp: Heavily edit and extend commit message with the reasoning behind all
       this. ]
    
    Fixes: 30ec26da9967 ("x86/microcode: Do not upload microcode if CPUs are offline")
    Signed-off-by: Otavio Pontes <otavio.pontes@intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Ashok Raj <ashok.raj@intel.com>
    Link: https://lkml.kernel.org/r/20210319165515.9240-2-otavio.pontes@intel.com

commit 2db4215f47557703dade2baccfa8da7b7e42a7e4
Author: Johannes Thumshirn <johannes.thumshirn@wdc.com>
Date:   Wed Mar 10 18:48:06 2021 +0900

    scsi: sd_zbc: Update write pointer offset cache
    
    Recent changes changed the completion of SCSI commands from Soft-IRQ
    context to IRQ context. This triggers the following warning, when we're
    completing writes to zoned block devices that go through the zone append
    emulation:
    
     CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.12.0-rc2+ #2
     Hardware name: Supermicro Super Server/X10SRL-F, BIOS 2.0 12/17/2015
     RIP: 0010:__local_bh_disable_ip+0x3f/0x50
     RSP: 0018:ffff8883e1409ba8 EFLAGS: 00010006
     RAX: 0000000080010001 RBX: 0000000000000001 RCX: 0000000000000013
     RDX: ffff888129e4d200 RSI: 0000000000000201 RDI: ffffffff915b9dbd
     RBP: ffff888113e9a540 R08: ffff888113e9a540 R09: 00000000000077f0
     R10: 0000000000080000 R11: 0000000000000001 R12: ffff888129e4d200
     R13: 0000000000001000 R14: 00000000000077f0 R15: ffff888129e4d218
     FS:  0000000000000000(0000) GS:ffff8883e1400000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     CR2: 00007f2f8418ebc0 CR3: 000000021202a006 CR4: 00000000001706f0
     Call Trace:
      <IRQ>
      _raw_spin_lock_bh+0x18/0x40
      sd_zbc_complete+0x43d/0x1150
      sd_done+0x631/0x1040
      ? mark_lock+0xe4/0x2fd0
      ? provisioning_mode_store+0x3f0/0x3f0
      scsi_finish_command+0x31b/0x5c0
      _scsih_io_done+0x960/0x29e0 [mpt3sas]
      ? mpt3sas_scsih_scsi_lookup_get+0x1c7/0x340 [mpt3sas]
      ? __lock_acquire+0x166b/0x58b0
      ? _get_st_from_smid+0x4a/0x80 [mpt3sas]
      _base_process_reply_queue+0x23f/0x26e0 [mpt3sas]
      ? lock_is_held_type+0x98/0x110
      ? find_held_lock+0x2c/0x110
      ? mpt3sas_base_sync_reply_irqs+0x360/0x360 [mpt3sas]
      _base_interrupt+0x8d/0xd0 [mpt3sas]
      ? rcu_read_lock_sched_held+0x3f/0x70
      __handle_irq_event_percpu+0x24d/0x600
      handle_irq_event+0xef/0x240
      ? handle_irq_event_percpu+0x110/0x110
      handle_edge_irq+0x1f6/0xb60
      __common_interrupt+0x75/0x160
      common_interrupt+0x7b/0xa0
      </IRQ>
      asm_common_interrupt+0x1e/0x40
    
    Don't use spin_lock_bh() to protect the update of the write pointer offset
    cache, but use spin_lock_irqsave() for it.
    
    Link: https://lore.kernel.org/r/3cfebe48d09db73041b7849be71ffbcec7ee40b3.1615369586.git.johannes.thumshirn@wdc.com
    Fixes: 664f0dce2058 ("scsi: mpt3sas: Add support for shared host tagset for CPU hotplug")
    Reported-by: Shinichiro Kawasaki <shinichiro.kawasaki@wdc.com>
    Tested-by: Shin'ichiro Kawasaki <shinichiro.kawasaki@wdc.com>
    Reviewed-by: Damien Le Moal <damien.lemoal@wdc.com>
    Signed-off-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit 5e59fba573e64cffc3a7a3113fff2336d652f45a
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Fri Jan 15 13:30:38 2021 -0800

    rcutorture: Fix testing of RCU priority boosting
    
    Currently, rcutorture refuses to test RCU priority boosting in
    CONFIG_HOTPLUG_CPU=y kernels, which are the only kind normally built on
    x86 these days.  This commit therefore updates rcutorture's tests of RCU
    priority boosting to make them safe for CPU hotplug.  However, these tests
    will fail unless TIMER_SOFTIRQ runs at realtime priority, which does not
    happen in current mainline.  This commit therefore also refuses to test
    RCU priority boosting except in kernels built with CONFIG_PREEMPT_RT=y.
    
    While in the area, this commt adds some debug output at boost-fail time
    that helps diagnose the cause of the failure, for example, failing to
    run TIMER_SOFTIRQ at realtime priority.
    
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Scott Wood <swood@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit 7d19ea5e99731f5346b047003bbe65117a86d12d
Merge: 0b311e34d503 dd2d082b5760
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 28 12:01:23 2021 -0800

    Merge tag 'riscv-for-linus-5.12-mw1' of git://git.kernel.org/pub/scm/linux/kernel/git/riscv/linux
    
    Pull more RISC-V updates from Palmer Dabbelt:
     "A pair of patches that slipped through the cracks:
    
       - enable CPU hotplug in the defconfigs
    
       - some cleanups to setup_bootmem
    
      There's also a single fix for some randconfig build failures:
    
       - make NUMA depend on SMP"
    
    * tag 'riscv-for-linus-5.12-mw1' of git://git.kernel.org/pub/scm/linux/kernel/git/riscv/linux:
      riscv: Cleanup setup_bootmem()
      RISC-V: Enable CPU Hotplug in defconfigs
      RISC-V: Make NUMA depend on SMP

commit b122c7a32593190c8a82f5470e2788bed451f98f
Author: Anup Patel <anup.patel@wdc.com>
Date:   Tue Feb 9 11:16:20 2021 +0530

    RISC-V: Enable CPU Hotplug in defconfigs
    
    The CPU hotplug support has been tested on QEMU, Spike, and SiFive
    Unleashed so let's enable it by default in RV32 and RV64 defconfigs.
    
    Signed-off-by: Anup Patel <anup.patel@wdc.com>
    Signed-off-by: Palmer Dabbelt <palmerdabbelt@google.com>

commit 005d3bd9e332faa976320cfaa2ae0637c8e94c51
Merge: e0fbd25bb37e 08c2a406b974
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 23 14:59:46 2021 -0800

    Merge tag 'pm-5.12-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull more power management updates from Rafael Wysocki:
     "These are fixes and cleanups on top of the power management material
      for 5.12-rc1 merged previously.
    
      Specifics:
    
       - Address cpufreq regression introduced in 5.11 that causes CPU
         frequency reporting to be distorted on systems with CPPC that use
         acpi-cpufreq as the scaling driver (Rafael Wysocki).
    
       - Fix regression introduced during the 5.10 development cycle related
         to CPU hotplug and policy recreation in the qcom-cpufreq-hw driver
         (Shawn Guo).
    
       - Fix recent regression in the operating performance points (OPP)
         framework that may cause frequency updates to be skipped by mistake
         in some cases (Jonathan Marek).
    
       - Simplify schedutil governor code and remove a misleading comment
         from it (Yue Hu).
    
       - Fix kerneldoc comment typo in the cpufreq core (Yue Hu)"
    
    * tag 'pm-5.12-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm:
      cpufreq: Fix typo in kerneldoc comment
      cpufreq: schedutil: Remove update_lock comment from struct sugov_policy definition
      cpufreq: schedutil: Remove needless sg_policy parameter from ignore_dl_rate_limit()
      cpufreq: ACPI: Set cpuinfo.max_freq directly if max boost is known
      cpufreq: qcom-hw: drop devm_xxx() calls from init/exit hooks
      opp: Don't skip freq update for different frequency

commit d089f48fba28db14d0fe7753248f2575a9ddfc73
Merge: 3f6ec19f2d05 2b392cb11c0d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 21 12:04:41 2021 -0800

    Merge tag 'core-rcu-2021-02-17' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "These are the latest RCU updates for v5.12:
    
       - Documentation updates.
    
       - Miscellaneous fixes.
    
       - kfree_rcu() updates: Addition of mem_dump_obj() to provide
         allocator return addresses to more easily locate bugs. This has a
         couple of RCU-related commits, but is mostly MM. Was pulled in with
         akpm's agreement.
    
       - Per-callback-batch tracking of numbers of callbacks, which enables
         better debugging information and smarter reactions to large numbers
         of callbacks.
    
       - The first round of changes to allow CPUs to be runtime switched
         from and to callback-offloaded state.
    
       - CONFIG_PREEMPT_RT-related changes.
    
       - RCU CPU stall warning updates.
    
       - Addition of polling grace-period APIs for SRCU.
    
       - Torture-test and torture-test scripting updates, including a
         "torture everything" script that runs rcutorture, locktorture,
         scftorture, rcuscale, and refscale. Plus does an allmodconfig
         build.
    
       - nolibc fixes for the torture tests"
    
    * tag 'core-rcu-2021-02-17' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (130 commits)
      percpu_ref: Dump mem_dump_obj() info upon reference-count underflow
      rcu: Make call_rcu() print mem_dump_obj() info for double-freed callback
      mm: Make mem_obj_dump() vmalloc() dumps include start and length
      mm: Make mem_dump_obj() handle vmalloc() memory
      mm: Make mem_dump_obj() handle NULL and zero-sized pointers
      mm: Add mem_dump_obj() to print source of memory block
      tools/rcutorture: Fix position of -lgcc in mkinitrd.sh
      tools/nolibc: Fix position of -lgcc in the documented example
      tools/nolibc: Emit detailed error for missing alternate syscall number definitions
      tools/nolibc: Remove incorrect definitions of __ARCH_WANT_*
      tools/nolibc: Get timeval, timespec and timezone from linux/time.h
      tools/nolibc: Implement poll() based on ppoll()
      tools/nolibc: Implement fork() based on clone()
      tools/nolibc: Make getpgrp() fall back to getpgid(0)
      tools/nolibc: Make dup2() rely on dup3() when available
      tools/nolibc: Add the definition for dup()
      rcutorture: Add rcutree.use_softirq=0 to RUDE01 and TASKS01
      torture: Maintain torture-specific set of CPUs-online books
      torture: Clean up after torture-test CPU hotplugging
      rcutorture: Make object_debug also double call_rcu() heap object
      ...

commit 486c1525eba3b0d2ec8b7f621ebf213d7f552f88
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 5 23:28:29 2021 +0100

    Revert "lib: Restrict cpumask_local_spread to houskeeping CPUs"
    
    [ Upstream commit 2452483d9546de1c540f330469dc4042ff089731 ]
    
    This reverts commit 1abdfe706a579a702799fce465bceb9fb01d407c.
    
    This change is broken and not solving any problem it claims to solve.
    
    Robin reported that cpumask_local_spread() now returns any cpu out of
    cpu_possible_mask in case that NOHZ_FULL is disabled (runtime or compile
    time). It can also return any offline or not-present CPU in the
    housekeeping mask. Before that it was returning a CPU out of
    online_cpu_mask.
    
    While the function is racy against CPU hotplug if the caller does not
    protect against it, the actual use cases are not caring much about it as
    they use it mostly as hint for:
    
     - the user space affinity hint which is unused by the kernel
     - memory node selection which is just suboptimal
     - network queue affinity which might fail but is handled gracefully
    
    But the occasional fail vs. hotplug is very different from returning
    anything from possible_cpu_mask which can have a large amount of offline
    CPUs obviously.
    
    The changelog of the commit claims:
    
     "The current implementation of cpumask_local_spread() does not respect
      the isolated CPUs, i.e., even if a CPU has been isolated for Real-Time
      task, it will return it to the caller for pinning of its IRQ
      threads. Having these unwanted IRQ threads on an isolated CPU adds up
      to a latency overhead."
    
    The only correct part of this changelog is:
    
     "The current implementation of cpumask_local_spread() does not respect
      the isolated CPUs."
    
    Everything else is just disjunct from reality.
    
    Reported-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Nitesh Narayan Lal <nitesh@redhat.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: abelits@marvell.com
    Cc: davem@davemloft.net
    Link: https://lore.kernel.org/r/87y2g26tnt.fsf@nanos.tec.linutronix.de
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 664f0dce20580837c7fa136a03b3a9fc43034104
Author: Sreekanth Reddy <sreekanth.reddy@broadcom.com>
Date:   Tue Feb 2 15:28:32 2021 +0530

    scsi: mpt3sas: Add support for shared host tagset for CPU hotplug
    
    MPT Fusion adapters can steer completions to individual queues and we now
    have support for shared host-wide tags in the I/O stack. The addition of
    the host-wide tags allows us to enable multiqueue support for MPT Fusion
    adapters. Once host-wise tags are enabled, the CPU hotplug feature is also
    supported.
    
    Allow use of host-wide tags to be disabled through the "host_tagset_enable"
    module parameter. Once we do not have any major performance regressions
    using host-wide tags, we will drop the hand-crafted interrupt affinity
    settings.
    
    Performance is meeting expectations. About 3.1M IOPS using 24 Drive SSD on
    Aero controllers.
    
    Link: https://lore.kernel.org/r/20210202095832.23072-1-sreekanth.reddy@broadcom.com
    Reported-by: kernel test robot <lkp@intel.com>
    Signed-off-by: Sreekanth Reddy <sreekanth.reddy@broadcom.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit 2452483d9546de1c540f330469dc4042ff089731
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 5 23:28:29 2021 +0100

    Revert "lib: Restrict cpumask_local_spread to houskeeping CPUs"
    
    This reverts commit 1abdfe706a579a702799fce465bceb9fb01d407c.
    
    This change is broken and not solving any problem it claims to solve.
    
    Robin reported that cpumask_local_spread() now returns any cpu out of
    cpu_possible_mask in case that NOHZ_FULL is disabled (runtime or compile
    time). It can also return any offline or not-present CPU in the
    housekeeping mask. Before that it was returning a CPU out of
    online_cpu_mask.
    
    While the function is racy against CPU hotplug if the caller does not
    protect against it, the actual use cases are not caring much about it as
    they use it mostly as hint for:
    
     - the user space affinity hint which is unused by the kernel
     - memory node selection which is just suboptimal
     - network queue affinity which might fail but is handled gracefully
    
    But the occasional fail vs. hotplug is very different from returning
    anything from possible_cpu_mask which can have a large amount of offline
    CPUs obviously.
    
    The changelog of the commit claims:
    
     "The current implementation of cpumask_local_spread() does not respect
      the isolated CPUs, i.e., even if a CPU has been isolated for Real-Time
      task, it will return it to the caller for pinning of its IRQ
      threads. Having these unwanted IRQ threads on an isolated CPU adds up
      to a latency overhead."
    
    The only correct part of this changelog is:
    
     "The current implementation of cpumask_local_spread() does not respect
      the isolated CPUs."
    
    Everything else is just disjunct from reality.
    
    Reported-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Nitesh Narayan Lal <nitesh@redhat.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: abelits@marvell.com
    Cc: davem@davemloft.net
    Link: https://lore.kernel.org/r/87y2g26tnt.fsf@nanos.tec.linutronix.de

commit 24c56ee06c4d4b410ac1d248869c14e391d66b8c
Merge: 025929f46813 741ba80f6f9a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jan 24 10:09:20 2021 -0800

    Merge tag 'sched_urgent_for_v5.11_rc5' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Borislav Petkov:
    
     - Correct the marking of kthreads which are supposed to run on a
       specific, single CPU vs such which are affine to only one CPU, mark
       per-cpu workqueue threads as such and make sure that marking
       "survives" CPU hotplug. Fix CPU hotplug issues with such kthreads.
    
     - A fix to not push away tasks on CPUs coming online.
    
     - Have workqueue CPU hotplug code use cpu_possible_mask when breaking
       affinity on CPU offlining so that pending workers can finish on newly
       arrived onlined CPUs too.
    
     - Dump tasks which haven't vacated a CPU which is currently being
       unplugged.
    
     - Register a special scale invariance callback which gets called on
       resume from RAM to read out APERF/MPERF after resume and thus make
       the schedutil scaling governor more precise.
    
    * tag 'sched_urgent_for_v5.11_rc5' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Relax the set_cpus_allowed_ptr() semantics
      sched: Fix CPU hotplug / tighten is_per_cpu_kthread()
      sched: Prepare to use balance_push in ttwu()
      workqueue: Restrict affinity change to rescuer
      workqueue: Tag bound workers with KTHREAD_IS_PER_CPU
      kthread: Extract KTHREAD_IS_PER_CPU
      sched: Don't run cpu-online with balance_push() enabled
      workqueue: Use cpu_possible_mask instead of cpu_active_mask to break affinity
      sched/core: Print out straggler tasks in sched_cpu_dying()
      x86: PM: Register syscore_ops for scale invariance

commit 5ba2ffba13a1e24e7b153683e97300f9cc6f605a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jan 12 11:28:16 2021 +0100

    sched: Fix CPU hotplug / tighten is_per_cpu_kthread()
    
    Prior to commit 1cf12e08bc4d ("sched/hotplug: Consolidate task
    migration on CPU unplug") we'd leave any task on the dying CPU and
    break affinity and force them off at the very end.
    
    This scheme had to change in order to enable migrate_disable(). One
    cannot wait for migrate_disable() to complete while stuck in
    stop_machine(). Furthermore, since we need at the very least: idle,
    hotplug and stop threads at any point before stop_machine, we can't
    break affinity and/or push those away.
    
    Under the assumption that all per-cpu kthreads are sanely handled by
    CPU hotplug, the new code no long breaks affinity or migrates any of
    them (which then includes the critical ones above).
    
    However, there's an important difference between per-cpu kthreads and
    kthreads that happen to have a single CPU affinity which is lost. The
    latter class very much relies on the forced affinity breaking and
    migration semantics previously provided.
    
    Use the new kthread_is_per_cpu() infrastructure to tighten
    is_per_cpu_kthread() and fix the hot-unplug problems stemming from the
    change.
    
    Fixes: 1cf12e08bc4d ("sched/hotplug: Consolidate task migration on CPU unplug")
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
    Tested-by: Valentin Schneider <valentin.schneider@arm.com>
    Link: https://lkml.kernel.org/r/20210121103507.102416009@infradead.org

commit 0b962c8fe0e5c72a252b236814a6b6e9df799061
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Sat Dec 19 07:05:58 2020 -0800

    torture: Clean up after torture-test CPU hotplugging
    
    This commit puts all CPUs back online at the end of a torture test,
    and also unconditionally puts them online at the beginning of the test,
    rather than just in the case of built-in tests.  This allows torture tests
    to behave in a predictable manner, whether built-in or based on modules.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit 74a2921948ed8c0e7f079a98442ec3493168cc85
Author: John Garry <john.garry@huawei.com>
Date:   Wed Dec 2 18:36:57 2020 +0800

    scsi: hisi_sas: Expose HW queues for v2 hw
    
    As a performance enhancement, make the completion queue interrupts managed.
    
    In addition, in commit bf0beec0607d ("blk-mq: drain I/O when all CPUs in a
    hctx are offline"), CPU hotplug for MQ devices using managed interrupts is
    made safe. So expose HW queues to blk-mq to take advantage of this.
    
    Flag Scsi_host.host_tagset is also set to ensure that the HBA is not sent
    more commands than it can handle. However the driver still does not use
    request tag for IPTT as there are many HW bugs means that special rules
    apply for IPTT allocation.
    
    Link: https://lore.kernel.org/r/1606905417-183214-6-git-send-email-john.garry@huawei.com
    Signed-off-by: John Garry <john.garry@huawei.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit ebb2bdcef8a00d59b27d3532c423110559821e1d
Author: Petr Mladek <pmladek@suse.com>
Date:   Mon Dec 14 19:03:18 2020 -0800

    kthread_worker: document CPU hotplug handling
    
    The kthread worker API is simple.  In short, it allows to create, use, and
    destroy workers.  kthread_create_worker_on_cpu() just allows to bind a
    newly created worker to a given CPU.
    
    It is up to the API user how to handle CPU hotplug.  They have to decide
    how to handle pending work items, prevent queuing new ones, and restore
    the functionality when the CPU goes off and on.  There are few catches:
    
       + The CPU affinity gets lost when it is scheduled on an offline CPU.
    
       + The worker might not exist when the CPU was off when the user
         created the workers.
    
    A good practice is to implement two CPU hotplug callbacks and
    destroy/create the worker when CPU goes down/up.
    
    Mention this in the function description.
    
    [akpm@linux-foundation.org: grammar tweaks]
    
    Link: https://lore.kernel.org/r/20201028073031.4536-1-qiang.zhang@windriver.com
    Link: https://lkml.kernel.org/r/20201102101039.19227-1-pmladek@suse.com
    Reported-by: Zhang Qiang <Qiang.Zhang@windriver.com>
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 1791ebd131c46539b024c0f2ebf12b6c88a265b9
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Dec 14 21:56:16 2020 +1100

    powerpc: Inline setup_kup()
    
    setup_kup() is used by both 64-bit and 32-bit code. However on 64-bit
    it must not be __init, because it's used for CPU hotplug, whereas on
    32-bit it should be __init because it calls setup_kuap/kuep() which
    are __init.
    
    We worked around that problem in the past by marking it __ref, see
    commit 67d53f30e23e ("powerpc/mm: fix section mismatch for
    setup_kup()").
    
    Marking it __ref basically just omits it from section mismatch
    checking, which can lead to bugs, and in fact it did, see commit
    44b4c4450f8d ("powerpc/64s: Mark the kuap/kuep functions non __init")
    
    We can avoid all these problems by just making it static inline.
    Because all it does is call other functions, making it inline actually
    shrinks the 32-bit vmlinux by ~76 bytes.
    
    Make it __always_inline as pointed out by Christophe.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20201214123011.311024-1-mpe@ellerman.id.au

commit 41ea667227bad5c247d76e6605054e96e4d95f51
Author: Nathan Fontenot <nathan.fontenot@amd.com>
Date:   Thu Nov 12 19:26:12 2020 +0100

    x86, sched: Calculate frequency invariance for AMD systems
    
    This is the first pass in creating the ability to calculate the
    frequency invariance on AMD systems. This approach uses the CPPC
    highest performance and nominal performance values that range from
    0 - 255 instead of a high and base frquency. This is because we do
    not have the ability on AMD to get a highest frequency value.
    
    On AMD systems the highest performance and nominal performance
    vaues do correspond to the highest and base frequencies for the system
    so using them should produce an appropriate ratio but some tweaking
    is likely necessary.
    
    Due to CPPC being initialized later in boot than when the frequency
    invariant calculation is currently made, I had to create a callback
    from the CPPC init code to do the calculation after we have CPPC
    data.
    
    Special thanks to "kernel test robot <lkp@intel.com>" for reporting that
    compilation of drivers/acpi/cppc_acpi.c is conditional to
    CONFIG_ACPI_CPPC_LIB, not just CONFIG_ACPI.
    
    [ ggherdovich@suse.cz: made safe under CPU hotplug, edited changelog. ]
    
    Signed-off-by: Nathan Fontenot <nathan.fontenot@amd.com>
    Signed-off-by: Giovanni Gherdovich <ggherdovich@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20201112182614.10700-2-ggherdovich@suse.cz

commit 2efc35dc439740652c46133357090fb5f03a90d0
Merge: 334d09c218c1 f9135aaf2767
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Dec 9 16:58:47 2020 +0100

    Merge tag 'samsung-soc-5.11' of git://git.kernel.org/pub/scm/linux/kernel/git/krzk/linux into arm/soc
    
    Samsung mach/soc changes for v5.11
    
    1. Do not use of_machine_is_compatible() in early CPU hotplug core. Full
       device tree walk causes "suspicious RCU usage" warnings.
    2. Clear prefetch bits in default l2c_aux_val of L310 L2C - they are not
       needed.
    3. Extend cpuidle support to P4 Note boards (Exynos4412).
    
    * tag 'samsung-soc-5.11' of git://git.kernel.org/pub/scm/linux/kernel/git/krzk/linux:
      ARM: exynos: extend cpuidle support to P4 Note boards
      ARM: exynos: clear prefetch bits in default l2c_aux_val
      ARM: exynos: Simplify code in Exynos3250 CPU core restart path
    
    Link: https://lore.kernel.org/r/20201201204404.22675-4-krzk@kernel.org
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 359db63378eded1ee9c8c9ad72245f9b0158ae95
Author: Xiang Chen <chenxiang66@hisilicon.com>
Date:   Mon Dec 7 21:30:55 2020 +0800

    scsi: hisi_sas: Select a suitable queue for internal I/Os
    
    For when managed interrupts are used (and shost->nr_hw_queues is set), a
    fixed queue - set per-device - is still used for internal I/Os.
    
    If all the CPUs mapped to that queue are offlined, then the completions for
    that queue are not serviced and any internal I/Os will time out.
    
    Fix by selecting a queue for internal I/Os from the queue mapped from the
    current CPU in this scenario.
    
    This is still not ideal as it does not deal with CPU hotplug for inflight
    internal I/Os, and needs proper support from [0].
    
    [0] https://lore.kernel.org/linux-scsi/20200703130122.111448-1-hare@suse.de/T/#m7d77d049b18f33a24ef206af69ebb66d07440556
    
    Link: https://lore.kernel.org/r/1607347855-59091-1-git-send-email-john.garry@huawei.com
    Fixes: 8d98416a55eb ("scsi: hisi_sas: Switch v3 hw to MQ")
    Signed-off-by: Xiang Chen <chenxiang66@hisilicon.com>
    Signed-off-by: John Garry <john.garry@huawei.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit 9f866dac94292f93d3b6bf8dbe860a44b954e555
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Tue Sep 29 15:29:27 2020 -0400

    rcu/tree: Add a warning if CPU being onlined did not report QS already
    
    Currently, rcu_cpu_starting() checks to see if the RCU core expects a
    quiescent state from the incoming CPU.  However, the current interaction
    between RCU quiescent-state reporting and CPU-hotplug operations should
    mean that the incoming CPU never needs to report a quiescent state.
    First, the outgoing CPU reports a quiescent state if needed.  Second,
    the race where the CPU is leaving just as RCU is initializing a new
    grace period is handled by an explicit check for this condition.  Third,
    the CPU's leaf rcu_node structure's ->lock serializes these checks.
    
    This means that if rcu_cpu_starting() ever feels the need to report
    a quiescent state, then there is a bug somewhere in the CPU hotplug
    code or the RCU grace-period handling code.  This commit therefore
    adds a WARN_ON_ONCE() to bring that bug to everyone's attention.
    
    Cc: Neeraj Upadhyay <neeraju@codeaurora.org>
    Suggested-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit ed73860cecc3ec12aa50a6dcfb4900e5b4ae9507
Author: Neeraj Upadhyay <neeraju@codeaurora.org>
Date:   Wed Sep 23 12:59:33 2020 +0530

    rcu: Fix single-CPU check in rcu_blocking_is_gp()
    
    Currently, for CONFIG_PREEMPTION=n kernels, rcu_blocking_is_gp() uses
    num_online_cpus() to determine whether there is only one CPU online.  When
    there is only a single CPU online, the simple fact that synchronize_rcu()
    could be legally called implies that a full grace period has elapsed.
    Therefore, in the single-CPU case, synchronize_rcu() simply returns
    immediately.  Unfortunately, num_online_cpus() is unreliable while a
    CPU-hotplug operation is transitioning to or from single-CPU operation
    because:
    
    1.      num_online_cpus() uses atomic_read(&__num_online_cpus) to
            locklessly sample the number of online CPUs.  The hotplug locks
            are not held, which means that an incoming CPU can concurrently
            update this count.  This in turn means that an RCU read-side
            critical section on the incoming CPU might observe updates
            prior to the grace period, but also that this critical section
            might extend beyond the end of the optimized synchronize_rcu().
            This breaks RCU's fundamental guarantee.
    
    2.      In addition, num_online_cpus() does no ordering, thus providing
            another way that RCU's fundamental guarantee can be broken by
            the current code.
    
    3.      The most probable failure mode happens on outgoing CPUs.
            The outgoing CPU updates the count of online CPUs in the
            CPUHP_TEARDOWN_CPU stop-machine handler, which is fine in
            and of itself due to preemption being disabled at the call
            to num_online_cpus().  Unfortunately, after that stop-machine
            handler returns, the CPU takes one last trip through the
            scheduler (which has RCU readers) and, after the resulting
            context switch, one final dive into the idle loop.  During this
            time, RCU needs to keep track of two CPUs, but num_online_cpus()
            will say that there is only one, which in turn means that the
            surviving CPU will incorrectly ignore the outgoing CPU's RCU
            read-side critical sections.
    
    This problem is illustrated by the following litmus test in which P0()
    corresponds to synchronize_rcu() and P1() corresponds to the incoming CPU.
    The herd7 tool confirms that the "exists" clause can be satisfied,
    thus demonstrating that this breakage can happen according to the Linux
    kernel memory model.
    
       {
         int x = 0;
         atomic_t numonline = ATOMIC_INIT(1);
       }
    
       P0(int *x, atomic_t *numonline)
       {
         int r0;
         WRITE_ONCE(*x, 1);
         r0 = atomic_read(numonline);
         if (r0 == 1) {
           smp_mb();
         } else {
           synchronize_rcu();
         }
         WRITE_ONCE(*x, 2);
       }
    
       P1(int *x, atomic_t *numonline)
       {
         int r0; int r1;
    
         atomic_inc(numonline);
         smp_mb();
         rcu_read_lock();
         r0 = READ_ONCE(*x);
         smp_rmb();
         r1 = READ_ONCE(*x);
         rcu_read_unlock();
       }
    
       locations [x;numonline;]
    
       exists (1:r0=0 /\ 1:r1=2)
    
    It is important to note that these problems arise only when the system
    is transitioning to or from single-CPU operation.
    
    One solution would be to hold the CPU-hotplug locks while sampling
    num_online_cpus(), which was in fact the intent of the (redundant)
    preempt_disable() and preempt_enable() surrounding this call to
    num_online_cpus().  Actually blocking CPU hotplug would not only result
    in excessive overhead, but would also unnecessarily impede CPU-hotplug
    operations.
    
    This commit therefore follows long-standing RCU tradition by maintaining
    a separate RCU-specific set of CPU-hotplug books.
    
    This separate set of books is implemented by a new ->n_online_cpus field
    in the rcu_state structure that maintains RCU's count of the online CPUs.
    This count is incremented early in the CPU-online process, so that
    the critical transition away from single-CPU operation will occur when
    there is only a single CPU.  Similarly for the critical transition to
    single-CPU operation, the counter is decremented late in the CPU-offline
    process, again while there is only a single CPU.  Because there is only
    ever a single CPU when the ->n_online_cpus field undergoes the critical
    1->2 and 2->1 transitions, full memory ordering and mutual exclusion is
    provided implicitly and, better yet, for free.
    
    In the case where the CPU is coming online, nothing will happen until
    the current CPU helps it come online.  Therefore, the new CPU will see
    all accesses prior to the optimized grace period, which means that RCU
    does not need to further delay this new CPU.  In the case where the CPU
    is going offline, the outgoing CPU is totally out of the picture before
    the optimized grace period starts, which means that this outgoing CPU
    cannot see any of the accesses following that grace period.  Again,
    RCU needs no further interaction with the outgoing CPU.
    
    This does mean that synchronize_rcu() will unnecessarily do a few grace
    periods the hard way just before the second CPU comes online and just
    after the second-to-last CPU goes offline, but it is not worth optimizing
    this uncommon case.
    
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Neeraj Upadhyay <neeraju@codeaurora.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit 334a1683935fceba346768b62cb3bb2d3e045578
Merge: 24bdae6993f7 695dc55b5739
Author: Dave Airlie <airlied@redhat.com>
Date:   Fri Nov 13 15:01:13 2020 +1000

    Merge tag 'drm-intel-gt-next-2020-11-12-1' of git://anongit.freedesktop.org/drm/drm-intel into drm-next
    
    Cross-subsystem Changes:
    - DMA mapped scatterlist fixes in i915 to unblock merging of
      https://lkml.org/lkml/2020/9/27/70 (Tvrtko, Tom)
    
    Driver Changes:
    
    - Fix for user reported issue #2381 (Graphical output stops with "switching to inteldrmfb from simple"):
      Mark ininitial fb obj as WT on eLLC machines to avoid rcu lockup during fbdev init (Ville, Chris)
    - Fix for Tigerlake (and earlier) to avoid spurious empty CSB events leading to hang (Chris, Bruce)
    - Delay execlist processing for Tigerlake to avoid hang (Chris)
    - Fix for Tigerlake RCS engine health check through heartbeat (Chris)
    - Fix for Tigerlake reserved MOCS entries (Ayaz, Chris)
    - Fix Media power gate sequence on Tigerlake (Rodrigo)
    - Enable eLLC caching of display buffers for SKL+ (Ville)
    - Support parsing of oversize batches on Gen9 (Matt, Chris)
    - Exclude low pages (128KiB) of stolen from use to avoid thrashing during reset (Chris)
    - Flush engines before Tigerlake breadcrumbs (Chris)
    
    - Use the local HWSP offset during submission (Chris)
    - Flush coherency domains on first set-domain-ioctl (Chris, Zbigniew)
    - Use the active reference on the vma while capturing to avoid use-after-free (Chris)
    - Fix MOCS PTE setting for gen9+ (Ville)
    - Avoid NULL dereference on IPS driver callback while unbinding i915 (Chris)
    - Avoid NULL dereference from PT/PD stash allocation error (Matt)
    - Hold request reference for canceling an active context (Chris)
    - Avoid infinite loop on x86-32 when mapping a lot of objects (Chris)
    - Disallow WC mappings when processor doesn't support them (Chris)
    - Return correct error in i915_gem_object_copy_blt() error path (Dan)
    - Return correct error in intel_context_create_request() error path (Maarten)
    - Tune down GuC communication enabled/disabled messages to debug (Jani)
    - Fix rebased commit "Remove i915_request.lock requirement for execution callbacks" (Chris)
    - Cancel outstanding work after disabling heartbeats on an engine (Chris)
    - Signal cancelled requests (Chris)
    - Retire cancelled requests on unload (Chris)
    - Scrub HW state on driver remove (Chris)
    - Undo forced context restores after trivial preemptions (Chris)
    - Handle PCI unbind in PMU code (Tvrtko)
    - Fix CPU hotplug with multiple GPUs in PMU code (Trtkko)
    - Correctly set SFC capability for video engines (Venkata)
    
    - Update GuC code to use firmware v49.0.1 (John, Matthew B., Daniele, Oscar, Michel, Rodrigo, Michal)
    - Improve GuC warnings on loading failure (John)
    - Avoid ownership race in buffer pool by clearing age (Chris)
    - Use MMIO to read CSB in case of failure (Chris, Mika)
    - Show engine properties in engine state dump to indicate changes (Chris, Joonas)
    - Break up error capture compression loops with cond_resched() (Chris)
    - Reduce GPU error capture mutex hold time to avoid khungtaskd (Chris)
    - Serialise debugfs i915_gem_objects with ctx->mutex (Chris)
    - Always test execution status on closing the context and close if not persistent (Chris)
    - Avoid mixing integer types during batch copies (Chris, Jared)
    - Skip over MI_NOOP when parsing to avoid overhead (Chris)
    - Hold onto an explicit ref to i915_vma_work.pinned (Chris)
    - Perform all asynchronous waits prior to marking payload start (Chris)
    - Pull phys pread/pwrite implementations to the backend (Matt)
    
    - Improve record of hung engines in error state (Tvrtko)
    - Allow backends to override pread implementation (Matt)
    - Reinforce LRC poisoning checks to confirm context survives execution (Chris)
    - Fix memory region max size calculation (Matt)
    - Fix order when adding blocks to memory region (Matt)
    - Eliminate unused intel_virtual_engine_get_sibling func (Chris)
    - Cleanup kasan warning for on-stack (unsigned long) casting (Chris)
    - Onion unwind for scratch page allocation failure (Chris)
    - Poison stolen pages before use (Chris)
    - Selftest improvements (Chris)
    
    Signed-off-by: Dave Airlie <airlied@redhat.com>
    
    From: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20201112163407.GA20320@jlahtine-mobl.ger.corp.intel.com

commit 3015ef4b98f53fe7eba4f5f82f562c0e074d213c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 26 14:08:10 2020 +0200

    sched/core: Make migrate disable and CPU hotplug cooperative
    
    On CPU unplug tasks which are in a migrate disabled region cannot be pushed
    to a different CPU until they returned to migrateable state.
    
    Account the number of tasks on a runqueue which are in a migrate disabled
    section and make the hotplug wait mechanism respect that.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
    Reviewed-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Link: https://lkml.kernel.org/r/20201023102347.067278757@infradead.org

commit 1cf12e08bc4d50a76b80c42a3109c53d8794a0c9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 16 09:27:18 2020 +0200

    sched/hotplug: Consolidate task migration on CPU unplug
    
    With the new mechanism which kicks tasks off the outgoing CPU at the end of
    schedule() the situation on an outgoing CPU right before the stopper thread
    brings it down completely is:
    
     - All user tasks and all unbound kernel threads have either been migrated
       away or are not running and the next wakeup will move them to a online CPU.
    
     - All per CPU kernel threads, except cpu hotplug thread and the stopper
       thread have either been unbound or parked by the responsible CPU hotplug
       callback.
    
    That means that at the last step before the stopper thread is invoked the
    cpu hotplug thread is the last legitimate running task on the outgoing
    CPU.
    
    Add a final wait step right before the stopper thread is kicked which
    ensures that any still running tasks on the way to park or on the way to
    kick themself of the CPU are either sleeping or gone.
    
    This allows to remove the migrate_tasks() crutch in sched_cpu_dying(). If
    sched_cpu_dying() detects that there is still another running task aside of
    the stopper thread then it will explode with the appropriate fireworks.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
    Reviewed-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Link: https://lkml.kernel.org/r/20201023102346.547163969@infradead.org

commit f2469a1fb43f85d243ce72638367fb6e15c33491
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 14 14:47:28 2020 +0200

    sched/core: Wait for tasks being pushed away on hotplug
    
    RT kernels need to ensure that all tasks which are not per CPU kthreads
    have left the outgoing CPU to guarantee that no tasks are force migrated
    within a migrate disabled section.
    
    There is also some desire to (ab)use fine grained CPU hotplug control to
    clear a CPU from active state to force migrate tasks which are not per CPU
    kthreads away for power control purposes.
    
    Add a mechanism which waits until all tasks which should leave the CPU
    after the CPU active flag is cleared have moved to a different online CPU.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
    Reviewed-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Link: https://lkml.kernel.org/r/20201023102346.377836842@infradead.org

commit a043260740d5d6ec5be59c3fb595c719890a0b0b
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Tue Sep 29 15:29:28 2020 -0400

    docs: Update RCU's hotplug requirements with a bit about design
    
    The rcu_barrier() section of the "Hotplug CPU" section discusses
    deadlocks, however the description of deadlocks other than those involving
    rcu_barrier() is rather incomplete.
    
    This commit therefore continues the section by describing how RCU's
    design handles CPU hotplug in a deadlock-free way.
    
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit 30f3f68e27d14a237acc339975e18670e58927ca
Merge: 4257087e8feb 108aa503657e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 6 12:42:49 2020 -0800

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes from Will Deacon:
     "Here's the weekly batch of fixes for arm64. Not an awful lot here, but
      there are still a few unresolved issues relating to CPU hotplug, RCU
      and IRQ tracing that I hope to queue fixes for next week.
    
      Summary:
    
       - Fix early use of kprobes
    
       - Fix kernel placement in kexec_file_load()
    
       - Bump maximum number of NUMA nodes"
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: kexec_file: try more regions if loading segments fails
      arm64: kprobes: Use BRK instead of single-step when executing instructions out-of-line
      arm64: NUMA: Kconfig: Increase NODES_SHIFT to 4

commit 8a988434456f0d6bac04377906d936a9b21837c8
Author: Tingwei Zhang <tingwei@codeaurora.org>
Date:   Wed Sep 16 13:17:27 2020 -0600

    coresight: cti: remove pm_runtime_get_sync() from CPU hotplug
    
    [ Upstream commit 6e8836c6df5327bdb24211424f1ad1411d1ed64a ]
    
    Below BUG is triggered by call pm_runtime_get_sync() in
    cti_cpuhp_enable_hw(). It's in CPU hotplug callback with interrupt
    disabled. Pm_runtime_get_sync() calls clock driver to enable clock
    which could sleep. Remove pm_runtime_get_sync() in cti_cpuhp_enable_hw()
    since pm_runtime_get_sync() is called in cti_enabld and pm_runtime_put()
    is called in cti_disabled. No need to increase pm count when CPU gets
    online since it's not decreased when CPU is offline.
    
    [  105.800279] BUG: scheduling while atomic: swapper/1/0/0x00000002
    [  105.800290] Modules linked in:
    [  105.800327] CPU: 1 PID: 0 Comm: swapper/1 Tainted: G        W
    5.9.0-rc1-gff1304be0a05-dirty #21
    [  105.800337] Hardware name: Thundercomm Dragonboard 845c (DT)
    [  105.800353] Call trace:
    [  105.800414]  dump_backtrace+0x0/0x1d4
    [  105.800439]  show_stack+0x14/0x1c
    [  105.800462]  dump_stack+0xc0/0x100
    [  105.800490]  __schedule_bug+0x58/0x74
    [  105.800523]  __schedule+0x590/0x65c
    [  105.800538]  schedule+0x78/0x10c
    [  105.800553]  schedule_timeout+0x188/0x250
    [  105.800585]  qmp_send.constprop.10+0x12c/0x1b0
    [  105.800599]  qmp_qdss_clk_prepare+0x18/0x20
    [  105.800622]  clk_core_prepare+0x48/0xd4
    [  105.800639]  clk_prepare+0x20/0x34
    [  105.800663]  amba_pm_runtime_resume+0x54/0x90
    [  105.800695]  __rpm_callback+0xdc/0x138
    [  105.800709]  rpm_callback+0x24/0x78
    [  105.800724]  rpm_resume+0x328/0x47c
    [  105.800739]  __pm_runtime_resume+0x50/0x74
    [  105.800768]  cti_starting_cpu+0x40/0xa4
    [  105.800795]  cpuhp_invoke_callback+0x84/0x1e0
    [  105.800814]  notify_cpu_starting+0x9c/0xb8
    [  105.800834]  secondary_start_kernel+0xd8/0x164
    [  105.800933] CPU1: Booted secondary processor 0x0000000100 [0x517f803c]
    
    Fixes: e9b880581d55 ("coresight: cti: Add CPU Hotplug handling to CTI driver")
    Reviewed-by: Mike Leach <mike.leach@linaro.org>
    Signed-off-by: Tingwei Zhang <tingwei@codeaurora.org>
    Signed-off-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Link: https://lore.kernel.org/r/20200916191737.4001561-7-mathieu.poirier@linaro.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 435fd705a501a6099155f896263dd230339aef80
Author: Tingwei Zhang <tingwei@codeaurora.org>
Date:   Wed Sep 16 13:17:27 2020 -0600

    coresight: cti: remove pm_runtime_get_sync() from CPU hotplug
    
    [ Upstream commit 6e8836c6df5327bdb24211424f1ad1411d1ed64a ]
    
    Below BUG is triggered by call pm_runtime_get_sync() in
    cti_cpuhp_enable_hw(). It's in CPU hotplug callback with interrupt
    disabled. Pm_runtime_get_sync() calls clock driver to enable clock
    which could sleep. Remove pm_runtime_get_sync() in cti_cpuhp_enable_hw()
    since pm_runtime_get_sync() is called in cti_enabld and pm_runtime_put()
    is called in cti_disabled. No need to increase pm count when CPU gets
    online since it's not decreased when CPU is offline.
    
    [  105.800279] BUG: scheduling while atomic: swapper/1/0/0x00000002
    [  105.800290] Modules linked in:
    [  105.800327] CPU: 1 PID: 0 Comm: swapper/1 Tainted: G        W
    5.9.0-rc1-gff1304be0a05-dirty #21
    [  105.800337] Hardware name: Thundercomm Dragonboard 845c (DT)
    [  105.800353] Call trace:
    [  105.800414]  dump_backtrace+0x0/0x1d4
    [  105.800439]  show_stack+0x14/0x1c
    [  105.800462]  dump_stack+0xc0/0x100
    [  105.800490]  __schedule_bug+0x58/0x74
    [  105.800523]  __schedule+0x590/0x65c
    [  105.800538]  schedule+0x78/0x10c
    [  105.800553]  schedule_timeout+0x188/0x250
    [  105.800585]  qmp_send.constprop.10+0x12c/0x1b0
    [  105.800599]  qmp_qdss_clk_prepare+0x18/0x20
    [  105.800622]  clk_core_prepare+0x48/0xd4
    [  105.800639]  clk_prepare+0x20/0x34
    [  105.800663]  amba_pm_runtime_resume+0x54/0x90
    [  105.800695]  __rpm_callback+0xdc/0x138
    [  105.800709]  rpm_callback+0x24/0x78
    [  105.800724]  rpm_resume+0x328/0x47c
    [  105.800739]  __pm_runtime_resume+0x50/0x74
    [  105.800768]  cti_starting_cpu+0x40/0xa4
    [  105.800795]  cpuhp_invoke_callback+0x84/0x1e0
    [  105.800814]  notify_cpu_starting+0x9c/0xb8
    [  105.800834]  secondary_start_kernel+0xd8/0x164
    [  105.800933] CPU1: Booted secondary processor 0x0000000100 [0x517f803c]
    
    Fixes: e9b880581d55 ("coresight: cti: Add CPU Hotplug handling to CTI driver")
    Reviewed-by: Mike Leach <mike.leach@linaro.org>
    Signed-off-by: Tingwei Zhang <tingwei@codeaurora.org>
    Signed-off-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Link: https://lore.kernel.org/r/20200916191737.4001561-7-mathieu.poirier@linaro.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 970f6cf2e91465c53dbbf9d80d755a7e8139fedb
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Oct 27 21:17:16 2020 +0100

    ARM: exynos: Simplify code in Exynos3250 CPU core restart path
    
    exynos_core_restart() is called by secondary CPU boot procedure, used by
    CPU hotplug and coupled CPU idle. Replace of_machine_is_compatible() call
    with a simple SoC revision check.
    
    of_machine_is_compatible() function performs a dozen of string comparisons
    during the full device tree walk, while soc_is_exynos3250() is a simple
    integer check on SoC revision variable. This change also fixes the
    following warning:
    
    =============================
    WARNING: suspicious RCU usage
    5.10.0-rc1-00001-g6f65599d1f4f-dirty #1800 Not tainted
    -----------------------------
    ./include/trace/events/lock.h:37 suspicious rcu_dereference_check() usage!
    
    other info that might help us debug this:
    
    rcu_scheduler_active = 2, debug_locks = 1
    RCU used illegally from extended quiescent state!
    no locks held by swapper/0/0.
    
    stack backtrace:
    CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.10.0-rc1-00001-g6f65599d1f4f-dirty #1800
    Hardware name: Samsung Exynos (Flattened Device Tree)
    [<c0111514>] (unwind_backtrace) from [<c010ceb8>] (show_stack+0x10/0x14)
    [<c010ceb8>] (show_stack) from [<c0b1d8dc>] (dump_stack+0xb4/0xd4)
    [<c0b1d8dc>] (dump_stack) from [<c0194acc>] (lock_acquire+0x418/0x584)
    [<c0194acc>] (lock_acquire) from [<c0b29e58>] (_raw_spin_lock_irqsave+0x4c/0x60)
    [<c0b29e58>] (_raw_spin_lock_irqsave) from [<c0897af4>] (of_device_is_compatible+0x1c/0x4c)
    [<c0897af4>] (of_device_is_compatible) from [<c01216d8>] (exynos_core_restart+0x14/0xb0)
    [<c01216d8>] (exynos_core_restart) from [<c0120a78>] (exynos_cpu0_enter_aftr+0x1d0/0x1dc)
    [<c0120a78>] (exynos_cpu0_enter_aftr) from [<c08575b0>] (exynos_enter_coupled_lowpower+0x44/0x74)
    [<c08575b0>] (exynos_enter_coupled_lowpower) from [<c085477c>] (cpuidle_enter_state+0x178/0x660)
    [<c085477c>] (cpuidle_enter_state) from [<c08572dc>] (cpuidle_enter_state_coupled+0x35c/0x378)
    [<c08572dc>] (cpuidle_enter_state_coupled) from [<c0854cc8>] (cpuidle_enter+0x50/0x54)
    [<c0854cc8>] (cpuidle_enter) from [<c0164854>] (do_idle+0x224/0x2a4)
    [<c0164854>] (do_idle) from [<c0164c88>] (cpu_startup_entry+0x18/0x1c)
    [<c0164c88>] (cpu_startup_entry) from [<c1100fa0>] (start_kernel+0x640/0x67c)
    [<c1100fa0>] (start_kernel) from [<00000000>] (0x0)
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Link: https://lore.kernel.org/r/20201027201716.15745-1-m.szyprowski@samsung.com
    Signed-off-by: Krzysztof Kozlowski <krzk@kernel.org>

commit 537f9c84a42754f89977bc2d19f2f69503a3a02a
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Tue Oct 20 17:11:44 2020 +0100

    drm/i915/pmu: Fix CPU hotplug with multiple GPUs
    
    Since we keep a driver global mask of online CPUs and base the decision
    whether PMU needs to be migrated upon it, we need to make sure the
    migration is done for all registered PMUs (so GPUs).
    
    To do this we need to track the current CPU for each PMU and base the
    decision on whether to migrate on a comparison between global and local
    state.
    
    At the same time, since dynamic CPU hotplug notification slots are a
    scarce resource and given how we already register the multi instance type
    state, we can and should add multiple instance of the i915 PMU to this
    same state and not allocate a new one for every GPU.
    
    v2:
     * Use pr_notice. (Chris)
    
    v3:
     * Handle a nasty interaction where unregistration which triggers a false
       CPU offline event. (Chris)
    
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Suggested-by: Daniel Vetter <daniel.vetter@intel.com> # dynamic slot optimisation
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20201020161144.678668-1-tvrtko.ursulin@linux.intel.com

commit a98fcb2fc19b158a1d0aa68235827149f7637110
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Sep 22 14:40:46 2020 +0200

    clk: samsung: exynos4: mark 'chipid' clock as CLK_IGNORE_UNUSED
    
    [ Upstream commit f3bb0f796f5ffe32f0fbdce5b1b12eb85511158f ]
    
    The ChipID IO region has it's own clock, which is being disabled while
    scanning for unused clocks. It turned out that some CPU hotplug, CPU idle
    or even SOC firmware code depends on the reads from that area. Fix the
    mysterious hang caused by entering deep CPU idle state by ignoring the
    'chipid' clock during unused clocks scan, as there are no direct clients
    for it which will keep it enabled.
    
    Fixes: e062b571777f ("clk: exynos4: register clocks using common clock framework")
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Link: https://lore.kernel.org/r/20200922124046.10496-1-m.szyprowski@samsung.com
    Reviewed-by: Krzysztof Kozlowski <krzk@kernel.org>
    Acked-by: Sylwester Nawrocki <s.nawrocki@samsung.com>
    Signed-off-by: Stephen Boyd <sboyd@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit eb8eda5b07505fca0b3c134c1d3526795eca2fe3
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Sep 22 14:40:46 2020 +0200

    clk: samsung: exynos4: mark 'chipid' clock as CLK_IGNORE_UNUSED
    
    [ Upstream commit f3bb0f796f5ffe32f0fbdce5b1b12eb85511158f ]
    
    The ChipID IO region has it's own clock, which is being disabled while
    scanning for unused clocks. It turned out that some CPU hotplug, CPU idle
    or even SOC firmware code depends on the reads from that area. Fix the
    mysterious hang caused by entering deep CPU idle state by ignoring the
    'chipid' clock during unused clocks scan, as there are no direct clients
    for it which will keep it enabled.
    
    Fixes: e062b571777f ("clk: exynos4: register clocks using common clock framework")
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Link: https://lore.kernel.org/r/20200922124046.10496-1-m.szyprowski@samsung.com
    Reviewed-by: Krzysztof Kozlowski <krzk@kernel.org>
    Acked-by: Sylwester Nawrocki <s.nawrocki@samsung.com>
    Signed-off-by: Stephen Boyd <sboyd@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 695e83a1ff2ba63aa391245c71090156b071bd1d
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Sep 22 14:40:46 2020 +0200

    clk: samsung: exynos4: mark 'chipid' clock as CLK_IGNORE_UNUSED
    
    [ Upstream commit f3bb0f796f5ffe32f0fbdce5b1b12eb85511158f ]
    
    The ChipID IO region has it's own clock, which is being disabled while
    scanning for unused clocks. It turned out that some CPU hotplug, CPU idle
    or even SOC firmware code depends on the reads from that area. Fix the
    mysterious hang caused by entering deep CPU idle state by ignoring the
    'chipid' clock during unused clocks scan, as there are no direct clients
    for it which will keep it enabled.
    
    Fixes: e062b571777f ("clk: exynos4: register clocks using common clock framework")
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Link: https://lore.kernel.org/r/20200922124046.10496-1-m.szyprowski@samsung.com
    Reviewed-by: Krzysztof Kozlowski <krzk@kernel.org>
    Acked-by: Sylwester Nawrocki <s.nawrocki@samsung.com>
    Signed-off-by: Stephen Boyd <sboyd@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 40e2e6c71ac13ffd03d46493eaacb6dfc6944cda
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Sep 22 14:40:46 2020 +0200

    clk: samsung: exynos4: mark 'chipid' clock as CLK_IGNORE_UNUSED
    
    [ Upstream commit f3bb0f796f5ffe32f0fbdce5b1b12eb85511158f ]
    
    The ChipID IO region has it's own clock, which is being disabled while
    scanning for unused clocks. It turned out that some CPU hotplug, CPU idle
    or even SOC firmware code depends on the reads from that area. Fix the
    mysterious hang caused by entering deep CPU idle state by ignoring the
    'chipid' clock during unused clocks scan, as there are no direct clients
    for it which will keep it enabled.
    
    Fixes: e062b571777f ("clk: exynos4: register clocks using common clock framework")
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Link: https://lore.kernel.org/r/20200922124046.10496-1-m.szyprowski@samsung.com
    Reviewed-by: Krzysztof Kozlowski <krzk@kernel.org>
    Acked-by: Sylwester Nawrocki <s.nawrocki@samsung.com>
    Signed-off-by: Stephen Boyd <sboyd@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 08e66c0c1c0e3e2ca65b8fb1e609105b0b991302
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Sep 22 14:40:46 2020 +0200

    clk: samsung: exynos4: mark 'chipid' clock as CLK_IGNORE_UNUSED
    
    [ Upstream commit f3bb0f796f5ffe32f0fbdce5b1b12eb85511158f ]
    
    The ChipID IO region has it's own clock, which is being disabled while
    scanning for unused clocks. It turned out that some CPU hotplug, CPU idle
    or even SOC firmware code depends on the reads from that area. Fix the
    mysterious hang caused by entering deep CPU idle state by ignoring the
    'chipid' clock during unused clocks scan, as there are no direct clients
    for it which will keep it enabled.
    
    Fixes: e062b571777f ("clk: exynos4: register clocks using common clock framework")
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Link: https://lore.kernel.org/r/20200922124046.10496-1-m.szyprowski@samsung.com
    Reviewed-by: Krzysztof Kozlowski <krzk@kernel.org>
    Acked-by: Sylwester Nawrocki <s.nawrocki@samsung.com>
    Signed-off-by: Stephen Boyd <sboyd@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit a84da5ea38334ff79c13259b7aa5cf50ed52aa67
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Sep 22 14:40:46 2020 +0200

    clk: samsung: exynos4: mark 'chipid' clock as CLK_IGNORE_UNUSED
    
    [ Upstream commit f3bb0f796f5ffe32f0fbdce5b1b12eb85511158f ]
    
    The ChipID IO region has it's own clock, which is being disabled while
    scanning for unused clocks. It turned out that some CPU hotplug, CPU idle
    or even SOC firmware code depends on the reads from that area. Fix the
    mysterious hang caused by entering deep CPU idle state by ignoring the
    'chipid' clock during unused clocks scan, as there are no direct clients
    for it which will keep it enabled.
    
    Fixes: e062b571777f ("clk: exynos4: register clocks using common clock framework")
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Link: https://lore.kernel.org/r/20200922124046.10496-1-m.szyprowski@samsung.com
    Reviewed-by: Krzysztof Kozlowski <krzk@kernel.org>
    Acked-by: Sylwester Nawrocki <s.nawrocki@samsung.com>
    Signed-off-by: Stephen Boyd <sboyd@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 88451f2cd3cec2abc30debdf129422d2699d1eba
Author: Zqiang <qiang.zhang@windriver.com>
Date:   Tue Sep 8 14:27:09 2020 +0800

    debugobjects: Free per CPU pool after CPU unplug
    
    If a CPU is offlined the debug objects per CPU pool is not cleaned up. If
    the CPU is never onlined again then the objects in the pool are wasted.
    
    Add a CPU hotplug callback which is invoked after the CPU is dead to free
    the pool.
    
    [ tglx: Massaged changelog and added comment about remote access safety ]
    
    Signed-off-by: Zqiang <qiang.zhang@windriver.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman Long <longman@redhat.com>
    Link: https://lore.kernel.org/r/20200908062709.11441-1-qiang.zhang@windriver.com

commit f4673625a52c69a12d2a8f71bcf408c685c148ec
Author: Gavin Shan <gshan@redhat.com>
Date:   Tue Sep 22 23:04:21 2020 +1000

    firmware: arm_sdei: Introduce sdei_do_local_call()
    
    During the CPU hotplug, the private events are registered, enabled
    or unregistered on the specific CPU. It repeats the same steps:
    initializing cross call argument, make function call on local CPU,
    check the returned error.
    
    This introduces sdei_do_local_call() to cover the first steps. The
    other benefit is to make CROSSCALL_INIT and struct sdei_crosscall_args
    are only visible to sdei_do_{cross, local}_call().
    
    Signed-off-by: Gavin Shan <gshan@redhat.com>
    Reviewed-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Link: https://lore.kernel.org/r/20200922130423.10173-12-gshan@redhat.com
    Signed-off-by: Will Deacon <will@kernel.org>

commit f3bb0f796f5ffe32f0fbdce5b1b12eb85511158f
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Sep 22 14:40:46 2020 +0200

    clk: samsung: exynos4: mark 'chipid' clock as CLK_IGNORE_UNUSED
    
    The ChipID IO region has it's own clock, which is being disabled while
    scanning for unused clocks. It turned out that some CPU hotplug, CPU idle
    or even SOC firmware code depends on the reads from that area. Fix the
    mysterious hang caused by entering deep CPU idle state by ignoring the
    'chipid' clock during unused clocks scan, as there are no direct clients
    for it which will keep it enabled.
    
    Fixes: e062b571777f ("clk: exynos4: register clocks using common clock framework")
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Link: https://lore.kernel.org/r/20200922124046.10496-1-m.szyprowski@samsung.com
    Reviewed-by: Krzysztof Kozlowski <krzk@kernel.org>
    Acked-by: Sylwester Nawrocki <s.nawrocki@samsung.com>
    Signed-off-by: Stephen Boyd <sboyd@kernel.org>

commit 6e8836c6df5327bdb24211424f1ad1411d1ed64a
Author: Tingwei Zhang <tingwei@codeaurora.org>
Date:   Wed Sep 16 13:17:27 2020 -0600

    coresight: cti: remove pm_runtime_get_sync() from CPU hotplug
    
    Below BUG is triggered by call pm_runtime_get_sync() in
    cti_cpuhp_enable_hw(). It's in CPU hotplug callback with interrupt
    disabled. Pm_runtime_get_sync() calls clock driver to enable clock
    which could sleep. Remove pm_runtime_get_sync() in cti_cpuhp_enable_hw()
    since pm_runtime_get_sync() is called in cti_enabld and pm_runtime_put()
    is called in cti_disabled. No need to increase pm count when CPU gets
    online since it's not decreased when CPU is offline.
    
    [  105.800279] BUG: scheduling while atomic: swapper/1/0/0x00000002
    [  105.800290] Modules linked in:
    [  105.800327] CPU: 1 PID: 0 Comm: swapper/1 Tainted: G        W
    5.9.0-rc1-gff1304be0a05-dirty #21
    [  105.800337] Hardware name: Thundercomm Dragonboard 845c (DT)
    [  105.800353] Call trace:
    [  105.800414]  dump_backtrace+0x0/0x1d4
    [  105.800439]  show_stack+0x14/0x1c
    [  105.800462]  dump_stack+0xc0/0x100
    [  105.800490]  __schedule_bug+0x58/0x74
    [  105.800523]  __schedule+0x590/0x65c
    [  105.800538]  schedule+0x78/0x10c
    [  105.800553]  schedule_timeout+0x188/0x250
    [  105.800585]  qmp_send.constprop.10+0x12c/0x1b0
    [  105.800599]  qmp_qdss_clk_prepare+0x18/0x20
    [  105.800622]  clk_core_prepare+0x48/0xd4
    [  105.800639]  clk_prepare+0x20/0x34
    [  105.800663]  amba_pm_runtime_resume+0x54/0x90
    [  105.800695]  __rpm_callback+0xdc/0x138
    [  105.800709]  rpm_callback+0x24/0x78
    [  105.800724]  rpm_resume+0x328/0x47c
    [  105.800739]  __pm_runtime_resume+0x50/0x74
    [  105.800768]  cti_starting_cpu+0x40/0xa4
    [  105.800795]  cpuhp_invoke_callback+0x84/0x1e0
    [  105.800814]  notify_cpu_starting+0x9c/0xb8
    [  105.800834]  secondary_start_kernel+0xd8/0x164
    [  105.800933] CPU1: Booted secondary processor 0x0000000100 [0x517f803c]
    
    Fixes: e9b880581d55 ("coresight: cti: Add CPU Hotplug handling to CTI driver")
    Reviewed-by: Mike Leach <mike.leach@linaro.org>
    Signed-off-by: Tingwei Zhang <tingwei@codeaurora.org>
    Signed-off-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Link: https://lore.kernel.org/r/20200916191737.4001561-7-mathieu.poirier@linaro.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 32bc15afed04bd73e29d713d8db47818d6aa89af
Author: John Garry <john.garry@huawei.com>
Date:   Wed Aug 19 23:20:24 2020 +0800

    blk-mq: Facilitate a shared sbitmap per tagset
    
    Some SCSI HBAs (such as HPSA, megaraid, mpt3sas, hisi_sas_v3 ..) support
    multiple reply queues with single hostwide tags.
    
    In addition, these drivers want to use interrupt assignment in
    pci_alloc_irq_vectors(PCI_IRQ_AFFINITY). However, as discussed in [0],
    CPU hotplug may cause in-flight IO completion to not be serviced when an
    interrupt is shutdown. That problem is solved in commit bf0beec0607d
    ("blk-mq: drain I/O when all CPUs in a hctx are offline").
    
    However, to take advantage of that blk-mq feature, the HBA HW queuess are
    required to be mapped to that of the blk-mq hctx's; to do that, the HBA HW
    queues need to be exposed to the upper layer.
    
    In making that transition, the per-SCSI command request tags are no
    longer unique per Scsi host - they are just unique per hctx. As such, the
    HBA LLDD would have to generate this tag internally, which has a certain
    performance overhead.
    
    However another problem is that blk-mq assumes the host may accept
    (Scsi_host.can_queue * #hw queue) commands. In commit 6eb045e092ef ("scsi:
     core: avoid host-wide host_busy counter for scsi_mq"), the Scsi host busy
    counter was removed, which would stop the LLDD being sent more than
    .can_queue commands; however, it should still be ensured that the block
    layer does not issue more than .can_queue commands to the Scsi host.
    
    To solve this problem, introduce a shared sbitmap per blk_mq_tag_set,
    which may be requested at init time.
    
    New flag BLK_MQ_F_TAG_HCTX_SHARED should be set when requesting the
    tagset to indicate whether the shared sbitmap should be used.
    
    Even when BLK_MQ_F_TAG_HCTX_SHARED is set, a full set of tags and requests
    are still allocated per hctx; the reason for this is that if tags and
    requests were only allocated for a single hctx - like hctx0 - it may break
    block drivers which expect a request be associated with a specific hctx,
    i.e. not always hctx0. This will introduce extra memory usage.
    
    This change is based on work originally from Ming Lei in [1] and from
    Bart's suggestion in [2].
    
    [0] https://lore.kernel.org/linux-block/alpine.DEB.2.21.1904051331270.1802@nanos.tec.linutronix.de/
    [1] https://lore.kernel.org/linux-block/20190531022801.10003-1-ming.lei@redhat.com/
    [2] https://lore.kernel.org/linux-block/ff77beff-5fd9-9f05-12b6-826922bace1f@huawei.com/T/#m3db0a602f095cbcbff27e9c884d6b4ae826144be
    
    Signed-off-by: John Garry <john.garry@huawei.com>
    Tested-by: Don Brace<don.brace@microsemi.com> #SCSI resv cmds patches used
    Tested-by: Douglas Gilbert <dgilbert@interlog.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit cb95712138ec5e480db5160b41172bbc6f6494cc
Merge: 550c2129d93d 64ef8f2c4791
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Aug 23 11:37:23 2020 -0700

    Merge tag 'powerpc-5.9-3' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc fixes from Michael Ellerman:
    
     - Add perf support for emitting extended registers for power10.
    
     - A fix for CPU hotplug on pseries, where on large/loaded systems we
       may not wait long enough for the CPU to be offlined, leading to
       crashes.
    
     - Addition of a raw cputable entry for Power10, which is not required
       to boot, but is required to make our PMU setup work correctly in
       guests.
    
     - Three fixes for the recent changes on 32-bit Book3S to move modules
       into their own segment for strict RWX.
    
     - A fix for a recent change in our powernv PCI code that could lead to
       crashes.
    
     - A change to our perf interrupt accounting to avoid soft lockups when
       using some events, found by syzkaller.
    
     - A change in the way we handle power loss events from the hypervisor
       on pseries. We no longer immediately shut down if we're told we're
       running on a UPS.
    
     - A few other minor fixes.
    
    Thanks to Alexey Kardashevskiy, Andreas Schwab, Aneesh Kumar K.V, Anju T
    Sudhakar, Athira Rajeev, Christophe Leroy, Frederic Barrat, Greg Kurz,
    Kajol Jain, Madhavan Srinivasan, Michael Neuling, Michael Roth,
    Nageswara R Sastry, Oliver O'Halloran, Thiago Jung Bauermann,
    Vaidyanathan Srinivasan, Vasant Hegde.
    
    * tag 'powerpc-5.9-3' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux:
      powerpc/perf/hv-24x7: Move cpumask file to top folder of hv-24x7 driver
      powerpc/32s: Fix module loading failure when VMALLOC_END is over 0xf0000000
      powerpc/pseries: Do not initiate shutdown when system is running on UPS
      powerpc/perf: Fix soft lockups due to missed interrupt accounting
      powerpc/powernv/pci: Fix possible crash when releasing DMA resources
      powerpc/pseries/hotplug-cpu: wait indefinitely for vCPU death
      powerpc/32s: Fix is_module_segment() when MODULES_VADDR is defined
      powerpc/kasan: Fix KASAN_SHADOW_START on BOOK3S_32
      powerpc/fixmap: Fix the size of the early debug area
      powerpc/pkeys: Fix build error with PPC_MEM_KEYS disabled
      powerpc/kernel: Cleanup machine check function declarations
      powerpc: Add POWER10 raw mode cputable entry
      powerpc/perf: Add extended regs support for power10 platform
      powerpc/perf: Add support for outputting extended regs in perf intr_regs
      powerpc: Fix P10 PVR revision in /proc/cpuinfo for SMT4 cores

commit 93d0c1ab23281fda96490f23cd6f2a1966fdc030
Author: Sumit Gupta <sumitg@nvidia.com>
Date:   Wed Aug 12 01:13:17 2020 +0530

    cpufreq: replace cpu_logical_map() with read_cpuid_mpir()
    
    Commit eaecca9e7710 ("arm64: Fix __cpu_logical_map undefined issue")
    fixes the issue with building tegra194 cpufreq driver as module. But
    the fix might cause problem while supporting physical CPU hotplug[1].
    
    This patch fixes the original problem by avoiding use of cpu_logical_map().
    Instead calling read_cpuid_mpidr() to get MPIDR on target CPU.
    
    [1] https://lore.kernel.org/linux-arm-kernel/20200724131059.GB6521@bogus/
    
    Fixes: df320f89359c ("cpufreq: Add Tegra194 cpufreq driver")
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Sumit Gupta <sumitg@nvidia.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    [ rjw: Subject & changelog edits ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit d411475796be1873e993e28d5c9d5a88adcc05ba
Author: Nathan Lynch <nathanl@linux.ibm.com>
Date:   Fri Jun 12 00:12:21 2020 -0500

    powerpc/pseries: remove cede offline state for CPUs
    
    [ Upstream commit 48f6e7f6d948b56489da027bc3284c709b939d28 ]
    
    This effectively reverts commit 3aa565f53c39 ("powerpc/pseries: Add
    hooks to put the CPU into an appropriate offline state"), which added
    an offline mode for CPUs which uses the H_CEDE hcall instead of the
    architected stop-self RTAS function in order to facilitate "folding"
    of dedicated mode processors on PowerVM platforms to achieve energy
    savings. This has been the default offline mode since its
    introduction.
    
    There's nothing about stop-self that would prevent the hypervisor from
    achieving the energy savings available via H_CEDE, so the original
    premise of this change appears to be flawed.
    
    I also have encountered the claim that the transition to and from
    ceded state is much faster than stop-self/start-cpu. Certainly we
    would not want to use stop-self as an *idle* mode. That is what H_CEDE
    is for. However, this difference is insignificant in the context of
    Linux CPU hotplug, where the latency of an offline or online operation
    on current systems is on the order of 100ms, mainly attributable to
    all the various subsystems' cpuhp callbacks.
    
    The cede offline mode also prevents accurate accounting, as discussed
    before:
    https://lore.kernel.org/linuxppc-dev/1571740391-3251-1-git-send-email-ego@linux.vnet.ibm.com/
    
    Unconditionally use stop-self to offline processor threads. This is
    the architected method for offlining CPUs on PAPR systems.
    
    The "cede_offline" boot parameter is rendered obsolete.
    
    Removing this code enables the removal of the partition suspend code
    which temporarily onlines all present CPUs.
    
    Fixes: 3aa565f53c39 ("powerpc/pseries: Add hooks to put the CPU into an appropriate offline state")
    Signed-off-by: Nathan Lynch <nathanl@linux.ibm.com>
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200612051238.1007764-2-nathanl@linux.ibm.com
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit ac26d372f1a333d8ea9dd47d043e41dd2cc35e3d
Author: Zenghui Yu <yuzenghui@huawei.com>
Date:   Tue Jun 30 21:37:46 2020 +0800

    irqchip/gic-v4.1: Use GFP_ATOMIC flag in allocate_vpe_l1_table()
    
    [ Upstream commit d1bd7e0ba533a2a6f313579ec9b504f6614c35c4 ]
    
    Booting the latest kernel with DEBUG_ATOMIC_SLEEP=y on a GICv4.1 enabled
    box, I get the following kernel splat:
    
    [    0.053766] BUG: sleeping function called from invalid context at mm/slab.h:567
    [    0.053767] in_atomic(): 1, irqs_disabled(): 128, non_block: 0, pid: 0, name: swapper/1
    [    0.053769] CPU: 1 PID: 0 Comm: swapper/1 Not tainted 5.8.0-rc3+ #23
    [    0.053770] Call trace:
    [    0.053774]  dump_backtrace+0x0/0x218
    [    0.053775]  show_stack+0x2c/0x38
    [    0.053777]  dump_stack+0xc4/0x10c
    [    0.053779]  ___might_sleep+0xfc/0x140
    [    0.053780]  __might_sleep+0x58/0x90
    [    0.053782]  slab_pre_alloc_hook+0x7c/0x90
    [    0.053783]  kmem_cache_alloc_trace+0x60/0x2f0
    [    0.053785]  its_cpu_init+0x6f4/0xe40
    [    0.053786]  gic_starting_cpu+0x24/0x38
    [    0.053788]  cpuhp_invoke_callback+0xa0/0x710
    [    0.053789]  notify_cpu_starting+0xcc/0xd8
    [    0.053790]  secondary_start_kernel+0x148/0x200
    
     # ./scripts/faddr2line vmlinux its_cpu_init+0x6f4/0xe40
    its_cpu_init+0x6f4/0xe40:
    allocate_vpe_l1_table at drivers/irqchip/irq-gic-v3-its.c:2818
    (inlined by) its_cpu_init_lpis at drivers/irqchip/irq-gic-v3-its.c:3138
    (inlined by) its_cpu_init at drivers/irqchip/irq-gic-v3-its.c:5166
    
    It turned out that we're allocating memory using GFP_KERNEL (may sleep)
    within the CPU hotplug notifier, which is indeed an atomic context. Bad
    thing may happen if we're playing on a system with more than a single
    CommonLPIAff group. Avoid it by turning this into an atomic allocation.
    
    Fixes: 5e5168461c22 ("irqchip/gic-v4.1: VPE table (aka GICR_VPROPBASER) allocation")
    Signed-off-by: Zenghui Yu <yuzenghui@huawei.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200630133746.816-1-yuzenghui@huawei.com
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 4d9938b9e54f25414e4f7e4dfb0649e3a8bfc2ef
Author: Nathan Lynch <nathanl@linux.ibm.com>
Date:   Fri Jun 12 00:12:21 2020 -0500

    powerpc/pseries: remove cede offline state for CPUs
    
    [ Upstream commit 48f6e7f6d948b56489da027bc3284c709b939d28 ]
    
    This effectively reverts commit 3aa565f53c39 ("powerpc/pseries: Add
    hooks to put the CPU into an appropriate offline state"), which added
    an offline mode for CPUs which uses the H_CEDE hcall instead of the
    architected stop-self RTAS function in order to facilitate "folding"
    of dedicated mode processors on PowerVM platforms to achieve energy
    savings. This has been the default offline mode since its
    introduction.
    
    There's nothing about stop-self that would prevent the hypervisor from
    achieving the energy savings available via H_CEDE, so the original
    premise of this change appears to be flawed.
    
    I also have encountered the claim that the transition to and from
    ceded state is much faster than stop-self/start-cpu. Certainly we
    would not want to use stop-self as an *idle* mode. That is what H_CEDE
    is for. However, this difference is insignificant in the context of
    Linux CPU hotplug, where the latency of an offline or online operation
    on current systems is on the order of 100ms, mainly attributable to
    all the various subsystems' cpuhp callbacks.
    
    The cede offline mode also prevents accurate accounting, as discussed
    before:
    https://lore.kernel.org/linuxppc-dev/1571740391-3251-1-git-send-email-ego@linux.vnet.ibm.com/
    
    Unconditionally use stop-self to offline processor threads. This is
    the architected method for offlining CPUs on PAPR systems.
    
    The "cede_offline" boot parameter is rendered obsolete.
    
    Removing this code enables the removal of the partition suspend code
    which temporarily onlines all present CPUs.
    
    Fixes: 3aa565f53c39 ("powerpc/pseries: Add hooks to put the CPU into an appropriate offline state")
    Signed-off-by: Nathan Lynch <nathanl@linux.ibm.com>
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200612051238.1007764-2-nathanl@linux.ibm.com
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 9dcd0ec3989f6f838d0fccf1c2780c6c62819133
Author: Zenghui Yu <yuzenghui@huawei.com>
Date:   Tue Jun 30 21:37:46 2020 +0800

    irqchip/gic-v4.1: Use GFP_ATOMIC flag in allocate_vpe_l1_table()
    
    [ Upstream commit d1bd7e0ba533a2a6f313579ec9b504f6614c35c4 ]
    
    Booting the latest kernel with DEBUG_ATOMIC_SLEEP=y on a GICv4.1 enabled
    box, I get the following kernel splat:
    
    [    0.053766] BUG: sleeping function called from invalid context at mm/slab.h:567
    [    0.053767] in_atomic(): 1, irqs_disabled(): 128, non_block: 0, pid: 0, name: swapper/1
    [    0.053769] CPU: 1 PID: 0 Comm: swapper/1 Not tainted 5.8.0-rc3+ #23
    [    0.053770] Call trace:
    [    0.053774]  dump_backtrace+0x0/0x218
    [    0.053775]  show_stack+0x2c/0x38
    [    0.053777]  dump_stack+0xc4/0x10c
    [    0.053779]  ___might_sleep+0xfc/0x140
    [    0.053780]  __might_sleep+0x58/0x90
    [    0.053782]  slab_pre_alloc_hook+0x7c/0x90
    [    0.053783]  kmem_cache_alloc_trace+0x60/0x2f0
    [    0.053785]  its_cpu_init+0x6f4/0xe40
    [    0.053786]  gic_starting_cpu+0x24/0x38
    [    0.053788]  cpuhp_invoke_callback+0xa0/0x710
    [    0.053789]  notify_cpu_starting+0xcc/0xd8
    [    0.053790]  secondary_start_kernel+0x148/0x200
    
     # ./scripts/faddr2line vmlinux its_cpu_init+0x6f4/0xe40
    its_cpu_init+0x6f4/0xe40:
    allocate_vpe_l1_table at drivers/irqchip/irq-gic-v3-its.c:2818
    (inlined by) its_cpu_init_lpis at drivers/irqchip/irq-gic-v3-its.c:3138
    (inlined by) its_cpu_init at drivers/irqchip/irq-gic-v3-its.c:5166
    
    It turned out that we're allocating memory using GFP_KERNEL (may sleep)
    within the CPU hotplug notifier, which is indeed an atomic context. Bad
    thing may happen if we're playing on a system with more than a single
    CommonLPIAff group. Avoid it by turning this into an atomic allocation.
    
    Fixes: 5e5168461c22 ("irqchip/gic-v4.1: VPE table (aka GICR_VPROPBASER) allocation")
    Signed-off-by: Zenghui Yu <yuzenghui@huawei.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200630133746.816-1-yuzenghui@huawei.com
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d1bd7e0ba533a2a6f313579ec9b504f6614c35c4
Author: Zenghui Yu <yuzenghui@huawei.com>
Date:   Tue Jun 30 21:37:46 2020 +0800

    irqchip/gic-v4.1: Use GFP_ATOMIC flag in allocate_vpe_l1_table()
    
    Booting the latest kernel with DEBUG_ATOMIC_SLEEP=y on a GICv4.1 enabled
    box, I get the following kernel splat:
    
    [    0.053766] BUG: sleeping function called from invalid context at mm/slab.h:567
    [    0.053767] in_atomic(): 1, irqs_disabled(): 128, non_block: 0, pid: 0, name: swapper/1
    [    0.053769] CPU: 1 PID: 0 Comm: swapper/1 Not tainted 5.8.0-rc3+ #23
    [    0.053770] Call trace:
    [    0.053774]  dump_backtrace+0x0/0x218
    [    0.053775]  show_stack+0x2c/0x38
    [    0.053777]  dump_stack+0xc4/0x10c
    [    0.053779]  ___might_sleep+0xfc/0x140
    [    0.053780]  __might_sleep+0x58/0x90
    [    0.053782]  slab_pre_alloc_hook+0x7c/0x90
    [    0.053783]  kmem_cache_alloc_trace+0x60/0x2f0
    [    0.053785]  its_cpu_init+0x6f4/0xe40
    [    0.053786]  gic_starting_cpu+0x24/0x38
    [    0.053788]  cpuhp_invoke_callback+0xa0/0x710
    [    0.053789]  notify_cpu_starting+0xcc/0xd8
    [    0.053790]  secondary_start_kernel+0x148/0x200
    
     # ./scripts/faddr2line vmlinux its_cpu_init+0x6f4/0xe40
    its_cpu_init+0x6f4/0xe40:
    allocate_vpe_l1_table at drivers/irqchip/irq-gic-v3-its.c:2818
    (inlined by) its_cpu_init_lpis at drivers/irqchip/irq-gic-v3-its.c:3138
    (inlined by) its_cpu_init at drivers/irqchip/irq-gic-v3-its.c:5166
    
    It turned out that we're allocating memory using GFP_KERNEL (may sleep)
    within the CPU hotplug notifier, which is indeed an atomic context. Bad
    thing may happen if we're playing on a system with more than a single
    CommonLPIAff group. Avoid it by turning this into an atomic allocation.
    
    Fixes: 5e5168461c22 ("irqchip/gic-v4.1: VPE table (aka GICR_VPROPBASER) allocation")
    Signed-off-by: Zenghui Yu <yuzenghui@huawei.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200630133746.816-1-yuzenghui@huawei.com

commit 92fa276bc6c62cff4f4e3f6eaf8bdafd57d381ef
Author: Mike Leach <mike.leach@linaro.org>
Date:   Wed Jul 1 10:08:52 2020 -0600

    coresight: etmv4: Fix CPU power management setup in probe() function
    
    commit 9b6a3f3633a5cc928b78627764793b60cb62e0f6 upstream.
    
    The current probe() function calls a pair of cpuhp_xxx API functions to
    setup CPU hotplug handling. The hotplug lock is held for the duration of
    the two calls and other CPU related code using cpus_read_lock() /
    cpus_read_unlock() calls.
    
    The problem is that on error states, goto: statements bypass the
    cpus_read_unlock() call. This code has increased in complexity as the
    driver has developed.
    
    This patch introduces a pair of helper functions etm4_pm_setup_cpuslocked()
    and etm4_pm_clear() which correct the issues above and group the PM code a
    little better.
    
    The two functions etm4_cpu_pm_register() and etm4_cpu_pm_unregister() are
    dropped as these call cpu_pm_register_notifier() / ..unregister_notifier()
    dependent on CONFIG_CPU_PM - but this define is used to nop these functions
    out in the pm headers - so the wrapper functions are superfluous.
    
    Fixes: f188b5e76aae ("coresight: etm4x: Save/restore state across CPU low power states")
    Fixes: e9f5d63f84fe ("hwtracing/coresight-etm4x: Use cpuhp_setup_state_nocalls_cpuslocked()")
    Fixes: 58eb457be028 ("hwtracing/coresight-etm4x: Convert to hotplug state machine")
    Signed-off-by: Mike Leach <mike.leach@linaro.org>
    Cc: stable <stable@vger.kernel.org>
    Reviewed-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Link: https://lore.kernel.org/r/20200701160852.2782823-3-mathieu.poirier@linaro.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 48f6e7f6d948b56489da027bc3284c709b939d28
Author: Nathan Lynch <nathanl@linux.ibm.com>
Date:   Fri Jun 12 00:12:21 2020 -0500

    powerpc/pseries: remove cede offline state for CPUs
    
    This effectively reverts commit 3aa565f53c39 ("powerpc/pseries: Add
    hooks to put the CPU into an appropriate offline state"), which added
    an offline mode for CPUs which uses the H_CEDE hcall instead of the
    architected stop-self RTAS function in order to facilitate "folding"
    of dedicated mode processors on PowerVM platforms to achieve energy
    savings. This has been the default offline mode since its
    introduction.
    
    There's nothing about stop-self that would prevent the hypervisor from
    achieving the energy savings available via H_CEDE, so the original
    premise of this change appears to be flawed.
    
    I also have encountered the claim that the transition to and from
    ceded state is much faster than stop-self/start-cpu. Certainly we
    would not want to use stop-self as an *idle* mode. That is what H_CEDE
    is for. However, this difference is insignificant in the context of
    Linux CPU hotplug, where the latency of an offline or online operation
    on current systems is on the order of 100ms, mainly attributable to
    all the various subsystems' cpuhp callbacks.
    
    The cede offline mode also prevents accurate accounting, as discussed
    before:
    https://lore.kernel.org/linuxppc-dev/1571740391-3251-1-git-send-email-ego@linux.vnet.ibm.com/
    
    Unconditionally use stop-self to offline processor threads. This is
    the architected method for offlining CPUs on PAPR systems.
    
    The "cede_offline" boot parameter is rendered obsolete.
    
    Removing this code enables the removal of the partition suspend code
    which temporarily onlines all present CPUs.
    
    Fixes: 3aa565f53c39 ("powerpc/pseries: Add hooks to put the CPU into an appropriate offline state")
    Signed-off-by: Nathan Lynch <nathanl@linux.ibm.com>
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200612051238.1007764-2-nathanl@linux.ibm.com

commit 9b6a3f3633a5cc928b78627764793b60cb62e0f6
Author: Mike Leach <mike.leach@linaro.org>
Date:   Wed Jul 1 10:08:52 2020 -0600

    coresight: etmv4: Fix CPU power management setup in probe() function
    
    The current probe() function calls a pair of cpuhp_xxx API functions to
    setup CPU hotplug handling. The hotplug lock is held for the duration of
    the two calls and other CPU related code using cpus_read_lock() /
    cpus_read_unlock() calls.
    
    The problem is that on error states, goto: statements bypass the
    cpus_read_unlock() call. This code has increased in complexity as the
    driver has developed.
    
    This patch introduces a pair of helper functions etm4_pm_setup_cpuslocked()
    and etm4_pm_clear() which correct the issues above and group the PM code a
    little better.
    
    The two functions etm4_cpu_pm_register() and etm4_cpu_pm_unregister() are
    dropped as these call cpu_pm_register_notifier() / ..unregister_notifier()
    dependent on CONFIG_CPU_PM - but this define is used to nop these functions
    out in the pm headers - so the wrapper functions are superfluous.
    
    Fixes: f188b5e76aae ("coresight: etm4x: Save/restore state across CPU low power states")
    Fixes: e9f5d63f84fe ("hwtracing/coresight-etm4x: Use cpuhp_setup_state_nocalls_cpuslocked()")
    Fixes: 58eb457be028 ("hwtracing/coresight-etm4x: Convert to hotplug state machine")
    Signed-off-by: Mike Leach <mike.leach@linaro.org>
    Cc: stable <stable@vger.kernel.org>
    Reviewed-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Link: https://lore.kernel.org/r/20200701160852.2782823-3-mathieu.poirier@linaro.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ccf0ff3530571237859110aac6b07eaf8dae5f3a
Author: Anup Patel <anup.patel@wdc.com>
Date:   Mon Jun 1 10:36:56 2020 +0530

    RISC-V: Don't mark init section as non-executable
    
    commit 4e0f9e3a6104261f25b16fcab02fc96f5666ba11 upstream.
    
    The head text section (i.e. _start, secondary_start_sbi, etc) and the
    init section fall under same page table level-1 mapping.
    
    Currently, the runtime CPU hotplug is broken because we are marking
    init section as non-executable which in-turn marks head text section
    as non-executable.
    
    Further investigating other architectures, it seems marking the init
    section as non-executable is redundant because the init section pages
    are anyway poisoned and freed.
    
    To fix broken runtime CPU hotplug, we simply remove the code marking
    the init section as non-executable.
    
    Fixes: d27c3c90817e ("riscv: add STRICT_KERNEL_RWX support")
    Cc: stable@vger.kernel.org
    Signed-off-by: Anup Patel <anup.patel@wdc.com>
    Reviewed-by: Zong Li <zong.li@sifive.com>
    Reviewed-by: Atish Patra <atish.patra@wdc.com>
    Signed-off-by: Palmer Dabbelt <palmerdabbelt@google.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a9429089d3e822d45be01a9635f0685174508fd3
Merge: 076f14be7fc9 7ccddc4613db
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 13 10:21:00 2020 -0700

    Merge tag 'ras-core-2020-06-12' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 RAS updates from Thomas Gleixner:
     "RAS updates from Borislav Petkov:
    
       - Unmap a whole guest page if an MCE is encountered in it to avoid
         follow-on MCEs leading to the guest crashing, by Tony Luck.
    
         This change collided with the entry changes and the merge
         resolution would have been rather unpleasant. To avoid that the
         entry branch was merged in before applying this. The resulting code
         did not change over the rebase.
    
       - AMD MCE error thresholding machinery cleanup and hotplug
         sanitization, by Thomas Gleixner.
    
       - Change the MCE notifiers to denote whether they have handled the
         error and not break the chain early by returning NOTIFY_STOP, thus
         giving the opportunity for the later handlers in the chain to see
         it. By Tony Luck.
    
       - Add AMD family 0x17, models 0x60-6f support, by Alexander Monakov.
    
       - Last but not least, the usual round of fixes and improvements"
    
    * tag 'ras-core-2020-06-12' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      x86/mce/dev-mcelog: Fix -Wstringop-truncation warning about strncpy()
      x86/{mce,mm}: Unmap the entire page if the whole page is affected and poisoned
      EDAC/amd64: Add AMD family 17h model 60h PCI IDs
      hwmon: (k10temp) Add AMD family 17h model 60h PCI match
      x86/amd_nb: Add AMD family 17h model 60h PCI IDs
      x86/mcelog: Add compat_ioctl for 32-bit mcelog support
      x86/mce: Drop bogus comment about mce.kflags
      x86/mce: Fixup exception only for the correct MCEs
      EDAC: Drop the EDAC report status checks
      x86/mce: Add mce=print_all option
      x86/mce: Change default MCE logger to check mce->kflags
      x86/mce: Fix all mce notifiers to update the mce->kflags bitmask
      x86/mce: Add a struct mce.kflags field
      x86/mce: Convert the CEC to use the MCE notifier
      x86/mce: Rename "first" function as "early"
      x86/mce/amd, edac: Remove report_gart_errors
      x86/mce/amd: Make threshold bank setting hotplug robust
      x86/mce/amd: Cleanup threshold device remove path
      x86/mce/amd: Straighten CPU hotplug path
      x86/mce/amd: Sanitize thresholding device creation hotplug path
      ...

commit cd16ed33c3c618930ccda7049dcea05ee707a9c0
Merge: 55d728b2b05f 01f76386b0ac
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 11 12:55:20 2020 -0700

    Merge tag 'riscv-for-linus-5.8-mw1' of git://git.kernel.org/pub/scm/linux/kernel/git/riscv/linux
    
    Pull more RISC-V updates from Palmer Dabbelt:
    
     - Kconfig select statements are now sorted alphanumerically
    
     - first-level interrupts are now handled via a full irqchip driver
    
     - CPU hotplug is fixed
    
     - vDSO calls now use the common vDSO infrastructure
    
    * tag 'riscv-for-linus-5.8-mw1' of git://git.kernel.org/pub/scm/linux/kernel/git/riscv/linux:
      riscv: set the permission of vdso_data to read-only
      riscv: use vDSO common flow to reduce the latency of the time-related functions
      riscv: fix build warning of missing prototypes
      RISC-V: Don't mark init section as non-executable
      RISC-V: Force select RISCV_INTC for CONFIG_RISCV
      RISC-V: Remove do_IRQ() function
      clocksource/drivers/timer-riscv: Use per-CPU timer interrupt
      irqchip: RISC-V per-HART local interrupt controller driver
      RISC-V: Rename and move plic_find_hart_id() to arch directory
      RISC-V: self-contained IPI handling routine
      RISC-V: Sort select statements alphanumerically

commit 4e0f9e3a6104261f25b16fcab02fc96f5666ba11
Author: Anup Patel <anup.patel@wdc.com>
Date:   Mon Jun 1 10:36:56 2020 +0530

    RISC-V: Don't mark init section as non-executable
    
    The head text section (i.e. _start, secondary_start_sbi, etc) and the
    init section fall under same page table level-1 mapping.
    
    Currently, the runtime CPU hotplug is broken because we are marking
    init section as non-executable which in-turn marks head text section
    as non-executable.
    
    Further investigating other architectures, it seems marking the init
    section as non-executable is redundant because the init section pages
    are anyway poisoned and freed.
    
    To fix broken runtime CPU hotplug, we simply remove the code marking
    the init section as non-executable.
    
    Fixes: d27c3c90817e ("riscv: add STRICT_KERNEL_RWX support")
    Cc: stable@vger.kernel.org
    Signed-off-by: Anup Patel <anup.patel@wdc.com>
    Reviewed-by: Zong Li <zong.li@sifive.com>
    Reviewed-by: Atish Patra <atish.patra@wdc.com>
    Signed-off-by: Palmer Dabbelt <palmerdabbelt@google.com>

commit 6b2591c21273ebf65c13dae5d260ce88f0f197dd
Merge: f1e455352b6f afaa33da08ab
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 15:00:05 2020 -0700

    Merge tag 'hyperv-next-signed' of git://git.kernel.org/pub/scm/linux/kernel/git/hyperv/linux
    
    Pull hyper-v updates from Wei Liu:
    
     - a series from Andrea to support channel reassignment
    
     - a series from Vitaly to clean up Vmbus message handling
    
     - a series from Michael to clean up and augment hyperv-tlfs.h
    
     - patches from Andy to clean up GUID usage in Hyper-V code
    
     - a few other misc patches
    
    * tag 'hyperv-next-signed' of git://git.kernel.org/pub/scm/linux/kernel/git/hyperv/linux: (29 commits)
      Drivers: hv: vmbus: Resolve more races involving init_vp_index()
      Drivers: hv: vmbus: Resolve race between init_vp_index() and CPU hotplug
      vmbus: Replace zero-length array with flexible-array
      Driver: hv: vmbus: drop a no long applicable comment
      hyper-v: Switch to use UUID types directly
      hyper-v: Replace open-coded variant of %*phN specifier
      hyper-v: Supply GUID pointer to printf() like functions
      hyper-v: Use UUID API for exporting the GUID (part 2)
      asm-generic/hyperv: Add definitions for Get/SetVpRegister hypercalls
      x86/hyperv: Split hyperv-tlfs.h into arch dependent and independent files
      x86/hyperv: Remove HV_PROCESSOR_POWER_STATE #defines
      KVM: x86: hyperv: Remove duplicate definitions of Reference TSC Page
      drivers: hv: remove redundant assignment to pointer primary_channel
      scsi: storvsc: Re-init stor_chns when a channel interrupt is re-assigned
      Drivers: hv: vmbus: Introduce the CHANNELMSG_MODIFYCHANNEL message type
      Drivers: hv: vmbus: Synchronize init_vp_index() vs. CPU hotplug
      Drivers: hv: vmbus: Remove the unused HV_LOCALIZED channel affinity logic
      PCI: hv: Prepare hv_compose_msi_msg() for the VMBus-channel-interrupt-to-vCPU reassignment functionality
      Drivers: hv: vmbus: Use a spin lock for synchronizing channel scheduling vs. channel removal
      hv_utils: Always execute the fcopy and vss callbacks in a tasklet
      ...

commit 750a02ab8d3c49ca7d23102be90d3d1db19e2827
Merge: 1966391fa576 abb30460bda2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 2 15:29:19 2020 -0700

    Merge tag 'for-5.8/block-2020-06-01' of git://git.kernel.dk/linux-block
    
    Pull block updates from Jens Axboe:
     "Core block changes that have been queued up for this release:
    
       - Remove dead blk-throttle and blk-wbt code (Guoqing)
    
       - Include pid in blktrace note traces (Jan)
    
       - Don't spew I/O errors on wouldblock termination (me)
    
       - Zone append addition (Johannes, Keith, Damien)
    
       - IO accounting improvements (Konstantin, Christoph)
    
       - blk-mq hardware map update improvements (Ming)
    
       - Scheduler dispatch improvement (Salman)
    
       - Inline block encryption support (Satya)
    
       - Request map fixes and improvements (Weiping)
    
       - blk-iocost tweaks (Tejun)
    
       - Fix for timeout failing with error injection (Keith)
    
       - Queue re-run fixes (Douglas)
    
       - CPU hotplug improvements (Christoph)
    
       - Queue entry/exit improvements (Christoph)
    
       - Move DMA drain handling to the few drivers that use it (Christoph)
    
       - Partition handling cleanups (Christoph)"
    
    * tag 'for-5.8/block-2020-06-01' of git://git.kernel.dk/linux-block: (127 commits)
      block: mark bio_wouldblock_error() bio with BIO_QUIET
      blk-wbt: rename __wbt_update_limits to wbt_update_limits
      blk-wbt: remove wbt_update_limits
      blk-throttle: remove tg_drain_bios
      blk-throttle: remove blk_throtl_drain
      null_blk: force complete for timeout request
      blk-mq: drain I/O when all CPUs in a hctx are offline
      blk-mq: add blk_mq_all_tag_iter
      blk-mq: open code __blk_mq_alloc_request in blk_mq_alloc_request_hctx
      blk-mq: use BLK_MQ_NO_TAG in more places
      blk-mq: rename BLK_MQ_TAG_FAIL to BLK_MQ_NO_TAG
      blk-mq: move more request initialization to blk_mq_rq_ctx_init
      blk-mq: simplify the blk_mq_get_request calling convention
      blk-mq: remove the bio argument to ->prepare_request
      nvme: force complete cancelled requests
      blk-mq: blk-mq: provide forced completion method
      block: fix a warning when blkdev.h is included for !CONFIG_BLOCK builds
      block: blk-crypto-fallback: remove redundant initialization of variable err
      block: reduce part_stat_lock() scope
      block: use __this_cpu_add() instead of access by smp_processor_id()
      ...

commit 342403bcb4dfe41324a0f6f4cb5a8d324f31c725
Merge: 09cda9a71350 fd868f148189 b130a8f70cbb 184dbc152e39 357dd8a2aff2 4fc92254bf86 10f6cd2af21b c0fc00ec6304 472de63b0b83 269fd61e15d7 7e9f5e6629f6
Author: Will Deacon <will@kernel.org>
Date:   Thu May 28 17:47:34 2020 +0100

    Merge branches 'for-next/acpi', 'for-next/bpf', 'for-next/cpufeature', 'for-next/docs', 'for-next/kconfig', 'for-next/misc', 'for-next/perf', 'for-next/ptr-auth', 'for-next/sdei', 'for-next/smccc' and 'for-next/vdso' into for-next/core
    
    ACPI and IORT updates
    (Lorenzo Pieralisi)
    * for-next/acpi:
      ACPI/IORT: Remove the unused __get_pci_rid()
      ACPI/IORT: Fix PMCG node single ID mapping handling
      ACPI: IORT: Add comments for not calling acpi_put_table()
      ACPI: GTDT: Put GTDT table after parsing
      ACPI: IORT: Add extra message "applying workaround" for off-by-1 issue
      ACPI/IORT: work around num_ids ambiguity
      Revert "ACPI/IORT: Fix 'Number of IDs' handling in iort_id_map()"
      ACPI/IORT: take _DMA methods into account for named components
    
    BPF JIT optimisations for immediate value generation
    (Luke Nelson)
    * for-next/bpf:
      bpf, arm64: Optimize ADD,SUB,JMP BPF_K using arm64 add/sub immediates
      bpf, arm64: Optimize AND,OR,XOR,JSET BPF_K using arm64 logical immediates
      arm64: insn: Fix two bugs in encoding 32-bit logical immediates
    
    Addition of new CPU ID register fields and removal of some benign sanity checks
    (Anshuman Khandual and others)
    * for-next/cpufeature: (27 commits)
      KVM: arm64: Check advertised Stage-2 page size capability
      arm64/cpufeature: Add get_arm64_ftr_reg_nowarn()
      arm64/cpuinfo: Add ID_MMFR4_EL1 into the cpuinfo_arm64 context
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR1 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR0 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64ISAR0 register
      arm64/cpufeature: Add remaining feature bits in ID_MMFR4 register
      arm64/cpufeature: Add remaining feature bits in ID_PFR0 register
      arm64/cpufeature: Introduce ID_MMFR5 CPU register
      arm64/cpufeature: Introduce ID_DFR1 CPU register
      arm64/cpufeature: Introduce ID_PFR2 CPU register
      arm64/cpufeature: Make doublelock a signed feature in ID_AA64DFR0
      arm64/cpufeature: Drop TraceFilt feature exposure from ID_DFR0 register
      arm64/cpufeature: Add explicit ftr_id_isar0[] for ID_ISAR0 register
      arm64/cpufeature: Drop open encodings while extracting parange
      arm64/cpufeature: Validate hypervisor capabilities during CPU hotplug
      arm64: cpufeature: Group indexed system register definitions by name
      arm64: cpufeature: Extend comment to describe absence of field info
      arm64: drop duplicate definitions of ID_AA64MMFR0_TGRAN constants
      arm64: cpufeature: Add an overview comment for the cpufeature framework
      ...
    
    Minor documentation tweaks for silicon errata and booting requirements
    (Rob Herring and Will Deacon)
    * for-next/docs:
      arm64: silicon-errata.rst: Sort the Cortex-A55 entries
      arm64: docs: Mandate that the I-cache doesn't hold stale kernel text
    
    Minor Kconfig cleanups
    (Geert Uytterhoeven)
    * for-next/kconfig:
      arm64: cpufeature: Add "or" to mitigations for multiple errata
      arm64: Sort vendor-specific errata
    
    Miscellaneous updates
    (Ard Biesheuvel and others)
    * for-next/misc:
      arm64: mm: Add asid_gen_match() helper
      arm64: stacktrace: Factor out some common code into on_stack()
      arm64: Call debug_traps_init() from trap_init() to help early kgdb
      arm64: cacheflush: Fix KGDB trap detection
      arm64/cpuinfo: Move device_initcall() near cpuinfo_regs_init()
      arm64: kexec_file: print appropriate variable
      arm: mm: use __pfn_to_section() to get mem_section
      arm64: Reorder the macro arguments in the copy routines
      efi/libstub/arm64: align PE/COFF sections to segment alignment
      KVM: arm64: Drop PTE_S2_MEMATTR_MASK
      arm64/kernel: Fix range on invalidating dcache for boot page tables
      arm64: set TEXT_OFFSET to 0x0 in preparation for removing it entirely
      arm64: lib: Consistently enable crc32 extension
      arm64/mm: Use phys_to_page() to access pgtable memory
      arm64: smp: Make cpus_stuck_in_kernel static
      arm64: entry: remove unneeded semicolon in el1_sync_handler()
      arm64/kernel: vmlinux.lds: drop redundant discard/keep macros
      arm64: drop GZFLAGS definition and export
      arm64: kexec_file: Avoid temp buffer for RNG seed
      arm64: rename stext to primary_entry
    
    Perf PMU driver updates
    (Tang Bin and others)
    * for-next/perf:
      pmu/smmuv3: Clear IRQ affinity hint on device removal
      drivers/perf: hisi: Permit modular builds of HiSilicon uncore drivers
      drivers/perf: hisi: Fix typo in events attribute array
      drivers/perf: arm_spe_pmu: Avoid duplicate printouts
      drivers/perf: arm_dsu_pmu: Avoid duplicate printouts
    
    Pointer authentication updates and support for vmcoreinfo
    (Amit Daniel Kachhap and Mark Rutland)
    * for-next/ptr-auth:
      Documentation/vmcoreinfo: Add documentation for 'KERNELPACMASK'
      arm64/crash_core: Export KERNELPACMASK in vmcoreinfo
      arm64: simplify ptrauth initialization
      arm64: remove ptrauth_keys_install_kernel sync arg
    
    SDEI cleanup and non-critical fixes
    (James Morse and others)
    * for-next/sdei:
      firmware: arm_sdei: Document the motivation behind these set_fs() calls
      firmware: arm_sdei: remove unused interfaces
      firmware: arm_sdei: Put the SDEI table after using it
      firmware: arm_sdei: Drop check for /firmware/ node and always register driver
    
    SMCCC updates and refactoring
    (Sudeep Holla)
    * for-next/smccc:
      firmware: smccc: Fix missing prototype warning for arm_smccc_version_init
      firmware: smccc: Add function to fetch SMCCC version
      firmware: smccc: Refactor SMCCC specific bits into separate file
      firmware: smccc: Drop smccc_version enum and use ARM_SMCCC_VERSION_1_x instead
      firmware: smccc: Add the definition for SMCCCv1.2 version/error codes
      firmware: smccc: Update link to latest SMCCC specification
      firmware: smccc: Add HAVE_ARM_SMCCC_DISCOVERY to identify SMCCC v1.1 and above
    
    vDSO cleanup and non-critical fixes
    (Mark Rutland and Vincenzo Frascino)
    * for-next/vdso:
      arm64: vdso: Add --eh-frame-hdr to ldflags
      arm64: vdso: use consistent 'map' nomenclature
      arm64: vdso: use consistent 'abi' nomenclature
      arm64: vdso: simplify arch_vdso_type ifdeffery
      arm64: vdso: remove aarch32_vdso_pages[]
      arm64: vdso: Add '-Bsymbolic' to ldflags

commit 1538674ceea853dc922c0a56dda331fb9b359d2e
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Thu May 21 16:40:50 2020 -0400

    padata: initialize pd->cpu with effective cpumask
    
    [ Upstream commit ec9c7d19336ee98ecba8de80128aa405c45feebb ]
    
    Exercising CPU hotplug on a 5.2 kernel with recent padata fixes from
    cryptodev-2.6.git in an 8-CPU kvm guest...
    
        # modprobe tcrypt alg="pcrypt(rfc4106(gcm(aes)))" type=3
        # echo 0 > /sys/devices/system/cpu/cpu1/online
        # echo c > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
        # modprobe tcrypt mode=215
    
    ...caused the following crash:
    
        BUG: kernel NULL pointer dereference, address: 0000000000000000
        #PF: supervisor read access in kernel mode
        #PF: error_code(0x0000) - not-present page
        PGD 0 P4D 0
        Oops: 0000 [#1] SMP PTI
        CPU: 2 PID: 134 Comm: kworker/2:2 Not tainted 5.2.0-padata-base+ #7
        Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-<snip>
        Workqueue: pencrypt padata_parallel_worker
        RIP: 0010:padata_reorder+0xcb/0x180
        ...
        Call Trace:
         padata_do_serial+0x57/0x60
         pcrypt_aead_enc+0x3a/0x50 [pcrypt]
         padata_parallel_worker+0x9b/0xe0
         process_one_work+0x1b5/0x3f0
         worker_thread+0x4a/0x3c0
         ...
    
    In padata_alloc_pd, pd->cpu is set using the user-supplied cpumask
    instead of the effective cpumask, and in this case cpumask_first picked
    an offline CPU.
    
    The offline CPU's reorder->list.next is NULL in padata_reorder because
    the list wasn't initialized in padata_init_pqueues, which only operates
    on CPUs in the effective mask.
    
    Fix by using the effective mask in padata_alloc_pd.
    
    Fixes: 6fc4dbcf0276 ("padata: Replace delayed timer with immediate workqueue in padata_reorder")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 5ced157512660b2c5687199f4462578d4d064500
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Thu May 21 16:46:57 2020 -0400

    padata: initialize pd->cpu with effective cpumask
    
    [ Upstream commit ec9c7d19336ee98ecba8de80128aa405c45feebb ]
    
    Exercising CPU hotplug on a 5.2 kernel with recent padata fixes from
    cryptodev-2.6.git in an 8-CPU kvm guest...
    
        # modprobe tcrypt alg="pcrypt(rfc4106(gcm(aes)))" type=3
        # echo 0 > /sys/devices/system/cpu/cpu1/online
        # echo c > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
        # modprobe tcrypt mode=215
    
    ...caused the following crash:
    
        BUG: kernel NULL pointer dereference, address: 0000000000000000
        #PF: supervisor read access in kernel mode
        #PF: error_code(0x0000) - not-present page
        PGD 0 P4D 0
        Oops: 0000 [#1] SMP PTI
        CPU: 2 PID: 134 Comm: kworker/2:2 Not tainted 5.2.0-padata-base+ #7
        Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-<snip>
        Workqueue: pencrypt padata_parallel_worker
        RIP: 0010:padata_reorder+0xcb/0x180
        ...
        Call Trace:
         padata_do_serial+0x57/0x60
         pcrypt_aead_enc+0x3a/0x50 [pcrypt]
         padata_parallel_worker+0x9b/0xe0
         process_one_work+0x1b5/0x3f0
         worker_thread+0x4a/0x3c0
         ...
    
    In padata_alloc_pd, pd->cpu is set using the user-supplied cpumask
    instead of the effective cpumask, and in this case cpumask_first picked
    an offline CPU.
    
    The offline CPU's reorder->list.next is NULL in padata_reorder because
    the list wasn't initialized in padata_init_pqueues, which only operates
    on CPUs in the effective mask.
    
    Fix by using the effective mask in padata_alloc_pd.
    
    Fixes: 6fc4dbcf0276 ("padata: Replace delayed timer with immediate workqueue in padata_reorder")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 52a3af1c9321ff2bc7d91b2465af9843f1afabf3
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Thu May 21 16:48:46 2020 -0400

    padata: initialize pd->cpu with effective cpumask
    
    [ Upstream commit ec9c7d19336ee98ecba8de80128aa405c45feebb ]
    
    Exercising CPU hotplug on a 5.2 kernel with recent padata fixes from
    cryptodev-2.6.git in an 8-CPU kvm guest...
    
        # modprobe tcrypt alg="pcrypt(rfc4106(gcm(aes)))" type=3
        # echo 0 > /sys/devices/system/cpu/cpu1/online
        # echo c > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
        # modprobe tcrypt mode=215
    
    ...caused the following crash:
    
        BUG: kernel NULL pointer dereference, address: 0000000000000000
        #PF: supervisor read access in kernel mode
        #PF: error_code(0x0000) - not-present page
        PGD 0 P4D 0
        Oops: 0000 [#1] SMP PTI
        CPU: 2 PID: 134 Comm: kworker/2:2 Not tainted 5.2.0-padata-base+ #7
        Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-<snip>
        Workqueue: pencrypt padata_parallel_worker
        RIP: 0010:padata_reorder+0xcb/0x180
        ...
        Call Trace:
         padata_do_serial+0x57/0x60
         pcrypt_aead_enc+0x3a/0x50 [pcrypt]
         padata_parallel_worker+0x9b/0xe0
         process_one_work+0x1b5/0x3f0
         worker_thread+0x4a/0x3c0
         ...
    
    In padata_alloc_pd, pd->cpu is set using the user-supplied cpumask
    instead of the effective cpumask, and in this case cpumask_first picked
    an offline CPU.
    
    The offline CPU's reorder->list.next is NULL in padata_reorder because
    the list wasn't initialized in padata_init_pqueues, which only operates
    on CPUs in the effective mask.
    
    Fix by using the effective mask in padata_alloc_pd.
    
    Fixes: 6fc4dbcf0276 ("padata: Replace delayed timer with immediate workqueue in padata_reorder")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit d08fb9f169eb5b2660da3f6497d5f226673bee46
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Thu May 21 16:51:44 2020 -0400

    padata: initialize pd->cpu with effective cpumask
    
    [ Upstream commit ec9c7d19336ee98ecba8de80128aa405c45feebb ]
    
    Exercising CPU hotplug on a 5.2 kernel with recent padata fixes from
    cryptodev-2.6.git in an 8-CPU kvm guest...
    
        # modprobe tcrypt alg="pcrypt(rfc4106(gcm(aes)))" type=3
        # echo 0 > /sys/devices/system/cpu/cpu1/online
        # echo c > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
        # modprobe tcrypt mode=215
    
    ...caused the following crash:
    
        BUG: kernel NULL pointer dereference, address: 0000000000000000
        #PF: supervisor read access in kernel mode
        #PF: error_code(0x0000) - not-present page
        PGD 0 P4D 0
        Oops: 0000 [#1] SMP PTI
        CPU: 2 PID: 134 Comm: kworker/2:2 Not tainted 5.2.0-padata-base+ #7
        Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-<snip>
        Workqueue: pencrypt padata_parallel_worker
        RIP: 0010:padata_reorder+0xcb/0x180
        ...
        Call Trace:
         padata_do_serial+0x57/0x60
         pcrypt_aead_enc+0x3a/0x50 [pcrypt]
         padata_parallel_worker+0x9b/0xe0
         process_one_work+0x1b5/0x3f0
         worker_thread+0x4a/0x3c0
         ...
    
    In padata_alloc_pd, pd->cpu is set using the user-supplied cpumask
    instead of the effective cpumask, and in this case cpumask_first picked
    an offline CPU.
    
    The offline CPU's reorder->list.next is NULL in padata_reorder because
    the list wasn't initialized in padata_init_pqueues, which only operates
    on CPUs in the effective mask.
    
    Fix by using the effective mask in padata_alloc_pd.
    
    Fixes: 6fc4dbcf0276 ("padata: Replace delayed timer with immediate workqueue in padata_reorder")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit a949e86c0d7802c05b2ae726a84fae89ddb5be7d
Author: Andrea Parri (Microsoft) <parri.andrea@gmail.com>
Date:   Fri May 22 19:19:00 2020 +0200

    Drivers: hv: vmbus: Resolve race between init_vp_index() and CPU hotplug
    
    vmbus_process_offer() does two things (among others):
    
     1) first, it sets the channel's target CPU with cpu_hotplug_lock;
     2) it then adds the channel to the channel list(s) with channel_mutex.
    
    Since cpu_hotplug_lock is released before (2), the channel's target CPU
    (as designated in (1)) can be deemed "free" by hv_synic_cleanup() and go
    offline before the channel is added to the list.
    
    Fix the race condition by "extending" the cpu_hotplug_lock critical
    section to include (2) (and (1)), nesting the channel_mutex critical
    section within the cpu_hotplug_lock critical section as done elsewhere
    (hv_synic_cleanup(), target_cpu_store()) in the hyperv drivers code.
    
    Move even further by extending the channel_mutex critical section to
    include (1) (and (2)): this change allows to remove (the now redundant)
    bind_channel_to_cpu_lock, and generally simplifies the handling of the
    target CPUs (that are now always modified with channel_mutex held).
    
    Fixes: d570aec0f2154e ("Drivers: hv: vmbus: Synchronize init_vp_index() vs. CPU hotplug")
    Signed-off-by: Andrea Parri (Microsoft) <parri.andrea@gmail.com>
    Reviewed-by: Michael Kelley <mikelley@microsoft.com>
    Link: https://lore.kernel.org/r/20200522171901.204127-2-parri.andrea@gmail.com
    Signed-off-by: Wei Liu <wei.liu@kernel.org>

commit 1c115b879a9cf24999a0d896bc5d9af9049a6258
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Tue Dec 3 14:31:11 2019 -0500

    padata: always acquire cpu_hotplug_lock before pinst->lock
    
    commit 38228e8848cd7dd86ccb90406af32de0cad24be3 upstream.
    
    lockdep complains when padata's paths to update cpumasks via CPU hotplug
    and sysfs are both taken:
    
      # echo 0 > /sys/devices/system/cpu/cpu1/online
      # echo ff > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
    
      ======================================================
      WARNING: possible circular locking dependency detected
      5.4.0-rc8-padata-cpuhp-v3+ #1 Not tainted
      ------------------------------------------------------
      bash/205 is trying to acquire lock:
      ffffffff8286bcd0 (cpu_hotplug_lock.rw_sem){++++}, at: padata_set_cpumask+0x2b/0x120
    
      but task is already holding lock:
      ffff8880001abfa0 (&pinst->lock){+.+.}, at: padata_set_cpumask+0x26/0x120
    
      which lock already depends on the new lock.
    
    padata doesn't take cpu_hotplug_lock and pinst->lock in a consistent
    order.  Which should be first?  CPU hotplug calls into padata with
    cpu_hotplug_lock already held, so it should have priority.
    
    Fixes: 6751fb3c0e0c ("padata: Use get_online_cpus/put_online_cpus")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Eric Biggers <ebiggers@kernel.org>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit abf56e87d70c00ffca2b71bd22cd25b34b886bcd
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Thu Aug 8 12:05:35 2019 -0400

    padata: initialize pd->cpu with effective cpumask
    
    commit ec9c7d19336ee98ecba8de80128aa405c45feebb upstream.
    
    Exercising CPU hotplug on a 5.2 kernel with recent padata fixes from
    cryptodev-2.6.git in an 8-CPU kvm guest...
    
        # modprobe tcrypt alg="pcrypt(rfc4106(gcm(aes)))" type=3
        # echo 0 > /sys/devices/system/cpu/cpu1/online
        # echo c > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
        # modprobe tcrypt mode=215
    
    ...caused the following crash:
    
        BUG: kernel NULL pointer dereference, address: 0000000000000000
        #PF: supervisor read access in kernel mode
        #PF: error_code(0x0000) - not-present page
        PGD 0 P4D 0
        Oops: 0000 [#1] SMP PTI
        CPU: 2 PID: 134 Comm: kworker/2:2 Not tainted 5.2.0-padata-base+ #7
        Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-<snip>
        Workqueue: pencrypt padata_parallel_worker
        RIP: 0010:padata_reorder+0xcb/0x180
        ...
        Call Trace:
         padata_do_serial+0x57/0x60
         pcrypt_aead_enc+0x3a/0x50 [pcrypt]
         padata_parallel_worker+0x9b/0xe0
         process_one_work+0x1b5/0x3f0
         worker_thread+0x4a/0x3c0
         ...
    
    In padata_alloc_pd, pd->cpu is set using the user-supplied cpumask
    instead of the effective cpumask, and in this case cpumask_first picked
    an offline CPU.
    
    The offline CPU's reorder->list.next is NULL in padata_reorder because
    the list wasn't initialized in padata_init_pqueues, which only operates
    on CPUs in the effective mask.
    
    Fix by using the effective mask in padata_alloc_pd.
    
    Fixes: 6fc4dbcf0276 ("padata: Replace delayed timer with immediate workqueue in padata_reorder")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit c73433fc630cda102f6527d4e5dfd289a9baec08
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue May 12 07:27:27 2020 +0530

    arm64/cpufeature: Validate hypervisor capabilities during CPU hotplug
    
    This validates hypervisor capabilities like VMID width, IPA range for any
    hot plug CPU against system finalized values. KVM's view of the IPA space
    is used while allowing a given CPU to come up. While here, it factors out
    get_vmid_bits() for general use.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: kvmarm@lists.cs.columbia.edu
    Cc: linux-kernel@vger.kernel.org
    
    Suggested-by: Suzuki Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/1589248647-22925-1-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

commit f06f7a923d1de2542cdcc5a9a157df5eb072913b
Author: Gabriel Krisman Bertazi <krisman@linux.vnet.ibm.com>
Date:   Mon Aug 1 08:23:39 2016 -0600

    blk-mq: Allow timeouts to run while queue is freezing
    
    commit 71f79fb3179e69b0c1448a2101a866d871c66e7f upstream.
    
    In case a submitted request gets stuck for some reason, the block layer
    can prevent the request starvation by starting the scheduled timeout work.
    If this stuck request occurs at the same time another thread has started
    a queue freeze, the blk_mq_timeout_work will not be able to acquire the
    queue reference and will return silently, thus not issuing the timeout.
    But since the request is already holding a q_usage_counter reference and
    is unable to complete, it will never release its reference, preventing
    the queue from completing the freeze started by first thread.  This puts
    the request_queue in a hung state, forever waiting for the freeze
    completion.
    
    This was observed while running IO to a NVMe device at the same time we
    toggled the CPU hotplug code. Eventually, once a request got stuck
    requiring a timeout during a queue freeze, we saw the CPU Hotplug
    notification code get stuck inside blk_mq_freeze_queue_wait, as shown in
    the trace below.
    
    [c000000deaf13690] [c000000deaf13738] 0xc000000deaf13738 (unreliable)
    [c000000deaf13860] [c000000000015ce8] __switch_to+0x1f8/0x350
    [c000000deaf138b0] [c000000000ade0e4] __schedule+0x314/0x990
    [c000000deaf13940] [c000000000ade7a8] schedule+0x48/0xc0
    [c000000deaf13970] [c0000000005492a4] blk_mq_freeze_queue_wait+0x74/0x110
    [c000000deaf139e0] [c00000000054b6a8] blk_mq_queue_reinit_notify+0x1a8/0x2e0
    [c000000deaf13a40] [c0000000000e7878] notifier_call_chain+0x98/0x100
    [c000000deaf13a90] [c0000000000b8e08] cpu_notify_nofail+0x48/0xa0
    [c000000deaf13ac0] [c0000000000b92f0] _cpu_down+0x2a0/0x400
    [c000000deaf13b90] [c0000000000b94a8] cpu_down+0x58/0xa0
    [c000000deaf13bc0] [c0000000006d5dcc] cpu_subsys_offline+0x2c/0x50
    [c000000deaf13bf0] [c0000000006cd244] device_offline+0x104/0x140
    [c000000deaf13c30] [c0000000006cd40c] online_store+0x6c/0xc0
    [c000000deaf13c80] [c0000000006c8c78] dev_attr_store+0x68/0xa0
    [c000000deaf13cc0] [c0000000003974d0] sysfs_kf_write+0x80/0xb0
    [c000000deaf13d00] [c0000000003963e8] kernfs_fop_write+0x188/0x200
    [c000000deaf13d50] [c0000000002e0f6c] __vfs_write+0x6c/0xe0
    [c000000deaf13d90] [c0000000002e1ca0] vfs_write+0xc0/0x230
    [c000000deaf13de0] [c0000000002e2cdc] SyS_write+0x6c/0x110
    [c000000deaf13e30] [c000000000009204] system_call+0x38/0xb4
    
    The fix is to allow the timeout work to execute in the window between
    dropping the initial refcount reference and the release of the last
    reference, which actually marks the freeze completion.  This can be
    achieved with percpu_refcount_tryget, which does not require the counter
    to be alive.  This way the timeout work can do it's job and terminate a
    stuck request even during a freeze, returning its reference and avoiding
    the deadlock.
    
    Allowing the timeout to run is just a part of the fix, since for some
    devices, we might get stuck again inside the device driver's timeout
    handler, should it attempt to allocate a new request in that path -
    which is a quite common action for Abort commands, which need to be sent
    after a timeout.  In NVMe, for instance, we call blk_mq_alloc_request
    from inside the timeout handler, which will fail during a freeze, since
    it also tries to acquire a queue reference.
    
    I considered a similar change to blk_mq_alloc_request as a generic
    solution for further device driver hangs, but we can't do that, since it
    would allow new requests to disturb the freeze process.  I thought about
    creating a new function in the block layer to support unfreezable
    requests for these occasions, but after working on it for a while, I
    feel like this should be handled in a per-driver basis.  I'm now
    experimenting with changes to the NVMe timeout path, but I'm open to
    suggestions of ways to make this generic.
    
    Signed-off-by: Gabriel Krisman Bertazi <krisman@linux.vnet.ibm.com>
    Cc: Brian King <brking@linux.vnet.ibm.com>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: linux-nvme@lists.infradead.org
    Cc: linux-block@vger.kernel.org
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Giuliano Procida <gprocida@google.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e9b880581d555c8f7b58c7d19cc3f8f9016a1b5f
Author: Mike Leach <mike.leach@linaro.org>
Date:   Mon May 18 12:02:41 2020 -0600

    coresight: cti: Add CPU Hotplug handling to CTI driver
    
    Adds registration of CPU start and stop functions to CPU hotplug
    mechanisms - for any CPU bound CTI.
    
    Sets CTI powered flag according to state.
    Will enable CTI on CPU start if there are existing enable requests.
    
    Signed-off-by: Mike Leach <mike.leach@linaro.org>
    Signed-off-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Link: https://lore.kernel.org/r/20200518180242.7916-23-mathieu.poirier@linaro.org
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4cf3afdf9794ebafbdee83507a7f3f2e09f8fa35
Author: James Hogan <james.hogan@imgtec.com>
Date:   Wed Jul 13 14:12:45 2016 +0100

    MIPS: SMP: Update cpu_foreign_map on CPU disable
    
    commit 826e99be6ab5189dbfb096389016ffb8d20a683e upstream.
    
    When a CPU is disabled via CPU hotplug, cpu_foreign_map is not updated.
    This could result in cache management SMP calls being sent to offline
    CPUs instead of online siblings in the same core.
    
    Add a call to calculate_cpu_foreign_map() in the various MIPS cpu
    disable callbacks after set_cpu_online(). All cases are updated for
    consistency and to keep cpu_foreign_map strictly up to date, not just
    those which may support hardware multithreading.
    
    Fixes: cccf34e9411c ("MIPS: c-r4k: Fix cache flushing for MT cores")
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Kevin Cernekee <cernekee@gmail.com>
    Cc: Florian Fainelli <f.fainelli@gmail.com>
    Cc: Huacai Chen <chenhc@lemote.com>
    Cc: Hongliang Tao <taohl@lemote.com>
    Cc: Hua Yan <yanh@lemote.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/13799/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 81b4a7bc3b54b0b839dbf3d2b8c9a353ae910688
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Sun Mar 22 10:10:07 2020 -0700

    rcu-tasks: Disable CPU hotplug across RCU tasks trace scans
    
    This commit disables CPU hotplug across RCU tasks trace scans, which
    is a first step towards correctly recognizing idle tasks "running" on
    offline CPUs.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit 8f0f235c52494fa43305aa659fe309bd6bc29767
Author: Qian Cai <cai@lca.pw>
Date:   Fri Apr 3 10:03:45 2020 -0400

    x86: ACPI: fix CPU hotplug deadlock
    
    [ Upstream commit 696ac2e3bf267f5a2b2ed7d34e64131f2287d0ad ]
    
    Similar to commit 0266d81e9bf5 ("acpi/processor: Prevent cpu hotplug
    deadlock") except this is for acpi_processor_ffh_cstate_probe():
    
    "The problem is that the work is scheduled on the current CPU from the
    hotplug thread associated with that CPU.
    
    It's not required to invoke these functions via the workqueue because
    the hotplug thread runs on the target CPU already.
    
    Check whether current is a per cpu thread pinned on the target CPU and
    invoke the function directly to avoid the workqueue."
    
     WARNING: possible circular locking dependency detected
     ------------------------------------------------------
     cpuhp/1/15 is trying to acquire lock:
     ffffc90003447a28 ((work_completion)(&wfc.work)){+.+.}-{0:0}, at: __flush_work+0x4c6/0x630
    
     but task is already holding lock:
     ffffffffafa1c0e8 (cpuidle_lock){+.+.}-{3:3}, at: cpuidle_pause_and_lock+0x17/0x20
    
     which lock already depends on the new lock.
    
     the existing dependency chain (in reverse order) is:
    
     -> #1 (cpu_hotplug_lock){++++}-{0:0}:
     cpus_read_lock+0x3e/0xc0
     irq_calc_affinity_vectors+0x5f/0x91
     __pci_enable_msix_range+0x10f/0x9a0
     pci_alloc_irq_vectors_affinity+0x13e/0x1f0
     pci_alloc_irq_vectors_affinity at drivers/pci/msi.c:1208
     pqi_ctrl_init+0x72f/0x1618 [smartpqi]
     pqi_pci_probe.cold.63+0x882/0x892 [smartpqi]
     local_pci_probe+0x7a/0xc0
     work_for_cpu_fn+0x2e/0x50
     process_one_work+0x57e/0xb90
     worker_thread+0x363/0x5b0
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
     -> #0 ((work_completion)(&wfc.work)){+.+.}-{0:0}:
     __lock_acquire+0x2244/0x32a0
     lock_acquire+0x1a2/0x680
     __flush_work+0x4e6/0x630
     work_on_cpu+0x114/0x160
     acpi_processor_ffh_cstate_probe+0x129/0x250
     acpi_processor_evaluate_cst+0x4c8/0x580
     acpi_processor_get_power_info+0x86/0x740
     acpi_processor_hotplug+0xc3/0x140
     acpi_soft_cpu_online+0x102/0x1d0
     cpuhp_invoke_callback+0x197/0x1120
     cpuhp_thread_fun+0x252/0x2f0
     smpboot_thread_fn+0x255/0x440
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
     other info that might help us debug this:
    
     Chain exists of:
     (work_completion)(&wfc.work) --> cpuhp_state-up --> cpuidle_lock
    
     Possible unsafe locking scenario:
    
     CPU0                    CPU1
     ----                    ----
     lock(cpuidle_lock);
                             lock(cpuhp_state-up);
                             lock(cpuidle_lock);
     lock((work_completion)(&wfc.work));
    
     *** DEADLOCK ***
    
     3 locks held by cpuhp/1/15:
     #0: ffffffffaf51ab10 (cpu_hotplug_lock){++++}-{0:0}, at: cpuhp_thread_fun+0x69/0x2f0
     #1: ffffffffaf51ad40 (cpuhp_state-up){+.+.}-{0:0}, at: cpuhp_thread_fun+0x69/0x2f0
     #2: ffffffffafa1c0e8 (cpuidle_lock){+.+.}-{3:3}, at: cpuidle_pause_and_lock+0x17/0x20
    
     Call Trace:
     dump_stack+0xa0/0xea
     print_circular_bug.cold.52+0x147/0x14c
     check_noncircular+0x295/0x2d0
     __lock_acquire+0x2244/0x32a0
     lock_acquire+0x1a2/0x680
     __flush_work+0x4e6/0x630
     work_on_cpu+0x114/0x160
     acpi_processor_ffh_cstate_probe+0x129/0x250
     acpi_processor_evaluate_cst+0x4c8/0x580
     acpi_processor_get_power_info+0x86/0x740
     acpi_processor_hotplug+0xc3/0x140
     acpi_soft_cpu_online+0x102/0x1d0
     cpuhp_invoke_callback+0x197/0x1120
     cpuhp_thread_fun+0x252/0x2f0
     smpboot_thread_fn+0x255/0x440
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Tested-by: Borislav Petkov <bp@suse.de>
    [ rjw: Subject ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 5246c08f70b4d721a48b9a98417cb178fa0531cf
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 21 16:21:05 2020 +0000

    x86/resctrl: Preserve CDP enable over CPU hotplug
    
    commit 9fe0450785abbc04b0ed5d3cf61fcdb8ab656b4b upstream.
    
    Resctrl assumes that all CPUs are online when the filesystem is mounted,
    and that CPUs remember their CDP-enabled state over CPU hotplug.
    
    This goes wrong when resctrl's CDP-enabled state changes while all the
    CPUs in a domain are offline.
    
    When a domain comes online, enable (or disable!) CDP to match resctrl's
    current setting.
    
    Fixes: 5ff193fbde20 ("x86/intel_rdt: Add basic resctrl filesystem support")
    Suggested-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org>
    Link: https://lkml.kernel.org/r/20200221162105.154163-1-james.morse@arm.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1733d2a94f6414ba905d91ff14322093fda7c398
Author: Zhenzhong Duan <zhenzhong.duan@oracle.com>
Date:   Thu Jan 17 02:10:59 2019 -0800

    x86/speculation: Remove redundant arch_smt_update() invocation
    
    commit 34d66caf251df91ff27b24a3a786810d29989eca upstream.
    
    With commit a74cfffb03b7 ("x86/speculation: Rework SMT state change"),
    arch_smt_update() is invoked from each individual CPU hotplug function.
    
    Therefore the extra arch_smt_update() call in the sysfs SMT control is
    redundant.
    
    Fixes: a74cfffb03b7 ("x86/speculation: Rework SMT state change")
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: <konrad.wilk@oracle.com>
    Cc: <dwmw@amazon.co.uk>
    Cc: <bp@suse.de>
    Cc: <srinivas.eeda@oracle.com>
    Cc: <peterz@infradead.org>
    Cc: <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/e2e064f2-e8ef-42ca-bf4f-76b612964752@default
    Cc: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f2a02afce64066409928d68d6f70e222529eb519
Author: Zhenzhong Duan <zhenzhong.duan@oracle.com>
Date:   Thu Jan 17 02:10:59 2019 -0800

    x86/speculation: Remove redundant arch_smt_update() invocation
    
    commit 34d66caf251df91ff27b24a3a786810d29989eca upstream.
    
    With commit a74cfffb03b7 ("x86/speculation: Rework SMT state change"),
    arch_smt_update() is invoked from each individual CPU hotplug function.
    
    Therefore the extra arch_smt_update() call in the sysfs SMT control is
    redundant.
    
    Fixes: a74cfffb03b7 ("x86/speculation: Rework SMT state change")
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: <konrad.wilk@oracle.com>
    Cc: <dwmw@amazon.co.uk>
    Cc: <bp@suse.de>
    Cc: <srinivas.eeda@oracle.com>
    Cc: <peterz@infradead.org>
    Cc: <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/e2e064f2-e8ef-42ca-bf4f-76b612964752@default
    Cc: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d570aec0f2154e1bfba14ffd0df164a185e363b5
Author: Andrea Parri (Microsoft) <parri.andrea@gmail.com>
Date:   Mon Apr 6 02:15:12 2020 +0200

    Drivers: hv: vmbus: Synchronize init_vp_index() vs. CPU hotplug
    
    init_vp_index() may access the cpu_online_mask mask via its calls of
    cpumask_of_node().  Make sure to protect these accesses with a
    cpus_read_lock() critical section.
    
    Also, remove some (hardcoded) instances of CPU(0) from init_vp_index()
    and replace them with VMBUS_CONNECT_CPU.  The connect CPU can not go
    offline, since Hyper-V does not provide a way to change it.
    
    Finally, order the accesses of target_cpu from init_vp_index() and
    hv_synic_cleanup() by relying on the channel_mutex; this is achieved
    by moving the call of init_vp_index() into vmbus_process_offer().
    
    Signed-off-by: Andrea Parri (Microsoft) <parri.andrea@gmail.com>
    Link: https://lore.kernel.org/r/20200406001514.19876-10-parri.andrea@gmail.com
    Reviewed-by: Michael Kelley <mikelley@microsoft.com>
    Signed-off-by: Wei Liu <wei.liu@kernel.org>

commit 3875db666d95c1cbe98236d3034bbc863cca813c
Author: Qian Cai <cai@lca.pw>
Date:   Fri Apr 3 10:03:45 2020 -0400

    x86: ACPI: fix CPU hotplug deadlock
    
    [ Upstream commit 696ac2e3bf267f5a2b2ed7d34e64131f2287d0ad ]
    
    Similar to commit 0266d81e9bf5 ("acpi/processor: Prevent cpu hotplug
    deadlock") except this is for acpi_processor_ffh_cstate_probe():
    
    "The problem is that the work is scheduled on the current CPU from the
    hotplug thread associated with that CPU.
    
    It's not required to invoke these functions via the workqueue because
    the hotplug thread runs on the target CPU already.
    
    Check whether current is a per cpu thread pinned on the target CPU and
    invoke the function directly to avoid the workqueue."
    
     WARNING: possible circular locking dependency detected
     ------------------------------------------------------
     cpuhp/1/15 is trying to acquire lock:
     ffffc90003447a28 ((work_completion)(&wfc.work)){+.+.}-{0:0}, at: __flush_work+0x4c6/0x630
    
     but task is already holding lock:
     ffffffffafa1c0e8 (cpuidle_lock){+.+.}-{3:3}, at: cpuidle_pause_and_lock+0x17/0x20
    
     which lock already depends on the new lock.
    
     the existing dependency chain (in reverse order) is:
    
     -> #1 (cpu_hotplug_lock){++++}-{0:0}:
     cpus_read_lock+0x3e/0xc0
     irq_calc_affinity_vectors+0x5f/0x91
     __pci_enable_msix_range+0x10f/0x9a0
     pci_alloc_irq_vectors_affinity+0x13e/0x1f0
     pci_alloc_irq_vectors_affinity at drivers/pci/msi.c:1208
     pqi_ctrl_init+0x72f/0x1618 [smartpqi]
     pqi_pci_probe.cold.63+0x882/0x892 [smartpqi]
     local_pci_probe+0x7a/0xc0
     work_for_cpu_fn+0x2e/0x50
     process_one_work+0x57e/0xb90
     worker_thread+0x363/0x5b0
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
     -> #0 ((work_completion)(&wfc.work)){+.+.}-{0:0}:
     __lock_acquire+0x2244/0x32a0
     lock_acquire+0x1a2/0x680
     __flush_work+0x4e6/0x630
     work_on_cpu+0x114/0x160
     acpi_processor_ffh_cstate_probe+0x129/0x250
     acpi_processor_evaluate_cst+0x4c8/0x580
     acpi_processor_get_power_info+0x86/0x740
     acpi_processor_hotplug+0xc3/0x140
     acpi_soft_cpu_online+0x102/0x1d0
     cpuhp_invoke_callback+0x197/0x1120
     cpuhp_thread_fun+0x252/0x2f0
     smpboot_thread_fn+0x255/0x440
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
     other info that might help us debug this:
    
     Chain exists of:
     (work_completion)(&wfc.work) --> cpuhp_state-up --> cpuidle_lock
    
     Possible unsafe locking scenario:
    
     CPU0                    CPU1
     ----                    ----
     lock(cpuidle_lock);
                             lock(cpuhp_state-up);
                             lock(cpuidle_lock);
     lock((work_completion)(&wfc.work));
    
     *** DEADLOCK ***
    
     3 locks held by cpuhp/1/15:
     #0: ffffffffaf51ab10 (cpu_hotplug_lock){++++}-{0:0}, at: cpuhp_thread_fun+0x69/0x2f0
     #1: ffffffffaf51ad40 (cpuhp_state-up){+.+.}-{0:0}, at: cpuhp_thread_fun+0x69/0x2f0
     #2: ffffffffafa1c0e8 (cpuidle_lock){+.+.}-{3:3}, at: cpuidle_pause_and_lock+0x17/0x20
    
     Call Trace:
     dump_stack+0xa0/0xea
     print_circular_bug.cold.52+0x147/0x14c
     check_noncircular+0x295/0x2d0
     __lock_acquire+0x2244/0x32a0
     lock_acquire+0x1a2/0x680
     __flush_work+0x4e6/0x630
     work_on_cpu+0x114/0x160
     acpi_processor_ffh_cstate_probe+0x129/0x250
     acpi_processor_evaluate_cst+0x4c8/0x580
     acpi_processor_get_power_info+0x86/0x740
     acpi_processor_hotplug+0xc3/0x140
     acpi_soft_cpu_online+0x102/0x1d0
     cpuhp_invoke_callback+0x197/0x1120
     cpuhp_thread_fun+0x252/0x2f0
     smpboot_thread_fn+0x255/0x440
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Tested-by: Borislav Petkov <bp@suse.de>
    [ rjw: Subject ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 88591187bebc0140acb3901cb7e73c7442f92771
Author: Qian Cai <cai@lca.pw>
Date:   Fri Apr 3 10:03:45 2020 -0400

    x86: ACPI: fix CPU hotplug deadlock
    
    [ Upstream commit 696ac2e3bf267f5a2b2ed7d34e64131f2287d0ad ]
    
    Similar to commit 0266d81e9bf5 ("acpi/processor: Prevent cpu hotplug
    deadlock") except this is for acpi_processor_ffh_cstate_probe():
    
    "The problem is that the work is scheduled on the current CPU from the
    hotplug thread associated with that CPU.
    
    It's not required to invoke these functions via the workqueue because
    the hotplug thread runs on the target CPU already.
    
    Check whether current is a per cpu thread pinned on the target CPU and
    invoke the function directly to avoid the workqueue."
    
     WARNING: possible circular locking dependency detected
     ------------------------------------------------------
     cpuhp/1/15 is trying to acquire lock:
     ffffc90003447a28 ((work_completion)(&wfc.work)){+.+.}-{0:0}, at: __flush_work+0x4c6/0x630
    
     but task is already holding lock:
     ffffffffafa1c0e8 (cpuidle_lock){+.+.}-{3:3}, at: cpuidle_pause_and_lock+0x17/0x20
    
     which lock already depends on the new lock.
    
     the existing dependency chain (in reverse order) is:
    
     -> #1 (cpu_hotplug_lock){++++}-{0:0}:
     cpus_read_lock+0x3e/0xc0
     irq_calc_affinity_vectors+0x5f/0x91
     __pci_enable_msix_range+0x10f/0x9a0
     pci_alloc_irq_vectors_affinity+0x13e/0x1f0
     pci_alloc_irq_vectors_affinity at drivers/pci/msi.c:1208
     pqi_ctrl_init+0x72f/0x1618 [smartpqi]
     pqi_pci_probe.cold.63+0x882/0x892 [smartpqi]
     local_pci_probe+0x7a/0xc0
     work_for_cpu_fn+0x2e/0x50
     process_one_work+0x57e/0xb90
     worker_thread+0x363/0x5b0
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
     -> #0 ((work_completion)(&wfc.work)){+.+.}-{0:0}:
     __lock_acquire+0x2244/0x32a0
     lock_acquire+0x1a2/0x680
     __flush_work+0x4e6/0x630
     work_on_cpu+0x114/0x160
     acpi_processor_ffh_cstate_probe+0x129/0x250
     acpi_processor_evaluate_cst+0x4c8/0x580
     acpi_processor_get_power_info+0x86/0x740
     acpi_processor_hotplug+0xc3/0x140
     acpi_soft_cpu_online+0x102/0x1d0
     cpuhp_invoke_callback+0x197/0x1120
     cpuhp_thread_fun+0x252/0x2f0
     smpboot_thread_fn+0x255/0x440
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
     other info that might help us debug this:
    
     Chain exists of:
     (work_completion)(&wfc.work) --> cpuhp_state-up --> cpuidle_lock
    
     Possible unsafe locking scenario:
    
     CPU0                    CPU1
     ----                    ----
     lock(cpuidle_lock);
                             lock(cpuhp_state-up);
                             lock(cpuidle_lock);
     lock((work_completion)(&wfc.work));
    
     *** DEADLOCK ***
    
     3 locks held by cpuhp/1/15:
     #0: ffffffffaf51ab10 (cpu_hotplug_lock){++++}-{0:0}, at: cpuhp_thread_fun+0x69/0x2f0
     #1: ffffffffaf51ad40 (cpuhp_state-up){+.+.}-{0:0}, at: cpuhp_thread_fun+0x69/0x2f0
     #2: ffffffffafa1c0e8 (cpuidle_lock){+.+.}-{3:3}, at: cpuidle_pause_and_lock+0x17/0x20
    
     Call Trace:
     dump_stack+0xa0/0xea
     print_circular_bug.cold.52+0x147/0x14c
     check_noncircular+0x295/0x2d0
     __lock_acquire+0x2244/0x32a0
     lock_acquire+0x1a2/0x680
     __flush_work+0x4e6/0x630
     work_on_cpu+0x114/0x160
     acpi_processor_ffh_cstate_probe+0x129/0x250
     acpi_processor_evaluate_cst+0x4c8/0x580
     acpi_processor_get_power_info+0x86/0x740
     acpi_processor_hotplug+0xc3/0x140
     acpi_soft_cpu_online+0x102/0x1d0
     cpuhp_invoke_callback+0x197/0x1120
     cpuhp_thread_fun+0x252/0x2f0
     smpboot_thread_fn+0x255/0x440
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Tested-by: Borislav Petkov <bp@suse.de>
    [ rjw: Subject ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 67e5b7090905050a8c5efbe14889695a1c6b77f6
Author: Qian Cai <cai@lca.pw>
Date:   Fri Apr 3 10:03:45 2020 -0400

    x86: ACPI: fix CPU hotplug deadlock
    
    [ Upstream commit 696ac2e3bf267f5a2b2ed7d34e64131f2287d0ad ]
    
    Similar to commit 0266d81e9bf5 ("acpi/processor: Prevent cpu hotplug
    deadlock") except this is for acpi_processor_ffh_cstate_probe():
    
    "The problem is that the work is scheduled on the current CPU from the
    hotplug thread associated with that CPU.
    
    It's not required to invoke these functions via the workqueue because
    the hotplug thread runs on the target CPU already.
    
    Check whether current is a per cpu thread pinned on the target CPU and
    invoke the function directly to avoid the workqueue."
    
     WARNING: possible circular locking dependency detected
     ------------------------------------------------------
     cpuhp/1/15 is trying to acquire lock:
     ffffc90003447a28 ((work_completion)(&wfc.work)){+.+.}-{0:0}, at: __flush_work+0x4c6/0x630
    
     but task is already holding lock:
     ffffffffafa1c0e8 (cpuidle_lock){+.+.}-{3:3}, at: cpuidle_pause_and_lock+0x17/0x20
    
     which lock already depends on the new lock.
    
     the existing dependency chain (in reverse order) is:
    
     -> #1 (cpu_hotplug_lock){++++}-{0:0}:
     cpus_read_lock+0x3e/0xc0
     irq_calc_affinity_vectors+0x5f/0x91
     __pci_enable_msix_range+0x10f/0x9a0
     pci_alloc_irq_vectors_affinity+0x13e/0x1f0
     pci_alloc_irq_vectors_affinity at drivers/pci/msi.c:1208
     pqi_ctrl_init+0x72f/0x1618 [smartpqi]
     pqi_pci_probe.cold.63+0x882/0x892 [smartpqi]
     local_pci_probe+0x7a/0xc0
     work_for_cpu_fn+0x2e/0x50
     process_one_work+0x57e/0xb90
     worker_thread+0x363/0x5b0
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
     -> #0 ((work_completion)(&wfc.work)){+.+.}-{0:0}:
     __lock_acquire+0x2244/0x32a0
     lock_acquire+0x1a2/0x680
     __flush_work+0x4e6/0x630
     work_on_cpu+0x114/0x160
     acpi_processor_ffh_cstate_probe+0x129/0x250
     acpi_processor_evaluate_cst+0x4c8/0x580
     acpi_processor_get_power_info+0x86/0x740
     acpi_processor_hotplug+0xc3/0x140
     acpi_soft_cpu_online+0x102/0x1d0
     cpuhp_invoke_callback+0x197/0x1120
     cpuhp_thread_fun+0x252/0x2f0
     smpboot_thread_fn+0x255/0x440
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
     other info that might help us debug this:
    
     Chain exists of:
     (work_completion)(&wfc.work) --> cpuhp_state-up --> cpuidle_lock
    
     Possible unsafe locking scenario:
    
     CPU0                    CPU1
     ----                    ----
     lock(cpuidle_lock);
                             lock(cpuhp_state-up);
                             lock(cpuidle_lock);
     lock((work_completion)(&wfc.work));
    
     *** DEADLOCK ***
    
     3 locks held by cpuhp/1/15:
     #0: ffffffffaf51ab10 (cpu_hotplug_lock){++++}-{0:0}, at: cpuhp_thread_fun+0x69/0x2f0
     #1: ffffffffaf51ad40 (cpuhp_state-up){+.+.}-{0:0}, at: cpuhp_thread_fun+0x69/0x2f0
     #2: ffffffffafa1c0e8 (cpuidle_lock){+.+.}-{3:3}, at: cpuidle_pause_and_lock+0x17/0x20
    
     Call Trace:
     dump_stack+0xa0/0xea
     print_circular_bug.cold.52+0x147/0x14c
     check_noncircular+0x295/0x2d0
     __lock_acquire+0x2244/0x32a0
     lock_acquire+0x1a2/0x680
     __flush_work+0x4e6/0x630
     work_on_cpu+0x114/0x160
     acpi_processor_ffh_cstate_probe+0x129/0x250
     acpi_processor_evaluate_cst+0x4c8/0x580
     acpi_processor_get_power_info+0x86/0x740
     acpi_processor_hotplug+0xc3/0x140
     acpi_soft_cpu_online+0x102/0x1d0
     cpuhp_invoke_callback+0x197/0x1120
     cpuhp_thread_fun+0x252/0x2f0
     smpboot_thread_fn+0x255/0x440
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Tested-by: Borislav Petkov <bp@suse.de>
    [ rjw: Subject ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 1249ec7d1d831dd8995c85eee2dd9a1d6fc20830
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 21 16:21:05 2020 +0000

    x86/resctrl: Preserve CDP enable over CPU hotplug
    
    commit 9fe0450785abbc04b0ed5d3cf61fcdb8ab656b4b upstream.
    
    Resctrl assumes that all CPUs are online when the filesystem is mounted,
    and that CPUs remember their CDP-enabled state over CPU hotplug.
    
    This goes wrong when resctrl's CDP-enabled state changes while all the
    CPUs in a domain are offline.
    
    When a domain comes online, enable (or disable!) CDP to match resctrl's
    current setting.
    
    Fixes: 5ff193fbde20 ("x86/intel_rdt: Add basic resctrl filesystem support")
    Suggested-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org>
    Link: https://lkml.kernel.org/r/20200221162105.154163-1-james.morse@arm.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b3383bb299e7d6e71879123c60f83cf5c97942da
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 21 16:21:05 2020 +0000

    x86/resctrl: Preserve CDP enable over CPU hotplug
    
    commit 9fe0450785abbc04b0ed5d3cf61fcdb8ab656b4b upstream.
    
    Resctrl assumes that all CPUs are online when the filesystem is mounted,
    and that CPUs remember their CDP-enabled state over CPU hotplug.
    
    This goes wrong when resctrl's CDP-enabled state changes while all the
    CPUs in a domain are offline.
    
    When a domain comes online, enable (or disable!) CDP to match resctrl's
    current setting.
    
    Fixes: 5ff193fbde20 ("x86/intel_rdt: Add basic resctrl filesystem support")
    Suggested-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org>
    Link: https://lkml.kernel.org/r/20200221162105.154163-1-james.morse@arm.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3652782e3a87984efd24adf8efcb33e89778acc9
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 21 16:21:05 2020 +0000

    x86/resctrl: Preserve CDP enable over CPU hotplug
    
    commit 9fe0450785abbc04b0ed5d3cf61fcdb8ab656b4b upstream.
    
    Resctrl assumes that all CPUs are online when the filesystem is mounted,
    and that CPUs remember their CDP-enabled state over CPU hotplug.
    
    This goes wrong when resctrl's CDP-enabled state changes while all the
    CPUs in a domain are offline.
    
    When a domain comes online, enable (or disable!) CDP to match resctrl's
    current setting.
    
    Fixes: 5ff193fbde20 ("x86/intel_rdt: Add basic resctrl filesystem support")
    Suggested-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org>
    Link: https://lkml.kernel.org/r/20200221162105.154163-1-james.morse@arm.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e7e08add4be230ebd81569efa1307988d48916d9
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 21 16:21:05 2020 +0000

    x86/resctrl: Preserve CDP enable over CPU hotplug
    
    commit 9fe0450785abbc04b0ed5d3cf61fcdb8ab656b4b upstream.
    
    Resctrl assumes that all CPUs are online when the filesystem is mounted,
    and that CPUs remember their CDP-enabled state over CPU hotplug.
    
    This goes wrong when resctrl's CDP-enabled state changes while all the
    CPUs in a domain are offline.
    
    When a domain comes online, enable (or disable!) CDP to match resctrl's
    current setting.
    
    Fixes: 5ff193fbde20 ("x86/intel_rdt: Add basic resctrl filesystem support")
    Suggested-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org>
    Link: https://lkml.kernel.org/r/20200221162105.154163-1-james.morse@arm.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0fe5f9ca223573167c4c4156903d751d2c8e160e
Merge: 3e0dea57686d 8b9a18a9f249
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 19 11:58:32 2020 -0700

    Merge tag 'x86-urgent-2020-04-19' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 and objtool fixes from Thomas Gleixner:
     "A set of fixes for x86 and objtool:
    
      objtool:
    
       - Ignore the double UD2 which is emitted in BUG() when
         CONFIG_UBSAN_TRAP is enabled.
    
       - Support clang non-section symbols in objtool ORC dump
    
       - Fix switch table detection in .text.unlikely
    
       - Make the BP scratch register warning more robust.
    
      x86:
    
       - Increase microcode maximum patch size for AMD to cope with new CPUs
         which have a larger patch size.
    
       - Fix a crash in the resource control filesystem when the removal of
         the default resource group is attempted.
    
       - Preserve Code and Data Prioritization enabled state accross CPU
         hotplug.
    
       - Update split lock cpu matching to use the new X86_MATCH macros.
    
       - Change the split lock enumeration as Intel finaly decided that the
         IA32_CORE_CAPABILITIES bits are not architectural contrary to what
         the SDM claims. !@#%$^!
    
       - Add Tremont CPU models to the split lock detection cpu match.
    
       - Add a missing static attribute to make sparse happy"
    
    * tag 'x86-urgent-2020-04-19' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/split_lock: Add Tremont family CPU models
      x86/split_lock: Bits in IA32_CORE_CAPABILITIES are not architectural
      x86/resctrl: Preserve CDP enable over CPU hotplug
      x86/resctrl: Fix invalid attempt at removing the default resource group
      x86/split_lock: Update to use X86_MATCH_INTEL_FAM6_MODEL()
      x86/umip: Make umip_insns static
      x86/microcode/AMD: Increase microcode PATCH_MAX_SIZE
      objtool: Make BP scratch register warning more robust
      objtool: Fix switch table detection in .text.unlikely
      objtool: Support Clang non-section symbols in ORC generation
      objtool: Support Clang non-section symbols in ORC dump
      objtool: Fix CONFIG_UBSAN_TRAP unreachable warnings

commit 9fe0450785abbc04b0ed5d3cf61fcdb8ab656b4b
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 21 16:21:05 2020 +0000

    x86/resctrl: Preserve CDP enable over CPU hotplug
    
    Resctrl assumes that all CPUs are online when the filesystem is mounted,
    and that CPUs remember their CDP-enabled state over CPU hotplug.
    
    This goes wrong when resctrl's CDP-enabled state changes while all the
    CPUs in a domain are offline.
    
    When a domain comes online, enable (or disable!) CDP to match resctrl's
    current setting.
    
    Fixes: 5ff193fbde20 ("x86/intel_rdt: Add basic resctrl filesystem support")
    Suggested-by: Reinette Chatre <reinette.chatre@intel.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org>
    Link: https://lkml.kernel.org/r/20200221162105.154163-1-james.morse@arm.com

commit 6209e0981bc4c1a022d09a64a47e9109a85c4cd6
Author: Zhenzhong Duan <zhenzhong.duan@oracle.com>
Date:   Thu Jan 17 02:10:59 2019 -0800

    x86/speculation: Remove redundant arch_smt_update() invocation
    
    commit 34d66caf251df91ff27b24a3a786810d29989eca upstream.
    
    With commit a74cfffb03b7 ("x86/speculation: Rework SMT state change"),
    arch_smt_update() is invoked from each individual CPU hotplug function.
    
    Therefore the extra arch_smt_update() call in the sysfs SMT control is
    redundant.
    
    Fixes: a74cfffb03b7 ("x86/speculation: Rework SMT state change")
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: <konrad.wilk@oracle.com>
    Cc: <dwmw@amazon.co.uk>
    Cc: <bp@suse.de>
    Cc: <srinivas.eeda@oracle.com>
    Cc: <peterz@infradead.org>
    Cc: <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/e2e064f2-e8ef-42ca-bf4f-76b612964752@default
    Cc: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4b2dfe1e7799d0e20b55711dfcc45d2ad35ff46e
Author: Marc Zyngier <maz@kernel.org>
Date:   Fri Apr 10 12:11:39 2020 +0100

    irqchip/gic-v4.1: Update effective affinity of virtual SGIs
    
    Although the vSGIs are not directly visible to the host, they still
    get moved around by the CPU hotplug, for example. This results in
    the kernel moaning on the console, such as:
    
      genirq: irq_chip GICv4.1-sgi did not update eff. affinity mask of irq 38
    
    Updating the effective affinity on set_affinity() fixes it.
    
    Reviewed-by: Zenghui Yu <yuzenghui@huawei.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

commit 6458de97fc15530b54477c4e2b70af653e8ac3d9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Mar 30 20:30:45 2020 +0200

    x86/mce/amd: Straighten CPU hotplug path
    
    mce_threshold_create_device() hotplug callback runs on the plugged in
    CPU so:
    
     - use this_cpu_read() which is faster
     - pass in struct threshold_bank **bp to threshold_create_bank() and
       instead of doing per-CPU accesses
     - Use rdmsr_safe() instead of rdmsr_safe_on_cpu() which avoids an IPI.
    
    No functional changes.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Link: https://lkml.kernel.org/r/20200403161943.1458-6-bp@alien8.de

commit bf498d6b8d6017c7e1c7908190fa57508f823268
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Tue Dec 3 14:31:11 2019 -0500

    padata: always acquire cpu_hotplug_lock before pinst->lock
    
    commit 38228e8848cd7dd86ccb90406af32de0cad24be3 upstream.
    
    lockdep complains when padata's paths to update cpumasks via CPU hotplug
    and sysfs are both taken:
    
      # echo 0 > /sys/devices/system/cpu/cpu1/online
      # echo ff > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
    
      ======================================================
      WARNING: possible circular locking dependency detected
      5.4.0-rc8-padata-cpuhp-v3+ #1 Not tainted
      ------------------------------------------------------
      bash/205 is trying to acquire lock:
      ffffffff8286bcd0 (cpu_hotplug_lock.rw_sem){++++}, at: padata_set_cpumask+0x2b/0x120
    
      but task is already holding lock:
      ffff8880001abfa0 (&pinst->lock){+.+.}, at: padata_set_cpumask+0x26/0x120
    
      which lock already depends on the new lock.
    
    padata doesn't take cpu_hotplug_lock and pinst->lock in a consistent
    order.  Which should be first?  CPU hotplug calls into padata with
    cpu_hotplug_lock already held, so it should have priority.
    
    Fixes: 6751fb3c0e0c ("padata: Use get_online_cpus/put_online_cpus")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Eric Biggers <ebiggers@kernel.org>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2d8260be1c2cc83a0e43b0304db490421a531d9a
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Tue Dec 3 14:31:11 2019 -0500

    padata: always acquire cpu_hotplug_lock before pinst->lock
    
    commit 38228e8848cd7dd86ccb90406af32de0cad24be3 upstream.
    
    lockdep complains when padata's paths to update cpumasks via CPU hotplug
    and sysfs are both taken:
    
      # echo 0 > /sys/devices/system/cpu/cpu1/online
      # echo ff > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
    
      ======================================================
      WARNING: possible circular locking dependency detected
      5.4.0-rc8-padata-cpuhp-v3+ #1 Not tainted
      ------------------------------------------------------
      bash/205 is trying to acquire lock:
      ffffffff8286bcd0 (cpu_hotplug_lock.rw_sem){++++}, at: padata_set_cpumask+0x2b/0x120
    
      but task is already holding lock:
      ffff8880001abfa0 (&pinst->lock){+.+.}, at: padata_set_cpumask+0x26/0x120
    
      which lock already depends on the new lock.
    
    padata doesn't take cpu_hotplug_lock and pinst->lock in a consistent
    order.  Which should be first?  CPU hotplug calls into padata with
    cpu_hotplug_lock already held, so it should have priority.
    
    Fixes: 6751fb3c0e0c ("padata: Use get_online_cpus/put_online_cpus")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Eric Biggers <ebiggers@kernel.org>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b1d47d77e0395cd10962830747436f3a014de7af
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Tue Dec 3 14:31:11 2019 -0500

    padata: always acquire cpu_hotplug_lock before pinst->lock
    
    commit 38228e8848cd7dd86ccb90406af32de0cad24be3 upstream.
    
    lockdep complains when padata's paths to update cpumasks via CPU hotplug
    and sysfs are both taken:
    
      # echo 0 > /sys/devices/system/cpu/cpu1/online
      # echo ff > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
    
      ======================================================
      WARNING: possible circular locking dependency detected
      5.4.0-rc8-padata-cpuhp-v3+ #1 Not tainted
      ------------------------------------------------------
      bash/205 is trying to acquire lock:
      ffffffff8286bcd0 (cpu_hotplug_lock.rw_sem){++++}, at: padata_set_cpumask+0x2b/0x120
    
      but task is already holding lock:
      ffff8880001abfa0 (&pinst->lock){+.+.}, at: padata_set_cpumask+0x26/0x120
    
      which lock already depends on the new lock.
    
    padata doesn't take cpu_hotplug_lock and pinst->lock in a consistent
    order.  Which should be first?  CPU hotplug calls into padata with
    cpu_hotplug_lock already held, so it should have priority.
    
    Fixes: 6751fb3c0e0c ("padata: Use get_online_cpus/put_online_cpus")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Eric Biggers <ebiggers@kernel.org>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 80c4e3a257b95fec296eb1ea1b320a0fa1396182
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Tue Dec 3 14:31:11 2019 -0500

    padata: always acquire cpu_hotplug_lock before pinst->lock
    
    commit 38228e8848cd7dd86ccb90406af32de0cad24be3 upstream.
    
    lockdep complains when padata's paths to update cpumasks via CPU hotplug
    and sysfs are both taken:
    
      # echo 0 > /sys/devices/system/cpu/cpu1/online
      # echo ff > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
    
      ======================================================
      WARNING: possible circular locking dependency detected
      5.4.0-rc8-padata-cpuhp-v3+ #1 Not tainted
      ------------------------------------------------------
      bash/205 is trying to acquire lock:
      ffffffff8286bcd0 (cpu_hotplug_lock.rw_sem){++++}, at: padata_set_cpumask+0x2b/0x120
    
      but task is already holding lock:
      ffff8880001abfa0 (&pinst->lock){+.+.}, at: padata_set_cpumask+0x26/0x120
    
      which lock already depends on the new lock.
    
    padata doesn't take cpu_hotplug_lock and pinst->lock in a consistent
    order.  Which should be first?  CPU hotplug calls into padata with
    cpu_hotplug_lock already held, so it should have priority.
    
    Fixes: 6751fb3c0e0c ("padata: Use get_online_cpus/put_online_cpus")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Eric Biggers <ebiggers@kernel.org>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 141a37be67abbf16553e84bdf6bca1a3028ccdf0
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Tue Dec 3 14:31:11 2019 -0500

    padata: always acquire cpu_hotplug_lock before pinst->lock
    
    commit 38228e8848cd7dd86ccb90406af32de0cad24be3 upstream.
    
    lockdep complains when padata's paths to update cpumasks via CPU hotplug
    and sysfs are both taken:
    
      # echo 0 > /sys/devices/system/cpu/cpu1/online
      # echo ff > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
    
      ======================================================
      WARNING: possible circular locking dependency detected
      5.4.0-rc8-padata-cpuhp-v3+ #1 Not tainted
      ------------------------------------------------------
      bash/205 is trying to acquire lock:
      ffffffff8286bcd0 (cpu_hotplug_lock.rw_sem){++++}, at: padata_set_cpumask+0x2b/0x120
    
      but task is already holding lock:
      ffff8880001abfa0 (&pinst->lock){+.+.}, at: padata_set_cpumask+0x26/0x120
    
      which lock already depends on the new lock.
    
    padata doesn't take cpu_hotplug_lock and pinst->lock in a consistent
    order.  Which should be first?  CPU hotplug calls into padata with
    cpu_hotplug_lock already held, so it should have priority.
    
    Fixes: 6751fb3c0e0c ("padata: Use get_online_cpus/put_online_cpus")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Eric Biggers <ebiggers@kernel.org>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c3d4e6fc4b37f60ab41f663c3d3fb57570f446bf
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Tue Dec 3 14:31:11 2019 -0500

    padata: always acquire cpu_hotplug_lock before pinst->lock
    
    commit 38228e8848cd7dd86ccb90406af32de0cad24be3 upstream.
    
    lockdep complains when padata's paths to update cpumasks via CPU hotplug
    and sysfs are both taken:
    
      # echo 0 > /sys/devices/system/cpu/cpu1/online
      # echo ff > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
    
      ======================================================
      WARNING: possible circular locking dependency detected
      5.4.0-rc8-padata-cpuhp-v3+ #1 Not tainted
      ------------------------------------------------------
      bash/205 is trying to acquire lock:
      ffffffff8286bcd0 (cpu_hotplug_lock.rw_sem){++++}, at: padata_set_cpumask+0x2b/0x120
    
      but task is already holding lock:
      ffff8880001abfa0 (&pinst->lock){+.+.}, at: padata_set_cpumask+0x26/0x120
    
      which lock already depends on the new lock.
    
    padata doesn't take cpu_hotplug_lock and pinst->lock in a consistent
    order.  Which should be first?  CPU hotplug calls into padata with
    cpu_hotplug_lock already held, so it should have priority.
    
    Fixes: 6751fb3c0e0c ("padata: Use get_online_cpus/put_online_cpus")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Eric Biggers <ebiggers@kernel.org>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7e63420847ae5f1036e4f7c42f0b3282e73efbc2
Merge: ef05db16bbd8 33ae7f715e30
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 6 10:35:06 2020 -0700

    Merge tag 'acpi-5.7-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull more ACPI updates from Rafael Wysocki:
     "Additional ACPI updates.
    
      These update the ACPICA code in the kernel to the 20200326 upstream
      revision, fix an ACPI-related CPU hotplug deadlock on x86, update
      Intel Tiger Lake device IDs in some places, add a new ACPI backlight
      blacklist entry, update the "acpi_backlight" kernel command line
      switch documentation and clean up a CPPC library routine.
    
      Specifics:
    
       - Update the ACPICA code in the kernel to upstream revision 20200326
         including:
          * Fix for a typo in a comment field (Bob Moore)
          * acpiExec namespace init file fixes (Bob Moore)
          * Addition of NHLT to the known tables list (Cezary Rojewski)
          * Conversion of PlatformCommChannel ASL keyword to PCC (Erik
            Kaneda)
          * acpiexec cleanup (Erik Kaneda)
          * WSMT-related typo fix (Erik Kaneda)
          * sprintf() utility function fix (John Levon)
          * IVRS IVHD type 11h parsing implementation (Micha ygowski)
          * IVRS IVHD type 10h reserved field name fix (Micha ygowski)
    
       - Fix ACPI-related CPU hotplug deadlock on x86 (Qian Cai)
    
       - Fix Intel Tiger Lake ACPI device IDs in several places (Gayatri
         Kammela)
    
       - Add ACPI backlight blacklist entry for Acer Aspire 5783z (Hans de
         Goede)
    
       - Fix documentation of the "acpi_backlight" kernel command line
         switch (Randy Dunlap)
    
       - Clean up the acpi_get_psd_map() CPPC library routine (Liguang
         Zhang)"
    
    * tag 'acpi-5.7-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm:
      x86: ACPI: fix CPU hotplug deadlock
      thermal: int340x_thermal: fix: Update Tiger Lake ACPI device IDs
      platform/x86: intel-hid: fix: Update Tiger Lake ACPI device ID
      ACPI: Update Tiger Lake ACPI device IDs
      ACPI: video: Use native backlight on Acer Aspire 5783z
      ACPI: video: Docs update for "acpi_backlight" kernel parameter options
      ACPICA: Update version 20200326
      ACPICA: Fixes for acpiExec namespace init file
      ACPICA: Add NHLT table signature
      ACPICA: WSMT: Fix typo, no functional change
      ACPICA: utilities: fix sprintf()
      ACPICA: acpiexec: remove redeclaration of acpi_gbl_db_opt_no_region_support
      ACPICA: Change PlatformCommChannel ASL keyword to PCC
      ACPICA: Fix IVRS IVHD type 10h reserved field name
      ACPICA: Implement IVRS IVHD type 11h parsing
      ACPICA: Fix a typo in a comment field
      ACPI: CPPC: clean up acpi_get_psd_map()

commit 696ac2e3bf267f5a2b2ed7d34e64131f2287d0ad
Author: Qian Cai <cai@lca.pw>
Date:   Fri Apr 3 10:03:45 2020 -0400

    x86: ACPI: fix CPU hotplug deadlock
    
    Similar to commit 0266d81e9bf5 ("acpi/processor: Prevent cpu hotplug
    deadlock") except this is for acpi_processor_ffh_cstate_probe():
    
    "The problem is that the work is scheduled on the current CPU from the
    hotplug thread associated with that CPU.
    
    It's not required to invoke these functions via the workqueue because
    the hotplug thread runs on the target CPU already.
    
    Check whether current is a per cpu thread pinned on the target CPU and
    invoke the function directly to avoid the workqueue."
    
     WARNING: possible circular locking dependency detected
     ------------------------------------------------------
     cpuhp/1/15 is trying to acquire lock:
     ffffc90003447a28 ((work_completion)(&wfc.work)){+.+.}-{0:0}, at: __flush_work+0x4c6/0x630
    
     but task is already holding lock:
     ffffffffafa1c0e8 (cpuidle_lock){+.+.}-{3:3}, at: cpuidle_pause_and_lock+0x17/0x20
    
     which lock already depends on the new lock.
    
     the existing dependency chain (in reverse order) is:
    
     -> #1 (cpu_hotplug_lock){++++}-{0:0}:
     cpus_read_lock+0x3e/0xc0
     irq_calc_affinity_vectors+0x5f/0x91
     __pci_enable_msix_range+0x10f/0x9a0
     pci_alloc_irq_vectors_affinity+0x13e/0x1f0
     pci_alloc_irq_vectors_affinity at drivers/pci/msi.c:1208
     pqi_ctrl_init+0x72f/0x1618 [smartpqi]
     pqi_pci_probe.cold.63+0x882/0x892 [smartpqi]
     local_pci_probe+0x7a/0xc0
     work_for_cpu_fn+0x2e/0x50
     process_one_work+0x57e/0xb90
     worker_thread+0x363/0x5b0
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
     -> #0 ((work_completion)(&wfc.work)){+.+.}-{0:0}:
     __lock_acquire+0x2244/0x32a0
     lock_acquire+0x1a2/0x680
     __flush_work+0x4e6/0x630
     work_on_cpu+0x114/0x160
     acpi_processor_ffh_cstate_probe+0x129/0x250
     acpi_processor_evaluate_cst+0x4c8/0x580
     acpi_processor_get_power_info+0x86/0x740
     acpi_processor_hotplug+0xc3/0x140
     acpi_soft_cpu_online+0x102/0x1d0
     cpuhp_invoke_callback+0x197/0x1120
     cpuhp_thread_fun+0x252/0x2f0
     smpboot_thread_fn+0x255/0x440
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
     other info that might help us debug this:
    
     Chain exists of:
     (work_completion)(&wfc.work) --> cpuhp_state-up --> cpuidle_lock
    
     Possible unsafe locking scenario:
    
     CPU0                    CPU1
     ----                    ----
     lock(cpuidle_lock);
                             lock(cpuhp_state-up);
                             lock(cpuidle_lock);
     lock((work_completion)(&wfc.work));
    
     *** DEADLOCK ***
    
     3 locks held by cpuhp/1/15:
     #0: ffffffffaf51ab10 (cpu_hotplug_lock){++++}-{0:0}, at: cpuhp_thread_fun+0x69/0x2f0
     #1: ffffffffaf51ad40 (cpuhp_state-up){+.+.}-{0:0}, at: cpuhp_thread_fun+0x69/0x2f0
     #2: ffffffffafa1c0e8 (cpuidle_lock){+.+.}-{3:3}, at: cpuidle_pause_and_lock+0x17/0x20
    
     Call Trace:
     dump_stack+0xa0/0xea
     print_circular_bug.cold.52+0x147/0x14c
     check_noncircular+0x295/0x2d0
     __lock_acquire+0x2244/0x32a0
     lock_acquire+0x1a2/0x680
     __flush_work+0x4e6/0x630
     work_on_cpu+0x114/0x160
     acpi_processor_ffh_cstate_probe+0x129/0x250
     acpi_processor_evaluate_cst+0x4c8/0x580
     acpi_processor_get_power_info+0x86/0x740
     acpi_processor_hotplug+0xc3/0x140
     acpi_soft_cpu_online+0x102/0x1d0
     cpuhp_invoke_callback+0x197/0x1120
     cpuhp_thread_fun+0x252/0x2f0
     smpboot_thread_fn+0x255/0x440
     kthread+0x1f4/0x220
     ret_from_fork+0x27/0x50
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Tested-by: Borislav Petkov <bp@suse.de>
    [ rjw: Subject ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit e011995e826f85fbe55dc7d4ce649461163d1052
Author: Atish Patra <atish.patra@wdc.com>
Date:   Tue Mar 17 18:11:39 2020 -0700

    RISC-V: Move relocate and few other functions out of __init
    
    The secondary hart booting and relocation code are under .init section.
    As a result, it will be freed once kernel booting is done. However,
    ordered booting protocol and CPU hotplug always requires these functions
    to be present to bringup harts after initial kernel boot.
    
    Move the required functions to a different section and make sure that
    they are in memory within first 2MB offset as trampoline page directory
    only maps first 2MB.
    
    Signed-off-by: Atish Patra <atish.patra@wdc.com>
    Reviewed-by: Anup Patel <anup@brainfault.org>
    Signed-off-by: Palmer Dabbelt <palmerdabbelt@google.com>

commit 992a1a3b45b5c0b6e69ecc2a3f32b0d02da28d58
Merge: 2d385336afcc e98eac6ff1b4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 18:06:39 2020 -0700

    Merge tag 'smp-core-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core SMP updates from Thomas Gleixner:
     "CPU (hotplug) updates:
    
       - Support for locked CSD objects in smp_call_function_single_async()
         which allows to simplify callsites in the scheduler core and MIPS
    
       - Treewide consolidation of CPU hotplug functions which ensures the
         consistency between the sysfs interface and kernel state. The low
         level functions cpu_up/down() are now confined to the core code and
         not longer accessible from random code"
    
    * tag 'smp-core-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (22 commits)
      cpu/hotplug: Ignore pm_wakeup_pending() for disable_nonboot_cpus()
      cpu/hotplug: Hide cpu_up/down()
      cpu/hotplug: Move bringup of secondary CPUs out of smp_init()
      torture: Replace cpu_up/down() with add/remove_cpu()
      firmware: psci: Replace cpu_up/down() with add/remove_cpu()
      xen/cpuhotplug: Replace cpu_up/down() with device_online/offline()
      parisc: Replace cpu_up/down() with add/remove_cpu()
      sparc: Replace cpu_up/down() with add/remove_cpu()
      powerpc: Replace cpu_up/down() with add/remove_cpu()
      x86/smp: Replace cpu_up/down() with add/remove_cpu()
      arm64: hibernate: Use bringup_hibernate_cpu()
      cpu/hotplug: Provide bringup_hibernate_cpu()
      arm64: Use reboot_cpu instead of hardconding it to 0
      arm64: Don't use disable_nonboot_cpus()
      ARM: Use reboot_cpu instead of hardcoding it to 0
      ARM: Don't use disable_nonboot_cpus()
      ia64: Replace cpu_down() with smp_shutdown_nonboot_cpus()
      cpu/hotplug: Create a new function to shutdown nonboot cpus
      cpu/hotplug: Add new {add,remove}_cpu() functions
      sched/core: Remove rq.hrtick_csd_pending
      ...

commit 8a13b02a010a743ea0725e9a5454f42cddb65cf0
Merge: ba947241f125 771df8cf0bc3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Mar 29 22:20:48 2020 +0200

    Merge tag 'irqchip-5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/maz/arm-platforms into irq/core
    
    Pull irqchip updates from Marc Zyngier:
    
     - Second batch of the GICv4.1 support saga
     - Level triggered interrupt support for the stm32 controller
     - Versatile-fpga chained interrupt fixes
     - DT support for cascaded VIC interrupt controller
     - RPi irqchip initialization fixes
     - Multi-instance support for the Xilinx interrupt controller
     - Multi-instance support for the PLIC interrupt controller
     - CPU hotplug support for the PLIC interrupt controller
     - Ingenic X1000 TCU support
     - Small fixes all over the shop (GICv3, GICv4, Xilinx, Atmel, sa1111)
     - Cleanups (setup_irq removal, zero-length array removal)

commit ccbe80bad571c2f967ad42b25bbb3ef7a4a24705
Author: Atish Patra <atish.patra@wdc.com>
Date:   Mon Mar 2 15:11:45 2020 -0800

    irqchip/sifive-plic: Enable/Disable external interrupts upon cpu online/offline
    
    Currently, PLIC threshold is only initialized once in the beginning.
    However, threshold can be set to disabled if a CPU is marked offline with
    CPU hotplug feature. This will not allow to change the irq affinity to a
    CPU that just came online.
    
    Add PLIC specific CPU hotplug callbacks and enable the threshold when a CPU
    comes online. Take this opportunity to move the external interrupt enable
    code from trap init to PLIC driver as well. On cpu offline path, the driver
    performs the exact opposite operations i.e. disable the interrupt and
    the threshold.
    
    Signed-off-by: Atish Patra <atish.patra@wdc.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Reviewed-by: Anup Patel <anup@brainfault.org>
    Link: https://lore.kernel.org/r/20200302231146.15530-2-atish.patra@wdc.com

commit ec181b7f30bdae2fbbba1c0dd76aeaad89c7963e
Merge: e99bc917fe02 469ff207b4c4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 15 12:52:56 2020 -0700

    Merge tag 'x86-urgent-2020-03-15' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Thomas Gleixner:
     "Two fixes for x86:
    
       - Map EFI runtime service data as encrypted when SEV is enabled.
    
         Otherwise e.g. SMBIOS data cannot be properly decoded by dmidecode.
    
       - Remove the warning in the vector management code which triggered
         when a managed interrupt affinity changed outside of a CPU hotplug
         operation.
    
         The warning was correct until the recent core code change that
         introduced a CPU isolation feature which needs to migrate managed
         interrupts away from online CPUs under certain conditions to
         achieve the isolation"
    
    * tag 'x86-urgent-2020-03-15' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/vector: Remove warning on managed interrupt migration
      x86/ioremap: Map EFI runtime services data as encrypted for SEV

commit df25e554887f47c8e9c110e7cca2f27dbe6ef4fb
Author: Dmitry Osipenko <digetx@gmail.com>
Date:   Tue Feb 25 01:40:41 2020 +0300

    ARM: tegra: Compile sleep-tegra20/30.S unconditionally
    
    The sleep-tegra*.S provides functionality required for suspend/resume
    and CPU hotplugging. The new unified CPUIDLE driver will support multiple
    hardware generations starting from Terga20 and ending with Tegra124, the
    driver will utilize functions that are provided by the assembly and thus
    it is cleaner to compile that code without any build-dependencies in order
    to avoid churning with #ifdef's.
    
    Acked-by: Peter De Schrijver <pdeschrijver@nvidia.com>
    Tested-by: Peter Geis <pgwipeout@gmail.com>
    Tested-by: Jasper Korten <jja2000@gmail.com>
    Tested-by: David Heidelberg <david@ixit.cz>
    Tested-by: Nicolas Chauvet <kwizart@gmail.com>
    Signed-off-by: Dmitry Osipenko <digetx@gmail.com>
    Signed-off-by: Thierry Reding <treding@nvidia.com>

commit b46c8fdba51271e6ce6b021265ac8b69f8aba907
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 12 12:19:41 2020 +0100

    genirq/proc: Reject invalid affinity masks (again)
    
    commit cba6437a1854fde5934098ec3bd0ee83af3129f5 upstream.
    
    Qian Cai reported that the WARN_ON() in the x86/msi affinity setting code,
    which catches cases where the affinity setting is not done on the CPU which
    is the current target of the interrupt, triggers during CPU hotplug stress
    testing.
    
    It turns out that the warning which was added with the commit addressing
    the MSI affinity race unearthed yet another long standing bug.
    
    If user space writes a bogus affinity mask, i.e. it contains no online CPUs,
    then it calls irq_select_affinity_usr(). This was introduced for ALPHA in
    
      eee45269b0f5 ("[PATCH] Alpha: convert to generic irq framework (generic part)")
    
    and subsequently made available for all architectures in
    
      18404756765c ("genirq: Expose default irq affinity mask (take 3)")
    
    which introduced the circumvention of the affinity setting restrictions for
    interrupt which cannot be moved in process context.
    
    The whole exercise is bogus in various aspects:
    
      1) If the interrupt is already started up then there is absolutely
         no point to honour a bogus interrupt affinity setting from user
         space. The interrupt is already assigned to an online CPU and it
         does not make any sense to reassign it to some other randomly
         chosen online CPU.
    
      2) If the interupt is not yet started up then there is no point
         either. A subsequent startup of the interrupt will invoke
         irq_setup_affinity() anyway which will chose a valid target CPU.
    
    So the only correct solution is to just return -EINVAL in case user space
    wrote an affinity mask which does not contain any online CPUs, except for
    ALPHA which has it's own magic sauce for this.
    
    Fixes: 18404756765c ("genirq: Expose default irq affinity mask (take 3)")
    Reported-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Qian Cai <cai@lca.pw>
    Link: https://lkml.kernel.org/r/878sl8xdbm.fsf@nanos.tec.linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2463a30f6678db61e3675957cee7016c238b3639
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 12 12:19:41 2020 +0100

    genirq/proc: Reject invalid affinity masks (again)
    
    commit cba6437a1854fde5934098ec3bd0ee83af3129f5 upstream.
    
    Qian Cai reported that the WARN_ON() in the x86/msi affinity setting code,
    which catches cases where the affinity setting is not done on the CPU which
    is the current target of the interrupt, triggers during CPU hotplug stress
    testing.
    
    It turns out that the warning which was added with the commit addressing
    the MSI affinity race unearthed yet another long standing bug.
    
    If user space writes a bogus affinity mask, i.e. it contains no online CPUs,
    then it calls irq_select_affinity_usr(). This was introduced for ALPHA in
    
      eee45269b0f5 ("[PATCH] Alpha: convert to generic irq framework (generic part)")
    
    and subsequently made available for all architectures in
    
      18404756765c ("genirq: Expose default irq affinity mask (take 3)")
    
    which introduced the circumvention of the affinity setting restrictions for
    interrupt which cannot be moved in process context.
    
    The whole exercise is bogus in various aspects:
    
      1) If the interrupt is already started up then there is absolutely
         no point to honour a bogus interrupt affinity setting from user
         space. The interrupt is already assigned to an online CPU and it
         does not make any sense to reassign it to some other randomly
         chosen online CPU.
    
      2) If the interupt is not yet started up then there is no point
         either. A subsequent startup of the interrupt will invoke
         irq_setup_affinity() anyway which will chose a valid target CPU.
    
    So the only correct solution is to just return -EINVAL in case user space
    wrote an affinity mask which does not contain any online CPUs, except for
    ALPHA which has it's own magic sauce for this.
    
    Fixes: 18404756765c ("genirq: Expose default irq affinity mask (take 3)")
    Reported-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Qian Cai <cai@lca.pw>
    Link: https://lkml.kernel.org/r/878sl8xdbm.fsf@nanos.tec.linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3132696dd748f44b60f940e78c8d10203e33117a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 12 12:19:41 2020 +0100

    genirq/proc: Reject invalid affinity masks (again)
    
    commit cba6437a1854fde5934098ec3bd0ee83af3129f5 upstream.
    
    Qian Cai reported that the WARN_ON() in the x86/msi affinity setting code,
    which catches cases where the affinity setting is not done on the CPU which
    is the current target of the interrupt, triggers during CPU hotplug stress
    testing.
    
    It turns out that the warning which was added with the commit addressing
    the MSI affinity race unearthed yet another long standing bug.
    
    If user space writes a bogus affinity mask, i.e. it contains no online CPUs,
    then it calls irq_select_affinity_usr(). This was introduced for ALPHA in
    
      eee45269b0f5 ("[PATCH] Alpha: convert to generic irq framework (generic part)")
    
    and subsequently made available for all architectures in
    
      18404756765c ("genirq: Expose default irq affinity mask (take 3)")
    
    which introduced the circumvention of the affinity setting restrictions for
    interrupt which cannot be moved in process context.
    
    The whole exercise is bogus in various aspects:
    
      1) If the interrupt is already started up then there is absolutely
         no point to honour a bogus interrupt affinity setting from user
         space. The interrupt is already assigned to an online CPU and it
         does not make any sense to reassign it to some other randomly
         chosen online CPU.
    
      2) If the interupt is not yet started up then there is no point
         either. A subsequent startup of the interrupt will invoke
         irq_setup_affinity() anyway which will chose a valid target CPU.
    
    So the only correct solution is to just return -EINVAL in case user space
    wrote an affinity mask which does not contain any online CPUs, except for
    ALPHA which has it's own magic sauce for this.
    
    Fixes: 18404756765c ("genirq: Expose default irq affinity mask (take 3)")
    Reported-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Qian Cai <cai@lca.pw>
    Link: https://lkml.kernel.org/r/878sl8xdbm.fsf@nanos.tec.linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d3daa3edcf879828fe6767f71b00fc44e24bdd6e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 12 12:19:41 2020 +0100

    genirq/proc: Reject invalid affinity masks (again)
    
    commit cba6437a1854fde5934098ec3bd0ee83af3129f5 upstream.
    
    Qian Cai reported that the WARN_ON() in the x86/msi affinity setting code,
    which catches cases where the affinity setting is not done on the CPU which
    is the current target of the interrupt, triggers during CPU hotplug stress
    testing.
    
    It turns out that the warning which was added with the commit addressing
    the MSI affinity race unearthed yet another long standing bug.
    
    If user space writes a bogus affinity mask, i.e. it contains no online CPUs,
    then it calls irq_select_affinity_usr(). This was introduced for ALPHA in
    
      eee45269b0f5 ("[PATCH] Alpha: convert to generic irq framework (generic part)")
    
    and subsequently made available for all architectures in
    
      18404756765c ("genirq: Expose default irq affinity mask (take 3)")
    
    which introduced the circumvention of the affinity setting restrictions for
    interrupt which cannot be moved in process context.
    
    The whole exercise is bogus in various aspects:
    
      1) If the interrupt is already started up then there is absolutely
         no point to honour a bogus interrupt affinity setting from user
         space. The interrupt is already assigned to an online CPU and it
         does not make any sense to reassign it to some other randomly
         chosen online CPU.
    
      2) If the interupt is not yet started up then there is no point
         either. A subsequent startup of the interrupt will invoke
         irq_setup_affinity() anyway which will chose a valid target CPU.
    
    So the only correct solution is to just return -EINVAL in case user space
    wrote an affinity mask which does not contain any online CPUs, except for
    ALPHA which has it's own magic sauce for this.
    
    Fixes: 18404756765c ("genirq: Expose default irq affinity mask (take 3)")
    Reported-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Qian Cai <cai@lca.pw>
    Link: https://lkml.kernel.org/r/878sl8xdbm.fsf@nanos.tec.linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 45d0b75b98bf1de4b3a5b09188c75f3bfa3152b0
Merge: 7058b837899f f091bf39700d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 27 21:52:18 2020 -0800

    Merge tag 'drm-fixes-2020-02-28' of git://anongit.freedesktop.org/drm/drm
    
    Pull drm fixes from Dave Airlie:
     "Just some fixes for this week: amdgpu, radeon and i915.
    
      The main i915 one is a regression Gen7 (Ivybridge/Haswell), this moves
      them back from trying to use the full-ppgtt support to the aliasing
      version it used to use due to gpu hangs. Otherwise it's pretty quiet.
    
      amdgpu:
       - Drop DRIVER_USE_AGP
       - Fix memory leak in GPU reset
       - Resume fix for raven
    
      radeon:
       - Drop DRIVER_USE_AGP
    
      i915:
       - downgrade gen7 back to aliasing-ppgtt to avoid GPU hangs
       - shrinker fix
       - pmu leak and double free fixes
       - gvt user after free and virtual display reset fixes
       - randconfig build fix"
    
    * tag 'drm-fixes-2020-02-28' of git://anongit.freedesktop.org/drm/drm:
      drm/radeon: Inline drm_get_pci_dev
      drm/amdgpu: Drop DRIVER_USE_AGP
      drm/i915: Avoid recursing onto active vma from the shrinker
      drm/i915/pmu: Avoid using globals for PMU events
      drm/i915/pmu: Avoid using globals for CPU hotplug state
      drm/i915/gtt: Downgrade gen7 (ivb, byt, hsw) back to aliasing-ppgtt
      drm/i915: fix header test with GCOV
      amdgpu/gmc_v9: save/restore sdpif regs during S3
      drm/amdgpu: fix memory leak during TDR test(v2)
      drm/i915/gvt: Fix orphan vgpu dmabuf_objs' lifetime
      drm/i915/gvt: Separate display reset from ALL_ENGINES reset

commit 19ee5e8da6129d8d828201a12264ab3d09153ec4
Author: Micha Winiarski <michal.winiarski@intel.com>
Date:   Wed Feb 19 17:18:21 2020 +0100

    drm/i915/pmu: Avoid using globals for CPU hotplug state
    
    Attempting to bind / unbind module from devices where we have both
    integrated and discreete GPU handled by i915 can lead to leaks and
    warnings from cpuhp:
    Error: Removing state XXX which has instances left.
    
    Let's move the state to i915_pmu.
    
    Fixes: 05488673a4d4 ("drm/i915/pmu: Support multiple GPUs")
    Signed-off-by: Micha Winiarski <michal.winiarski@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Michal Wajdeczko <michal.wajdeczko@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200219161822.24592-1-michal.winiarski@intel.com
    (cherry picked from commit f5a179d4687d4e7bfadd7cbda7ee5d0bad76761f)
    Signed-off-by: Jani Nikula <jani.nikula@intel.com>

commit 4c6a908ed7d55074aa334abe235ddee383c3a7f9
Author: Leonard Crestez <leonard.crestez@nxp.com>
Date:   Tue Jan 14 22:25:46 2020 +0200

    perf/imx_ddr: Fix cpu hotplug state cleanup
    
    [ Upstream commit 9ee68b314e9aa63ed11b98beb8a68810b8234dcf ]
    
    This driver allocates a dynamic cpu hotplug state but never releases it.
    If reloaded in a loop it will quickly trigger a WARN message:
    
            "No more dynamic states available for CPU hotplug"
    
    Fix by calling cpuhp_remove_multi_state on remove like several other
    perf pmu drivers.
    
    Also fix the cleanup logic on probe error paths: add the missing
    cpuhp_remove_multi_state call and properly check the return value from
    cpuhp_state_add_instant_nocalls.
    
    Fixes: 9a66d36cc7ac ("drivers/perf: imx_ddr: Add DDR performance counter support to perf")
    Acked-by: Joakim Zhang <qiangqing.zhang@nxp.com>
    Signed-off-by: Leonard Crestez <leonard.crestez@nxp.com>
    Signed-off-by: Will Deacon <will@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 3f4d3d010477365ee1aefedc3ad52563740b777e
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Tue Dec 3 14:31:10 2019 -0500

    padata: validate cpumask without removed CPU during offline
    
    [ Upstream commit 894c9ef9780c5cf2f143415e867ee39a33ecb75d ]
    
    Configuring an instance's parallel mask without any online CPUs...
    
      echo 2 > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
      echo 0 > /sys/devices/system/cpu/cpu1/online
    
    ...makes tcrypt mode=215 crash like this:
    
      divide error: 0000 [#1] SMP PTI
      CPU: 4 PID: 283 Comm: modprobe Not tainted 5.4.0-rc8-padata-doc-v2+ #2
      Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS ?-20191013_105130-anatol 04/01/2014
      RIP: 0010:padata_do_parallel+0x114/0x300
      Call Trace:
       pcrypt_aead_encrypt+0xc0/0xd0 [pcrypt]
       crypto_aead_encrypt+0x1f/0x30
       do_mult_aead_op+0x4e/0xdf [tcrypt]
       test_mb_aead_speed.constprop.0.cold+0x226/0x564 [tcrypt]
       do_test+0x28c2/0x4d49 [tcrypt]
       tcrypt_mod_init+0x55/0x1000 [tcrypt]
       ...
    
    cpumask_weight() in padata_cpu_hash() returns 0 because the mask has no
    CPUs.  The problem is __padata_remove_cpu() checks for valid masks too
    early and so doesn't mark the instance PADATA_INVALID as expected, which
    would have made padata_do_parallel() return error before doing the
    division.
    
    Fix by introducing a second padata CPU hotplug state before
    CPUHP_BRINGUP_CPU so that __padata_remove_cpu() sees the online mask
    without @cpu.  No need for the second argument to padata_replace() since
    @cpu is now already missing from the online mask.
    
    Fixes: 33e54450683c ("padata: Handle empty padata cpumasks")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Eric Biggers <ebiggers@kernel.org>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 9112d1ef5a1b8e1f4b47a9926fa7abd8274c50ab
Author: Leonard Crestez <leonard.crestez@nxp.com>
Date:   Tue Jan 14 22:25:46 2020 +0200

    perf/imx_ddr: Fix cpu hotplug state cleanup
    
    [ Upstream commit 9ee68b314e9aa63ed11b98beb8a68810b8234dcf ]
    
    This driver allocates a dynamic cpu hotplug state but never releases it.
    If reloaded in a loop it will quickly trigger a WARN message:
    
            "No more dynamic states available for CPU hotplug"
    
    Fix by calling cpuhp_remove_multi_state on remove like several other
    perf pmu drivers.
    
    Also fix the cleanup logic on probe error paths: add the missing
    cpuhp_remove_multi_state call and properly check the return value from
    cpuhp_state_add_instant_nocalls.
    
    Fixes: 9a66d36cc7ac ("drivers/perf: imx_ddr: Add DDR performance counter support to perf")
    Acked-by: Joakim Zhang <qiangqing.zhang@nxp.com>
    Signed-off-by: Leonard Crestez <leonard.crestez@nxp.com>
    Signed-off-by: Will Deacon <will@kernel.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 0685dfa0a2ff7635c0b64f7b7f0fafbf1c3e0c14
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Tue Dec 3 14:31:10 2019 -0500

    padata: validate cpumask without removed CPU during offline
    
    [ Upstream commit 894c9ef9780c5cf2f143415e867ee39a33ecb75d ]
    
    Configuring an instance's parallel mask without any online CPUs...
    
      echo 2 > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
      echo 0 > /sys/devices/system/cpu/cpu1/online
    
    ...makes tcrypt mode=215 crash like this:
    
      divide error: 0000 [#1] SMP PTI
      CPU: 4 PID: 283 Comm: modprobe Not tainted 5.4.0-rc8-padata-doc-v2+ #2
      Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS ?-20191013_105130-anatol 04/01/2014
      RIP: 0010:padata_do_parallel+0x114/0x300
      Call Trace:
       pcrypt_aead_encrypt+0xc0/0xd0 [pcrypt]
       crypto_aead_encrypt+0x1f/0x30
       do_mult_aead_op+0x4e/0xdf [tcrypt]
       test_mb_aead_speed.constprop.0.cold+0x226/0x564 [tcrypt]
       do_test+0x28c2/0x4d49 [tcrypt]
       tcrypt_mod_init+0x55/0x1000 [tcrypt]
       ...
    
    cpumask_weight() in padata_cpu_hash() returns 0 because the mask has no
    CPUs.  The problem is __padata_remove_cpu() checks for valid masks too
    early and so doesn't mark the instance PADATA_INVALID as expected, which
    would have made padata_do_parallel() return error before doing the
    division.
    
    Fix by introducing a second padata CPU hotplug state before
    CPUHP_BRINGUP_CPU so that __padata_remove_cpu() sees the online mask
    without @cpu.  No need for the second argument to padata_replace() since
    @cpu is now already missing from the online mask.
    
    Fixes: 33e54450683c ("padata: Handle empty padata cpumasks")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Eric Biggers <ebiggers@kernel.org>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit f5a179d4687d4e7bfadd7cbda7ee5d0bad76761f
Author: Micha Winiarski <michal.winiarski@intel.com>
Date:   Wed Feb 19 17:18:21 2020 +0100

    drm/i915/pmu: Avoid using globals for CPU hotplug state
    
    Attempting to bind / unbind module from devices where we have both
    integrated and discreete GPU handled by i915 can lead to leaks and
    warnings from cpuhp:
    Error: Removing state XXX which has instances left.
    
    Let's move the state to i915_pmu.
    
    Fixes: 05488673a4d4 ("drm/i915/pmu: Support multiple GPUs")
    Signed-off-by: Micha Winiarski <michal.winiarski@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Michal Wajdeczko <michal.wajdeczko@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20200219161822.24592-1-michal.winiarski@intel.com

commit a59ee765a6890e7f4281070008a2654337458311
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Thu Dec 5 10:49:11 2019 -0800

    torture: Forgive -EBUSY from boottime CPU-hotplug operations
    
    During boot, CPU hotplug is often disabled, for example by PCI probing.
    On large systems that take substantial time to boot, this can result
    in spurious RCU_HOTPLUG errors.  This commit therefore forgives any
    boottime -EBUSY CPU-hotplug failures by adjusting counters to pretend
    that the corresponding attempt never happened.  A non-splat record
    of the failed attempt is emitted to the console with the added string
    "(-EBUSY forgiven during boot)".
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit 28e09a2e48486ce8ff0a72e21570d59b1243b308
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Thu Jan 30 20:37:04 2020 -0800

    locktorture: Forgive apparent unfairness if CPU hotplug
    
    If CPU hotplug testing is enabled, a lock might appear to be maximally
    unfair just because one of the CPUs was offline almost all the time.
    This commit therefore forgives unfairness if CPU hotplug testing was
    enabled.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

commit cba6437a1854fde5934098ec3bd0ee83af3129f5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 12 12:19:41 2020 +0100

    genirq/proc: Reject invalid affinity masks (again)
    
    Qian Cai reported that the WARN_ON() in the x86/msi affinity setting code,
    which catches cases where the affinity setting is not done on the CPU which
    is the current target of the interrupt, triggers during CPU hotplug stress
    testing.
    
    It turns out that the warning which was added with the commit addressing
    the MSI affinity race unearthed yet another long standing bug.
    
    If user space writes a bogus affinity mask, i.e. it contains no online CPUs,
    then it calls irq_select_affinity_usr(). This was introduced for ALPHA in
    
      eee45269b0f5 ("[PATCH] Alpha: convert to generic irq framework (generic part)")
    
    and subsequently made available for all architectures in
    
      18404756765c ("genirq: Expose default irq affinity mask (take 3)")
    
    which introduced the circumvention of the affinity setting restrictions for
    interrupt which cannot be moved in process context.
    
    The whole exercise is bogus in various aspects:
    
      1) If the interrupt is already started up then there is absolutely
         no point to honour a bogus interrupt affinity setting from user
         space. The interrupt is already assigned to an online CPU and it
         does not make any sense to reassign it to some other randomly
         chosen online CPU.
    
      2) If the interupt is not yet started up then there is no point
         either. A subsequent startup of the interrupt will invoke
         irq_setup_affinity() anyway which will chose a valid target CPU.
    
    So the only correct solution is to just return -EINVAL in case user space
    wrote an affinity mask which does not contain any online CPUs, except for
    ALPHA which has it's own magic sauce for this.
    
    Fixes: 18404756765c ("genirq: Expose default irq affinity mask (take 3)")
    Reported-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Qian Cai <cai@lca.pw>
    Link: https://lkml.kernel.org/r/878sl8xdbm.fsf@nanos.tec.linutronix.de

commit b843cd306a170d007f1438e67d7282095e6f8126
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Sun Sep 27 02:09:25 2015 +0900

    blk-mq: fix deadlock when reading cpu_list
    
    commit 60de074ba1e8f327db19bc33d8530131ac01695d upstream.
    
    CPU hotplug handling for blk-mq (blk_mq_queue_reinit) acquires
    all_q_mutex in blk_mq_queue_reinit_notify() and then removes sysfs
    entries by blk_mq_sysfs_unregister().  Removing sysfs entry needs to
    be blocked until the active reference of the kernfs_node to be zero.
    
    On the other hand, reading blk_mq_hw_sysfs_cpu sysfs entry (e.g.
    /sys/block/nullb0/mq/0/cpu_list) acquires all_q_mutex in
    blk_mq_hw_sysfs_cpus_show().
    
    If these happen at the same time, a deadlock can happen.  Because one
    can wait for the active reference to be zero with holding all_q_mutex,
    and the other tries to acquire all_q_mutex with holding the active
    reference.
    
    The reason that all_q_mutex is acquired in blk_mq_hw_sysfs_cpus_show()
    is to avoid reading an imcomplete hctx->cpumask.  Since reading sysfs
    entry for blk-mq needs to acquire q->sysfs_lock, we can avoid deadlock
    and reading an imcomplete hctx->cpumask by protecting q->sysfs_lock
    while hctx->cpumask is being updated.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Reviewed-by: Ming Lei <tom.leiming@gmail.com>
    Cc: Ming Lei <tom.leiming@gmail.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 93501e7330aacb38e15d26aaddc1df3485acff91
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Thu Mar 7 08:41:22 2019 +0300

    xen, cpu_hotplug: Prevent an out of bounds access
    
    [ Upstream commit 201676095dda7e5b31a5e1d116d10fc22985075e ]
    
    The "cpu" variable comes from the sscanf() so Smatch marks it as
    untrusted data.  We can't pass a higher value than "nr_cpu_ids" to
    cpu_possible() or it results in an out of bounds access.
    
    Fixes: d68d82afd4c8 ("xen: implement CPU hotplugging")
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 7c7d32d6927bf5b3f5434f73e03d58827b42b184
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Thu Mar 7 08:41:22 2019 +0300

    xen, cpu_hotplug: Prevent an out of bounds access
    
    [ Upstream commit 201676095dda7e5b31a5e1d116d10fc22985075e ]
    
    The "cpu" variable comes from the sscanf() so Smatch marks it as
    untrusted data.  We can't pass a higher value than "nr_cpu_ids" to
    cpu_possible() or it results in an out of bounds access.
    
    Fixes: d68d82afd4c8 ("xen: implement CPU hotplugging")
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit a663874605954bb50b940e6c315774c9da01b4e7
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Thu Mar 7 08:41:22 2019 +0300

    xen, cpu_hotplug: Prevent an out of bounds access
    
    [ Upstream commit 201676095dda7e5b31a5e1d116d10fc22985075e ]
    
    The "cpu" variable comes from the sscanf() so Smatch marks it as
    untrusted data.  We can't pass a higher value than "nr_cpu_ids" to
    cpu_possible() or it results in an out of bounds access.
    
    Fixes: d68d82afd4c8 ("xen: implement CPU hotplugging")
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2cd5e08b9af2b40ca7537ec2c66f1459cd95ad8b
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Thu Mar 7 08:41:22 2019 +0300

    xen, cpu_hotplug: Prevent an out of bounds access
    
    [ Upstream commit 201676095dda7e5b31a5e1d116d10fc22985075e ]
    
    The "cpu" variable comes from the sscanf() so Smatch marks it as
    untrusted data.  We can't pass a higher value than "nr_cpu_ids" to
    cpu_possible() or it results in an out of bounds access.
    
    Fixes: d68d82afd4c8 ("xen: implement CPU hotplugging")
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 11ea68f553e244851d15793a7fa33a97c46d8271
Author: Ming Lei <ming.lei@redhat.com>
Date:   Mon Jan 20 17:16:25 2020 +0800

    genirq, sched/isolation: Isolate from handling managed interrupts
    
    The affinity of managed interrupts is completely handled in the kernel and
    cannot be changed via the /proc/irq/* interfaces from user space. As the
    kernel tries to spread out interrupts evenly accross CPUs on x86 to prevent
    vector exhaustion, it can happen that a managed interrupt whose affinity
    mask contains both isolated and housekeeping CPUs is routed to an isolated
    CPU. As a consequence IO submitted on a housekeeping CPU causes interrupts
    on the isolated CPU.
    
    Add a new sub-parameter 'managed_irq' for 'isolcpus' and the corresponding
    logic in the interrupt affinity selection code.
    
    The subparameter indicates to the interrupt affinity selection logic that
    it should try to avoid the above scenario.
    
    This isolation is best effort and only effective if the automatically
    assigned interrupt mask of a device queue contains isolated and
    housekeeping CPUs. If housekeeping CPUs are online then such interrupts are
    directed to the housekeeping CPU so that IO submitted on the housekeeping
    CPU cannot disturb the isolated CPU.
    
    If a queue's affinity mask contains only isolated CPUs then this parameter
    has no effect on the interrupt routing decision, though interrupts are only
    happening when tasks running on those isolated CPUs submit IO. IO submitted
    on housekeeping CPUs has no influence on those queues.
    
    If the affinity mask contains both housekeeping and isolated CPUs, but none
    of the contained housekeeping CPUs is online, then the interrupt is also
    routed to an isolated CPU. Interrupts are only delivered when one of the
    isolated CPUs in the affinity mask submits IO. If one of the contained
    housekeeping CPUs comes online, the CPU hotplug logic migrates the
    interrupt automatically back to the upcoming housekeeping CPU. Depending on
    the type of interrupt controller, this can require that at least one
    interrupt is delivered to the isolated CPU in order to complete the
    migration.
    
    [ tglx: Removed unused parameter, added and edited comments/documentation
            and rephrased the changelog so it contains more details. ]
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20200120091625.17912-1-ming.lei@redhat.com

commit f6c771f16fd492889bac4459c9da50ecb628b43d
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Fri Nov 29 16:23:02 2019 +0100

    ARM: 8943/1: Fix topology setup in case of CPU hotplug for CONFIG_SCHED_MC
    
    commit ff98a5f624d2910de050f1fc7f2a32769da86b51 upstream.
    
    Commit ca74b316df96 ("arm: Use common cpu_topology structure and
    functions.") changed cpu_coregroup_mask() from the ARM32 specific
    implementation in arch/arm/include/asm/topology.h to the one shared
    with ARM64 and RISCV in drivers/base/arch_topology.c.
    
    Currently on ARM32 (TC2 w/ CONFIG_SCHED_MC) the task scheduler setup
    code (w/ CONFIG_SCHED_DEBUG) shows this during CPU hotplug:
    
      ERROR: groups don't span domain->span
    
    It happens to CPUs of the cluster of the CPU which gets hot-plugged
    out on scheduler domain MC.
    
    Turns out that the shared cpu_coregroup_mask() requires that the
    hot-plugged CPU is removed from the core_sibling mask via
    remove_cpu_topology(). Otherwise the 'is core_sibling subset of
    cpumask_of_node()' doesn't work. In this case the task scheduler has to
    deal with cpumask_of_node instead of core_sibling which is wrong on
    scheduler domain MC.
    
    e.g. CPU3 hot-plugged out on TC2 [cluster0: 0,3-4 cluster1: 1-2]:
    
      cpu_coregroup_mask(): CPU3 cpumask_of_node=0-2,4 core_sibling=0,3-4
                                                                      ^
    should be:
    
      cpu_coregroup_mask(): CPU3 cpumask_of_node=0-2,4 core_sibling=0,4
    
    Add remove_cpu_topology() to __cpu_disable() to remove the CPU from the
    topology masks in case of a CPU hotplug out operation.
    
    At the same time tweak store_cpu_topology() slightly so it will call
    update_siblings_masks() in case of CPU hotplug in operation via
    secondary_start_kernel()->smp_store_cpu_info().
    
    This aligns the ARM32 implementation with the ARM64 one.
    
    Guarding remove_cpu_topology() with CONFIG_GENERIC_ARCH_TOPOLOGY is
    necessary since some Arm32 defconfigs (aspeed_g5_defconfig,
    milbeaut_m10v_defconfig, spear13xx_defconfig) specify an explicit
    
     # CONFIG_ARM_CPU_TOPOLOGY is not set
    
    w/ ./arch/arm/Kconfig: select GENERIC_ARCH_TOPOLOGY if ARM_CPU_TOPOLOGY
    
    Fixes: ca74b316df96 ("arm: Use common cpu_topology structure and functions")
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Reviewed-by: Lukasz Luba <lukasz.luba@arm.com>
    Tested-by: Lukasz Luba <lukasz.luba@arm.com>
    Tested-by: Ondrej Jirman <megous@megous.com>
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9ee68b314e9aa63ed11b98beb8a68810b8234dcf
Author: Leonard Crestez <leonard.crestez@nxp.com>
Date:   Tue Jan 14 22:25:46 2020 +0200

    perf/imx_ddr: Fix cpu hotplug state cleanup
    
    This driver allocates a dynamic cpu hotplug state but never releases it.
    If reloaded in a loop it will quickly trigger a WARN message:
    
            "No more dynamic states available for CPU hotplug"
    
    Fix by calling cpuhp_remove_multi_state on remove like several other
    perf pmu drivers.
    
    Also fix the cleanup logic on probe error paths: add the missing
    cpuhp_remove_multi_state call and properly check the return value from
    cpuhp_state_add_instant_nocalls.
    
    Fixes: 9a66d36cc7ac ("drivers/perf: imx_ddr: Add DDR performance counter support to perf")
    Acked-by: Joakim Zhang <qiangqing.zhang@nxp.com>
    Signed-off-by: Leonard Crestez <leonard.crestez@nxp.com>
    Signed-off-by: Will Deacon <will@kernel.org>

commit d7ce45829cbfff0c4fc082d05823dac8cd241fdf
Author: Harry Pan <harry.pan@intel.com>
Date:   Mon Dec 30 22:36:56 2019 +0800

    powercap: intel_rapl: add NULL pointer check to rapl_mmio_cpu_online()
    
    commit 3aa3c5882e4fb2274448908aaed605a3ed7dd15d upstream.
    
    RAPL MMIO support depends on the RAPL common driver.  During CPU
    initialization rapl_mmio_cpu_online() is called via CPU hotplug
    to initialize the MMIO RAPL for the new CPU, but if that CPU is
    not present in the common RAPL driver's support list, rapl_defaults
    is NULL and the kernel crashes on an attempt to dereference it:
    
    [    4.188566] BUG: kernel NULL pointer dereference, address: 0000000000000020
    ...snip...
    [    4.189555] RIP: 0010:rapl_add_package+0x223/0x574
    [    4.189555] Code: b5 a0 31 c0 49 8b 4d 78 48 01 d9 48 8b 0c c1 49 89 4c c6 10 48 ff c0 48 83 f8 05 75 e7 49 83 ff 03 75 15 48 8b 05 09 bc 18 01 <8b> 70 20 41 89 b6 0c 05 00 00 85 f6 75 1a 49 81 c6 18 9
    [    4.189555] RSP: 0000:ffffb3adc00b3d90 EFLAGS: 00010246
    [    4.189555] RAX: 0000000000000000 RBX: 0000000000000098 RCX: 0000000000000000
    [    4.267161] usb 1-1: New USB device found, idVendor=2109, idProduct=2812, bcdDevice= b.e0
    [    4.189555] RDX: 0000000000001000 RSI: 0000000000000000 RDI: ffff9340caafd000
    [    4.189555] RBP: ffffb3adc00b3df8 R08: ffffffffa0246e28 R09: ffff9340caafc000
    [    4.189555] R10: 000000000000024a R11: ffffffff9ff1f6f2 R12: 00000000ffffffed
    [    4.189555] R13: ffff9340caa94800 R14: ffff9340caafc518 R15: 0000000000000003
    [    4.189555] FS:  0000000000000000(0000) GS:ffff9340ce200000(0000) knlGS:0000000000000000
    [    4.189555] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [    4.189555] CR2: 0000000000000020 CR3: 0000000302c14001 CR4: 00000000003606f0
    [    4.189555] Call Trace:
    [    4.189555]  ? __switch_to_asm+0x40/0x70
    [    4.189555]  rapl_mmio_cpu_online+0x47/0x64
    [    4.189555]  ? rapl_mmio_write_raw+0x33/0x33
    [    4.281059] usb 1-1: New USB device strings: Mfr=1, Product=2, SerialNumber=0
    [    4.189555]  cpuhp_invoke_callback+0x29f/0x66f
    [    4.189555]  ? __schedule+0x46d/0x6a0
    [    4.189555]  cpuhp_thread_fun+0xb9/0x11c
    [    4.189555]  smpboot_thread_fn+0x17d/0x22f
    [    4.297006] usb 1-1: Product: USB2.0 Hub
    [    4.189555]  ? cpu_report_death+0x43/0x43
    [    4.189555]  kthread+0x137/0x13f
    [    4.189555]  ? cpu_report_death+0x43/0x43
    [    4.189555]  ? kthread_blkcg+0x2e/0x2e
    [    4.312951] usb 1-1: Manufacturer: VIA Labs, Inc.
    [    4.189555]  ret_from_fork+0x1f/0x40
    [    4.189555] Modules linked in:
    [    4.189555] CR2: 0000000000000020
    [    4.189555] ---[ end trace 01bb812aabc791f4 ]---
    
    To avoid that problem, check rapl_defaults NULL upfront and return an
    error code if it is NULL.  [Note that it does not make sense to even
    try to allocate memory in that case, because it is not going to be
    used anyway.]
    
    Fixes: 555c45fe0d04 ("int340X/processor_thermal_device: add support for MMIO RAPL")
    Cc: 5.3+ <stable@vger.kernel.org> # 5.3+
    Signed-off-by: Harry Pan <harry.pan@intel.com>
    [ rjw: Subject & changelog ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 324172d2adcce8d2b927c0d667ca4da6f88bbb92
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed Nov 27 15:56:40 2019 +0000

    ARM: vexpress: Set-up shared OPP table instead of individual for each CPU
    
    [ Upstream commit 2a76352ad2cc6b78e58f737714879cc860903802 ]
    
    Currently we add individual copy of same OPP table for each CPU within
    the cluster. This is redundant and doesn't reflect the reality.
    
    We can't use core cpumask to set policy->cpus in ve_spc_cpufreq_init()
    anymore as it gets called via cpuhp_cpufreq_online()->cpufreq_online()
    ->cpufreq_driver->init() and the cpumask gets updated upon CPU hotplug
    operations. It also may cause issues when the vexpress_spc_cpufreq
    driver is built as a module.
    
    Since ve_spc_clk_init is built-in device initcall, we should be able to
    use the same topology_core_cpumask to set the opp sharing cpumask via
    dev_pm_opp_set_sharing_cpus and use the same later in the driver via
    dev_pm_opp_get_sharing_cpus.
    
    Cc: Liviu Dudau <liviu.dudau@arm.com>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 829fde079bd99fc03bacb0c3b98cf27b43799a67
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed Nov 27 15:56:40 2019 +0000

    ARM: vexpress: Set-up shared OPP table instead of individual for each CPU
    
    [ Upstream commit 2a76352ad2cc6b78e58f737714879cc860903802 ]
    
    Currently we add individual copy of same OPP table for each CPU within
    the cluster. This is redundant and doesn't reflect the reality.
    
    We can't use core cpumask to set policy->cpus in ve_spc_cpufreq_init()
    anymore as it gets called via cpuhp_cpufreq_online()->cpufreq_online()
    ->cpufreq_driver->init() and the cpumask gets updated upon CPU hotplug
    operations. It also may cause issues when the vexpress_spc_cpufreq
    driver is built as a module.
    
    Since ve_spc_clk_init is built-in device initcall, we should be able to
    use the same topology_core_cpumask to set the opp sharing cpumask via
    dev_pm_opp_set_sharing_cpus and use the same later in the driver via
    dev_pm_opp_get_sharing_cpus.
    
    Cc: Liviu Dudau <liviu.dudau@arm.com>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 210670f32876544b6cb7613dc4d1c7b63dec03d0
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed Nov 27 15:56:40 2019 +0000

    ARM: vexpress: Set-up shared OPP table instead of individual for each CPU
    
    [ Upstream commit 2a76352ad2cc6b78e58f737714879cc860903802 ]
    
    Currently we add individual copy of same OPP table for each CPU within
    the cluster. This is redundant and doesn't reflect the reality.
    
    We can't use core cpumask to set policy->cpus in ve_spc_cpufreq_init()
    anymore as it gets called via cpuhp_cpufreq_online()->cpufreq_online()
    ->cpufreq_driver->init() and the cpumask gets updated upon CPU hotplug
    operations. It also may cause issues when the vexpress_spc_cpufreq
    driver is built as a module.
    
    Since ve_spc_clk_init is built-in device initcall, we should be able to
    use the same topology_core_cpumask to set the opp sharing cpumask via
    dev_pm_opp_set_sharing_cpus and use the same later in the driver via
    dev_pm_opp_get_sharing_cpus.
    
    Cc: Liviu Dudau <liviu.dudau@arm.com>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit b8c68da60cfae131b888c78a8bcc2a1d1df04e43
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed Nov 27 15:56:40 2019 +0000

    ARM: vexpress: Set-up shared OPP table instead of individual for each CPU
    
    [ Upstream commit 2a76352ad2cc6b78e58f737714879cc860903802 ]
    
    Currently we add individual copy of same OPP table for each CPU within
    the cluster. This is redundant and doesn't reflect the reality.
    
    We can't use core cpumask to set policy->cpus in ve_spc_cpufreq_init()
    anymore as it gets called via cpuhp_cpufreq_online()->cpufreq_online()
    ->cpufreq_driver->init() and the cpumask gets updated upon CPU hotplug
    operations. It also may cause issues when the vexpress_spc_cpufreq
    driver is built as a module.
    
    Since ve_spc_clk_init is built-in device initcall, we should be able to
    use the same topology_core_cpumask to set the opp sharing cpumask via
    dev_pm_opp_set_sharing_cpus and use the same later in the driver via
    dev_pm_opp_get_sharing_cpus.
    
    Cc: Liviu Dudau <liviu.dudau@arm.com>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2c88e1c45bcbdc82f1404c1fd91177f1687b7227
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed Nov 27 15:56:40 2019 +0000

    ARM: vexpress: Set-up shared OPP table instead of individual for each CPU
    
    [ Upstream commit 2a76352ad2cc6b78e58f737714879cc860903802 ]
    
    Currently we add individual copy of same OPP table for each CPU within
    the cluster. This is redundant and doesn't reflect the reality.
    
    We can't use core cpumask to set policy->cpus in ve_spc_cpufreq_init()
    anymore as it gets called via cpuhp_cpufreq_online()->cpufreq_online()
    ->cpufreq_driver->init() and the cpumask gets updated upon CPU hotplug
    operations. It also may cause issues when the vexpress_spc_cpufreq
    driver is built as a module.
    
    Since ve_spc_clk_init is built-in device initcall, we should be able to
    use the same topology_core_cpumask to set the opp sharing cpumask via
    dev_pm_opp_set_sharing_cpus and use the same later in the driver via
    dev_pm_opp_get_sharing_cpus.
    
    Cc: Liviu Dudau <liviu.dudau@arm.com>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 40a9012a3b24334796403491b317a83935719809
Merge: 1bee7aaa075b e37131556801
Author: Olof Johansson <olof@lixom.net>
Date:   Wed Jan 8 10:26:26 2020 -0800

    Merge tag 'cpuidle_psci-v5.5-rc4' of git://git.linaro.org/people/ulf.hansson/linux-pm into arm/drivers
    
    Initial support for hierarchical CPU arrangement, managed by PSCI and its
    corresponding cpuidle driver. This support is based upon using the generic
    PM domain, which already supports devices belonging to CPUs.
    
    Finally, these is a DTS patch that enables the hierarchical topology to be
    used for the Qcom 410c Dragonboard, which supports the PSCI OS-initiated
    mode.
    
    * tag 'cpuidle_psci-v5.5-rc4' of git://git.linaro.org/people/ulf.hansson/linux-pm: (611 commits)
      arm64: dts: Convert to the hierarchical CPU topology layout for MSM8916
      cpuidle: psci: Add support for PM domains by using genpd
      PM / Domains: Introduce a genpd OF helper that removes a subdomain
      cpuidle: psci: Support CPU hotplug for the hierarchical model
      cpuidle: psci: Manage runtime PM in the idle path
      cpuidle: psci: Prepare to use OS initiated suspend mode via PM domains
      cpuidle: psci: Attach CPU devices to their PM domains
      cpuidle: psci: Add a helper to attach a CPU to its PM domain
      cpuidle: psci: Support hierarchical CPU idle states
      cpuidle: psci: Simplify OF parsing of CPU idle state nodes
      cpuidle: dt: Support hierarchical CPU idle states
      of: base: Add of_get_cpu_state_node() to get idle states for a CPU node
      firmware: psci: Export functions to manage the OSI mode
      dt: psci: Update DT bindings to support hierarchical PSCI states
      cpuidle: psci: Align psci_power_state count with idle state count
      Linux 5.5-rc4
      locks: print unsigned ino in /proc/locks
      riscv: export flush_icache_all to modules
      riscv: reject invalid syscalls below -1
      riscv: fix compile failure with EXPORT_SYMBOL() & !MMU
      ...
    
    Link: https://lore.kernel.org/r/20200102160820.3572-1-ulf.hansson@linaro.org
    Signed-off-by: Olof Johansson <olof@lixom.net>

commit 3aa3c5882e4fb2274448908aaed605a3ed7dd15d
Author: Harry Pan <harry.pan@intel.com>
Date:   Mon Dec 30 22:36:56 2019 +0800

    powercap: intel_rapl: add NULL pointer check to rapl_mmio_cpu_online()
    
    RAPL MMIO support depends on the RAPL common driver.  During CPU
    initialization rapl_mmio_cpu_online() is called via CPU hotplug
    to initialize the MMIO RAPL for the new CPU, but if that CPU is
    not present in the common RAPL driver's support list, rapl_defaults
    is NULL and the kernel crashes on an attempt to dereference it:
    
    [    4.188566] BUG: kernel NULL pointer dereference, address: 0000000000000020
    ...snip...
    [    4.189555] RIP: 0010:rapl_add_package+0x223/0x574
    [    4.189555] Code: b5 a0 31 c0 49 8b 4d 78 48 01 d9 48 8b 0c c1 49 89 4c c6 10 48 ff c0 48 83 f8 05 75 e7 49 83 ff 03 75 15 48 8b 05 09 bc 18 01 <8b> 70 20 41 89 b6 0c 05 00 00 85 f6 75 1a 49 81 c6 18 9
    [    4.189555] RSP: 0000:ffffb3adc00b3d90 EFLAGS: 00010246
    [    4.189555] RAX: 0000000000000000 RBX: 0000000000000098 RCX: 0000000000000000
    [    4.267161] usb 1-1: New USB device found, idVendor=2109, idProduct=2812, bcdDevice= b.e0
    [    4.189555] RDX: 0000000000001000 RSI: 0000000000000000 RDI: ffff9340caafd000
    [    4.189555] RBP: ffffb3adc00b3df8 R08: ffffffffa0246e28 R09: ffff9340caafc000
    [    4.189555] R10: 000000000000024a R11: ffffffff9ff1f6f2 R12: 00000000ffffffed
    [    4.189555] R13: ffff9340caa94800 R14: ffff9340caafc518 R15: 0000000000000003
    [    4.189555] FS:  0000000000000000(0000) GS:ffff9340ce200000(0000) knlGS:0000000000000000
    [    4.189555] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [    4.189555] CR2: 0000000000000020 CR3: 0000000302c14001 CR4: 00000000003606f0
    [    4.189555] Call Trace:
    [    4.189555]  ? __switch_to_asm+0x40/0x70
    [    4.189555]  rapl_mmio_cpu_online+0x47/0x64
    [    4.189555]  ? rapl_mmio_write_raw+0x33/0x33
    [    4.281059] usb 1-1: New USB device strings: Mfr=1, Product=2, SerialNumber=0
    [    4.189555]  cpuhp_invoke_callback+0x29f/0x66f
    [    4.189555]  ? __schedule+0x46d/0x6a0
    [    4.189555]  cpuhp_thread_fun+0xb9/0x11c
    [    4.189555]  smpboot_thread_fn+0x17d/0x22f
    [    4.297006] usb 1-1: Product: USB2.0 Hub
    [    4.189555]  ? cpu_report_death+0x43/0x43
    [    4.189555]  kthread+0x137/0x13f
    [    4.189555]  ? cpu_report_death+0x43/0x43
    [    4.189555]  ? kthread_blkcg+0x2e/0x2e
    [    4.312951] usb 1-1: Manufacturer: VIA Labs, Inc.
    [    4.189555]  ret_from_fork+0x1f/0x40
    [    4.189555] Modules linked in:
    [    4.189555] CR2: 0000000000000020
    [    4.189555] ---[ end trace 01bb812aabc791f4 ]---
    
    To avoid that problem, check rapl_defaults NULL upfront and return an
    error code if it is NULL.  [Note that it does not make sense to even
    try to allocate memory in that case, because it is not going to be
    used anyway.]
    
    Fixes: 555c45fe0d04 ("int340X/processor_thermal_device: add support for MMIO RAPL")
    Cc: 5.3+ <stable@vger.kernel.org> # 5.3+
    Signed-off-by: Harry Pan <harry.pan@intel.com>
    [ rjw: Subject & changelog ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 0ecb8a7272f6725d42e9967f0b96d9f437d77f25
Author: Yunfeng Ye <yeyunfeng@huawei.com>
Date:   Mon Oct 21 19:31:21 2019 +0800

    arm64: psci: Reduce the waiting time for cpu_psci_cpu_kill()
    
    [ Upstream commit bfcef4ab1d7ee8921bc322109b1692036cc6cbe0 ]
    
    In cases like suspend-to-disk and suspend-to-ram, a large number of CPU
    cores need to be shut down. At present, the CPU hotplug operation is
    serialised, and the CPU cores can only be shut down one by one. In this
    process, if PSCI affinity_info() does not return LEVEL_OFF quickly,
    cpu_psci_cpu_kill() needs to wait for 10ms. If hundreds of CPU cores
    need to be shut down, it will take a long time.
    
    Normally, there is no need to wait 10ms in cpu_psci_cpu_kill(). So
    change the wait interval from 10 ms to max 1 ms and use usleep_range()
    instead of msleep() for more accurate timer.
    
    In addition, reducing the time interval will increase the messages
    output, so remove the "Retry ..." message, instead, track time and
    output to the the sucessful message.
    
    Signed-off-by: Yunfeng Ye <yeyunfeng@huawei.com>
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit afd3db1ca9048815b76ca20fca5d779169d4d995
Author: Yunfeng Ye <yeyunfeng@huawei.com>
Date:   Mon Oct 21 19:31:21 2019 +0800

    arm64: psci: Reduce the waiting time for cpu_psci_cpu_kill()
    
    [ Upstream commit bfcef4ab1d7ee8921bc322109b1692036cc6cbe0 ]
    
    In cases like suspend-to-disk and suspend-to-ram, a large number of CPU
    cores need to be shut down. At present, the CPU hotplug operation is
    serialised, and the CPU cores can only be shut down one by one. In this
    process, if PSCI affinity_info() does not return LEVEL_OFF quickly,
    cpu_psci_cpu_kill() needs to wait for 10ms. If hundreds of CPU cores
    need to be shut down, it will take a long time.
    
    Normally, there is no need to wait 10ms in cpu_psci_cpu_kill(). So
    change the wait interval from 10 ms to max 1 ms and use usleep_range()
    instead of msleep() for more accurate timer.
    
    In addition, reducing the time interval will increase the messages
    output, so remove the "Retry ..." message, instead, track time and
    output to the the sucessful message.
    
    Signed-off-by: Yunfeng Ye <yeyunfeng@huawei.com>
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 9c6ceecb6541954dfc410aa8883f872469326c73
Author: Ulf Hansson <ulf.hansson@linaro.org>
Date:   Thu Oct 10 12:01:48 2019 +0200

    cpuidle: psci: Support CPU hotplug for the hierarchical model
    
    When the hierarchical CPU topology is used and when a CPU is put offline,
    that CPU prevents its PM domain from being powered off, which is because
    genpd observes the corresponding attached device as being active from a
    runtime PM point of view. Furthermore, any potential master PM domains are
    also prevented from being powered off.
    
    To address this limitation, let's add add a new CPU hotplug state
    (CPUHP_AP_CPU_PM_STARTING) and register up/down callbacks for it, which
    allows us to deal with runtime PM accordingly.
    
    Signed-off-by: Ulf Hansson <ulf.hansson@linaro.org>
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Acked-by: Rafael J. Wysocki <rafael@kernel.org>

commit ed3b3838130d702d266cc0e72564c9cf686d329f
Author: Yunfeng Ye <yeyunfeng@huawei.com>
Date:   Mon Oct 21 19:31:21 2019 +0800

    arm64: psci: Reduce the waiting time for cpu_psci_cpu_kill()
    
    [ Upstream commit bfcef4ab1d7ee8921bc322109b1692036cc6cbe0 ]
    
    In cases like suspend-to-disk and suspend-to-ram, a large number of CPU
    cores need to be shut down. At present, the CPU hotplug operation is
    serialised, and the CPU cores can only be shut down one by one. In this
    process, if PSCI affinity_info() does not return LEVEL_OFF quickly,
    cpu_psci_cpu_kill() needs to wait for 10ms. If hundreds of CPU cores
    need to be shut down, it will take a long time.
    
    Normally, there is no need to wait 10ms in cpu_psci_cpu_kill(). So
    change the wait interval from 10 ms to max 1 ms and use usleep_range()
    instead of msleep() for more accurate timer.
    
    In addition, reducing the time interval will increase the messages
    output, so remove the "Retry ..." message, instead, track time and
    output to the the sucessful message.
    
    Signed-off-by: Yunfeng Ye <yeyunfeng@huawei.com>
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 3bb8863b05cf525688376c43ca8028b720801291
Author: Yunfeng Ye <yeyunfeng@huawei.com>
Date:   Mon Oct 21 19:31:21 2019 +0800

    arm64: psci: Reduce the waiting time for cpu_psci_cpu_kill()
    
    [ Upstream commit bfcef4ab1d7ee8921bc322109b1692036cc6cbe0 ]
    
    In cases like suspend-to-disk and suspend-to-ram, a large number of CPU
    cores need to be shut down. At present, the CPU hotplug operation is
    serialised, and the CPU cores can only be shut down one by one. In this
    process, if PSCI affinity_info() does not return LEVEL_OFF quickly,
    cpu_psci_cpu_kill() needs to wait for 10ms. If hundreds of CPU cores
    need to be shut down, it will take a long time.
    
    Normally, there is no need to wait 10ms in cpu_psci_cpu_kill(). So
    change the wait interval from 10 ms to max 1 ms and use usleep_range()
    instead of msleep() for more accurate timer.
    
    In addition, reducing the time interval will increase the messages
    output, so remove the "Retry ..." message, instead, track time and
    output to the the sucessful message.
    
    Signed-off-by: Yunfeng Ye <yeyunfeng@huawei.com>
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 893a491267c56b9f7f8917c57241aad5ae6e77f6
Author: Yunfeng Ye <yeyunfeng@huawei.com>
Date:   Mon Oct 21 19:31:21 2019 +0800

    arm64: psci: Reduce the waiting time for cpu_psci_cpu_kill()
    
    [ Upstream commit bfcef4ab1d7ee8921bc322109b1692036cc6cbe0 ]
    
    In cases like suspend-to-disk and suspend-to-ram, a large number of CPU
    cores need to be shut down. At present, the CPU hotplug operation is
    serialised, and the CPU cores can only be shut down one by one. In this
    process, if PSCI affinity_info() does not return LEVEL_OFF quickly,
    cpu_psci_cpu_kill() needs to wait for 10ms. If hundreds of CPU cores
    need to be shut down, it will take a long time.
    
    Normally, there is no need to wait 10ms in cpu_psci_cpu_kill(). So
    change the wait interval from 10 ms to max 1 ms and use usleep_range()
    instead of msleep() for more accurate timer.
    
    In addition, reducing the time interval will increase the messages
    output, so remove the "Retry ..." message, instead, track time and
    output to the the sucessful message.
    
    Signed-off-by: Yunfeng Ye <yeyunfeng@huawei.com>
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 38228e8848cd7dd86ccb90406af32de0cad24be3
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Tue Dec 3 14:31:11 2019 -0500

    padata: always acquire cpu_hotplug_lock before pinst->lock
    
    lockdep complains when padata's paths to update cpumasks via CPU hotplug
    and sysfs are both taken:
    
      # echo 0 > /sys/devices/system/cpu/cpu1/online
      # echo ff > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
    
      ======================================================
      WARNING: possible circular locking dependency detected
      5.4.0-rc8-padata-cpuhp-v3+ #1 Not tainted
      ------------------------------------------------------
      bash/205 is trying to acquire lock:
      ffffffff8286bcd0 (cpu_hotplug_lock.rw_sem){++++}, at: padata_set_cpumask+0x2b/0x120
    
      but task is already holding lock:
      ffff8880001abfa0 (&pinst->lock){+.+.}, at: padata_set_cpumask+0x26/0x120
    
      which lock already depends on the new lock.
    
    padata doesn't take cpu_hotplug_lock and pinst->lock in a consistent
    order.  Which should be first?  CPU hotplug calls into padata with
    cpu_hotplug_lock already held, so it should have priority.
    
    Fixes: 6751fb3c0e0c ("padata: Use get_online_cpus/put_online_cpus")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Eric Biggers <ebiggers@kernel.org>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 894c9ef9780c5cf2f143415e867ee39a33ecb75d
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Tue Dec 3 14:31:10 2019 -0500

    padata: validate cpumask without removed CPU during offline
    
    Configuring an instance's parallel mask without any online CPUs...
    
      echo 2 > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
      echo 0 > /sys/devices/system/cpu/cpu1/online
    
    ...makes tcrypt mode=215 crash like this:
    
      divide error: 0000 [#1] SMP PTI
      CPU: 4 PID: 283 Comm: modprobe Not tainted 5.4.0-rc8-padata-doc-v2+ #2
      Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS ?-20191013_105130-anatol 04/01/2014
      RIP: 0010:padata_do_parallel+0x114/0x300
      Call Trace:
       pcrypt_aead_encrypt+0xc0/0xd0 [pcrypt]
       crypto_aead_encrypt+0x1f/0x30
       do_mult_aead_op+0x4e/0xdf [tcrypt]
       test_mb_aead_speed.constprop.0.cold+0x226/0x564 [tcrypt]
       do_test+0x28c2/0x4d49 [tcrypt]
       tcrypt_mod_init+0x55/0x1000 [tcrypt]
       ...
    
    cpumask_weight() in padata_cpu_hash() returns 0 because the mask has no
    CPUs.  The problem is __padata_remove_cpu() checks for valid masks too
    early and so doesn't mark the instance PADATA_INVALID as expected, which
    would have made padata_do_parallel() return error before doing the
    division.
    
    Fix by introducing a second padata CPU hotplug state before
    CPUHP_BRINGUP_CPU so that __padata_remove_cpu() sees the online mask
    without @cpu.  No need for the second argument to padata_replace() since
    @cpu is now already missing from the online mask.
    
    Fixes: 33e54450683c ("padata: Handle empty padata cpumasks")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Eric Biggers <ebiggers@kernel.org>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 2a76352ad2cc6b78e58f737714879cc860903802
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed Nov 27 15:56:40 2019 +0000

    ARM: vexpress: Set-up shared OPP table instead of individual for each CPU
    
    Currently we add individual copy of same OPP table for each CPU within
    the cluster. This is redundant and doesn't reflect the reality.
    
    We can't use core cpumask to set policy->cpus in ve_spc_cpufreq_init()
    anymore as it gets called via cpuhp_cpufreq_online()->cpufreq_online()
    ->cpufreq_driver->init() and the cpumask gets updated upon CPU hotplug
    operations. It also may cause issues when the vexpress_spc_cpufreq
    driver is built as a module.
    
    Since ve_spc_clk_init is built-in device initcall, we should be able to
    use the same topology_core_cpumask to set the opp sharing cpumask via
    dev_pm_opp_set_sharing_cpus and use the same later in the driver via
    dev_pm_opp_get_sharing_cpus.
    
    Cc: Liviu Dudau <liviu.dudau@arm.com>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>

commit eea2d5da29e396b6cc1fb35e36bcbf5f57731015
Merge: 347f56fb3890 04bb96427d4e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 6 16:12:39 2019 -0800

    Merge tag 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm
    
    Pull ARM fixes from Russell King:
    
     - fix CPU topology setup for SCHED_MC case
    
     - fix VDSO regression
    
    * tag 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm:
      ARM: 8947/1: Fix __arch_get_hw_counter() access to CNTVCT
      ARM: 8943/1: Fix topology setup in case of CPU hotplug for CONFIG_SCHED_MC

commit ff98a5f624d2910de050f1fc7f2a32769da86b51
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Fri Nov 29 16:23:02 2019 +0100

    ARM: 8943/1: Fix topology setup in case of CPU hotplug for CONFIG_SCHED_MC
    
    Commit ca74b316df96 ("arm: Use common cpu_topology structure and
    functions.") changed cpu_coregroup_mask() from the ARM32 specific
    implementation in arch/arm/include/asm/topology.h to the one shared
    with ARM64 and RISCV in drivers/base/arch_topology.c.
    
    Currently on ARM32 (TC2 w/ CONFIG_SCHED_MC) the task scheduler setup
    code (w/ CONFIG_SCHED_DEBUG) shows this during CPU hotplug:
    
      ERROR: groups don't span domain->span
    
    It happens to CPUs of the cluster of the CPU which gets hot-plugged
    out on scheduler domain MC.
    
    Turns out that the shared cpu_coregroup_mask() requires that the
    hot-plugged CPU is removed from the core_sibling mask via
    remove_cpu_topology(). Otherwise the 'is core_sibling subset of
    cpumask_of_node()' doesn't work. In this case the task scheduler has to
    deal with cpumask_of_node instead of core_sibling which is wrong on
    scheduler domain MC.
    
    e.g. CPU3 hot-plugged out on TC2 [cluster0: 0,3-4 cluster1: 1-2]:
    
      cpu_coregroup_mask(): CPU3 cpumask_of_node=0-2,4 core_sibling=0,3-4
                                                                      ^
    should be:
    
      cpu_coregroup_mask(): CPU3 cpumask_of_node=0-2,4 core_sibling=0,4
    
    Add remove_cpu_topology() to __cpu_disable() to remove the CPU from the
    topology masks in case of a CPU hotplug out operation.
    
    At the same time tweak store_cpu_topology() slightly so it will call
    update_siblings_masks() in case of CPU hotplug in operation via
    secondary_start_kernel()->smp_store_cpu_info().
    
    This aligns the ARM32 implementation with the ARM64 one.
    
    Guarding remove_cpu_topology() with CONFIG_GENERIC_ARCH_TOPOLOGY is
    necessary since some Arm32 defconfigs (aspeed_g5_defconfig,
    milbeaut_m10v_defconfig, spear13xx_defconfig) specify an explicit
    
     # CONFIG_ARM_CPU_TOPOLOGY is not set
    
    w/ ./arch/arm/Kconfig: select GENERIC_ARCH_TOPOLOGY if ARM_CPU_TOPOLOGY
    
    Fixes: ca74b316df96 ("arm: Use common cpu_topology structure and functions")
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Reviewed-by: Lukasz Luba <lukasz.luba@arm.com>
    Tested-by: Lukasz Luba <lukasz.luba@arm.com>
    Tested-by: Ondrej Jirman <megous@megous.com>
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

commit 092e18f0c326b7f36c42958a6549c1f0329a9efa
Author: Nathan Fontenot <nfont@linux.vnet.ibm.com>
Date:   Mon Sep 17 14:14:02 2018 -0500

    powerpc/pseries: Disable CPU hotplug across migrations
    
    [ Upstream commit 85a88cabad57d26d826dd94ea34d3a785824d802 ]
    
    When performing partition migrations all present CPUs must be online
    as all present CPUs must make the H_JOIN call as part of the migration
    process. Once all present CPUs make the H_JOIN call, one CPU is returned
    to make the rtas call to perform the migration to the destination system.
    
    During testing of migration and changing the SMT state we have found
    instances where CPUs are offlined, as part of the SMT state change,
    before they make the H_JOIN call. This results in a hung system where
    every CPU is either in H_JOIN or offline.
    
    To prevent this this patch disables CPU hotplug during the migration
    process.
    
    Signed-off-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Reviewed-by: Tyrel Datwyler <tyreld@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 3e6636c924297a2a4e1e8aa2f22c7481a01ce6f3
Author: Nathan Fontenot <nfont@linux.vnet.ibm.com>
Date:   Mon Sep 17 14:14:02 2018 -0500

    powerpc/pseries: Disable CPU hotplug across migrations
    
    [ Upstream commit 85a88cabad57d26d826dd94ea34d3a785824d802 ]
    
    When performing partition migrations all present CPUs must be online
    as all present CPUs must make the H_JOIN call as part of the migration
    process. Once all present CPUs make the H_JOIN call, one CPU is returned
    to make the rtas call to perform the migration to the destination system.
    
    During testing of migration and changing the SMT state we have found
    instances where CPUs are offlined, as part of the SMT state change,
    before they make the H_JOIN call. This results in a hung system where
    every CPU is either in H_JOIN or offline.
    
    To prevent this this patch disables CPU hotplug during the migration
    process.
    
    Signed-off-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Reviewed-by: Tyrel Datwyler <tyreld@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit e7b37640916fdfede5569f467a109aad97ff909b
Author: Nathan Fontenot <nfont@linux.vnet.ibm.com>
Date:   Mon Sep 17 14:14:02 2018 -0500

    powerpc/pseries: Disable CPU hotplug across migrations
    
    [ Upstream commit 85a88cabad57d26d826dd94ea34d3a785824d802 ]
    
    When performing partition migrations all present CPUs must be online
    as all present CPUs must make the H_JOIN call as part of the migration
    process. Once all present CPUs make the H_JOIN call, one CPU is returned
    to make the rtas call to perform the migration to the destination system.
    
    During testing of migration and changing the SMT state we have found
    instances where CPUs are offlined, as part of the SMT state change,
    before they make the H_JOIN call. This results in a hung system where
    every CPU is either in H_JOIN or offline.
    
    To prevent this this patch disables CPU hotplug during the migration
    process.
    
    Signed-off-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Reviewed-by: Tyrel Datwyler <tyreld@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2c4c8ad782e4215b6f374535b4cac085e6228aff
Author: Nathan Fontenot <nfont@linux.vnet.ibm.com>
Date:   Mon Sep 17 14:14:02 2018 -0500

    powerpc/pseries: Disable CPU hotplug across migrations
    
    [ Upstream commit 85a88cabad57d26d826dd94ea34d3a785824d802 ]
    
    When performing partition migrations all present CPUs must be online
    as all present CPUs must make the H_JOIN call as part of the migration
    process. Once all present CPUs make the H_JOIN call, one CPU is returned
    to make the rtas call to perform the migration to the destination system.
    
    During testing of migration and changing the SMT state we have found
    instances where CPUs are offlined, as part of the SMT state change,
    before they make the H_JOIN call. This results in a hung system where
    every CPU is either in H_JOIN or offline.
    
    To prevent this this patch disables CPU hotplug during the migration
    process.
    
    Signed-off-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Reviewed-by: Tyrel Datwyler <tyreld@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 4df4cb9e99f83b70d54bc0e25081ac23cceafcbc
Author: Michael Kelley <mikelley@microsoft.com>
Date:   Wed Nov 13 01:11:49 2019 +0000

    x86/hyperv: Initialize clockevents earlier in CPU onlining
    
    Hyper-V has historically initialized stimer-based clockevents late in the
    process of onlining a CPU because clockevents depend on stimer
    interrupts. In the original Hyper-V design, stimer interrupts generate a
    VMbus message, so the VMbus machinery must be running first, and VMbus
    can't be initialized until relatively late. On x86/64, LAPIC timer based
    clockevents are used during early initialization before VMbus and
    stimer-based clockevents are ready, and again during CPU offlining after
    the stimer clockevents have been shut down.
    
    Unfortunately, this design creates problems when offlining CPUs for
    hibernation or other purposes. stimer-based clockevents are shut down
    relatively early in the offlining process, so clockevents_unbind_device()
    must be used to fallback to the LAPIC-based clockevents for the remainder
    of the offlining process.  Furthermore, the late initialization and early
    shutdown of stimer-based clockevents doesn't work well on ARM64 since there
    is no other timer like the LAPIC to fallback to. So CPU onlining and
    offlining doesn't work properly.
    
    Fix this by recognizing that stimer Direct Mode is the normal path for
    newer versions of Hyper-V on x86/64, and the only path on other
    architectures. With stimer Direct Mode, stimer interrupts don't require any
    VMbus machinery. stimer clockevents can be initialized and shut down
    consistent with how it is done for other clockevent devices. While the old
    VMbus-based stimer interrupts must still be supported for backward
    compatibility on x86, that mode of operation can be treated as legacy.
    
    So add a new Hyper-V stimer entry in the CPU hotplug state list, and use
    that new state when in Direct Mode. Update the Hyper-V clocksource driver
    to allocate and initialize stimer clockevents earlier during boot. Update
    Hyper-V initialization and the VMbus driver to use this new design. As a
    result, the LAPIC timer is no longer used during boot or CPU
    onlining/offlining and clockevents_unbind_device() is not called.  But
    retain the old design as a legacy implementation for older versions of
    Hyper-V that don't support Direct Mode.
    
    Signed-off-by: Michael Kelley <mikelley@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Dexuan Cui <decui@microsoft.com>
    Reviewed-by: Dexuan Cui <decui@microsoft.com>
    Link: https://lkml.kernel.org/r/1573607467-9456-1-git-send-email-mikelley@microsoft.com

commit 52117da87994ff1c428c68d9a493c16168969065
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Fri Nov 8 13:35:53 2019 +0100

    ARM: ensure that processor vtables is not lost after boot
    
    Commit 3a4d0c2172bcf15b7a3d9d498b2b355f9864286b upstream.
    
    Marek Szyprowski reported problems with CPU hotplug in current kernels.
    This was tracked down to the processor vtables being located in an
    init section, and therefore discarded after kernel boot, despite being
    required after boot to properly initialise the non-boot CPUs.
    
    Arrange for these tables to end up in .rodata when required.
    
    Reported-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Krzysztof Kozlowski <krzk@kernel.org>
    Fixes: 383fb3ee8024 ("ARM: spectre-v2: per-CPU vtables to work around big.Little systems")
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: David A. Long <dave.long@linaro.org>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>
    Signed-off-by: Ard Biesheuvel <ardb@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 80c784282859cc39617b808440a34d0be9502b87
Author: Nathan Lynch <nathanl@linux.ibm.com>
Date:   Wed Oct 16 13:36:11 2019 -0500

    powerpc/pseries: safely roll back failed DLPAR cpu add
    
    dlpar_online_cpu() attempts to online all threads of a core that has
    been added to an LPAR. If onlining a non-primary thread
    fails (e.g. due to an allocation failure), the core is left with at
    least one thread online. dlpar_cpu_add() attempts to roll back the
    whole operation, releasing the core back to the platform. However,
    since some threads of the core being removed are still online, the
    BUG_ON(cpu_online(cpu)) in pseries_remove_processor() strikes:
    
    LE PAGE_SIZE=64K MMU=Hash SMP NR_CPUS=2048 NUMA pSeries
    Modules linked in:
    CPU: 3 PID: 8587 Comm: drmgr Not tainted 5.3.0-rc2-00190-g9b123d1ea237-dirty #46
    NIP:  c0000000000eeb2c LR: c0000000000eeac4 CTR: c0000000000ee9e0
    REGS: c0000001f745b6c0 TRAP: 0700   Not tainted  (5.3.0-rc2-00190-g9b123d1ea237-dirty)
    MSR:  800000010282b033 <SF,VEC,VSX,EE,FP,ME,IR,DR,RI,LE,TM[E]>  CR: 44002448  XER: 00000000
    CFAR: c00000000195d718 IRQMASK: 0
    GPR00: c0000000000eeac4 c0000001f745b950 c0000000032f6200 0000000000000008
    GPR04: 0000000000000008 c000000003349c78 0000000000000040 00000000000001ff
    GPR08: 0000000000000008 0000000000000000 0000000000000001 0007ffffffffffff
    GPR12: 0000000084002844 c00000001ecacb80 0000000000000000 0000000000000000
    GPR16: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
    GPR20: 0000000000000000 0000000000000000 0000000000000000 0000000000000008
    GPR24: c000000003349ee0 c00000000334a2e4 c0000000fca4d7a8 c000000001d20048
    GPR28: 0000000000000001 ffffffffffffffff ffffffffffffffff c0000000fca4d7c4
    NIP [c0000000000eeb2c] pseries_smp_notifier+0x14c/0x2e0
    LR [c0000000000eeac4] pseries_smp_notifier+0xe4/0x2e0
    Call Trace:
    [c0000001f745b950] [c0000000000eeac4] pseries_smp_notifier+0xe4/0x2e0 (unreliable)
    [c0000001f745ba10] [c0000000001ac774] notifier_call_chain+0xb4/0x190
    [c0000001f745bab0] [c0000000001ad62c] blocking_notifier_call_chain+0x7c/0xb0
    [c0000001f745baf0] [c00000000167bda0] of_detach_node+0xc0/0x110
    [c0000001f745bb50] [c0000000000e7ae4] dlpar_detach_node+0x64/0xa0
    [c0000001f745bb80] [c0000000000edefc] dlpar_cpu_add+0x31c/0x360
    [c0000001f745bc10] [c0000000000ee980] dlpar_cpu_probe+0x50/0xb0
    [c0000001f745bc50] [c00000000002cf70] arch_cpu_probe+0x40/0x70
    [c0000001f745bc70] [c000000000ccd808] cpu_probe_store+0x48/0x80
    [c0000001f745bcb0] [c000000000cbcef8] dev_attr_store+0x38/0x60
    [c0000001f745bcd0] [c00000000059c980] sysfs_kf_write+0x70/0xb0
    [c0000001f745bd10] [c00000000059afb8] kernfs_fop_write+0xf8/0x280
    [c0000001f745bd60] [c0000000004b437c] __vfs_write+0x3c/0x70
    [c0000001f745bd80] [c0000000004b8710] vfs_write+0xd0/0x220
    [c0000001f745bdd0] [c0000000004b8acc] ksys_write+0x7c/0x140
    [c0000001f745be20] [c00000000000bbd8] system_call+0x5c/0x68
    
    Move dlpar_offline_cpu() up in the file so that dlpar_online_cpu() can
    use it to re-offline any threads that have been onlined when an error
    is encountered.
    
    Signed-off-by: Nathan Lynch <nathanl@linux.ibm.com>
    Fixes: e666ae0b10aa ("powerpc/pseries: Update CPU hotplug error recovery")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20191016183611.10867-3-nathanl@linux.ibm.com

commit 7c202575ef63f1c67832acea2179f07f8a2cf7bc
Merge: e44ff9ea8f4c 7d6475051fb3
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Nov 4 21:01:59 2019 +1100

    Merge branch 'fixes' into next
    
    Merge our fixes branch, primarily to bring in the powernv CPU hotplug
    warning fix.

commit 53fafdbb8b21fa99dfd8376ca056bffde8cafc11
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Mon Oct 28 12:36:22 2019 -0200

    KVM: x86: switch KVMCLOCK base to monotonic raw clock
    
    Commit 0bc48bea36d1 ("KVM: x86: update master clock before computing
    kvmclock_offset")
    switches the order of operations to avoid the conversion
    
    TSC (without frequency correction) ->
    system_timestamp (with frequency correction),
    
    which might cause a time jump.
    
    However, it leaves any other masterclock update unsafe, which includes,
    at the moment:
    
            * HV_X64_MSR_REFERENCE_TSC MSR write.
            * TSC writes.
            * Host suspend/resume.
    
    Avoid the time jump issue by using frequency uncorrected
    CLOCK_MONOTONIC_RAW clock.
    
    Its the guests time keeping software responsability
    to track and correct a reference clock such as UTC.
    
    This fixes forward time jump (which can result in
    failure to bring up a vCPU) during vCPU hotplug:
    
    Oct 11 14:48:33 storage kernel: CPU2 has been hot-added
    Oct 11 14:48:34 storage kernel: CPU3 has been hot-added
    Oct 11 14:49:22 storage kernel: smpboot: Booting Node 0 Processor 2 APIC 0x2          <-- time jump of almost 1 minute
    Oct 11 14:49:22 storage kernel: smpboot: do_boot_cpu failed(-1) to wakeup CPU#2
    Oct 11 14:49:23 storage kernel: smpboot: Booting Node 0 Processor 3 APIC 0x3
    Oct 11 14:49:23 storage kernel: kvm-clock: cpu 3, msr 0:7ff640c1, secondary cpu clock
    
    Which happens because:
    
                    /*
                     * Wait 10s total for a response from AP
                     */
                    boot_error = -1;
                    timeout = jiffies + 10*HZ;
                    while (time_before(jiffies, timeout)) {
                             ...
                    }
    
    Analyzed-by: Igor Mammedov <imammedo@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit bfcef4ab1d7ee8921bc322109b1692036cc6cbe0
Author: Yunfeng Ye <yeyunfeng@huawei.com>
Date:   Mon Oct 21 19:31:21 2019 +0800

    arm64: psci: Reduce the waiting time for cpu_psci_cpu_kill()
    
    In cases like suspend-to-disk and suspend-to-ram, a large number of CPU
    cores need to be shut down. At present, the CPU hotplug operation is
    serialised, and the CPU cores can only be shut down one by one. In this
    process, if PSCI affinity_info() does not return LEVEL_OFF quickly,
    cpu_psci_cpu_kill() needs to wait for 10ms. If hundreds of CPU cores
    need to be shut down, it will take a long time.
    
    Normally, there is no need to wait 10ms in cpu_psci_cpu_kill(). So
    change the wait interval from 10 ms to max 1 ms and use usleep_range()
    instead of msleep() for more accurate timer.
    
    In addition, reducing the time interval will increase the messages
    output, so remove the "Retry ..." message, instead, track time and
    output to the the sucessful message.
    
    Signed-off-by: Yunfeng Ye <yeyunfeng@huawei.com>
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

commit 4fe34d61a3a9d9ed954cbc90713506a7598d6b36
Merge: 81c4bc31c4cd 228d120051a2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 20 06:31:14 2019 -0400

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Thomas Gleixner:
     "A small set of x86 fixes:
    
       - Prevent a NULL pointer dereference in the X2APIC code in case of a
         CPU hotplug failure.
    
       - Prevent boot failures on HP superdome machines by invalidating the
         level2 kernel pagetable entries outside of the kernel area as
         invalid so BIOS reserved space won't be touched unintentionally.
    
         Also ensure that memory holes are rounded up to the next PMD
         boundary correctly.
    
       - Enable X2APIC support on Hyper-V to prevent boot failures.
    
       - Set the paravirt name when running on Hyper-V for consistency
    
       - Move a function under the appropriate ifdef guard to prevent build
         warnings"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/boot/acpi: Move get_cmdline_acpi_rsdp() under #ifdef guard
      x86/hyperv: Set pv_info.name to "Hyper-V"
      x86/apic/x2apic: Fix a NULL pointer deref when handling a dying cpu
      x86/hyperv: Make vapic support x2apic mode
      x86/boot/64: Round memory hole size up to next PMD page
      x86/boot/64: Make level2_kernel_pgt pages invalid outside kernel area

commit 8b53c76533aa4356602aea98f98a2f3b4051464c
Merge: 6cfae0c26b21 9575d1a5c078
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 18 12:11:14 2019 -0700

    Merge branch 'linus' of git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6
    
    Pull crypto updates from Herbert Xu:
     "API:
       - Add the ability to abort a skcipher walk.
    
      Algorithms:
       - Fix XTS to actually do the stealing.
       - Add library helpers for AES and DES for single-block users.
       - Add library helpers for SHA256.
       - Add new DES key verification helper.
       - Add surrounding bits for ESSIV generator.
       - Add accelerations for aegis128.
       - Add test vectors for lzo-rle.
    
      Drivers:
       - Add i.MX8MQ support to caam.
       - Add gcm/ccm/cfb/ofb aes support in inside-secure.
       - Add ofb/cfb aes support in media-tek.
       - Add HiSilicon ZIP accelerator support.
    
      Others:
       - Fix potential race condition in padata.
       - Use unbound workqueues in padata"
    
    * 'linus' of git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6: (311 commits)
      crypto: caam - Cast to long first before pointer conversion
      crypto: ccree - enable CTS support in AES-XTS
      crypto: inside-secure - Probe transform record cache RAM sizes
      crypto: inside-secure - Base RD fetchcount on actual RD FIFO size
      crypto: inside-secure - Base CD fetchcount on actual CD FIFO size
      crypto: inside-secure - Enable extended algorithms on newer HW
      crypto: inside-secure: Corrected configuration of EIP96_TOKEN_CTRL
      crypto: inside-secure - Add EIP97/EIP197 and endianness detection
      padata: remove cpu_index from the parallel_queue
      padata: unbind parallel jobs from specific CPUs
      padata: use separate workqueues for parallel and serial work
      padata, pcrypt: take CPU hotplug lock internally in padata_alloc_possible
      crypto: pcrypt - remove padata cpumask notifier
      padata: make padata_do_parallel find alternate callback CPU
      workqueue: require CPU hotplug read exclusion for apply_workqueue_attrs
      workqueue: unconfine alloc/apply/free_workqueue_attrs()
      padata: allocate workqueue internally
      arm64: dts: imx8mq: Add CAAM node
      random: Use wait_event_freezable() in add_hwgenerator_randomness()
      crypto: ux500 - Fix COMPILE_TEST warnings
      ...

commit 3cd0462230d806077c709e44af8733795eaa712c
Merge: 16208cd6c36a 0c09ab96fc82
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 17 10:32:05 2019 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug updates from Thomas Gleixner:
     "A small update for the SMP hotplug code code:
    
       - Track "booted once" CPUs in a cpumask so the x86 APIC code has an
         easy way to decide whether broadcast IPIs are safe to use or not.
    
       - Implement a cpumask_or_equal() helper for the IPI broadcast
         evaluation.
    
         The above two changes have been also pulled into the x86/apic
         branch for implementing the conditional IPI broadcast feature.
    
       - Cache the number of online CPUs instead of reevaluating it over and
         over. num_online_cpus() is an unreliable snapshot anyway except
         when it is used outside a cpu hotplug locked region. The cached
         access is not changing this, but it's definitely faster than
         calculating the bitmap wheight especially in hot paths"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Cache number of online CPUs
      cpumask: Implement cpumask_or_equal()
      smp/hotplug: Track booted once CPUs in a cpumask

commit 7e67a859997aad47727aff9c5a32e160da079ce3
Merge: 772c1d06bd40 563c4f85f9f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 16 17:25:49 2019 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - MAINTAINERS: Add Mark Rutland as perf submaintainer, Juri Lelli and
       Vincent Guittot as scheduler submaintainers. Add Dietmar Eggemann,
       Steven Rostedt, Ben Segall and Mel Gorman as scheduler reviewers.
    
       As perf and the scheduler is getting bigger and more complex,
       document the status quo of current responsibilities and interests,
       and spread the review pain^H^H^H^H fun via an increase in the Cc:
       linecount generated by scripts/get_maintainer.pl. :-)
    
     - Add another series of patches that brings the -rt (PREEMPT_RT) tree
       closer to mainline: split the monolithic CONFIG_PREEMPT dependencies
       into a new CONFIG_PREEMPTION category that will allow the eventual
       introduction of CONFIG_PREEMPT_RT. Still a few more hundred patches
       to go though.
    
     - Extend the CPU cgroup controller with uclamp.min and uclamp.max to
       allow the finer shaping of CPU bandwidth usage.
    
     - Micro-optimize energy-aware wake-ups from O(CPUS^2) to O(CPUS).
    
     - Improve the behavior of high CPU count, high thread count
       applications running under cpu.cfs_quota_us constraints.
    
     - Improve balancing with SCHED_IDLE (SCHED_BATCH) tasks present.
    
     - Improve CPU isolation housekeeping CPU allocation NUMA locality.
    
     - Fix deadline scheduler bandwidth calculations and logic when cpusets
       rebuilds the topology, or when it gets deadline-throttled while it's
       being offlined.
    
     - Convert the cpuset_mutex to percpu_rwsem, to allow it to be used from
       setscheduler() system calls without creating global serialization.
       Add new synchronization between cpuset topology-changing events and
       the deadline acceptance tests in setscheduler(), which were broken
       before.
    
     - Rework the active_mm state machine to be less confusing and more
       optimal.
    
     - Rework (simplify) the pick_next_task() slowpath.
    
     - Improve load-balancing on AMD EPYC systems.
    
     - ... and misc cleanups, smaller fixes and improvements - please see
       the Git log for more details.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (53 commits)
      sched/psi: Correct overly pessimistic size calculation
      sched/fair: Speed-up energy-aware wake-ups
      sched/uclamp: Always use 'enum uclamp_id' for clamp_id values
      sched/uclamp: Update CPU's refcount on TG's clamp changes
      sched/uclamp: Use TG's clamps to restrict TASK's clamps
      sched/uclamp: Propagate system defaults to the root group
      sched/uclamp: Propagate parent clamps
      sched/uclamp: Extend CPU's cgroup controller
      sched/topology: Improve load balancing on AMD EPYC systems
      arch, ia64: Make NUMA select SMP
      sched, perf: MAINTAINERS update, add submaintainers and reviewers
      sched/fair: Use rq_lock/unlock in online_fair_sched_group
      cpufreq: schedutil: fix equation in comment
      sched: Rework pick_next_task() slow-path
      sched: Allow put_prev_task() to drop rq->lock
      sched/fair: Expose newidle_balance()
      sched: Add task_struct pointer to sched_class::set_curr_task
      sched: Rework CPU hotplug task selection
      sched/{rt,deadline}: Fix set_next_task vs pick_next_task
      sched: Fix kerneldoc comment for ia64_set_curr_task
      ...

commit cc491d8e6486c56e07e60d9992cd56f63dc9fd6c
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Thu Sep 5 21:40:26 2019 -0400

    padata, pcrypt: take CPU hotplug lock internally in padata_alloc_possible
    
    With pcrypt's cpumask no longer used, take the CPU hotplug lock inside
    padata_alloc_possible.
    
    Useful later in the series for avoiding nested acquisition of the CPU
    hotplug lock in padata when padata_alloc_possible is allocating an
    unbound workqueue.
    
    Without this patch, this nested acquisition would happen later in the
    series:
    
          pcrypt_init_padata
            get_online_cpus
            alloc_padata_possible
              alloc_padata
                alloc_workqueue(WQ_UNBOUND)   // later in the series
                  alloc_and_link_pwqs
                    apply_wqattrs_lock
                      get_online_cpus         // recursive rwsem acquisition
    
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Acked-by: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit e6ce0e0807e90d38a2cefa524ac253d7a85c3f2f
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Thu Sep 5 21:40:24 2019 -0400

    padata: make padata_do_parallel find alternate callback CPU
    
    padata_do_parallel currently returns -EINVAL if the callback CPU isn't
    in the callback cpumask.
    
    pcrypt tries to prevent this situation by keeping its own callback
    cpumask in sync with padata's and checks that the callback CPU it passes
    to padata is valid.  Make padata handle this instead.
    
    padata_do_parallel now takes a pointer to the callback CPU and updates
    it for the caller if an alternate CPU is used.  Overall behavior in
    terms of which callback CPUs are chosen stays the same.
    
    Prepares for removal of the padata cpumask notifier in pcrypt, which
    will fix a lockdep complaint about nested acquisition of the CPU hotplug
    lock later in the series.
    
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Acked-by: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 509b3204890ab31c3e652c26424a0706bb809933
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Thu Sep 5 21:40:23 2019 -0400

    workqueue: require CPU hotplug read exclusion for apply_workqueue_attrs
    
    Change the calling convention for apply_workqueue_attrs to require CPU
    hotplug read exclusion.
    
    Avoids lockdep complaints about nested calls to get_online_cpus in a
    future patch where padata calls apply_workqueue_attrs when changing
    other CPU-hotplug-sensitive data structures with the CPU read lock
    already held.
    
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 1a7fd193e9d85d2a6b11f16e19bbaf28f75ff11b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 7 14:25:54 2019 -0700

    Revert "x86/apic: Include the LDR when clearing out APIC registers"
    
    [ Upstream commit 950b07c14e8c59444e2359f15fd70ed5112e11a0 ]
    
    This reverts commit 558682b5291937a70748d36fd9ba757fb25b99ae.
    
    Chris Wilson reports that it breaks his CPU hotplug test scripts.  In
    particular, it breaks offlining and then re-onlining the boot CPU, which
    we treat specially (and the BIOS does too).
    
    The symptoms are that we can offline the CPU, but it then does not come
    back online again:
    
        smpboot: CPU 0 is now offline
        smpboot: Booting Node 0 Processor 0 APIC 0x0
        smpboot: do_boot_cpu failed(-1) to wakeup CPU#0
    
    Thomas says he knows why it's broken (my personal suspicion: our magic
    handling of the "cpu0_logical_apicid" thing), but for 5.3 the right fix
    is to just revert it, since we've never touched the LDR bits before, and
    it's not worth the risk to do anything else at this stage.
    
    [ Hotpluging of the boot CPU is special anyway, and should be off by
      default. See the "BOOTPARAM_HOTPLUG_CPU0" config option and the
      cpu0_hotplug kernel parameter.
    
      In general you should not do it, and it has various known limitations
      (hibernate and suspend require the boot CPU, for example).
    
      But it should work, even if the boot CPU is special and needs careful
      treatment       - Linus ]
    
    Link: https://lore.kernel.org/lkml/156785100521.13300.14461504732265570003@skylake-alporthouse-com/
    Reported-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bandan Das <bsd@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 991467a47cf250abfc624acdc1929a5936cfefa9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 7 14:25:54 2019 -0700

    Revert "x86/apic: Include the LDR when clearing out APIC registers"
    
    [ Upstream commit 950b07c14e8c59444e2359f15fd70ed5112e11a0 ]
    
    This reverts commit 558682b5291937a70748d36fd9ba757fb25b99ae.
    
    Chris Wilson reports that it breaks his CPU hotplug test scripts.  In
    particular, it breaks offlining and then re-onlining the boot CPU, which
    we treat specially (and the BIOS does too).
    
    The symptoms are that we can offline the CPU, but it then does not come
    back online again:
    
        smpboot: CPU 0 is now offline
        smpboot: Booting Node 0 Processor 0 APIC 0x0
        smpboot: do_boot_cpu failed(-1) to wakeup CPU#0
    
    Thomas says he knows why it's broken (my personal suspicion: our magic
    handling of the "cpu0_logical_apicid" thing), but for 5.3 the right fix
    is to just revert it, since we've never touched the LDR bits before, and
    it's not worth the risk to do anything else at this stage.
    
    [ Hotpluging of the boot CPU is special anyway, and should be off by
      default. See the "BOOTPARAM_HOTPLUG_CPU0" config option and the
      cpu0_hotplug kernel parameter.
    
      In general you should not do it, and it has various known limitations
      (hibernate and suspend require the boot CPU, for example).
    
      But it should work, even if the boot CPU is special and needs careful
      treatment       - Linus ]
    
    Link: https://lore.kernel.org/lkml/156785100521.13300.14461504732265570003@skylake-alporthouse-com/
    Reported-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bandan Das <bsd@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 8e3956fc974c616851b2e14c02ec029074e192a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 7 14:25:54 2019 -0700

    Revert "x86/apic: Include the LDR when clearing out APIC registers"
    
    [ Upstream commit 950b07c14e8c59444e2359f15fd70ed5112e11a0 ]
    
    This reverts commit 558682b5291937a70748d36fd9ba757fb25b99ae.
    
    Chris Wilson reports that it breaks his CPU hotplug test scripts.  In
    particular, it breaks offlining and then re-onlining the boot CPU, which
    we treat specially (and the BIOS does too).
    
    The symptoms are that we can offline the CPU, but it then does not come
    back online again:
    
        smpboot: CPU 0 is now offline
        smpboot: Booting Node 0 Processor 0 APIC 0x0
        smpboot: do_boot_cpu failed(-1) to wakeup CPU#0
    
    Thomas says he knows why it's broken (my personal suspicion: our magic
    handling of the "cpu0_logical_apicid" thing), but for 5.3 the right fix
    is to just revert it, since we've never touched the LDR bits before, and
    it's not worth the risk to do anything else at this stage.
    
    [ Hotpluging of the boot CPU is special anyway, and should be off by
      default. See the "BOOTPARAM_HOTPLUG_CPU0" config option and the
      cpu0_hotplug kernel parameter.
    
      In general you should not do it, and it has various known limitations
      (hibernate and suspend require the boot CPU, for example).
    
      But it should work, even if the boot CPU is special and needs careful
      treatment       - Linus ]
    
    Link: https://lore.kernel.org/lkml/156785100521.13300.14461504732265570003@skylake-alporthouse-com/
    Reported-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bandan Das <bsd@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 5ad424f5d529657039cbbb89018ac8706af173a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 7 14:25:54 2019 -0700

    Revert "x86/apic: Include the LDR when clearing out APIC registers"
    
    [ Upstream commit 950b07c14e8c59444e2359f15fd70ed5112e11a0 ]
    
    This reverts commit 558682b5291937a70748d36fd9ba757fb25b99ae.
    
    Chris Wilson reports that it breaks his CPU hotplug test scripts.  In
    particular, it breaks offlining and then re-onlining the boot CPU, which
    we treat specially (and the BIOS does too).
    
    The symptoms are that we can offline the CPU, but it then does not come
    back online again:
    
        smpboot: CPU 0 is now offline
        smpboot: Booting Node 0 Processor 0 APIC 0x0
        smpboot: do_boot_cpu failed(-1) to wakeup CPU#0
    
    Thomas says he knows why it's broken (my personal suspicion: our magic
    handling of the "cpu0_logical_apicid" thing), but for 5.3 the right fix
    is to just revert it, since we've never touched the LDR bits before, and
    it's not worth the risk to do anything else at this stage.
    
    [ Hotpluging of the boot CPU is special anyway, and should be off by
      default. See the "BOOTPARAM_HOTPLUG_CPU0" config option and the
      cpu0_hotplug kernel parameter.
    
      In general you should not do it, and it has various known limitations
      (hibernate and suspend require the boot CPU, for example).
    
      But it should work, even if the boot CPU is special and needs careful
      treatment       - Linus ]
    
    Link: https://lore.kernel.org/lkml/156785100521.13300.14461504732265570003@skylake-alporthouse-com/
    Reported-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bandan Das <bsd@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 94eb5357f6d688b364eaf52d4f7fa187102396a7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 7 14:25:54 2019 -0700

    Revert "x86/apic: Include the LDR when clearing out APIC registers"
    
    [ Upstream commit 950b07c14e8c59444e2359f15fd70ed5112e11a0 ]
    
    This reverts commit 558682b5291937a70748d36fd9ba757fb25b99ae.
    
    Chris Wilson reports that it breaks his CPU hotplug test scripts.  In
    particular, it breaks offlining and then re-onlining the boot CPU, which
    we treat specially (and the BIOS does too).
    
    The symptoms are that we can offline the CPU, but it then does not come
    back online again:
    
        smpboot: CPU 0 is now offline
        smpboot: Booting Node 0 Processor 0 APIC 0x0
        smpboot: do_boot_cpu failed(-1) to wakeup CPU#0
    
    Thomas says he knows why it's broken (my personal suspicion: our magic
    handling of the "cpu0_logical_apicid" thing), but for 5.3 the right fix
    is to just revert it, since we've never touched the LDR bits before, and
    it's not worth the risk to do anything else at this stage.
    
    [ Hotpluging of the boot CPU is special anyway, and should be off by
      default. See the "BOOTPARAM_HOTPLUG_CPU0" config option and the
      cpu0_hotplug kernel parameter.
    
      In general you should not do it, and it has various known limitations
      (hibernate and suspend require the boot CPU, for example).
    
      But it should work, even if the boot CPU is special and needs careful
      treatment       - Linus ]
    
    Link: https://lore.kernel.org/lkml/156785100521.13300.14461504732265570003@skylake-alporthouse-com/
    Reported-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bandan Das <bsd@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 950b07c14e8c59444e2359f15fd70ed5112e11a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 7 14:25:54 2019 -0700

    Revert "x86/apic: Include the LDR when clearing out APIC registers"
    
    This reverts commit 558682b5291937a70748d36fd9ba757fb25b99ae.
    
    Chris Wilson reports that it breaks his CPU hotplug test scripts.  In
    particular, it breaks offlining and then re-onlining the boot CPU, which
    we treat specially (and the BIOS does too).
    
    The symptoms are that we can offline the CPU, but it then does not come
    back online again:
    
        smpboot: CPU 0 is now offline
        smpboot: Booting Node 0 Processor 0 APIC 0x0
        smpboot: do_boot_cpu failed(-1) to wakeup CPU#0
    
    Thomas says he knows why it's broken (my personal suspicion: our magic
    handling of the "cpu0_logical_apicid" thing), but for 5.3 the right fix
    is to just revert it, since we've never touched the LDR bits before, and
    it's not worth the risk to do anything else at this stage.
    
    [ Hotpluging of the boot CPU is special anyway, and should be off by
      default. See the "BOOTPARAM_HOTPLUG_CPU0" config option and the
      cpu0_hotplug kernel parameter.
    
      In general you should not do it, and it has various known limitations
      (hibernate and suspend require the boot CPU, for example).
    
      But it should work, even if the boot CPU is special and needs careful
      treatment       - Linus ]
    
    Link: https://lore.kernel.org/lkml/156785100521.13300.14461504732265570003@skylake-alporthouse-com/
    Reported-by: Chris Wilson <chris@chris-wilson.co.uk>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Bandan Das <bsd@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit d55c5f28afafb6b1f0a6978916b23338b383faab
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed Jun 12 13:51:37 2019 +0100

    arm64: smp: disable hotplug on trusted OS resident CPU
    
    The trusted OS may reject CPU_OFF calls to its resident CPU, so we must
    avoid issuing those. We never migrate a Trusted OS and we already take
    care to prevent CPU_OFF PSCI call. However, this is not reflected
    explicitly to the userspace. Any user can attempt to hotplug trusted OS
    resident CPU. The entire motion of going through the various state
    transitions in the CPU hotplug state machine gets executed and the
    PSCI layer finally refuses to make CPU_OFF call.
    
    This results is unnecessary unwinding of CPU hotplug state machine in
    the kernel. Instead we can mark the trusted OS resident CPU as not
    available for hotplug, so that the user attempt or request to do the
    same will get immediately rejected.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

commit e7409258845a0f64967f8377e99294d438137537
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Fri Aug 9 18:40:37 2019 -0700

    x86/umwait: Fix error handling in umwait_init()
    
    Currently, failure of cpuhp_setup_state() is ignored and the syscore ops
    and the control interfaces can still be added even after the failure. But,
    this error handling will cause a few issues:
    
    1. The CPUs may have different values in the IA32_UMWAIT_CONTROL
       MSR because there is no way to roll back the control MSR on
       the CPUs which already set the MSR before the failure.
    
    2. If the sysfs interface is added successfully, there will be a mismatch
       between the global control value and the control MSR:
       - The interface shows the default global control value. But,
         the control MSR is not set to the value because the CPU online
         function, which is supposed to set the MSR to the value,
         is not installed.
       - If the sysadmin changes the global control value through
         the interface, the control MSR on all current online CPUs is
         set to the new value. But, the control MSR on newly onlined CPUs
         after the value change will not be set to the new value due to
         lack of the CPU online function.
    
    3. On resume from suspend/hibernation, the boot CPU restores the control
       MSR to the global control value through the syscore ops. But, the
       control MSR on all APs is not set due to lake of the CPU online
       function.
    
    To solve the issues and enforce consistent behavior on the failure
    of the CPU hotplug setup, make the following changes:
    
    1. Cache the original control MSR value which is configured by
       hardware or BIOS before kernel boot. This value is likely to
       be 0. But it could be a different number as well. Cache the
       control MSR only once before the MSR is changed.
    2. Add the CPU offline function so that the MSR is restored to the
       original control value on all CPUs on the failure.
    3. On the failure, exit from cpumait_init() so that the syscore ops
       and the control interfaces are not added.
    
    Reported-by: Valdis Kletnieks <valdis.kletnieks@vt.edu>
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/1565401237-60936-1-git-send-email-fenghua.yu@intel.com

commit ec9c7d19336ee98ecba8de80128aa405c45feebb
Author: Daniel Jordan <daniel.m.jordan@oracle.com>
Date:   Thu Aug 8 12:05:35 2019 -0400

    padata: initialize pd->cpu with effective cpumask
    
    Exercising CPU hotplug on a 5.2 kernel with recent padata fixes from
    cryptodev-2.6.git in an 8-CPU kvm guest...
    
        # modprobe tcrypt alg="pcrypt(rfc4106(gcm(aes)))" type=3
        # echo 0 > /sys/devices/system/cpu/cpu1/online
        # echo c > /sys/kernel/pcrypt/pencrypt/parallel_cpumask
        # modprobe tcrypt mode=215
    
    ...caused the following crash:
    
        BUG: kernel NULL pointer dereference, address: 0000000000000000
        #PF: supervisor read access in kernel mode
        #PF: error_code(0x0000) - not-present page
        PGD 0 P4D 0
        Oops: 0000 [#1] SMP PTI
        CPU: 2 PID: 134 Comm: kworker/2:2 Not tainted 5.2.0-padata-base+ #7
        Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-<snip>
        Workqueue: pencrypt padata_parallel_worker
        RIP: 0010:padata_reorder+0xcb/0x180
        ...
        Call Trace:
         padata_do_serial+0x57/0x60
         pcrypt_aead_enc+0x3a/0x50 [pcrypt]
         padata_parallel_worker+0x9b/0xe0
         process_one_work+0x1b5/0x3f0
         worker_thread+0x4a/0x3c0
         ...
    
    In padata_alloc_pd, pd->cpu is set using the user-supplied cpumask
    instead of the effective cpumask, and in this case cpumask_first picked
    an offline CPU.
    
    The offline CPU's reorder->list.next is NULL in padata_reorder because
    the list wasn't initialized in padata_init_pqueues, which only operates
    on CPUs in the effective mask.
    
    Fix by using the effective mask in padata_alloc_pd.
    
    Fixes: 6fc4dbcf0276 ("padata: Replace delayed timer with immediate workqueue in padata_reorder")
    Signed-off-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Steffen Klassert <steffen.klassert@secunet.com>
    Cc: linux-crypto@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

commit 10e7071b2f491b0fb981717ea0a585c441906ede
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Aug 6 15:13:17 2019 +0200

    sched: Rework CPU hotplug task selection
    
    The CPU hotplug task selection is the only place where we used
    put_prev_task() on a task that is not current. While looking at that,
    it occured to me that we can simplify all that by by using a custom
    pick loop.
    
    Since we don't need to put current, we can do away with the fake task
    too.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Aaron Lu <aaron.lwe@gmail.com>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: mingo@kernel.org
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Julien Desfossez <jdesfossez@digitalocean.com>
    Cc: Nishanth Aravamudan <naravamudan@digitalocean.com>

commit 5d87874f9076dd6196db8473f2811db2d08ca45c
Author: Nathan Lynch <nathanl@linux.ibm.com>
Date:   Tue Jun 11 23:45:05 2019 -0500

    powerpc/pseries/mobility: prevent cpu hotplug during DT update
    
    [ Upstream commit e59a175faa8df9d674247946f2a5a9c29c835725 ]
    
    CPU online/offline code paths are sensitive to parts of the device
    tree (various cpu node properties, cache nodes) that can be changed as
    a result of a migration.
    
    Prevent CPU hotplug while the device tree potentially is inconsistent.
    
    Fixes: 410bccf97881 ("powerpc/pseries: Partition migration in the kernel")
    Signed-off-by: Nathan Lynch <nathanl@linux.ibm.com>
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit fd0d171c706462bcb5bfd2fc21997aac6dc3d801
Author: Nathan Lynch <nathanl@linux.ibm.com>
Date:   Tue Jun 11 23:45:05 2019 -0500

    powerpc/pseries/mobility: prevent cpu hotplug during DT update
    
    [ Upstream commit e59a175faa8df9d674247946f2a5a9c29c835725 ]
    
    CPU online/offline code paths are sensitive to parts of the device
    tree (various cpu node properties, cache nodes) that can be changed as
    a result of a migration.
    
    Prevent CPU hotplug while the device tree potentially is inconsistent.
    
    Fixes: 410bccf97881 ("powerpc/pseries: Partition migration in the kernel")
    Signed-off-by: Nathan Lynch <nathanl@linux.ibm.com>
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit e26242e226785f6148689673ec281f89b0bf2931
Author: Nathan Lynch <nathanl@linux.ibm.com>
Date:   Tue Jun 11 23:45:05 2019 -0500

    powerpc/pseries/mobility: prevent cpu hotplug during DT update
    
    [ Upstream commit e59a175faa8df9d674247946f2a5a9c29c835725 ]
    
    CPU online/offline code paths are sensitive to parts of the device
    tree (various cpu node properties, cache nodes) that can be changed as
    a result of a migration.
    
    Prevent CPU hotplug while the device tree potentially is inconsistent.
    
    Fixes: 410bccf97881 ("powerpc/pseries: Partition migration in the kernel")
    Signed-off-by: Nathan Lynch <nathanl@linux.ibm.com>
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 0c09ab96fc820109d63097a2adcbbd20836b655f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 9 16:23:40 2019 +0200

    cpu/hotplug: Cache number of online CPUs
    
    Re-evaluating the bitmap wheight of the online cpus bitmap in every
    invocation of num_online_cpus() over and over is a pretty useless
    exercise. Especially when num_online_cpus() is used in code paths
    like the IPI delivery of x86 or the membarrier code.
    
    Cache the number of online CPUs in the core and just return the cached
    variable. The accessor function provides only a snapshot when used without
    protection against concurrent CPU hotplug.
    
    The storage needs to use an atomic_t because the kexec and reboot code
    (ab)use set_cpu_online() in their 'shutdown' handlers without any form of
    serialization as pointed out by Mathieu. Regular CPU hotplug usage is
    properly serialized.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1907091622590.1634@nanos.tec.linutronix.de

commit c6811c231798d516cab653b34a1d44d7491edf27
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Thu Mar 7 08:41:22 2019 +0300

    xen, cpu_hotplug: Prevent an out of bounds access
    
    commit 201676095dda7e5b31a5e1d116d10fc22985075e upstream.
    
    The "cpu" variable comes from the sscanf() so Smatch marks it as
    untrusted data.  We can't pass a higher value than "nr_cpu_ids" to
    cpu_possible() or it results in an out of bounds access.
    
    Fixes: d68d82afd4c8 ("xen: implement CPU hotplugging")
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit e0e86b111bca6bbf746c03ec5cf3e6a61fa3f8e9
Merge: 568521d058aa caa759323c73
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 10:39:56 2019 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull SMP/hotplug updates from Thomas Gleixner:
     "A small set of updates for SMP and CPU hotplug:
    
       - Abort disabling secondary CPUs in the freezer when a wakeup is
         pending instead of evaluating it only after all CPUs have been
         offlined.
    
       - Remove the shared annotation for the strict per CPU cfd_data in the
         smp function call core code.
    
       - Remove the return values of smp_call_function() and on_each_cpu()
         as they are unconditionally 0. Fixup the few callers which actually
         bothered to check the return value"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      smp: Remove smp_call_function() and on_each_cpu() return values
      smp: Do not mark call_function_data as shared
      cpu/hotplug: Abort disabling secondary CPUs if wakeup is pending
      cpu/hotplug: Fix notify_cpu_starting() reference in bringup_wait_for_ap()

commit 4d8b4ca8554465925a48f39ede16fd740c89287c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 18 22:31:40 2019 +0200

    x86/microcode: Fix the microcode load on CPU hotplug for real
    
    commit 5423f5ce5ca410b3646f355279e4e937d452e622 upstream.
    
    A recent change moved the microcode loader hotplug callback into the early
    startup phase which is running with interrupts disabled. It missed that
    the callbacks invoke sysfs functions which might sleep causing nice 'might
    sleep' splats with proper debugging enabled.
    
    Split the callbacks and only load the microcode in the early startup phase
    and move the sysfs handling back into the later threaded and preemptible
    bringup phase where it was before.
    
    Fixes: 78f4e932f776 ("x86/microcode, cpuhotplug: Add a microcode loader CPU hotplug callback")
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: stable@vger.kernel.org
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1906182228350.1766@nanos.tec.linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1746dc52910481af79df3382789e56073017ab29
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 18 22:31:40 2019 +0200

    x86/microcode: Fix the microcode load on CPU hotplug for real
    
    commit 5423f5ce5ca410b3646f355279e4e937d452e622 upstream.
    
    A recent change moved the microcode loader hotplug callback into the early
    startup phase which is running with interrupts disabled. It missed that
    the callbacks invoke sysfs functions which might sleep causing nice 'might
    sleep' splats with proper debugging enabled.
    
    Split the callbacks and only load the microcode in the early startup phase
    and move the sysfs handling back into the later threaded and preemptible
    bringup phase where it was before.
    
    Fixes: 78f4e932f776 ("x86/microcode, cpuhotplug: Add a microcode loader CPU hotplug callback")
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: stable@vger.kernel.org
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1906182228350.1766@nanos.tec.linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3c762ccd9d57fbe34d4439cc167ea59803b3fb05
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 18 22:31:40 2019 +0200

    x86/microcode: Fix the microcode load on CPU hotplug for real
    
    commit 5423f5ce5ca410b3646f355279e4e937d452e622 upstream.
    
    A recent change moved the microcode loader hotplug callback into the early
    startup phase which is running with interrupts disabled. It missed that
    the callbacks invoke sysfs functions which might sleep causing nice 'might
    sleep' splats with proper debugging enabled.
    
    Split the callbacks and only load the microcode in the early startup phase
    and move the sysfs handling back into the later threaded and preemptible
    bringup phase where it was before.
    
    Fixes: 78f4e932f776 ("x86/microcode, cpuhotplug: Add a microcode loader CPU hotplug callback")
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: stable@vger.kernel.org
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1906182228350.1766@nanos.tec.linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 728254541ebcc7fee869c3c4c3f36f96be791edb
Merge: 57103eb7c6ca ae6a45a08689
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 29 19:42:30 2019 +0800

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar:
     "Misc fixes all over the place:
    
       - might_sleep() atomicity fix in the microcode loader
    
       - resctrl boundary condition fix
    
       - APIC arithmethics bug fix for frequencies >= 4.2 GHz
    
       - three 5-level paging crash fixes
    
       - two speculation fixes
    
       - a perf/stacktrace fix"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/unwind/orc: Fall back to using frame pointers for generated code
      perf/x86: Always store regs->ip in perf_callchain_kernel()
      x86/speculation: Allow guests to use SSBD even if host does not
      x86/mm: Handle physical-virtual alignment mismatch in phys_p4d_init()
      x86/boot/64: Add missing fixup_pointer() for next_early_pgt access
      x86/boot/64: Fix crash if kernel image crosses page table boundary
      x86/apic: Fix integer overflow on 10 bit left shift of cpu_khz
      x86/resctrl: Prevent possible overrun during bitmap operations
      x86/microcode: Fix the microcode load on CPU hotplug for real

commit 36b9017f0250a5299bb715b3b8c41b5e2b05b320
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jun 23 15:23:41 2019 +0200

    x86/hpet: Simplify CPU online code
    
    The indirection via work scheduled on the upcoming CPU was necessary with the
    old hotplug code because the online callback was invoked on the control CPU
    not on the upcoming CPU. The rework of the CPU hotplug core guarantees that
    the online callbacks are invoked on the upcoming CPU.
    
    Remove the now pointless work redirection.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Cc: Ashok Raj <ashok.raj@intel.com>
    Cc: Andi Kleen <andi.kleen@intel.com>
    Cc: Suravee Suthikulpanit <Suravee.Suthikulpanit@amd.com>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Ravi Shankar <ravi.v.shankar@intel.com>
    Link: https://lkml.kernel.org/r/20190623132434.047254075@linutronix.de

commit 882f261874c9aa3792e37d631342e84c84766dd0
Merge: 5c4c8b4a9990 9e0babf2c06c
Author: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
Date:   Fri Jun 21 16:09:25 2019 -0400

    Merge tag 'v5.2-rc5' into patchwork
    
    Linux 5.2-rc5
    
    There are some media fixes on -rc5, so merge from it at media
    devel tree.
    
    * tag 'v5.2-rc5': (210 commits)
      Linux 5.2-rc5
      x86/microcode, cpuhotplug: Add a microcode loader CPU hotplug callback
      Smack: Restore the smackfsdef mount option and add missing prefixes
      ftrace: Fix NULL pointer dereference in free_ftrace_func_mapper()
      module: Fix livepatch/ftrace module text permissions race
      tracing/uprobe: Fix obsolete comment on trace_uprobe_create()
      tracing/uprobe: Fix NULL pointer dereference in trace_uprobe_create()
      tracing: Make two symbols static
      tracing: avoid build warning with HAVE_NOP_MCOUNT
      tracing: Fix out-of-range read in trace_stack_print()
      gfs2: Fix rounding error in gfs2_iomap_page_prepare
      x86/kasan: Fix boot with 5-level paging and KASAN
      timekeeping: Repair ktime_get_coarse*() granularity
      Revert "ALSA: hda/realtek - Improve the headset mic for Acer Aspire laptops"
      mm/devm_memremap_pages: fix final page put race
      PCI/P2PDMA: track pgmap references per resource, not globally
      lib/genalloc: introduce chunk owners
      PCI/P2PDMA: fix the gen_pool_add_virt() failure path
      mm/devm_memremap_pages: introduce devm_memunmap_pages
      drivers/base/devres: introduce devm_release_action()
      ...

commit 5423f5ce5ca410b3646f355279e4e937d452e622
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 18 22:31:40 2019 +0200

    x86/microcode: Fix the microcode load on CPU hotplug for real
    
    A recent change moved the microcode loader hotplug callback into the early
    startup phase which is running with interrupts disabled. It missed that
    the callbacks invoke sysfs functions which might sleep causing nice 'might
    sleep' splats with proper debugging enabled.
    
    Split the callbacks and only load the microcode in the early startup phase
    and move the sysfs handling back into the later threaded and preemptible
    bringup phase where it was before.
    
    Fixes: 78f4e932f776 ("x86/microcode, cpuhotplug: Add a microcode loader CPU hotplug callback")
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: stable@vger.kernel.org
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1906182228350.1766@nanos.tec.linutronix.de

commit 9e7018426849aebaddbd48fe5d16727ead98f711
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Jun 13 15:49:02 2019 +0200

    x86/microcode, cpuhotplug: Add a microcode loader CPU hotplug callback
    
    commit 78f4e932f7760d965fb1569025d1576ab77557c5 upstream.
    
    Adric Blake reported the following warning during suspend-resume:
    
      Enabling non-boot CPUs ...
      x86: Booting SMP configuration:
      smpboot: Booting Node 0 Processor 1 APIC 0x2
      unchecked MSR access error: WRMSR to 0x10f (tried to write 0x0000000000000000) \
       at rIP: 0xffffffff8d267924 (native_write_msr+0x4/0x20)
      Call Trace:
       intel_set_tfa
       intel_pmu_cpu_starting
       ? x86_pmu_dead_cpu
       x86_pmu_starting_cpu
       cpuhp_invoke_callback
       ? _raw_spin_lock_irqsave
       notify_cpu_starting
       start_secondary
       secondary_startup_64
      microcode: sig=0x806ea, pf=0x80, revision=0x96
      microcode: updated to revision 0xb4, date = 2019-04-01
      CPU1 is up
    
    The MSR in question is MSR_TFA_RTM_FORCE_ABORT and that MSR is emulated
    by microcode. The log above shows that the microcode loader callback
    happens after the PMU restoration, leading to the conjecture that
    because the microcode hasn't been updated yet, that MSR is not present
    yet, leading to the #GP.
    
    Add a microcode loader-specific hotplug vector which comes before
    the PERF vectors and thus executes earlier and makes sure the MSR is
    present.
    
    Fixes: 400816f60c54 ("perf/x86/intel: Implement support for TSX Force Abort")
    Reported-by: Adric Blake <promarbler14@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: <stable@vger.kernel.org>
    Cc: x86@kernel.org
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=203637
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ecec31ce4f33c927997f179f5d8f1bc4efdd68b5
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Jun 13 15:49:02 2019 +0200

    x86/microcode, cpuhotplug: Add a microcode loader CPU hotplug callback
    
    commit 78f4e932f7760d965fb1569025d1576ab77557c5 upstream.
    
    Adric Blake reported the following warning during suspend-resume:
    
      Enabling non-boot CPUs ...
      x86: Booting SMP configuration:
      smpboot: Booting Node 0 Processor 1 APIC 0x2
      unchecked MSR access error: WRMSR to 0x10f (tried to write 0x0000000000000000) \
       at rIP: 0xffffffff8d267924 (native_write_msr+0x4/0x20)
      Call Trace:
       intel_set_tfa
       intel_pmu_cpu_starting
       ? x86_pmu_dead_cpu
       x86_pmu_starting_cpu
       cpuhp_invoke_callback
       ? _raw_spin_lock_irqsave
       notify_cpu_starting
       start_secondary
       secondary_startup_64
      microcode: sig=0x806ea, pf=0x80, revision=0x96
      microcode: updated to revision 0xb4, date = 2019-04-01
      CPU1 is up
    
    The MSR in question is MSR_TFA_RTM_FORCE_ABORT and that MSR is emulated
    by microcode. The log above shows that the microcode loader callback
    happens after the PMU restoration, leading to the conjecture that
    because the microcode hasn't been updated yet, that MSR is not present
    yet, leading to the #GP.
    
    Add a microcode loader-specific hotplug vector which comes before
    the PERF vectors and thus executes earlier and makes sure the MSR is
    present.
    
    Fixes: 400816f60c54 ("perf/x86/intel: Implement support for TSX Force Abort")
    Reported-by: Adric Blake <promarbler14@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: <stable@vger.kernel.org>
    Cc: x86@kernel.org
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=203637
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7ce303bd5678bafca266a8e4db56752a87f8fada
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Jun 13 15:49:02 2019 +0200

    x86/microcode, cpuhotplug: Add a microcode loader CPU hotplug callback
    
    commit 78f4e932f7760d965fb1569025d1576ab77557c5 upstream.
    
    Adric Blake reported the following warning during suspend-resume:
    
      Enabling non-boot CPUs ...
      x86: Booting SMP configuration:
      smpboot: Booting Node 0 Processor 1 APIC 0x2
      unchecked MSR access error: WRMSR to 0x10f (tried to write 0x0000000000000000) \
       at rIP: 0xffffffff8d267924 (native_write_msr+0x4/0x20)
      Call Trace:
       intel_set_tfa
       intel_pmu_cpu_starting
       ? x86_pmu_dead_cpu
       x86_pmu_starting_cpu
       cpuhp_invoke_callback
       ? _raw_spin_lock_irqsave
       notify_cpu_starting
       start_secondary
       secondary_startup_64
      microcode: sig=0x806ea, pf=0x80, revision=0x96
      microcode: updated to revision 0xb4, date = 2019-04-01
      CPU1 is up
    
    The MSR in question is MSR_TFA_RTM_FORCE_ABORT and that MSR is emulated
    by microcode. The log above shows that the microcode loader callback
    happens after the PMU restoration, leading to the conjecture that
    because the microcode hasn't been updated yet, that MSR is not present
    yet, leading to the #GP.
    
    Add a microcode loader-specific hotplug vector which comes before
    the PERF vectors and thus executes earlier and makes sure the MSR is
    present.
    
    Fixes: 400816f60c54 ("perf/x86/intel: Implement support for TSX Force Abort")
    Reported-by: Adric Blake <promarbler14@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: <stable@vger.kernel.org>
    Cc: x86@kernel.org
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=203637
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 963172d9c7e862654d3d24cbcafb33f33ae697a8
Merge: efba92d58fa3 78f4e932f776
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jun 16 07:28:14 2019 -1000

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Thomas Gleixner:
     "The accumulated fixes from this and last week:
    
       - Fix vmalloc TLB flush and map range calculations which lead to
         stale TLBs, spurious faults and other hard to diagnose issues.
    
       - Use fault_in_pages_writable() for prefaulting the user stack in the
         FPU code as it's less fragile than the current solution
    
       - Use the PF_KTHREAD flag when checking for a kernel thread instead
         of current->mm as the latter can give the wrong answer due to
         use_mm()
    
       - Compute the vmemmap size correctly for KASLR and 5-Level paging.
         Otherwise this can end up with a way too small vmemmap area.
    
       - Make KASAN and 5-level paging work again by making sure that all
         invalid bits are masked out when computing the P4D offset. This
         worked before but got broken recently when the LDT remap area was
         moved.
    
       - Prevent a NULL pointer dereference in the resource control code
         which can be triggered with certain mount options when the
         requested resource is not available.
    
       - Enforce ordering of microcode loading vs. perf initialization on
         secondary CPUs. Otherwise perf tries to access a non-existing MSR
         as the boot CPU marked it as available.
    
       - Don't stop the resource control group walk early otherwise the
         control bitmaps are not updated correctly and become inconsistent.
    
       - Unbreak kgdb by returning 0 on success from
         kgdb_arch_set_breakpoint() instead of an error code.
    
       - Add more Icelake CPU model defines so depending changes can be
         queued in other trees"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/microcode, cpuhotplug: Add a microcode loader CPU hotplug callback
      x86/kasan: Fix boot with 5-level paging and KASAN
      x86/fpu: Don't use current->mm to check for a kthread
      x86/kgdb: Return 0 from kgdb_arch_set_breakpoint()
      x86/resctrl: Prevent NULL pointer dereference when local MBM is disabled
      x86/resctrl: Don't stop walking closids when a locksetup group is found
      x86/fpu: Update kernel's FPU state before using for the fsave header
      x86/mm/KASLR: Compute the size of the vmemmap section properly
      x86/fpu: Use fault_in_pages_writeable() for pre-faulting
      x86/CPU: Add more Icelake model numbers
      mm/vmalloc: Avoid rare case of flushing TLB with weird arguments
      mm/vmalloc: Fix calculation of direct map addr range

commit 78f4e932f7760d965fb1569025d1576ab77557c5
Author: Borislav Petkov <bp@suse.de>
Date:   Thu Jun 13 15:49:02 2019 +0200

    x86/microcode, cpuhotplug: Add a microcode loader CPU hotplug callback
    
    Adric Blake reported the following warning during suspend-resume:
    
      Enabling non-boot CPUs ...
      x86: Booting SMP configuration:
      smpboot: Booting Node 0 Processor 1 APIC 0x2
      unchecked MSR access error: WRMSR to 0x10f (tried to write 0x0000000000000000) \
       at rIP: 0xffffffff8d267924 (native_write_msr+0x4/0x20)
      Call Trace:
       intel_set_tfa
       intel_pmu_cpu_starting
       ? x86_pmu_dead_cpu
       x86_pmu_starting_cpu
       cpuhp_invoke_callback
       ? _raw_spin_lock_irqsave
       notify_cpu_starting
       start_secondary
       secondary_startup_64
      microcode: sig=0x806ea, pf=0x80, revision=0x96
      microcode: updated to revision 0xb4, date = 2019-04-01
      CPU1 is up
    
    The MSR in question is MSR_TFA_RTM_FORCE_ABORT and that MSR is emulated
    by microcode. The log above shows that the microcode loader callback
    happens after the PMU restoration, leading to the conjecture that
    because the microcode hasn't been updated yet, that MSR is not present
    yet, leading to the #GP.
    
    Add a microcode loader-specific hotplug vector which comes before
    the PERF vectors and thus executes earlier and makes sure the MSR is
    present.
    
    Fixes: 400816f60c54 ("perf/x86/intel: Implement support for TSX Force Abort")
    Reported-by: Adric Blake <promarbler14@gmail.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: <stable@vger.kernel.org>
    Cc: x86@kernel.org
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=203637

commit e59a175faa8df9d674247946f2a5a9c29c835725
Author: Nathan Lynch <nathanl@linux.ibm.com>
Date:   Tue Jun 11 23:45:05 2019 -0500

    powerpc/pseries/mobility: prevent cpu hotplug during DT update
    
    CPU online/offline code paths are sensitive to parts of the device
    tree (various cpu node properties, cache nodes) that can be changed as
    a result of a migration.
    
    Prevent CPU hotplug while the device tree potentially is inconsistent.
    
    Fixes: 410bccf97881 ("powerpc/pseries: Partition migration in the kernel")
    Signed-off-by: Nathan Lynch <nathanl@linux.ibm.com>
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit a66d955e910ab0e598d7a7450cbe6139f52befe7
Author: Pavankumar Kondeti <pkondeti@codeaurora.org>
Date:   Mon Jun 3 10:01:03 2019 +0530

    cpu/hotplug: Abort disabling secondary CPUs if wakeup is pending
    
    When "deep" suspend is enabled, all CPUs except the primary CPU are frozen
    via CPU hotplug one by one. After all secondary CPUs are unplugged the
    wakeup pending condition is evaluated and if pending the suspend operation
    is aborted and the secondary CPUs are brought up again.
    
    CPU hotplug is a slow operation, so it makes sense to check for wakeup
    pending in the freezer loop before bringing down the next CPU. This
    improves the system suspend abort latency significantly.
    
    [ tglx: Massaged changelog and improved printk message ]
    
    Signed-off-by: Pavankumar Kondeti <pkondeti@codeaurora.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: iri Kosina <jkosina@suse.cz>
    Cc: Mukesh Ojha <mojha@codeaurora.org>
    Cc: linux-pm@vger.kernel.org
    Link: https://lkml.kernel.org/r/1559536263-16472-1-git-send-email-pkondeti@codeaurora.org

commit 6f3710f1f65fdc0da2b042ea6a9a738ddd146d4e
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed May 29 14:29:45 2019 +0100

    arm: dts: vexpress-v2p-ca15_a7: disable NOR flash node by default
    
    Accessing the NOR flash memory from the kernel will disrupt CPU sleep/
    idles states and CPU hotplugging. We need to disable this DT node by
    default. Setups that want to access the flash can modify this entry to
    enable the flash again but also ensuring to disable CPU idle states and
    CPU hotplug.
    
    The platform firmware assumes the flash is always in read mode while
    Linux kernel driver leaves NOR flash in "read id" mode after
    initialization. If it gets used actively, it can be in some other state.
    
    So far we had not seen this issue as the NOR flash drivers in kernel
    were not enabled by default. However it was enable in multi_v7 config by
    Commit 5f068190cc10 ("ARM: multi_v7_defconfig: Enable support for CFI NOR FLASH")
    
    So, let's mark the NOR flash disabled so that the platform can boot
    again. This based on:
    Commit 980bbff018f6 ("ARM64: juno: disable NOR flash node by default")
    
    Cc: Liviu Dudau <liviu.dudau@arm.com>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Reviewed-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>

commit c682db558e6eec10a711b0a6bcb8c35fd15f6a39
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Fri Apr 19 07:38:27 2019 -0700

    rcutorture: Add trivial RCU implementation
    
    I have been showing off a trivial RCU implementation for non-preemptive
    environments for some time now:
    
            #define rcu_read_lock()
            #define rcu_read_unlock()
            #define rcu_dereference(p) READ_ONCE(p)
            #define rcu_assign_pointer(p, v) smp_store_release(&(p), (v))
            void synchronize_rcu(void)
            {
            int cpu;
                    for_each_online_cpu(cpu)
                            sched_setaffinity(current->pid, cpumask_of(cpu));
            }
    
    Trivial or not, as the old saying goes, "if it ain't tested, it don't
    work!".  This commit therefore adds a "trivial" flavor to rcutorture
    and a corresponding TRIVIAL test scenario.  This variant does not handle
    CPU hotplug, which is unconditionally enabled on x86 for post-v5.1-rc3
    kernels, which is why the TRIVIAL.boot says "rcutorture.onoff_interval=0".
    This commit actually does handle CONFIG_PREEMPT=y kernels, but only
    because it turns back the Linux-kernel clock in order to provide these
    alternative definitions (or the moral equivalent thereof):
    
            #define rcu_read_lock() preempt_disable()
            #define rcu_read_unlock() preempt_enable()
    
    In CONFIG_PREEMPT=n kernels without debugging, these are equivalent to
    empty macros give or take a compiler barrier.  However, the have been
    successfully tested with actual empty macros as well.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>
    [ paulmck: Fix symbol issue reported by kbuild test robot <lkp@intel.com>. ]
    [ paulmck: Work around sched_setaffinity() issue noted by Andrea Parri. ]
    [ paulmck: Add rcutorture.shuffle_interval=0 to TRIVIAL.boot to fix
      interaction with shuffler task noted by Peter Zijlstra. ]
    Tested-by: Andrea Parri <andrea.parri@amarulasolutions.com>

commit d6e245acc900feeeae021c96d6b33a1c71598373
Merge: 3c7f51bfad49 7278358407be
Author: Olof Johansson <olof@lixom.net>
Date:   Thu May 16 10:55:23 2019 -0700

    Merge tag 'tegra-for-5.2-arm64-dt-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tegra/linux into arm/late
    
    arm64: tegra: Device tree fixes for v5.2-rc1
    
    This contains one patch to disable the recently added XUSB support on
    Jetson TX2 which is reported to cause boot and CPU hotplug failures in
    some cases and doesn't allow the core power rail to be switched off.
    
    Furthermore there are some changes to enable IOMMU support on more
    devices. This is needed in order to prevent these devices from breaking
    with the policy change in the ARM SMMU driver to break insecure devices
    that is currently headed for v5.2.
    
    * tag 'tegra-for-5.2-arm64-dt-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tegra/linux:
      arm64: tegra: Disable XUSB support on Jetson TX2
      arm64: tegra: Enable SMMU translation for PCI on Tegra186
      arm64: tegra: Fix insecure SMMU users for Tegra186
    
    Signed-off-by: Olof Johansson <olof@lixom.net>

commit 8f147727030bf9e81331ab9b8f42d4611bb6a3d9
Merge: 53f8b081c184 2c4645439e8f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 15:56:41 2019 -0700

    Merge branch 'x86-irq-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 irq updates from Ingo Molnar:
     "Here are the main changes in this tree:
    
       - Introduce x86-64 IRQ/exception/debug stack guard pages to detect
         stack overflows immediately and deterministically.
    
       - Clean up over a decade worth of cruft accumulated.
    
      The outcome of this should be more clear-cut faults/crashes when any
      of the low level x86 CPU stacks overflow, instead of silent memory
      corruption and sporadic failures much later on"
    
    * 'x86-irq-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (33 commits)
      x86/irq: Fix outdated comments
      x86/irq/64: Remove stack overflow debug code
      x86/irq/64: Remap the IRQ stack with guard pages
      x86/irq/64: Split the IRQ stack into its own pages
      x86/irq/64: Init hardirq_stack_ptr during CPU hotplug
      x86/irq/32: Handle irq stack allocation failure proper
      x86/irq/32: Invoke irq_ctx_init() from init_IRQ()
      x86/irq/64: Rename irq_stack_ptr to hardirq_stack_ptr
      x86/irq/32: Rename hard/softirq_stack to hard/softirq_stack_ptr
      x86/irq/32: Make irq stack a character array
      x86/irq/32: Define IRQ_STACK_SIZE
      x86/dumpstack/64: Speedup in_exception_stack()
      x86/exceptions: Split debug IST stack
      x86/exceptions: Enable IST guard pages
      x86/exceptions: Disconnect IST index and stack order
      x86/cpu: Remove orig_ist array
      x86/cpu: Prepare TSS.IST setup for guard pages
      x86/dumpstack/64: Use cpu_entry_area instead of orig_ist
      x86/irq/64: Use cpu entry area instead of orig_ist
      x86/traps: Use cpu_entry_area instead of orig_ist
      ...

commit a0e928ed7c603a47dca8643e58db224a799ff2c5
Merge: 5a2bf1abbf96 13e792a19d4e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 14:50:46 2019 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Ingo Molnar:
     "This cycle had the following changes:
    
       - Timer tracing improvements (Anna-Maria Gleixner)
    
       - Continued tasklet reduction work: remove the hrtimer_tasklet
         (Thomas Gleixner)
    
       - Fix CPU hotplug remove race in the tick-broadcast mask handling
         code (Thomas Gleixner)
    
       - Force upper bound for setting CLOCK_REALTIME, to fix ABI
         inconsistencies with handling values that are close to the maximum
         supported and the vagueness of when uptime related wraparound might
         occur. Make the consistent maximum the year 2232 across all
         relevant ABIs and APIs. (Thomas Gleixner)
    
       - various cleanups and smaller fixes"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      tick: Fix typos in comments
      tick/broadcast: Fix warning about undefined tick_broadcast_oneshot_offline()
      timekeeping: Force upper bound for setting CLOCK_REALTIME
      timer/trace: Improve timer tracing
      timer/trace: Replace deprecated vsprintf pointer extension %pf by %ps
      timer: Move trace point to get proper index
      tick/sched: Update tick_sched struct documentation
      tick: Remove outgoing CPU from broadcast masks
      timekeeping: Consistently use unsigned int for seqcount snapshot
      softirq: Remove tasklet_hrtimer
      xfrm: Replace hrtimer tasklet with softirq hrtimer
      mac80211_hwsim: Replace hrtimer tasklet with softirq hrtimer

commit 5a2bf1abbf96fca02b9785c252e569ef8e004851
Merge: e00d4135751b d4645d30b50d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 14:44:49 2019 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug updates from Ingo Molnar:
     "Two changes in this cycle:
    
       - Make the /sys/devices/system/cpu/smt/* files available on all
         arches, so user space has a consistent way to detect whether SMT is
         enabled.
    
       - Sparse annotation fix"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      smpboot: Place the __percpu annotation correctly
      cpu/hotplug: Create SMT sysfs interface for all arches

commit 6beb5579501fdda398c6b08f9af5771e72afc967
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Wed Jan 9 13:00:03 2019 +0100

    s390/smp: fix CPU hotplug deadlock with CPU rescan
    
    commit b7cb707c373094ce4008d4a6ac9b6b366ec52da5 upstream.
    
    smp_rescan_cpus() is called without the device_hotplug_lock, which can lead
    to a dedlock when a new CPU is found and immediately set online by a udev
    rule.
    
    This was observed on an older kernel version, where the cpu_hotplug_begin()
    loop was still present, and it resulted in hanging chcpu and systemd-udev
    processes. This specific deadlock will not show on current kernels. However,
    there may be other possible deadlocks, and since smp_rescan_cpus() can still
    trigger a CPU hotplug operation, the device_hotplug_lock should be held.
    
    For reference, this was the deadlock with the old cpu_hotplug_begin() loop:
    
            chcpu (rescan)                       systemd-udevd
    
     echo 1 > /sys/../rescan
     -> smp_rescan_cpus()
     -> (*) get_online_cpus()
        (increases refcount)
     -> smp_add_present_cpu()
        (new CPU found)
     -> register_cpu()
     -> device_add()
     -> udev "add" event triggered -----------> udev rule sets CPU online
                                             -> echo 1 > /sys/.../online
                                             -> lock_device_hotplug_sysfs()
                                                (this is missing in rescan path)
                                             -> device_online()
                                             -> (**) device_lock(new CPU dev)
                                             -> cpu_up()
                                             -> cpu_hotplug_begin()
                                                (loops until refcount == 0)
                                                -> deadlock with (*)
     -> bus_probe_device()
     -> device_attach()
     -> device_lock(new CPU dev)
        -> deadlock with (**)
    
    Fix this by taking the device_hotplug_lock in the CPU rescan path.
    
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 9bcb929f969e4054732158908b1d70e787ef780f
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Tue Apr 16 16:24:25 2019 +0100

    perf/arm-ccn: Clean up CPU hotplug handling
    
    Like arm-cci, arm-ccn has the same issue of disabling preemption around
    operations which can take mutexes. Again, remove the definite bug by
    simply not trying to fight the theoretical races. And since we are
    touching the hotplug handling code, take the opportunity to streamline
    it, as there's really no need to store a full-sized cpumask to keep
    track of a single CPU ID.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 18b7a6bef62de1d598fbff23b52114b7775ecf00
Author: Andy Lutomirski <luto@kernel.org>
Date:   Sun Apr 14 18:00:07 2019 +0200

    x86/irq/64: Remap the IRQ stack with guard pages
    
    The IRQ stack lives in percpu space, so an IRQ handler that overflows it
    will overwrite other data structures.
    
    Use vmap() to remap the IRQ stack so that it will have the usual guard
    pages that vmap()/vmalloc() allocations have. With this, the kernel will
    panic immediately on an IRQ stack overflow.
    
    [ tglx: Move the map code to a proper place and invoke it only when a CPU
            is about to be brought online. No point in installing the map at
            early boot for all possible CPUs. Fail the CPU bringup if the vmap()
            fails as done for all other preparatory stages in CPU hotplug. ]
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: x86-ml <x86@kernel.org>
    Link: https://lkml.kernel.org/r/20190414160146.363733568@linutronix.de

commit 0ac26104208450d35c4e68754ce0c67b3a4d7802
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 18:00:05 2019 +0200

    x86/irq/64: Init hardirq_stack_ptr during CPU hotplug
    
    Preparatory change for disentangling the irq stack union as a
    prerequisite for irq stacks with guard pages.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: "Chang S. Bae" <chang.seok.bae@intel.com>
    Cc: Dominik Brodowski <linux@dominikbrodowski.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: x86-ml <x86@kernel.org>
    Cc: Yi Wang <wang.yi59@zte.com.cn>
    Link: https://lkml.kernel.org/r/20190414160146.177558566@linutronix.de

commit 66c7ceb47f628c8bd4f84a6d01c2725ded6a342d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Apr 14 18:00:04 2019 +0200

    x86/irq/32: Handle irq stack allocation failure proper
    
    irq_ctx_init() crashes hard on page allocation failures. While that's ok
    during early boot, it's just wrong in the CPU hotplug bringup code.
    
    Check the page allocation failure and return -ENOMEM and handle it at the
    call sites. On early boot the only way out is to BUG(), but on CPU hotplug
    there is no reason to crash, so just abort the operation.
    
    Rename the function to something more sensible while at it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Alison Schofield <alison.schofield@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: Pu Wen <puwen@hygon.cn>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Shaokun Zhang <zhangshaokun@hisilicon.com>
    Cc: Stefano Stabellini <sstabellini@kernel.org>
    Cc: Suravee Suthikulpanit <suravee.suthikulpanit@amd.com>
    Cc: x86-ml <x86@kernel.org>
    Cc: xen-devel@lists.xenproject.org
    Cc: Yazen Ghannam <yazen.ghannam@amd.com>
    Cc: Yi Wang <wang.yi59@zte.com.cn>
    Cc: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Link: https://lkml.kernel.org/r/20190414160146.089060584@linutronix.de

commit c3bcf031466592a5d58eafae1d8df5bc4448122c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 26 17:36:05 2019 +0100

    cpu/hotplug: Prevent crash when CPU bringup fails on CONFIG_HOTPLUG_CPU=n
    
    commit 206b92353c839c0b27a0b9bec24195f93fd6cf7a upstream.
    
    Tianyu reported a crash in a CPU hotplug teardown callback when booting a
    kernel which has CONFIG_HOTPLUG_CPU disabled with the 'nosmt' boot
    parameter.
    
    It turns out that the SMP=y CONFIG_HOTPLUG_CPU=n case has been broken
    forever in case that a bringup callback fails. Unfortunately this issue was
    not recognized when the CPU hotplug code was reworked, so the shortcoming
    just stayed in place.
    
    When a bringup callback fails, the CPU hotplug code rolls back the
    operation and takes the CPU offline.
    
    The 'nosmt' command line argument uses a bringup failure to abort the
    bringup of SMT sibling CPUs. This partial bringup is required due to the
    MCE misdesign on Intel CPUs.
    
    With CONFIG_HOTPLUG_CPU=y the rollback works perfectly fine, but
    CONFIG_HOTPLUG_CPU=n lacks essential mechanisms to exercise the low level
    teardown of a CPU including the synchronizations in various facilities like
    RCU, NOHZ and others.
    
    As a consequence the teardown callbacks which must be executed on the
    outgoing CPU within stop machine with interrupts disabled are executed on
    the control CPU in interrupt enabled and preemptible context causing the
    kernel to crash and burn. The pre state machine code has a different
    failure mode which is more subtle and resulting in a less obvious use after
    free crash because the control side frees resources which are still in use
    by the undead CPU.
    
    But this is not a x86 only problem. Any architecture which supports the
    SMP=y HOTPLUG_CPU=n combination suffers from the same issue. It's just less
    likely to be triggered because in 99.99999% of the cases all bringup
    callbacks succeed.
    
    The easy solution of making HOTPLUG_CPU mandatory for SMP is not working on
    all architectures as the following architectures have either no hotplug
    support at all or not all subarchitectures support it:
    
     alpha, arc, hexagon, openrisc, riscv, sparc (32bit), mips (partial).
    
    Crashing the kernel in such a situation is not an acceptable state
    either.
    
    Implement a minimal rollback variant by limiting the teardown to the point
    where all regular teardown callbacks have been invoked and leave the CPU in
    the 'dead' idle state. This has the following consequences:
    
     - the CPU is brought down to the point where the stop_machine takedown
       would happen.
    
     - the CPU stays there forever and is idle
    
     - The CPU is cleared in the CPU active mask, but not in the CPU online
       mask which is a legit state.
    
     - Interrupts are not forced away from the CPU
    
     - All facilities which only look at online mask would still see it, but
       that is the case during normal hotplug/unplug operations as well. It's
       just a (way) longer time frame.
    
    This will expose issues, which haven't been exposed before or only seldom,
    because now the normally transient state of being non active but online is
    a permanent state. In testing this exposed already an issue vs. work queues
    where the vmstat code schedules work on the almost dead CPU which ends up
    in an unbound workqueue and triggers 'preemtible context' warnings. This is
    not a problem of this change, it merily exposes an already existing issue.
    Still this is better than crashing fully without a chance to debug it.
    
    This is mainly thought as workaround for those architectures which do not
    support HOTPLUG_CPU. All others should enforce HOTPLUG_CPU for SMP.
    
    Fixes: 2e1a3483ce74 ("cpu/hotplug: Split out the state walk into functions")
    Reported-by: Tianyu Lan <Tianyu.Lan@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Tianyu Lan <Tianyu.Lan@microsoft.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Konrad Wilk <konrad.wilk@oracle.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Mukesh Ojha <mojha@codeaurora.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Micheal Kelley <michael.h.kelley@microsoft.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: K. Y. Srinivasan <kys@microsoft.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190326163811.503390616@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 53464ca9130be5466ada7b9dd653059c3a26cad9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 26 22:51:02 2019 +0100

    watchdog: Respect watchdog cpumask on CPU hotplug
    
    commit 7dd47617114921fdd8c095509e5e7b4373cc44a1 upstream.
    
    The rework of the watchdog core to use cpu_stop_work broke the watchdog
    cpumask on CPU hotplug.
    
    The watchdog_enable/disable() functions are now called unconditionally from
    the hotplug callback, i.e. even on CPUs which are not in the watchdog
    cpumask. As a consequence the watchdog can become unstoppable.
    
    Only invoke them when the plugged CPU is in the watchdog cpumask.
    
    Fixes: 9cf57731b63e ("watchdog/softlockup: Replace "watchdog/%u" threads with cpu_stop_work")
    Reported-by: Maxime Coquelin <maxime.coquelin@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Maxime Coquelin <maxime.coquelin@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1903262245490.1789@nanos.tec.linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a56aa02e6f154ff83d8271244402a6392cde7b0f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 26 17:36:05 2019 +0100

    cpu/hotplug: Prevent crash when CPU bringup fails on CONFIG_HOTPLUG_CPU=n
    
    commit 206b92353c839c0b27a0b9bec24195f93fd6cf7a upstream.
    
    Tianyu reported a crash in a CPU hotplug teardown callback when booting a
    kernel which has CONFIG_HOTPLUG_CPU disabled with the 'nosmt' boot
    parameter.
    
    It turns out that the SMP=y CONFIG_HOTPLUG_CPU=n case has been broken
    forever in case that a bringup callback fails. Unfortunately this issue was
    not recognized when the CPU hotplug code was reworked, so the shortcoming
    just stayed in place.
    
    When a bringup callback fails, the CPU hotplug code rolls back the
    operation and takes the CPU offline.
    
    The 'nosmt' command line argument uses a bringup failure to abort the
    bringup of SMT sibling CPUs. This partial bringup is required due to the
    MCE misdesign on Intel CPUs.
    
    With CONFIG_HOTPLUG_CPU=y the rollback works perfectly fine, but
    CONFIG_HOTPLUG_CPU=n lacks essential mechanisms to exercise the low level
    teardown of a CPU including the synchronizations in various facilities like
    RCU, NOHZ and others.
    
    As a consequence the teardown callbacks which must be executed on the
    outgoing CPU within stop machine with interrupts disabled are executed on
    the control CPU in interrupt enabled and preemptible context causing the
    kernel to crash and burn. The pre state machine code has a different
    failure mode which is more subtle and resulting in a less obvious use after
    free crash because the control side frees resources which are still in use
    by the undead CPU.
    
    But this is not a x86 only problem. Any architecture which supports the
    SMP=y HOTPLUG_CPU=n combination suffers from the same issue. It's just less
    likely to be triggered because in 99.99999% of the cases all bringup
    callbacks succeed.
    
    The easy solution of making HOTPLUG_CPU mandatory for SMP is not working on
    all architectures as the following architectures have either no hotplug
    support at all or not all subarchitectures support it:
    
     alpha, arc, hexagon, openrisc, riscv, sparc (32bit), mips (partial).
    
    Crashing the kernel in such a situation is not an acceptable state
    either.
    
    Implement a minimal rollback variant by limiting the teardown to the point
    where all regular teardown callbacks have been invoked and leave the CPU in
    the 'dead' idle state. This has the following consequences:
    
     - the CPU is brought down to the point where the stop_machine takedown
       would happen.
    
     - the CPU stays there forever and is idle
    
     - The CPU is cleared in the CPU active mask, but not in the CPU online
       mask which is a legit state.
    
     - Interrupts are not forced away from the CPU
    
     - All facilities which only look at online mask would still see it, but
       that is the case during normal hotplug/unplug operations as well. It's
       just a (way) longer time frame.
    
    This will expose issues, which haven't been exposed before or only seldom,
    because now the normally transient state of being non active but online is
    a permanent state. In testing this exposed already an issue vs. work queues
    where the vmstat code schedules work on the almost dead CPU which ends up
    in an unbound workqueue and triggers 'preemtible context' warnings. This is
    not a problem of this change, it merily exposes an already existing issue.
    Still this is better than crashing fully without a chance to debug it.
    
    This is mainly thought as workaround for those architectures which do not
    support HOTPLUG_CPU. All others should enforce HOTPLUG_CPU for SMP.
    
    Fixes: 2e1a3483ce74 ("cpu/hotplug: Split out the state walk into functions")
    Reported-by: Tianyu Lan <Tianyu.Lan@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Tianyu Lan <Tianyu.Lan@microsoft.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Konrad Wilk <konrad.wilk@oracle.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Mukesh Ojha <mojha@codeaurora.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Micheal Kelley <michael.h.kelley@microsoft.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: K. Y. Srinivasan <kys@microsoft.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190326163811.503390616@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 336f6b23b5b842df367b2e78879a7605dd5c180c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 26 22:51:02 2019 +0100

    watchdog: Respect watchdog cpumask on CPU hotplug
    
    commit 7dd47617114921fdd8c095509e5e7b4373cc44a1 upstream.
    
    The rework of the watchdog core to use cpu_stop_work broke the watchdog
    cpumask on CPU hotplug.
    
    The watchdog_enable/disable() functions are now called unconditionally from
    the hotplug callback, i.e. even on CPUs which are not in the watchdog
    cpumask. As a consequence the watchdog can become unstoppable.
    
    Only invoke them when the plugged CPU is in the watchdog cpumask.
    
    Fixes: 9cf57731b63e ("watchdog/softlockup: Replace "watchdog/%u" threads with cpu_stop_work")
    Reported-by: Maxime Coquelin <maxime.coquelin@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Maxime Coquelin <maxime.coquelin@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1903262245490.1789@nanos.tec.linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5f6b5b8b609bbe3d40b95aa611f66ab967fb2011
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 26 17:36:05 2019 +0100

    cpu/hotplug: Prevent crash when CPU bringup fails on CONFIG_HOTPLUG_CPU=n
    
    commit 206b92353c839c0b27a0b9bec24195f93fd6cf7a upstream.
    
    Tianyu reported a crash in a CPU hotplug teardown callback when booting a
    kernel which has CONFIG_HOTPLUG_CPU disabled with the 'nosmt' boot
    parameter.
    
    It turns out that the SMP=y CONFIG_HOTPLUG_CPU=n case has been broken
    forever in case that a bringup callback fails. Unfortunately this issue was
    not recognized when the CPU hotplug code was reworked, so the shortcoming
    just stayed in place.
    
    When a bringup callback fails, the CPU hotplug code rolls back the
    operation and takes the CPU offline.
    
    The 'nosmt' command line argument uses a bringup failure to abort the
    bringup of SMT sibling CPUs. This partial bringup is required due to the
    MCE misdesign on Intel CPUs.
    
    With CONFIG_HOTPLUG_CPU=y the rollback works perfectly fine, but
    CONFIG_HOTPLUG_CPU=n lacks essential mechanisms to exercise the low level
    teardown of a CPU including the synchronizations in various facilities like
    RCU, NOHZ and others.
    
    As a consequence the teardown callbacks which must be executed on the
    outgoing CPU within stop machine with interrupts disabled are executed on
    the control CPU in interrupt enabled and preemptible context causing the
    kernel to crash and burn. The pre state machine code has a different
    failure mode which is more subtle and resulting in a less obvious use after
    free crash because the control side frees resources which are still in use
    by the undead CPU.
    
    But this is not a x86 only problem. Any architecture which supports the
    SMP=y HOTPLUG_CPU=n combination suffers from the same issue. It's just less
    likely to be triggered because in 99.99999% of the cases all bringup
    callbacks succeed.
    
    The easy solution of making HOTPLUG_CPU mandatory for SMP is not working on
    all architectures as the following architectures have either no hotplug
    support at all or not all subarchitectures support it:
    
     alpha, arc, hexagon, openrisc, riscv, sparc (32bit), mips (partial).
    
    Crashing the kernel in such a situation is not an acceptable state
    either.
    
    Implement a minimal rollback variant by limiting the teardown to the point
    where all regular teardown callbacks have been invoked and leave the CPU in
    the 'dead' idle state. This has the following consequences:
    
     - the CPU is brought down to the point where the stop_machine takedown
       would happen.
    
     - the CPU stays there forever and is idle
    
     - The CPU is cleared in the CPU active mask, but not in the CPU online
       mask which is a legit state.
    
     - Interrupts are not forced away from the CPU
    
     - All facilities which only look at online mask would still see it, but
       that is the case during normal hotplug/unplug operations as well. It's
       just a (way) longer time frame.
    
    This will expose issues, which haven't been exposed before or only seldom,
    because now the normally transient state of being non active but online is
    a permanent state. In testing this exposed already an issue vs. work queues
    where the vmstat code schedules work on the almost dead CPU which ends up
    in an unbound workqueue and triggers 'preemtible context' warnings. This is
    not a problem of this change, it merily exposes an already existing issue.
    Still this is better than crashing fully without a chance to debug it.
    
    This is mainly thought as workaround for those architectures which do not
    support HOTPLUG_CPU. All others should enforce HOTPLUG_CPU for SMP.
    
    Fixes: 2e1a3483ce74 ("cpu/hotplug: Split out the state walk into functions")
    Reported-by: Tianyu Lan <Tianyu.Lan@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Tianyu Lan <Tianyu.Lan@microsoft.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Konrad Wilk <konrad.wilk@oracle.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Mukesh Ojha <mojha@codeaurora.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Micheal Kelley <michael.h.kelley@microsoft.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: K. Y. Srinivasan <kys@microsoft.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190326163811.503390616@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ce4fbb9f4ee472a7b50431b134a0bab26fca922e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 26 17:36:05 2019 +0100

    cpu/hotplug: Prevent crash when CPU bringup fails on CONFIG_HOTPLUG_CPU=n
    
    commit 206b92353c839c0b27a0b9bec24195f93fd6cf7a upstream.
    
    Tianyu reported a crash in a CPU hotplug teardown callback when booting a
    kernel which has CONFIG_HOTPLUG_CPU disabled with the 'nosmt' boot
    parameter.
    
    It turns out that the SMP=y CONFIG_HOTPLUG_CPU=n case has been broken
    forever in case that a bringup callback fails. Unfortunately this issue was
    not recognized when the CPU hotplug code was reworked, so the shortcoming
    just stayed in place.
    
    When a bringup callback fails, the CPU hotplug code rolls back the
    operation and takes the CPU offline.
    
    The 'nosmt' command line argument uses a bringup failure to abort the
    bringup of SMT sibling CPUs. This partial bringup is required due to the
    MCE misdesign on Intel CPUs.
    
    With CONFIG_HOTPLUG_CPU=y the rollback works perfectly fine, but
    CONFIG_HOTPLUG_CPU=n lacks essential mechanisms to exercise the low level
    teardown of a CPU including the synchronizations in various facilities like
    RCU, NOHZ and others.
    
    As a consequence the teardown callbacks which must be executed on the
    outgoing CPU within stop machine with interrupts disabled are executed on
    the control CPU in interrupt enabled and preemptible context causing the
    kernel to crash and burn. The pre state machine code has a different
    failure mode which is more subtle and resulting in a less obvious use after
    free crash because the control side frees resources which are still in use
    by the undead CPU.
    
    But this is not a x86 only problem. Any architecture which supports the
    SMP=y HOTPLUG_CPU=n combination suffers from the same issue. It's just less
    likely to be triggered because in 99.99999% of the cases all bringup
    callbacks succeed.
    
    The easy solution of making HOTPLUG_CPU mandatory for SMP is not working on
    all architectures as the following architectures have either no hotplug
    support at all or not all subarchitectures support it:
    
     alpha, arc, hexagon, openrisc, riscv, sparc (32bit), mips (partial).
    
    Crashing the kernel in such a situation is not an acceptable state
    either.
    
    Implement a minimal rollback variant by limiting the teardown to the point
    where all regular teardown callbacks have been invoked and leave the CPU in
    the 'dead' idle state. This has the following consequences:
    
     - the CPU is brought down to the point where the stop_machine takedown
       would happen.
    
     - the CPU stays there forever and is idle
    
     - The CPU is cleared in the CPU active mask, but not in the CPU online
       mask which is a legit state.
    
     - Interrupts are not forced away from the CPU
    
     - All facilities which only look at online mask would still see it, but
       that is the case during normal hotplug/unplug operations as well. It's
       just a (way) longer time frame.
    
    This will expose issues, which haven't been exposed before or only seldom,
    because now the normally transient state of being non active but online is
    a permanent state. In testing this exposed already an issue vs. work queues
    where the vmstat code schedules work on the almost dead CPU which ends up
    in an unbound workqueue and triggers 'preemtible context' warnings. This is
    not a problem of this change, it merily exposes an already existing issue.
    Still this is better than crashing fully without a chance to debug it.
    
    This is mainly thought as workaround for those architectures which do not
    support HOTPLUG_CPU. All others should enforce HOTPLUG_CPU for SMP.
    
    Fixes: 2e1a3483ce74 ("cpu/hotplug: Split out the state walk into functions")
    Reported-by: Tianyu Lan <Tianyu.Lan@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Tianyu Lan <Tianyu.Lan@microsoft.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Konrad Wilk <konrad.wilk@oracle.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Mukesh Ojha <mojha@codeaurora.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Micheal Kelley <michael.h.kelley@microsoft.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: K. Y. Srinivasan <kys@microsoft.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190326163811.503390616@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c29d85417c5f9a0a970ebd2571b65f0d52f110f5
Merge: 573efdc5ea95 bebd024e4815
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 31 08:22:12 2019 -0700

    Merge branch 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug fixes from Thomas Gleixner:
     "Two SMT/hotplug related fixes:
    
       - Prevent crash when HOTPLUG_CPU is disabled and the CPU bringup
         aborts. This is triggered with the 'nosmt' command line option, but
         can happen by any abort condition. As the real unplug code is not
         compiled in, prevent the fail by keeping the CPU in zombie state.
    
       - Enforce HOTPLUG_CPU for SMP on x86 to avoid the above situation
         completely. With 'nosmt' being a popular option it's required to
         unplug the half brought up sibling CPUs (due to the MCE wreckage)
         completely"
    
    * 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/smp: Enforce CONFIG_HOTPLUG_CPU when SMP=y
      cpu/hotplug: Prevent crash when CPU bringup fails on CONFIG_HOTPLUG_CPU=n

commit f78b5be2a5d08709177963df17b8df42e690a652
Merge: 6536c5f2c8cf 7dd476171149
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 31 07:47:21 2019 -0700

    Merge branch 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core fixes from Thomas Gleixner:
     "A small set of core updates:
    
       - Make the watchdog respect the selected CPU mask again. That was
         broken by the rework of the watchdog thread management and caused
         inconsistent state and NMI watchdog being unstoppable.
    
       - Ensure that the objtool build can find the libelf location.
    
       - Remove dead kcore stub code"
    
    * 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      watchdog: Respect watchdog cpumask on CPU hotplug
      objtool: Query pkg-config for libelf location
      proc/kcore: Remove unused kclist_add_remap()

commit 206b92353c839c0b27a0b9bec24195f93fd6cf7a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 26 17:36:05 2019 +0100

    cpu/hotplug: Prevent crash when CPU bringup fails on CONFIG_HOTPLUG_CPU=n
    
    Tianyu reported a crash in a CPU hotplug teardown callback when booting a
    kernel which has CONFIG_HOTPLUG_CPU disabled with the 'nosmt' boot
    parameter.
    
    It turns out that the SMP=y CONFIG_HOTPLUG_CPU=n case has been broken
    forever in case that a bringup callback fails. Unfortunately this issue was
    not recognized when the CPU hotplug code was reworked, so the shortcoming
    just stayed in place.
    
    When a bringup callback fails, the CPU hotplug code rolls back the
    operation and takes the CPU offline.
    
    The 'nosmt' command line argument uses a bringup failure to abort the
    bringup of SMT sibling CPUs. This partial bringup is required due to the
    MCE misdesign on Intel CPUs.
    
    With CONFIG_HOTPLUG_CPU=y the rollback works perfectly fine, but
    CONFIG_HOTPLUG_CPU=n lacks essential mechanisms to exercise the low level
    teardown of a CPU including the synchronizations in various facilities like
    RCU, NOHZ and others.
    
    As a consequence the teardown callbacks which must be executed on the
    outgoing CPU within stop machine with interrupts disabled are executed on
    the control CPU in interrupt enabled and preemptible context causing the
    kernel to crash and burn. The pre state machine code has a different
    failure mode which is more subtle and resulting in a less obvious use after
    free crash because the control side frees resources which are still in use
    by the undead CPU.
    
    But this is not a x86 only problem. Any architecture which supports the
    SMP=y HOTPLUG_CPU=n combination suffers from the same issue. It's just less
    likely to be triggered because in 99.99999% of the cases all bringup
    callbacks succeed.
    
    The easy solution of making HOTPLUG_CPU mandatory for SMP is not working on
    all architectures as the following architectures have either no hotplug
    support at all or not all subarchitectures support it:
    
     alpha, arc, hexagon, openrisc, riscv, sparc (32bit), mips (partial).
    
    Crashing the kernel in such a situation is not an acceptable state
    either.
    
    Implement a minimal rollback variant by limiting the teardown to the point
    where all regular teardown callbacks have been invoked and leave the CPU in
    the 'dead' idle state. This has the following consequences:
    
     - the CPU is brought down to the point where the stop_machine takedown
       would happen.
    
     - the CPU stays there forever and is idle
    
     - The CPU is cleared in the CPU active mask, but not in the CPU online
       mask which is a legit state.
    
     - Interrupts are not forced away from the CPU
    
     - All facilities which only look at online mask would still see it, but
       that is the case during normal hotplug/unplug operations as well. It's
       just a (way) longer time frame.
    
    This will expose issues, which haven't been exposed before or only seldom,
    because now the normally transient state of being non active but online is
    a permanent state. In testing this exposed already an issue vs. work queues
    where the vmstat code schedules work on the almost dead CPU which ends up
    in an unbound workqueue and triggers 'preemtible context' warnings. This is
    not a problem of this change, it merily exposes an already existing issue.
    Still this is better than crashing fully without a chance to debug it.
    
    This is mainly thought as workaround for those architectures which do not
    support HOTPLUG_CPU. All others should enforce HOTPLUG_CPU for SMP.
    
    Fixes: 2e1a3483ce74 ("cpu/hotplug: Split out the state walk into functions")
    Reported-by: Tianyu Lan <Tianyu.Lan@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Tianyu Lan <Tianyu.Lan@microsoft.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Konrad Wilk <konrad.wilk@oracle.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Mukesh Ojha <mojha@codeaurora.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Micheal Kelley <michael.h.kelley@microsoft.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: K. Y. Srinivasan <kys@microsoft.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190326163811.503390616@linutronix.de

commit 7dd47617114921fdd8c095509e5e7b4373cc44a1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 26 22:51:02 2019 +0100

    watchdog: Respect watchdog cpumask on CPU hotplug
    
    The rework of the watchdog core to use cpu_stop_work broke the watchdog
    cpumask on CPU hotplug.
    
    The watchdog_enable/disable() functions are now called unconditionally from
    the hotplug callback, i.e. even on CPUs which are not in the watchdog
    cpumask. As a consequence the watchdog can become unstoppable.
    
    Only invoke them when the plugged CPU is in the watchdog cpumask.
    
    Fixes: 9cf57731b63e ("watchdog/softlockup: Replace "watchdog/%u" threads with cpu_stop_work")
    Reported-by: Maxime Coquelin <maxime.coquelin@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Maxime Coquelin <maxime.coquelin@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Ricardo Neri <ricardo.neri-calderon@linux.intel.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1903262245490.1789@nanos.tec.linutronix.de

commit 0be288630752e6358d02eba7b283c1783a5c7c38
Merge: e8a71a386689 4c2741ac5e10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 15 14:37:46 2019 -0700

    Merge tag 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm
    
    Pull ARM updates from Russell King:
    
     - An improvement from Ard Biesheuvel, who noted that the identity map
       setup was taking a long time due to flush_cache_louis().
    
     - Update a comment about dma_ops from Wolfram Sang.
    
     - Remove use of "-p" with ld, where this flag has been a no-op since
       2004.
    
     - Remove the printing of the virtual memory layout, which is no longer
       useful since we hide pointers.
    
     - Correct SCU help text.
    
     - Remove legacy TWD registration method.
    
     - Add pgprot_device() implementation for mapping PCI sysfs resource
       files.
    
     - Initialise PFN limits earlier for kmemleak.
    
     - Fix argument count to match macro definition (affects clang builds)
    
     - Use unified assembler language almost everywhere for clang, and other
       clang improvements (from Stefan Agner, Nathan Chancellor).
    
     - Support security extension for noMMU and other noMMU cleanups (from
       Vladimir Murzin).
    
     - Remove unnecessary SMP bringup code (which was incorrectly copy'n'
       pasted from the ARM platform implementations) and remove it from the
       arch code to discourge further copys of it appearing.
    
     - Add Cortex A9 erratum preventing kexec working on some SoCs.
    
     - AMBA bus identification updates from Mike Leach.
    
     - More use of raw spinlocks to avoid -RT kernel issues (from Yang Shi
       and Sebastian Andrzej Siewior).
    
     - MCPM hyp/svc mode mismatch fixes from Marek Szyprowski.
    
    * tag 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm: (32 commits)
      ARM: 8849/1: NOMMU: Fix encodings for PMSAv8's PRBAR4/PRLAR4
      ARM: 8848/1: virt: Align GIC version check with arm64 counterpart
      ARM: 8847/1: pm: fix HYP/SVC mode mismatch when MCPM is used
      ARM: 8845/1: use unified assembler in c files
      ARM: 8844/1: use unified assembler in assembly files
      ARM: 8843/1: use unified assembler in headers
      ARM: 8841/1: use unified assembler in macros
      ARM: 8840/1: use a raw_spinlock_t in unwind
      ARM: 8839/1: kprobe: make patch_lock a raw_spinlock_t
      ARM: 8837/1: coresight: etmv4: Update ID register table to add UCI support
      ARM: 8836/1: drivers: amba: Update component matching to use the CoreSight UCI values.
      ARM: 8838/1: drivers: amba: Updates to component identification for driver matching.
      ARM: 8833/1: Ensure that NEON code always compiles with Clang
      ARM: avoid Cortex-A9 livelock on tight dmb loops
      ARM: smp: remove arch-provided "pen_release"
      ARM: actions: remove boot_lock and pen_release
      ARM: oxnas: remove CPU hotplug implementation
      ARM: qcom: remove unnecessary boot_lock
      ARM: 8832/1: NOMMU: Limit visibility for CONFIG_FLASH_{MEM_BASE,SIZE}
      ARM: 8831/1: NOMMU: pmsa-v8: remove unneeded semicolon
      ...

commit 201676095dda7e5b31a5e1d116d10fc22985075e
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Thu Mar 7 08:41:22 2019 +0300

    xen, cpu_hotplug: Prevent an out of bounds access
    
    The "cpu" variable comes from the sscanf() so Smatch marks it as
    untrusted data.  We can't pass a higher value than "nr_cpu_ids" to
    cpu_possible() or it results in an out of bounds access.
    
    Fixes: d68d82afd4c8 ("xen: implement CPU hotplugging")
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

commit e7a8dabe92f1c59bb5f992e4d40bb88dbedd9baf
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Wed Feb 13 16:32:22 2019 -0500

    ARM: ensure that processor vtables is not lost after boot
    
    Commit 3a4d0c2172bcf15b7a3d9d498b2b355f9864286b upstream.
    
    Marek Szyprowski reported problems with CPU hotplug in current kernels.
    This was tracked down to the processor vtables being located in an
    init section, and therefore discarded after kernel boot, despite being
    required after boot to properly initialise the non-boot CPUs.
    
    Arrange for these tables to end up in .rodata when required.
    
    Reported-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Krzysztof Kozlowski <krzk@kernel.org>
    Fixes: 383fb3ee8024 ("ARM: spectre-v2: per-CPU vtables to work around big.Little systems")
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: David A. Long <dave.long@linaro.org>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit a7fb58b2c8a16c7e9435255bd95d4bf218542145
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Wed Feb 13 21:10:21 2019 -0500

    ARM: ensure that processor vtables is not lost after boot
    
    Commit 3a4d0c2172bcf15b7a3d9d498b2b355f9864286b upstream.
    
    Marek Szyprowski reported problems with CPU hotplug in current kernels.
    This was tracked down to the processor vtables being located in an
    init section, and therefore discarded after kernel boot, despite being
    required after boot to properly initialise the non-boot CPUs.
    
    Arrange for these tables to end up in .rodata when required.
    
    Reported-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Krzysztof Kozlowski <krzk@kernel.org>
    Fixes: 383fb3ee8024 ("ARM: spectre-v2: per-CPU vtables to work around big.Little systems")
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: David A. Long <dave.long@linaro.org>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 2307923f865c500b3b05ff131f00a9388bd442dd
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Thu Feb 14 09:49:29 2019 -0500

    ARM: ensure that processor vtables is not lost after boot
    
    Commit 3a4d0c2172bcf15b7a3d9d498b2b355f9864286b upstream.
    
    Marek Szyprowski reported problems with CPU hotplug in current kernels.
    This was tracked down to the processor vtables being located in an
    init section, and therefore discarded after kernel boot, despite being
    required after boot to properly initialise the non-boot CPUs.
    
    Arrange for these tables to end up in .rodata when required.
    
    Reported-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Krzysztof Kozlowski <krzk@kernel.org>
    Fixes: 383fb3ee8024 ("ARM: spectre-v2: per-CPU vtables to work around big.Little systems")
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: David A. Long <dave.long@linaro.org>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 5cbde46eb9c89c5fa02a5927103c4e738c5d08b9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Dec 19 17:53:50 2018 +0100

    perf/x86/intel: Delay memory deallocation until x86_pmu_dead_cpu()
    
    commit 602cae04c4864bb3487dfe4c2126c8d9e7e1614a upstream.
    
    intel_pmu_cpu_prepare() allocated memory for ->shared_regs among other
    members of struct cpu_hw_events. This memory is released in
    intel_pmu_cpu_dying() which is wrong. The counterpart of the
    intel_pmu_cpu_prepare() callback is x86_pmu_dead_cpu().
    
    Otherwise if the CPU fails on the UP path between CPUHP_PERF_X86_PREPARE
    and CPUHP_AP_PERF_X86_STARTING then it won't release the memory but
    allocate new memory on the next attempt to online the CPU (leaking the
    old memory).
    Also, if the CPU down path fails between CPUHP_AP_PERF_X86_STARTING and
    CPUHP_PERF_X86_PREPARE then the CPU will go back online but never
    allocate the memory that was released in x86_pmu_dying_cpu().
    
    Make the memory allocation/free symmetrical in regard to the CPU hotplug
    notifier by moving the deallocation to intel_pmu_cpu_dead().
    
    This started in commit:
    
       a7e3ed1e47011 ("perf: Add support for supplementary event registers").
    
    In principle the bug was introduced in v2.6.39 (!), but it will almost
    certainly not backport cleanly across the big CPU hotplug rewrite between v4.7-v4.15...
    
    [ bigeasy: Added patch description. ]
    [ mingo: Added backporting guidance. ]
    
    Reported-by: He Zhe <zhe.he@windriver.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org> # With developer hat on
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org> # With maintainer hat on
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@kernel.org
    Cc: bp@alien8.de
    Cc: hpa@zytor.com
    Cc: jolsa@kernel.org
    Cc: kan.liang@linux.intel.com
    Cc: namhyung@kernel.org
    Cc: <stable@vger.kernel.org>
    Fixes: a7e3ed1e47011 ("perf: Add support for supplementary event registers").
    Link: https://lkml.kernel.org/r/20181219165350.6s3jvyxbibpvlhtq@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6dd7618d7dead7b4f369d3e24e80232b8eb676c3
Author: Long Li <longli@microsoft.com>
Date:   Fri Nov 2 18:02:48 2018 +0000

    genirq/affinity: Spread IRQs to all available NUMA nodes
    
    [ Upstream commit b82592199032bf7c778f861b936287e37ebc9f62 ]
    
    If the number of NUMA nodes exceeds the number of MSI/MSI-X interrupts
    which are allocated for a device, the interrupt affinity spreading code
    fails to spread them across all nodes.
    
    The reason is, that the spreading code starts from node 0 and continues up
    to the number of interrupts requested for allocation. This leaves the nodes
    past the last interrupt unused.
    
    This results in interrupt concentration on the first nodes which violates
    the assumption of the block layer that all nodes are covered evenly. As a
    consequence the NUMA nodes above the number of interrupts are all assigned
    to hardware queue 0 and therefore NUMA node 0, which results in bad
    performance and has CPU hotplug implications, because queue 0 gets shut
    down when the last CPU of node 0 is offlined.
    
    Go over all NUMA nodes and assign them round-robin to all requested
    interrupts to solve this.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Long Li <longli@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Link: https://lkml.kernel.org/r/20181102180248.13583-1-longli@linuxonhyperv.com
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit 8b71aa1a3bb85562561baf5651ebb88def3e2525
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Dec 19 17:53:50 2018 +0100

    perf/x86/intel: Delay memory deallocation until x86_pmu_dead_cpu()
    
    commit 602cae04c4864bb3487dfe4c2126c8d9e7e1614a upstream.
    
    intel_pmu_cpu_prepare() allocated memory for ->shared_regs among other
    members of struct cpu_hw_events. This memory is released in
    intel_pmu_cpu_dying() which is wrong. The counterpart of the
    intel_pmu_cpu_prepare() callback is x86_pmu_dead_cpu().
    
    Otherwise if the CPU fails on the UP path between CPUHP_PERF_X86_PREPARE
    and CPUHP_AP_PERF_X86_STARTING then it won't release the memory but
    allocate new memory on the next attempt to online the CPU (leaking the
    old memory).
    Also, if the CPU down path fails between CPUHP_AP_PERF_X86_STARTING and
    CPUHP_PERF_X86_PREPARE then the CPU will go back online but never
    allocate the memory that was released in x86_pmu_dying_cpu().
    
    Make the memory allocation/free symmetrical in regard to the CPU hotplug
    notifier by moving the deallocation to intel_pmu_cpu_dead().
    
    This started in commit:
    
       a7e3ed1e47011 ("perf: Add support for supplementary event registers").
    
    In principle the bug was introduced in v2.6.39 (!), but it will almost
    certainly not backport cleanly across the big CPU hotplug rewrite between v4.7-v4.15...
    
    [ bigeasy: Added patch description. ]
    [ mingo: Added backporting guidance. ]
    
    Reported-by: He Zhe <zhe.he@windriver.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org> # With developer hat on
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org> # With maintainer hat on
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@kernel.org
    Cc: bp@alien8.de
    Cc: hpa@zytor.com
    Cc: jolsa@kernel.org
    Cc: kan.liang@linux.intel.com
    Cc: namhyung@kernel.org
    Cc: <stable@vger.kernel.org>
    Fixes: a7e3ed1e47011 ("perf: Add support for supplementary event registers").
    Link: https://lkml.kernel.org/r/20181219165350.6s3jvyxbibpvlhtq@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    [ He Zhe: Fixes conflict caused by missing disable_counter_freeze which is
     introduced since v4.20 af3bdb991a5cb. ]
    Signed-off-by: He Zhe <zhe.he@windriver.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 46ed4f4fa1cf98b3da433f76ca4c7ac33f45d423
Author: Long Li <longli@microsoft.com>
Date:   Fri Nov 2 18:02:48 2018 +0000

    genirq/affinity: Spread IRQs to all available NUMA nodes
    
    [ Upstream commit b82592199032bf7c778f861b936287e37ebc9f62 ]
    
    If the number of NUMA nodes exceeds the number of MSI/MSI-X interrupts
    which are allocated for a device, the interrupt affinity spreading code
    fails to spread them across all nodes.
    
    The reason is, that the spreading code starts from node 0 and continues up
    to the number of interrupts requested for allocation. This leaves the nodes
    past the last interrupt unused.
    
    This results in interrupt concentration on the first nodes which violates
    the assumption of the block layer that all nodes are covered evenly. As a
    consequence the NUMA nodes above the number of interrupts are all assigned
    to hardware queue 0 and therefore NUMA node 0, which results in bad
    performance and has CPU hotplug implications, because queue 0 gets shut
    down when the last CPU of node 0 is offlined.
    
    Go over all NUMA nodes and assign them round-robin to all requested
    interrupts to solve this.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Long Li <longli@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Link: https://lkml.kernel.org/r/20181102180248.13583-1-longli@linuxonhyperv.com
    Signed-off-by: Sasha Levin <sashal@kernel.org>

commit db85eb4162da282b1c25b1d62ed01a434bfdd6d0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Dec 19 17:53:50 2018 +0100

    perf/x86/intel: Delay memory deallocation until x86_pmu_dead_cpu()
    
    commit 602cae04c4864bb3487dfe4c2126c8d9e7e1614a upstream.
    
    intel_pmu_cpu_prepare() allocated memory for ->shared_regs among other
    members of struct cpu_hw_events. This memory is released in
    intel_pmu_cpu_dying() which is wrong. The counterpart of the
    intel_pmu_cpu_prepare() callback is x86_pmu_dead_cpu().
    
    Otherwise if the CPU fails on the UP path between CPUHP_PERF_X86_PREPARE
    and CPUHP_AP_PERF_X86_STARTING then it won't release the memory but
    allocate new memory on the next attempt to online the CPU (leaking the
    old memory).
    Also, if the CPU down path fails between CPUHP_AP_PERF_X86_STARTING and
    CPUHP_PERF_X86_PREPARE then the CPU will go back online but never
    allocate the memory that was released in x86_pmu_dying_cpu().
    
    Make the memory allocation/free symmetrical in regard to the CPU hotplug
    notifier by moving the deallocation to intel_pmu_cpu_dead().
    
    This started in commit:
    
       a7e3ed1e47011 ("perf: Add support for supplementary event registers").
    
    In principle the bug was introduced in v2.6.39 (!), but it will almost
    certainly not backport cleanly across the big CPU hotplug rewrite between v4.7-v4.15...
    
    [ bigeasy: Added patch description. ]
    [ mingo: Added backporting guidance. ]
    
    Reported-by: He Zhe <zhe.he@windriver.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org> # With developer hat on
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org> # With maintainer hat on
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@kernel.org
    Cc: bp@alien8.de
    Cc: hpa@zytor.com
    Cc: jolsa@kernel.org
    Cc: kan.liang@linux.intel.com
    Cc: namhyung@kernel.org
    Cc: <stable@vger.kernel.org>
    Fixes: a7e3ed1e47011 ("perf: Add support for supplementary event registers").
    Link: https://lkml.kernel.org/r/20181219165350.6s3jvyxbibpvlhtq@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    [ He Zhe: Fixes conflict caused by missing disable_counter_freeze which is
     introduced since v4.20 af3bdb991a5cb. ]
    Signed-off-by: He Zhe <zhe.he@windriver.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit bd10eb88e3d0ad4d140b397f1509bc37bbaa2c27
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Dec 19 17:53:50 2018 +0100

    perf/x86/intel: Delay memory deallocation until x86_pmu_dead_cpu()
    
    commit 602cae04c4864bb3487dfe4c2126c8d9e7e1614a upstream.
    
    intel_pmu_cpu_prepare() allocated memory for ->shared_regs among other
    members of struct cpu_hw_events. This memory is released in
    intel_pmu_cpu_dying() which is wrong. The counterpart of the
    intel_pmu_cpu_prepare() callback is x86_pmu_dead_cpu().
    
    Otherwise if the CPU fails on the UP path between CPUHP_PERF_X86_PREPARE
    and CPUHP_AP_PERF_X86_STARTING then it won't release the memory but
    allocate new memory on the next attempt to online the CPU (leaking the
    old memory).
    Also, if the CPU down path fails between CPUHP_AP_PERF_X86_STARTING and
    CPUHP_PERF_X86_PREPARE then the CPU will go back online but never
    allocate the memory that was released in x86_pmu_dying_cpu().
    
    Make the memory allocation/free symmetrical in regard to the CPU hotplug
    notifier by moving the deallocation to intel_pmu_cpu_dead().
    
    This started in commit:
    
       a7e3ed1e47011 ("perf: Add support for supplementary event registers").
    
    In principle the bug was introduced in v2.6.39 (!), but it will almost
    certainly not backport cleanly across the big CPU hotplug rewrite between v4.7-v4.15...
    
    [ bigeasy: Added patch description. ]
    [ mingo: Added backporting guidance. ]
    
    Reported-by: He Zhe <zhe.he@windriver.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org> # With developer hat on
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org> # With maintainer hat on
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@kernel.org
    Cc: bp@alien8.de
    Cc: hpa@zytor.com
    Cc: jolsa@kernel.org
    Cc: kan.liang@linux.intel.com
    Cc: namhyung@kernel.org
    Cc: <stable@vger.kernel.org>
    Fixes: a7e3ed1e47011 ("perf: Add support for supplementary event registers").
    Link: https://lkml.kernel.org/r/20181219165350.6s3jvyxbibpvlhtq@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    [ He Zhe: Fixes conflict caused by missing disable_counter_freeze which is
     introduced since v4.20 af3bdb991a5cb. ]
    Signed-off-by: He Zhe <zhe.he@windriver.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0121805d9d2b1fff371e195c28e9b86ae38b5e47
Author: Matthias Kaehlcke <mka@chromium.org>
Date:   Mon Jan 28 15:46:24 2019 -0800

    kthread: Add __kthread_should_park()
    
    kthread_should_park() is used to check if the calling kthread ('current')
    should park, but there is no function to check whether an arbitrary kthread
    should be parked. The latter is required to plug a CPU hotplug race vs. a
    parking ksoftirqd thread.
    
    The new __kthread_should_park() receives a task_struct as parameter to
    check if the corresponding kernel thread should be parked.
    
    Call __kthread_should_park() from kthread_should_park() to avoid code
    duplication.
    
    Signed-off-by: Matthias Kaehlcke <mka@chromium.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E . McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Douglas Anderson <dianders@chromium.org>
    Cc: Stephen Boyd <swboyd@chromium.org>
    Link: https://lkml.kernel.org/r/20190128234625.78241-2-mka@chromium.org

commit 212146f0800e151bd61b98fb6fe4b8b6778a649a
Merge: d2a6aae99f5f 3bb2600657da
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 10 09:48:18 2019 -0800

    Merge branch 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf fixes from Ingo Molnar:
     "A couple of kernel side fixes:
    
       - Fix the Intel uncore driver on certain hardware configurations
    
       - Fix a CPU hotplug related memory allocation bug
    
       - Remove a spurious WARN()
    
      ... plus also a handful of perf tooling fixes"
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf script python: Add Python3 support to tests/attr.py
      perf trace: Support multiple "vfs_getname" probes
      perf symbols: Filter out hidden symbols from labels
      perf symbols: Add fallback definitions for GELF_ST_VISIBILITY()
      tools headers uapi: Sync linux/in.h copy from the kernel sources
      perf clang: Do not use 'return std::move(something)'
      perf mem/c2c: Fix perf_mem_events to support powerpc
      perf tests evsel-tp-sched: Fix bitwise operator
      perf/core: Don't WARN() for impossible ring-buffer sizes
      perf/x86/intel: Delay memory deallocation until x86_pmu_dead_cpu()
      perf/x86/intel/uncore: Add Node ID mask

commit 86dd006cffecc263f8dc2cf2a787e0c53bb03b80
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Wed Jan 9 13:00:03 2019 +0100

    s390/smp: fix CPU hotplug deadlock with CPU rescan
    
    commit b7cb707c373094ce4008d4a6ac9b6b366ec52da5 upstream.
    
    smp_rescan_cpus() is called without the device_hotplug_lock, which can lead
    to a dedlock when a new CPU is found and immediately set online by a udev
    rule.
    
    This was observed on an older kernel version, where the cpu_hotplug_begin()
    loop was still present, and it resulted in hanging chcpu and systemd-udev
    processes. This specific deadlock will not show on current kernels. However,
    there may be other possible deadlocks, and since smp_rescan_cpus() can still
    trigger a CPU hotplug operation, the device_hotplug_lock should be held.
    
    For reference, this was the deadlock with the old cpu_hotplug_begin() loop:
    
            chcpu (rescan)                       systemd-udevd
    
     echo 1 > /sys/../rescan
     -> smp_rescan_cpus()
     -> (*) get_online_cpus()
        (increases refcount)
     -> smp_add_present_cpu()
        (new CPU found)
     -> register_cpu()
     -> device_add()
     -> udev "add" event triggered -----------> udev rule sets CPU online
                                             -> echo 1 > /sys/.../online
                                             -> lock_device_hotplug_sysfs()
                                                (this is missing in rescan path)
                                             -> device_online()
                                             -> (**) device_lock(new CPU dev)
                                             -> cpu_up()
                                             -> cpu_hotplug_begin()
                                                (loops until refcount == 0)
                                                -> deadlock with (*)
     -> bus_probe_device()
     -> device_attach()
     -> device_lock(new CPU dev)
        -> deadlock with (**)
    
    Fix this by taking the device_hotplug_lock in the CPU rescan path.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0c4373e61d68178be9f20cd465e48579b799922a
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Wed Jan 9 13:00:03 2019 +0100

    s390/smp: fix CPU hotplug deadlock with CPU rescan
    
    commit b7cb707c373094ce4008d4a6ac9b6b366ec52da5 upstream.
    
    smp_rescan_cpus() is called without the device_hotplug_lock, which can lead
    to a dedlock when a new CPU is found and immediately set online by a udev
    rule.
    
    This was observed on an older kernel version, where the cpu_hotplug_begin()
    loop was still present, and it resulted in hanging chcpu and systemd-udev
    processes. This specific deadlock will not show on current kernels. However,
    there may be other possible deadlocks, and since smp_rescan_cpus() can still
    trigger a CPU hotplug operation, the device_hotplug_lock should be held.
    
    For reference, this was the deadlock with the old cpu_hotplug_begin() loop:
    
            chcpu (rescan)                       systemd-udevd
    
     echo 1 > /sys/../rescan
     -> smp_rescan_cpus()
     -> (*) get_online_cpus()
        (increases refcount)
     -> smp_add_present_cpu()
        (new CPU found)
     -> register_cpu()
     -> device_add()
     -> udev "add" event triggered -----------> udev rule sets CPU online
                                             -> echo 1 > /sys/.../online
                                             -> lock_device_hotplug_sysfs()
                                                (this is missing in rescan path)
                                             -> device_online()
                                             -> (**) device_lock(new CPU dev)
                                             -> cpu_up()
                                             -> cpu_hotplug_begin()
                                                (loops until refcount == 0)
                                                -> deadlock with (*)
     -> bus_probe_device()
     -> device_attach()
     -> device_lock(new CPU dev)
        -> deadlock with (**)
    
    Fix this by taking the device_hotplug_lock in the CPU rescan path.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 602cae04c4864bb3487dfe4c2126c8d9e7e1614a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Dec 19 17:53:50 2018 +0100

    perf/x86/intel: Delay memory deallocation until x86_pmu_dead_cpu()
    
    intel_pmu_cpu_prepare() allocated memory for ->shared_regs among other
    members of struct cpu_hw_events. This memory is released in
    intel_pmu_cpu_dying() which is wrong. The counterpart of the
    intel_pmu_cpu_prepare() callback is x86_pmu_dead_cpu().
    
    Otherwise if the CPU fails on the UP path between CPUHP_PERF_X86_PREPARE
    and CPUHP_AP_PERF_X86_STARTING then it won't release the memory but
    allocate new memory on the next attempt to online the CPU (leaking the
    old memory).
    Also, if the CPU down path fails between CPUHP_AP_PERF_X86_STARTING and
    CPUHP_PERF_X86_PREPARE then the CPU will go back online but never
    allocate the memory that was released in x86_pmu_dying_cpu().
    
    Make the memory allocation/free symmetrical in regard to the CPU hotplug
    notifier by moving the deallocation to intel_pmu_cpu_dead().
    
    This started in commit:
    
       a7e3ed1e47011 ("perf: Add support for supplementary event registers").
    
    In principle the bug was introduced in v2.6.39 (!), but it will almost
    certainly not backport cleanly across the big CPU hotplug rewrite between v4.7-v4.15...
    
    [ bigeasy: Added patch description. ]
    [ mingo: Added backporting guidance. ]
    
    Reported-by: He Zhe <zhe.he@windriver.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org> # With developer hat on
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org> # With maintainer hat on
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@kernel.org
    Cc: bp@alien8.de
    Cc: hpa@zytor.com
    Cc: jolsa@kernel.org
    Cc: kan.liang@linux.intel.com
    Cc: namhyung@kernel.org
    Cc: <stable@vger.kernel.org>
    Fixes: a7e3ed1e47011 ("perf: Add support for supplementary event registers").
    Link: https://lkml.kernel.org/r/20181219165350.6s3jvyxbibpvlhtq@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 0eb037998afe5514c8534276c152da31d2fabf07
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Thu Dec 13 12:54:26 2018 +0000

    ARM: oxnas: remove CPU hotplug implementation
    
    The CPU hotplug implementation on this platform is cargo-culted from
    the plat-versatile implementation, and is buggy.  Once a CPU hits the
    "low power" loop, it will wait for pen_release to be set to the CPU
    number to wake up again - but nothing in this implementation does that.
    
    So, once a CPU has entered cpu_die() it will never, ever leave.
    
    Remove this useless cargo-culted implementation.
    
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

commit 90814f0a3e4cd27e7dc765a768e38789f9d367fe
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Wed Jan 9 13:00:03 2019 +0100

    s390/smp: fix CPU hotplug deadlock with CPU rescan
    
    commit b7cb707c373094ce4008d4a6ac9b6b366ec52da5 upstream.
    
    smp_rescan_cpus() is called without the device_hotplug_lock, which can lead
    to a dedlock when a new CPU is found and immediately set online by a udev
    rule.
    
    This was observed on an older kernel version, where the cpu_hotplug_begin()
    loop was still present, and it resulted in hanging chcpu and systemd-udev
    processes. This specific deadlock will not show on current kernels. However,
    there may be other possible deadlocks, and since smp_rescan_cpus() can still
    trigger a CPU hotplug operation, the device_hotplug_lock should be held.
    
    For reference, this was the deadlock with the old cpu_hotplug_begin() loop:
    
            chcpu (rescan)                       systemd-udevd
    
     echo 1 > /sys/../rescan
     -> smp_rescan_cpus()
     -> (*) get_online_cpus()
        (increases refcount)
     -> smp_add_present_cpu()
        (new CPU found)
     -> register_cpu()
     -> device_add()
     -> udev "add" event triggered -----------> udev rule sets CPU online
                                             -> echo 1 > /sys/.../online
                                             -> lock_device_hotplug_sysfs()
                                                (this is missing in rescan path)
                                             -> device_online()
                                             -> (**) device_lock(new CPU dev)
                                             -> cpu_up()
                                             -> cpu_hotplug_begin()
                                                (loops until refcount == 0)
                                                -> deadlock with (*)
     -> bus_probe_device()
     -> device_attach()
     -> device_lock(new CPU dev)
        -> deadlock with (**)
    
    Fix this by taking the device_hotplug_lock in the CPU rescan path.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 049c7b068dd15fedfb863bcfbde0241acbbb808f
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Wed Jan 9 13:00:03 2019 +0100

    s390/smp: fix CPU hotplug deadlock with CPU rescan
    
    commit b7cb707c373094ce4008d4a6ac9b6b366ec52da5 upstream.
    
    smp_rescan_cpus() is called without the device_hotplug_lock, which can lead
    to a dedlock when a new CPU is found and immediately set online by a udev
    rule.
    
    This was observed on an older kernel version, where the cpu_hotplug_begin()
    loop was still present, and it resulted in hanging chcpu and systemd-udev
    processes. This specific deadlock will not show on current kernels. However,
    there may be other possible deadlocks, and since smp_rescan_cpus() can still
    trigger a CPU hotplug operation, the device_hotplug_lock should be held.
    
    For reference, this was the deadlock with the old cpu_hotplug_begin() loop:
    
            chcpu (rescan)                       systemd-udevd
    
     echo 1 > /sys/../rescan
     -> smp_rescan_cpus()
     -> (*) get_online_cpus()
        (increases refcount)
     -> smp_add_present_cpu()
        (new CPU found)
     -> register_cpu()
     -> device_add()
     -> udev "add" event triggered -----------> udev rule sets CPU online
                                             -> echo 1 > /sys/.../online
                                             -> lock_device_hotplug_sysfs()
                                                (this is missing in rescan path)
                                             -> device_online()
                                             -> (**) device_lock(new CPU dev)
                                             -> cpu_up()
                                             -> cpu_hotplug_begin()
                                                (loops until refcount == 0)
                                                -> deadlock with (*)
     -> bus_probe_device()
     -> device_attach()
     -> device_lock(new CPU dev)
        -> deadlock with (**)
    
    Fix this by taking the device_hotplug_lock in the CPU rescan path.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 826ea4c10833b734b9a403b2bd1d857a58cf8fb3
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Wed Jan 9 13:00:03 2019 +0100

    s390/smp: fix CPU hotplug deadlock with CPU rescan
    
    commit b7cb707c373094ce4008d4a6ac9b6b366ec52da5 upstream.
    
    smp_rescan_cpus() is called without the device_hotplug_lock, which can lead
    to a dedlock when a new CPU is found and immediately set online by a udev
    rule.
    
    This was observed on an older kernel version, where the cpu_hotplug_begin()
    loop was still present, and it resulted in hanging chcpu and systemd-udev
    processes. This specific deadlock will not show on current kernels. However,
    there may be other possible deadlocks, and since smp_rescan_cpus() can still
    trigger a CPU hotplug operation, the device_hotplug_lock should be held.
    
    For reference, this was the deadlock with the old cpu_hotplug_begin() loop:
    
            chcpu (rescan)                       systemd-udevd
    
     echo 1 > /sys/../rescan
     -> smp_rescan_cpus()
     -> (*) get_online_cpus()
        (increases refcount)
     -> smp_add_present_cpu()
        (new CPU found)
     -> register_cpu()
     -> device_add()
     -> udev "add" event triggered -----------> udev rule sets CPU online
                                             -> echo 1 > /sys/.../online
                                             -> lock_device_hotplug_sysfs()
                                                (this is missing in rescan path)
                                             -> device_online()
                                             -> (**) device_lock(new CPU dev)
                                             -> cpu_up()
                                             -> cpu_hotplug_begin()
                                                (loops until refcount == 0)
                                                -> deadlock with (*)
     -> bus_probe_device()
     -> device_attach()
     -> device_lock(new CPU dev)
        -> deadlock with (**)
    
    Fix this by taking the device_hotplug_lock in the CPU rescan path.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7ed458fd0b7eda471bb1cf20e4579f75711d4026
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Wed Jan 9 13:00:03 2019 +0100

    s390/smp: fix CPU hotplug deadlock with CPU rescan
    
    commit b7cb707c373094ce4008d4a6ac9b6b366ec52da5 upstream.
    
    smp_rescan_cpus() is called without the device_hotplug_lock, which can lead
    to a dedlock when a new CPU is found and immediately set online by a udev
    rule.
    
    This was observed on an older kernel version, where the cpu_hotplug_begin()
    loop was still present, and it resulted in hanging chcpu and systemd-udev
    processes. This specific deadlock will not show on current kernels. However,
    there may be other possible deadlocks, and since smp_rescan_cpus() can still
    trigger a CPU hotplug operation, the device_hotplug_lock should be held.
    
    For reference, this was the deadlock with the old cpu_hotplug_begin() loop:
    
            chcpu (rescan)                       systemd-udevd
    
     echo 1 > /sys/../rescan
     -> smp_rescan_cpus()
     -> (*) get_online_cpus()
        (increases refcount)
     -> smp_add_present_cpu()
        (new CPU found)
     -> register_cpu()
     -> device_add()
     -> udev "add" event triggered -----------> udev rule sets CPU online
                                             -> echo 1 > /sys/.../online
                                             -> lock_device_hotplug_sysfs()
                                                (this is missing in rescan path)
                                             -> device_online()
                                             -> (**) device_lock(new CPU dev)
                                             -> cpu_up()
                                             -> cpu_hotplug_begin()
                                                (loops until refcount == 0)
                                                -> deadlock with (*)
     -> bus_probe_device()
     -> device_attach()
     -> device_lock(new CPU dev)
        -> deadlock with (**)
    
    Fix this by taking the device_hotplug_lock in the CPU rescan path.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 34d66caf251df91ff27b24a3a786810d29989eca
Author: Zhenzhong Duan <zhenzhong.duan@oracle.com>
Date:   Thu Jan 17 02:10:59 2019 -0800

    x86/speculation: Remove redundant arch_smt_update() invocation
    
    With commit a74cfffb03b7 ("x86/speculation: Rework SMT state change"),
    arch_smt_update() is invoked from each individual CPU hotplug function.
    
    Therefore the extra arch_smt_update() call in the sysfs SMT control is
    redundant.
    
    Fixes: a74cfffb03b7 ("x86/speculation: Rework SMT state change")
    Signed-off-by: Zhenzhong Duan <zhenzhong.duan@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: <konrad.wilk@oracle.com>
    Cc: <dwmw@amazon.co.uk>
    Cc: <bp@suse.de>
    Cc: <srinivas.eeda@oracle.com>
    Cc: <peterz@infradead.org>
    Cc: <hpa@zytor.com>
    Link: https://lkml.kernel.org/r/e2e064f2-e8ef-42ca-bf4f-76b612964752@default

commit 09c2fe608a2608f9c7de7928f96f0ebc6197e195
Merge: 333478a7eb21 60f1bf29c0b2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 24 08:58:01 2019 +1300

    Merge tag 's390-5.0-2' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 fixes from Martin Schwidefsky:
    
     - Do not claim to run under z/VM if the hypervisor can not be
       identified
    
     - Fix crashes due to outdated ASCEs in CR1
    
     - Avoid a deadlock in regard to CPU hotplug
    
     - Really fix the vdso mapping issue for compat tasks
    
     - Avoid crash on restart due to an incorrect stack address
    
    * tag 's390-5.0-2' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux:
      s390/smp: Fix calling smp_call_ipl_cpu() from ipl CPU
      s390/vdso: correct vdso mapping for compat tasks
      s390/smp: fix CPU hotplug deadlock with CPU rescan
      s390/mm: always force a load of the primary ASCE on context switch
      s390/early: improve machine detection

commit b7cb707c373094ce4008d4a6ac9b6b366ec52da5
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Wed Jan 9 13:00:03 2019 +0100

    s390/smp: fix CPU hotplug deadlock with CPU rescan
    
    smp_rescan_cpus() is called without the device_hotplug_lock, which can lead
    to a dedlock when a new CPU is found and immediately set online by a udev
    rule.
    
    This was observed on an older kernel version, where the cpu_hotplug_begin()
    loop was still present, and it resulted in hanging chcpu and systemd-udev
    processes. This specific deadlock will not show on current kernels. However,
    there may be other possible deadlocks, and since smp_rescan_cpus() can still
    trigger a CPU hotplug operation, the device_hotplug_lock should be held.
    
    For reference, this was the deadlock with the old cpu_hotplug_begin() loop:
    
            chcpu (rescan)                       systemd-udevd
    
     echo 1 > /sys/../rescan
     -> smp_rescan_cpus()
     -> (*) get_online_cpus()
        (increases refcount)
     -> smp_add_present_cpu()
        (new CPU found)
     -> register_cpu()
     -> device_add()
     -> udev "add" event triggered -----------> udev rule sets CPU online
                                             -> echo 1 > /sys/.../online
                                             -> lock_device_hotplug_sysfs()
                                                (this is missing in rescan path)
                                             -> device_online()
                                             -> (**) device_lock(new CPU dev)
                                             -> cpu_up()
                                             -> cpu_hotplug_begin()
                                                (loops until refcount == 0)
                                                -> deadlock with (*)
     -> bus_probe_device()
     -> device_attach()
     -> device_lock(new CPU dev)
        -> deadlock with (**)
    
    Fix this by taking the device_hotplug_lock in the CPU rescan path.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit 1205b62390eed4e747232d183fbf412a5aecacd9
Merge: 9ee3b3f4a5eb 6de92920a717
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 5 11:23:17 2019 -0800

    Merge tag 'for-4.21' of git://git.armlinux.org.uk/~rmk/linux-arm
    
    Pull ARM updates from Russell King:
     "Included in this update:
    
       - Florian Fainelli noticed that userspace segfaults caused by the
         lack of kernel-userspace helpers was hard to diagnose; we now issue
         a warning when userspace tries to use the helpers but the kernel
         has them disabled.
    
       - Ben Dooks wants compatibility for the old ATAG serial number with
         DT systems.
    
       - Some cleanup of assembly by Nicolas Pitre.
    
       - User accessors optimisation from Vincent Whitchurch.
    
       - More robust kdump on SMP systems from Yufen Wang.
    
       - Sebastian Andrzej Siewior noticed problems with the SMP "boot_lock"
         on RT kernels, and so we convert the Versatile series of platforms
         to use a raw spinlock instead, consolidating the Versatile
         implementation. We entirely remove the boot_lock on OMAP systems,
         where it's unnecessary. Further patches for other systems will be
         submitted for the following merge window.
    
       - Start switching old StrongARM-11x0 systems to use gpiolib rather
         than their private GPIO implementation - mostly PCMCIA bits.
    
       - ARM Kconfig cleanups.
    
       - Cleanup a mostly harmless mistake in the recent Spectre patch in
         4.20 (which had the effect that data that can be placed into the
         init sections was incorrectly always placed in the rodata section)"
    
    * tag 'for-4.21' of git://git.armlinux.org.uk/~rmk/linux-arm: (25 commits)
      ARM: omap2: remove unnecessary boot_lock
      ARM: versatile: rename and comment SMP implementation
      ARM: versatile: convert boot_lock to raw
      ARM: vexpress/realview: consolidate immitation CPU hotplug
      ARM: fix the cockup in the previous patch
      ARM: sa1100/cerf: switch to using gpio_led_register_device()
      ARM: sa1100/assabet: switch to using gpio leds
      ARM: sa1100/assabet: add gpio keys support for right-hand two buttons
      ARM: sa1111: remove legacy GPIO interfaces
      pcmcia: sa1100*: remove redundant bvd1/bvd2 setting
      ARM: pxa/lubbock: switch PCMCIA to MAX1600 library
      ARM: pxa/mainstone: switch PCMCIA to MAX1600 library and gpiod APIs
      ARM: sa1100/neponset: switch PCMCIA to MAX1600 library and gpiod APIs
      ARM: sa1100/jornada720: switch PCMCIA to gpiod APIs
      pcmcia: add MAX1600 library
      ARM: sa1100: explicitly register sa11x0-pcmcia devices
      ARM: 8813/1: Make aligned 2-byte getuser()/putuser() atomic on ARMv6+
      ARM: 8812/1: Optimise copy_{from/to}_user for !CPU_USE_DOMAINS
      ARM: 8811/1: always list both ldrd/strd registers explicitly
      ARM: 8808/1: kexec:offline panic_smp_self_stop CPU
      ...

commit 6f9d71c9c759b1e7d31189a4de228983192c7dc7
Merge: 55db91fbaad9 3fc9c12d27b4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 29 10:57:20 2018 -0800

    Merge branch 'for-4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
    
     - Waiman's cgroup2 cpuset support has been finally merged closing one
       of the last remaining feature gaps.
    
     - cgroup.procs could show non-leader threads when cgroup2 threaded mode
       was used in certain ways. I forgot to push the fix during the last
       cycle.
    
     - A patch to fix mount option parsing when all mount options have been
       consumed by someone else (LSM).
    
     - cgroup_no_v1 boot param can now block named cgroup1 hierarchies too.
    
    * 'for-4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: Add named hierarchy disabling to cgroup_no_v1 boot param
      cgroup: fix parsing empty mount option string
      cpuset: Remove set but not used variable 'cs'
      cgroup: fix CSS_TASK_ITER_PROCS
      cgroup: Add .__DEBUG__. prefix to debug file names
      cpuset: Minor cgroup2 interface updates
      cpuset: Expose cpuset.cpus.subpartitions with cgroup_debug
      cpuset: Add documentation about the new "cpuset.sched.partition" flag
      cpuset: Use descriptive text when reading/writing cpuset.sched.partition
      cpuset: Expose cpus.effective and mems.effective on cgroup v2 root
      cpuset: Make generate_sched_domains() work with partition
      cpuset: Make CPU hotplug work with partition
      cpuset: Track cpusets that use parent's effective_cpus
      cpuset: Add an error state to cpuset.sched.partition
      cpuset: Add new v2 cpuset.sched.partition flag
      cpuset: Simply allocation and freeing of cpumasks
      cpuset: Define data structures to support scheduling partition
      cpuset: Enable cpuset controller in default hierarchy
      cgroup: remove unnecessary unlikely()

commit 4fb68e12f2cf93176f1b7542c754a4d0413eb290
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Thu Dec 13 12:54:26 2018 +0000

    ARM: vexpress/realview: consolidate immitation CPU hotplug
    
    The only difference between the hotplug implementation for Realview
    and Versatile Express are the bit in the auxiliary control register
    to disable coherency.  Combine the two implentations accounting for
    that difference.
    
    Rename the functions to try to discourage cargo-cult copying of this
    code.
    
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Acked-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

commit b72f711a4efadfaa8a16f9cb708bfe1ce6125906
Merge: 7e40b56c776f 3a4d0c2172bc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 6 16:45:36 2018 -0800

    Merge branch 'spectre' of git://git.armlinux.org.uk/~rmk/linux-arm
    
    Pull ARM spectre fix from Russell King:
     "Exynos folk noticed that CPU hotplug wasn't working with their kernel
      configuration, and have tested this as fixing the problem"
    
    * 'spectre' of git://git.armlinux.org.uk/~rmk/linux-arm:
      ARM: ensure that processor vtables is not lost after boot

commit 3a4d0c2172bcf15b7a3d9d498b2b355f9864286b
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Thu Dec 6 16:36:38 2018 +0000

    ARM: ensure that processor vtables is not lost after boot
    
    Marek Szyprowski reported problems with CPU hotplug in current kernels.
    This was tracked down to the processor vtables being located in an
    init section, and therefore discarded after kernel boot, despite being
    required after boot to properly initialise the non-boot CPUs.
    
    Arrange for these tables to end up in .rodata when required.
    
    Reported-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Krzysztof Kozlowski <krzk@kernel.org>
    Fixes: 383fb3ee8024 ("ARM: spectre-v2: per-CPU vtables to work around big.Little systems")
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

commit cbb72a3c19eff0ea3ccb0b068eca189063c86174
Author: Hoan Tran <Hoan@os.amperecomputing.com>
Date:   Wed Nov 7 19:40:58 2018 +0000

    drivers/perf: xgene: Add CPU hotplug support
    
    If the CPU assigned to the xgene PMU is taken offline, then subsequent
    perf invocations on the PMU will fail:
    
      # echo 0 > /sys/devices/system/cpu/cpu0/online
      # perf stat -a -e l3c0/cycle-count/,l3c0/write/ sleep 1
        Error:
        The sys_perf_event_open() syscall returned with 19 (No such device) for event (l3c0/cycle-count/).
        /bin/dmesg may provide additional information.
        No CONFIG_PERF_EVENTS=y kernel support configured?
    
    This patch implements a hotplug notifier in the xgene PMU driver so that
    the PMU context is migrated to another online CPU should its assigned
    CPU disappear.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Hoan Tran <hoan.tran@amperecomputing.com>
    [will: Made naming of new cpuhp_state enum entry consistent]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit ef268de19756e6bc78cd3e6d9b15545f7df97ef2
Merge: 50d25bdc6431 b2fed34a628d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 16 10:14:54 2018 -0600

    Merge tag 'powerpc-4.20-3' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc fixes from Michael Ellerman:
     "Two weeks worth of fixes since rc1.
    
       - I broke 16-byte alignment of the stack when we moved PPR into
         pt_regs. Despite being required by the ABI this broke almost
         nothing, we eventually hit it in code where GCC does arithmetic on
         the stack pointer assuming the bottom 4 bits are clear. Fix it by
         padding the in-kernel pt_regs by 8 bytes.
    
       - A couple of commits fixing minor bugs in the recent SLB rewrite.
    
       - A build fix related to tracepoints in KVM in some configurations.
    
       - Our old "IO workarounds" code written for Cell couldn't coexist in
         a kernel that runs on Power9 with the Radix MMU, fix that.
    
       - Remove the NPU DMA ops, these just printed a warning and should
         never have been called.
    
       - Suppress an overly chatty message triggered by CPU hotplug in some
         configs.
    
       - Two small selftest fixes.
    
      Thanks to: Alistair Popple, Gustavo Romero, Nicholas Piggin, Satheesh
      Rajendran, Scott Wood"
    
    * tag 'powerpc-4.20-3' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux:
      selftests/powerpc: Adjust wild_bctr to build with old binutils
      powerpc/64: Fix kernel stack 16-byte alignment
      powerpc/numa: Suppress "VPHN is not supported" messages
      selftests/powerpc: Fix wild_bctr test to work on ppc64
      powerpc/io: Fix the IO workarounds code to work with Radix
      powerpc/mm/64s: Fix preempt warning in slb_allocate_kernel()
      KVM: PPC: Move and undef TRACE_INCLUDE_PATH/FILE
      powerpc/mm/64s: Only use slbfee on CPUs that support it
      powerpc/mm/64s: Use PPC_SLBFEE macro
      powerpc/mm/64s: Consolidate SLB assertions
      powerpc/powernv/npu: Remove NPU DMA ops

commit 4b842da276a8a1057aed7af6b2a5da471f840dd0
Author: Waiman Long <longman@redhat.com>
Date:   Thu Nov 8 10:08:41 2018 -0500

    cpuset: Make CPU hotplug work with partition
    
    When there is a cpu hotplug event (CPU online or offline), the partitions
    may need to be reconfigured and regenerated. So code is added to the
    hotplug functions to make them work with new subparts_cpus mask to
    compute the right effective_cpus for each of the affected cpusets.
    It may also change the state of a partition root from real one to an
    erroneous one or vice versa.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit b82592199032bf7c778f861b936287e37ebc9f62
Author: Long Li <longli@microsoft.com>
Date:   Fri Nov 2 18:02:48 2018 +0000

    genirq/affinity: Spread IRQs to all available NUMA nodes
    
    If the number of NUMA nodes exceeds the number of MSI/MSI-X interrupts
    which are allocated for a device, the interrupt affinity spreading code
    fails to spread them across all nodes.
    
    The reason is, that the spreading code starts from node 0 and continues up
    to the number of interrupts requested for allocation. This leaves the nodes
    past the last interrupt unused.
    
    This results in interrupt concentration on the first nodes which violates
    the assumption of the block layer that all nodes are covered evenly. As a
    consequence the NUMA nodes above the number of interrupts are all assigned
    to hardware queue 0 and therefore NUMA node 0, which results in bad
    performance and has CPU hotplug implications, because queue 0 gets shut
    down when the last CPU of node 0 is offlined.
    
    Go over all NUMA nodes and assign them round-robin to all requested
    interrupts to solve this.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Long Li <longli@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Link: https://lkml.kernel.org/r/20181102180248.13583-1-longli@linuxonhyperv.com

commit 685f7e4f161425b137056abe35ba8ef7b669d83d
Merge: c7a2c49ea6c9 58cfbac25b1f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 26 14:36:21 2018 -0700

    Merge tag 'powerpc-4.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - A large series to rewrite our SLB miss handling, replacing a lot of
         fairly complicated asm with much fewer lines of C.
    
       - Following on from that, we now maintain a cache of SLB entries for
         each process and preload them on context switch. Leading to a 27%
         speedup for our context switch benchmark on Power9.
    
       - Improvements to our handling of SLB multi-hit errors. We now print
         more debug information when they occur, and try to continue running
         by flushing the SLB and reloading, rather than treating them as
         fatal.
    
       - Enable THP migration on 64-bit Book3S machines (eg. Power7/8/9).
    
       - Add support for physical memory up to 2PB in the linear mapping on
         64-bit Book3S. We only support up to 512TB as regular system
         memory, otherwise the percpu allocator runs out of vmalloc space.
    
       - Add stack protector support for 32 and 64-bit, with a per-task
         canary.
    
       - Add support for PTRACE_SYSEMU and PTRACE_SYSEMU_SINGLESTEP.
    
       - Support recognising "big cores" on Power9, where two SMT4 cores are
         presented to us as a single SMT8 core.
    
       - A large series to cleanup some of our ioremap handling and PTE
         flags.
    
       - Add a driver for the PAPR SCM (storage class memory) interface,
         allowing guests to operate on SCM devices (acked by Dan).
    
       - Changes to our ftrace code to handle very large kernels, where we
         need to use a trampoline to get to ftrace_caller().
    
      And many other smaller enhancements and cleanups.
    
      Thanks to: Alan Modra, Alistair Popple, Aneesh Kumar K.V, Anton
      Blanchard, Aravinda Prasad, Bartlomiej Zolnierkiewicz, Benjamin
      Herrenschmidt, Breno Leitao, Cdric Le Goater, Christophe Leroy,
      Christophe Lombard, Dan Carpenter, Daniel Axtens, Finn Thain, Gautham
      R. Shenoy, Gustavo Romero, Haren Myneni, Hari Bathini, Jia Hongtao,
      Joel Stanley, John Allen, Laurent Dufour, Madhavan Srinivasan, Mahesh
      Salgaonkar, Mark Hairgrove, Masahiro Yamada, Michael Bringmann,
      Michael Neuling, Michal Suchanek, Murilo Opsfelder Araujo, Nathan
      Fontenot, Naveen N. Rao, Nicholas Piggin, Nick Desaulniers, Oliver
      O'Halloran, Paul Mackerras, Petr Vorel, Rashmica Gupta, Reza Arbab,
      Rob Herring, Sam Bobroff, Samuel Mendoza-Jonas, Scott Wood, Stan
      Johnson, Stephen Rothwell, Stewart Smith, Suraj Jitindar Singh, Tyrel
      Datwyler, Vaibhav Jain, Vasant Hegde, YueHaibing, zhong jiang"
    
    * tag 'powerpc-4.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (221 commits)
      Revert "selftests/powerpc: Fix out-of-tree build errors"
      powerpc/msi: Fix compile error on mpc83xx
      powerpc: Fix stack protector crashes on CPU hotplug
      powerpc/traps: restore recoverability of machine_check interrupts
      powerpc/64/module: REL32 relocation range check
      powerpc/64s/radix: Fix radix__flush_tlb_collapsed_pmd double flushing pmd
      selftests/powerpc: Add a test of wild bctr
      powerpc/mm: Fix page table dump to work on Radix
      powerpc/mm/radix: Display if mappings are exec or not
      powerpc/mm/radix: Simplify split mapping logic
      powerpc/mm/radix: Remove the retry in the split mapping logic
      powerpc/mm/radix: Fix small page at boundary when splitting
      powerpc/mm/radix: Fix overuse of small pages in splitting logic
      powerpc/mm/radix: Fix off-by-one in split mapping logic
      powerpc/ftrace: Handle large kernel configs
      powerpc/mm: Fix WARN_ON with THP NUMA migration
      selftests/powerpc: Fix out-of-tree build errors
      powerpc/time: no steal_time when CONFIG_PPC_SPLPAR is not selected
      powerpc/time: Only set CONFIG_ARCH_HAS_SCALED_CPUTIME on PPC64
      powerpc/time: isolate scaled cputime accounting in dedicated functions.
      ...

commit b6aeddea74b08518289fc86545297cf18a0b53a7
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri Oct 19 16:19:10 2018 +1100

    powerpc: Fix stack protector crashes on CPU hotplug
    
    Recently in commit 7241d26e8175 ("powerpc/64: properly initialise
    the stackprotector canary on SMP.") we fixed a crash with stack
    protector on SMP by initialising the stack canary in
    cpu_idle_thread_init().
    
    But this can also causes crashes, when a CPU comes back online after
    being offline:
    
      Kernel panic - not syncing: stack-protector: Kernel stack is corrupted in: pnv_smp_cpu_kill_self+0x2a0/0x2b0
      CPU: 1 PID: 0 Comm: swapper/1 Not tainted 4.19.0-rc3-gcc-7.3.1-00168-g4ffe713b7587 #94
      Call Trace:
        dump_stack+0xb0/0xf4 (unreliable)
        panic+0x144/0x328
        __stack_chk_fail+0x2c/0x30
        pnv_smp_cpu_kill_self+0x2a0/0x2b0
        cpu_die+0x48/0x70
        arch_cpu_idle_dead+0x20/0x40
        do_idle+0x274/0x390
        cpu_startup_entry+0x38/0x50
        start_secondary+0x5e4/0x600
        start_secondary_prolog+0x10/0x14
    
    Looking at the stack we see that the canary value in the stack frame
    doesn't match the canary in the task/paca. That is because we have
    reinitialised the task/paca value, but then the CPU coming online has
    returned into a function using the old canary value. That causes the
    comparison to fail.
    
    Instead we can call boot_init_stack_canary() from start_secondary()
    which never returns. This is essentially what the generic code does in
    cpu_startup_entry() under #ifdef X86, we should make that non-x86
    specific in a future patch.
    
    Fixes: 7241d26e8175 ("powerpc/64: properly initialise the stackprotector canary on SMP.")
    Reported-by: Joel Stanley <joel@jms.id.au>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>

commit dfd718a2ed1f678e66749ffe41bdeafedf3f4314
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Mon Oct 1 16:10:39 2018 +0530

    powerpc/rtas: Fix a potential race between CPU-Offline & Migration
    
    Live Partition Migrations require all the present CPUs to execute the
    H_JOIN call, and hence rtas_ibm_suspend_me() onlines any offline CPUs
    before initiating the migration for this purpose.
    
    The commit 85a88cabad57
    ("powerpc/pseries: Disable CPU hotplug across migrations")
    disables any CPU-hotplug operations once all the offline CPUs are
    brought online to prevent any further state change. Once the
    CPU-Hotplug operation is disabled, the code assumes that all the CPUs
    are online.
    
    However, there is a minor window in rtas_ibm_suspend_me() between
    onlining the offline CPUs and disabling CPU-Hotplug when a concurrent
    CPU-offline operations initiated by the userspace can succeed thereby
    nullifying the the aformentioned assumption. In this unlikely case
    these offlined CPUs will not call H_JOIN, resulting in a system hang.
    
    Fix this by verifying that all the present CPUs are actually online
    after CPU-Hotplug has been disabled, failing which we restore the
    state of the offline CPUs in rtas_ibm_suspend_me() and return an
    -EBUSY.
    
    Cc: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Cc: Tyrel Datwyler <tyreld@linux.vnet.ibm.com>
    Suggested-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 75bda3609f94a24a213fa56235bf369056565299
Merge: e51e8d5de999 6d06009cb216
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Oct 8 14:44:40 2018 +0200

    Merge tag 'soc-fsl-next-v4.20-2' of git://git.kernel.org/pub/scm/linux/kernel/git/leo/linux into next/drivers
    
    NXP/FSL SoC drivers updates for v4.20 take 2
    
    - Update qbman driver to better work with CPU hotplug
    - Add Kconfig dependency of 64-bit DMA addressing for qbman driver
    - Use last reponse to determine valid bit for qbman driver
    - Defer bman_portals probe if bman is not probed
    - Add interrupt coalescing APIs to qbman driver
    
    * tag 'soc-fsl-next-v4.20-2' of git://git.kernel.org/pub/scm/linux/kernel/git/leo/linux:
      soc: fsl: qbman: add interrupt coalesce changing APIs
      soc: fsl: bman_portals: defer probe after bman's probe
      soc: fsl: qbman: Use last response to determine valid bit
      soc: fsl: qbman: Add 64 bit DMA addressing requirement to QBMan
      soc: fsl: qbman: replace CPU 0 with any online CPU in hotplug handlers
      soc: fsl: qbman: Check if CPU is offline when initializing portals
      soc: fsl: qman_portals: defer probe after qman's probe
      soc: fsl: qbman: add APIs to retrieve the probing status
      soc: fsl: qe: Fix copy/paste bug in ucc_get_tdm_sync_shift()
      soc: fsl: qbman: qman: avoid allocating from non existing gen_pool
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 11e37d357f6ba7a9af850a872396082cc0a0001f
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jul 27 13:38:54 2018 +0100

    irqchip/gic-v3-its: Move pending table allocation to init time
    
    Pending tables for the redistributors are currently allocated
    one at a time as each CPU boots. This is causing some grief
    for Linux/RT (allocation from within a CPU hotplug notifier is
    frown upon).
    
    Let's move this allocation to take place at init time, when we
    only have a single CPU. It means we're allocating memory for CPUs
    that are not online yet, but most system will boot all of their
    CPUs anyway, so that's not completely wasted.
    
    Tested-by: Jeremy Linton <jeremy.linton@arm.com>
    Tested-by: Bhupesh Sharma <bhsharma@redhat.com>
    Tested-by: Lei Zhang <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

commit c48fb3bbe912a295e5b75eaabaf39874d5b9b773
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Thu Sep 20 13:17:48 2018 -0600

    coresight: perf: Avoid unncessary CPU hotplug read lock
    
    We hold the read lock on CPU hotplug to simply copy the
    online mask, which is not really needed. And this can
    cause a lockdep warning, like :
    
    [   54.632093] ======================================================
    [   54.638207] WARNING: possible circular locking dependency detected
    [   54.644322] 4.18.0-rc3-00042-g2d39e6356bb7-dirty #309 Not tainted
    [   54.650350] ------------------------------------------------------
    [   54.656464] perf/2862 is trying to acquire lock:
    [   54.661031] 000000007e21d170 (&event->mmap_mutex){+.+.}, at: perf_event_set_output+0x98/0x138
    [   54.669486]
    [   54.669486] but task is already holding lock:
    [   54.675256] 000000001080eb1b (&cpuctx_mutex){+.+.}, at: perf_event_ctx_lock_nested+0xf8/0x1f0
    [   54.683704]
    [   54.683704] which lock already depends on the new lock.
    [   54.683704]
    [   54.691797]
    [   54.691797] the existing dependency chain (in reverse order) is:
    [   54.699201]
    [   54.699201] -> #3 (&cpuctx_mutex){+.+.}:
    [   54.704556]        __mutex_lock+0x70/0x808
    [   54.708608]        mutex_lock_nested+0x1c/0x28
    [   54.713005]        perf_event_init_cpu+0x8c/0xd8
    [   54.717574]        perf_event_init+0x194/0x1d4
    [   54.721971]        start_kernel+0x2b8/0x42c
    [   54.726107]
    [   54.726107] -> #2 (pmus_lock){+.+.}:
    [   54.731114]        __mutex_lock+0x70/0x808
    [   54.735165]        mutex_lock_nested+0x1c/0x28
    [   54.739560]        perf_event_init_cpu+0x30/0xd8
    [   54.744129]        cpuhp_invoke_callback+0x84/0x248
    [   54.748954]        _cpu_up+0xe8/0x1c8
    [   54.752576]        do_cpu_up+0xa8/0xc8
    [   54.756283]        cpu_up+0x10/0x18
    [   54.759731]        smp_init+0xa0/0x114
    [   54.763438]        kernel_init_freeable+0x120/0x288
    [   54.768264]        kernel_init+0x10/0x108
    [   54.772230]        ret_from_fork+0x10/0x18
    [   54.776279]
    [   54.776279] -> #1 (cpu_hotplug_lock.rw_sem){++++}:
    [   54.782492]        cpus_read_lock+0x34/0xb0
    [   54.786631]        etm_setup_aux+0x5c/0x308
    [   54.790769]        rb_alloc_aux+0x1ec/0x300
    [   54.794906]        perf_mmap+0x284/0x610
    [   54.798787]        mmap_region+0x388/0x570
    [   54.802838]        do_mmap+0x344/0x4f8
    [   54.806544]        vm_mmap_pgoff+0xe4/0x110
    [   54.810682]        ksys_mmap_pgoff+0xa8/0x240
    [   54.814992]        sys_mmap+0x18/0x28
    [   54.818613]        el0_svc_naked+0x30/0x34
    [   54.822661]
    [   54.822661] -> #0 (&event->mmap_mutex){+.+.}:
    [   54.828445]        lock_acquire+0x48/0x68
    [   54.832409]        __mutex_lock+0x70/0x808
    [   54.836459]        mutex_lock_nested+0x1c/0x28
    [   54.840855]        perf_event_set_output+0x98/0x138
    [   54.845680]        _perf_ioctl+0x2a0/0x6a0
    [   54.849731]        perf_ioctl+0x3c/0x68
    [   54.853526]        do_vfs_ioctl+0xb8/0xa20
    [   54.857577]        ksys_ioctl+0x80/0xb8
    [   54.861370]        sys_ioctl+0xc/0x18
    [   54.864990]        el0_svc_naked+0x30/0x34
    [   54.869039]
    [   54.869039] other info that might help us debug this:
    [   54.869039]
    [   54.876960] Chain exists of:
    [   54.876960]   &event->mmap_mutex --> pmus_lock --> &cpuctx_mutex
    [   54.876960]
    [   54.887217]  Possible unsafe locking scenario:
    [   54.887217]
    [   54.893073]        CPU0                    CPU1
    [   54.897552]        ----                    ----
    [   54.902030]   lock(&cpuctx_mutex);
    [   54.905396]                                lock(pmus_lock);
    [   54.910911]                                lock(&cpuctx_mutex);
    [   54.916770]   lock(&event->mmap_mutex);
    [   54.920566]
    [   54.920566]  *** DEADLOCK ***
    [   54.920566]
    [   54.926424] 1 lock held by perf/2862:
    [   54.930042]  #0: 000000001080eb1b (&cpuctx_mutex){+.+.}, at: perf_event_ctx_lock_nested+0xf8/0x1f0
    
    Since we have per-cpu array for the paths, we simply don't care about
    the number of online CPUs. This patch gets rid of the
    {get/put}_online_cpus().
    
    Reported-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1d92a611db50f1b19d5d7ed27bd4dec6000d06e4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Sep 6 15:21:38 2018 +0200

    cpu/hotplug: Prevent state corruption on error rollback
    
    commit 69fa6eb7d6a64801ea261025cce9723d9442d773 upstream.
    
    When a teardown callback fails, the CPU hotplug code brings the CPU back to
    the previous state. The previous state becomes the new target state. The
    rollback happens in undo_cpu_down() which increments the state
    unconditionally even if the state is already the same as the target.
    
    As a consequence the next CPU hotplug operation will start at the wrong
    state. This is easily to observe when __cpu_disable() fails.
    
    Prevent the unconditional undo by checking the state vs. target before
    incrementing state and fix up the consequently wrong conditional in the
    unplug code which handles the failure of the final CPU take down on the
    control CPU side.
    
    Fixes: 4dddfb5faa61 ("smp/hotplug: Rewrite AP state machine core")
    Reported-by: Neeraj Upadhyay <neeraju@codeaurora.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Tested-by: Neeraj Upadhyay <neeraju@codeaurora.org>
    Cc: josh@joshtriplett.org
    Cc: peterz@infradead.org
    Cc: jiangshanlai@gmail.com
    Cc: dzickus@redhat.com
    Cc: brendan.jackman@arm.com
    Cc: malat@debian.org
    Cc: sramana@codeaurora.org
    Cc: linux-arm-msm@vger.kernel.org
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1809051419580.1416@nanos.tec.linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ----

commit 51d34e94c4701f125907c026272870790a37c4a1
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 5 10:41:58 2018 +0200

    clocksource: Revert "Remove kthread"
    
    commit e2c631ba75a7e727e8db0a9d30a06bfd434adb3a upstream.
    
    I turns out that the silly spawn kthread from worker was actually needed.
    
    clocksource_watchdog_kthread() cannot be called directly from
    clocksource_watchdog_work(), because clocksource_select() calls
    timekeeping_notify() which uses stop_machine(). One cannot use
    stop_machine() from a workqueue() due lock inversions wrt CPU hotplug.
    
    Revert the patch but add a comment that explain why we jump through such
    apparently silly hoops.
    
    Fixes: 7197e77abcb6 ("clocksource: Remove kthread")
    Reported-by: Siegfried Metz <frame@mailbox.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Niklas Cassel <niklas.cassel@linaro.org>
    Tested-by: Kevin Shanahan <kevin@shanahan.id.au>
    Tested-by: viktor_jaegerskuepper@freenet.de
    Tested-by: Siegfried Metz <frame@mailbox.org>
    Cc: rafael.j.wysocki@intel.com
    Cc: len.brown@intel.com
    Cc: diego.viola@gmail.com
    Cc: rui.zhang@intel.com
    Cc: bjorn.andersson@linaro.org
    Link: https://lkml.kernel.org/r/20180905084158.GR24124@hirez.programming.kicks-ass.net
    Cc: Siegfried Metz <frame@mailbox.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 875872547357d4f6646679f1070598118397a28b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Sep 6 15:21:38 2018 +0200

    cpu/hotplug: Prevent state corruption on error rollback
    
    commit 69fa6eb7d6a64801ea261025cce9723d9442d773 upstream.
    
    When a teardown callback fails, the CPU hotplug code brings the CPU back to
    the previous state. The previous state becomes the new target state. The
    rollback happens in undo_cpu_down() which increments the state
    unconditionally even if the state is already the same as the target.
    
    As a consequence the next CPU hotplug operation will start at the wrong
    state. This is easily to observe when __cpu_disable() fails.
    
    Prevent the unconditional undo by checking the state vs. target before
    incrementing state and fix up the consequently wrong conditional in the
    unplug code which handles the failure of the final CPU take down on the
    control CPU side.
    
    Fixes: 4dddfb5faa61 ("smp/hotplug: Rewrite AP state machine core")
    Reported-by: Neeraj Upadhyay <neeraju@codeaurora.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Tested-by: Neeraj Upadhyay <neeraju@codeaurora.org>
    Cc: josh@joshtriplett.org
    Cc: peterz@infradead.org
    Cc: jiangshanlai@gmail.com
    Cc: dzickus@redhat.com
    Cc: brendan.jackman@arm.com
    Cc: malat@debian.org
    Cc: sramana@codeaurora.org
    Cc: linux-arm-msm@vger.kernel.org
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1809051419580.1416@nanos.tec.linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    
    ----

commit 85a88cabad57d26d826dd94ea34d3a785824d802
Author: Nathan Fontenot <nfont@linux.vnet.ibm.com>
Date:   Mon Sep 17 14:14:02 2018 -0500

    powerpc/pseries: Disable CPU hotplug across migrations
    
    When performing partition migrations all present CPUs must be online
    as all present CPUs must make the H_JOIN call as part of the migration
    process. Once all present CPUs make the H_JOIN call, one CPU is returned
    to make the rtas call to perform the migration to the destination system.
    
    During testing of migration and changing the SMT state we have found
    instances where CPUs are offlined, as part of the SMT state change,
    before they make the H_JOIN call. This results in a hung system where
    every CPU is either in H_JOIN or offline.
    
    To prevent this this patch disables CPU hotplug during the migration
    process.
    
    Signed-off-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Reviewed-by: Tyrel Datwyler <tyreld@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 6a1cac56f41f9ea94e440dfcc1cac44b41a1b194
Author: Brijesh Singh <brijesh.singh@amd.com>
Date:   Fri Sep 14 08:45:59 2018 -0500

    x86/kvm: Use __bss_decrypted attribute in shared variables
    
    The recent removal of the memblock dependency from kvmclock caused a SEV
    guest regression because the wall_clock and hv_clock_boot variables are
    no longer mapped decrypted when SEV is active.
    
    Use the __bss_decrypted attribute to put the static wall_clock and
    hv_clock_boot in the .bss..decrypted section so that they are mapped
    decrypted during boot.
    
    In the preparatory stage of CPU hotplug, the per-cpu pvclock data pointer
    assigns either an element of the static array or dynamically allocated
    memory for the pvclock data pointer. The static array are now mapped
    decrypted but the dynamically allocated memory is not mapped decrypted.
    However, when SEV is active this memory range must be mapped decrypted.
    
    Add a function which is called after the page allocator is up, and
    allocate memory for the pvclock data pointers for the all possible cpus.
    Map this memory range as decrypted when SEV is active.
    
    Fixes: 368a540e0232 ("x86/kvmclock: Remove memblock dependency")
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Brijesh Singh <brijesh.singh@amd.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: "Radim Krm" <rkrcmar@redhat.com>
    Cc: kvm@vger.kernel.org
    Link: https://lkml.kernel.org/r/1536932759-12905-3-git-send-email-brijesh.singh@amd.com

commit e2c631ba75a7e727e8db0a9d30a06bfd434adb3a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 5 10:41:58 2018 +0200

    clocksource: Revert "Remove kthread"
    
    I turns out that the silly spawn kthread from worker was actually needed.
    
    clocksource_watchdog_kthread() cannot be called directly from
    clocksource_watchdog_work(), because clocksource_select() calls
    timekeeping_notify() which uses stop_machine(). One cannot use
    stop_machine() from a workqueue() due lock inversions wrt CPU hotplug.
    
    Revert the patch but add a comment that explain why we jump through such
    apparently silly hoops.
    
    Fixes: 7197e77abcb6 ("clocksource: Remove kthread")
    Reported-by: Siegfried Metz <frame@mailbox.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Niklas Cassel <niklas.cassel@linaro.org>
    Tested-by: Kevin Shanahan <kevin@shanahan.id.au>
    Tested-by: viktor_jaegerskuepper@freenet.de
    Tested-by: Siegfried Metz <frame@mailbox.org>
    Cc: rafael.j.wysocki@intel.com
    Cc: len.brown@intel.com
    Cc: diego.viola@gmail.com
    Cc: rui.zhang@intel.com
    Cc: bjorn.andersson@linaro.org
    Link: https://lkml.kernel.org/r/20180905084158.GR24124@hirez.programming.kicks-ass.net

commit 69fa6eb7d6a64801ea261025cce9723d9442d773
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Sep 6 15:21:38 2018 +0200

    cpu/hotplug: Prevent state corruption on error rollback
    
    When a teardown callback fails, the CPU hotplug code brings the CPU back to
    the previous state. The previous state becomes the new target state. The
    rollback happens in undo_cpu_down() which increments the state
    unconditionally even if the state is already the same as the target.
    
    As a consequence the next CPU hotplug operation will start at the wrong
    state. This is easily to observe when __cpu_disable() fails.
    
    Prevent the unconditional undo by checking the state vs. target before
    incrementing state and fix up the consequently wrong conditional in the
    unplug code which handles the failure of the final CPU take down on the
    control CPU side.
    
    Fixes: 4dddfb5faa61 ("smp/hotplug: Rewrite AP state machine core")
    Reported-by: Neeraj Upadhyay <neeraju@codeaurora.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Tested-by: Neeraj Upadhyay <neeraju@codeaurora.org>
    Cc: josh@joshtriplett.org
    Cc: peterz@infradead.org
    Cc: jiangshanlai@gmail.com
    Cc: dzickus@redhat.com
    Cc: brendan.jackman@arm.com
    Cc: malat@debian.org
    Cc: sramana@codeaurora.org
    Cc: linux-arm-msm@vger.kernel.org
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1809051419580.1416@nanos.tec.linutronix.de
    
    ----

commit 1395d109cddcf6c8ebf20ba3bfaa2beb48febfbc
Merge: 501dacbc2435 6fb86d972078
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Sep 2 10:09:35 2018 -0700

    Merge branch 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug fix from Thomas Gleixner:
     "Remove the stale skip_onerr member from the hotplug states"
    
    * 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Remove skip_onerr field from cpuhp_step structure

commit 780cd590836fe24bc2a81b8cd7c2f9cbe495421e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 3 17:22:34 2018 -0700

    rcu: Remove rsp parameter from CPU hotplug functions
    
    There now is only one rcu_state structure in a given build of the
    Linux kernel, so there is no need to pass it as a parameter to RCU's
    functions.  This commit therefore removes the rsp parameter from
    rcu_cleanup_dying_cpu() and rcu_cleanup_dead_cpu().  And, as long as
    we are in the neighborhood, inlines them into rcutree_dying_cpu() and
    rcutree_dead_cpu(), respectively.  This also eliminates a pair of
    for_each_rcu_flavor() loops.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit c04dd09bd38c0df1aa6318164a51eccbc3a9fa5e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jul 23 14:16:47 2018 -0700

    rcutorture: Adjust number of reader kthreads per CPU-hotplug operations
    
    Currently, rcutorture provisions rcu_torture_reader() kthreads based
    on the initial number of CPUs.  This can be problematic when CPU hotplug
    is enabled, as a system with a very large number of CPUs will provision
    a very large number of rcu_torture_reader() kthreads.  All of these
    kthreads will continue running even if the CPU-hotplug operations result
    in only one remaining online CPU.  This can result in all sorts of strange
    artifacts due simply to massive overload.
    
    This commit therefore causes the rcu_torture_reader() kthreads to start
    blocking as the number of online CPUs decreases.  This is accomplished
    by numbering these kthreads, and having each check to make sure that the
    number of online CPUs is at least as large as its assigned number.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 71141fa9f7796f01a7566c4eab35483553a69c7c
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 22 10:52:54 2018 +0100

    irqchip/gic-v3-its: Fix reprogramming of redistributors on CPU hotplug
    
    [ Upstream commit 82f499c8811149069ec958b72a86643a7a289b25 ]
    
    Enabling LPIs was made a lot stricter recently, by checking that they are
    disabled before enabling them. By doing so, the CPU hotplug case was missed
    altogether, which leaves LPIs enabled on hotplug off (expecting the CPU to
    eventually come back), and won't write a different value anyway on hotplug
    on.
    
    So skip that check if that particular case is detected
    
    Fixes: 6eb486b66a30 ("irqchip/gic-v3: Ensure GICR_CTLR.EnableLPI=0 is observed before enabling")
    Reported-by: Sumit Garg <sumit.garg@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Sumit Garg <sumit.garg@linaro.org>
    Cc: Jason Cooper <jason@lakedaemon.net>
    Cc: Alexandre Belloni <alexandre.belloni@bootlin.com>
    Cc: Yang Yingliang <yangyingliang@huawei.com>
    Link: https://lkml.kernel.org/r/20180622095254.5906-8-marc.zyngier@arm.com
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fc890e9b571fb273e714bae5de682226eaed9cb2
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 29 16:43:46 2018 +0200

    sched/smt: Update sched_smt_present at runtime
    
    commit ba2591a5993eabcc8e874e30f361d8ffbb10d6d4 upstream
    
    The static key sched_smt_present is only updated at boot time when SMT
    siblings have been detected. Booting with maxcpus=1 and bringing the
    siblings online after boot rebuilds the scheduling domains correctly but
    does not update the static key, so the SMT code is not enabled.
    
    Let the key be updated in the scheduler CPU hotplug code to fix this.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f3839aa4a2b9b04b5b4144412e436d6811972d2b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 29 16:43:46 2018 +0200

    sched/smt: Update sched_smt_present at runtime
    
    commit ba2591a5993eabcc8e874e30f361d8ffbb10d6d4 upstream
    
    The static key sched_smt_present is only updated at boot time when SMT
    siblings have been detected. Booting with maxcpus=1 and bringing the
    siblings online after boot rebuilds the scheduling domains correctly but
    does not update the static key, so the SMT code is not enabled.
    
    Let the key be updated in the scheduler CPU hotplug code to fix this.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 71ef4580dc21eebd43a8b22a372fce2e28235728
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 29 16:43:46 2018 +0200

    sched/smt: Update sched_smt_present at runtime
    
    commit ba2591a5993eabcc8e874e30f361d8ffbb10d6d4 upstream.
    
    The static key sched_smt_present is only updated at boot time when SMT
    siblings have been detected. Booting with maxcpus=1 and bringing the
    siblings online after boot rebuilds the scheduling domains correctly but
    does not update the static key, so the SMT code is not enabled.
    
    Let the key be updated in the scheduler CPU hotplug code to fix this.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b018fc9800557bd14a40d69501e19c340eb2c521
Merge: c07b3682cd12 7425ecd5e3e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 13:12:24 2018 -0700

    Merge tag 'pm-4.19-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "These add a new framework for CPU idle time injection, to be used by
      all of the idle injection code in the kernel in the future, fix some
      issues and add a number of relatively small extensions in multiple
      places.
    
      Specifics:
    
       - Add a new framework for CPU idle time injection (Daniel Lezcano).
    
       - Add AVS support to the armada-37xx cpufreq driver (Gregory
         CLEMENT).
    
       - Add support for current CPU frequency reporting to the ACPI CPPC
         cpufreq driver (George Cherian).
    
       - Rework the cooling device registration in the imx6q/thermal driver
         (Bastian Stender).
    
       - Make the pcc-cpufreq driver refuse to work with dynamic scaling
         governors on systems with many CPUs to avoid scalability issues
         with it (Rafael Wysocki).
    
       - Fix the intel_pstate driver to report different maximum CPU
         frequencies on systems where they really are different and to
         ignore the turbo active ratio if hardware-managend P-states (HWP)
         are in use; make it use the match_string() helper (Xie Yisheng,
         Srinivas Pandruvada).
    
       - Fix a minor deferred probe issue in the qcom-kryo cpufreq driver
         (Niklas Cassel).
    
       - Add a tracepoint for the tracking of frequency limits changes (from
         Andriod) to the cpufreq core (Ruchi Kandoi).
    
       - Fix a circular lock dependency between CPU hotplug and sysfs
         locking in the cpufreq core reported by lockdep (Waiman Long).
    
       - Avoid excessive error reports on driver registration failures in
         the ARM cpuidle driver (Sudeep Holla).
    
       - Add a new device links flag to the driver core to make links go
         away automatically on supplier driver removal (Vivek Gautam).
    
       - Eliminate potential race condition between system-wide power
         management transitions and system shutdown (Pingfan Liu).
    
       - Add a quirk to save NVS memory on system suspend for the ASUS 1025C
         laptop (Willy Tarreau).
    
       - Make more systems use suspend-to-idle (instead of ACPI S3) by
         default (Tristian Celestin).
    
       - Get rid of stack VLA usage in the low-level hibernation code on
         64-bit x86 (Kees Cook).
    
       - Fix error handling in the hibernation core and mark an expected
         fall-through switch in it (Chengguang Xu, Gustavo Silva).
    
       - Extend the generic power domains (genpd) framework to support
         attaching a device to a power domain by name (Ulf Hansson).
    
       - Fix device reference counting and user limits initialization in the
         devfreq core (Arvind Yadav, Matthias Kaehlcke).
    
       - Fix a few issues in the rk3399_dmc devfreq driver and improve its
         documentation (Enric Balletbo i Serra, Lin Huang, Nick Milner).
    
       - Drop a redundant error message from the exynos-ppmu devfreq driver
         (Markus Elfring)"
    
    * tag 'pm-4.19-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (35 commits)
      PM / reboot: Eliminate race between reboot and suspend
      PM / hibernate: Mark expected switch fall-through
      cpufreq: intel_pstate: Ignore turbo active ratio in HWP
      cpufreq: Fix a circular lock dependency problem
      cpu/hotplug: Add a cpus_read_trylock() function
      x86/power/hibernate_64: Remove VLA usage
      cpufreq: trace frequency limits change
      cpufreq: intel_pstate: Show different max frequency with turbo 3 and HWP
      cpufreq: pcc-cpufreq: Disable dynamic scaling on many-CPU systems
      cpufreq: qcom-kryo: Silently error out on EPROBE_DEFER
      cpufreq / CPPC: Add cpuinfo_cur_freq support for CPPC
      cpufreq: armada-37xx: Add AVS support
      dt-bindings: marvell: Add documentation for the Armada 3700 AVS binding
      PM / devfreq: rk3399_dmc: Fix duplicated opp table on reload.
      PM / devfreq: Init user limits from OPP limits, not viceversa
      PM / devfreq: rk3399_dmc: fix spelling mistakes.
      PM / devfreq: rk3399_dmc: do not print error when get supply and clk defer.
      dt-bindings: devfreq: rk3399_dmc: move interrupts to be optional.
      PM / devfreq: rk3399_dmc: remove wait for dcf irq event.
      dt-bindings: clock: add rk3399 DDR3 standard speed bins.
      ...

commit 1c594774283a7cfe6dc0f8ffdfb2dbfc497502c4
Merge: f7951c33f0fe d018031f562b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 13 12:21:46 2018 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug update from Thomas Gleixner:
     "A trivial name fix for the hotplug state machine"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Clarify CPU hotplug step name for timers

commit 0cdf6d4607df37804955530e153cee2d363c426e
Merge: b9fb1fc7f921 ce03b6d2b610
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Aug 5 09:13:07 2018 -0700

    Merge branch 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf fixes from Thomas Gleixner:
     "A set of fixes for perf:
    
      Kernel side:
    
       - Fix the hardcoded index of extra PCI devices on Broadwell which
         caused a resource conflict and triggered warnings on CPU hotplug.
    
      Tooling:
    
       - Update the tools copy of several files, including perf_event.h,
         powerpc's asm/unistd.h (new io_pgetevents syscall), bpf.h and x86's
         memcpy_64.s (used in 'perf bench mem'), silencing the respective
         warnings during the perf tools build.
    
       - Fix the build on the alpine:edge distro"
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf/x86/intel/uncore: Fix hardcoded index of Broadwell extra PCI devices
      perf tools: Fix the build on the alpine:edge distro
      tools arch: Update arch/x86/lib/memcpy_64.S copy used in 'perf bench mem memcpy'
      tools headers uapi: Refresh linux/bpf.h copy
      tools headers powerpc: Update asm/unistd.h copy to pick new
      tools headers uapi: Update tools's copy of linux/perf_event.h

commit 827faa4eb5668e25baf7f3752dc7ae7fd46894c2
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Fri May 11 09:05:02 2018 -0600

    vfio/type1: Fix task tracking for QEMU vCPU hotplug
    
    [ Upstream commit 48d8476b41eed63567dd2f0ad125c895b9ac648a ]
    
    MAP_DMA ioctls might be called from various threads within a process,
    for example when using QEMU, the vCPU threads are often generating
    these calls and we therefore take a reference to that vCPU task.
    However, QEMU also supports vCPU hotplug on some machines and the task
    that called MAP_DMA may have exited by the time UNMAP_DMA is called,
    resulting in the mm_struct pointer being NULL and thus a failure to
    match against the existing mapping.
    
    To resolve this, we instead take a reference to the thread
    group_leader, which has the same mm_struct and resource limits, but
    is less likely exit, at least in the QEMU case.  A difficulty here is
    guaranteeing that the capabilities of the group_leader match that of
    the calling thread, which we resolve by tracking CAP_IPC_LOCK at the
    time of calling rather than at an indeterminate time in the future.
    Potentially this also results in better efficiency as this is now
    recorded once per MAP_DMA ioctl.
    
    Reported-by: Xu Yandong <xuyandong2@huawei.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2e446d115741314cc3cb99ac11782868004dc61b
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Fri May 11 09:05:02 2018 -0600

    vfio/type1: Fix task tracking for QEMU vCPU hotplug
    
    [ Upstream commit 48d8476b41eed63567dd2f0ad125c895b9ac648a ]
    
    MAP_DMA ioctls might be called from various threads within a process,
    for example when using QEMU, the vCPU threads are often generating
    these calls and we therefore take a reference to that vCPU task.
    However, QEMU also supports vCPU hotplug on some machines and the task
    that called MAP_DMA may have exited by the time UNMAP_DMA is called,
    resulting in the mm_struct pointer being NULL and thus a failure to
    match against the existing mapping.
    
    To resolve this, we instead take a reference to the thread
    group_leader, which has the same mm_struct and resource limits, but
    is less likely exit, at least in the QEMU case.  A difficulty here is
    guaranteeing that the capabilities of the group_leader match that of
    the calling thread, which we resolve by tracking CAP_IPC_LOCK at the
    time of calling rather than at an indeterminate time in the future.
    Potentially this also results in better efficiency as this is now
    recorded once per MAP_DMA ioctl.
    
    Reported-by: Xu Yandong <xuyandong2@huawei.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d018031f562b9c2eff038969ab1955a370c52d8f
Author: Mukesh Ojha <mojha@codeaurora.org>
Date:   Tue Jul 24 20:17:48 2018 +0530

    cpu/hotplug: Clarify CPU hotplug step name for timers
    
    After commit 249d4a9b3246 ("timers: Reinitialize per cpu bases on hotplug")
    i.e. the introduction of state CPUHP_TIMERS_PREPARE instead of
    CPUHP_TIMERS_DEAD the step name "timers:dead" is not longer accurate.
    
    Rename it to "timers:prepare".
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Mukesh Ojha <mojha@codeaurora.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: gkohli@codeaurora.org
    Cc: neeraju@codeaurora.org
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Brendan Jackman <brendan.jackman@arm.com>
    Cc: Mathieu Malaterre <malat@debian.org>
    Link: https://lkml.kernel.org/r/1532443668-26810-1-git-send-email-mojha@codeaurora.org

commit f7e2a152231f4a0308cc8f9c2296ba4e419ae945
Author: Alexey Spirkov <AlexeiS@astrosoft.ru>
Date:   Thu Jul 26 12:52:50 2018 +0000

    powerpc/44x: Mark mmu_init_secondary() as __init
    
    mmu_init_secondary() calls ppc44x_pin_tlb() which is marked __init,
    leading to a warning:
    
      The function mmu_init_secondary() references
      the function __init ppc44x_pin_tlb().
    
    There's no CPU hotplug support on 44x so mmu_init_secondary() will
    only be called at boot. Therefore we should mark it as __init.
    
    Signed-off-by: Alexey Spirkov <alexeis@astrosoft.ru>
    [mpe: Flesh out change log details]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 7401056de5f8d4eabe71a4c4aa80d0e278856e07
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed Jul 25 11:48:09 2018 +0100

    drivers/firmware: psci_checker: stash and use topology_core_cpumask for hotplug tests
    
    Commit 7f9545aa1a91 ("arm64: smp: remove cpu and numa topology
    information when hotplugging out CPU") updates the cpu topology when
    the CPU is hotplugged out. However the PSCI checker code uses the
    topology_core_cpumask pointers for some of the cpu hotplug testing.
    Since the pointer to the core_cpumask of the first CPU in the group
    is used, which when that CPU itself is hotpugged out is just set to
    itself, the testing terminates after that particular CPU is tested out.
    But the intention of this tests is to cover all the CPU in the group.
    
    In order to support that, we need to stash the topology_core_cpumask
    before the start of the test and use that value instead of pointer to
    a cpumask which will be updated on CPU hotplug.
    
    Fixes: 7f9545aa1a91a9a4 ("arm64: smp: remove cpu and numa topology
            information when hotplugging out CPU")
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Olof Johansson <olof@lixom.net>

commit 95a3d4454bb1cf5bfd666c27fdd2dc188e17c14d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 19 16:55:26 2018 -0400

    x86/kvmclock: Switch kvmclock data to a PER_CPU variable
    
    The previous removal of the memblock dependency from kvmclock introduced a
    static data array sized 64bytes * CONFIG_NR_CPUS. That's wasteful on large
    systems when kvmclock is not used.
    
    Replace it with:
    
     - A static page sized array of pvclock data. It's page sized because the
       pvclock data of the boot cpu is mapped into the VDSO so otherwise random
       other data would be exposed to the vDSO
    
     - A PER_CPU variable of pvclock data pointers. This is used to access the
       pcvlock data storage on each CPU.
    
    The setup is done in two stages:
    
     - Early boot stores the pointer to the static page for the boot CPU in
       the per cpu data.
    
     - In the preparatory stage of CPU hotplug assign either an element of
       the static array (when the CPU number is in that range) or allocate
       memory and initialize the per cpu pointer.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: peterz@infradead.org
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-8-pasha.tatashin@oracle.com

commit ef86f3a72adb8a7931f67335560740a7ad696d1d
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 12 10:53:05 2018 +0800

    genirq/affinity: assign vectors to all possible CPUs
    
    commit 84676c1f21e8ff54befe985f4f14dc1edc10046b upstream.
    
    Currently we assign managed interrupt vectors to all present CPUs.  This
    works fine for systems were we only online/offline CPUs.  But in case of
    systems that support physical CPU hotplug (or the virtualized version of
    it) this means the additional CPUs covered for in the ACPI tables or on
    the command line are not catered for.  To fix this we'd either need to
    introduce new hotplug CPU states just for this case, or we can start
    assining vectors to possible but not present CPUs.
    
    Reported-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Tested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Tested-by: Stefan Haberland <sth@linux.vnet.ibm.com>
    Fixes: 4b855ad37194 ("blk-mq: Create hctx for each present CPU")
    Cc: linux-kernel@vger.kernel.org
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f34f2f5852e556ee1c3b3b294571086b1791008a
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu May 3 13:40:25 2018 -0700

    rcu: Move grace-period pre-init delay after pre-init
    
    The main race with the early part of grace-period initialization appears
    to be with CPU hotplug.  To more fully open this race window, this commit
    moves the rcu_gp_slow() from the beginning of the early initialization
    loop to follow that loop, thus widening the race window, especially for
    the rcu_node structures that are initialized last.  This commit also
    expands rcutree.gp_preinit_delay from 3 to 12, giving the same overall
    delay in the grace period, but concentrated in the spot where it will
    do the most good.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 7f9545aa1a91a9a4dc8c3e1476dbbfa98dd38b81
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Fri Jul 6 12:02:46 2018 +0100

    arm64: smp: remove cpu and numa topology information when hotplugging out CPU
    
    We already repopulate the information on CPU hotplug-in, so we can safely
    remove the CPU topology and NUMA cpumap information during CPU hotplug
    out operation. This will help to provide the correct cpumask for
    scheduler domains.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Tested-by: Ganapatrao Kulkarni <ganapatrao.kulkarni@cavium.com>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 5bdd2b3f0f8830f281bb568e65df6eabf655dd0d
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Fri Jul 6 12:02:44 2018 +0100

    arm64: topology: add support to remove cpu topology sibling masks
    
    This patch adds support to remove all the CPU topology information using
    clear_cpu_topology and also resetting the sibling information on other
    sibling CPUs. This will be used in cpu_disable so that all the topology
    sibling information is removed on CPU hotplug out.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Tested-by: Ganapatrao Kulkarni <ganapatrao.kulkarni@cavium.com>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 97fd6016a7b3df00901b4cfdd883eac01e89fa8a
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Fri Jul 6 12:02:43 2018 +0100

    arm64: numa: separate out updates to percpu nodeid and NUMA node cpumap
    
    Currently numa_clear_node removes both cpu information from the NUMA
    node cpumap as well as the NUMA node id from the cpu. Similarly
    numa_store_cpu_info updates both percpu nodeid and NUMA cpumap.
    
    However we need to retain the numa node id for the cpu and only remove
    the cpu information from the numa node cpumap during CPU hotplug out.
    The same can be extended for hotplugging in the CPU.
    
    This patch separates out numa_{add,remove}_cpu from numa_clear_node and
    numa_store_cpu_info.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Ganapatrao Kulkarni <ganapatrao.kulkarni@cavium.com>
    Tested-by: Ganapatrao Kulkarni <ganapatrao.kulkarni@cavium.com>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 78fea6334f725f1a55cb5761730ceab64255cf1a
Merge: e0bc833d108e bed9df97b39e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jun 24 19:01:18 2018 +0800

    Merge branch 'irq-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq fixes from Thomas Gleixner:
     "A set of fixes mostly for the ARM/GIC world:
    
       - Fix the MSI affinity handling in the ls-scfg irq chip driver so it
         updates and uses the effective affinity mask correctly
    
       - Prevent binding LPIs to offline CPUs and respect the Cavium erratum
         which requires that LPIs which belong to an offline NUMA node are
         not bound to a CPU on a different NUMA node.
    
       - Free only the amount of allocated interrupts in the GIC-V2M driver
         instead of trying to free log2(nrirqs).
    
       - Prevent emitting SYNC and VSYNC targetting non existing interrupt
         collections in the GIC-V3 ITS driver
    
       - Ensure that the GIV-V3 interrupt redistributor is correctly
         reprogrammed on CPU hotplug
    
       - Remove a stale unused helper function"
    
    * 'irq-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      irqdesc: Delete irq_desc_get_msi_desc()
      irqchip/gic-v3-its: Fix reprogramming of redistributors on CPU hotplug
      irqchip/gic-v3-its: Only emit VSYNC if targetting a valid collection
      irqchip/gic-v3-its: Only emit SYNC if targetting a valid collection
      irqchip/gic-v3-its: Don't bind LPI to unavailable NUMA node
      irqchip/gic-v2m: Fix SPI release on error path
      irqchip/ls-scfg-msi: Fix MSI affinity handling
      genirq/debugfs: Add missing IRQCHIP_SUPPORTS_LEVEL_MSI debug

commit 82f499c8811149069ec958b72a86643a7a289b25
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 22 10:52:54 2018 +0100

    irqchip/gic-v3-its: Fix reprogramming of redistributors on CPU hotplug
    
    Enabling LPIs was made a lot stricter recently, by checking that they are
    disabled before enabling them. By doing so, the CPU hotplug case was missed
    altogether, which leaves LPIs enabled on hotplug off (expecting the CPU to
    eventually come back), and won't write a different value anyway on hotplug
    on.
    
    So skip that check if that particular case is detected
    
    Fixes: 6eb486b66a30 ("irqchip/gic-v3: Ensure GICR_CTLR.EnableLPI=0 is observed before enabling")
    Reported-by: Sumit Garg <sumit.garg@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Sumit Garg <sumit.garg@linaro.org>
    Cc: Jason Cooper <jason@lakedaemon.net>
    Cc: Alexandre Belloni <alexandre.belloni@bootlin.com>
    Cc: Yang Yingliang <yangyingliang@huawei.com>
    Link: https://lkml.kernel.org/r/20180622095254.5906-8-marc.zyngier@arm.com

commit ba2591a5993eabcc8e874e30f361d8ffbb10d6d4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 29 16:43:46 2018 +0200

    sched/smt: Update sched_smt_present at runtime
    
    The static key sched_smt_present is only updated at boot time when SMT
    siblings have been detected. Booting with maxcpus=1 and bringing the
    siblings online after boot rebuilds the scheduling domains correctly but
    does not update the static key, so the SMT code is not enabled.
    
    Let the key be updated in the scheduler CPU hotplug code to fix this.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>

commit fa6e9cb2aca1cdca1b85e5b19f9cda8aac78752b
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Wed May 30 17:15:26 2018 +0200

    ARM: shmobile: apmu: Move cpu_leave_lowpower() to SUSPEND section
    
    cpu_leave_lowpower() is used for suspend only, not for CPU hotplug.
    Hence move it from the HOTPLUG_CPU || SUSPEND section to the SUSPEND
    section.
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Simon Horman <horms+renesas@verge.net.au>

commit 467590e055f5c714fb457803250415879d0da9e5
Merge: 763f96944c95 c1abca96b252
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 13:11:26 2018 -0700

    Merge tag 'vfio-v4.18-rc1' of git://github.com/awilliam/linux-vfio
    
    Pull VFIO updates from Alex Williamson:
    
     - Bind type1 task tracking to group_leader to facilitate vCPU hotplug
       in QEMU (Alex Williamson)
    
     - Sample mdev display drivers, including region-based host and guest
       Linux drivers and bochs compatible dmabuf device
       (Gerd Hoffmann)
    
     - Fix vfio-platform reset module leak (Geert Uytterhoeven)
    
     - vfio-platform error message consistency (Geert Uytterhoeven)
    
     - Global checking for mdev uuid collisions rather than per parent
       device (Alex Williamson)
    
     - Use match_string() helper (Yisheng Xie)
    
     - vfio-platform PM domain fixes (Geert Uytterhoeven)
    
     - Fix sample mbochs driver build dependency (Arnd Bergmann)
    
    * tag 'vfio-v4.18-rc1' of git://github.com/awilliam/linux-vfio:
      samples: mbochs: add DMA_SHARED_BUFFER dependency
      vfio: platform: Fix using devices in PM Domains
      vfio: use match_string() helper
      vfio/mdev: Re-order sysfs attribute creation
      vfio/mdev: Check globally for duplicate devices
      vfio: platform: Make printed error messages more consistent
      vfio: platform: Fix reset module leak in error path
      sample: vfio bochs vbe display (host device for bochs-drm)
      sample: vfio mdev display - guest driver
      sample: vfio mdev display - host device
      vfio/type1: Fix task tracking for QEMU vCPU hotplug

commit 48d8476b41eed63567dd2f0ad125c895b9ac648a
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Fri May 11 09:05:02 2018 -0600

    vfio/type1: Fix task tracking for QEMU vCPU hotplug
    
    MAP_DMA ioctls might be called from various threads within a process,
    for example when using QEMU, the vCPU threads are often generating
    these calls and we therefore take a reference to that vCPU task.
    However, QEMU also supports vCPU hotplug on some machines and the task
    that called MAP_DMA may have exited by the time UNMAP_DMA is called,
    resulting in the mm_struct pointer being NULL and thus a failure to
    match against the existing mapping.
    
    To resolve this, we instead take a reference to the thread
    group_leader, which has the same mm_struct and resource limits, but
    is less likely exit, at least in the QEMU case.  A difficulty here is
    guaranteeing that the capabilities of the group_leader match that of
    the calling thread, which we resolve by tracking CAP_IPC_LOCK at the
    time of calling rather than at an indeterminate time in the future.
    Potentially this also results in better efficiency as this is now
    recorded once per MAP_DMA ioctl.
    
    Reported-by: Xu Yandong <xuyandong2@huawei.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>

commit e156ab71a974737c279530e3b868131291fe677e
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Wed Jun 6 11:38:46 2018 -0500

    arm64: topology: Avoid checking numa mask for scheduler MC selection
    
    The numa mask subset check can often lead to system hang or crash during
    CPU hotplug and system suspend operation if NUMA is disabled. This is
    mostly observed on HMP systems where the CPU compute capacities are
    different and ends up in different scheduler domains. Since
    cpumask_of_node is returned instead core_sibling, the scheduler is
    confused with incorrect cpumasks(e.g. one CPU in two different sched
    domains at the same time) on CPU hotplug.
    
    Lets disable the NUMA siblings checks for the time being, as NUMA in
    socket machines have LLC's that will assure that the scheduler topology
    isn't "borken".
    
    The NUMA check exists to assure that if a LLC within a socket crosses
    NUMA nodes/chiplets the scheduler domains remain consistent. This code will
    likely have to be re-enabled in the near future once the NUMA mask story
    is sorted.  At the moment its not necessary because the NUMA in socket
    machines LLC's are contained within the NUMA domains.
    
    Further, as a defensive mechanism during hot-plug, lets assure that the
    LLC siblings are also masked.
    
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

commit 1415eea15edcdf86632d762f81511be6df1128f8
Author: Borislav Petkov <bp@suse.de>
Date:   Thu May 17 10:46:26 2018 +0200

    x86/MCE/AMD: Cache SMCA MISC block addresses
    
    commit 78ce241099bb363b19dbd0245442e66c8de8f567 upstream.
    
    ... into a global, two-dimensional array and service subsequent reads from
    that cache to avoid rdmsr_on_cpu() calls during CPU hotplug (IPIs with IRQs
    disabled).
    
    In addition, this fixes a KASAN slab-out-of-bounds read due to wrong usage
    of the bank->blocks pointer.
    
    Fixes: 27bd59502702 ("x86/mce/AMD: Get address from already initialized block")
    Reported-by: Johannes Hirte <johannes.hirte@datenkhaos.de>
    Tested-by: Johannes Hirte <johannes.hirte@datenkhaos.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yazen Ghannam <yazen.ghannam@amd.com>
    Link: http://lkml.kernel.org/r/20180414004230.GA2033@probook
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4cbe6caa4c6c3e9099d73160c59fe3c11ca5b3c5
Author: Borislav Petkov <bp@suse.de>
Date:   Thu May 17 10:46:26 2018 +0200

    x86/MCE/AMD: Cache SMCA MISC block addresses
    
    commit 78ce241099bb363b19dbd0245442e66c8de8f567 upstream.
    
    ... into a global, two-dimensional array and service subsequent reads from
    that cache to avoid rdmsr_on_cpu() calls during CPU hotplug (IPIs with IRQs
    disabled).
    
    In addition, this fixes a KASAN slab-out-of-bounds read due to wrong usage
    of the bank->blocks pointer.
    
    Fixes: 27bd59502702 ("x86/mce/AMD: Get address from already initialized block")
    Reported-by: Johannes Hirte <johannes.hirte@datenkhaos.de>
    Tested-by: Johannes Hirte <johannes.hirte@datenkhaos.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yazen Ghannam <yazen.ghannam@amd.com>
    Link: http://lkml.kernel.org/r/20180414004230.GA2033@probook
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0ef283d4c7808cb264f904de5e29a0b661747fc4
Merge: db020be9f7a0 fbf96cf904dc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 20:26:07 2018 -0700

    Merge branch 'ras-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 RAS updates from Thomas Gleixner:
    
     - Fix a stack out of bounds write in the MCE error injection code.
    
     - Avoid IPIs during CPU hotplug to read the MCx_MISC block address from
       a remote CPU. That's fragile and pointless because the block
       addresses are the same on all CPUs. So they can be read once and
       local.
    
     - Add support for MCE broadcasting on newer VIA Centaur CPUs.
    
    * 'ras-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/MCE/AMD: Read MCx_MISC block addresses on any CPU
      x86/MCE: Fix stack out-of-bounds write in mce-inject.c: Flags_read()
      x86/MCE: Enable MCE broadcasting on new Centaur CPUs

commit dff5da4724bb13687b65072ae5347c0b3f84326f
Author: Samuel Neves <sneves@dei.uc.pt>
Date:   Wed Feb 21 20:50:36 2018 +0000

    x86/topology: Update the 'cpu cores' field in /proc/cpuinfo correctly across CPU hotplug operations
    
    [ Upstream commit 4596749339e06dc7a424fc08a15eded850ed78b7 ]
    
    Without this fix, /proc/cpuinfo will display an incorrect amount
    of CPU cores, after bringing them offline and online again, as
    exemplified below:
    
      $ cat /proc/cpuinfo | grep cores
      cpu cores     : 4
      cpu cores     : 8
      cpu cores     : 8
      cpu cores     : 20
      cpu cores     : 4
      cpu cores     : 3
      cpu cores     : 2
      cpu cores     : 2
    
    This patch fixes this by always zeroing the booted_cores variable
    upon turning off a logical CPU.
    
    Tested-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Samuel Neves <sneves@dei.uc.pt>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: jgross@suse.com
    Cc: luto@kernel.org
    Cc: prarit@redhat.com
    Cc: vkuznets@redhat.com
    Link: http://lkml.kernel.org/r/20180221205036.5244-1-sneves@dei.uc.pt
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b8421da4b57b47dfb9ee9aacfb2612bb188f5aa5
Author: Samuel Neves <sneves@dei.uc.pt>
Date:   Wed Feb 21 20:50:36 2018 +0000

    x86/topology: Update the 'cpu cores' field in /proc/cpuinfo correctly across CPU hotplug operations
    
    [ Upstream commit 4596749339e06dc7a424fc08a15eded850ed78b7 ]
    
    Without this fix, /proc/cpuinfo will display an incorrect amount
    of CPU cores, after bringing them offline and online again, as
    exemplified below:
    
      $ cat /proc/cpuinfo | grep cores
      cpu cores     : 4
      cpu cores     : 8
      cpu cores     : 8
      cpu cores     : 20
      cpu cores     : 4
      cpu cores     : 3
      cpu cores     : 2
      cpu cores     : 2
    
    This patch fixes this by always zeroing the booted_cores variable
    upon turning off a logical CPU.
    
    Tested-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Samuel Neves <sneves@dei.uc.pt>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: jgross@suse.com
    Cc: luto@kernel.org
    Cc: prarit@redhat.com
    Cc: vkuznets@redhat.com
    Link: http://lkml.kernel.org/r/20180221205036.5244-1-sneves@dei.uc.pt
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2f2f95d96b8e59e29701621f94354325479cd91e
Author: Michael Bringmann <mwb@linux.vnet.ibm.com>
Date:   Tue Nov 28 16:58:40 2017 -0600

    powerpc/numa: Ensure nodes initialized for hotplug
    
    [ Upstream commit ea05ba7c559c8e5a5946c3a94a2a266e9a6680a6 ]
    
    This patch fixes some problems encountered at runtime with
    configurations that support memory-less nodes, or that hot-add CPUs
    into nodes that are memoryless during system execution after boot. The
    problems of interest include:
    
    * Nodes known to powerpc to be memoryless at boot, but to have CPUs in
      them are allowed to be 'possible' and 'online'. Memory allocations
      for those nodes are taken from another node that does have memory
      until and if memory is hot-added to the node.
    
    * Nodes which have no resources assigned at boot, but which may still
      be referenced subsequently by affinity or associativity attributes,
      are kept in the list of 'possible' nodes for powerpc. Hot-add of
      memory or CPUs to the system can reference these nodes and bring
      them online instead of redirecting the references to one of the set
      of nodes known to have memory at boot.
    
    Note that this software operates under the context of CPU hotplug. We
    are not doing memory hotplug in this code, but rather updating the
    kernel's CPU topology (i.e. arch_update_cpu_topology /
    numa_update_cpu_topology). We are initializing a node that may be used
    by CPUs or memory before it can be referenced as invalid by a CPU
    hotplug operation. CPU hotplug operations are protected by a range of
    APIs including cpu_maps_update_begin/cpu_maps_update_done,
    cpus_read/write_lock / cpus_read/write_unlock, device locks, and more.
    Memory hotplug operations, including try_online_node, are protected by
    mem_hotplug_begin/mem_hotplug_done, device locks, and more. In the
    case of CPUs being hot-added to a previously memoryless node, the
    try_online_node operation occurs wholly within the CPU locks with no
    overlap. Using HMC hot-add/hot-remove operations, we have been able to
    add and remove CPUs to any possible node without failures. HMC
    operations involve a degree self-serialization, though.
    
    Signed-off-by: Michael Bringmann <mwb@linux.vnet.ibm.com>
    Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 24cdc06fcf9dc32243a7e2693e3df3336ec5a7ef
Author: Samuel Neves <sneves@dei.uc.pt>
Date:   Wed Feb 21 20:50:36 2018 +0000

    x86/topology: Update the 'cpu cores' field in /proc/cpuinfo correctly across CPU hotplug operations
    
    [ Upstream commit 4596749339e06dc7a424fc08a15eded850ed78b7 ]
    
    Without this fix, /proc/cpuinfo will display an incorrect amount
    of CPU cores, after bringing them offline and online again, as
    exemplified below:
    
      $ cat /proc/cpuinfo | grep cores
      cpu cores     : 4
      cpu cores     : 8
      cpu cores     : 8
      cpu cores     : 20
      cpu cores     : 4
      cpu cores     : 3
      cpu cores     : 2
      cpu cores     : 2
    
    This patch fixes this by always zeroing the booted_cores variable
    upon turning off a logical CPU.
    
    Tested-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Samuel Neves <sneves@dei.uc.pt>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: jgross@suse.com
    Cc: luto@kernel.org
    Cc: prarit@redhat.com
    Cc: vkuznets@redhat.com
    Link: http://lkml.kernel.org/r/20180221205036.5244-1-sneves@dei.uc.pt
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b4e84e5aed7680dedd44727bcf8ed7fdbd409b86
Author: Michael Bringmann <mwb@linux.vnet.ibm.com>
Date:   Tue Nov 28 16:58:40 2017 -0600

    powerpc/numa: Ensure nodes initialized for hotplug
    
    [ Upstream commit ea05ba7c559c8e5a5946c3a94a2a266e9a6680a6 ]
    
    This patch fixes some problems encountered at runtime with
    configurations that support memory-less nodes, or that hot-add CPUs
    into nodes that are memoryless during system execution after boot. The
    problems of interest include:
    
    * Nodes known to powerpc to be memoryless at boot, but to have CPUs in
      them are allowed to be 'possible' and 'online'. Memory allocations
      for those nodes are taken from another node that does have memory
      until and if memory is hot-added to the node.
    
    * Nodes which have no resources assigned at boot, but which may still
      be referenced subsequently by affinity or associativity attributes,
      are kept in the list of 'possible' nodes for powerpc. Hot-add of
      memory or CPUs to the system can reference these nodes and bring
      them online instead of redirecting the references to one of the set
      of nodes known to have memory at boot.
    
    Note that this software operates under the context of CPU hotplug. We
    are not doing memory hotplug in this code, but rather updating the
    kernel's CPU topology (i.e. arch_update_cpu_topology /
    numa_update_cpu_topology). We are initializing a node that may be used
    by CPUs or memory before it can be referenced as invalid by a CPU
    hotplug operation. CPU hotplug operations are protected by a range of
    APIs including cpu_maps_update_begin/cpu_maps_update_done,
    cpus_read/write_lock / cpus_read/write_unlock, device locks, and more.
    Memory hotplug operations, including try_online_node, are protected by
    mem_hotplug_begin/mem_hotplug_done, device locks, and more. In the
    case of CPUs being hot-added to a previously memoryless node, the
    try_online_node operation occurs wholly within the CPU locks with no
    overlap. Using HMC hot-add/hot-remove operations, we have been able to
    add and remove CPUs to any possible node without failures. HMC
    operations involve a degree self-serialization, though.
    
    Signed-off-by: Michael Bringmann <mwb@linux.vnet.ibm.com>
    Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 836053fdf00c8294e70ed69802b503220544b6de
Author: Samuel Neves <sneves@dei.uc.pt>
Date:   Wed Feb 21 20:50:36 2018 +0000

    x86/topology: Update the 'cpu cores' field in /proc/cpuinfo correctly across CPU hotplug operations
    
    [ Upstream commit 4596749339e06dc7a424fc08a15eded850ed78b7 ]
    
    Without this fix, /proc/cpuinfo will display an incorrect amount
    of CPU cores, after bringing them offline and online again, as
    exemplified below:
    
      $ cat /proc/cpuinfo | grep cores
      cpu cores     : 4
      cpu cores     : 8
      cpu cores     : 8
      cpu cores     : 20
      cpu cores     : 4
      cpu cores     : 3
      cpu cores     : 2
      cpu cores     : 2
    
    This patch fixes this by always zeroing the booted_cores variable
    upon turning off a logical CPU.
    
    Tested-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Samuel Neves <sneves@dei.uc.pt>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: jgross@suse.com
    Cc: luto@kernel.org
    Cc: prarit@redhat.com
    Cc: vkuznets@redhat.com
    Link: http://lkml.kernel.org/r/20180221205036.5244-1-sneves@dei.uc.pt
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 071290f8c52b301fc9aaaae7db579fcb9d3edddb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:36 2017 +0200

    cpufreq/sh: Replace racy task affinity logic
    
    [ Upstream commit 205dcc1ecbc566cbc20acf246e68de3b080b3ecf ]
    
    The target() callback must run on the affected cpu. This is achieved by
    temporarily setting the affinity of the calling thread to the requested CPU
    and reset it to the original affinity afterwards.
    
    That's racy vs. concurrent affinity settings for that thread resulting in
    code executing on the wrong CPU.
    
    Replace it by work_on_cpu(). All call pathes which invoke the callbacks are
    already protected against CPU hotplug.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: linux-pm@vger.kernel.org
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.958216363@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>

commit 39596a8023eb6b544f803b4a5e916625aaa7176c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:34 2017 +0200

    ACPI/processor: Replace racy task affinity logic
    
    [ Upstream commit 8153f9ac43897f9f4786b30badc134fcc1a4fb11 ]
    
    acpi_processor_get_throttling() requires to invoke the getter function on
    the target CPU. This is achieved by temporarily setting the affinity of the
    calling user space thread to the requested CPU and reset it to the original
    affinity afterwards.
    
    That's racy vs. CPU hotplug and concurrent affinity settings for that
    thread resulting in code executing on the wrong CPU and overwriting the
    new affinity setting.
    
    acpi_processor_get_throttling() is invoked in two ways:
    
    1) The CPU online callback, which is already running on the target CPU and
       obviously protected against hotplug and not affected by affinity
       settings.
    
    2) The ACPI driver probe function, which is not protected against hotplug
       during modprobe.
    
    Switch it over to work_on_cpu() and protect the probe function against CPU
    hotplug.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: linux-acpi@vger.kernel.org
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.785920903@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>

commit 74cce52f9f92b64febd7c84ed68f5a5607c779b6
Merge: 95bcce4d4240 78ce241099bb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 20 11:20:40 2018 -0700

    Merge branch 'ras-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RAS fix from Thomas Gleixner:
     "Fix a regression in the new AMD SMCA code which issues an SMP function
      call from the early interrupt disabled region of CPU hotplug. To avoid
      that, use cached block addresses which can be used directly"
    
    * 'ras-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/MCE/AMD: Cache SMCA MISC block addresses

commit fbf96cf904dc154a28338fe68f72902e9af57afc
Author: Borislav Petkov <bp@suse.de>
Date:   Thu May 17 18:32:33 2018 +0200

    x86/MCE/AMD: Read MCx_MISC block addresses on any CPU
    
    We used rdmsr_safe_on_cpu() to make sure we're reading the proper CPU's
    MISC block addresses. However, that caused trouble with CPU hotplug due to
    the _on_cpu() helper issuing an IPI while IRQs are disabled.
    
    But we don't have to do that: the block addresses are the same on any CPU
    so we can read them on any CPU. (What practically happens is, we read them
    on the BSP and cache them, and for later reads, we service them from the
    cache).
    
    Suggested-by: Yazen Ghannam <Yazen.Ghannam@amd.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 78ce241099bb363b19dbd0245442e66c8de8f567
Author: Borislav Petkov <bp@suse.de>
Date:   Thu May 17 10:46:26 2018 +0200

    x86/MCE/AMD: Cache SMCA MISC block addresses
    
    ... into a global, two-dimensional array and service subsequent reads from
    that cache to avoid rdmsr_on_cpu() calls during CPU hotplug (IPIs with IRQs
    disabled).
    
    In addition, this fixes a KASAN slab-out-of-bounds read due to wrong usage
    of the bank->blocks pointer.
    
    Fixes: 27bd59502702 ("x86/mce/AMD: Get address from already initialized block")
    Reported-by: Johannes Hirte <johannes.hirte@datenkhaos.de>
    Tested-by: Johannes Hirte <johannes.hirte@datenkhaos.de>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yazen Ghannam <yazen.ghannam@amd.com>
    Link: http://lkml.kernel.org/r/20180414004230.GA2033@probook

commit 98f1b5a762f7bee20785ed576630386ddcc62483
Author: Grygorii Strashko <grygorii.strashko@ti.com>
Date:   Tue May 8 15:19:40 2018 +0100

    ARM: 8765/1: smp: Move clear_tasks_mm_cpumask() call to __cpu_die()
    
    Suspending a CPU on a RT kernel results in the following backtrace:
    
    | Disabling non-boot CPUs ...
    | BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:917
    | in_atomic(): 1, irqs_disabled(): 128, pid: 18, name: migration/1
    | INFO: lockdep is turned off.
    | irq event stamp: 122
    | hardirqs last  enabled at (121): [<c06ac0ac>] _raw_spin_unlock_irqrestore+0x88/0x90
    | hardirqs last disabled at (122): [<c06abed0>] _raw_spin_lock_irq+0x28/0x5c
    |  CPU: 1 PID: 18 Comm: migration/1 Tainted: G        W       4.1.4-rt3-01046-g96ac8da #204
    | Hardware name: Generic DRA74X (Flattened Device Tree)
    | [<c0019134>] (unwind_backtrace) from [<c0014774>] (show_stack+0x20/0x24)
    | [<c0014774>] (show_stack) from [<c06a70f4>] (dump_stack+0x88/0xdc)
    | [<c06a70f4>] (dump_stack) from [<c006cab8>] (___might_sleep+0x198/0x2a8)
    | [<c006cab8>] (___might_sleep) from [<c06ac4dc>] (rt_spin_lock+0x30/0x70)
    | [<c06ac4dc>] (rt_spin_lock) from [<c013f790>] (find_lock_task_mm+0x9c/0x174)
    | [<c013f790>] (find_lock_task_mm) from [<c00409ac>] (clear_tasks_mm_cpumask+0xb4/0x1ac)
    | [<c00409ac>] (clear_tasks_mm_cpumask) from [<c00166a4>] (__cpu_disable+0x98/0xbc)
    | [<c00166a4>] (__cpu_disable) from [<c06a2e8c>] (take_cpu_down+0x1c/0x50)
    | [<c06a2e8c>] (take_cpu_down) from [<c00f2600>] (multi_cpu_stop+0x11c/0x158)
    | [<c00f2600>] (multi_cpu_stop) from [<c00f2a9c>] (cpu_stopper_thread+0xc4/0x184)
    | [<c00f2a9c>] (cpu_stopper_thread) from [<c0069058>] (smpboot_thread_fn+0x18c/0x324)
    | [<c0069058>] (smpboot_thread_fn) from [<c00649c4>] (kthread+0xe8/0x104)
    | [<c00649c4>] (kthread) from [<c0010058>] (ret_from_fork+0x14/0x3c)
    | CPU1: shutdown
    
    The root cause of above backtrace is task_lock() which takes a sleeping
    lock on -RT.
    
    To fix the issue, move clear_tasks_mm_cpumask() call from __cpu_disable()
    to __cpu_die() which is called on the thread which is asking for a target
    CPU to be shutdown. In addition, this change restores CPU hotplug
    functionality on ARM CPU1 can be unplugged/plugged many times.
    
    Link: http://lkml.kernel.org/r/1441995683-30817-1-git-send-email-grygorii.strashko@ti.com
    [bigeasy: slighty edited the commit message]
    
    Signed-off-by: Grygorii Strashko <grygorii.strashko@ti.com>
    Cc: <linux-arm-kernel@lists.infradead.org>
    Cc: Sekhar Nori <nsekhar@ti.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

commit 8b578325b805fad03a4e56528f767197df20de69
Author: Mylne Josserand <mylene.josserand@bootlin.com>
Date:   Fri May 4 21:05:36 2018 +0200

    ARM: dts: sun8i: Add CPUCFG device node for A83T dtsi
    
    As we found in sun9i-a80, CPUCFG is a collection of registers that are
    mapped to the SoC's signals from each individual processor core and
    associated peripherals.
    
    These registers are used for SMP bringup and CPU hotplugging.
    
    Signed-off-by: Mylne Josserand <mylene.josserand@bootlin.com>
    Reviewed-by: Chen-Yu Tsai <wens@csie.org>
    Signed-off-by: Maxime Ripard <maxime.ripard@bootlin.com>

commit c11a6ed509f086efebc52ad4fd8713e7acfbc076
Author: Borislav Petkov <bp@suse.de>
Date:   Sat Apr 21 10:19:29 2018 +0200

    x86/microcode/intel: Save microcode patch unconditionally
    
    commit 84749d83758af6576552046b215b9b7f37f9556b upstream.
    
    save_mc_for_early() was a no-op on !CONFIG_HOTPLUG_CPU but the
    generic_load_microcode() path saves the microcode patches it has found into
    the cache of patches which is used for late loading too. Regardless of
    whether CPU hotplug is used or not.
    
    Make the saving unconditional so that late loading can find the proper
    patch.
    
    Reported-by: Vitezslav Samel <vitezslav@samel.cz>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Vitezslav Samel <vitezslav@samel.cz>
    Tested-by: Ashok Raj <ashok.raj@intel.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/20180418081140.GA2439@pc11.op.pod.cz
    Link: https://lkml.kernel.org/r/20180421081930.15741-1-bp@alien8.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7c6bcaac737fa72dd8aef00cb38b9c96b9b04cd8
Author: Borislav Petkov <bp@suse.de>
Date:   Sat Apr 21 10:19:29 2018 +0200

    x86/microcode/intel: Save microcode patch unconditionally
    
    commit 84749d83758af6576552046b215b9b7f37f9556b upstream.
    
    save_mc_for_early() was a no-op on !CONFIG_HOTPLUG_CPU but the
    generic_load_microcode() path saves the microcode patches it has found into
    the cache of patches which is used for late loading too. Regardless of
    whether CPU hotplug is used or not.
    
    Make the saving unconditional so that late loading can find the proper
    patch.
    
    Reported-by: Vitezslav Samel <vitezslav@samel.cz>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Vitezslav Samel <vitezslav@samel.cz>
    Tested-by: Ashok Raj <ashok.raj@intel.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/20180418081140.GA2439@pc11.op.pod.cz
    Link: https://lkml.kernel.org/r/20180421081930.15741-1-bp@alien8.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 166e0b4343baa1d3bf27ef1649d8f58c589af5b1
Author: Borislav Petkov <bp@suse.de>
Date:   Sat Apr 21 10:19:29 2018 +0200

    x86/microcode/intel: Save microcode patch unconditionally
    
    commit 84749d83758af6576552046b215b9b7f37f9556b upstream.
    
    save_mc_for_early() was a no-op on !CONFIG_HOTPLUG_CPU but the
    generic_load_microcode() path saves the microcode patches it has found into
    the cache of patches which is used for late loading too. Regardless of
    whether CPU hotplug is used or not.
    
    Make the saving unconditional so that late loading can find the proper
    patch.
    
    Reported-by: Vitezslav Samel <vitezslav@samel.cz>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Vitezslav Samel <vitezslav@samel.cz>
    Tested-by: Ashok Raj <ashok.raj@intel.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/20180418081140.GA2439@pc11.op.pod.cz
    Link: https://lkml.kernel.org/r/20180421081930.15741-1-bp@alien8.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0bddd43ac2001d87471117ff29e789aa3bcfd18b
Author: Michael Bringmann <mwb@linux.vnet.ibm.com>
Date:   Tue Nov 28 16:58:40 2017 -0600

    powerpc/numa: Ensure nodes initialized for hotplug
    
    
    [ Upstream commit ea05ba7c559c8e5a5946c3a94a2a266e9a6680a6 ]
    
    This patch fixes some problems encountered at runtime with
    configurations that support memory-less nodes, or that hot-add CPUs
    into nodes that are memoryless during system execution after boot. The
    problems of interest include:
    
    * Nodes known to powerpc to be memoryless at boot, but to have CPUs in
      them are allowed to be 'possible' and 'online'. Memory allocations
      for those nodes are taken from another node that does have memory
      until and if memory is hot-added to the node.
    
    * Nodes which have no resources assigned at boot, but which may still
      be referenced subsequently by affinity or associativity attributes,
      are kept in the list of 'possible' nodes for powerpc. Hot-add of
      memory or CPUs to the system can reference these nodes and bring
      them online instead of redirecting the references to one of the set
      of nodes known to have memory at boot.
    
    Note that this software operates under the context of CPU hotplug. We
    are not doing memory hotplug in this code, but rather updating the
    kernel's CPU topology (i.e. arch_update_cpu_topology /
    numa_update_cpu_topology). We are initializing a node that may be used
    by CPUs or memory before it can be referenced as invalid by a CPU
    hotplug operation. CPU hotplug operations are protected by a range of
    APIs including cpu_maps_update_begin/cpu_maps_update_done,
    cpus_read/write_lock / cpus_read/write_unlock, device locks, and more.
    Memory hotplug operations, including try_online_node, are protected by
    mem_hotplug_begin/mem_hotplug_done, device locks, and more. In the
    case of CPUs being hot-added to a previously memoryless node, the
    try_online_node operation occurs wholly within the CPU locks with no
    overlap. Using HMC hot-add/hot-remove operations, we have been able to
    add and remove CPUs to any possible node without failures. HMC
    operations involve a degree self-serialization, though.
    
    Signed-off-by: Michael Bringmann <mwb@linux.vnet.ibm.com>
    Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 84749d83758af6576552046b215b9b7f37f9556b
Author: Borislav Petkov <bp@suse.de>
Date:   Sat Apr 21 10:19:29 2018 +0200

    x86/microcode/intel: Save microcode patch unconditionally
    
    save_mc_for_early() was a no-op on !CONFIG_HOTPLUG_CPU but the
    generic_load_microcode() path saves the microcode patches it has found into
    the cache of patches which is used for late loading too. Regardless of
    whether CPU hotplug is used or not.
    
    Make the saving unconditional so that late loading can find the proper
    patch.
    
    Reported-by: Vitezslav Samel <vitezslav@samel.cz>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Vitezslav Samel <vitezslav@samel.cz>
    Tested-by: Ashok Raj <ashok.raj@intel.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/20180418081140.GA2439@pc11.op.pod.cz
    Link: https://lkml.kernel.org/r/20180421081930.15741-1-bp@alien8.de

commit 37a712b4ddf012dec426b3dd2a13d5a17dd3820f
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 11 15:17:59 2018 +1000

    powerpc/xive: Fix trying to "push" an already active pool VP
    
    commit b32e56e5a87a1f9243db92bc7a5df0ffb4627cfb upstream.
    
    When setting up a CPU, we "push" (activate) a pool VP for it.
    
    However it's an error to do so if it already has an active
    pool VP.
    
    This happens when doing soft CPU hotplug on powernv since we
    don't tear down the CPU on unplug. The HW flags the error which
    gets captured by the diagnostics.
    
    Fix this by making sure to "pull" out any already active pool
    first.
    
    Fixes: 243e25112d06 ("powerpc/xive: Native exploitation of the XIVE interrupt controller")
    Cc: stable@vger.kernel.org # v4.12+
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ce3b0b0589a8b7e0241c0d1b1049a715108b78fb
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 11 15:17:59 2018 +1000

    powerpc/xive: Fix trying to "push" an already active pool VP
    
    commit b32e56e5a87a1f9243db92bc7a5df0ffb4627cfb upstream.
    
    When setting up a CPU, we "push" (activate) a pool VP for it.
    
    However it's an error to do so if it already has an active
    pool VP.
    
    This happens when doing soft CPU hotplug on powernv since we
    don't tear down the CPU on unplug. The HW flags the error which
    gets captured by the diagnostics.
    
    Fix this by making sure to "pull" out any already active pool
    first.
    
    Fixes: 243e25112d06 ("powerpc/xive: Native exploitation of the XIVE interrupt controller")
    Cc: stable@vger.kernel.org # v4.12+
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b32e56e5a87a1f9243db92bc7a5df0ffb4627cfb
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 11 15:17:59 2018 +1000

    powerpc/xive: Fix trying to "push" an already active pool VP
    
    When setting up a CPU, we "push" (activate) a pool VP for it.
    
    However it's an error to do so if it already has an active
    pool VP.
    
    This happens when doing soft CPU hotplug on powernv since we
    don't tear down the CPU on unplug. The HW flags the error which
    gets captured by the diagnostics.
    
    Fix this by making sure to "pull" out any already active pool
    first.
    
    Fixes: 243e25112d06 ("powerpc/xive: Native exploitation of the XIVE interrupt controller")
    Cc: stable@vger.kernel.org # v4.12+
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit c198e227316337301f682c97c5880014b3aeb3aa
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 24 10:15:43 2017 +0200

    cpuhotplug: Link lock stacks for hotplug callbacks
    
    
    [ Upstream commit 49dfe2a6779717d9c18395684ee31bdc98b22e53 ]
    
    The CPU hotplug callbacks are not covered by lockdep versus the cpu hotplug
    rwsem.
    
    CPU0                                            CPU1
    cpuhp_setup_state(STATE, startup, teardown);
     cpus_read_lock();
      invoke_callback_on_ap();
        kick_hotplug_thread(ap);
        wait_for_completion();                      hotplug_thread_fn()
                                                      lock(m);
                                                      do_stuff();
                                                      unlock(m);
    
    Lockdep does not know about this dependency and will not trigger on the
    following code sequence:
    
              lock(m);
              cpus_read_lock();
    
    Add a lockdep map and connect the initiators lock chain with the hotplug
    thread lock chain, so potential deadlocks can be detected.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170524081549.709375845@linutronix.de
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 59162c936692ff064c12676e708c8ef82a82436d
Merge: 8650b9feb0d8 01d675f159e0
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Mar 27 15:45:03 2018 +0200

    Merge tag 'renesas-soc-for-v4.17' of ssh://gitolite.kernel.org/pub/scm/linux/kernel/git/horms/renesas into next/soc
    
    Pull "Renesas ARM Based SoC Updates for v4.17" from Simon Horman:
    
    01d675f159e0 ARM: shmobile: rcar-gen2: Add watchdog support
    58adf1ba0d22 ARM: shmobile: Add watchdog support
    
    * SoC
      - Identify R-Car V3H (r8a77980) and M3N (r8a77965)
    
      - Enable R-Car Gen2 regulator quirk for Stout board with H3 (r8a7790) SoC
    
        Marek Vaust says "Regulator setup is suboptimal on H2 Stout too. The
        Stout newly has two DA9210 regulators, so the quirk is extended to
        handle another DA9210 at i2c address 0x70."
    
      - Add watchdog support
    
        This is the SoC portion of the following solution. It is not yet
        enabled in DT as it is not functional without clock dependencies
        in place.
    
        Fabrizio Castro says "this series has been around for some time as RFC,
        and it has collected useful comments from the community along the way.
        The solution proposed by this patch set works for most R-Car Gen2 and
        RZ/G1 devices, but not all of them. We now know that for some R-Car
        Gen2 early revisions there is no proper software fix. Anyway, no
        product has been built around early revisions, but development boards
        mounting early revisions (basically prototypes) are still out there.
        As a result, this series isn't enabling the internal watchdog on R-Car
        Gen2 boards, developers may enable it in board specific device trees if
        needed.  This series has been tested by me on the iwg20d, iwg22d,
        Lager, Alt, and Koelsch boards.
    
       The problem
       ===========
       To deal with SMP on R-Car Gen2 and RZ/G1, we install a reset vector to
       ICRAM1 and we program the [S]BAR registers so that when we turn ON the
       non-boot CPUs they are redirected to the reset vector installed by Linux
       in ICRAM1, and eventually they continue the execution to RAM, where the
       SMP bring-up code will take care of the rest.  The content of the [S]BAR
       registers survives a watchdog triggered reset, and as such after the
       watchdog fires the boot core will try and execute the SMP bring-up code
       instead of jumping to the bootrom code.
    
       The fix
       =======
       The main strategy for the solution is to let the reset vector decide if
       it needs to jump to shmobile_boot_fn or to the bootrom code.  In a
       watchdog triggered reset scenario, since the [S]BAR registers keep their
       values, the boot CPU will jump into the newly designed reset vector, the
       assembly routine will eventually test WOVF (a bit in register RWTCSRA
       that indicates if the watchdog counter has overflown, the value of this
       bit gets retained in this scenario), and jump to the bootrom code which
       will in turn load up the bootloader, etc.  When bringing up SMP or using
       CPU hotplug, the reset vector will jump to shmobile_boot_fn instead."
    
    * R-Car Rst
      - Add support for R-Car V3H (r8a77980) and V3H (r8a77980)
    
    * R-Car SYSC
      - Mark rcar_sysc_matches[] __initconst
    
       Geert Uytterhoeven says "This frees another 1764 bytes
       (arm32/shmobile_defconfig) or 1000 bytes (arm64/renesas_defconfig) of
       memory after kernel init."
    
      - Fix power area parents
    
        Sergei Shtylyov says "According to the figure 9.2(b) of the R-Car
        Series, 3rd Generation Users Manual: Hardware Rev. 0.80 the A2IRn and
        A2SCn power areas in R8A77970 have the A3IR area as a parent, thus the
        SYSC driver has those parents wrong.."
    
      - Add support for R-Car V3H (r8a77980) and V3H (r8a77980)
    
    * tag 'renesas-soc-for-v4.17' of ssh://gitolite.kernel.org/pub/scm/linux/kernel/git/horms/renesas:
      ARM: shmobile: rcar-gen2: Add watchdog support
      ARM: shmobile: Add watchdog support
      ARM: shmobile: rcar-gen2: Fix error check in regulator quirk
      soc: renesas: rcar-rst: Add support for R-Car M3-N
      ARM: shmobile: stout: enable R-Car Gen2 regulator quirk
      soc: renesas: rcar-sysc: Add R-Car M3-N support
      soc: renesas: Identify R-Car M3-N
      soc: renesas: rcar-sysc: add R8A77980 support
      dt-bindings: power: add R8A77980 SYSC power domain definitions
      soc: renesas: r8a77970-sysc: fix power area parents
      soc: renesas: rcar-rst: Enable watchdog as reset trigger for Gen2
      soc: renesas: rcar-rst: add R8A77980 support
      soc: renesas: identify R-Car V3H
      soc: renesas: rcar-sysc: Mark rcar_sysc_matches[] __initconst

commit ade9f4ba0d7af1ee2a310f1dda96c9174e35202b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:36 2017 +0200

    cpufreq/sh: Replace racy task affinity logic
    
    
    [ Upstream commit 205dcc1ecbc566cbc20acf246e68de3b080b3ecf ]
    
    The target() callback must run on the affected cpu. This is achieved by
    temporarily setting the affinity of the calling thread to the requested CPU
    and reset it to the original affinity afterwards.
    
    That's racy vs. concurrent affinity settings for that thread resulting in
    code executing on the wrong CPU.
    
    Replace it by work_on_cpu(). All call pathes which invoke the callbacks are
    already protected against CPU hotplug.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: linux-pm@vger.kernel.org
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.958216363@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b7e5f1a204e1209c6abeda71956b58f0f6ee21fe
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:34 2017 +0200

    ACPI/processor: Replace racy task affinity logic
    
    
    [ Upstream commit 8153f9ac43897f9f4786b30badc134fcc1a4fb11 ]
    
    acpi_processor_get_throttling() requires to invoke the getter function on
    the target CPU. This is achieved by temporarily setting the affinity of the
    calling user space thread to the requested CPU and reset it to the original
    affinity afterwards.
    
    That's racy vs. CPU hotplug and concurrent affinity settings for that
    thread resulting in code executing on the wrong CPU and overwriting the
    new affinity setting.
    
    acpi_processor_get_throttling() is invoked in two ways:
    
    1) The CPU online callback, which is already running on the target CPU and
       obviously protected against hotplug and not affected by affinity
       settings.
    
    2) The ACPI driver probe function, which is not protected against hotplug
       during modprobe.
    
    Switch it over to work_on_cpu() and protect the probe function against CPU
    hotplug.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: linux-acpi@vger.kernel.org
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.785920903@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 88a056441da87d815f0dbfd80d063b941d430371
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:36 2017 +0200

    cpufreq/sh: Replace racy task affinity logic
    
    
    [ Upstream commit 205dcc1ecbc566cbc20acf246e68de3b080b3ecf ]
    
    The target() callback must run on the affected cpu. This is achieved by
    temporarily setting the affinity of the calling thread to the requested CPU
    and reset it to the original affinity afterwards.
    
    That's racy vs. concurrent affinity settings for that thread resulting in
    code executing on the wrong CPU.
    
    Replace it by work_on_cpu(). All call pathes which invoke the callbacks are
    already protected against CPU hotplug.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: linux-pm@vger.kernel.org
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.958216363@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f1a0a872d0e308574187f1b6e89f89d290893897
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:34 2017 +0200

    ACPI/processor: Replace racy task affinity logic
    
    
    [ Upstream commit 8153f9ac43897f9f4786b30badc134fcc1a4fb11 ]
    
    acpi_processor_get_throttling() requires to invoke the getter function on
    the target CPU. This is achieved by temporarily setting the affinity of the
    calling user space thread to the requested CPU and reset it to the original
    affinity afterwards.
    
    That's racy vs. CPU hotplug and concurrent affinity settings for that
    thread resulting in code executing on the wrong CPU and overwriting the
    new affinity setting.
    
    acpi_processor_get_throttling() is invoked in two ways:
    
    1) The CPU online callback, which is already running on the target CPU and
       obviously protected against hotplug and not affected by affinity
       settings.
    
    2) The ACPI driver probe function, which is not protected against hotplug
       during modprobe.
    
    Switch it over to work_on_cpu() and protect the probe function against CPU
    hotplug.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: linux-acpi@vger.kernel.org
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.785920903@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fad89533e0d5067d38ffd119b4eee5e19d945821
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:36 2017 +0200

    cpufreq/sh: Replace racy task affinity logic
    
    
    [ Upstream commit 205dcc1ecbc566cbc20acf246e68de3b080b3ecf ]
    
    The target() callback must run on the affected cpu. This is achieved by
    temporarily setting the affinity of the calling thread to the requested CPU
    and reset it to the original affinity afterwards.
    
    That's racy vs. concurrent affinity settings for that thread resulting in
    code executing on the wrong CPU.
    
    Replace it by work_on_cpu(). All call pathes which invoke the callbacks are
    already protected against CPU hotplug.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: linux-pm@vger.kernel.org
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.958216363@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2d2cfeb58d8c120a95150006bc39063db2b7ac54
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:34 2017 +0200

    ACPI/processor: Replace racy task affinity logic
    
    
    [ Upstream commit 8153f9ac43897f9f4786b30badc134fcc1a4fb11 ]
    
    acpi_processor_get_throttling() requires to invoke the getter function on
    the target CPU. This is achieved by temporarily setting the affinity of the
    calling user space thread to the requested CPU and reset it to the original
    affinity afterwards.
    
    That's racy vs. CPU hotplug and concurrent affinity settings for that
    thread resulting in code executing on the wrong CPU and overwriting the
    new affinity setting.
    
    acpi_processor_get_throttling() is invoked in two ways:
    
    1) The CPU online callback, which is already running on the target CPU and
       obviously protected against hotplug and not affected by affinity
       settings.
    
    2) The ACPI driver probe function, which is not protected against hotplug
       during modprobe.
    
    Switch it over to work_on_cpu() and protect the probe function against CPU
    hotplug.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: linux-acpi@vger.kernel.org
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.785920903@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sasha Levin <alexander.levin@microsoft.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8591743f12636a247bdab56c8c355ea9686f2d97
Author: Chen-Yu Tsai <wens@csie.org>
Date:   Thu Mar 8 23:00:10 2018 +0800

    ARM: sunxi: mc-smp: Use DT enable-method for sun9i A80 SMP
    
    Instead of having an early init function check the machine compatible
    and installing multi-cluster SMP support for the A80 if it matches,
    use a new cpu enable-method string. This makes the platform support
    future proof in case anyone manages to add PSCI support.
    
    The original init code for the SMP support is changed into the
    .prepare_cpus callback in the smp_operations structure. Instead of
    panicing when resources are missing like on some platforms, our code
    merely guards against engaging SMP or CPU hotplug and returns an error.
    
    Acked-by: Maxime Ripard <maxime.ripard@bootlin.com>
    Reviewed-by: Rob Herring <robh@kernel.org>
    Signed-off-by: Chen-Yu Tsai <wens@csie.org>

commit 03057f2626e955ebea88a668a6d7d699f836e5c0
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Thu Feb 15 18:51:43 2018 +0000

    perf/arm-cci: Simplify CPU hotplug
    
    Realistically, systems with multiple CCIs are unlikely to ever exist,
    and since the driver only actually supports a single instance anyway
    there's really no need to do the multi-instance hotplug state dance.
    
    Take the opportunity to simplify the hotplug-related code all over,
    addressing the context-migration TODO in the process for good measure.
    
    Acked-by: Punit Agrawal <punit.agrawal@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 491b0fc4001bc7fd2383e6587b623a7506d93e66
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jan 26 14:54:32 2018 +0100

    hrtimer: Reset hrtimer cpu base proper on CPU hotplug
    
    commit d5421ea43d30701e03cadc56a38854c36a8b4433 upstream.
    
    The hrtimer interrupt code contains a hang detection and mitigation
    mechanism, which prevents that a long delayed hrtimer interrupt causes a
    continous retriggering of interrupts which prevent the system from making
    progress. If a hang is detected then the timer hardware is programmed with
    a certain delay into the future and a flag is set in the hrtimer cpu base
    which prevents newly enqueued timers from reprogramming the timer hardware
    prior to the chosen delay. The subsequent hrtimer interrupt after the delay
    clears the flag and resumes normal operation.
    
    If such a hang happens in the last hrtimer interrupt before a CPU is
    unplugged then the hang_detected flag is set and stays that way when the
    CPU is plugged in again. At that point the timer hardware is not armed and
    it cannot be armed because the hang_detected flag is still active, so
    nothing clears that flag. As a consequence the CPU does not receive hrtimer
    interrupts and no timers expire on that CPU which results in RCU stalls and
    other malfunctions.
    
    Clear the flag along with some other less critical members of the hrtimer
    cpu base to ensure starting from a clean state when a CPU is plugged in.
    
    Thanks to Paul, Sebastian and Anna-Maria for their help to get down to the
    root cause of that hard to reproduce heisenbug. Once understood it's
    trivial and certainly justifies a brown paperbag.
    
    Fixes: 41d2e4949377 ("hrtimer: Tune hrtimer_interrupt hang logic")
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Sewior <bigeasy@linutronix.de>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1801261447590.2067@nanos
    [bwh: Backported to 3.16:
     - There's no next_timer field to reset
     - Adjust filename, context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit d7b68bb22c750e91ae16f8a539bd67c09abf3fba
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jan 26 14:54:32 2018 +0100

    hrtimer: Reset hrtimer cpu base proper on CPU hotplug
    
    commit d5421ea43d30701e03cadc56a38854c36a8b4433 upstream.
    
    The hrtimer interrupt code contains a hang detection and mitigation
    mechanism, which prevents that a long delayed hrtimer interrupt causes a
    continous retriggering of interrupts which prevent the system from making
    progress. If a hang is detected then the timer hardware is programmed with
    a certain delay into the future and a flag is set in the hrtimer cpu base
    which prevents newly enqueued timers from reprogramming the timer hardware
    prior to the chosen delay. The subsequent hrtimer interrupt after the delay
    clears the flag and resumes normal operation.
    
    If such a hang happens in the last hrtimer interrupt before a CPU is
    unplugged then the hang_detected flag is set and stays that way when the
    CPU is plugged in again. At that point the timer hardware is not armed and
    it cannot be armed because the hang_detected flag is still active, so
    nothing clears that flag. As a consequence the CPU does not receive hrtimer
    interrupts and no timers expire on that CPU which results in RCU stalls and
    other malfunctions.
    
    Clear the flag along with some other less critical members of the hrtimer
    cpu base to ensure starting from a clean state when a CPU is plugged in.
    
    Thanks to Paul, Sebastian and Anna-Maria for their help to get down to the
    root cause of that hard to reproduce heisenbug. Once understood it's
    trivial and certainly justifies a brown paperbag.
    
    Fixes: 41d2e4949377 ("hrtimer: Tune hrtimer_interrupt hang logic")
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Sewior <bigeasy@linutronix.de>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1801261447590.2067@nanos
    [bwh: Backported to 3.2:
     - There's no next_timer field to reset
     - Adjust filename, context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit b08e5fd90bfc7553d36fa42a03fb7f5e82d252eb
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Feb 26 16:10:56 2018 +0000

    arm_pmu: Use disable_irq_nosync when disabling SPI in CPU teardown hook
    
    Commit 6de3f79112cc ("arm_pmu: explicitly enable/disable SPIs at hotplug")
    moved all of the arm_pmu IRQ enable/disable calls to the CPU hotplug hooks,
    regardless of whether they are implemented as PPIs or SPIs. This can
    lead to us sleeping from atomic context due to disable_irq blocking:
    
     | BUG: sleeping function called from invalid context at kernel/irq/manage.c:112
     | in_atomic(): 1, irqs_disabled(): 128, pid: 15, name: migration/1
     | no locks held by migration/1/15.
     | irq event stamp: 192
     | hardirqs last  enabled at (191): [<00000000803c2507>]
     | _raw_spin_unlock_irq+0x2c/0x4c
     | hardirqs last disabled at (192): [<000000007f57ad28>] multi_cpu_stop+0x9c/0x140
     | softirqs last  enabled at (0): [<0000000004ee1b58>]
     | copy_process.isra.77.part.78+0x43c/0x1504
     | softirqs last disabled at (0): [<          (null)>]           (null)
     | CPU: 1 PID: 15 Comm: migration/1 Not tainted 4.16.0-rc3-salvator-x #1651
     | Hardware name: Renesas Salvator-X board based on r8a7796 (DT)
     | Call trace:
     |  dump_backtrace+0x0/0x140
     |  show_stack+0x14/0x1c
     |  dump_stack+0xb4/0xf0
     |  ___might_sleep+0x1fc/0x218
     |  __might_sleep+0x70/0x80
     |  synchronize_irq+0x40/0xa8
     |  disable_irq+0x20/0x2c
     |  arm_perf_teardown_cpu+0x80/0xac
    
    Since the interrupt is always CPU-affine and this code is running with
    interrupts disabled, we can just use disable_irq_nosync as we know there
    isn't a concurrent invocation of the handler to worry about.
    
    Fixes: 6de3f79112cc ("arm_pmu: explicitly enable/disable SPIs at hotplug")
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

commit 7f55f13e7c96ca31525d6b82465b45ab63a4e77d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 22 12:08:05 2018 +0100

    genirq/matrix: Handle CPU offlining proper
    
    commit 651ca2c00405a2ae3870cc0b4f15a182eb6fbe26 upstream.
    
    At CPU hotunplug the corresponding per cpu matrix allocator is shut down and
    the allocated interrupt bits are discarded under the assumption that all
    allocated bits have been either migrated away or shut down through the
    managed interrupts mechanism.
    
    This is not true because interrupts which are not started up might have a
    vector allocated on the outgoing CPU. When the interrupt is started up
    later or completely shutdown and freed then the allocated vector is handed
    back, triggering warnings or causing accounting issues which result in
    suspend failures and other issues.
    
    Change the CPU hotplug mechanism of the matrix allocator so that the
    remaining allocations at unplug time are preserved and global accounting at
    hotplug is correctly readjusted to take the dormant vectors into account.
    
    Fixes: 2f75d9e1c905 ("genirq: Implement bitmap matrix allocator")
    Reported-by: Yuriy Vostrikov <delamonpansie@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Yuriy Vostrikov <delamonpansie@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180222112316.849980972@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c23a75759191e84f4ba15b85ea4f97bd544b5362
Merge: e912bf2cf7cd 4596749339e0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 25 16:58:55 2018 -0800

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Thomas Gleixner:
     "A small set of fixes:
    
       - UAPI data type correction for hyperv
    
       - correct the cpu cores field in /proc/cpuinfo on CPU hotplug
    
       - return proper error code in the resctrl file system failure path to
         avoid silent subsequent failures
    
       - correct a subtle accounting issue in the new vector allocation code
         which went unnoticed for a while and caused suspend/resume
         failures"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/topology: Update the 'cpu cores' field in /proc/cpuinfo correctly across CPU hotplug operations
      x86/topology: Fix function name in documentation
      x86/intel_rdt: Fix incorrect returned value when creating rdgroup sub-directory in resctrl file system
      x86/apic/vector: Handle vector release on CPU unplug correctly
      genirq/matrix: Handle CPU offlining proper
      x86/headers/UAPI: Use __u64 instead of u64 in <uapi/asm/hyperv.h>

commit 4596749339e06dc7a424fc08a15eded850ed78b7
Author: Samuel Neves <sneves@dei.uc.pt>
Date:   Wed Feb 21 20:50:36 2018 +0000

    x86/topology: Update the 'cpu cores' field in /proc/cpuinfo correctly across CPU hotplug operations
    
    Without this fix, /proc/cpuinfo will display an incorrect amount
    of CPU cores, after bringing them offline and online again, as
    exemplified below:
    
      $ cat /proc/cpuinfo | grep cores
      cpu cores     : 4
      cpu cores     : 8
      cpu cores     : 8
      cpu cores     : 20
      cpu cores     : 4
      cpu cores     : 3
      cpu cores     : 2
      cpu cores     : 2
    
    This patch fixes this by always zeroing the booted_cores variable
    upon turning off a logical CPU.
    
    Tested-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Signed-off-by: Samuel Neves <sneves@dei.uc.pt>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: jgross@suse.com
    Cc: luto@kernel.org
    Cc: prarit@redhat.com
    Cc: vkuznets@redhat.com
    Link: http://lkml.kernel.org/r/20180221205036.5244-1-sneves@dei.uc.pt
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 651ca2c00405a2ae3870cc0b4f15a182eb6fbe26
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 22 12:08:05 2018 +0100

    genirq/matrix: Handle CPU offlining proper
    
    At CPU hotunplug the corresponding per cpu matrix allocator is shut down and
    the allocated interrupt bits are discarded under the assumption that all
    allocated bits have been either migrated away or shut down through the
    managed interrupts mechanism.
    
    This is not true because interrupts which are not started up might have a
    vector allocated on the outgoing CPU. When the interrupt is started up
    later or completely shutdown and freed then the allocated vector is handed
    back, triggering warnings or causing accounting issues which result in
    suspend failures and other issues.
    
    Change the CPU hotplug mechanism of the matrix allocator so that the
    remaining allocations at unplug time are preserved and global accounting at
    hotplug is correctly readjusted to take the dormant vectors into account.
    
    Fixes: 2f75d9e1c905 ("genirq: Implement bitmap matrix allocator")
    Reported-by: Yuriy Vostrikov <delamonpansie@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Yuriy Vostrikov <delamonpansie@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180222112316.849980972@linutronix.de

commit 745373e3d6ee3e398494d3aebe923b57a90ebadf
Author: Chen-Yu Tsai <wens@csie.org>
Date:   Wed Jan 17 16:46:47 2018 +0800

    ARM: sun9i: Support SMP bring-up on A80
    
    The A80 is a big.LITTLE SoC with 1 cluster of 4 Cortex-A7s and
    1 cluster of 4 Cortex-A15s.
    
    This patch adds support to bring up the second cluster and thus all
    cores using custom platform SMP code. Core/cluster power down has not
    been implemented, thus CPU hotplugging is not supported.
    
    Parts of the trampoline and re-entry code for the boot cpu was adapted
    from the MCPM framework.
    
    Signed-off-by: Chen-Yu Tsai <wens@csie.org>

commit fd4b0c334040bb39087454d700155e5f25227ead
Author: Chen-Yu Tsai <wens@csie.org>
Date:   Wed Jan 17 16:46:50 2018 +0800

    ARM: dts: sun9i: Add PRCM device node for the A80 dtsi
    
    The PRCM is a collection of clock controls, reset controls, and various
    power switches/gates. Some of these can be independently listed and
    supported, while a number of CPU related ones are used in tandem with
    CPUCFG for SMP bringup and CPU hotplugging.
    
    Acked-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Signed-off-by: Chen-Yu Tsai <wens@csie.org>

commit 61cf3ed092c68c9652271320b3808ecf4f5ed12f
Author: Chen-Yu Tsai <wens@csie.org>
Date:   Wed Jan 17 16:46:49 2018 +0800

    ARM: dts: sun9i: Add CPUCFG device node for A80 dtsi
    
    CPUCFG is a collection of registers that are mapped to the SoC's signals
    from each individual processor core and associated peripherals, such as
    resets for processors, L1/L2 cache and other things.
    
    These registers are used for SMP bringup and CPU hotplugging.
    
    Acked-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Signed-off-by: Chen-Yu Tsai <wens@csie.org>

commit e525de3ab04621d227330aa82cd4073c0b0f3579
Merge: d4667ca14261 fd0e786d9d09
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 14 17:31:51 2018 -0800

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar:
     "Misc fixes all across the map:
    
       - /proc/kcore vsyscall related fixes
       - LTO fix
       - build warning fix
       - CPU hotplug fix
       - Kconfig NR_CPUS cleanups
       - cpu_has() cleanups/robustification
       - .gitignore fix
       - memory-failure unmapping fix
       - UV platform fix"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm, mm/hwpoison: Don't unconditionally unmap kernel 1:1 pages
      x86/error_inject: Make just_return_func() globally visible
      x86/platform/UV: Fix GAM Range Table entries less than 1GB
      x86/build: Add arch/x86/tools/insn_decoder_test to .gitignore
      x86/smpboot: Fix uncore_pci_remove() indexing bug when hot-removing a physical CPU
      x86/mm/kcore: Add vsyscall page to /proc/kcore conditionally
      vfs/proc/kcore, x86/mm/kcore: Fix SMAP fault when dumping vsyscall user page
      x86/Kconfig: Further simplify the NR_CPUS config
      x86/Kconfig: Simplify NR_CPUS config
      x86/MCE: Fix build warning introduced by "x86: do not use print_symbol()"
      x86/cpufeature: Update _static_cpu_has() to use all named variables
      x86/cpufeature: Reindent _static_cpu_has()

commit 694a20dae6efc3803740971fa9f7c86b8c8d4685
Merge: 61f14c015f5b ecdf06e1ea53
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 14 10:06:41 2018 -0800

    Merge tag 'powerpc-4.16-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc fixes from Michael Ellerman:
     "A larger batch of fixes than we'd like. Roughly 1/3 fixes for new
      code, 1/3 fixes for stable and 1/3 minor things.
    
      There's four commits fixing bugs when using 16GB huge pages on hash,
      caused by some of the preparatory changes for pkeys.
    
      Two fixes for bugs in the enhanced IRQ soft masking for local_t, one
      of which broke KVM in some circumstances.
    
      Four fixes for Power9. The most bizarre being a bug where futexes
      stopped working because a NULL pointer dereference didn't trap during
      early boot (it aliased the kernel mapping). A fix for memory hotplug
      when using the Radix MMU, and a fix for live migration of guests using
      the Radix MMU.
    
      Two fixes for hotplug on pseries machines. One where we weren't
      correctly updating NUMA info when CPUs are added and removed. And the
      other fixes crashes/hangs seen when doing memory hot remove during
      boot, which is apparently a thing people do.
    
      Finally a handful of build fixes for obscure configs and other minor
      fixes.
    
      Thanks to: Alexey Kardashevskiy, Aneesh Kumar K.V, Balbir Singh, Colin
      Ian King, Daniel Henrique Barboza, Florian Weimer, Guenter Roeck,
      Harish, Laurent Vivier, Madhavan Srinivasan, Mauricio Faria de
      Oliveira, Nathan Fontenot, Nicholas Piggin, Sam Bobroff"
    
    * tag 'powerpc-4.16-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux:
      selftests/powerpc: Fix to use ucontext_t instead of struct ucontext
      powerpc/kdump: Fix powernv build break when KEXEC_CORE=n
      powerpc/pseries: Fix build break for SPLPAR=n and CPU hotplug
      powerpc/mm/hash64: Zero PGD pages on allocation
      powerpc/mm/hash64: Store the slot information at the right offset for hugetlb
      powerpc/mm/hash64: Allocate larger PMD table if hugetlb config is enabled
      powerpc/mm: Fix crashes with 16G huge pages
      powerpc/mm: Flush radix process translations when setting MMU type
      powerpc/vas: Don't set uses_vas for kernel windows
      powerpc/pseries: Enable RAS hotplug events later
      powerpc/mm/radix: Split linear mapping on hot-unplug
      powerpc/64s/radix: Boot-time NULL pointer protection using a guard-PID
      ocxl: fix signed comparison with less than zero
      powerpc/64s: Fix may_hard_irq_enable() for PMI soft masking
      powerpc/64s: Fix MASKABLE_RELON_EXCEPTION_HV_OOL macro
      powerpc/numa: Invalidate numa_cpu_lookup_table on cpu remove

commit 82343484a2d4c97a03bfd81303b5493c65f05c50
Author: Guenter Roeck <linux@roeck-us.net>
Date:   Mon Feb 12 14:34:08 2018 -0800

    powerpc/pseries: Fix build break for SPLPAR=n and CPU hotplug
    
    Commit e67e02a544e9 ("powerpc/pseries: Fix cpu hotplug crash with
    memoryless nodes") adds an unconditional call to
    find_and_online_cpu_nid(), which is only declared if CONFIG_PPC_SPLPAR
    is enabled. This results in the following build error if this is not
    the case.
    
      arch/powerpc/platforms/pseries/hotplug-cpu.o: In function `dlpar_online_cpu':
      arch/powerpc/platforms/pseries/hotplug-cpu.c:369:
                            undefined reference to `.find_and_online_cpu_nid'
    
    Follow the guideline provided by similar functions and provide a dummy
    function if CONFIG_PPC_SPLPAR is not enabled. This also moves the
    external function declaration into an include file where it should be.
    
    Fixes: e67e02a544e9 ("powerpc/pseries: Fix cpu hotplug crash with memoryless nodes")
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    [mpe: Change subject to emphasise the build fix]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 367b0df173b0ebea5d18b6971c244e260b5feb17
Merge: 44644391ff18 3a175cdf4392
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 2 09:50:51 2018 -0800

    Merge branch 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm
    
    Pull ARM updates from Russell King:
    
     - StrongARM SA1111 updates to modernise and remove cruft
    
     - Add StrongARM gpio drivers for board GPIOs
    
     - Verify size of zImage is what we expect to avoid issues with
       appended DTB
    
     - nommu updates from Vladimir Murzin
    
     - page table read-write-execute checking from Jinbum Park
    
     - Broadcom Brahma-B15 cache updates from Florian Fainelli
    
     - Avoid failure with kprobes test caused by inappropriately
       placed kprobes
    
     - Remove __memzero optimisation (which was incorrectly being
       used directly by some drivers)
    
    * 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm: (32 commits)
      ARM: 8745/1: get rid of __memzero()
      ARM: 8744/1: don't discard memblock for kexec
      ARM: 8743/1: bL_switcher: add MODULE_LICENSE tag
      ARM: 8742/1: Always use REFCOUNT_FULL
      ARM: 8741/1: B15: fix unused label warnings
      ARM: 8740/1: NOMMU: Make sure we do not hold stale data in mem[] array
      ARM: 8739/1: NOMMU: Setup VBAR/Hivecs for secondaries cores
      ARM: 8738/1: Disable CONFIG_DEBUG_VIRTUAL for NOMMU
      ARM: 8737/1: mm: dump: add checking for writable and executable
      ARM: 8736/1: mm: dump: make the page table dumping seq_file
      ARM: 8735/1: mm: dump: make page table dumping reusable
      ARM: sa1100/neponset: add GPIO drivers for control and modem registers
      ARM: sa1100/assabet: add BCR/BSR GPIO driver
      ARM: 8734/1: mm: idmap: Mark variables as ro_after_init
      ARM: 8733/1: hw_breakpoint: Mark variables as __ro_after_init
      ARM: 8732/1: NOMMU: Allow userspace to access background MPU region
      ARM: 8727/1: MAINTAINERS: Update brcmstb entries to cover B15 code
      ARM: 8728/1: B15: Register reboot notifier for KEXEC
      ARM: 8730/1: B15: Add suspend/resume hooks
      ARM: 8726/1: B15: Add CPU hotplug awareness
      ...

commit 7303968d539622a5173d7f08e6938c91b48d3cd8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jan 26 14:54:32 2018 +0100

    hrtimer: Reset hrtimer cpu base proper on CPU hotplug
    
    commit d5421ea43d30701e03cadc56a38854c36a8b4433 upstream.
    
    The hrtimer interrupt code contains a hang detection and mitigation
    mechanism, which prevents that a long delayed hrtimer interrupt causes a
    continous retriggering of interrupts which prevent the system from making
    progress. If a hang is detected then the timer hardware is programmed with
    a certain delay into the future and a flag is set in the hrtimer cpu base
    which prevents newly enqueued timers from reprogramming the timer hardware
    prior to the chosen delay. The subsequent hrtimer interrupt after the delay
    clears the flag and resumes normal operation.
    
    If such a hang happens in the last hrtimer interrupt before a CPU is
    unplugged then the hang_detected flag is set and stays that way when the
    CPU is plugged in again. At that point the timer hardware is not armed and
    it cannot be armed because the hang_detected flag is still active, so
    nothing clears that flag. As a consequence the CPU does not receive hrtimer
    interrupts and no timers expire on that CPU which results in RCU stalls and
    other malfunctions.
    
    Clear the flag along with some other less critical members of the hrtimer
    cpu base to ensure starting from a clean state when a CPU is plugged in.
    
    Thanks to Paul, Sebastian and Anna-Maria for their help to get down to the
    root cause of that hard to reproduce heisenbug. Once understood it's
    trivial and certainly justifies a brown paperbag.
    
    Fixes: 41d2e4949377 ("hrtimer: Tune hrtimer_interrupt hang logic")
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Sewior <bigeasy@linutronix.de>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1801261447590.2067@nanos
    [bigeasy: backport to v3.18, drop ->next_timer it was introduced later]
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fdd88d753d4b3142f7cd38b0278c29b03c1e0929
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jan 26 14:54:32 2018 +0100

    hrtimer: Reset hrtimer cpu base proper on CPU hotplug
    
    commit d5421ea43d30701e03cadc56a38854c36a8b4433 upstream.
    
    The hrtimer interrupt code contains a hang detection and mitigation
    mechanism, which prevents that a long delayed hrtimer interrupt causes a
    continous retriggering of interrupts which prevent the system from making
    progress. If a hang is detected then the timer hardware is programmed with
    a certain delay into the future and a flag is set in the hrtimer cpu base
    which prevents newly enqueued timers from reprogramming the timer hardware
    prior to the chosen delay. The subsequent hrtimer interrupt after the delay
    clears the flag and resumes normal operation.
    
    If such a hang happens in the last hrtimer interrupt before a CPU is
    unplugged then the hang_detected flag is set and stays that way when the
    CPU is plugged in again. At that point the timer hardware is not armed and
    it cannot be armed because the hang_detected flag is still active, so
    nothing clears that flag. As a consequence the CPU does not receive hrtimer
    interrupts and no timers expire on that CPU which results in RCU stalls and
    other malfunctions.
    
    Clear the flag along with some other less critical members of the hrtimer
    cpu base to ensure starting from a clean state when a CPU is plugged in.
    
    Thanks to Paul, Sebastian and Anna-Maria for their help to get down to the
    root cause of that hard to reproduce heisenbug. Once understood it's
    trivial and certainly justifies a brown paperbag.
    
    Fixes: 41d2e4949377 ("hrtimer: Tune hrtimer_interrupt hang logic")
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Sewior <bigeasy@linutronix.de>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1801261447590.2067@nanos
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c98ff7299b404f110167883695f81080723e6e15
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jan 26 14:54:32 2018 +0100

    hrtimer: Reset hrtimer cpu base proper on CPU hotplug
    
    commit d5421ea43d30701e03cadc56a38854c36a8b4433 upstream.
    
    The hrtimer interrupt code contains a hang detection and mitigation
    mechanism, which prevents that a long delayed hrtimer interrupt causes a
    continous retriggering of interrupts which prevent the system from making
    progress. If a hang is detected then the timer hardware is programmed with
    a certain delay into the future and a flag is set in the hrtimer cpu base
    which prevents newly enqueued timers from reprogramming the timer hardware
    prior to the chosen delay. The subsequent hrtimer interrupt after the delay
    clears the flag and resumes normal operation.
    
    If such a hang happens in the last hrtimer interrupt before a CPU is
    unplugged then the hang_detected flag is set and stays that way when the
    CPU is plugged in again. At that point the timer hardware is not armed and
    it cannot be armed because the hang_detected flag is still active, so
    nothing clears that flag. As a consequence the CPU does not receive hrtimer
    interrupts and no timers expire on that CPU which results in RCU stalls and
    other malfunctions.
    
    Clear the flag along with some other less critical members of the hrtimer
    cpu base to ensure starting from a clean state when a CPU is plugged in.
    
    Thanks to Paul, Sebastian and Anna-Maria for their help to get down to the
    root cause of that hard to reproduce heisenbug. Once understood it's
    trivial and certainly justifies a brown paperbag.
    
    Fixes: 41d2e4949377 ("hrtimer: Tune hrtimer_interrupt hang logic")
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Sewior <bigeasy@linutronix.de>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1801261447590.2067@nanos
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 360496cab53af2f1dd77dbe353b7b665bcfdc1e3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jan 26 14:54:32 2018 +0100

    hrtimer: Reset hrtimer cpu base proper on CPU hotplug
    
    commit d5421ea43d30701e03cadc56a38854c36a8b4433 upstream.
    
    The hrtimer interrupt code contains a hang detection and mitigation
    mechanism, which prevents that a long delayed hrtimer interrupt causes a
    continous retriggering of interrupts which prevent the system from making
    progress. If a hang is detected then the timer hardware is programmed with
    a certain delay into the future and a flag is set in the hrtimer cpu base
    which prevents newly enqueued timers from reprogramming the timer hardware
    prior to the chosen delay. The subsequent hrtimer interrupt after the delay
    clears the flag and resumes normal operation.
    
    If such a hang happens in the last hrtimer interrupt before a CPU is
    unplugged then the hang_detected flag is set and stays that way when the
    CPU is plugged in again. At that point the timer hardware is not armed and
    it cannot be armed because the hang_detected flag is still active, so
    nothing clears that flag. As a consequence the CPU does not receive hrtimer
    interrupts and no timers expire on that CPU which results in RCU stalls and
    other malfunctions.
    
    Clear the flag along with some other less critical members of the hrtimer
    cpu base to ensure starting from a clean state when a CPU is plugged in.
    
    Thanks to Paul, Sebastian and Anna-Maria for their help to get down to the
    root cause of that hard to reproduce heisenbug. Once understood it's
    trivial and certainly justifies a brown paperbag.
    
    Fixes: 41d2e4949377 ("hrtimer: Tune hrtimer_interrupt hang logic")
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Sewior <bigeasy@linutronix.de>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1801261447590.2067@nanos
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a46d3f9b1c9888a244ed1ce8da0eca98c3f378e2
Merge: 7bcd34259466 303c146df1c4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 29 16:50:58 2018 -0800

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "The timer departement presents:
    
       - A rather large rework of the hrtimer infrastructure which
         introduces softirq based hrtimers to replace the spread of
         hrtimer/tasklet combos which force the actual callback execution
         into softirq context. The approach is completely different from the
         initial implementation which you cursed at 10 years ago rightfully.
    
         The softirq based timers have their own queues and there is no
         nasty indirection and list reshuffling in the hard interrupt
         anymore. This comes with conversion of some of the hrtimer/tasklet
         users, the rest and the final removal of that horrible interface
         will come towards the end of the merge window or go through the
         relevant maintainer trees.
    
         Note: The top commit merged the last minute bugfix for the 10 years
         old CPU hotplug bug as I wanted to make sure that I fatfinger the
         merge conflict resolution myself.
    
       - The overhaul of the STM32 clocksource/clockevents driver
    
       - A new driver for the Spreadtrum SC9860 timer
    
       - A new driver dor the Actions Semi S700 timer
    
       - The usual set of fixes and updates all over the place"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (53 commits)
      usb/gadget/NCM: Replace tasklet with softirq hrtimer
      ALSA/dummy: Replace tasklet with softirq hrtimer
      hrtimer: Implement SOFT/HARD clock base selection
      hrtimer: Implement support for softirq based hrtimers
      hrtimer: Prepare handling of hard and softirq based hrtimers
      hrtimer: Add clock bases and hrtimer mode for softirq context
      hrtimer: Use irqsave/irqrestore around __run_hrtimer()
      hrtimer: Factor out __hrtimer_next_event_base()
      hrtimer: Factor out __hrtimer_start_range_ns()
      hrtimer: Remove the 'base' parameter from hrtimer_reprogram()
      hrtimer: Make remote enqueue decision less restrictive
      hrtimer: Unify remote enqueue handling
      hrtimer: Unify hrtimer removal handling
      hrtimer: Make hrtimer_force_reprogramm() unconditionally available
      hrtimer: Make hrtimer_reprogramm() unconditional
      hrtimer: Make hrtimer_cpu_base.next_timer handling unconditional
      hrtimer: Make the remote enqueue check unconditional
      hrtimer: Use accesor functions instead of direct access
      hrtimer: Make the hrtimer_cpu_base::hres_active field unconditional, to simplify the code
      hrtimer: Make room in 'struct hrtimer_cpu_base'
      ...

commit 07b0137c0268b8d0694a5f09284449353a1a6fed
Merge: 624441927ff6 d5421ea43d30
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jan 28 12:17:35 2018 -0800

    Merge branch 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer fix from Thomas Gleixner:
     "A single fix for a ~10 years old problem which causes high resolution
      timers to stop after a CPU unplug/plug cycle due to a stale flag in
      the per CPU hrtimer base struct.
    
      Paul McKenney was hunting this for about a year, but the heisenbug
      nature made it resistant against debug attempts for quite some time"
    
    * 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      hrtimer: Reset hrtimer cpu base proper on CPU hotplug

commit d5421ea43d30701e03cadc56a38854c36a8b4433
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jan 26 14:54:32 2018 +0100

    hrtimer: Reset hrtimer cpu base proper on CPU hotplug
    
    The hrtimer interrupt code contains a hang detection and mitigation
    mechanism, which prevents that a long delayed hrtimer interrupt causes a
    continous retriggering of interrupts which prevent the system from making
    progress. If a hang is detected then the timer hardware is programmed with
    a certain delay into the future and a flag is set in the hrtimer cpu base
    which prevents newly enqueued timers from reprogramming the timer hardware
    prior to the chosen delay. The subsequent hrtimer interrupt after the delay
    clears the flag and resumes normal operation.
    
    If such a hang happens in the last hrtimer interrupt before a CPU is
    unplugged then the hang_detected flag is set and stays that way when the
    CPU is plugged in again. At that point the timer hardware is not armed and
    it cannot be armed because the hang_detected flag is still active, so
    nothing clears that flag. As a consequence the CPU does not receive hrtimer
    interrupts and no timers expire on that CPU which results in RCU stalls and
    other malfunctions.
    
    Clear the flag along with some other less critical members of the hrtimer
    cpu base to ensure starting from a clean state when a CPU is plugged in.
    
    Thanks to Paul, Sebastian and Anna-Maria for their help to get down to the
    root cause of that hard to reproduce heisenbug. Once understood it's
    trivial and certainly justifies a brown paperbag.
    
    Fixes: 41d2e4949377 ("hrtimer: Tune hrtimer_interrupt hang logic")
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Sewior <bigeasy@linutronix.de>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1801261447590.2067@nanos

commit ea05ba7c559c8e5a5946c3a94a2a266e9a6680a6
Author: Michael Bringmann <mwb@linux.vnet.ibm.com>
Date:   Tue Nov 28 16:58:40 2017 -0600

    powerpc/numa: Ensure nodes initialized for hotplug
    
    This patch fixes some problems encountered at runtime with
    configurations that support memory-less nodes, or that hot-add CPUs
    into nodes that are memoryless during system execution after boot. The
    problems of interest include:
    
    * Nodes known to powerpc to be memoryless at boot, but to have CPUs in
      them are allowed to be 'possible' and 'online'. Memory allocations
      for those nodes are taken from another node that does have memory
      until and if memory is hot-added to the node.
    
    * Nodes which have no resources assigned at boot, but which may still
      be referenced subsequently by affinity or associativity attributes,
      are kept in the list of 'possible' nodes for powerpc. Hot-add of
      memory or CPUs to the system can reference these nodes and bring
      them online instead of redirecting the references to one of the set
      of nodes known to have memory at boot.
    
    Note that this software operates under the context of CPU hotplug. We
    are not doing memory hotplug in this code, but rather updating the
    kernel's CPU topology (i.e. arch_update_cpu_topology /
    numa_update_cpu_topology). We are initializing a node that may be used
    by CPUs or memory before it can be referenced as invalid by a CPU
    hotplug operation. CPU hotplug operations are protected by a range of
    APIs including cpu_maps_update_begin/cpu_maps_update_done,
    cpus_read/write_lock / cpus_read/write_unlock, device locks, and more.
    Memory hotplug operations, including try_online_node, are protected by
    mem_hotplug_begin/mem_hotplug_done, device locks, and more. In the
    case of CPUs being hot-added to a previously memoryless node, the
    try_online_node operation occurs wholly within the CPU locks with no
    overlap. Using HMC hot-add/hot-remove operations, we have been able to
    add and remove CPUs to any possible node without failures. HMC
    operations involve a degree self-serialization, though.
    
    Signed-off-by: Michael Bringmann <mwb@linux.vnet.ibm.com>
    Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit a5281feafd7b1e352c1472143dcbc05ba30cfb1c
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Jan 8 14:21:49 2018 +0100

    ARM: 8741/1: B15: fix unused label warnings
    
    The new conditionally compiled code leaves some labels and one
    variable unreferenced when CONFIG_HOTPLUG_CPU and CONFIG_PM_SLEEP
    are disabled:
    
    arch/arm/mm/cache-b15-rac.c: In function 'b15_rac_init':
    arch/arm/mm/cache-b15-rac.c:353:1: error: label 'out_unmap' defined but not used [-Werror=unused-label]
     out_unmap:
     ^~~~~~~~~
    arch/arm/mm/cache-b15-rac.c:351:1: error: label 'out_cpu_dead' defined but not used [-Werror=unused-label]
     out_cpu_dead:
     ^~~~~~~~~~~~
    At top level:
    arch/arm/mm/cache-b15-rac.c:53:12: error: 'rac_config0_reg' defined but not used [-Werror=unused-variable]
    
    This replaces the existing #ifdef conditionals with IS_ENABLED()
    checks that let the compiler figure out for itself which code to
    drop.
    
    Fixes: 55de88778f4b ("ARM: 8726/1: B15: Add CPU hotplug awareness")
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

commit 4917d5df3851d0d3febc8e4bbfd964132e746149
Merge: 9abc937836b4 1b689a95ce74
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 19 11:19:11 2018 -0800

    Merge tag 'powerpc-4.15-8' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc fixes from Michael Ellerman:
     "More than we'd like after rc8, but nothing very alarming either, just
      tying up loose ends before the release:
    
      Since we changed powernv to use cpufreq_get() from show_cpuinfo(), we
      see warnings with PREEMPT enabled. But the preempt_disable() in
      show_cpuinfo() doesn't actually prevent CPU hotplug as it suggests, so
      remove it.
    
      Two updates to the recently merged RFI flush code. Wire up the generic
      sysfs file to report the status, and add a debugfs file to allow
      enabling/disabling it at runtime.
    
      Two updates to xmon, one to add the RFI flush related fields to the
      paca dump, and another to not use hashed pointers in the paca dump.
    
      And one minor fix to add a missing include of linux/types.h in
      asm/hvcall.h, not seen to break the build in upstream, but correct
      anyway.
    
      Thanks to: Benjamin Herrenschmidt, Michal Suchanek, Nicholas Piggin"
    
    * tag 'powerpc-4.15-8' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux:
      powerpc/pseries: include linux/types.h in asm/hvcall.h
      powerpc/64s: Allow control of RFI flush via debugfs
      powerpc/64s: Wire up cpu_show_meltdown()
      powerpc: Don't preempt_disable() in show_cpuinfo()
      powerpc/xmon: Don't print hashed pointers in paca dump
      powerpc/xmon: Add RFI flush related fields to paca dump

commit da351827240e1705cca64bb8ae526f0ce1068048
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 8 15:38:13 2018 +0000

    firmware: arm_sdei: Add support for CPU and system power states
    
    When a CPU enters an idle lower-power state or is powering off, we
    need to mask SDE events so that no events can be delivered while we
    are messing with the MMU as the registered entry points won't be valid.
    
    If the system reboots, we want to unregister all events and mask the CPUs.
    For kexec this allows us to hand a clean slate to the next kernel
    instead of relying on it to call sdei_{private,system}_data_reset().
    
    For hibernate we unregister all events and re-register them on restore,
    in case we restored with the SDE code loaded at a different address.
    (e.g. KASLR).
    
    Add all the notifiers necessary to do this. We only support shared events
    so all events are left registered and enabled over CPU hotplug.
    
    Reviewed-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    [catalin.marinas@arm.com: added CPU_PM_ENTER_FAILED case]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

commit 20e4d813931961fe26d26a1e98b3aba6ec00b130
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 12 10:53:06 2018 +0800

    blk-mq: simplify queue mapping & schedule with each possisble CPU
    
    The previous patch assigns interrupt vectors to all possible CPUs, so
    now hctx can be mapped to possible CPUs, this patch applies this fact
    to simplify queue mapping & schedule so that we don't need to handle
    CPU hotplug for dealing with physical CPU plug & unplug. With this
    simplication, we can work well on physical CPU plug & unplug, which
    is a normal use case for VM at least.
    
    Make sure we allocate blk_mq_ctx structures for all possible CPUs, and
    set hctx->numa_node for possible CPUs which are mapped to this hctx. And
    only choose the online CPUs for schedule.
    
    Reported-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Tested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Tested-by: Stefan Haberland <sth@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Fixes: 4b855ad37194 ("blk-mq: Create hctx for each present CPU")
    (merged the three into one because any single one may not work, and fix
    selecting online CPUs for scheduler)
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 84676c1f21e8ff54befe985f4f14dc1edc10046b
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 12 10:53:05 2018 +0800

    genirq/affinity: assign vectors to all possible CPUs
    
    Currently we assign managed interrupt vectors to all present CPUs.  This
    works fine for systems were we only online/offline CPUs.  But in case of
    systems that support physical CPU hotplug (or the virtualized version of
    it) this means the additional CPUs covered for in the ACPI tables or on
    the command line are not catered for.  To fix this we'd either need to
    introduce new hotplug CPU states just for this case, or we can start
    assining vectors to possible but not present CPUs.
    
    Reported-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Tested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Tested-by: Stefan Haberland <sth@linux.vnet.ibm.com>
    Fixes: 4b855ad37194 ("blk-mq: Create hctx for each present CPU")
    Cc: linux-kernel@vger.kernel.org
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

commit 29f7e499413aab1d33a5313147d2f7e026f67d7c
Merge: b2cd1df66037 74d0833c659a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 8 11:13:08 2018 -0800

    Merge branch 'for-4.15-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup fixes from Tejun Heo:
     "This contains fixes for the following two non-trivial issues:
    
       - The task iterator got broken while adding thread mode support for
         v4.14. It was less visible because it only triggers when both
         cgroup1 and cgroup2 hierarchies are in use. The recent versions of
         systemd uses cgroup2 for process management even when cgroup1 is
         used for resource control exposing this issue.
    
       - cpuset CPU hotplug path could deadlock when racing against exits.
    
      There also are two patches to replace unlimited strcpy() usages with
      strlcpy()"
    
    * 'for-4.15-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: fix css_task_iter crash on CSS_TASK_ITER_PROC
      cgroup: Fix deadlock in cpu hotplug path
      cgroup: use strlcpy() instead of strscpy() to avoid spurious warning
      cgroup: avoid copying strings longer than the buffers

commit 249d4a9b3246f4ec92433ba8ea3bae5ceb4dc1ed
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 27 21:37:25 2017 +0100

    timers: Reinitialize per cpu bases on hotplug
    
    commit 26456f87aca7157c057de65c9414b37f1ab881d1 upstream.
    
    The timer wheel bases are not (re)initialized on CPU hotplug. That leaves
    them with a potentially stale clk and next_expiry valuem, which can cause
    trouble then the CPU is plugged.
    
    Add a prepare callback which forwards the clock, sets next_expiry to far in
    the future and reset the control flags to a known state.
    
    Set base->must_forward_clk so the first timer which is queued will try to
    forward the clock to current jiffies.
    
    Fixes: 500462a9de65 ("timers: Switch to a non-cascading wheel")
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1712272152200.2431@nanos
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6fae6de72ad44e98b5ae58a662d110c58594aad9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 27 21:37:25 2017 +0100

    timers: Reinitialize per cpu bases on hotplug
    
    commit 26456f87aca7157c057de65c9414b37f1ab881d1 upstream.
    
    The timer wheel bases are not (re)initialized on CPU hotplug. That leaves
    them with a potentially stale clk and next_expiry valuem, which can cause
    trouble then the CPU is plugged.
    
    Add a prepare callback which forwards the clock, sets next_expiry to far in
    the future and reset the control flags to a known state.
    
    Set base->must_forward_clk so the first timer which is queued will try to
    forward the clock to current jiffies.
    
    Fixes: 500462a9de65 ("timers: Switch to a non-cascading wheel")
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1712272152200.2431@nanos
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 26456f87aca7157c057de65c9414b37f1ab881d1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 27 21:37:25 2017 +0100

    timers: Reinitialize per cpu bases on hotplug
    
    The timer wheel bases are not (re)initialized on CPU hotplug. That leaves
    them with a potentially stale clk and next_expiry valuem, which can cause
    trouble then the CPU is plugged.
    
    Add a prepare callback which forwards the clock, sets next_expiry to far in
    the future and reset the control flags to a known state.
    
    Set base->must_forward_clk so the first timer which is queued will try to
    forward the clock to current jiffies.
    
    Fixes: 500462a9de65 ("timers: Switch to a non-cascading wheel")
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1712272152200.2431@nanos

commit 55de88778f4bfe6333db4e475afb15ef413b4874
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Fri Dec 1 01:10:11 2017 +0100

    ARM: 8726/1: B15: Add CPU hotplug awareness
    
    The Broadcom Brahma-B15 readahead cache needs to be disabled,
    respectively re-enable during a CPU hotplug. In case we were not to do,
    CPU hotplug would occasionally fail with random crashes when a given CPU
    exits the coherency domain while the RAC is still enabled, as it would
    get stale data from the RAC.
    
    In order to avoid adding any specific B15 readahead-cache awareness to
    arch/arm/mach-bcm/hotplug-brcmstb.c we use a CPU hotplug state machine
    which allows us to catch CPU hotplug events and disable/flush enable the
    RAC accordingly.
    
    Signed-off-by: Alamy Liu <alamyliu@broadcom.com>
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

commit 8741b5ab49403be43771adc57e1c0031514910b9
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Oct 3 18:14:12 2017 +0100

    bus: arm-ccn: Fix use of smp_processor_id() in preemptible context
    
    commit b18c2b9487d8e797fc0a757e57ac3645348c5fba upstream.
    
    Booting a DEBUG_PREEMPT enabled kernel on a CCN-based system
    results in the following splat:
    
    [...]
    arm-ccn e8000000.ccn: No access to interrupts, using timer.
    BUG: using smp_processor_id() in preemptible [00000000] code: swapper/0/1
    caller is debug_smp_processor_id+0x1c/0x28
    CPU: 1 PID: 1 Comm: swapper/0 Not tainted 4.13.0 #6111
    Hardware name: AMD Seattle/Seattle, BIOS 17:08:23 Jun 26 2017
    Call trace:
    [<ffff000008089e78>] dump_backtrace+0x0/0x278
    [<ffff00000808a22c>] show_stack+0x24/0x30
    [<ffff000008bc3bc4>] dump_stack+0x8c/0xb0
    [<ffff00000852b534>] check_preemption_disabled+0xfc/0x100
    [<ffff00000852b554>] debug_smp_processor_id+0x1c/0x28
    [<ffff000008551bd8>] arm_ccn_probe+0x358/0x4f0
    [...]
    
    as we use smp_processor_id() in the wrong context.
    
    Turn this into a get_cpu()/put_cpu() that extends over the CPU hotplug
    registration, making sure that we don't race against a CPU down operation.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Pawel Moll <pawel.moll@arm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 793eed33da5fd9745f8b62e2a93b4b4a3e36caa3
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Oct 3 18:14:12 2017 +0100

    bus: arm-ccn: Fix use of smp_processor_id() in preemptible context
    
    commit b18c2b9487d8e797fc0a757e57ac3645348c5fba upstream.
    
    Booting a DEBUG_PREEMPT enabled kernel on a CCN-based system
    results in the following splat:
    
    [...]
    arm-ccn e8000000.ccn: No access to interrupts, using timer.
    BUG: using smp_processor_id() in preemptible [00000000] code: swapper/0/1
    caller is debug_smp_processor_id+0x1c/0x28
    CPU: 1 PID: 1 Comm: swapper/0 Not tainted 4.13.0 #6111
    Hardware name: AMD Seattle/Seattle, BIOS 17:08:23 Jun 26 2017
    Call trace:
    [<ffff000008089e78>] dump_backtrace+0x0/0x278
    [<ffff00000808a22c>] show_stack+0x24/0x30
    [<ffff000008bc3bc4>] dump_stack+0x8c/0xb0
    [<ffff00000852b534>] check_preemption_disabled+0xfc/0x100
    [<ffff00000852b554>] debug_smp_processor_id+0x1c/0x28
    [<ffff000008551bd8>] arm_ccn_probe+0x358/0x4f0
    [...]
    
    as we use smp_processor_id() in the wrong context.
    
    Turn this into a get_cpu()/put_cpu() that extends over the CPU hotplug
    registration, making sure that we don't race against a CPU down operation.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Pawel Moll <pawel.moll@arm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit ce22f7f94f48f6280e278d905665ababd757d302
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Wed Sep 27 10:13:25 2017 +0100

    MIPS: SMP: Fix deadlock & online race
    
    [ Upstream commit 9e8c399a88f0b87e41a894911475ed2a8f8dff9e ]
    
    Commit 6f542ebeaee0 ("MIPS: Fix race on setting and getting
    cpu_online_mask") effectively reverted commit 8f46cca1e6c06 ("MIPS: SMP:
    Fix possibility of deadlock when bringing CPUs online") and thus has
    reinstated the possibility of deadlock.
    
    The commit was based on testing of kernel v4.4, where the CPU hotplug
    core code issued a BUG() if the starting CPU is not marked online when
    the boot CPU returns from __cpu_up. The commit fixes this race (in
    v4.4), but re-introduces the deadlock situation.
    
    As noted in the commit message, upstream differs in this area. Commit
    8df3e07e7f21f ("cpu/hotplug: Let upcoming cpu bring itself fully up")
    adds a completion event in the CPU hotplug core code, making this race
    impossible. However, people were unhappy with relying on the core code
    to do the right thing.
    
    To address the issues both commits were trying to fix, add a second
    completion event in the MIPS smp hotplug path. It removes the
    possibility of a race, since the MIPS smp hotplug code now synchronises
    both the boot and secondary CPUs before they return to the hotplug core
    code. It also addresses the deadlock by ensuring that the secondary CPU
    is not marked online before it's counters are synchronised.
    
    This fix should also be backported to fix the race condition introduced
    by the backport of commit 8f46cca1e6c06 ("MIPS: SMP: Fix possibility of
    deadlock when bringing CPUs online"), through really that race only
    existed before commit 8df3e07e7f21f ("cpu/hotplug: Let upcoming cpu
    bring itself fully up").
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Fixes: 6f542ebeaee0 ("MIPS: Fix race on setting and getting cpu_online_mask")
    CC: Matija Glavinic Pecotic <matija.glavinic-pecotic.ext@nokia.com>
    Cc: <stable@vger.kernel.org> # v4.1+: 8f46cca1e6c0: "MIPS: SMP: Fix possibility of deadlock when bringing CPUs online"
    Cc: <stable@vger.kernel.org> # v4.1+: a00eeede507c: "MIPS: SMP: Use a completion event to signal CPU up"
    Cc: <stable@vger.kernel.org> # v4.1+: 6f542ebeaee0: "MIPS: Fix race on setting and getting cpu_online_mask"
    Cc: <stable@vger.kernel.org> # v4.1+
    Patchwork: https://patchwork.linux-mips.org/patch/17376/
    Signed-off-by: James Hogan <jhogan@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>

commit 4b43a3bc20ec6ecebb651f48a670373f9dfa1dbb
Merge: e017b4db26d0 46febd37f9c7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 6 17:45:36 2017 -0800

    Merge branch 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug fix from Ingo Molnar:
     "A single fix moving the smp-call queue flush step to the intended
      point in the state machine"
    
    * 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      smp/hotplug: Move step CPUHP_AP_SMPCFD_DYING to the correct place

commit e8b3f8db7aad99fcc5234fc5b89984ff6620de3d
Author: Lai Jiangshan <jiangshanlai@gmail.com>
Date:   Fri Dec 1 22:20:36 2017 +0800

    workqueue/hotplug: simplify workqueue_offline_cpu()
    
    Since the recent cpu/hotplug refactoring, workqueue_offline_cpu() is
    guaranteed to run on the local cpu which is going offline.
    
    This also fixes the following deadlock by removing work item
    scheduling and flushing from CPU hotplug path.
    
     http://lkml.kernel.org/r/1504764252-29091-1-git-send-email-prsood@codeaurora.org
    
    tj: Description update.
    
    Signed-off-by: Lai Jiangshan <jiangshanlai@gmail.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit b18c2b9487d8e797fc0a757e57ac3645348c5fba
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Oct 3 18:14:12 2017 +0100

    bus: arm-ccn: Fix use of smp_processor_id() in preemptible context
    
    Booting a DEBUG_PREEMPT enabled kernel on a CCN-based system
    results in the following splat:
    
    [...]
    arm-ccn e8000000.ccn: No access to interrupts, using timer.
    BUG: using smp_processor_id() in preemptible [00000000] code: swapper/0/1
    caller is debug_smp_processor_id+0x1c/0x28
    CPU: 1 PID: 1 Comm: swapper/0 Not tainted 4.13.0 #6111
    Hardware name: AMD Seattle/Seattle, BIOS 17:08:23 Jun 26 2017
    Call trace:
    [<ffff000008089e78>] dump_backtrace+0x0/0x278
    [<ffff00000808a22c>] show_stack+0x24/0x30
    [<ffff000008bc3bc4>] dump_stack+0x8c/0xb0
    [<ffff00000852b534>] check_preemption_disabled+0xfc/0x100
    [<ffff00000852b554>] debug_smp_processor_id+0x1c/0x28
    [<ffff000008551bd8>] arm_ccn_probe+0x358/0x4f0
    [...]
    
    as we use smp_processor_id() in the wrong context.
    
    Turn this into a get_cpu()/put_cpu() that extends over the CPU hotplug
    registration, making sure that we don't race against a CPU down operation.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Cc: stable@vger.kernel.org # 4.2+
    Signed-off-by: Pawel Moll <pawel.moll@arm.com>

commit 00a797225e53a4f488ab725e0b438c2970aa41d7
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Tue Nov 28 10:55:15 2017 +0000

    drm/i915/pmu: Return -EINVAL when selecting the inactive CPU
    
    In commit 0426c0465461 ("drm/i915/pmu: Only allow running on a single
    CPU") I attempted to clarify the CPU hotplug logic in our PMU
    implementation, but missed that a more logical error to return, when
    attempting to initialize an event on a currently inactive CPU, is -EINVAL
    rather than -ENODEV.
    
    This is because i915 PMU explicitly disallows running counters on more
    than one CPU at a time, and is not reporting that the requested CPU does
    not exist, or is off-line.
    
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reported-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20171128105515.21998-1-tvrtko.ursulin@linux.intel.com

commit b3add01ee217db8e0da8a6596f03c487ed33b06e
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Tue Nov 21 18:18:49 2017 +0000

    drm/i915/pmu: Wire up engine busy stats to PMU
    
    We can use engine busy stats instead of the sampling timer for
    better accuracy.
    
    By doing this we replace the stohastic sampling with busyness
    metric derived directly from engine activity. This is context
    switch interrupt driven, so as accurate as we can get from
    software tracking.
    
    As a secondary benefit, we can also not run the sampling timer
    in cases only busyness metric is enabled.
    
    v2: Rebase.
    v3:
     * Rebase, comments.
     * Leave engine busyness controls out of workers.
    v4: Checkpatch cleanup.
    v5: Added comment to pmu_needs_timer change.
    v6:
     * Rebase.
     * Fix style of some comments. (Chris Wilson)
    v7: Rebase and commit message update. (Chris Wilson)
    v8: Add delayed stats disabling to improve accuracy in face of
        CPU hotplug events.
    v9: Rebase.
    v10: Rebase - i915_modparams.enable_execlists removal.
    
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Link: https://patchwork.freedesktop.org/patch/msgid/20171121181852.16128-6-tvrtko.ursulin@linux.intel.com

commit b46a33e271ed81bd765c632b972c49d5b44729c7
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Tue Nov 21 18:18:45 2017 +0000

    drm/i915/pmu: Expose a PMU interface for perf queries
    
    From: Chris Wilson <chris@chris-wilson.co.uk>
    From: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    From: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>
    
    The first goal is to be able to measure GPU (and invidual ring) busyness
    without having to poll registers from userspace. (Which not only incurs
    holding the forcewake lock indefinitely, perturbing the system, but also
    runs the risk of hanging the machine.) As an alternative we can use the
    perf event counter interface to sample the ring registers periodically
    and send those results to userspace.
    
    Functionality we are exporting to userspace is via the existing perf PMU
    API and can be exercised via the existing tools. For example:
    
      perf stat -a -e i915/rcs0-busy/ -I 1000
    
    Will print the render engine busynnes once per second. All the performance
    counters can be enumerated (perf list) and have their unit of measure
    correctly reported in sysfs.
    
    v1-v2 (Chris Wilson):
    
    v2: Use a common timer for the ring sampling.
    
    v3: (Tvrtko Ursulin)
     * Decouple uAPI from i915 engine ids.
     * Complete uAPI defines.
     * Refactor some code to helpers for clarity.
     * Skip sampling disabled engines.
     * Expose counters in sysfs.
     * Pass in fake regs to avoid null ptr deref in perf core.
     * Convert to class/instance uAPI.
     * Use shared driver code for rc6 residency, power and frequency.
    
    v4: (Dmitry Rogozhkin)
     * Register PMU with .task_ctx_nr=perf_invalid_context
     * Expose cpumask for the PMU with the single CPU in the mask
     * Properly support pmu->stop(): it should call pmu->read()
     * Properly support pmu->del(): it should call stop(event, PERF_EF_UPDATE)
     * Introduce refcounting of event subscriptions.
     * Make pmu.busy_stats a refcounter to avoid busy stats going away
       with some deleted event.
     * Expose cpumask for i915 PMU to avoid multiple events creation of
       the same type followed by counter aggregation by perf-stat.
     * Track CPUs getting online/offline to migrate perf context. If (likely)
       cpumask will initially set CPU0, CONFIG_BOOTPARAM_HOTPLUG_CPU0 will be
       needed to see effect of CPU status tracking.
     * End result is that only global events are supported and perf stat
       works correctly.
     * Deny perf driver level sampling - it is prohibited for uncore PMU.
    
    v5: (Tvrtko Ursulin)
    
     * Don't hardcode number of engine samplers.
     * Rewrite event ref-counting for correctness and simplicity.
     * Store initial counter value when starting already enabled events
       to correctly report values to all listeners.
     * Fix RC6 residency readout.
     * Comments, GPL header.
    
    v6:
     * Add missing entry to v4 changelog.
     * Fix accounting in CPU hotplug case by copying the approach from
       arch/x86/events/intel/cstate.c. (Dmitry Rogozhkin)
    
    v7:
     * Log failure message only on failure.
     * Remove CPU hotplug notification state on unregister.
    
    v8:
     * Fix error unwind on failed registration.
     * Checkpatch cleanup.
    
    v9:
     * Drop the energy metric, it is available via intel_rapl_perf.
       (Ville Syrjl)
     * Use HAS_RC6(p). (Chris Wilson)
     * Handle unsupported non-engine events. (Dmitry Rogozhkin)
     * Rebase for intel_rc6_residency_ns needing caller managed
       runtime pm.
     * Drop HAS_RC6 checks from the read callback since creating those
       events will be rejected at init time already.
     * Add counter units to sysfs so perf stat output is nicer.
     * Cleanup the attribute tables for brevity and readability.
    
    v10:
     * Fixed queued accounting.
    
    v11:
     * Move intel_engine_lookup_user to intel_engine_cs.c
     * Commit update. (Joonas Lahtinen)
    
    v12:
     * More accurate sampling. (Chris Wilson)
     * Store and report frequency in MHz for better usability from
       perf stat.
     * Removed metrics: queued, interrupts, rc6 counters.
     * Sample engine busyness based on seqno difference only
       for less MMIO (and forcewake) on all platforms. (Chris Wilson)
    
    v13:
     * Comment spelling, use mul_u32_u32 to work around potential GCC
       issue and somne code alignment changes. (Chris Wilson)
    
    v14:
     * Rebase.
    
    v15:
     * Rebase for RPS refactoring.
    
    v16:
     * Use the dynamic slot in the CPU hotplug state machine so that we are
       free to setup our state as multi-instance. Previously we were re-using
       the CPUHP_AP_PERF_X86_UNCORE_ONLINE slot which is neither used as
       multi-instance, nor owned by our driver to start with.
     * Register the CPU hotplug handlers after the PMU, otherwise the callback
       will get called before the PMU is initialized which can end up in
       perf_pmu_migrate_context with an un-initialized base.
     * Added workaround for a probable bug in cpuhp core.
    
    v17:
     * Remove workaround for the cpuhp bug.
    
    v18:
     * Rebase for drm_i915_gem_engine_class getting upstream before us.
    
    v19:
     * Rebase. (trivial)
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Signed-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>
    Cc: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20171121181852.16128-2-tvrtko.ursulin@linux.intel.com

commit 6004eb4d1adca173f1a55dc2121f25da71e96b71
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Wed Sep 27 10:13:25 2017 +0100

    MIPS: SMP: Fix deadlock & online race
    
    commit 9e8c399a88f0b87e41a894911475ed2a8f8dff9e upstream.
    
    Commit 6f542ebeaee0 ("MIPS: Fix race on setting and getting
    cpu_online_mask") effectively reverted commit 8f46cca1e6c06 ("MIPS: SMP:
    Fix possibility of deadlock when bringing CPUs online") and thus has
    reinstated the possibility of deadlock.
    
    The commit was based on testing of kernel v4.4, where the CPU hotplug
    core code issued a BUG() if the starting CPU is not marked online when
    the boot CPU returns from __cpu_up. The commit fixes this race (in
    v4.4), but re-introduces the deadlock situation.
    
    As noted in the commit message, upstream differs in this area. Commit
    8df3e07e7f21f ("cpu/hotplug: Let upcoming cpu bring itself fully up")
    adds a completion event in the CPU hotplug core code, making this race
    impossible. However, people were unhappy with relying on the core code
    to do the right thing.
    
    To address the issues both commits were trying to fix, add a second
    completion event in the MIPS smp hotplug path. It removes the
    possibility of a race, since the MIPS smp hotplug code now synchronises
    both the boot and secondary CPUs before they return to the hotplug core
    code. It also addresses the deadlock by ensuring that the secondary CPU
    is not marked online before it's counters are synchronised.
    
    This fix should also be backported to fix the race condition introduced
    by the backport of commit 8f46cca1e6c06 ("MIPS: SMP: Fix possibility of
    deadlock when bringing CPUs online"), through really that race only
    existed before commit 8df3e07e7f21f ("cpu/hotplug: Let upcoming cpu
    bring itself fully up").
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Fixes: 6f542ebeaee0 ("MIPS: Fix race on setting and getting cpu_online_mask")
    CC: Matija Glavinic Pecotic <matija.glavinic-pecotic.ext@nokia.com>
    Patchwork: https://patchwork.linux-mips.org/patch/17376/
    Signed-off-by: James Hogan <jhogan@kernel.org>
    [jhogan@kernel.org: Backported 4.1..4.9]
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b98220399dc42a81405a4f7962d00fd658fd4cfc
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Wed Sep 27 10:13:25 2017 +0100

    MIPS: SMP: Fix deadlock & online race
    
    commit 9e8c399a88f0b87e41a894911475ed2a8f8dff9e upstream.
    
    Commit 6f542ebeaee0 ("MIPS: Fix race on setting and getting
    cpu_online_mask") effectively reverted commit 8f46cca1e6c06 ("MIPS: SMP:
    Fix possibility of deadlock when bringing CPUs online") and thus has
    reinstated the possibility of deadlock.
    
    The commit was based on testing of kernel v4.4, where the CPU hotplug
    core code issued a BUG() if the starting CPU is not marked online when
    the boot CPU returns from __cpu_up. The commit fixes this race (in
    v4.4), but re-introduces the deadlock situation.
    
    As noted in the commit message, upstream differs in this area. Commit
    8df3e07e7f21f ("cpu/hotplug: Let upcoming cpu bring itself fully up")
    adds a completion event in the CPU hotplug core code, making this race
    impossible. However, people were unhappy with relying on the core code
    to do the right thing.
    
    To address the issues both commits were trying to fix, add a second
    completion event in the MIPS smp hotplug path. It removes the
    possibility of a race, since the MIPS smp hotplug code now synchronises
    both the boot and secondary CPUs before they return to the hotplug core
    code. It also addresses the deadlock by ensuring that the secondary CPU
    is not marked online before it's counters are synchronised.
    
    This fix should also be backported to fix the race condition introduced
    by the backport of commit 8f46cca1e6c06 ("MIPS: SMP: Fix possibility of
    deadlock when bringing CPUs online"), through really that race only
    existed before commit 8df3e07e7f21f ("cpu/hotplug: Let upcoming cpu
    bring itself fully up").
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Fixes: 6f542ebeaee0 ("MIPS: Fix race on setting and getting cpu_online_mask")
    CC: Matija Glavinic Pecotic <matija.glavinic-pecotic.ext@nokia.com>
    Patchwork: https://patchwork.linux-mips.org/patch/17376/
    Signed-off-by: James Hogan <jhogan@kernel.org>
    [jhogan@kernel.org: Backported 4.1..4.9]
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 7d58e1c9059eefe0066c5acf2ffa582f6f0180e3
Merge: 2bcc67310126 f4c09f87adfe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 13 18:23:19 2017 -0800

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull smp/hotplug updates from Thomas Gleixner:
     "No functional changes, just removal of obsolete and outdated defines,
      macros and documentation"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Get rid of CPU hotplug notifier leftovers
      cpu/hotplug: Remove obsolete notifier macros

commit f4c09f87adfe31587aa4b2aea2cb2dbde2150f54
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 13 09:39:01 2017 +0100

    cpu/hotplug: Get rid of CPU hotplug notifier leftovers
    
    The CPU hotplug notifiers are history. Remove the last reminders.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 539954a756f7749bae8775bdaae3ff4d477fa1e7
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Wed Sep 27 10:13:25 2017 +0100

    MIPS: SMP: Fix deadlock & online race
    
    commit 9e8c399a88f0b87e41a894911475ed2a8f8dff9e upstream.
    
    Commit 6f542ebeaee0 ("MIPS: Fix race on setting and getting
    cpu_online_mask") effectively reverted commit 8f46cca1e6c06 ("MIPS: SMP:
    Fix possibility of deadlock when bringing CPUs online") and thus has
    reinstated the possibility of deadlock.
    
    The commit was based on testing of kernel v4.4, where the CPU hotplug
    core code issued a BUG() if the starting CPU is not marked online when
    the boot CPU returns from __cpu_up. The commit fixes this race (in
    v4.4), but re-introduces the deadlock situation.
    
    As noted in the commit message, upstream differs in this area. Commit
    8df3e07e7f21f ("cpu/hotplug: Let upcoming cpu bring itself fully up")
    adds a completion event in the CPU hotplug core code, making this race
    impossible. However, people were unhappy with relying on the core code
    to do the right thing.
    
    To address the issues both commits were trying to fix, add a second
    completion event in the MIPS smp hotplug path. It removes the
    possibility of a race, since the MIPS smp hotplug code now synchronises
    both the boot and secondary CPUs before they return to the hotplug core
    code. It also addresses the deadlock by ensuring that the secondary CPU
    is not marked online before it's counters are synchronised.
    
    This fix should also be backported to fix the race condition introduced
    by the backport of commit 8f46cca1e6c06 ("MIPS: SMP: Fix possibility of
    deadlock when bringing CPUs online"), through really that race only
    existed before commit 8df3e07e7f21f ("cpu/hotplug: Let upcoming cpu
    bring itself fully up").
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Fixes: 6f542ebeaee0 ("MIPS: Fix race on setting and getting cpu_online_mask")
    CC: Matija Glavinic Pecotic <matija.glavinic-pecotic.ext@nokia.com>
    Patchwork: https://patchwork.linux-mips.org/patch/17376/
    Signed-off-by: James Hogan <jhogan@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit da61fcf9d62a05f3508f5646d353a9c2604bac76
Author: Paul Burton <paul.burton@mips.com>
Date:   Tue Oct 31 09:41:45 2017 -0700

    irqchip: mips-gic: Use irq_cpu_online to (un)mask all-VP(E) IRQs
    
    The gic_all_vpes_local_irq_controller chip currently attempts to operate
    on all CPUs/VPs in the system when masking or unmasking an interrupt.
    This has a few drawbacks:
    
     - In multi-cluster systems we may not always have access to all CPUs in
       the system. When all CPUs in a cluster are powered down that
       cluster's GIC may also power down, in which case we cannot configure
       its state.
    
     - Relatedly, if we power down a cluster after having configured
       interrupts for CPUs within it then the cluster's GIC may lose state &
       we need to reconfigure it. The current approach doesn't take this
       into account.
    
     - It's wasteful if we run Linux on fewer VPs than are present in the
       system. For example if we run a uniprocessor kernel on CPU0 of a
       system with 16 CPUs then there's no point in us configuring CPUs
       1-15.
    
     - The implementation is also lacking in that it expects the range
       0..gic_vpes-1 to represent valid Linux CPU numbers which may not
       always be the case - for example if we run on a system with more VPs
       than the kernel is configured to support.
    
    Fix all of these issues by only configuring the affected interrupts for
    CPUs which are online at the time, and recording the configuration in a
    new struct gic_all_vpes_chip_data for later use by CPUs being brought
    online. We register a CPU hotplug state (reusing
    CPUHP_AP_IRQ_GIC_STARTING which the ARM GIC driver uses, and which seems
    suitably generic for reuse with the MIPS GIC) and execute
    irq_cpu_online() in order to configure the interrupts on the newly
    onlined CPU.
    
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Cc: Jason Cooper <jason@lakedaemon.net>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-mips@linux-mips.org
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

commit 9e8c399a88f0b87e41a894911475ed2a8f8dff9e
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Wed Sep 27 10:13:25 2017 +0100

    MIPS: SMP: Fix deadlock & online race
    
    Commit 6f542ebeaee0 ("MIPS: Fix race on setting and getting
    cpu_online_mask") effectively reverted commit 8f46cca1e6c06 ("MIPS: SMP:
    Fix possibility of deadlock when bringing CPUs online") and thus has
    reinstated the possibility of deadlock.
    
    The commit was based on testing of kernel v4.4, where the CPU hotplug
    core code issued a BUG() if the starting CPU is not marked online when
    the boot CPU returns from __cpu_up. The commit fixes this race (in
    v4.4), but re-introduces the deadlock situation.
    
    As noted in the commit message, upstream differs in this area. Commit
    8df3e07e7f21f ("cpu/hotplug: Let upcoming cpu bring itself fully up")
    adds a completion event in the CPU hotplug core code, making this race
    impossible. However, people were unhappy with relying on the core code
    to do the right thing.
    
    To address the issues both commits were trying to fix, add a second
    completion event in the MIPS smp hotplug path. It removes the
    possibility of a race, since the MIPS smp hotplug code now synchronises
    both the boot and secondary CPUs before they return to the hotplug core
    code. It also addresses the deadlock by ensuring that the secondary CPU
    is not marked online before it's counters are synchronised.
    
    This fix should also be backported to fix the race condition introduced
    by the backport of commit 8f46cca1e6c06 ("MIPS: SMP: Fix possibility of
    deadlock when bringing CPUs online"), through really that race only
    existed before commit 8df3e07e7f21f ("cpu/hotplug: Let upcoming cpu
    bring itself fully up").
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Fixes: 6f542ebeaee0 ("MIPS: Fix race on setting and getting cpu_online_mask")
    CC: Matija Glavinic Pecotic <matija.glavinic-pecotic.ext@nokia.com>
    Cc: <stable@vger.kernel.org> # v4.1+: 8f46cca1e6c0: "MIPS: SMP: Fix possibility of deadlock when bringing CPUs online"
    Cc: <stable@vger.kernel.org> # v4.1+: a00eeede507c: "MIPS: SMP: Use a completion event to signal CPU up"
    Cc: <stable@vger.kernel.org> # v4.1+: 6f542ebeaee0: "MIPS: Fix race on setting and getting cpu_online_mask"
    Cc: <stable@vger.kernel.org> # v4.1+
    Patchwork: https://patchwork.linux-mips.org/patch/17376/
    Signed-off-by: James Hogan <jhogan@kernel.org>

commit 936a4174435b376557ee2610eae03592baeb9016
Author: Martin Blumenstingl <martin.blumenstingl@googlemail.com>
Date:   Sun Sep 17 18:45:20 2017 +0200

    ARM: smp_scu: allow the platform code to read the SCU CPU status
    
    On Amlogic Meson8 / Meson8m2 (both Cortex-A9) and Meson8b (Cortex-A5)
    the CPU hotplug code needs to wait until the SCU status of the CPU that
    is being taken offline is SCU_PM_POWEROFF.
    Provide a utility function (which can be invoked for example from
    .cpu_kill()) which allows reading the SCU status of a CPU.
    
    While here, replace the magic number 0x3 with a preprocessor macro
    (SCU_CPU_STATUS_MASK) so we don't have to duplicate this magic number in
    the new function.
    
    Signed-off-by: Martin Blumenstingl <martin.blumenstingl@googlemail.com>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Kevin Hilman <khilman@baylibre.com>

commit 62fba6cbbcbfa97ec789daa0b7fb836d94644f28
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Oct 4 14:46:17 2017 +0200

    s390/cputime: fix guest/irq/softirq times after CPU hotplug
    
    commit b7662eef14caf4f582d453d45395825b5a8f594c upstream.
    
    On CPU hotplug some cpu stats contain bogus values:
    
    $ cat /proc/stat
    cpu 0 0 49 1280 0 0 0 3 0 0
    cpu0 0 0 49 618 0 0 0 3 0 0
    cpu1 0 0 0 662 0 0 0 0 0 0
    [...]
    $ echo 0 > /sys/devices/system/cpu/cpu1/online
    $ echo 1 > /sys/devices/system/cpu/cpu1/online
    $ cat /proc/stat
    cpu 0 0 49 3200 0 450359962737 450359962737 3 0 0
    cpu0 0 0 49 1956 0 0 0 3 0 0
    cpu1 0 0 0 1244 0 450359962737 450359962737 0 0 0
    [...]
    
    pcpu_attach_task() needs the same assignments as vtime_task_switch.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Fixes: b7394a5f4ce9 ("sched/cputime, s390: Implement delayed accounting of system time")
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5670a8471e27ff400e9446b5bab6c296c8d8a733
Merge: 085cf9bfc92a 1f7c70d6b2bc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 22 06:54:42 2017 -0400

    Merge branch 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull smp/hotplug fix from Thomas Gleixner:
     "The recent rework of the callback invocation missed to cleanup the
      leftovers of the operation, so under certain circumstances a
      subsequent CPU hotplug operation accesses stale data and crashes.
      Clean it up."
    
    * 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Reset node state after operation

commit 96b0e525af62eb3dcc084b091b40add082467150
Merge: 503f7e297d76 0015a978a254
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 18 06:45:52 2017 -0400

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 fixes from Martin Schwidefsky:
     "Two bug fixes:
    
       - A fix for cputime accounting vs CPU hotplug
    
       - Add two options to zfcpdump_defconfig to make SCSI dump work again"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux:
      s390: fix zfcpdump-config
      s390/cputime: fix guest/irq/softirq times after CPU hotplug

commit c2e2b0db395b7ea655213495240a366c7dbe04f6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 4 21:07:38 2017 +0200

    genirq/cpuhotplug: Enforce affinity setting on startup of managed irqs
    
    commit e43b3b58548051f8809391eb7bec7a27ed3003ea upstream.
    
    Managed interrupts can end up in a stale state on CPU hotplug. If the
    interrupt is not targeting a single CPU, i.e. the affinity mask spawns
    multiple CPUs then the following can happen:
    
    After boot:
    
    dstate:   0x01601200
                IRQD_ACTIVATED
                IRQD_IRQ_STARTED
                IRQD_SINGLE_TARGET
                IRQD_AFFINITY_SET
                IRQD_AFFINITY_MANAGED
    node:     0
    affinity: 24-31
    effectiv: 24
    pending:  0
    
    After offlining CPU 31 - 24
    
    dstate:   0x01a31000
                IRQD_IRQ_DISABLED
                IRQD_IRQ_MASKED
                IRQD_SINGLE_TARGET
                IRQD_AFFINITY_SET
                IRQD_AFFINITY_MANAGED
                IRQD_MANAGED_SHUTDOWN
    node:     0
    affinity: 24-31
    effectiv: 24
    pending:  0
    
    Now CPU 25 gets onlined again, so it should get the effective interrupt
    affinity for this interruopt, but due to the x86 interrupt affinity setter
    restrictions this ends up after restarting the interrupt with:
    
    dstate:   0x01601300
                IRQD_ACTIVATED
                IRQD_IRQ_STARTED
                IRQD_SINGLE_TARGET
                IRQD_AFFINITY_SET
                IRQD_SETAFFINITY_PENDING
                IRQD_AFFINITY_MANAGED
    node:     0
    affinity: 24-31
    effectiv: 24
    pending:  24-31
    
    So the interrupt is still affine to CPU 24, which was the last CPU to go
    offline of that affinity set and the move to an online CPU within 24-31,
    in this case 25, is pending. This mechanism is x86/ia64 specific as those
    architectures cannot move interrupts from thread context and do this when
    an interrupt is actually handled. So the move is set to pending.
    
    Whats worse is that offlining CPU 25 again results in:
    
    dstate:   0x01601300
                IRQD_ACTIVATED
                IRQD_IRQ_STARTED
                IRQD_SINGLE_TARGET
                IRQD_AFFINITY_SET
                IRQD_SETAFFINITY_PENDING
                IRQD_AFFINITY_MANAGED
    node:     0
    affinity: 24-31
    effectiv: 24
    pending:  24-31
    
    This means the interrupt has not been shut down, because the outgoing CPU
    is not in the effective affinity mask, but of course nothing notices that
    the effective affinity mask is pointing at an offline CPU.
    
    In the case of restarting a managed interrupt the move restriction does not
    apply, so the affinity setting can be made unconditional. This needs to be
    done _before_ the interrupt is started up as otherwise the condition for
    moving it from thread context would not longer be fulfilled.
    
    With that change applied onlining CPU 25 after offlining 31-24 results in:
    
    dstate:   0x01600200
                IRQD_ACTIVATED
                IRQD_IRQ_STARTED
                IRQD_SINGLE_TARGET
                IRQD_AFFINITY_MANAGED
    node:     0
    affinity: 24-31
    effectiv: 25
    pending:
    
    And after offlining CPU 25:
    
    dstate:   0x01a30000
                IRQD_IRQ_DISABLED
                IRQD_IRQ_MASKED
                IRQD_SINGLE_TARGET
                IRQD_AFFINITY_MANAGED
                IRQD_MANAGED_SHUTDOWN
    node:     0
    affinity: 24-31
    effectiv: 25
    pending:
    
    which is the correct and expected result.
    
    Fixes: 761ea388e8c4 ("genirq: Handle managed irqs gracefully in irq_startup()")
    Reported-by: YASUAKI ISHIMATSU <yasu.isimatu@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: axboe@kernel.dk
    Cc: linux-scsi@vger.kernel.org
    Cc: Sumit Saxena <sumit.saxena@broadcom.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: mpe@ellerman.id.au
    Cc: Shivasharan Srikanteshwara <shivasharan.srikanteshwara@broadcom.com>
    Cc: Kashyap Desai <kashyap.desai@broadcom.com>
    Cc: keith.busch@intel.com
    Cc: peterz@infradead.org
    Cc: Hannes Reinecke <hare@suse.de>
    Cc: Christoph Hellwig <hch@lst.de>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1710042208400.2406@nanos
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b7662eef14caf4f582d453d45395825b5a8f594c
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Oct 4 14:46:17 2017 +0200

    s390/cputime: fix guest/irq/softirq times after CPU hotplug
    
    On CPU hotplug some cpu stats contain bogus values:
    
    $ cat /proc/stat
    cpu 0 0 49 1280 0 0 0 3 0 0
    cpu0 0 0 49 618 0 0 0 3 0 0
    cpu1 0 0 0 662 0 0 0 0 0 0
    [...]
    $ echo 0 > /sys/devices/system/cpu/cpu1/online
    $ echo 1 > /sys/devices/system/cpu/cpu1/online
    $ cat /proc/stat
    cpu 0 0 49 3200 0 450359962737 450359962737 3 0 0
    cpu0 0 0 49 1956 0 0 0 3 0 0
    cpu1 0 0 0 1244 0 450359962737 450359962737 0 0 0
    [...]
    
    pcpu_attach_task() needs the same assignments as vtime_task_switch.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Fixes: b7394a5f4ce9 ("sched/cputime, s390: Implement delayed accounting of system time")
    Cc: stable@vger.kernel.org # 4.11+
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit 2b34218e893a0ff39f6f46517cb5df2f990db8c0
Merge: a515d05e96ee e43b3b585480
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 14 15:11:21 2017 -0400

    Merge branch 'irq-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq fixes from Ingo Molnar:
     "A CPU hotplug related fix, plus two related sanity checks"
    
    * 'irq-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      genirq/cpuhotplug: Enforce affinity setting on startup of managed irqs
      genirq/cpuhotplug: Add sanity check for effective affinity mask
      genirq: Warn when effective affinity is not updated

commit 6b2c08f989250c54f31b53dba9ace863a1f3fff6
Author: Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>
Date:   Wed Oct 4 21:04:30 2017 -0300

    powerpc: Don't call lockdep_assert_cpus_held() from arch_update_cpu_topology()
    
    It turns out that not all paths calling arch_update_cpu_topology() hold
    cpu_hotplug_lock, but that's OK because those paths can't race with
    any concurrent hotplug events.
    
    Warnings were reported with the following trace:
    
      lockdep_assert_cpus_held
      arch_update_cpu_topology
      sched_init_domains
      sched_init_smp
      kernel_init_freeable
      kernel_init
      ret_from_kernel_thread
    
    Which is safe because it's called early in boot when hotplug is not
    live yet.
    
    And also this trace:
    
      lockdep_assert_cpus_held
      arch_update_cpu_topology
      partition_sched_domains
      cpuset_update_active_cpus
      sched_cpu_deactivate
      cpuhp_invoke_callback
      cpuhp_down_callbacks
      cpuhp_thread_fun
      smpboot_thread_fn
      kthread
      ret_from_kernel_thread
    
    Which is safe because it's called as part of CPU hotplug, so although
    we don't hold the CPU hotplug lock, there is another thread driving
    the CPU hotplug operation which does hold the lock, and there is no
    race.
    
    Thanks to tglx for deciphering it for us.
    
    Fixes: 3e401f7a2e51 ("powerpc: Only obtain cpu_hotplug_lock if called by rtasd")
    Signed-off-by: Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit e43b3b58548051f8809391eb7bec7a27ed3003ea
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 4 21:07:38 2017 +0200

    genirq/cpuhotplug: Enforce affinity setting on startup of managed irqs
    
    Managed interrupts can end up in a stale state on CPU hotplug. If the
    interrupt is not targeting a single CPU, i.e. the affinity mask spawns
    multiple CPUs then the following can happen:
    
    After boot:
    
    dstate:   0x01601200
                IRQD_ACTIVATED
                IRQD_IRQ_STARTED
                IRQD_SINGLE_TARGET
                IRQD_AFFINITY_SET
                IRQD_AFFINITY_MANAGED
    node:     0
    affinity: 24-31
    effectiv: 24
    pending:  0
    
    After offlining CPU 31 - 24
    
    dstate:   0x01a31000
                IRQD_IRQ_DISABLED
                IRQD_IRQ_MASKED
                IRQD_SINGLE_TARGET
                IRQD_AFFINITY_SET
                IRQD_AFFINITY_MANAGED
                IRQD_MANAGED_SHUTDOWN
    node:     0
    affinity: 24-31
    effectiv: 24
    pending:  0
    
    Now CPU 25 gets onlined again, so it should get the effective interrupt
    affinity for this interruopt, but due to the x86 interrupt affinity setter
    restrictions this ends up after restarting the interrupt with:
    
    dstate:   0x01601300
                IRQD_ACTIVATED
                IRQD_IRQ_STARTED
                IRQD_SINGLE_TARGET
                IRQD_AFFINITY_SET
                IRQD_SETAFFINITY_PENDING
                IRQD_AFFINITY_MANAGED
    node:     0
    affinity: 24-31
    effectiv: 24
    pending:  24-31
    
    So the interrupt is still affine to CPU 24, which was the last CPU to go
    offline of that affinity set and the move to an online CPU within 24-31,
    in this case 25, is pending. This mechanism is x86/ia64 specific as those
    architectures cannot move interrupts from thread context and do this when
    an interrupt is actually handled. So the move is set to pending.
    
    Whats worse is that offlining CPU 25 again results in:
    
    dstate:   0x01601300
                IRQD_ACTIVATED
                IRQD_IRQ_STARTED
                IRQD_SINGLE_TARGET
                IRQD_AFFINITY_SET
                IRQD_SETAFFINITY_PENDING
                IRQD_AFFINITY_MANAGED
    node:     0
    affinity: 24-31
    effectiv: 24
    pending:  24-31
    
    This means the interrupt has not been shut down, because the outgoing CPU
    is not in the effective affinity mask, but of course nothing notices that
    the effective affinity mask is pointing at an offline CPU.
    
    In the case of restarting a managed interrupt the move restriction does not
    apply, so the affinity setting can be made unconditional. This needs to be
    done _before_ the interrupt is started up as otherwise the condition for
    moving it from thread context would not longer be fulfilled.
    
    With that change applied onlining CPU 25 after offlining 31-24 results in:
    
    dstate:   0x01600200
                IRQD_ACTIVATED
                IRQD_IRQ_STARTED
                IRQD_SINGLE_TARGET
                IRQD_AFFINITY_MANAGED
    node:     0
    affinity: 24-31
    effectiv: 25
    pending:
    
    And after offlining CPU 25:
    
    dstate:   0x01a30000
                IRQD_IRQ_DISABLED
                IRQD_IRQ_MASKED
                IRQD_SINGLE_TARGET
                IRQD_AFFINITY_MANAGED
                IRQD_MANAGED_SHUTDOWN
    node:     0
    affinity: 24-31
    effectiv: 25
    pending:
    
    which is the correct and expected result.
    
    Fixes: 761ea388e8c4 ("genirq: Handle managed irqs gracefully in irq_startup()")
    Reported-by: YASUAKI ISHIMATSU <yasu.isimatu@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: axboe@kernel.dk
    Cc: linux-scsi@vger.kernel.org
    Cc: Sumit Saxena <sumit.saxena@broadcom.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: mpe@ellerman.id.au
    Cc: Shivasharan Srikanteshwara <shivasharan.srikanteshwara@broadcom.com>
    Cc: Kashyap Desai <kashyap.desai@broadcom.com>
    Cc: keith.busch@intel.com
    Cc: peterz@infradead.org
    Cc: Hannes Reinecke <hare@suse.de>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1710042208400.2406@nanos

commit 34ddaa3e5c0096fef52485186c7eb6cf56ddc686
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Oct 3 16:39:02 2017 +0200

    powerpc/watchdog: Make use of watchdog_nmi_probe()
    
    The rework of the core hotplug code triggers the WARN_ON in start_wd_cpu()
    on powerpc because it is called multiple times for the boot CPU.
    
    The first call is via:
    
      start_wd_on_cpu+0x80/0x2f0
      watchdog_nmi_reconfigure+0x124/0x170
      softlockup_reconfigure_threads+0x110/0x130
      lockup_detector_init+0xbc/0xe0
      kernel_init_freeable+0x18c/0x37c
      kernel_init+0x2c/0x160
      ret_from_kernel_thread+0x5c/0xbc
    
    And then again via the CPU hotplug registration:
    
      start_wd_on_cpu+0x80/0x2f0
      cpuhp_invoke_callback+0x194/0x620
      cpuhp_thread_fun+0x7c/0x1b0
      smpboot_thread_fn+0x290/0x2a0
      kthread+0x168/0x1b0
      ret_from_kernel_thread+0x5c/0xbc
    
    This can be avoided by setting up the cpu hotplug state with nocalls and
    move the initialization to the watchdog_nmi_probe() function. That
    initializes the hotplug callbacks without invoking the callback and the
    following core initialization function then configures the watchdog for the
    online CPUs (in this case CPU0) via softlockup_reconfigure_threads().
    
    Reported-and-tested-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: linuxppc-dev@lists.ozlabs.org

commit 82513545134453a9150a2177e7a5704dcd604f8e
Merge: 7e103ace9cea 1db49484f21e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 1 12:34:42 2017 -0700

    Merge branch 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull smp/hotplug fixes from Thomas Gleixner:
     "This addresses the fallout of the new lockdep mechanism which covers
      completions in the CPU hotplug code.
    
      The lockdep splats are false positives, but there is no way to
      annotate that reliably. The solution is to split the completions for
      CPU up and down, which requires some reshuffling of the failure
      rollback handling as well"
    
    * 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      smp/hotplug: Hotplug state fail injection
      smp/hotplug: Differentiate the AP completion between up and down
      smp/hotplug: Differentiate the AP-work lockdep class between up and down
      smp/hotplug: Callback vs state-machine consistency
      smp/hotplug: Rewrite AP state machine core
      smp/hotplug: Allow external multi-instance rollback
      smp/hotplug: Add state diagram

commit 27d720a564aafa22ed485c8b6e9d36a3b9a08e23
Author: Ethan Barnes <Ethan.Barnes@wdc.com>
Date:   Wed Jul 19 22:36:00 2017 +0000

    smp/hotplug: Handle removal correctly in cpuhp_store_callbacks()
    
    commit 0c96b27305faf06c068b45e07d28336c80dac286 upstream.
    
    If cpuhp_store_callbacks() is called for CPUHP_AP_ONLINE_DYN or
    CPUHP_BP_PREPARE_DYN, which are the indicators for dynamically allocated
    states, then cpuhp_store_callbacks() allocates a new dynamic state. The
    first allocation in each range returns CPUHP_AP_ONLINE_DYN or
    CPUHP_BP_PREPARE_DYN.
    
    If cpuhp_remove_state() is invoked for one of these states, then there is
    no protection against the allocation mechanism. So the removal, which
    should clear the callbacks and the name, gets a new state assigned and
    clears that one.
    
    As a consequence the state which should be cleared stays initialized. A
    consecutive CPU hotplug operation dereferences the state callbacks and
    accesses either freed or reused memory, resulting in crashes.
    
    Add a protection against this by checking the name argument for NULL. If
    it's NULL it's a removal. If not, it's an allocation.
    
    [ tglx: Added a comment and massaged changelog ]
    
    Fixes: 5b7aa87e0482 ("cpu/hotplug: Implement setup/removal interface")
    Signed-off-by: Ethan Barnes <ethan.barnes@sandisk.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.or>
    Cc: "Srivatsa S. Bhat" <srivatsa@mit.edu>
    Cc: Sebastian Siewior <bigeasy@linutronix.d>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/DM2PR04MB398242FC7776D603D9F99C894A60@DM2PR04MB398.namprd04.prod.outlook.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f0cc6ccaf7ba42a1247fe5a9244b6009a3beddd5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 13 23:29:29 2017 +0200

    x86/vector: Simplify the CPU hotplug vector update
    
    With single CPU affinities it's not longer required to scan all interrupts
    for potential destination masks which contain the newly booting CPU.
    
    Reduce it to install the active legacy PIC vectors on the newly booting CPU
    as those cannot be affinity controlled by the kernel and potentially end up
    at any CPU in the system.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Juergen Gross <jgross@suse.com>
    Tested-by: Yu Chen <yu.c.chen@intel.com>
    Acked-by: Juergen Gross <jgross@suse.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Alok Kataria <akataria@vmware.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Rui Zhang <rui.zhang@intel.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Link: https://lkml.kernel.org/r/20170913213154.388040204@linutronix.de

commit ab5fe3ff38ff9653490910cc71dbbedc95a86e41
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:23 2017 +0200

    watchdog/hardlockup: Clean up hotplug locking mess
    
    All watchdog thread related functions are delegated to the smpboot thread
    infrastructure, which handles serialization against CPU hotplug correctly.
    
    The sysctl interface is completely decoupled from anything which requires
    CPU hotplug protection.
    
    No need to protect the sysctl writes against cpu hotplug anymore. Remove it
    and add the now required protection to the powerpc arch_nmi_watchdog
    implementation.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/20170912194148.418497420@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a33d44843d4574ec05bec39527d8a87b7af2072c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:22 2017 +0200

    watchdog/hardlockup/perf: Simplify deferred event destroy
    
    Now that all functionality is properly serialized against CPU hotplug,
    remove the extra per cpu storage which holds the disabled events for
    cleanup. The core makes sure that cleanup happens before new events are
    created.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194148.340708074@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 941154bd6937a710ae9193a3c733c0029e5ae7b8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:04 2017 +0200

    watchdog/hardlockup/perf: Prevent CPU hotplug deadlock
    
    The following deadlock is possible in the watchdog hotplug code:
    
      cpus_write_lock()
        ...
          takedown_cpu()
            smpboot_park_threads()
              smpboot_park_thread()
                kthread_park()
                  ->park() := watchdog_disable()
                    watchdog_nmi_disable()
                      perf_event_release_kernel();
                        put_event()
                          _free_event()
                            ->destroy() := hw_perf_event_destroy()
                              x86_release_hardware()
                                release_ds_buffers()
                                  get_online_cpus()
    
    when a per cpu watchdog perf event is destroyed which drops the last
    reference to the PMU hardware. The cleanup code there invokes
    get_online_cpus() which instantly deadlocks because the hotplug percpu
    rwsem is write locked.
    
    To solve this add a deferring mechanism:
    
      cpus_write_lock()
                               kthread_park()
                                watchdog_nmi_disable(deferred)
                                  perf_event_disable(event);
                                  move_event_to_deferred(event);
                               ....
      cpus_write_unlock()
      cleaup_deferred_events()
        perf_event_release_kernel()
    
    This is still properly serialized against concurrent hotplug via the
    cpu_add_remove_lock, which is held by the task which initiated the hotplug
    event.
    
    This is also used to handle event destruction when the watchdog threads are
    parked via other mechanisms than CPU hotplug.
    
    Analyzed-by: Peter Zijlstra <peterz@infradead.org>
    
    Reported-by: Borislav Petkov <bp@alien8.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194146.884469246@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 20d853fd0703b1d73c35a22024c0d4fcbcc57c8c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:03 2017 +0200

    watchdog/hardlockup/perf: Remove broken self disable on failure
    
    The self disabling feature is broken vs. CPU hotplug locking:
    
    CPU 0                      CPU 1
    cpus_write_lock();
     cpu_up(1)
       wait_for_completion()
                               ....
                               unpark_watchdog()
                               ->unpark()
                                 perf_event_create() <- fails
                                   watchdog_enable &= ~NMI_WATCHDOG;
                               ....
    cpus_write_unlock();
                               CPU 2
    cpus_write_lock()
     cpu_down(2)
       wait_for_completion()
                               wakeup(watchdog);
                                 watchdog()
                                 if (!(watchdog_enable & NMI_WATCHDOG))
                                    watchdog_nmi_disable()
                                      perf_event_disable()
                                      ....
                                      cpus_read_lock();
    
                               stop_smpboot_threads()
                                 park_watchdog();
                                   wait_for_completion(watchdog->parked);
    
    Result: End of hotplug and instantaneous full lockup of the machine.
    
    There is a similar problem with disabling the watchdog via the user space
    interface as the sysctl function fiddles with watchdog_enable directly.
    
    It's very debatable whether this is required at all. If the watchdog works
    nicely on N CPUs and it fails to enable on the N + 1 CPU either during
    hotplug or because the user space interface disabled it via sysctl cpumask
    and then some perf user grabbed the counter which is then unavailable for
    the watchdog when the sysctl cpumask gets changed back.
    
    There is no real justification for this.
    
    One of the reasons WHY this is done is the utter stupidity of the init code
    of the perf NMI watchdog. Instead of checking upfront at boot whether PERF
    is available and functional at all, it just does this check at run time
    over and over when user space fiddles with the sysctl. That's broken beyond
    repair along with the idiotic error code dependent warn level printks and
    the even more silly printk rate limiting.
    
    If the init code checks whether perf works at boot time, then this mess can
    be more or less avoided completely. Perf does not come magically into life
    at runtime. Brain usage while coding is overrated.
    
    Remove the cruft and add a temporary safe guard which gets removed later.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194146.806708429@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b7a349819d4b9b5db64e523351e66a79a758eaa5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:00 2017 +0200

    watchdog/core: Rework CPU hotplug locking
    
    The watchdog proc interface causes extensive recursive locking of the CPU
    hotplug percpu rwsem, which is deadlock prone.
    
    Replace the get/put_online_cpus() pairs with cpu_hotplug_disable()/enable()
    calls for now. Later patches will remove that requirement completely.
    
    Reported-by: Borislav Petkov <bp@alien8.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194146.568079057@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 2406e3b166eee42777a6b0b38f52f924454474d7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 12 21:36:56 2017 +0200

    perf/x86/intel, watchdog/core: Sanitize PMU HT bug workaround
    
    The lockup_detector_suspend/resume() interface is broken in several ways
    especially as it results in recursive locking of the CPU hotplug lock.
    
    Use the new stop/restart interface in the perf NMI watchdog to temporarily
    disable and reenable the already active watchdog events. That's enough to
    handle it.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194146.247141871@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit ec846ecd6350857a8b8b9a6b78c763d45e0f09b8
Merge: b5df1b3a5637 9469eb01db89
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 13 12:22:32 2017 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "Three CPU hotplug related fixes and a debugging improvement"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/debug: Add debugfs knob for "sched_debug"
      sched/core: WARN() when migrating to an offline CPU
      sched/fair: Plug hole between hotplug and active_load_balance()
      sched/fair: Avoid newidle balance for !active CPUs

commit f57091767add2b79d76aac41b83b192d8ba1dce7
Merge: d725c7ac8b96 d56593eb5eda
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 13:56:37 2017 -0700

    Merge branch 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 cache quality monitoring update from Thomas Gleixner:
     "This update provides a complete rewrite of the Cache Quality
      Monitoring (CQM) facility.
    
      The existing CQM support was duct taped into perf with a lot of issues
      and the attempts to fix those turned out to be incomplete and
      horrible.
    
      After lengthy discussions it was decided to integrate the CQM support
      into the Resource Director Technology (RDT) facility, which is the
      obvious choise as in hardware CQM is part of RDT. This allowed to add
      Memory Bandwidth Monitoring support on top.
    
      As a result the mechanisms for allocating cache/memory bandwidth and
      the corresponding monitoring mechanisms are integrated into a single
      management facility with a consistent user interface"
    
    * 'x86-cache-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (37 commits)
      x86/intel_rdt: Turn off most RDT features on Skylake
      x86/intel_rdt: Add command line options for resource director technology
      x86/intel_rdt: Move special case code for Haswell to a quirk function
      x86/intel_rdt: Remove redundant ternary operator on return
      x86/intel_rdt/cqm: Improve limbo list processing
      x86/intel_rdt/mbm: Fix MBM overflow handler during CPU hotplug
      x86/intel_rdt: Modify the intel_pqr_state for better performance
      x86/intel_rdt/cqm: Clear the default RMID during hotcpu
      x86/intel_rdt: Show bitmask of shareable resource with other executing units
      x86/intel_rdt/mbm: Handle counter overflow
      x86/intel_rdt/mbm: Add mbm counter initialization
      x86/intel_rdt/mbm: Basic counting of MBM events (total and local)
      x86/intel_rdt/cqm: Add CPU hotplug support
      x86/intel_rdt/cqm: Add sched_in support
      x86/intel_rdt: Introduce rdt_enable_key for scheduling
      x86/intel_rdt/cqm: Add mount,umount support
      x86/intel_rdt/cqm: Add rmdir support
      x86/intel_rdt: Separate the ctrl bits from rmdir
      x86/intel_rdt/cqm: Add mon_data
      x86/intel_rdt: Prepare for RDT monitor data support
      ...

commit d725c7ac8b96cbdc28266895c6f7080c55bf2f23
Merge: 93cc1228b4a6 0c96b27305fa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 13:53:53 2017 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug fix from Thomas Gleixner:
     "A single fix to handle the removal of the first dynamic CPU hotplug
      state correctly"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      smp/hotplug: Handle removal correctly in cpuhp_store_callbacks()

commit f0be7fe7f619cf915320713371f47a7931245563
Author: Tyrel Datwyler <tyreld@linux.vnet.ibm.com>
Date:   Mon Apr 17 20:24:39 2017 -0400

    powerpc/sysfs: Fix reference leak of cpu device_nodes present at boot
    
    commit e76ca27790a514590af782f83f6eae49e0ccf8c9 upstream.
    
    For CPUs present at boot each logical CPU acquires a reference to the
    associated device node of the core. This happens in register_cpu() which
    is called by topology_init(). The result of this is that we end up with
    a reference held by each thread of the core. However, these references
    are never freed if the CPU core is DLPAR removed.
    
    This patch fixes the reference leaks by acquiring and releasing the references
    in the CPU hotplug callbacks un/register_cpu_online(). With this patch symmetric
    reference counting is observed with both CPUs present at boot, and those DLPAR
    added after boot.
    
    Fixes: f86e4718f24b ("driver/core: cpu: initialize of_node in cpu's device struture")
    Signed-off-by: Tyrel Datwyler <tyreld@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    [bwh: Backported to 3.16: adjust context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 7f680d7ec3153dffc4d37aea517ead2b9fb9b8e9
Merge: 2615a38f142f 45bd07ad8262
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Aug 20 09:36:52 2017 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Thomas Gleixner:
     "Another pile of small fixes and updates for x86:
    
       - Plug a hole in the SMAP implementation which misses to clear AC on
         NMI entry
    
       - Fix the norandmaps/ADDR_NO_RANDOMIZE logic so the command line
         parameter works correctly again
    
       - Use the proper accessor in the startup64 code for next_early_pgt to
         prevent accessing of invalid addresses and faulting in the early
         boot code.
    
       - Prevent CPU hotplug lock recursion in the MTRR code
    
       - Unbreak CPU0 hotplugging
    
       - Rename overly long CPUID bits which got introduced in this cycle
    
       - Two commits which mark data 'const' and restrict the scope of data
         and functions to file scope by making them 'static'"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Constify attribute_group structures
      x86/boot/64/clang: Use fixup_pointer() to access 'next_early_pgt'
      x86/elf: Remove the unnecessary ADDR_NO_RANDOMIZE checks
      x86: Fix norandmaps/ADDR_NO_RANDOMIZE
      x86/mtrr: Prevent CPU hotplug lock recursion
      x86: Mark various structures and functions as 'static'
      x86/cpufeature, kvm/svm: Rename (shorten) the new "virtualized VMSAVE/VMLOAD" CPUID flag
      x86/smpboot: Unbreak CPU0 hotplug
      x86/asm/64: Clear AC on NMI entries

commit bbc4615e0b7df5e21d0991adb4b2798508354924
Author: Vikas Shivappa <vikas.shivappa@linux.intel.com>
Date:   Tue Aug 15 18:00:42 2017 -0700

    x86/intel_rdt/mbm: Fix MBM overflow handler during CPU hotplug
    
    When a CPU is dying, the overflow worker is canceled and rescheduled on a
    different CPU in the same domain. But if the timer is already about to
    expire this essentially doubles the interval which might result in a non
    detected overflow.
    
    Cancel the overflow worker and reschedule it immediately on a different CPU
    in same domain. The work could be flushed as well, but that would
    reschedule it on the same CPU.
    
    [ tglx: Rewrote changelog once again ]
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: ravi.v.shankar@intel.com
    Cc: tony.luck@intel.com
    Cc: fenghua.yu@intel.com
    Cc: peterz@infradead.org
    Cc: eranian@google.com
    Cc: vikas.shivappa@intel.com
    Cc: ak@linux.intel.com
    Cc: davidcc@google.com
    Link: http://lkml.kernel.org/r/1502845243-20454-2-git-send-email-vikas.shivappa@linux.intel.com

commit 84393817db09bb436e934f8f8cc981cbca9ea4dc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Aug 15 13:03:47 2017 +0200

    x86/mtrr: Prevent CPU hotplug lock recursion
    
    Larry reported a CPU hotplug lock recursion in the MTRR code.
    
    ============================================
    WARNING: possible recursive locking detected
    
    systemd-udevd/153 is trying to acquire lock:
     (cpu_hotplug_lock.rw_sem){.+.+.+}, at: [<c030fc26>] stop_machine+0x16/0x30
    
     but task is already holding lock:
      (cpu_hotplug_lock.rw_sem){.+.+.+}, at: [<c0234353>] mtrr_add_page+0x83/0x470
    
    ....
    
     cpus_read_lock+0x48/0x90
     stop_machine+0x16/0x30
     mtrr_add_page+0x18b/0x470
     mtrr_add+0x3e/0x70
    
    mtrr_add_page() holds the hotplug rwsem already and calls stop_machine()
    which acquires it again.
    
    Call stop_machine_cpuslocked() instead.
    
    Reported-and-tested-by: Larry Finger <Larry.Finger@lwfinger.net>
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1708140920250.1865@nanos
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Borislav Petkov <bp@suse.de>

commit 450f9689f294c331c56ec37d68302ccc19c7caa2
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Aug 1 09:02:57 2017 +0100

    clocksource/arm_arch_timer: Use static_branch_enable_cpuslocked()
    
    Use the new static_branch_enable_cpuslocked() function to switch
    the workaround static key on the CPU hotplug path.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Leo Yan <leo.yan@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arm-kernel@lists.infradead.org
    Link: http://lkml.kernel.org/r/20170801080257.5056-5-marc.zyngier@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 5a40527f8f0798553764fc8db4111d7d9c33ea51
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Aug 1 09:02:56 2017 +0100

    jump_label: Provide hotplug context variants
    
    As using the normal static key API under the hotplug lock is
    pretty much impossible, let's provide a variant of some of them
    that require the hotplug lock to have already been taken.
    
    These function are only meant to be used in CPU hotplug callbacks.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Leo Yan <leo.yan@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arm-kernel@lists.infradead.org
    Link: http://lkml.kernel.org/r/20170801080257.5056-4-marc.zyngier@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b70cecf4b6b72a9977576ab32cca0e24f286f517
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Aug 1 09:02:54 2017 +0100

    jump_label: Move CPU hotplug locking
    
    As we're about to rework the locking, let's move the taking and
    release of the CPU hotplug lock to locations that will make its
    reworking completely obvious.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Leo Yan <leo.yan@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arm-kernel@lists.infradead.org
    Link: http://lkml.kernel.org/r/20170801080257.5056-2-marc.zyngier@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 26c5c6e129ee725f103938262a034861ada467ae
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Aug 9 22:41:23 2017 +1000

    powerpc/watchdog: Moderate touch_nmi_watchdog overhead
    
    Some code can go into a tight loop calling touch_nmi_watchdog (e.g.,
    stop_machine CPU hotplug code). This can cause contention on watchdog
    locks particularly if all CPUs with watchdog enabled are spinning in
    the loops.
    
    Avoid this storm of activity by running the watchdog timer callback
    from this path if we have exceeded the timer period since it was last
    run.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit de70be0ae3c9283a7d16fd5fbdc03840f01065cf
Merge: 51d96dc2e2dc f930c7043663
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 8 09:38:41 2017 -0700

    Merge tag 'scsi-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi
    
    Pull SCSI fixes from James Bottomley:
     "Two small fixes, one re-fix of a previous fix and five patches sorting
      out hotplug in the bnx2X class of drivers. The latter is rather
      involved, but necessary because these drivers have started dropping
      lockdep recursion warnings on the hotplug lock because of its
      conversion to a percpu rwsem"
    
    * tag 'scsi-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi:
      scsi: sg: only check for dxfer_len greater than 256M
      scsi: aacraid: reading out of bounds
      scsi: qedf: Limit number of CQs
      scsi: bnx2i: Simplify cpu hotplug code
      scsi: bnx2fc: Simplify CPU hotplug code
      scsi: bnx2i: Prevent recursive cpuhotplug locking
      scsi: bnx2fc: Prevent recursive cpuhotplug locking
      scsi: bnx2fc: Plug CPU hotplug race

commit 62208707b466cc3c6ce951a7c4b7b4bb9b9192f6
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Mon Jun 13 18:32:45 2016 +0800

    sched/cputime: Fix prev steal time accouting during CPU hotplug
    
    commit 3d89e5478bf550a50c99e93adf659369798263b0 upstream.
    
    Commit:
    
      e9532e69b8d1 ("sched/cputime: Fix steal time accounting vs. CPU hotplug")
    
    ... set rq->prev_* to 0 after a CPU hotplug comes back, in order to
    fix the case where (after CPU hotplug) steal time is smaller than
    rq->prev_steal_time.
    
    However, this should never happen. Steal time was only smaller because of the
    KVM-specific bug fixed by the previous patch.  Worse, the previous patch
    triggers a bug on CPU hot-unplug/plug operation: because
    rq->prev_steal_time is cleared, all of the CPU's past steal time will be
    accounted again on hot-plug.
    
    Since the root cause has been fixed, we can just revert commit e9532e69b8d1.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 'commit e9532e69b8d1 ("sched/cputime: Fix steal time accounting vs. CPU hotplug")'
    Link: http://lkml.kernel.org/r/1465813966-3116-3-git-send-email-wanpeng.li@hotmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Andres Oportus <andresoportus@google.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 895c663ecef16c8138e20a7d5c052e0fcc400241
Author: Vikas Shivappa <vikas.shivappa@linux.intel.com>
Date:   Tue Jul 25 14:14:44 2017 -0700

    x86/intel_rdt/cqm: Add CPU hotplug support
    
    Resource groups have a per domain directory under "mon_data". Add or
    remove these directories as and when domains come online and go offline.
    Also update the per cpu rmids and cache upon onlining and offlining
    cpus.
    
    Signed-off-by: Vikas Shivappa <vikas.shivappa@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: ravi.v.shankar@intel.com
    Cc: tony.luck@intel.com
    Cc: fenghua.yu@intel.com
    Cc: peterz@infradead.org
    Cc: eranian@google.com
    Cc: vikas.shivappa@intel.com
    Cc: ak@linux.intel.com
    Cc: davidcc@google.com
    Cc: reinette.chatre@intel.com
    Link: http://lkml.kernel.org/r/1501017287-28083-26-git-send-email-vikas.shivappa@linux.intel.com

commit 3d9d7405c0699ade882fec0c1cc6685cd5742ab3
Merge: 080012bad6e2 92bbd16e500c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 28 13:29:36 2017 -0700

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes from Will Deacon:
     "I'd been collecting these whilst we debugged a CPU hotplug failure,
      but we ended up diagnosing that one to tglx, who has taken a fix via
      the -tip tree separately.
    
      We're seeing some NFS issues that we haven't gotten to the bottom of
      yet, and we've uncovered some issues with our backtracing too so there
      might be another fixes pull before we're done.
    
      Summary:
    
       - Ensure we have a guard page after the kernel image in vmalloc
    
       - Fix incorrect prefetch stride in copy_page
    
       - Ensure irqs are disabled in die()
    
       - Fix for event group validation in QCOM L2 PMU driver
    
       - Fix requesting of PMU IRQs on AMD Seattle
    
       - Minor cleanups and fixes"
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: mmu: Place guard page after mapping of kernel image
      drivers/perf: arm_pmu: Request PMU SPIs with IRQF_PER_CPU
      arm64: sysreg: Fix unprotected macro argmuent in write_sysreg
      perf: qcom_l2: fix column exclusion check
      arm64/lib: copy_page: use consistent prefetch stride
      arm64/numa: Drop duplicate message
      perf: Convert to using %pOF instead of full_name
      arm64: Convert to using %pOF instead of full_name
      arm64: traps: disable irq in die()
      arm64: atomics: Remove '&' from '+&' asm constraint in lse atomics
      arm64: uaccess: Remove redundant __force from addr cast in __range_ok

commit 7b7622bb95eb587cbaa79608e47b832a82a262b1
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Jul 27 23:23:37 2017 +1000

    powerpc/smp: Call smp_ops->setup_cpu() directly on the boot CPU
    
    In smp_cpus_done() we need to call smp_ops->setup_cpu() for the boot
    CPU, which means it has to run *on* the boot CPU.
    
    In the past we ensured it ran on the boot CPU by changing the CPU
    affinity mask of current directly. That was removed in commit
    6d11b87d55eb ("powerpc/smp: Replace open coded task affinity logic"),
    and replaced with a work queue call.
    
    Unfortunately using a work queue leads to a lockdep warning, now that
    the CPU hotplug lock is a regular semaphore:
    
      ======================================================
      WARNING: possible circular locking dependency detected
      ...
      kworker/0:1/971 is trying to acquire lock:
       (cpu_hotplug_lock.rw_sem){++++++}, at: [<c000000000100974>] apply_workqueue_attrs+0x34/0xa0
    
      but task is already holding lock:
       ((&wfc.work)){+.+.+.}, at: [<c0000000000fdb2c>] process_one_work+0x25c/0x800
      ...
           CPU0                    CPU1
           ----                    ----
      lock((&wfc.work));
                                   lock(cpu_hotplug_lock.rw_sem);
                                   lock((&wfc.work));
      lock(cpu_hotplug_lock.rw_sem);
    
    Although the deadlock can't happen in practice, because
    smp_cpus_done() only runs in early boot before CPU hotplug is allowed,
    lockdep can't tell that.
    
    Luckily in commit 8fb12156b8db ("init: Pin init task to the boot CPU,
    initially") tglx changed the generic code to pin init to the boot CPU
    to begin with. The unpinning of init from the boot CPU happens in
    sched_init_smp(), which is called after smp_cpus_done().
    
    So smp_cpus_done() is always called on the boot CPU, which means we
    don't need the work queue call at all - and the lockdep warning goes
    away.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

commit 8397913303abc9333f376a518a8368fa22ca5e6e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 27 12:21:11 2017 +0200

    genirq/cpuhotplug: Revert "Set force affinity flag on hotplug migration"
    
    That commit was part of the changes moving x86 to the generic CPU hotplug
    interrupt migration code. The force flag was required on x86 before the
    hierarchical irqdomain rework, but invoking set_affinity() with force=true
    stayed and had no side effects.
    
    At some point in the past, the force flag got repurposed to support the
    exynos timer interrupt affinity setting to a not yet online CPU, so the
    interrupt controller callback does not verify the supplied affinity mask
    against cpu_online_mask.
    
    Setting the flag in the CPU hotplug code causes the cpu online masking to
    be blocked on these irq controllers and results in potentially affining an
    interrupt to the CPU which is unplugged, i.e. instead of moving it away,
    it's just reassigned to it.
    
    As the force flags is not longer needed on x86, it's safe to revert that
    patch so the ARM irqchips which use the force flag work again.
    
    Add comments to that effect, so this won't happen again.
    
    Note: The online mask handling should be done in the generic code and the
    force flag and the masking in the irq chips removed all together, but
    that's not a change possible for 4.13.
    
    Fixes: 77f85e66aa8b ("genirq/cpuhotplug: Set force affinity flag on hotplug migration")
    Reported-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: LAK <linux-arm-kernel@lists.infradead.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1707271217590.3109@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit f9f22a86912f9d36b50e9b3b383fabfb9f22dd46
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 24 12:53:00 2017 +0200

    scsi: bnx2i: Simplify cpu hotplug code
    
    The CPU hotplug related code of this driver can be simplified by:
    
    1) Consolidating the callbacks into a single state. The CPU thread can be
       torn down on the CPU which goes offline. There is no point in delaying
       that to the CPU dead state
    
    2) Let the core code invoke the online/offline callbacks and remove the
       extra for_each_online_cpu() loops.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Chad Dupuis <chad.dupuis@cavium.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit 1937f8a29f4a650bc27e0311b43b53509a34fd22
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 24 12:52:59 2017 +0200

    scsi: bnx2fc: Simplify CPU hotplug code
    
    The CPU hotplug related code of this driver can be simplified by:
    
    1) Consolidating the callbacks into a single state. The CPU thread can be
       torn down on the CPU which goes offline. There is no point in delaying
       that to the CPU dead state
    
    2) Let the core code invoke the online/offline callbacks and remove the
       extra for_each_online_cpu() loops.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit 8addebc14a322fa8ca67cd57c6038069acde8ddc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 24 12:52:56 2017 +0200

    scsi: bnx2fc: Plug CPU hotplug race
    
    bnx2fc_process_new_cqes() has protection against CPU hotplug, which relies
    on the per cpu thread pointer. This protection is racy because it happens
    only partially with the per cpu fp_work_lock held.
    
    If the CPU is unplugged after the lock is dropped, the wakeup code can
    dereference a NULL pointer or access freed and potentially reused memory.
    
    Restructure the code so the thread check and wakeup happens with the
    fp_work_lock held.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Chad Dupuis <chad.dupuis@cavium.com>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit f74c89bd80fb3f1328fdf4a44eeba793cdce4222
Author: Anju T Sudhakar <anju@linux.vnet.ibm.com>
Date:   Wed Jul 19 03:06:36 2017 +0530

    powerpc/perf: Add thread IMC PMU support
    
    Add support to register Thread In-Memory Collection PMU counters.
    Patch adds thread IMC specific data structures, along with memory
    init functions and CPU hotplug support.
    
    Signed-off-by: Anju T Sudhakar <anju@linux.vnet.ibm.com>
    Signed-off-by: Hemant Kumar <hemant@linux.vnet.ibm.com>
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 39a846db1d574a498511ffccd75223a35cdcb059
Author: Anju T Sudhakar <anju@linux.vnet.ibm.com>
Date:   Wed Jul 19 03:06:35 2017 +0530

    powerpc/perf: Add core IMC PMU support
    
    Add support to register Core In-Memory Collection PMU counters.
    Patch adds core IMC specific data structures, along with memory
    init functions and CPU hotplug support.
    
    Signed-off-by: Anju T Sudhakar <anju@linux.vnet.ibm.com>
    Signed-off-by: Hemant Kumar <hemant@linux.vnet.ibm.com>
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 885dcd709ba9120b9935415b8b0f9d1b94e5826b
Author: Anju T Sudhakar <anju@linux.vnet.ibm.com>
Date:   Wed Jul 19 03:06:34 2017 +0530

    powerpc/perf: Add nest IMC PMU support
    
    Add support to register Nest In-Memory Collection PMU counters.
    Patch adds a new device file called "imc-pmu.c" under powerpc/perf
    folder to contain all the device PMU functions.
    
    Device tree parser code added to parse the PMU events information
    and create sysfs event attributes for the PMU.
    
    Cpumask attribute added along with Cpu hotplug online/offline functions
    specific for nest PMU. A new state "CPUHP_AP_PERF_POWERPC_NEST_IMC_ONLINE"
    added for the cpu hotplug callbacks. Error handle path frees the memory
    and unregisters the CPU hotplug callbacks.
    
    Signed-off-by: Anju T Sudhakar <anju@linux.vnet.ibm.com>
    Signed-off-by: Hemant Kumar <hemant@linux.vnet.ibm.com>
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 0c96b27305faf06c068b45e07d28336c80dac286
Author: Ethan Barnes <Ethan.Barnes@wdc.com>
Date:   Wed Jul 19 22:36:00 2017 +0000

    smp/hotplug: Handle removal correctly in cpuhp_store_callbacks()
    
    If cpuhp_store_callbacks() is called for CPUHP_AP_ONLINE_DYN or
    CPUHP_BP_PREPARE_DYN, which are the indicators for dynamically allocated
    states, then cpuhp_store_callbacks() allocates a new dynamic state. The
    first allocation in each range returns CPUHP_AP_ONLINE_DYN or
    CPUHP_BP_PREPARE_DYN.
    
    If cpuhp_remove_state() is invoked for one of these states, then there is
    no protection against the allocation mechanism. So the removal, which
    should clear the callbacks and the name, gets a new state assigned and
    clears that one.
    
    As a consequence the state which should be cleared stays initialized. A
    consecutive CPU hotplug operation dereferences the state callbacks and
    accesses either freed or reused memory, resulting in crashes.
    
    Add a protection against this by checking the name argument for NULL. If
    it's NULL it's a removal. If not, it's an allocation.
    
    [ tglx: Added a comment and massaged changelog ]
    
    Fixes: 5b7aa87e0482 ("cpu/hotplug: Implement setup/removal interface")
    Signed-off-by: Ethan Barnes <ethan.barnes@sandisk.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.or>
    Cc: "Srivatsa S. Bhat" <srivatsa@mit.edu>
    Cc: Sebastian Siewior <bigeasy@linutronix.d>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/DM2PR04MB398242FC7776D603D9F99C894A60@DM2PR04MB398.namprd04.prod.outlook.com

commit fded17be01abfefe7218a72df703d8fe6b28206f
Author: Zhou Chengming <zhouchengming1@huawei.com>
Date:   Mon Jan 16 11:21:11 2017 +0800

    perf/x86/intel: Handle exclusive threadid correctly on CPU hotplug
    
    
    [ Upstream commit 4e71de7986386d5fd3765458f27d612931f27f5e ]
    
    The CPU hotplug function intel_pmu_cpu_starting() sets
    cpu_hw_events.excl_thread_id unconditionally to 1 when the shared exclusive
    counters data structure is already availabe for the sibling thread.
    
    This works during the boot process because the first sibling gets threadid
    0 assigned and the second sibling which shares the data structure gets 1.
    
    But when the first thread of the core is offlined and onlined again it
    shares the data structure with the second thread and gets exclusive thread
    id 1 assigned as well.
    
    Prevent this by checking the threadid of the already online thread.
    
    [ tglx: Rewrote changelog ]
    
    Signed-off-by: Zhou Chengming <zhouchengming1@huawei.com>
    Cc: NuoHan Qiao <qiaonuohan@huawei.com>
    Cc: ak@linux.intel.com
    Cc: peterz@infradead.org
    Cc: kan.liang@intel.com
    Cc: dave.hansen@linux.intel.com
    Cc: eranian@google.com
    Cc: qiaonuohan@huawei.com
    Cc: davidcc@google.com
    Cc: guohanjun@huawei.com
    Link: http://lkml.kernel.org/r/1484536871-3131-1-git-send-email-zhouchengming1@huawei.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9a9594efe54324e9124add7e7b1e7bdb6d0b08a3
Merge: 3ad918e65d69 993647a29381
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 3 18:08:06 2017 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull SMP hotplug updates from Thomas Gleixner:
     "This update is primarily a cleanup of the CPU hotplug locking code.
    
      The hotplug locking mechanism is an open coded RWSEM, which allows
      recursive locking. The main problem with that is the recursive nature
      as it evades the full lockdep coverage and hides potential deadlocks.
    
      The rework replaces the open coded RWSEM with a percpu RWSEM and
      establishes full lockdep coverage that way.
    
      The bulk of the changes fix up recursive locking issues and address
      the now fully reported potential deadlocks all over the place. Some of
      these deadlocks have been observed in the RT tree, but on mainline the
      probability was low enough to hide them away."
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (37 commits)
      cpu/hotplug: Constify attribute_group structures
      powerpc: Only obtain cpu_hotplug_lock if called by rtasd
      ARM/hw_breakpoint: Fix possible recursive locking for arch_hw_breakpoint_init
      cpu/hotplug: Remove unused check_for_tasks() function
      perf/core: Don't release cred_guard_mutex if not taken
      cpuhotplug: Link lock stacks for hotplug callbacks
      acpi/processor: Prevent cpu hotplug deadlock
      sched: Provide is_percpu_thread() helper
      cpu/hotplug: Convert hotplug locking to percpu rwsem
      s390: Prevent hotplug rwsem recursion
      arm: Prevent hotplug rwsem recursion
      arm64: Prevent cpu hotplug rwsem recursion
      kprobes: Cure hotplug lock ordering issues
      jump_label: Reorder hotplug lock and jump_label_lock
      perf/tracing/cpuhotplug: Fix locking order
      ACPI/processor: Use cpu_hotplug_disable() instead of get_online_cpus()
      PCI: Replace the racy recursion prevention
      PCI: Use cpu_hotplug_disable() instead of get_online_cpus()
      perf/x86/intel: Drop get_online_cpus() in intel_snb_check_microcode()
      x86/perf: Drop EXPORT of perf_check_microcode
      ...

commit 37bc3e5fd764fb258ff4fcbb90b6d1b67fb466c1
Author: Balbir Singh <bsingharora@gmail.com>
Date:   Thu Jun 29 03:04:05 2017 +1000

    powerpc/lib/code-patching: Use alternate map for patch_instruction()
    
    This patch creates the window using text_poke_area, allocated via
    get_vm_area(). text_poke_area is per CPU to avoid locking.
    text_poke_area for each cpu is setup using late_initcall, prior to
    setup of these alternate mapping areas, we continue to use direct
    write to change/modify kernel text. With the ability to use alternate
    mappings to write to kernel text, it provides us the freedom to then
    turn text read-only and implement CONFIG_STRICT_KERNEL_RWX.
    
    This code is CPU hotplug aware to ensure that the we have mappings for
    any new cpus as they come online and tear down mappings for any CPUs
    that go offline.
    
    Signed-off-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 3e401f7a2e5199151f735aee6a5c6b4776e6a35e
Author: Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>
Date:   Tue Jun 20 19:08:30 2017 -0300

    powerpc: Only obtain cpu_hotplug_lock if called by rtasd
    
    Calling arch_update_cpu_topology from a CPU hotplug state machine callback
    hits a deadlock because the function tries to get a read lock on
    cpu_hotplug_lock while the state machine still holds a write lock on it.
    
    Since all callers of arch_update_cpu_topology except rtasd already hold
    cpu_hotplug_lock, this patch changes the function to use
    stop_machine_cpuslocked and creates a separate function for rtasd which
    still tries to obtain the lock.
    
    Michael Bringmann investigated the bug and provided a detailed analysis
    of the deadlock on this previous RFC for an alternate solution:
    
    Signed-off-by: Thiago Jung Bauermann <bauerman@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: John Allen <jallen@linux.vnet.ibm.com>
    Cc: Michael Bringmann <mwb@linux.vnet.ibm.com>
    Cc: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/1497996510-4032-1-git-send-email-bauerman@linux.vnet.ibm.com
    Link: https://patchwork.ozlabs.org/patch/771293/

commit c5cb83bb337c25caae995d992d1cdf9b317f83de
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 20 01:37:51 2017 +0200

    genirq/cpuhotplug: Handle managed IRQs on CPU hotplug
    
    If a CPU goes offline, interrupts affine to the CPU are moved away. If the
    outgoing CPU is the last CPU in the affinity mask the migration code breaks
    the affinity and sets it it all online cpus.
    
    This is a problem for affinity managed interrupts as CPU hotplug is often
    used for power management purposes. If the affinity is broken, the
    interrupt is not longer affine to the CPUs to which it was allocated.
    
    The affinity spreading allows to lay out multi queue devices in a way that
    they are assigned to a single CPU or a group of CPUs. If the last CPU goes
    offline, then the queue is not longer used, so the interrupt can be
    shutdown gracefully and parked until one of the assigned CPUs comes online
    again.
    
    Add a graceful shutdown mechanism into the irq affinity breaking code path,
    mark the irq as MANAGED_SHUTDOWN and leave the affinity mask unmodified.
    
    In the online path, scan the active interrupts for managed interrupts and
    if the interrupt is functional and the newly online CPU is part of the
    affinity mask, restart the interrupt if it is marked MANAGED_SHUTDOWN or if
    the interrupts is started up, try to add the CPU back to the effective
    affinity mask.
    
    Originally-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20170619235447.273417334@linutronix.de

commit 0d3f54257dc300f2db480d6a46b34bdb87f18c1b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 20 01:37:38 2017 +0200

    genirq: Introduce effective affinity mask
    
    There is currently no way to evaluate the effective affinity mask of a
    given interrupt. Many irq chips allow only a single target CPU or a subset
    of CPUs in the affinity mask.
    
    Updating the mask at the time of setting the affinity to the subset would
    be counterproductive because information for cpu hotplug about assigned
    interrupt affinities gets lost. On CPU hotplug it's also pointless to force
    migrate an interrupt, which is not targeted at the CPU effectively. But
    currently the information is not available.
    
    Provide a seperate mask to be updated by the irq_chip->irq_set_affinity()
    implementations. Implement the read only proc files so the user can see the
    effective mask as well w/o trying to deduce it from /proc/interrupts.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Link: http://lkml.kernel.org/r/20170619235446.247834245@linutronix.de

commit 0fa4ce746d1d8c8aa3d09fbc675497fa4c4a5475
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Thu May 18 10:07:39 2017 +0200

    irqchip/armada-370-xp: Re-enable per-CPU interrupts at resume time
    
    Commit d17cab4451df1 ("irqchip: Kill off set_irq_flags usage") changed
    the code of armada_370_xp_mpic_irq_map() from using set_irq_flags() to
    irq_set_probe().
    
    While the commit log seems to imply that there are no functional
    changes, there are indeed functional changes introduced by this commit:
    the IRQ_NOAUTOEN flag is no longer cleared. This functional change
    caused a regression on Armada XP, which no longer works properly after
    suspend/resume because per-CPU interrupts remain disabled. This
    regression was temporarly worked around in commit
    353d6d6c82e5d ("irqchip/armada-370-xp: Fix regression by clearing
    IRQ_NOAUTOEN"), but it is not the most satisfying solution. This commit
    implements the solution that was initially discussed with Thomas
    Gleixner.
    
    Due to how the hardware registers work, the irq-armada-370-xp cannot
    simply save/restore a bunch of registers at suspend/resume to make sure
    that the interrupts remain in the same state after resuming. Therefore,
    it relies on the kernel to say whether the interrupt is disabled or not,
    using the irqd_irq_disabled() function. This was all working fine while
    the IRQ_NOAUTOEN flag was cleared.
    
    With the change introduced by Rob Herring in d17cab4451df1, the
    IRQ_NOAUTOEN flag is now set for all interrupts. irqd_irq_disabled()
    returns false for per-CPU interrupts, and therefore our per-CPU
    interrupts are no longer re-enabled after resume.
    
    This commit fixes that by using irqd_irq_disabled() only for global
    interrupts, and using the newly introduced irq_percpu_is_enabled() for
    per-CPU interrupts.
    
    Also, it fixes a related problems that per-CPU interrupts were only
    re-enabled on the boot CPU and not other CPUs. Until now this wasn't a
    problem since on this platform, only the local timers are using per-CPU
    interrupts and the local timers of secondary CPUs are turned off/on
    during CPU hotplug before suspend, after after resume. However, since
    Linux 4.4, we are also be using per-CPU interrupts for the network
    controller, so we need to properly restore the per-CPU interrupts on
    secondary CPUs as well.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

commit a7e7ade1b4c9e3fdf3a33c0cdb45682a54f4b638
Author: Dominik Brodowski <linux@dominikbrodowski.net>
Date:   Wed Jun 7 11:58:19 2017 +0200

    x86/microcode/intel: Clear patch pointer before jettisoning the initrd
    
    commit 5b0bc9ac2ce4881ee318a21f31140584ce4dbdad upstream.
    
    During early boot, load_ucode_intel_ap() uses __load_ucode_intel()
    to obtain a pointer to the relevant microcode patch (embedded in the
    initrd), and stores this value in 'intel_ucode_patch' to speed up the
    microcode patch application for subsequent CPUs.
    
    On resuming from suspend-to-RAM, however, load_ucode_ap() calls
    load_ucode_intel_ap() for each non-boot-CPU. By then the initramfs is
    long gone so the pointer stored in 'intel_ucode_patch' no longer points to
    a valid microcode patch.
    
    Clear that pointer so that we effectively fall back to the CPU hotplug
    notifier callbacks to update the microcode.
    
    Signed-off-by: Dominik Brodowski <linux@dominikbrodowski.net>
    [ Edit and massage commit message. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170607095819.9754-1-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 97d8b6e3b8538198aefb0003342920a82e062147
Author: Ashwanth Goli <ashwanth@codeaurora.org>
Date:   Tue Jun 13 16:54:55 2017 +0530

    net: rps: fix uninitialized symbol warning
    
    This patch fixes uninitialized symbol warning that
    got introduced by the following commit
    773fc8f6e8d6 ("net: rps: send out pending IPI's on CPU hotplug")
    
    Signed-off-by: Ashwanth Goli <ashwanth@codeaurora.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 45b44f0f287d6d396b78466e13be1d1ea3d3097b
Merge: 6b7ed4588ce6 40da1b11f01e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 10 10:49:42 2017 -0700

    Merge branch 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug fix from Ingo Molnar:
     "An error handling corner case fix"
    
    * 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Drop the device lock on error

commit 773fc8f6e8d63ec9d840588e161cbb73a01cfc45
Author: ashwanth@codeaurora.org <ashwanth@codeaurora.org>
Date:   Fri Jun 9 14:24:58 2017 +0530

    net: rps: send out pending IPI's on CPU hotplug
    
    IPI's from the victim cpu are not handled in dev_cpu_callback.
    So these pending IPI's would be sent to the remote cpu only when
    NET_RX is scheduled on the victim cpu and since this trigger is
    unpredictable it would result in packet latencies on the remote cpu.
    
    This patch add support to send the pending ipi's of victim cpu.
    
    Signed-off-by: Ashwanth Goli <ashwanth@codeaurora.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 5b0bc9ac2ce4881ee318a21f31140584ce4dbdad
Author: Dominik Brodowski <linux@dominikbrodowski.net>
Date:   Wed Jun 7 11:58:19 2017 +0200

    x86/microcode/intel: Clear patch pointer before jettisoning the initrd
    
    During early boot, load_ucode_intel_ap() uses __load_ucode_intel()
    to obtain a pointer to the relevant microcode patch (embedded in the
    initrd), and stores this value in 'intel_ucode_patch' to speed up the
    microcode patch application for subsequent CPUs.
    
    On resuming from suspend-to-RAM, however, load_ucode_ap() calls
    load_ucode_intel_ap() for each non-boot-CPU. By then the initramfs is
    long gone so the pointer stored in 'intel_ucode_patch' no longer points to
    a valid microcode patch.
    
    Clear that pointer so that we effectively fall back to the CPU hotplug
    notifier callbacks to update the microcode.
    
    Signed-off-by: Dominik Brodowski <linux@dominikbrodowski.net>
    [ Edit and massage commit message. ]
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: <stable@vger.kernel.org> # 4.10..
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170607095819.9754-1-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 49dfe2a6779717d9c18395684ee31bdc98b22e53
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 24 10:15:43 2017 +0200

    cpuhotplug: Link lock stacks for hotplug callbacks
    
    The CPU hotplug callbacks are not covered by lockdep versus the cpu hotplug
    rwsem.
    
    CPU0                                            CPU1
    cpuhp_setup_state(STATE, startup, teardown);
     cpus_read_lock();
      invoke_callback_on_ap();
        kick_hotplug_thread(ap);
        wait_for_completion();                      hotplug_thread_fn()
                                                      lock(m);
                                                      do_stuff();
                                                      unlock(m);
    
    Lockdep does not know about this dependency and will not trigger on the
    following code sequence:
    
              lock(m);
              cpus_read_lock();
    
    Add a lockdep map and connect the initiators lock chain with the hotplug
    thread lock chain, so potential deadlocks can be detected.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170524081549.709375845@linutronix.de

commit 0266d81e9bf5cc1fe6405c0523dfa015fe55aae1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 24 10:15:42 2017 +0200

    acpi/processor: Prevent cpu hotplug deadlock
    
    With the enhanced CPU hotplug lockdep coverage the following lockdep splat
    happens:
    
    ======================================================
    WARNING: possible circular locking dependency detected
    4.12.0-rc2+ #84 Tainted: G        W
    ------------------------------------------------------
    cpuhp/1/15 is trying to acquire lock:
    flush_work+0x39/0x2f0
    
    but task is already holding lock:
    cpuhp_thread_fun+0x30/0x160
    
    which lock already depends on the new lock.
    
    the existing dependency chain (in reverse order) is:
    
    -> #2 (cpuhp_state){+.+.+.}:
           lock_acquire+0xb4/0x200
           cpuhp_kick_ap_work+0x72/0x330
           _cpu_down+0x8b/0x100
           do_cpu_down+0x3e/0x60
           cpu_down+0x10/0x20
           cpu_subsys_offline+0x14/0x20
           device_offline+0x88/0xb0
           online_store+0x4c/0xa0
           dev_attr_store+0x18/0x30
           sysfs_kf_write+0x45/0x60
           kernfs_fop_write+0x156/0x1e0
           __vfs_write+0x37/0x160
           vfs_write+0xca/0x1c0
           SyS_write+0x58/0xc0
           entry_SYSCALL_64_fastpath+0x23/0xc2
    
    -> #1 (cpu_hotplug_lock.rw_sem){++++++}:
           lock_acquire+0xb4/0x200
           cpus_read_lock+0x3d/0xb0
           apply_workqueue_attrs+0x17/0x50
           __alloc_workqueue_key+0x1e1/0x530
           scsi_host_alloc+0x373/0x480 [scsi_mod]
           ata_scsi_add_hosts+0xcb/0x130 [libata]
           ata_host_register+0x11a/0x2c0 [libata]
           ata_host_activate+0xf0/0x150 [libata]
           ahci_host_activate+0x13e/0x170 [libahci]
           ahci_init_one+0xa3a/0xd3f [ahci]
           local_pci_probe+0x45/0xa0
           work_for_cpu_fn+0x14/0x20
           process_one_work+0x1f9/0x690
           worker_thread+0x200/0x3d0
           kthread+0x138/0x170
           ret_from_fork+0x31/0x40
    
    -> #0 ((&wfc.work)){+.+.+.}:
           __lock_acquire+0x11e1/0x13e0
           lock_acquire+0xb4/0x200
           flush_work+0x5c/0x2f0
           work_on_cpu+0xa1/0xd0
           acpi_processor_get_throttling+0x3d/0x50
           acpi_processor_reevaluate_tstate+0x2c/0x50
           acpi_soft_cpu_online+0x69/0xd0
           cpuhp_invoke_callback+0xb4/0x8b0
           cpuhp_up_callbacks+0x36/0xc0
           cpuhp_thread_fun+0x14e/0x160
           smpboot_thread_fn+0x1e8/0x300
           kthread+0x138/0x170
           ret_from_fork+0x31/0x40
    
    other info that might help us debug this:
    
    Chain exists of:
      (&wfc.work) --> cpu_hotplug_lock.rw_sem --> cpuhp_state
    
     Possible unsafe locking scenario:
    
           CPU0                    CPU1
           ----                    ----
      lock(cpuhp_state);
                                   lock(cpu_hotplug_lock.rw_sem);
                                   lock(cpuhp_state);
      lock((&wfc.work));
    
     *** DEADLOCK ***
    
    1 lock held by cpuhp/1/15:
    cpuhp_thread_fun+0x30/0x160
    
    stack backtrace:
    CPU: 1 PID: 15 Comm: cpuhp/1 Tainted: G        W       4.12.0-rc2+ #84
    Hardware name: Supermicro SYS-4048B-TR4FT/X10QBi, BIOS 1.1a 07/29/2015
    Call Trace:
     dump_stack+0x85/0xc4
     print_circular_bug+0x209/0x217
     __lock_acquire+0x11e1/0x13e0
     lock_acquire+0xb4/0x200
     ? lock_acquire+0xb4/0x200
     ? flush_work+0x39/0x2f0
     ? acpi_processor_start+0x50/0x50
     flush_work+0x5c/0x2f0
     ? flush_work+0x39/0x2f0
     ? acpi_processor_start+0x50/0x50
     ? mark_held_locks+0x6d/0x90
     ? queue_work_on+0x56/0x90
     ? trace_hardirqs_on_caller+0x154/0x1c0
     ? trace_hardirqs_on+0xd/0x10
     ? acpi_processor_start+0x50/0x50
     work_on_cpu+0xa1/0xd0
     ? find_worker_executing_work+0x50/0x50
     ? acpi_processor_power_exit+0x70/0x70
     acpi_processor_get_throttling+0x3d/0x50
     acpi_processor_reevaluate_tstate+0x2c/0x50
     acpi_soft_cpu_online+0x69/0xd0
     cpuhp_invoke_callback+0xb4/0x8b0
     ? lock_acquire+0xb4/0x200
     ? padata_replace+0x120/0x120
     cpuhp_up_callbacks+0x36/0xc0
     cpuhp_thread_fun+0x14e/0x160
     smpboot_thread_fn+0x1e8/0x300
     kthread+0x138/0x170
     ? sort_range+0x30/0x30
     ? kthread_create_on_node+0x70/0x70
     ret_from_fork+0x31/0x40
    
    The problem is that the work is scheduled on the current CPU from the
    hotplug thread associated with that CPU.
    
    It's not required to invoke these functions via the workqueue because the
    hotplug thread runs on the target CPU already.
    
    Check whether current is a per cpu thread pinned on the target CPU and
    invoke the function directly to avoid the workqueue.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: linux-acpi@vger.kernel.org
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170524081549.620489733@linutronix.de

commit f2545b2d4ce13e068897ef60ae64dffe215f4152
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 24 10:15:35 2017 +0200

    jump_label: Reorder hotplug lock and jump_label_lock
    
    The conversion of the hotplug locking to a percpu rwsem unearthed lock
    ordering issues all over the place.
    
    The jump_label code has two issues:
    
     1) Nested get_online_cpus() invocations
    
     2) Ordering problems vs. the cpus rwsem and the jump_label_mutex
    
    To cure these, the following lock order has been established;
    
       cpus_rwsem -> jump_label_lock -> text_mutex
    
    Even if not all architectures need protection against CPU hotplug, taking
    cpus_rwsem before jump_label_lock is now mandatory in code pathes which
    actually modify code and therefor need text_mutex protection.
    
    Move the get_online_cpus() invocations into the core jump label code and
    establish the proper lock order where required.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: "David S. Miller" <davem@davemloft.net>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Jason Baron <jbaron@akamai.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Link: http://lkml.kernel.org/r/20170524081549.025830817@linutronix.de

commit 547efeadd42a3c75e41e33c0637cba100fc18289
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed May 24 10:15:19 2017 +0200

    x86/mtrr: Remove get_online_cpus() from mtrr_save_state()
    
    mtrr_save_state() is invoked from native_cpu_up() which is in the context
    of a CPU hotplug operation and therefor calling get_online_cpus() is
    pointless.
    
    While this works in the current get_online_cpus() implementation it
    prevents from converting the hotplug locking to percpu rwsems.
    
    Remove it.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20170524081547.651378834@linutronix.de

commit eb0807b288ac5b75d80553148616160ebb4cb51f
Author: Tyrel Datwyler <tyreld@linux.vnet.ibm.com>
Date:   Mon Apr 17 20:24:39 2017 -0400

    powerpc/sysfs: Fix reference leak of cpu device_nodes present at boot
    
    commit e76ca27790a514590af782f83f6eae49e0ccf8c9 upstream.
    
    For CPUs present at boot each logical CPU acquires a reference to the
    associated device node of the core. This happens in register_cpu() which
    is called by topology_init(). The result of this is that we end up with
    a reference held by each thread of the core. However, these references
    are never freed if the CPU core is DLPAR removed.
    
    This patch fixes the reference leaks by acquiring and releasing the references
    in the CPU hotplug callbacks un/register_cpu_online(). With this patch symmetric
    reference counting is observed with both CPUs present at boot, and those DLPAR
    added after boot.
    
    Fixes: f86e4718f24b ("driver/core: cpu: initialize of_node in cpu's device struture")
    Signed-off-by: Tyrel Datwyler <tyreld@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 411fe24e6b7c283c3a1911450cdba6dd3aaea56e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Apr 21 16:00:54 2017 +0200

    nohz: Fix collision between tick and other hrtimers, again
    
    This restores commit:
    
      24b91e360ef5: ("nohz: Fix collision between tick and other hrtimers")
    
    ... which got reverted by commit:
    
      558e8e27e73f: ('Revert "nohz: Fix collision between tick and other hrtimers"')
    
    ... due to a regression where CPUs spuriously stopped ticking.
    
    The bug happened when a tick fired too early past its expected expiration:
    on IRQ exit the tick was scheduled again to the same deadline but skipped
    reprogramming because ts->next_tick still kept in cache the deadline.
    This has been fixed now with resetting ts->next_tick from the tick
    itself. Extra care has also been taken to prevent from obsolete values
    throughout CPU hotplug operations.
    
    When the tick is stopped and an interrupt occurs afterward, we check on
    that interrupt exit if the next tick needs to be rescheduled. If it
    doesn't need any update, we don't want to do anything.
    
    In order to check if the tick needs an update, we compare it against the
    clockevent device deadline. Now that's a problem because the clockevent
    device is at a lower level than the tick itself if it is implemented
    on top of hrtimer.
    
    Every hrtimer share this clockevent device. So comparing the next tick
    deadline against the clockevent device deadline is wrong because the
    device may be programmed for another hrtimer whose deadline collides
    with the tick. As a result we may end up not reprogramming the tick
    accidentally.
    
    In a worst case scenario under full dynticks mode, the tick stops firing
    as it is supposed to every 1hz, leaving /proc/stat stalled:
    
          Task in a full dynticks CPU
          ----------------------------
    
          * hrtimer A is queued 2 seconds ahead
          * the tick is stopped, scheduled 1 second ahead
          * tick fires 1 second later
          * on tick exit, nohz schedules the tick 1 second ahead but sees
            the clockevent device is already programmed to that deadline,
            fooled by hrtimer A, the tick isn't rescheduled.
          * hrtimer A is cancelled before its deadline
          * tick never fires again until an interrupt happens...
    
    In order to fix this, store the next tick deadline to the tick_sched
    local structure and reuse that value later to check whether we need to
    reprogram the clock after an interrupt.
    
    On the other hand, ts->sleep_length still wants to know about the next
    clock event and not just the tick, so we want to improve the related
    comment to avoid confusion.
    
    Reported-and-tested-by: Tim Wright <tim@binbash.co.uk>
    Reported-and-tested-by: Pavel Machek <pavel@ucw.cz>
    Reported-by: James Hartsock <hartsjc@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/1492783255-5051-2-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 7ad6de43deda083f1eab1e9a5cc48166e4ee52f5
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Mar 14 16:06:45 2017 +0100

    cpu/hotplug: Serialize callback invocations proper
    
    commit dc434e056fe1dada20df7ba07f32739d3a701adf upstream.
    
    The setup/remove_state/instance() functions in the hotplug core code are
    serialized against concurrent CPU hotplug, but unfortunately not serialized
    against themself.
    
    As a consequence a concurrent invocation of these function results in
    corruption of the callback machinery because two instances try to invoke
    callbacks on remote cpus at the same time. This results in missing callback
    invocations and initiator threads waiting forever on the completion.
    
    The obvious solution to replace get_cpu_online() with cpu_hotplug_begin()
    is not possible because at least one callsite calls into these functions
    from a get_online_cpu() locked region.
    
    Extend the protection scope of the cpuhp_state_mutex from solely protecting
    the state arrays to cover the callback invocation machinery as well.
    
    Fixes: 5b7aa87e0482 ("cpu/hotplug: Implement setup/removal interface")
    Reported-and-tested-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: hpa@zytor.com
    Cc: mingo@kernel.org
    Cc: akpm@linux-foundation.org
    Cc: torvalds@linux-foundation.org
    Link: http://lkml.kernel.org/r/20170314150645.g4tdyoszlcbajmna@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 044f1daaaaf7c86bc4fcf433848b7baae236946b
Merge: d557d1b58b35 daaadb3e9453
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat May 6 11:25:08 2017 -0700

    Merge branch 'for-linus' of git://git.kernel.dk/linux-block
    
    Pull block fixes and updates from Jens Axboe:
     "Some fixes and followup features/changes that should go in, in this
      merge window. This contains:
    
       - Two fixes for lightnvm from Javier, fixing problems in the new code
         merge previously in this merge window.
    
       - A fix from Jan for the backing device changes, fixing an issue in
         NFS that causes a failure to mount on certain setups.
    
       - A change from Christoph, cleaning up the blk-mq init and exit
         request paths.
    
       - Remove elevator_change(), which is now unused. From Bart.
    
       - A fix for queue operation invocation on a dead queue, from Bart.
    
       - A series fixing up mtip32xx for blk-mq scheduling, removing a
         bandaid we previously had in place for this. From me.
    
       - A regression fix for this series, fixing a case where we wait on
         workqueue flushing from an invalid (non-blocking) context. From me.
    
       - A fix/optimization from Ming, ensuring that we don't both quiesce
         and freeze a queue at the same time.
    
       - A fix from Peter on lock ordering for CPU hotplug. Not a real
         problem right now, but will be once the CPU hotplug rework goes in.
    
       - A series from Omar, cleaning up out blk-mq debugfs support, and
         adding support for exporting info from schedulers in debugfs as
         well. This is really useful in debugging stalls or livelocks. From
         Omar"
    
    * 'for-linus' of git://git.kernel.dk/linux-block: (28 commits)
      mq-deadline: add debugfs attributes
      kyber: add debugfs attributes
      blk-mq-debugfs: allow schedulers to register debugfs attributes
      blk-mq: untangle debugfs and sysfs
      blk-mq: move debugfs declarations to a separate header file
      blk-mq: Do not invoke queue operations on a dead queue
      blk-mq-debugfs: get rid of a bunch of boilerplate
      blk-mq-debugfs: rename hw queue directories from <n> to hctx<n>
      blk-mq-debugfs: don't open code strstrip()
      blk-mq-debugfs: error on long write to queue "state" file
      blk-mq-debugfs: clean up flag definitions
      blk-mq-debugfs: separate flags with |
      nfs: Fix bdi handling for cloned superblocks
      block/mq: Cure cpu hotplug lock inversion
      lightnvm: fix bad back free on error path
      lightnvm: create cmd before allocating request
      blk-mq: don't use sync workqueue flushing from drivers
      mtip32xx: convert internal commands to regular block infrastructure
      mtip32xx: cleanup internal tag assumptions
      block: don't call blk_mq_quiesce_queue() after queue is frozen
      ...

commit 9c35baf6cee9a5745d55de6f9995916dde642517
Merge: dd23f273d9a7 cf39bf58afda
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 3 18:29:28 2017 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/pmladek/printk
    
    Pull printk updates from Petr Mladek:
    
     - There is a situation when early console is not deregistered because
       the preferred one matches a wrong entry. It caused messages to appear
       twice.
    
       This is the 2nd attempt to fix it. The first one was wrong, see the
       commit c6c7d83b9c9e ('Revert "console: don't prefer first registered
       if DT specifies stdout-path"').
    
       The fix is coupled with some small code clean up. Well, the console
       registration code would deserve a big one. We need to think about it.
    
     - Do not lose information about the preemtive context when the console
       semaphore is re-taken.
    
     - Do not block CPU hotplug when someone else is already pushing
       messages to the console.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/pmladek/printk:
      printk: fix double printing with earlycon
      printk: rename selected_console -> preferred_console
      printk: fix name/type/scope of preferred_console var
      printk: Correctly handle preemption in console_unlock()
      printk: use console_trylock() in console_cpu_notify()

commit 13d97094021757db39ad52e4be39662f2f685e5f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Mar 14 16:06:45 2017 +0100

    cpu/hotplug: Serialize callback invocations proper
    
    commit dc434e056fe1dada20df7ba07f32739d3a701adf upstream.
    
    The setup/remove_state/instance() functions in the hotplug core code are
    serialized against concurrent CPU hotplug, but unfortunately not serialized
    against themself.
    
    As a consequence a concurrent invocation of these function results in
    corruption of the callback machinery because two instances try to invoke
    callbacks on remote cpus at the same time. This results in missing callback
    invocations and initiator threads waiting forever on the completion.
    
    The obvious solution to replace get_cpu_online() with cpu_hotplug_begin()
    is not possible because at least one callsite calls into these functions
    from a get_online_cpu() locked region.
    
    Extend the protection scope of the cpuhp_state_mutex from solely protecting
    the state arrays to cover the callback invocation machinery as well.
    
    Fixes: 5b7aa87e0482 ("cpu/hotplug: Implement setup/removal interface")
    Reported-and-tested-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: hpa@zytor.com
    Cc: mingo@kernel.org
    Cc: akpm@linux-foundation.org
    Cc: torvalds@linux-foundation.org
    Link: http://lkml.kernel.org/r/20170314150645.g4tdyoszlcbajmna@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3527d3e9514f013f361fba29fd71858d9361049d
Merge: 3711c94fd659 21173d0b4d2a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 1 19:12:53 2017 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - another round of rq-clock handling debugging, robustization and
         fixes
    
       - PELT accounting improvements
    
       - CPU hotplug related ->cpus_allowed affinity handling fixes all
         around the tree
    
       - ... plus misc fixes, cleanups and updates"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (35 commits)
      sched/x86: Update reschedule warning text
      crypto: N2 - Replace racy task affinity logic
      cpufreq/sparc-us2e: Replace racy task affinity logic
      cpufreq/sparc-us3: Replace racy task affinity logic
      cpufreq/sh: Replace racy task affinity logic
      cpufreq/ia64: Replace racy task affinity logic
      ACPI/processor: Replace racy task affinity logic
      ACPI/processor: Fix error handling in __acpi_processor_start()
      sparc/sysfs: Replace racy task affinity logic
      powerpc/smp: Replace open coded task affinity logic
      ia64/sn/hwperf: Replace racy task affinity logic
      ia64/salinfo: Replace racy task affinity logic
      workqueue: Provide work_on_cpu_safe()
      ia64/topology: Remove cpus_allowed manipulation
      sched/fair: Move the PELT constants into a generated header
      sched/fair: Increase PELT accuracy for small tasks
      sched/fair: Fix comments
      sched/Documentation: Add 'sched-pelt' tool
      sched/fair: Fix corner case in __accumulate_sum()
      sched/core: Remove 'task' parameter and rename tsk_restore_flags() to current_restore_flags()
      ...

commit e76ca27790a514590af782f83f6eae49e0ccf8c9
Author: Tyrel Datwyler <tyreld@linux.vnet.ibm.com>
Date:   Mon Apr 17 20:24:39 2017 -0400

    powerpc/sysfs: Fix reference leak of cpu device_nodes present at boot
    
    For CPUs present at boot each logical CPU acquires a reference to the
    associated device node of the core. This happens in register_cpu() which
    is called by topology_init(). The result of this is that we end up with
    a reference held by each thread of the core. However, these references
    are never freed if the CPU core is DLPAR removed.
    
    This patch fixes the reference leaks by acquiring and releasing the references
    in the CPU hotplug callbacks un/register_cpu_online(). With this patch symmetric
    reference counting is observed with both CPUs present at boot, and those DLPAR
    added after boot.
    
    Fixes: f86e4718f24b ("driver/core: cpu: initialize of_node in cpu's device struture")
    Cc: stable@vger.kernel.org # v3.12+
    Signed-off-by: Tyrel Datwyler <tyreld@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 73810a069120aa831debb4d967310ab900f628ad
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 13 10:20:23 2017 +0200

    crypto: N2 - Replace racy task affinity logic
    
    spu_queue_register() needs to invoke setup functions on a particular
    CPU. This is achieved by temporarily setting the affinity of the
    calling user space thread to the requested CPU and reset it to the original
    affinity afterwards.
    
    That's racy vs. CPU hotplug and concurrent affinity settings for that
    thread resulting in code executing on the wrong CPU and overwriting the
    new affinity setting.
    
    Replace it by using work_on_cpu_safe() which guarantees to run the code on
    the requested CPU or to fail in case the CPU is offline.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
    Acked-by: "David S. Miller" <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: linux-crypto@vger.kernel.org
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1704131019420.2408@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 12699ac53a2e5fbd1fd7c164b11685d55c8aa28b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 13 10:22:43 2017 +0200

    cpufreq/sparc-us2e: Replace racy task affinity logic
    
    The access to the HBIRD_ESTAR_MODE register in the cpu frequency control
    functions must happen on the target CPU. This is achieved by temporarily
    setting the affinity of the calling user space thread to the requested CPU
    and reset it to the original affinity afterwards.
    
    That's racy vs. CPU hotplug and concurrent affinity settings for that
    thread resulting in code executing on the wrong CPU and overwriting the
    new affinity setting.
    
    Replace it by a straight forward smp function call.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: linux-pm@vger.kernel.org
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1704131020280.2408@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 9fe24c4e92d3963d92d7d383e28ed098bd5689d8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:37 2017 +0200

    cpufreq/sparc-us3: Replace racy task affinity logic
    
    The access to the safari config register in the CPU frequency functions
    must be executed on the target CPU. This is achieved by temporarily setting
    the affinity of the calling user space thread to the requested CPU and
    reset it to the original affinity afterwards.
    
    That's racy vs. CPU hotplug and concurrent affinity settings for that
    thread resulting in code executing on the wrong CPU and overwriting the
    new affinity setting.
    
    Replace it by a straight forward smp function call.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: linux-pm@vger.kernel.org
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201043.047558840@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 205dcc1ecbc566cbc20acf246e68de3b080b3ecf
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:36 2017 +0200

    cpufreq/sh: Replace racy task affinity logic
    
    The target() callback must run on the affected cpu. This is achieved by
    temporarily setting the affinity of the calling thread to the requested CPU
    and reset it to the original affinity afterwards.
    
    That's racy vs. concurrent affinity settings for that thread resulting in
    code executing on the wrong CPU.
    
    Replace it by work_on_cpu(). All call pathes which invoke the callbacks are
    already protected against CPU hotplug.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: linux-pm@vger.kernel.org
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.958216363@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 38f05ed04beb276f780fcd2b5c0b78c76d0b3c0c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:55:03 2017 +0200

    cpufreq/ia64: Replace racy task affinity logic
    
    The get() and target() callbacks must run on the affected cpu. This is
    achieved by temporarily setting the affinity of the calling thread to the
    requested CPU and reset it to the original affinity afterwards.
    
    That's racy vs. concurrent affinity settings for that thread resulting in
    code executing on the wrong CPU and overwriting the new affinity setting.
    
    Replace it by work_on_cpu(). All call pathes which invoke the callbacks are
    already protected against CPU hotplug.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: linux-pm@vger.kernel.org
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1704122231100.2548@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 8153f9ac43897f9f4786b30badc134fcc1a4fb11
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:34 2017 +0200

    ACPI/processor: Replace racy task affinity logic
    
    acpi_processor_get_throttling() requires to invoke the getter function on
    the target CPU. This is achieved by temporarily setting the affinity of the
    calling user space thread to the requested CPU and reset it to the original
    affinity afterwards.
    
    That's racy vs. CPU hotplug and concurrent affinity settings for that
    thread resulting in code executing on the wrong CPU and overwriting the
    new affinity setting.
    
    acpi_processor_get_throttling() is invoked in two ways:
    
    1) The CPU online callback, which is already running on the target CPU and
       obviously protected against hotplug and not affected by affinity
       settings.
    
    2) The ACPI driver probe function, which is not protected against hotplug
       during modprobe.
    
    Switch it over to work_on_cpu() and protect the probe function against CPU
    hotplug.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: linux-acpi@vger.kernel.org
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.785920903@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit ea875ec94eafb858990f3fe9528501f983105653
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 13 10:17:07 2017 +0200

    sparc/sysfs: Replace racy task affinity logic
    
    The mmustat_enable sysfs file accessor functions must run code on the
    target CPU. This is achieved by temporarily setting the affinity of the
    calling user space thread to the requested CPU and reset it to the original
    affinity afterwards.
    
    That's racy vs. concurrent affinity settings for that thread resulting in
    code executing on the wrong CPU and overwriting the new affinity setting.
    
    Replace it by using work_on_cpu() which guarantees to run the code on the
    requested CPU.
    
    Protection against CPU hotplug is not required as the open sysfs file
    already prevents the removal from the CPU offline callback. Using the
    hotplug protected version would actually be wrong because it would deadlock
    against a CPU hotplug operation of the CPU associated to the sysfs file in
    progress.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: David S. Miller <davem@davemloft.net>
    Cc: fenghua.yu@intel.com
    Cc: tony.luck@intel.com
    Cc: herbert@gondor.apana.org.au
    Cc: rjw@rjwysocki.net
    Cc: peterz@infradead.org
    Cc: benh@kernel.crashing.org
    Cc: bigeasy@linutronix.de
    Cc: jiangshanlai@gmail.com
    Cc: sparclinux@vger.kernel.org
    Cc: viresh.kumar@linaro.org
    Cc: mpe@ellerman.id.au
    Cc: tj@kernel.org
    Cc: lenb@kernel.org
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1704131001270.2408@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 6d11b87d55eb75007a3721c2de5938f5bbf607fb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:31 2017 +0200

    powerpc/smp: Replace open coded task affinity logic
    
    Init task invokes smp_ops->setup_cpu() from smp_cpus_done(). Init task can
    run on any online CPU at this point, but the setup_cpu() callback requires
    to be invoked on the boot CPU. This is achieved by temporarily setting the
    affinity of the calling user space thread to the requested CPU and reset it
    to the original affinity afterwards.
    
    That's racy vs. CPU hotplug and concurrent affinity settings for that
    thread resulting in code executing on the wrong CPU and overwriting the
    new affinity setting.
    
    That's actually not a problem in this context as neither CPU hotplug nor
    affinity settings can happen, but the access to task_struct::cpus_allowed
    is about to restricted.
    
    Replace it with a call to work_on_cpu_safe() which achieves the same result.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.518053336@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 9feb42ac88b516e378b9782e82b651ca5bed95c4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 6 14:56:18 2017 +0200

    ia64/sn/hwperf: Replace racy task affinity logic
    
    sn_hwperf_op_cpu() which is invoked from an ioctl requires to run code on
    the requested cpu. This is achieved by temporarily setting the affinity of
    the calling user space thread to the requested CPU and reset it to the
    original affinity afterwards.
    
    That's racy vs. CPU hotplug and concurrent affinity settings for that
    thread resulting in code executing on the wrong CPU and overwriting the
    new affinity setting.
    
    Replace it by using work_on_cpu_safe() which guarantees to run the code on
    the requested CPU or to fail in case the CPU is offline.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: linux-ia64@vger.kernel.org
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1704122251450.2548@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 67cb85fdcee7fbc61c09c00360d1a4ae37641db4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:29 2017 +0200

    ia64/salinfo: Replace racy task affinity logic
    
    Some of the file operations in /proc/sal require to run code on the
    requested cpu. This is achieved by temporarily setting the affinity of the
    calling user space thread to the requested CPU and reset it to the original
    affinity afterwards.
    
    That's racy vs. CPU hotplug and concurrent affinity settings for that
    thread resulting in code executing on the wrong CPU and overwriting the
    new affinity setting.
    
    Replace it by using work_on_cpu_safe() which guarantees to run the code on
    the requested CPU or to fail in case the CPU is offline.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: linux-ia64@vger.kernel.org
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.341863457@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 0e8d6a9336b487a1dd6f1991ff376e669d4c87c6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:28 2017 +0200

    workqueue: Provide work_on_cpu_safe()
    
    work_on_cpu() is not protected against CPU hotplug. For code which requires
    to be either executed on an online CPU or to fail if the CPU is not
    available the callsite would have to protect against CPU hotplug.
    
    Provide a function which does get/put_online_cpus() around the call to
    work_on_cpu() and fails the call with -ENODEV if the target CPU is not
    online.
    
    Preparatory patch to convert several racy task affinity manipulations.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.262610721@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 048c9b954e20396e0c45ee778466994d1be2e612
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 12 22:07:27 2017 +0200

    ia64/topology: Remove cpus_allowed manipulation
    
    The CPU hotplug callback fiddles with the cpus_allowed pointer to pin the
    calling thread on the plugged CPU. That's already guaranteed by the hotplug
    core code.
    
    Remove it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: linux-ia64@vger.kernel.org
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/20170412201042.174518069@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit bac06cf0fb9dfd79877a4eaa2ec7c09a6b853ffa
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Fri Mar 31 11:51:08 2017 +0100

    MIPS: smp-cps: Fix potentially uninitialised value of core
    
    Turning on DEBUG in smp-cps.c, or compiling the kernel with
    CONFIG_DYNAMIC_DEBUG enabled results the build error:
    
    arch/mips/kernel/smp-cps.c: In function 'play_dead':
    ./include/linux/dynamic_debug.h:126:3: error: 'core' may be used
    uninitialized in this function [-Werror=maybe-uninitialized]
    
    Fix this by always initialising the variable.
    
    Fixes: 0d2808f338c7 ("MIPS: smp-cps: Add support for CPU hotplug of MIPSr6 processors")
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/15848/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit bd8fab39cd4fe429f161dfc86bf64993d5c17320
Author: Denis Plotnikov <dplotnikov@virtuozzo.com>
Date:   Fri Apr 7 12:09:53 2017 +0300

    KVM: x86: fix maintaining of kvm_clock stability on guest CPU hotplug
    
    VCPU TSC synchronization is perfromed in kvm_write_tsc() when the TSC
    value being set is within 1 second from the expected, as obtained by
    extrapolating of the TSC in already synchronized VCPUs.
    
    This is naturally achieved on all VCPUs at VM start and resume;
    however on VCPU hotplug it is not: the newly added VCPU is created
    with TSC == 0 while others are well ahead.
    
    To compensate for that, consider host-initiated kvm_write_tsc() with
    TSC == 0 a special case requiring synchronization regardless of the
    current TSC on other VCPUs.
    
    Signed-off-by: Denis Plotnikov <dplotnikov@virtuozzo.com>
    Reviewed-by: Roman Kagan <rkagan@virtuozzo.com>
    Signed-off-by: Radim Krm <rkrcmar@redhat.com>

commit ebe8bddb6e30d7a02775b9972099271fc5910f37
Author: Omar Sandoval <osandov@fb.com>
Date:   Fri Apr 7 08:53:11 2017 -0600

    blk-mq: remap queues when adding/removing hardware queues
    
    blk_mq_update_nr_hw_queues() used to remap hardware queues, which is the
    behavior that drivers expect. However, commit 4e68a011428a changed
    blk_mq_queue_reinit() to not remap queues for the case of CPU
    hotplugging, inadvertently making blk_mq_update_nr_hw_queues() not remap
    queues as well. This breaks, for example, NBD's multi-connection mode,
    leaving the added hardware queues unused. Fix it by making
    blk_mq_update_nr_hw_queues() explicitly remap the queues.
    
    Fixes: 4e68a011428a ("blk-mq: don't redistribute hardware queues on a CPU hotplug event")
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit c09adab01e4aeecfa3dfae0946409844400c5901
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Mar 10 10:46:15 2017 +0000

    drivers/perf: arm_pmu: split irq request from enable
    
    For historical reasons, we lazily request and free interrupts in the
    arm pmu driver. This requires us to refcount use of the pmu (by way of
    counting the active events) in order to request/free interrupts at the
    correct times, which complicates the driver somewhat.
    
    The existing logic is flawed, as it only considers currently online CPUs
    when requesting, freeing, or managing the affinity of interrupts.
    Intervening hotplug events can result in erroneous IRQ affinity, online
    CPUs for which interrupts have not been requested, or offline CPUs whose
    interrupts are still requested.
    
    To fix this, this patch splits the requesting of interrupts from any
    per-cpu management (i.e. per-cpu enable/disable, and configuration of
    cpu affinity). We now request all interrupts up-front at probe time (and
    never free them, since we never unregister PMUs).
    
    The management of affinity, and per-cpu enable/disable now happens in
    our cpu hotplug callback, ensuring it occurs consistently. This means
    that we must now invoke the CPU hotplug callback at boot time in order
    to configure IRQs, and since the callback also resets the PMU hardware,
    we can remove the duplicate reset in the probe path.
    
    This rework renders our event refcounting unnecessary, so this is
    removed.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    [will: make armpmu_get_cpu_irq static]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 64ca752dcbc018054bfea53b784d4c85d3ec896c
Author: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
Date:   Sat Jan 21 19:47:29 2017 +0900

    printk: use console_trylock() in console_cpu_notify()
    
    There is no need to always call blocking console_lock() in
    console_cpu_notify(), it's quite possible that console_sem can
    be locked by other CPU on the system, either already printing
    or soon to begin printing the messages. console_lock() in this
    case can simply block CPU hotplug for unknown period of time
    (console_unlock() is time unbound). Not that hotplug is very
    fast, but still, with other CPUs being online and doing
    printk() console_cpu_notify() can stuck.
    
    Use console_trylock() instead and opt-out if console_sem is
    already acquired from another CPU, since that CPU will do
    the printing for us.
    
    Link: http://lkml.kernel.org/r/20170121104729.8585-1-sergey.senozhatsky@gmail.com
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Signed-off-by: Petr Mladek <pmladek@suse.com>

commit 3e51f893de98296ba64a88d93a94d17e3df5de0b
Merge: 8d940990f51d dc434e056fe1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 18 08:33:44 2017 -0700

    Merge branch 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug fix from Thomas Gleixner:
     "A single fix preventing the concurrent execution of the CPU hotplug
      callback install/invocation machinery. Long standing bug caused by a
      massive brain slip of that Gleixner dude, which went unnoticed for
      almost a year"
    
    * 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Serialize callback invocations proper

commit dc434e056fe1dada20df7ba07f32739d3a701adf
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Mar 14 16:06:45 2017 +0100

    cpu/hotplug: Serialize callback invocations proper
    
    The setup/remove_state/instance() functions in the hotplug core code are
    serialized against concurrent CPU hotplug, but unfortunately not serialized
    against themself.
    
    As a consequence a concurrent invocation of these function results in
    corruption of the callback machinery because two instances try to invoke
    callbacks on remote cpus at the same time. This results in missing callback
    invocations and initiator threads waiting forever on the completion.
    
    The obvious solution to replace get_cpu_online() with cpu_hotplug_begin()
    is not possible because at least one callsite calls into these functions
    from a get_online_cpu() locked region.
    
    Extend the protection scope of the cpuhp_state_mutex from solely protecting
    the state arrays to cover the callback invocation machinery as well.
    
    Fixes: 5b7aa87e0482 ("cpu/hotplug: Implement setup/removal interface")
    Reported-and-tested-by: Bart Van Assche <Bart.VanAssche@sandisk.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: hpa@zytor.com
    Cc: mingo@kernel.org
    Cc: akpm@linux-foundation.org
    Cc: torvalds@linux-foundation.org
    Link: http://lkml.kernel.org/r/20170314150645.g4tdyoszlcbajmna@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit a2cfb460dcba9831b1e27ca0e4de2eab19d7f154
Author: Douglas Miller <dougmill@linux.vnet.ibm.com>
Date:   Sat Jan 28 06:42:20 2017 -0600

    percpu-refcount: fix reference leak during percpu-atomic transition
    
    [ Upstream commit 966d2b04e070bc040319aaebfec09e0144dc3341 ]
    
    percpu_ref_tryget() and percpu_ref_tryget_live() should return
    "true" IFF they acquire a reference. But the return value from
    atomic_long_inc_not_zero() is a long and may have high bits set,
    e.g. PERCPU_COUNT_BIAS, and the return value of the tryget routines
    is bool so the reference may actually be acquired but the routines
    return "false" which results in a reference leak since the caller
    assumes it does not need to do a corresponding percpu_ref_put().
    
    This was seen when performing CPU hotplug during I/O, as hangs in
    blk_mq_freeze_queue_wait where percpu_ref_kill (blk_mq_freeze_queue_start)
    raced with percpu_ref_tryget (blk_mq_timeout_work).
    Sample stack trace:
    
    __switch_to+0x2c0/0x450
    __schedule+0x2f8/0x970
    schedule+0x48/0xc0
    blk_mq_freeze_queue_wait+0x94/0x120
    blk_mq_queue_reinit_work+0xb8/0x180
    blk_mq_queue_reinit_prepare+0x84/0xa0
    cpuhp_invoke_callback+0x17c/0x600
    cpuhp_up_callbacks+0x58/0x150
    _cpu_up+0xf0/0x1c0
    do_cpu_up+0x120/0x150
    cpu_subsys_online+0x64/0xe0
    device_online+0xb4/0x120
    online_store+0xb4/0xc0
    dev_attr_store+0x68/0xa0
    sysfs_kf_write+0x80/0xb0
    kernfs_fop_write+0x17c/0x250
    __vfs_write+0x6c/0x1e0
    vfs_write+0xd0/0x270
    SyS_write+0x6c/0x110
    system_call+0x38/0xe0
    
    Examination of the queue showed a single reference (no PERCPU_COUNT_BIAS,
    and __PERCPU_REF_DEAD, __PERCPU_REF_ATOMIC set) and no requests.
    However, conditions at the time of the race are count of PERCPU_COUNT_BIAS + 0
    and __PERCPU_REF_DEAD and __PERCPU_REF_ATOMIC set.
    
    The fix is to make the tryget routines use an actual boolean internally instead
    of the atomic long result truncated to a int.
    
    Fixes: e625305b3907 percpu-refcount: make percpu_ref based on longs instead of ints
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=190751
    Signed-off-by: Douglas Miller <dougmill@linux.vnet.ibm.com>
    Reviewed-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Fixes: e625305b3907 ("percpu-refcount: make percpu_ref based on longs instead of ints")
    Cc: stable@vger.kernel.org # v3.18+
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>

commit 0ca0156973a47e689f3bc817e26e15fff3f84eec
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 14:52:01 2017 +0100

    sched/headers: Split hotplug CPU interfaces out of <linux/sched.h> into <linux/sched/hotplug.h>
    
    Split the CPU hotplug scheduler APIs out of the common header.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 252b95c0edead46fb188042584d3dcd6d6ede062
Merge: b8989bccd6a0 4610d240d691
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 21 13:53:41 2017 -0800

    Merge tag 'for-linus-4.11-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from Juergen Gross:
     "Xen features and fixes:
    
       - a series from Boris Ostrovsky adding support for booting Linux as
         Xen PVH guest
    
       - a series from Juergen Gross streamlining the xenbus driver
    
       - a series from Paul Durrant adding support for the new device model
         hypercall
    
       - several small corrections"
    
    * tag 'for-linus-4.11-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/privcmd: add IOCTL_PRIVCMD_RESTRICT
      xen/privcmd: Add IOCTL_PRIVCMD_DM_OP
      xen/privcmd: return -ENOTTY for unimplemented IOCTLs
      xen: optimize xenbus driver for multiple concurrent xenstore accesses
      xen: modify xenstore watch event interface
      xen: clean up xenbus internal headers
      xenbus: Neaten xenbus_va_dev_error
      xen/pvh: Use Xen's emergency_restart op for PVH guests
      xen/pvh: Enable CPU hotplug
      xen/pvh: PVH guests always have PV devices
      xen/pvh: Initialize grant table for PVH guests
      xen/pvh: Make sure we don't use ACPI_IRQ_MODEL_PIC for SCI
      xen/pvh: Bootstrap PVH guest
      xen/pvh: Import PVH-related Xen public interfaces
      xen/x86: Remove PVH support
      x86/boot/32: Convert the 32-bit pgtable setup code from assembly to C
      xen/manage: correct return value check on xenbus_scanf()
      x86/xen: Fix APIC id mismatch warning on Intel
      xen/netback: set default upper limit of tx/rx queues to 8
      xen/netfront: set default upper limit of tx/rx queues to 8

commit 43e31e40473a00c936ffb9c2eebedc0566c92e89
Merge: 02c3de110522 ac18c0c4e2b1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 17:55:15 2017 -0800

    Merge tag 'acpi-4.11-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull ACPI updates from Rafael Wysocki:
     "These update the ACPICA code in the kernel to upstream revision
      20170119, which among other things updates copyright notices in all of
      the ACPICA files, fix a couple of issues in the ACPI EC and button
      drivers, fix modalias handling for non-discoverable devices with
      DT-compatible identification strings, add a suspend quirk for one
      platform and fix a message in the APEI code.
    
      Specifics:
    
       - Update of the ACPICA code in the kernel to upstream revision
         20170119 including:
    
          + Fixes related to the handling of the bit width and bit offset
            fields in Generic Address Structure (Lv Zheng)
          + ACPI resources handling fix related to invalid resource
            descriptors (Bob Moore)
          + Fix to enable implicit result conversion for several ASL library
            functions (Bob Moore)
          + Support for method invocations as target operands in AML (Bob
            Moore)
          + Fix to use a correct operand type for DeRefOf() in some
            situations (Bob Moore)
          + Utilities updates (Bob Moore, Lv Zheng)
          + Disassembler/debugger updates (David Box, Lv Zheng)
          + Build fixes (Colin Ian King, Lv Zheng)
          + Update of copyright notices in all files (Bob Moore)
    
       - Fix for modalias handling for SPI and I2C devices with
         DT-compatible identification strings (Dan O'Donovan)
    
       - Fixes for the ACPI EC and button drivers (Lv Zheng)
    
       - ACPI processor handling fix related to CPU hotplug (online/offline)
         on x86 (Vitaly Kuznetsov)
    
       - Suspend quirk to save/restore NVS memory over S3 transitions for
         Lenovo G50-45 (Zhang Rui)
    
       - Message formatting fix for the ACPI APEI code (Colin Ian King)"
    
    * tag 'acpi-4.11-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (32 commits)
      ACPICA: Update version to 20170119
      ACPICA: Tools: Update common signon, remove compilation bit width
      ACPICA: Source tree: Update copyright notices to 2017
      ACPICA: Linuxize: Restore and fix Intel compiler build
      x86/ACPI: keep x86_cpu_to_acpiid mapping valid on CPU hotplug
      spi: acpi: Initialize modalias from of_compatible
      i2c: acpi: Initialize info.type from of_compatible
      ACPI / bus: Introduce acpi_of_modalias() equiv of of_modalias_node()
      ACPI: save NVS memory for Lenovo G50-45
      ACPI, APEI, EINJ: fix malformed newline escape
      ACPI / button: Remove lid_init_state=method mode
      ACPI / button: Change default behavior to lid_init_state=open
      ACPI / EC: Use busy polling mode when GPE is not enabled
      ACPI / EC: Remove old CLEAR_ON_RESUME quirk
      ACPICA: Update version to 20161222
      ACPICA: Parser: Update parse info table for some operators
      ACPICA: Fix a problem with recent extra support for control method invocations
      ACPICA: Parser: Allow method invocations as target operands
      ACPICA: Fix for implicit result conversion for the ToXXX functions
      ACPICA: Resources: Not a valid resource if buffer length too long
      ..

commit a74d1cafc22e100a9b59c50943ca09c37e03dce8
Merge: 014f40393ebc 0c6543f6cda4 cbc00c1310d3 febf2407418a
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Feb 20 14:28:03 2017 +0100

    Merge branches 'acpi-bus', 'acpi-sleep' and 'acpi-processor'
    
    * acpi-bus:
      spi: acpi: Initialize modalias from of_compatible
      i2c: acpi: Initialize info.type from of_compatible
      ACPI / bus: Introduce acpi_of_modalias() equiv of of_modalias_node()
    
    * acpi-sleep:
      ACPI: save NVS memory for Lenovo G50-45
    
    * acpi-processor:
      x86/ACPI: keep x86_cpu_to_acpiid mapping valid on CPU hotplug

commit 558e8e27e73f53f8a512485be538b07115fe5f3c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 16 12:19:18 2017 -0800

    Revert "nohz: Fix collision between tick and other hrtimers"
    
    This reverts commit 24b91e360ef521a2808771633d76ebc68bd5604b and commit
    7bdb59f1ad47 ("tick/nohz: Fix possible missing clock reprog after tick
    soft restart") that depends on it,
    
    Pavel reports that it causes occasional boot hangs for him that seem to
    depend on just how the machine was booted.  In particular, his machine
    hangs at around the PCI fixups of the EHCI USB host controller, but only
    hangs from cold boot, not from a warm boot.
    
    Thomas Gleixner suspecs it's a CPU hotplug interaction, particularly
    since Pavel also saw suspend/resume issues that seem to be related.
    We're reverting for now while trying to figure out the root cause.
    
    Reported-bisected-and-tested-by: Pavel Machek <pavel@ucw.cz>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@kernel.org  # reverted commits were marked for stable
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit e7f9f10bcc8dbbf0e09aba6765e9e07bc59910f1
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Feb 7 11:35:31 2017 +1100

    powerpc/powernv: Fix CPU hotplug to handle waking on HVI
    
    commit 9b256714979fad61ae11d90b53cf67dd5e6484eb upstream.
    
    The IPIs come in as HVI not EE, so we need to test the appropriate
    SRR1 bits. The encoding is such that it won't have false positives
    on P7 and P8 so we can just test it like that. We also need to handle
    the icp-opal variant of the flush.
    
    Fixes: d74361881f0d ("powerpc/xics: Add ICP OPAL backend")
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3ebc7033168d43d12e4941f48a6f257d3f1ea1b5
Merge: 3d88460dbd28 f83e6862047e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 10 14:10:35 2017 -0800

    Merge tag 'powerpc-4.10-4' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc fixes friom Michael Ellerman:
     "Apologies for the late pull request, but Ben has been busy finding bugs.
    
       - Userspace was semi-randomly segfaulting on radix due to us
         incorrectly handling a fault triggered by autonuma, caused by a
         patch we merged earlier in v4.10 to prevent the kernel executing
         userspace.
    
       - We weren't marking host IPIs properly for KVM in the OPAL ICP
         backend.
    
       - The ERAT flushing on radix was missing an isync and was incorrectly
         marked as DD1 only.
    
       - The powernv CPU hotplug code was missing a wakeup type and failing
         to flush the interrupt correctly when using OPAL ICP
    
      Thanks to Benjamin Herrenschmidt"
    
    * tag 'powerpc-4.10-4' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux:
      powerpc/powernv: Properly set "host-ipi" on IPIs
      powerpc/powernv: Fix CPU hotplug to handle waking on HVI
      powerpc/mm/radix: Update ERAT flushes when invalidating TLB
      powerpc/mm: Fix spurrious segfaults on radix with autonuma

commit 12f822d23deee45421bf65dc9f5ff0fdcc783701
Author: Douglas Miller <dougmill@linux.vnet.ibm.com>
Date:   Sat Jan 28 06:42:20 2017 -0600

    percpu-refcount: fix reference leak during percpu-atomic transition
    
    commit 966d2b04e070bc040319aaebfec09e0144dc3341 upstream.
    
    percpu_ref_tryget() and percpu_ref_tryget_live() should return
    "true" IFF they acquire a reference. But the return value from
    atomic_long_inc_not_zero() is a long and may have high bits set,
    e.g. PERCPU_COUNT_BIAS, and the return value of the tryget routines
    is bool so the reference may actually be acquired but the routines
    return "false" which results in a reference leak since the caller
    assumes it does not need to do a corresponding percpu_ref_put().
    
    This was seen when performing CPU hotplug during I/O, as hangs in
    blk_mq_freeze_queue_wait where percpu_ref_kill (blk_mq_freeze_queue_start)
    raced with percpu_ref_tryget (blk_mq_timeout_work).
    Sample stack trace:
    
    __switch_to+0x2c0/0x450
    __schedule+0x2f8/0x970
    schedule+0x48/0xc0
    blk_mq_freeze_queue_wait+0x94/0x120
    blk_mq_queue_reinit_work+0xb8/0x180
    blk_mq_queue_reinit_prepare+0x84/0xa0
    cpuhp_invoke_callback+0x17c/0x600
    cpuhp_up_callbacks+0x58/0x150
    _cpu_up+0xf0/0x1c0
    do_cpu_up+0x120/0x150
    cpu_subsys_online+0x64/0xe0
    device_online+0xb4/0x120
    online_store+0xb4/0xc0
    dev_attr_store+0x68/0xa0
    sysfs_kf_write+0x80/0xb0
    kernfs_fop_write+0x17c/0x250
    __vfs_write+0x6c/0x1e0
    vfs_write+0xd0/0x270
    SyS_write+0x6c/0x110
    system_call+0x38/0xe0
    
    Examination of the queue showed a single reference (no PERCPU_COUNT_BIAS,
    and __PERCPU_REF_DEAD, __PERCPU_REF_ATOMIC set) and no requests.
    However, conditions at the time of the race are count of PERCPU_COUNT_BIAS + 0
    and __PERCPU_REF_DEAD and __PERCPU_REF_ATOMIC set.
    
    The fix is to make the tryget routines use an actual boolean internally instead
    of the atomic long result truncated to a int.
    
    Fixes: e625305b3907 percpu-refcount: make percpu_ref based on longs instead of ints
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=190751
    Signed-off-by: Douglas Miller <dougmill@linux.vnet.ibm.com>
    Reviewed-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Fixes: e625305b3907 ("percpu-refcount: make percpu_ref based on longs instead of ints")
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b3c8c31ed24c39f9a9fb9bee1e42b16640ff0bee
Author: Douglas Miller <dougmill@linux.vnet.ibm.com>
Date:   Sat Jan 28 06:42:20 2017 -0600

    percpu-refcount: fix reference leak during percpu-atomic transition
    
    commit 966d2b04e070bc040319aaebfec09e0144dc3341 upstream.
    
    percpu_ref_tryget() and percpu_ref_tryget_live() should return
    "true" IFF they acquire a reference. But the return value from
    atomic_long_inc_not_zero() is a long and may have high bits set,
    e.g. PERCPU_COUNT_BIAS, and the return value of the tryget routines
    is bool so the reference may actually be acquired but the routines
    return "false" which results in a reference leak since the caller
    assumes it does not need to do a corresponding percpu_ref_put().
    
    This was seen when performing CPU hotplug during I/O, as hangs in
    blk_mq_freeze_queue_wait where percpu_ref_kill (blk_mq_freeze_queue_start)
    raced with percpu_ref_tryget (blk_mq_timeout_work).
    Sample stack trace:
    
    __switch_to+0x2c0/0x450
    __schedule+0x2f8/0x970
    schedule+0x48/0xc0
    blk_mq_freeze_queue_wait+0x94/0x120
    blk_mq_queue_reinit_work+0xb8/0x180
    blk_mq_queue_reinit_prepare+0x84/0xa0
    cpuhp_invoke_callback+0x17c/0x600
    cpuhp_up_callbacks+0x58/0x150
    _cpu_up+0xf0/0x1c0
    do_cpu_up+0x120/0x150
    cpu_subsys_online+0x64/0xe0
    device_online+0xb4/0x120
    online_store+0xb4/0xc0
    dev_attr_store+0x68/0xa0
    sysfs_kf_write+0x80/0xb0
    kernfs_fop_write+0x17c/0x250
    __vfs_write+0x6c/0x1e0
    vfs_write+0xd0/0x270
    SyS_write+0x6c/0x110
    system_call+0x38/0xe0
    
    Examination of the queue showed a single reference (no PERCPU_COUNT_BIAS,
    and __PERCPU_REF_DEAD, __PERCPU_REF_ATOMIC set) and no requests.
    However, conditions at the time of the race are count of PERCPU_COUNT_BIAS + 0
    and __PERCPU_REF_DEAD and __PERCPU_REF_ATOMIC set.
    
    The fix is to make the tryget routines use an actual boolean internally instead
    of the atomic long result truncated to a int.
    
    Fixes: e625305b3907 percpu-refcount: make percpu_ref based on longs instead of ints
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=190751
    Signed-off-by: Douglas Miller <dougmill@linux.vnet.ibm.com>
    Reviewed-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Fixes: e625305b3907 ("percpu-refcount: make percpu_ref based on longs instead of ints")
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9b256714979fad61ae11d90b53cf67dd5e6484eb
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Feb 7 11:35:31 2017 +1100

    powerpc/powernv: Fix CPU hotplug to handle waking on HVI
    
    The IPIs come in as HVI not EE, so we need to test the appropriate
    SRR1 bits. The encoding is such that it won't have false positives
    on P7 and P8 so we can just test it like that. We also need to handle
    the icp-opal variant of the flush.
    
    Fixes: d74361881f0d ("powerpc/xics: Add ICP OPAL backend")
    Cc: stable@vger.kernel.org # v4.8+
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 2a7197f02dddf1f9cee300bd12512375ed56524a
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Mon Feb 6 10:58:05 2017 -0500

    xen/pvh: Enable CPU hotplug
    
    PVH guests don't (yet) receive ACPI hotplug interrupts and therefore
    need to monitor xenstore for CPU hotplug event.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>

commit febf2407418a2d6c042fcd77b206040449cb9a70
Author: Vitaly Kuznetsov <vkuznets@redhat.com>
Date:   Mon Feb 6 18:01:51 2017 +0100

    x86/ACPI: keep x86_cpu_to_acpiid mapping valid on CPU hotplug
    
    We may or may not have all possible CPUs in MADT on boot but in any
    case we're overwriting x86_cpu_to_acpiid mapping with U32_MAX when
    acpi_register_lapic() is called again on the CPU hotplug path:
    
    acpi_processor_hotadd_init()
      -> acpi_map_cpu()
        -> acpi_register_lapic()
    
    As we have the required acpi_id information in acpi_processor_hotadd_init()
    propagate it to acpi_map_cpu() to always keep x86_cpu_to_acpiid
    mapping valid.
    
    Reported-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 891aa1e0f13c3aaa756c69b343d6ab6f3357009b
Merge: c67b42f3a3f0 fff4b87e594a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 2 13:30:19 2017 -0800

    Merge branch 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf fixes from Ingo Molnar:
     "Five kernel fixes:
    
       - an mmap tracing ABI fix for certain mappings
    
       - a use-after-free fix, found via KASAN
    
       - three CPU hotplug related x86 PMU driver fixes"
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf/x86/intel/uncore: Make package handling more robust
      perf/x86/intel/uncore: Clean up hotplug conversion fallout
      perf/x86/intel/rapl: Make package handling more robust
      perf/core: Fix PERF_RECORD_MMAP2 prot/flags for anonymous memory
      perf/core: Fix use-after-free bug

commit 0becc0ae5b42828785b589f686725ff5bc3b9b25
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jan 31 09:37:34 2017 +0100

    x86/mce: Make timer handling more robust
    
    Erik reported that on a preproduction hardware a CMCI storm triggers the
    BUG_ON in add_timer_on(). The reason is that the per CPU MCE timer is
    started by the CMCI logic before the MCE CPU hotplug callback starts the
    timer with add_timer_on(). So the timer is already queued which triggers
    the BUG.
    
    Using add_timer_on() is pretty pointless in this code because the timer is
    strictlty per CPU, initialized as pinned and all operations which arm the
    timer happen on the CPU to which the timer belongs.
    
    Simplify the whole machinery by using mod_timer() instead of add_timer_on()
    which avoids the problem because mod_timer() can handle already queued
    timers. Use __start_timer() everywhere so the earliest armed expiry time is
    preserved.
    
    Reported-by: Erik Veijola <erik.veijola@intel.com>
    Tested-by: Borislav Petkov <bp@alien8.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1701310936080.3457@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 966d2b04e070bc040319aaebfec09e0144dc3341
Author: Douglas Miller <dougmill@linux.vnet.ibm.com>
Date:   Sat Jan 28 06:42:20 2017 -0600

    percpu-refcount: fix reference leak during percpu-atomic transition
    
    percpu_ref_tryget() and percpu_ref_tryget_live() should return
    "true" IFF they acquire a reference. But the return value from
    atomic_long_inc_not_zero() is a long and may have high bits set,
    e.g. PERCPU_COUNT_BIAS, and the return value of the tryget routines
    is bool so the reference may actually be acquired but the routines
    return "false" which results in a reference leak since the caller
    assumes it does not need to do a corresponding percpu_ref_put().
    
    This was seen when performing CPU hotplug during I/O, as hangs in
    blk_mq_freeze_queue_wait where percpu_ref_kill (blk_mq_freeze_queue_start)
    raced with percpu_ref_tryget (blk_mq_timeout_work).
    Sample stack trace:
    
    __switch_to+0x2c0/0x450
    __schedule+0x2f8/0x970
    schedule+0x48/0xc0
    blk_mq_freeze_queue_wait+0x94/0x120
    blk_mq_queue_reinit_work+0xb8/0x180
    blk_mq_queue_reinit_prepare+0x84/0xa0
    cpuhp_invoke_callback+0x17c/0x600
    cpuhp_up_callbacks+0x58/0x150
    _cpu_up+0xf0/0x1c0
    do_cpu_up+0x120/0x150
    cpu_subsys_online+0x64/0xe0
    device_online+0xb4/0x120
    online_store+0xb4/0xc0
    dev_attr_store+0x68/0xa0
    sysfs_kf_write+0x80/0xb0
    kernfs_fop_write+0x17c/0x250
    __vfs_write+0x6c/0x1e0
    vfs_write+0xd0/0x270
    SyS_write+0x6c/0x110
    system_call+0x38/0xe0
    
    Examination of the queue showed a single reference (no PERCPU_COUNT_BIAS,
    and __PERCPU_REF_DEAD, __PERCPU_REF_ATOMIC set) and no requests.
    However, conditions at the time of the race are count of PERCPU_COUNT_BIAS + 0
    and __PERCPU_REF_DEAD and __PERCPU_REF_ATOMIC set.
    
    The fix is to make the tryget routines use an actual boolean internally instead
    of the atomic long result truncated to a int.
    
    Fixes: e625305b3907 percpu-refcount: make percpu_ref based on longs instead of ints
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=190751
    Signed-off-by: Douglas Miller <dougmill@linux.vnet.ibm.com>
    Reviewed-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Fixes: e625305b3907 ("percpu-refcount: make percpu_ref based on longs instead of ints")
    Cc: stable@vger.kernel.org # v3.18+

commit 81aaeaac461071c591cbd188748ad875e0efae7e
Merge: 2ed5e5af2f9d 4d191b1b63c2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 19 09:59:46 2017 -0800

    Merge tag 'pci-v4.10-fixes-1' of git://git.kernel.org/pub/scm/linux/kernel/git/helgaas/pci
    
    Pull PCI fixes from Bjorn Helgaas:
    
     - recognize that a PCI-to-PCIe bridge originates a PCIe hierarchy, so
       we enumerate that hierarchy correctly
    
     - X-Gene: fix a change merged for v4.10 that broke MSI
    
     - Keystone: avoid reading undefined registers, which can cause
       asynchronous external aborts
    
     - Supermicro X8DTH-i/6/iF/6F: ignore broken _CRS that caused us to
       change (and break) existing I/O port assignments
    
    * tag 'pci-v4.10-fixes-1' of git://git.kernel.org/pub/scm/linux/kernel/git/helgaas/pci:
      PCI/MSI: pci-xgene-msi: Fix CPU hotplug registration handling
      PCI: Enumerate switches below PCI-to-PCIe bridges
      x86/PCI: Ignore _CRS on Supermicro X8DTH-i/6/iF/6F
      PCI: designware: Check for iATU unroll only on platforms that use ATU

commit 9da96f99f15169b8bf77a1f27ed6d926f82ea59f
Merge: 0aa0313f9d57 31f5260a7653
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 18 10:45:22 2017 -0800

    Merge branch 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf fixes from Ingo Molnar:
     "An Intel PMU driver hotplug fix and three 'perf probe' tooling fixes"
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf/x86/intel: Handle exclusive threadid correctly on CPU hotplug
      perf probe: Fix to probe on gcc generated functions in modules
      perf probe: Add error checks to offline probe post-processing
      perf probe: Fix to show correct locations for events on modules

commit 4d191b1b63c209e37bf27938ef365244d3c41084
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Jan 17 14:21:56 2017 +0000

    PCI/MSI: pci-xgene-msi: Fix CPU hotplug registration handling
    
    The conversion to the new hotplug state machine introduced a regression
    where a successful hotplug registration would be treated as an error,
    effectively disabling the MSI driver forever.
    
    Fix it by doing the proper check on the return value.
    
    Fixes: 9c248f8896e6 ("PCI/xgene-msi: Convert to hotplug state machine")
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
    Acked-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Tested-by: Duc Dang <dhdang@apm.com>
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: stable@vger.kernel.org

commit 4e71de7986386d5fd3765458f27d612931f27f5e
Author: Zhou Chengming <zhouchengming1@huawei.com>
Date:   Mon Jan 16 11:21:11 2017 +0800

    perf/x86/intel: Handle exclusive threadid correctly on CPU hotplug
    
    The CPU hotplug function intel_pmu_cpu_starting() sets
    cpu_hw_events.excl_thread_id unconditionally to 1 when the shared exclusive
    counters data structure is already availabe for the sibling thread.
    
    This works during the boot process because the first sibling gets threadid
    0 assigned and the second sibling which shares the data structure gets 1.
    
    But when the first thread of the core is offlined and onlined again it
    shares the data structure with the second thread and gets exclusive thread
    id 1 assigned as well.
    
    Prevent this by checking the threadid of the already online thread.
    
    [ tglx: Rewrote changelog ]
    
    Signed-off-by: Zhou Chengming <zhouchengming1@huawei.com>
    Cc: NuoHan Qiao <qiaonuohan@huawei.com>
    Cc: ak@linux.intel.com
    Cc: peterz@infradead.org
    Cc: kan.liang@intel.com
    Cc: dave.hansen@linux.intel.com
    Cc: eranian@google.com
    Cc: qiaonuohan@huawei.com
    Cc: davidcc@google.com
    Cc: guohanjun@huawei.com
    Link: http://lkml.kernel.org/r/1484536871-3131-1-git-send-email-zhouchengming1@huawei.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    ---                                     ---
     arch/x86/events/intel/core.c |    7 +++++--
     1 file changed, 5 insertions(+), 2 deletions(-)

commit ff58fa7f556c1d87061e4a91ed875d5f8aa9571f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Dec 22 17:19:34 2016 +0100

    Documentation: Update CPU hotplug and move it to core-api
    
    The current CPU hotplug is outdated. During the update to what we
    currently have I rewrote it partly and moved to sphinx format.
    
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Mauro Carvalho Chehab <mchehab@kernel.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Ashok Raj <ashok.raj@intel.com>
    Cc: Joel Schopp <jschopp@austin.ibm.com>
    Cc: linux-doc@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>

commit 530e9b76ae8f863dfdef4a6ad0b38613d32e8c3f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 21 20:19:53 2016 +0100

    cpu/hotplug: Remove obsolete cpu hotplug register/unregister functions
    
    hotcpu_notifier(), cpu_notifier(), __hotcpu_notifier(), __cpu_notifier(),
    register_hotcpu_notifier(), register_cpu_notifier(),
    __register_hotcpu_notifier(), __register_cpu_notifier(),
    unregister_hotcpu_notifier(), unregister_cpu_notifier(),
    __unregister_hotcpu_notifier(), __unregister_cpu_notifier()
    
    are unused now. Remove them and all related code.
    
    Remove also the now pointless cpu notifier error injection mechanism. The
    states can be executed step by step and error rollback is the same as cpu
    down, so any state transition can be tested w/o requiring the notifier
    error injection.
    
    Some CPU hotplug states are kept as they are (ab)used for hotplug state
    tracking.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20161221192112.005642358@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit a98d1a0ca6d3fd6197f18749972d4cc21195b724
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Dec 24 12:34:02 2016 +0100

    scsi: qedi: Convert to hotplug state machine
    
    The CPU hotplug code is a trainwreck. It leaks a notifier in case of driver
    registration error and the per cpu loop is racy against cpu hotplug. Aside
    of that the driver should have been written and merged with the new state
    machine interfaces in the first place.
    
    Mop up the mess and Convert it to the hotplug state machine.
    
    Signed-off-by: Thomas Grumpy Gleixner <tglx@linutronix.de>
    Cc: Nilesh Javali <nilesh.javali@cavium.com>
    Cc: Adheer Chandravanshi <adheer.chandravanshi@qlogic.com>
    Cc: Chad Dupuis <chad.dupuis@cavium.com>
    Cc: Saurav Kashyap <saurav.kashyap@cavium.com>
    Cc: Arun Easi <arun.easi@cavium.com>
    Cc: Manish Rangankar <manish.rangankar@cavium.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: Hannes Reinecke <hare@suse.de>
    Cc: Martin K. Petersen <martin.petersen@oracle.com>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>

commit e1df4c5d40ceb63924167c87421b81fd3818a201
Author: Joonwoo Park <joonwoop@codeaurora.org>
Date:   Sun Sep 11 21:14:58 2016 -0700

    cpuset: handle race between CPU hotplug and cpuset_hotplug_work
    
    [ Upstream commit 28b89b9e6f7b6c8fef7b3af39828722bca20cfee ]
    
    A discrepancy between cpu_online_mask and cpuset's effective_cpus
    mask is inevitable during hotplug since cpuset defers updating of
    effective_cpus mask using a workqueue, during which time nothing
    prevents the system from more hotplug operations.  For that reason
    guarantee_online_cpus() walks up the cpuset hierarchy until it finds
    an intersection under the assumption that top cpuset's effective_cpus
    mask intersects with cpu_online_mask even with such a race occurring.
    
    However a sequence of CPU hotplugs can open a time window, during which
    none of the effective CPUs in the top cpuset intersect with
    cpu_online_mask.
    
    For example when there are 4 possible CPUs 0-3 and only CPU0 is online:
    
      ========================  ===========================
       cpu_online_mask           top_cpuset.effective_cpus
      ========================  ===========================
       echo 1 > cpu2/online.
       CPU hotplug notifier woke up hotplug work but not yet scheduled.
          [0,2]                     [0]
    
       echo 0 > cpu0/online.
       The workqueue is still runnable.
          [2]                       [0]
      ========================  ===========================
    
      Now there is no intersection between cpu_online_mask and
      top_cpuset.effective_cpus.  Thus invoking sys_sched_setaffinity() at
      this moment can cause following:
    
       Unable to handle kernel NULL pointer dereference at virtual address 000000d0
       ------------[ cut here ]------------
       Kernel BUG at ffffffc0001389b0 [verbose debug info unavailable]
       Internal error: Oops - BUG: 96000005 [#1] PREEMPT SMP
       Modules linked in:
       CPU: 2 PID: 1420 Comm: taskset Tainted: G        W       4.4.8+ #98
       task: ffffffc06a5c4880 ti: ffffffc06e124000 task.ti: ffffffc06e124000
       PC is at guarantee_online_cpus+0x2c/0x58
       LR is at cpuset_cpus_allowed+0x4c/0x6c
       <snip>
       Process taskset (pid: 1420, stack limit = 0xffffffc06e124020)
       Call trace:
       [<ffffffc0001389b0>] guarantee_online_cpus+0x2c/0x58
       [<ffffffc00013b208>] cpuset_cpus_allowed+0x4c/0x6c
       [<ffffffc0000d61f0>] sched_setaffinity+0xc0/0x1ac
       [<ffffffc0000d6374>] SyS_sched_setaffinity+0x98/0xac
       [<ffffffc000085cb0>] el0_svc_naked+0x24/0x28
    
    The top cpuset's effective_cpus are guaranteed to be identical to
    cpu_online_mask eventually.  Hence fall back to cpu_online_mask when
    there is no intersection between top cpuset's effective_cpus and
    cpu_online_mask.
    
    Signed-off-by: Joonwoo Park <joonwoop@codeaurora.org>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: cgroups@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: <stable@vger.kernel.org> # 3.17+
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>

commit 85ba70b6ceff7a2880f29b94e789cee436bc572f
Merge: 8d86cf8879e3 7b99f1aeed37
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 22 10:15:05 2016 -0800

    Merge tag 'pm-fixes-4.10-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management fixes from Rafael Wysocki:
     "They fix one bug introduced recently, a build warning and a kerneldoc
      function description.
    
      Specifics:
    
       - Prevent the acpi-cpufreq driver from crashing on exit by fixing a
         check against the __cpuhp_setup_state() return value and fix the
         kerneldoc description of that function to make it clear that it may
         return positive numbers on success too (Boris Ostrovsky)
    
       - Drop an incorrect __init annotation of a function in the s3c64xx
         cpufreq driver and fix a build warning generated (by older
         compilers) because of it (Arnd Bergmann)"
    
    * tag 'pm-fixes-4.10-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm:
      cpufreq: s3c64xx: remove incorrect __init annotation
      cpufreq: Remove CPU hotplug callbacks only if they were initialized
      CPU/hotplug: Clarify description of __cpuhp_setup_state() return value

commit 7b99f1aeed37196ad54099c30c2f154a7d6e91e0
Merge: 7ae123edd37a adec57c61c24
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Dec 22 14:34:55 2016 +0100

    Merge branch 'pm-cpufreq'
    
    * pm-cpufreq:
      cpufreq: s3c64xx: remove incorrect __init annotation
      cpufreq: Remove CPU hotplug callbacks only if they were initialized
      CPU/hotplug: Clarify description of __cpuhp_setup_state() return value

commit 791a928972743620602700e2e45341b5d1753a5e
Author: Joonwoo Park <joonwoop@codeaurora.org>
Date:   Sun Sep 11 21:14:58 2016 -0700

    cpuset: handle race between CPU hotplug and cpuset_hotplug_work
    
    [ Upstream commit 28b89b9e6f7b6c8fef7b3af39828722bca20cfee ]
    
    A discrepancy between cpu_online_mask and cpuset's effective_cpus
    mask is inevitable during hotplug since cpuset defers updating of
    effective_cpus mask using a workqueue, during which time nothing
    prevents the system from more hotplug operations.  For that reason
    guarantee_online_cpus() walks up the cpuset hierarchy until it finds
    an intersection under the assumption that top cpuset's effective_cpus
    mask intersects with cpu_online_mask even with such a race occurring.
    
    However a sequence of CPU hotplugs can open a time window, during which
    none of the effective CPUs in the top cpuset intersect with
    cpu_online_mask.
    
    For example when there are 4 possible CPUs 0-3 and only CPU0 is online:
    
      ========================  ===========================
       cpu_online_mask           top_cpuset.effective_cpus
      ========================  ===========================
       echo 1 > cpu2/online.
       CPU hotplug notifier woke up hotplug work but not yet scheduled.
          [0,2]                     [0]
    
       echo 0 > cpu0/online.
       The workqueue is still runnable.
          [2]                       [0]
      ========================  ===========================
    
      Now there is no intersection between cpu_online_mask and
      top_cpuset.effective_cpus.  Thus invoking sys_sched_setaffinity() at
      this moment can cause following:
    
       Unable to handle kernel NULL pointer dereference at virtual address 000000d0
       ------------[ cut here ]------------
       Kernel BUG at ffffffc0001389b0 [verbose debug info unavailable]
       Internal error: Oops - BUG: 96000005 [#1] PREEMPT SMP
       Modules linked in:
       CPU: 2 PID: 1420 Comm: taskset Tainted: G        W       4.4.8+ #98
       task: ffffffc06a5c4880 ti: ffffffc06e124000 task.ti: ffffffc06e124000
       PC is at guarantee_online_cpus+0x2c/0x58
       LR is at cpuset_cpus_allowed+0x4c/0x6c
       <snip>
       Process taskset (pid: 1420, stack limit = 0xffffffc06e124020)
       Call trace:
       [<ffffffc0001389b0>] guarantee_online_cpus+0x2c/0x58
       [<ffffffc00013b208>] cpuset_cpus_allowed+0x4c/0x6c
       [<ffffffc0000d61f0>] sched_setaffinity+0xc0/0x1ac
       [<ffffffc0000d6374>] SyS_sched_setaffinity+0x98/0xac
       [<ffffffc000085cb0>] el0_svc_naked+0x24/0x28
    
    The top cpuset's effective_cpus are guaranteed to be identical to
    cpu_online_mask eventually.  Hence fall back to cpu_online_mask when
    there is no intersection between top cpuset's effective_cpus and
    cpu_online_mask.
    
    Signed-off-by: Joonwoo Park <joonwoop@codeaurora.org>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: cgroups@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: <stable@vger.kernel.org> # 3.17+
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>

commit 2a8fa123d9a1d2ca391eefc81fea747108a5081f
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Thu Dec 15 10:00:58 2016 -0500

    cpufreq: Remove CPU hotplug callbacks only if they were initialized
    
    Since CPU hotplug callbacks are requested for CPUHP_AP_ONLINE_DYN state,
    successful callback initialization will result in cpuhp_setup_state()
    returning a positive value. Therefore acpi_cpufreq_online being zero
    indicates that callbacks have not been installed.
    
    This means that acpi_cpufreq_boost_exit() should only remove them if
    acpi_cpufreq_online is positive. Trying to call
    cpuhp_remove_state_nocalls(0) will cause a BUG().
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 8c9b9d87b855226a823b41a77a05f42324497603
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Dec 18 15:09:29 2016 +0100

    x86/tsc: Limit the adjust value further
    
    Adjust value 0x80000000 and other values larger than that render the TSC
    deadline timer disfunctional.
    
    We have not yet any information about this from Intel, but experimentation
    clearly proves that this is a 32/64 bit and sign extension issue.
    
    If adjust values larger than that are actually required, which might be the
    case for physical CPU hotplug, then we need to disable the deadline timer
    on the affected package/CPUs and use the local APIC timer instead.
    
    That requires some surgery in the APIC setup code, so we just limit the
    ADJUST register value into the known to work range for now and revisit this
    when Intel comes forth with proper information.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Roland Scheidegger <rscheidegger_lists@hispeed.ch>
    Cc: Bruce Schlobohm <bruce.schlobohm@intel.com>
    Cc: Kevin Stanton <kevin.b.stanton@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>

commit b9a0deb96b8b5a7e896da183974ba6feb727f14a
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Wed Dec 7 14:44:33 2016 -0800

    redo: radix tree test suite: fix compilation
    
    [ This resurrects commit 53855d10f456, which was reverted in
      2b41226b39b6.  It depended on commit d544abd5ff7d ("lib/radix-tree:
      Convert to hotplug state machine") so now it is correct to apply ]
    
    Patch "lib/radix-tree: Convert to hotplug state machine" breaks the test
    suite as it adds a call to cpuhp_setup_state_nocalls() which is not
    currently emulated in the test suite.  Add it, and delete the emulation
    of the old CPU hotplug mechanism.
    
    Link: http://lkml.kernel.org/r/1480369871-5271-36-git-send-email-mawilcox@linuxonhyperv.com
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    Tested-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Matthew Wilcox <mawilcox@microsoft.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 99e6f6e8134b0c9d5d82eb2af1068a57199125e4
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Dec 7 14:31:33 2016 +0100

    tracing/rb: Init the CPU mask on allocation
    
    Before commit b32614c03413 ("tracing/rb: Convert to hotplug state
    machine") the allocated cpumask was initialized to the mask of ONLINE or
    POSSIBLE CPUs. After the CPU hotplug changes the buffer initialisation
    moved to trace_rb_cpu_prepare() but I forgot to initially set the
    cpumask to zero. This is done now.
    
    Link: http://lkml.kernel.org/r/20161207133133.hzkcqfllxcdi3joz@linutronix.de
    
    Fixes: b32614c03413 ("tracing/rb: Convert to hotplug state machine")
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Tested-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit b19ad3b9f1fd46bb7d4ac623a4f2cb46fa2cb7a0
Merge: fecc8c0ebd30 404ea9f1a792
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Dec 12 20:46:15 2016 +0100

    Merge branch 'pm-cpuidle'
    
    * pm-cpuidle:
      cpuidle: Add a kerneldoc comment to cpuidle_use_deepest_state()
      cpuidle: fix improper return value on error
      intel_idle: Convert to hotplug state machine
      intel_idle: Remove superfluous SMP fuction call
      MAINTAINERS: Add Jacob Pan as a new intel_idle maintainer
      MAINTAINERS: Add bug tracking system location entries for cpuidle
      x86/intel_idle: Add Knights Mill CPUID
      x86/intel_idle: Add CPU model 0x4a (Atom Z34xx series)
      thermal/intel_powerclamp: stop sched tick in forced idle
      thermal/intel_powerclamp: Convert to CPU hotplug state
      thermal/intel_powerclamp: Convert the kthread to kthread worker API
      thermal/intel_powerclamp: Remove duplicated code that starts the kthread
      sched/idle: Add support for tasks that inject idle
      cpuidle: Allow enforcing deepest idle state selection
      cpuidle/powernv: staticise powernv_idle_driver
      cpuidle: dt: assign ->enter_freeze to same as ->enter callback function
      cpuidle: governors: Remove remaining old module code

commit 53855d10f4567a0577360b6448d52a863929775b
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Wed Dec 7 14:44:33 2016 -0800

    radix tree test suite: fix compilation
    
    Patch "lib/radix-tree: Convert to hotplug state machine" breaks the test
    suite as it adds a call to cpuhp_setup_state_nocalls() which is not
    currently emulated in the test suite.  Add it, and delete the emulation
    of the old CPU hotplug mechanism.
    
    Link: http://lkml.kernel.org/r/1480369871-5271-36-git-send-email-mawilcox@linuxonhyperv.com
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    Tested-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Matthew Wilcox <mawilcox@microsoft.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b18cc3de00ec3442cf40ac60787dbe0703b99e24
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Dec 7 14:31:33 2016 +0100

    tracing/rb: Init the CPU mask on allocation
    
    Before commit b32614c03413 ("tracing/rb: Convert to hotplug state machine")
    the allocated cpumask was initialized to the mask of online or possible
    CPUs. After the CPU hotplug changes the buffer initialization moved to
    trace_rb_cpu_prepare() but the cpumask is allocated with alloc_cpumask()
    and therefor has random content. As a consequence the cpu buffers are not
    initialized and a later access dereferences a NULL pointer.
    
    Use zalloc_cpumask() instead so trace_rb_cpu_prepare() initializes the
    buffers properly.
    
    Fixes: b32614c03413 ("tracing/rb: Convert to hotplug state machine")
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: rostedt@goodmis.org
    Link: http://lkml.kernel.org/r/20161207133133.hzkcqfllxcdi3joz@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit cb91fef1b71954e3edc79fb4171b43f6aa2028c7
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Nov 28 13:44:51 2016 -0800

    thermal/intel_powerclamp: Convert to CPU hotplug state
    
    This is a conversation to the new hotplug state machine with
    the difference that CPU_DEAD becomes CPU_PREDOWN.
    
    At the same time it makes the handling of the two states symmetrical.
    stop_power_clamp_worker() is called unconditionally and the controversial
    error message is removed.
    
    Finally, the hotplug state callbacks are removed after the powerclamping
    is stopped to avoid a potential race.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    [pmladek@suse.com: Fixed the possible race in powerclamp_exit()]
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 8b223bc7abe0e30e8d297a24ee6c6c07ef8d0bb9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Nov 19 13:47:36 2016 +0000

    x86/tsc: Store and check TSC ADJUST MSR
    
    The TSC_ADJUST MSR shows whether the TSC has been modified. This is helpful
    in a two aspects:
    
    1) It allows to detect BIOS wreckage, where SMM code tries to 'hide' the
       cycles spent by storing the TSC value at SMM entry and restoring it at
       SMM exit. On affected machines the TSCs run slowly out of sync up to the
       point where the clocksource watchdog (if available) detects it.
    
       The TSC_ADJUST MSR allows to detect the TSC modification before that and
       eventually restore it. This is also important for SoCs which have no
       watchdog clocksource and therefore TSC wreckage cannot be detected and
       acted upon.
    
    2) All threads in a package are required to have the same TSC_ADJUST
       value. Broken BIOSes break that and as a result the TSC synchronization
       check fails.
    
       The TSC_ADJUST MSR allows to detect the deviation when a CPU comes
       online. If detected set it to the value of an already online CPU in the
       same package. This also allows to reduce the number of sync tests
       because with that in place the test is only required for the first CPU
       in a package.
    
       In principle all CPUs in a system should have the same TSC_ADJUST value
       even across packages, but with physical CPU hotplug this assumption is
       not true because the TSC starts with power on, so physical hotplug has
       to do some trickery to bring the TSC into sync with already running
       packages, which requires to use an TSC_ADJUST value different from CPUs
       which got powered earlier.
    
       A final enhancement is the opportunity to compensate for unsynced TSCs
       accross nodes at boot time and make the TSC usable that way. It won't
       help for TSCs which run apart due to frequency skew between packages,
       but this gets detected by the clocksource watchdog later.
    
    The first step toward this is to store the TSC_ADJUST value of a starting
    CPU and compare it with the value of an already online CPU in the same
    package. If they differ, emit a warning and adjust it to the reference
    value. The !SMP version just stores the boot value for later verification.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Link: http://lkml.kernel.org/r/20161119134017.655323776@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit e00c7b216f34444252f3771f7d4ed48d4f032636
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Nov 26 01:28:09 2016 +0100

    bpf: fix multiple issues in selftest suite and samples
    
    1) The test_lru_map and test_lru_dist fails building on my machine since
       the sys/resource.h header is not included.
    
    2) test_verifier fails in one test case where we try to call an invalid
       function, since the verifier log output changed wrt printing function
       names.
    
    3) Current selftest suite code relies on sysconf(_SC_NPROCESSORS_CONF) for
       retrieving the number of possible CPUs. This is broken at least in our
       scenario and really just doesn't work.
    
       glibc tries a number of things for retrieving _SC_NPROCESSORS_CONF.
       First it tries equivalent of /sys/devices/system/cpu/cpu[0-9]* | wc -l,
       if that fails, depending on the config, it either tries to count CPUs
       in /proc/cpuinfo, or returns the _SC_NPROCESSORS_ONLN value instead.
       If /proc/cpuinfo has some issue, it returns just 1 worst case. This
       oddity is nothing new [1], but semantics/behaviour seems to be settled.
       _SC_NPROCESSORS_ONLN will parse /sys/devices/system/cpu/online, if
       that fails it looks into /proc/stat for cpuX entries, and if also that
       fails for some reason, /proc/cpuinfo is consulted (and returning 1 if
       unlikely all breaks down).
    
       While that might match num_possible_cpus() from the kernel in some
       cases, it's really not guaranteed with CPU hotplugging, and can result
       in a buffer overflow since the array in user space could have too few
       number of slots, and on perpcu map lookup, the kernel will write beyond
       that memory of the value buffer.
    
       William Tu reported such mismatches:
    
         [...] The fact that sysconf(_SC_NPROCESSORS_CONF) != num_possible_cpu()
         happens when CPU hotadd is enabled. For example, in Fusion when
         setting vcpu.hotadd = "TRUE" or in KVM, setting ./qemu-system-x86_64
         -smp 2, maxcpus=4 ... the num_possible_cpu() will be 4 and sysconf()
         will be 2 [2]. [...]
    
       Documentation/cputopology.txt says /sys/devices/system/cpu/possible
       outputs cpu_possible_mask. That is the same as in num_possible_cpus(),
       so first step would be to fix the _SC_NPROCESSORS_CONF calls with our
       own implementation. Later, we could add support to bpf(2) for passing
       a mask via CPU_SET(3), for example, to just select a subset of CPUs.
    
       BPF samples code needs this fix as well (at least so that people stop
       copying this). Thus, define bpf_num_possible_cpus() once in selftests
       and import it from there for the sample code to avoid duplicating it.
       The remaining sysconf(_SC_NPROCESSORS_CONF) in samples are unrelated.
    
    After all three issues are fixed, the test suite runs fine again:
    
      # make run_tests | grep self
      selftests: test_verifier [PASS]
      selftests: test_maps [PASS]
      selftests: test_lru_map [PASS]
      selftests: test_kmod.sh [PASS]
    
      [1] https://www.sourceware.org/ml/libc-alpha/2011-06/msg00079.html
      [2] https://www.mail-archive.com/netdev@vger.kernel.org/msg121183.html
    
    Fixes: 3059303f59cf ("samples/bpf: update tracex[23] examples to use per-cpu maps")
    Fixes: 86af8b4191d2 ("Add sample for adding simple drop program to link")
    Fixes: df570f577231 ("samples/bpf: unit test for BPF_MAP_TYPE_PERCPU_ARRAY")
    Fixes: e15596717948 ("samples/bpf: unit test for BPF_MAP_TYPE_PERCPU_HASH")
    Fixes: ebb676daa1a3 ("bpf: Print function name in addition to function id")
    Fixes: 5db58faf989f ("bpf: Add tests for the LRU bpf_htab")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: William Tu <u9012063@gmail.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b4005e9278a4e62819fb16ba4bc3430ca650d0ab
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 22 21:16:05 2016 +0000

    powercap/intel_rapl: Track active CPUs internally
    
    The ability of the CPU hotplug code to stop online/offline at each step
    makes it necessary to track the activated CPUs in a package directly,
    because outerwise a CPU offline callback can find CPUs which have already
    executed the offline callback, but are not yet marked offline in the
    topology mask. That could make such a CPU the package leader and in case
    that CPU goes fully offline leave the package lead orphaned.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 421a9d9588c4b68b8a80d70da08851f4cdd06179
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Mon Jun 13 18:32:45 2016 +0800

    sched/cputime: Fix prev steal time accouting during CPU hotplug
    
    commit 3d89e5478bf550a50c99e93adf659369798263b0 upstream.
    
    Commit:
    
      e9532e69b8d1 ("sched/cputime: Fix steal time accounting vs. CPU hotplug")
    
    ... set rq->prev_* to 0 after a CPU hotplug comes back, in order to
    fix the case where (after CPU hotplug) steal time is smaller than
    rq->prev_steal_time.
    
    However, this should never happen. Steal time was only smaller because of the
    KVM-specific bug fixed by the previous patch.  Worse, the previous patch
    triggers a bug on CPU hot-unplug/plug operation: because
    rq->prev_steal_time is cleared, all of the CPU's past steal time will be
    accounted again on hot-plug.
    
    Since the root cause has been fixed, we can just revert commit e9532e69b8d1.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 'commit e9532e69b8d1 ("sched/cputime: Fix steal time accounting vs. CPU hotplug")'
    Link: http://lkml.kernel.org/r/1465813966-3116-3-git-send-email-wanpeng.li@hotmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    [bwh: Backported to 3.16: adjust context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 97220c6cedc6e7f54f4db5ea999e48a7b774b9d4
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Mon Jun 13 18:32:45 2016 +0800

    sched/cputime: Fix prev steal time accouting during CPU hotplug
    
    commit 3d89e5478bf550a50c99e93adf659369798263b0 upstream.
    
    Commit:
    
      e9532e69b8d1 ("sched/cputime: Fix steal time accounting vs. CPU hotplug")
    
    ... set rq->prev_* to 0 after a CPU hotplug comes back, in order to
    fix the case where (after CPU hotplug) steal time is smaller than
    rq->prev_steal_time.
    
    However, this should never happen. Steal time was only smaller because of the
    KVM-specific bug fixed by the previous patch.  Worse, the previous patch
    triggers a bug on CPU hot-unplug/plug operation: because
    rq->prev_steal_time is cleared, all of the CPU's past steal time will be
    accounted again on hot-plug.
    
    Since the root cause has been fixed, we can just revert commit e9532e69b8d1.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 'commit e9532e69b8d1 ("sched/cputime: Fix steal time accounting vs. CPU hotplug")'
    Link: http://lkml.kernel.org/r/1465813966-3116-3-git-send-email-wanpeng.li@hotmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    [bwh: Backported to 3.2: adjust filename, context]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 2c11fc87caa9a60ada54f4bfc97f7b1abc38d7d0
Merge: b75d3886f34e 9cfb38a7ba5a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 18 09:53:59 2016 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fix from Ingo Molnar:
     "Fix a crash that can trigger when racing with CPU hotplug: we didn't
      use sched-domains data structures carefully enough in select_idle_cpu()"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/fair: Fix sched domains NULL dereference in select_idle_sibling()

commit 351267d941bffeddfaa55ba05c77f971b9f67cfe
Merge: 5aa43efe905b a705e07b9c80
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 18 08:35:07 2016 -0700

    Merge branch 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull misc fixes from Ingo Molnar:
     "A CPU hotplug debuggability fix and three objtool false positive
      warnings fixes for new GCC6 code generation patterns"
    
    * 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Use distinct name for cpu_hotplug.dep_map
      objtool: Skip all "unreachable instruction" warnings for gcov kernels
      objtool: Improve rare switch jump table pattern detection
      objtool: Support '-mtune=atom' stack frame setup instruction

commit 5fa0eb0b4d4780fbd6d8a09850cc4fd539e9fe65
Merge: c48ce9f19026 d4b05923f579
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 10 10:59:07 2016 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 updates from Thomas Gleixner:
     "A pile of regression fixes and updates:
    
       - address the fallout of the patches which made the cpuid - nodeid
         relation permanent: Handling of invalid APIC ids and preventing
         pointless warning messages.
    
       - force eager FPU when protection keys are enabled. Protection keys
         are not generating FPU exceptions so they cannot work with the lazy
         FPU mechanism.
    
       - prevent force migration of interrupts which are not part of the CPU
         vector domain.
    
       - handle the fact that APIC ids are not updated in the ACPI/MADT
         tables on physical CPU hotplug
    
       - remove bash-isms from syscall table generator script
    
       - use the hypervisor supplied APIC frequency when running on VMware"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/pkeys: Make protection keys an "eager" feature
      x86/apic: Prevent pointless warning messages
      x86/acpi: Prevent LAPIC id 0xff from being accounted
      arch/x86: Handle non enumerated CPU after physical hotplug
      x86/unwind: Fix oprofile module link error
      x86/vmware: Skip lapic calibration on VMware
      x86/syscalls: Remove bash-isms in syscall table generator
      x86/irq: Prevent force migration of irqs which are not in the vector domain

commit 24532f768121b07b16178ffb40442ece43365cbd
Merge: 12e3d3cdd975 97a32864e6de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 9 17:32:20 2016 -0700

    Merge branch 'for-4.9/block-smp' of git://git.kernel.dk/linux-block
    
    Pull blk-mq CPU hotplug update from Jens Axboe:
     "This is the conversion of blk-mq to the new hotplug state machine"
    
    * 'for-4.9/block-smp' of git://git.kernel.dk/linux-block:
      blk-mq: fixup "Convert to new hotplug state machine"
      blk-mq: Convert to new hotplug state machine
      blk-mq/cpu-notif: Convert to new hotplug state machine

commit 12e3d3cdd975fe986cc5c35f60b1467a8ec20b80
Merge: 48915c2cbc77 8ec2ef2b66ea
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 9 17:29:33 2016 -0700

    Merge branch 'for-4.9/block-irq' of git://git.kernel.dk/linux-block
    
    Pull blk-mq irq/cpu mapping updates from Jens Axboe:
     "This is the block-irq topic branch for 4.9-rc. It's mostly from
      Christoph, and it allows drivers to specify their own mappings, and
      more importantly, to share the blk-mq mappings with the IRQ affinity
      mappings. It's a good step towards making this work better out of the
      box"
    
    * 'for-4.9/block-irq' of git://git.kernel.dk/linux-block:
      blk_mq: linux/blk-mq.h does not include all the headers it depends on
      blk-mq: kill unused blk_mq_create_mq_map()
      blk-mq: get rid of the cpumask in struct blk_mq_tags
      nvme: remove the post_scan callout
      nvme: switch to use pci_alloc_irq_vectors
      blk-mq: provide a default queue mapping for PCI device
      blk-mq: allow the driver to pass in a queue mapping
      blk-mq: remove ->map_queue
      blk-mq: only allocate a single mq_map per tag_set
      blk-mq: don't redistribute hardware queues on a CPU hotplug event

commit 8132ffc977a4d4572b57362bce70e7e4405bc081
Author: Joonwoo Park <joonwoop@codeaurora.org>
Date:   Sun Sep 11 21:14:58 2016 -0700

    cpuset: handle race between CPU hotplug and cpuset_hotplug_work
    
    commit 28b89b9e6f7b6c8fef7b3af39828722bca20cfee upstream.
    
    A discrepancy between cpu_online_mask and cpuset's effective_cpus
    mask is inevitable during hotplug since cpuset defers updating of
    effective_cpus mask using a workqueue, during which time nothing
    prevents the system from more hotplug operations.  For that reason
    guarantee_online_cpus() walks up the cpuset hierarchy until it finds
    an intersection under the assumption that top cpuset's effective_cpus
    mask intersects with cpu_online_mask even with such a race occurring.
    
    However a sequence of CPU hotplugs can open a time window, during which
    none of the effective CPUs in the top cpuset intersect with
    cpu_online_mask.
    
    For example when there are 4 possible CPUs 0-3 and only CPU0 is online:
    
      ========================  ===========================
       cpu_online_mask           top_cpuset.effective_cpus
      ========================  ===========================
       echo 1 > cpu2/online.
       CPU hotplug notifier woke up hotplug work but not yet scheduled.
          [0,2]                     [0]
    
       echo 0 > cpu0/online.
       The workqueue is still runnable.
          [2]                       [0]
      ========================  ===========================
    
      Now there is no intersection between cpu_online_mask and
      top_cpuset.effective_cpus.  Thus invoking sys_sched_setaffinity() at
      this moment can cause following:
    
       Unable to handle kernel NULL pointer dereference at virtual address 000000d0
       ------------[ cut here ]------------
       Kernel BUG at ffffffc0001389b0 [verbose debug info unavailable]
       Internal error: Oops - BUG: 96000005 [#1] PREEMPT SMP
       Modules linked in:
       CPU: 2 PID: 1420 Comm: taskset Tainted: G        W       4.4.8+ #98
       task: ffffffc06a5c4880 ti: ffffffc06e124000 task.ti: ffffffc06e124000
       PC is at guarantee_online_cpus+0x2c/0x58
       LR is at cpuset_cpus_allowed+0x4c/0x6c
       <snip>
       Process taskset (pid: 1420, stack limit = 0xffffffc06e124020)
       Call trace:
       [<ffffffc0001389b0>] guarantee_online_cpus+0x2c/0x58
       [<ffffffc00013b208>] cpuset_cpus_allowed+0x4c/0x6c
       [<ffffffc0000d61f0>] sched_setaffinity+0xc0/0x1ac
       [<ffffffc0000d6374>] SyS_sched_setaffinity+0x98/0xac
       [<ffffffc000085cb0>] el0_svc_naked+0x24/0x28
    
    The top cpuset's effective_cpus are guaranteed to be identical to
    cpu_online_mask eventually.  Hence fall back to cpu_online_mask when
    there is no intersection between top cpuset's effective_cpus and
    cpu_online_mask.
    
    Signed-off-by: Joonwoo Park <joonwoop@codeaurora.org>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: cgroups@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit afca668faa80cbd97ca767d41c2845a175d931c2
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Mon Jun 13 18:32:45 2016 +0800

    sched/cputime: Fix prev steal time accouting during CPU hotplug
    
    commit 3d89e5478bf550a50c99e93adf659369798263b0 upstream.
    
    Commit:
    
      e9532e69b8d1 ("sched/cputime: Fix steal time accounting vs. CPU hotplug")
    
    ... set rq->prev_* to 0 after a CPU hotplug comes back, in order to
    fix the case where (after CPU hotplug) steal time is smaller than
    rq->prev_steal_time.
    
    However, this should never happen. Steal time was only smaller because of the
    KVM-specific bug fixed by the previous patch.  Worse, the previous patch
    triggers a bug on CPU hot-unplug/plug operation: because
    rq->prev_steal_time is cleared, all of the CPU's past steal time will be
    accounted again on hot-plug.
    
    Since the root cause has been fixed, we can just revert commit e9532e69b8d1.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 'commit e9532e69b8d1 ("sched/cputime: Fix steal time accounting vs. CPU hotplug")'
    Link: http://lkml.kernel.org/r/1465813966-3116-3-git-send-email-wanpeng.li@hotmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0ec4bc23c454f675a1ca3d3155e8ad1bfde4841f
Author: Joonwoo Park <joonwoop@codeaurora.org>
Date:   Sun Sep 11 21:14:58 2016 -0700

    cpuset: handle race between CPU hotplug and cpuset_hotplug_work
    
    commit 28b89b9e6f7b6c8fef7b3af39828722bca20cfee upstream.
    
    A discrepancy between cpu_online_mask and cpuset's effective_cpus
    mask is inevitable during hotplug since cpuset defers updating of
    effective_cpus mask using a workqueue, during which time nothing
    prevents the system from more hotplug operations.  For that reason
    guarantee_online_cpus() walks up the cpuset hierarchy until it finds
    an intersection under the assumption that top cpuset's effective_cpus
    mask intersects with cpu_online_mask even with such a race occurring.
    
    However a sequence of CPU hotplugs can open a time window, during which
    none of the effective CPUs in the top cpuset intersect with
    cpu_online_mask.
    
    For example when there are 4 possible CPUs 0-3 and only CPU0 is online:
    
      ========================  ===========================
       cpu_online_mask           top_cpuset.effective_cpus
      ========================  ===========================
       echo 1 > cpu2/online.
       CPU hotplug notifier woke up hotplug work but not yet scheduled.
          [0,2]                     [0]
    
       echo 0 > cpu0/online.
       The workqueue is still runnable.
          [2]                       [0]
      ========================  ===========================
    
      Now there is no intersection between cpu_online_mask and
      top_cpuset.effective_cpus.  Thus invoking sys_sched_setaffinity() at
      this moment can cause following:
    
       Unable to handle kernel NULL pointer dereference at virtual address 000000d0
       ------------[ cut here ]------------
       Kernel BUG at ffffffc0001389b0 [verbose debug info unavailable]
       Internal error: Oops - BUG: 96000005 [#1] PREEMPT SMP
       Modules linked in:
       CPU: 2 PID: 1420 Comm: taskset Tainted: G        W       4.4.8+ #98
       task: ffffffc06a5c4880 ti: ffffffc06e124000 task.ti: ffffffc06e124000
       PC is at guarantee_online_cpus+0x2c/0x58
       LR is at cpuset_cpus_allowed+0x4c/0x6c
       <snip>
       Process taskset (pid: 1420, stack limit = 0xffffffc06e124020)
       Call trace:
       [<ffffffc0001389b0>] guarantee_online_cpus+0x2c/0x58
       [<ffffffc00013b208>] cpuset_cpus_allowed+0x4c/0x6c
       [<ffffffc0000d61f0>] sched_setaffinity+0xc0/0x1ac
       [<ffffffc0000d6374>] SyS_sched_setaffinity+0x98/0xac
       [<ffffffc000085cb0>] el0_svc_naked+0x24/0x28
    
    The top cpuset's effective_cpus are guaranteed to be identical to
    cpu_online_mask eventually.  Hence fall back to cpu_online_mask when
    there is no intersection between top cpuset's effective_cpus and
    cpu_online_mask.
    
    Signed-off-by: Joonwoo Park <joonwoop@codeaurora.org>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: cgroups@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 541efb7632642cab55361178d73d544f025b593c
Merge: 6218590bcb45 a6a198bc60e6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 6 11:19:10 2016 -0700

    Merge tag 'for-linus-4.9-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from David Vrabel:
     "xen features and fixes for 4.9:
    
       - switch to new CPU hotplug mechanism
    
       - support driver_override in pciback
    
       - require vector callback for HVM guests (the alternate mechanism via
         the platform device has been broken for ages)"
    
    * tag 'for-linus-4.9-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/x86: Update topology map for PV VCPUs
      xen/x86: Initialize per_cpu(xen_vcpu, 0) a little earlier
      xen/pciback: support driver_override
      xen/pciback: avoid multiple entries in slot list
      xen/pciback: simplify pcistub device handling
      xen: Remove event channel notification through Xen PCI platform device
      xen/events: Convert to hotplug state machine
      xen/x86: Convert to hotplug state machine
      x86/xen: add missing \n at end of printk warning message
      xen/grant-table: Use kmalloc_array() in arch_gnttab_valloc()
      xen: Make VPMU init message look less scary
      xen: rename xen_pmu_init() in sys-hypervisor.c
      hotplug: Prevent alloc/free of irq descriptors during cpu up/down (again)
      xen/x86: Move irq allocation from Xen smp_op.cpu_up()

commit 597f03f9d133e9837d00965016170271d4f87dcf
Merge: 999dcbe2414e 0bf71e4d02ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 19:43:08 2016 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug updates from Thomas Gleixner:
     "Yet another batch of cpu hotplug core updates and conversions:
    
       - Provide core infrastructure for multi instance drivers so the
         drivers do not have to keep custom lists.
    
       - Convert custom lists to the new infrastructure. The block-mq custom
         list conversion comes through the block tree and makes the diffstat
         tip over to more lines removed than added.
    
       - Handle unbalanced hotplug enable/disable calls more gracefully.
    
       - Remove the obsolete CPU_STARTING/DYING notifier support.
    
       - Convert another batch of notifier users.
    
       The relayfs changes which conflicted with the conversion have been
       shipped to me by Andrew.
    
       The remaining lot is targeted for 4.10 so that we finally can remove
       the rest of the notifiers"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (46 commits)
      cpufreq: Fix up conversion to hotplug state machine
      blk/mq: Reserve hotplug states for block multiqueue
      x86/apic/uv: Convert to hotplug state machine
      s390/mm/pfault: Convert to hotplug state machine
      mips/loongson/smp: Convert to hotplug state machine
      mips/octeon/smp: Convert to hotplug state machine
      fault-injection/cpu: Convert to hotplug state machine
      padata: Convert to hotplug state machine
      cpufreq: Convert to hotplug state machine
      ACPI/processor: Convert to hotplug state machine
      virtio scsi: Convert to hotplug state machine
      oprofile/timer: Convert to hotplug state machine
      block/softirq: Convert to hotplug state machine
      lib/irq_poll: Convert to hotplug state machine
      x86/microcode: Convert to hotplug state machine
      sh/SH-X3 SMP: Convert to hotplug state machine
      ia64/mca: Convert to hotplug state machine
      ARM/OMAP/wakeupgen: Convert to hotplug state machine
      ARM/shmobile: Convert to hotplug state machine
      arm64/FP/SIMD: Convert to hotplug state machine
      ...

commit 110a9e42b68719f584879c5c5c727bbae90d15f9
Merge: af79ad2b1f33 eb6296dec19f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 15:36:06 2016 -0700

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 apic updates from Ingo Molnar:
     "The main changes are:
    
       - Persistent CPU/node numbering across CPU hotplug/unplug events.
         This is a pretty involved series of changes that first fetches all
         the information during bootup and then uses it for the various
         hotplug/unplug methods. (Gu Zheng, Dou Liyang)
    
       - IO-APIC hot-add/remove fixes and enhancements. (Rui Wang)
    
       - ... various fixes, cleanups and enhancements"
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (22 commits)
      x86/apic: Fix silent & fatal merge conflict in __generic_processor_info()
      acpi: Fix broken error check in map_processor()
      acpi: Validate processor id when mapping the processor
      acpi: Provide mechanism to validate processors in the ACPI tables
      x86/acpi: Set persistent cpuid <-> nodeid mapping when booting
      x86/acpi: Enable MADT APIs to return disabled apicids
      x86/acpi: Introduce persistent storage for cpuid <-> apicid mapping
      x86/acpi: Enable acpi to register all possible cpus at boot time
      x86/numa: Online memory-less nodes at boot time
      x86/apic: Get rid of apic_version[] array
      x86/apic: Order irq_enter/exit() calls correctly vs. ack_APIC_irq()
      x86/ioapic: Ignore root bridges without a companion ACPI device
      x86/apic: Update comment about disabling processor focus
      x86/smpboot: Check APIC ID before setting up default routing
      x86/ioapic: Fix IOAPIC failing to request resource
      x86/ioapic: Fix lost IOAPIC resource after hot-removal and hotadd
      x86/ioapic: Fix setup_res() failing to get resource
      x86/ioapic: Support hot-removal of IOAPICs present during boot
      x86/ioapic: Change prototype of acpi_ioapic_add()
      x86/apic, ACPI: Fix incorrect assignment when handling apic/x2apic entries
      ...

commit 4b978934a440c1aafce986353001b03289eaa040
Merge: 72a9cdd08300 2d8fbcd13ea1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 10:29:53 2016 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Expedited grace-period changes, most notably avoiding having user
         threads drive expedited grace periods, using a workqueue instead.
    
       - Miscellaneous fixes, including a performance fix for lists that was
         sent with the lists modifications.
    
       - CPU hotplug updates, most notably providing exact CPU-online
         tracking for RCU. This will in turn allow removal of the checks
         supporting RCU's prior heuristic that was based on the assumption
         that CPUs would take no longer than one jiffy to come online.
    
       - Torture-test updates.
    
       - Documentation updates"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (22 commits)
      list: Expand list_first_entry_or_null()
      torture: TOROUT_STRING(): Insert a space between flag and message
      rcuperf: Consistently insert space between flag and message
      rcutorture: Print out barrier error as document says
      torture: Add task state to writer-task stall printk()s
      torture: Convert torture_shutdown() to hrtimer
      rcutorture: Convert to hotplug state machine
      cpu/hotplug: Get rid of CPU_STARTING reference
      rcu: Provide exact CPU-online tracking for RCU
      rcu: Avoid redundant quiescent-state chasing
      rcu: Don't use modular infrastructure in non-modular code
      sched: Make wake_up_nohz_cpu() handle CPUs going offline
      rcu: Use rcu_gp_kthread_wake() to wake up grace period kthreads
      rcu: Use RCU's online-CPU state for expedited IPI retry
      rcu: Exclude RCU-offline CPUs from expedited grace periods
      rcu: Make expedited RCU CPU stall warnings respond to controls
      rcu: Stop disabling expedited RCU CPU stall warnings
      rcu: Drive expedited grace periods from workqueue
      rcu: Consolidate expedited grace period machinery
      documentation: Record reason for rcu_head two-byte alignment
      ...

commit 7af8a0f8088831428051976cb06cc1e450f8bab5
Merge: c8d2bc9bc39e db68f3e7594a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 08:58:35 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "It's a bit all over the place this time with no "killer feature" to
      speak of.  Support for mismatched cache line sizes should help people
      seeing whacky JIT failures on some SoCs, and the big.LITTLE perf
      updates have been a long time coming, but a lot of the changes here
      are cleanups.
    
      We stray outside arch/arm64 in a few areas: the arch/arm/ arch_timer
      workaround is acked by Russell, the DT/OF bits are acked by Rob, the
      arch_timer clocksource changes acked by Marc, CPU hotplug by tglx and
      jump_label by Peter (all CC'd).
    
      Summary:
    
       - Support for execute-only page permissions
       - Support for hibernate and DEBUG_PAGEALLOC
       - Support for heterogeneous systems with mismatches cache line sizes
       - Errata workarounds (A53 843419 update and QorIQ A-008585 timer bug)
       - arm64 PMU perf updates, including cpumasks for heterogeneous systems
       - Set UTS_MACHINE for building rpm packages
       - Yet another head.S tidy-up
       - Some cleanups and refactoring, particularly in the NUMA code
       - Lots of random, non-critical fixes across the board"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (100 commits)
      arm64: tlbflush.h: add __tlbi() macro
      arm64: Kconfig: remove SMP dependence for NUMA
      arm64: Kconfig: select OF/ACPI_NUMA under NUMA config
      arm64: fix dump_backtrace/unwind_frame with NULL tsk
      arm/arm64: arch_timer: Use archdata to indicate vdso suitability
      arm64: arch_timer: Work around QorIQ Erratum A-008585
      arm64: arch_timer: Add device tree binding for A-008585 erratum
      arm64: Correctly bounds check virt_addr_valid
      arm64: migrate exception table users off module.h and onto extable.h
      arm64: pmu: Hoist pmu platform device name
      arm64: pmu: Probe default hw/cache counters
      arm64: pmu: add fallback probe table
      MAINTAINERS: Update ARM PMU PROFILING AND DEBUGGING entry
      arm64: Improve kprobes test for atomic sequence
      arm64/kvm: use alternative auto-nop
      arm64: use alternative auto-nop
      arm64: alternative: add auto-nop infrastructure
      arm64: lse: convert lse alternatives NOP padding to use __nops
      arm64: barriers: introduce nops and __nops macros for NOP sequences
      arm64: sysreg: replace open-coded mrs_s/msr_s with {read,write}_sysreg_s
      ...

commit 4d737042d6c4ee10a632cf94b953169d13955a40
Author: Boris Ostrovsky <boris.ostrovsky@oracle.com>
Date:   Wed Sep 7 13:19:00 2016 -0400

    xen/x86: Convert to hotplug state machine
    
    Switch to new CPU hotplug infrastructure.
    
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Suggested-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

commit 6ca8ac773e97e2dfa5734ae435c40e672dd19ac4
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Thu Sep 22 11:59:47 2016 +0100

    MIPS: smp-cps: Avoid BUG() when offlining pre-r6 CPUs
    
    Commit 0d2808f338c7 ("MIPS: smp-cps: Add support for CPU hotplug of
    MIPSr6 processors") added a call to mips_cm_lock_other in order to lock
    the CPC in CPUs containing a version 3 or higher Coherence Manager,
    which use the general CM core other register, where previous CMs had a
    dedicated core other register for the CPC.
    
    A kernel BUG() is triggered, however, if mips_cm_lock_other is called
    with a VP other than 0 on a CPU with CM < 3, a condition introduced by
    0d2808f338c7.
    
    Avoid the BUG() by always locking VP0 when locking the CPC, since the
    required register, cpc_stat_conf, is shared by all vps in a core.
    
    Fixes: 0d2808f338c7 ("MIPS: smp-cps: Add support for CPU hotplug...)
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Cc: Qais Yousef <qsyousef@gmail.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/14297/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 8ab293e3a1376574e11f9059c09cc0db212546cb
Merge: 08895a8b6b06 9157056da8f8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 27 16:43:11 2016 -0700

    Merge branch 'for-4.8-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup fixes from Tejun Heo:
     "Three late fixes for cgroup: Two cpuset ones, one trivial and the
      other pretty obscure, and a cgroup core fix for a bug which impacts
      cgroup v2 namespace users"
    
    * 'for-4.8-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: fix invalid controller enable rejections with cgroup namespace
      cpuset: fix non static symbol warning
      cpuset: handle race between CPU hotplug and cpuset_hotplug_work

commit e2a738f7a88f32622684d972d654a9fed026555f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Sep 6 19:04:55 2016 +0200

    blk/mq: Reserve hotplug states for block multiqueue
    
    This patch only reserves two CPU hotplug states for block/mq so the block tree
    can apply the conversion patches.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160906170457.32393-20-bigeasy@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 2d8fbcd13ea1d0be3a7ea5f20c3a5b44b592e79c
Merge: 024c7e3756d8 d74b62bc3241
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Sep 16 09:08:43 2016 +0200

    Merge branch 'for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into core/rcu
    
    Pull RCU changes from Paul E. McKenney:
    
     - Expedited grace-period changes, most notably avoiding having
       user threads drive expedited grace periods, using a workqueue
       instead.
    
     - Miscellaneous fixes, including a performance fix for lists
       that was sent with the lists modifications (second URL below).
    
     - CPU hotplug updates, most notably providing exact CPU-online
       tracking for RCU.  This will in turn allow removal of the
       checks supporting RCU's prior heuristic that was based on the
       assumption that CPUs would take no longer than one jiffy to
       come online.
    
     - Torture-test updates.
    
     - Documentation updates.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 4e68a011428af3211facd932b4003b3fa3ef4faa
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Sep 14 16:18:52 2016 +0200

    blk-mq: don't redistribute hardware queues on a CPU hotplug event
    
    Currently blk-mq will totally remap hardware context when a CPU hotplug
    even happened, which causes major havoc for drivers, as they are never
    told about this remapping.  E.g. any carefully sorted out CPU affinity
    will just be completely messed up.
    
    The rebuild also doesn't really help for the common case of cpu
    hotplug, which is soft onlining / offlining of cpus - in this case we
    should just leave the queue and irq mapping as is.  If it actually
    worked it would have helped in the case of physical cpu hotplug,
    although for that we'd need a way to actually notify the driver.
    Note that drivers may already be able to accommodate such a topology
    change on their own, e.g. using the reset_controller sysfs file in NVMe
    will cause the driver to get things right for this case.
    
    With the rebuild removed we will simplify retain the queue mapping for
    a soft offlined CPU that will work when it comes back online, and will
    map any newly onlined CPU to queue 0 until the driver initiates
    a rebuild of the queue map.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Keith Busch <keith.busch@intel.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit 28b89b9e6f7b6c8fef7b3af39828722bca20cfee
Author: Joonwoo Park <joonwoop@codeaurora.org>
Date:   Sun Sep 11 21:14:58 2016 -0700

    cpuset: handle race between CPU hotplug and cpuset_hotplug_work
    
    A discrepancy between cpu_online_mask and cpuset's effective_cpus
    mask is inevitable during hotplug since cpuset defers updating of
    effective_cpus mask using a workqueue, during which time nothing
    prevents the system from more hotplug operations.  For that reason
    guarantee_online_cpus() walks up the cpuset hierarchy until it finds
    an intersection under the assumption that top cpuset's effective_cpus
    mask intersects with cpu_online_mask even with such a race occurring.
    
    However a sequence of CPU hotplugs can open a time window, during which
    none of the effective CPUs in the top cpuset intersect with
    cpu_online_mask.
    
    For example when there are 4 possible CPUs 0-3 and only CPU0 is online:
    
      ========================  ===========================
       cpu_online_mask           top_cpuset.effective_cpus
      ========================  ===========================
       echo 1 > cpu2/online.
       CPU hotplug notifier woke up hotplug work but not yet scheduled.
          [0,2]                     [0]
    
       echo 0 > cpu0/online.
       The workqueue is still runnable.
          [2]                       [0]
      ========================  ===========================
    
      Now there is no intersection between cpu_online_mask and
      top_cpuset.effective_cpus.  Thus invoking sys_sched_setaffinity() at
      this moment can cause following:
    
       Unable to handle kernel NULL pointer dereference at virtual address 000000d0
       ------------[ cut here ]------------
       Kernel BUG at ffffffc0001389b0 [verbose debug info unavailable]
       Internal error: Oops - BUG: 96000005 [#1] PREEMPT SMP
       Modules linked in:
       CPU: 2 PID: 1420 Comm: taskset Tainted: G        W       4.4.8+ #98
       task: ffffffc06a5c4880 ti: ffffffc06e124000 task.ti: ffffffc06e124000
       PC is at guarantee_online_cpus+0x2c/0x58
       LR is at cpuset_cpus_allowed+0x4c/0x6c
       <snip>
       Process taskset (pid: 1420, stack limit = 0xffffffc06e124020)
       Call trace:
       [<ffffffc0001389b0>] guarantee_online_cpus+0x2c/0x58
       [<ffffffc00013b208>] cpuset_cpus_allowed+0x4c/0x6c
       [<ffffffc0000d61f0>] sched_setaffinity+0xc0/0x1ac
       [<ffffffc0000d6374>] SyS_sched_setaffinity+0x98/0xac
       [<ffffffc000085cb0>] el0_svc_naked+0x24/0x28
    
    The top cpuset's effective_cpus are guaranteed to be identical to
    cpu_online_mask eventually.  Hence fall back to cpu_online_mask when
    there is no intersection between top cpuset's effective_cpus and
    cpu_online_mask.
    
    Signed-off-by: Joonwoo Park <joonwoop@codeaurora.org>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: cgroups@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: <stable@vger.kernel.org> # 3.17+
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit c47a1900ad710fd2c97127e2ba19da1df79cf733
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Sep 9 14:07:10 2016 +0100

    arm64: Rearrange CPU errata workaround checks
    
    Right now we run through the work around checks on a CPU
    from __cpuinfo_store_cpu. There are some problems with that:
    
    1) We initialise the system wide CPU feature registers only after the
    Boot CPU updates its cpuinfo. Now, if a work around depends on the
    variance of a CPU ID feature (e.g, check for Cache Line size mismatch),
    we have no way of performing it cleanly for the boot CPU.
    
    2) It is out of place, invoked from __cpuinfo_store_cpu() in cpuinfo.c. It
    is not an obvious place for that.
    
    This patch rearranges the CPU specific capability(aka work around) checks.
    
    1) At the moment we use verify_local_cpu_capabilities() to check if a new
    CPU has all the system advertised features. Use this for the secondary CPUs
    to perform the work around check. For that we rename
      verify_local_cpu_capabilities() => check_local_cpu_capabilities()
    which:
    
       If the system wide capabilities haven't been initialised (i.e, the CPU
       is activated at the boot), update the system wide detected work arounds.
    
       Otherwise (i.e a CPU hotplugged in later) verify that this CPU conforms to the
       system wide capabilities.
    
    2) Boot CPU updates the work arounds from smp_prepare_boot_cpu() after we have
    initialised the system wide CPU feature values.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 8017c279196ab29174bafc104ac4ebbd42c7ca7f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Aug 12 19:49:43 2016 +0200

    net/virtio-net: Convert to hotplug state machine
    
    Install the callbacks via the state machine.
    
    The driver supports multiple instances and therefore the new
    cpuhp_state_add_instance_nocalls() infrastrucure is used. The driver
    currently uses get_online_cpus() to avoid missing a CPU hotplug event while
    invoking virtnet_set_affinity(). This could be avoided by using
    cpuhp_state_add_instance() variant which holds the hotplug lock and invokes
    callback during registration. This is more or less a 1:1 conversion of the
    current code.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: netdev@vger.kernel.org
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: virtualization@lists.linux-foundation.org
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/1471024183-12666-7-git-send-email-bigeasy@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit e937dd5782688928d8c4050237b93b0a51faebee
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 16 11:29:17 2016 +0100

    arm64: debug: convert OS lock CPU hotplug notifier to new infrastructure
    
    The arm64 debug monitor initialisation code uses a CPU hotplug notifier
    to clear the OS lock when CPUs come online.
    
    This patch converts the code to the new hotplug mechanism.
    
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit d7a83d127a64fd91ef1ad39b7e2d78db36cf388b
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Aug 15 18:55:11 2016 +0100

    arm64: hw_breakpoint: convert CPU hotplug notifier to new infrastructure
    
    The arm64 hw_breakpoint implementation uses a CPU hotplug notifier to
    reset the {break,watch}point registers when CPUs come online.
    
    This patch converts the code to the new hotplug mechanism, whilst moving
    the invocation earlier to remove the need to disable IRQs explicitly in
    the driver (which could cause havok if we trip a watchpoint in an IRQ
    handler whilst restoring the debug register state).
    
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 385c859f678e8ee6b0b122086f34e72a0e861cef
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 30 12:16:11 2016 -0700

    rcu: Use RCU's online-CPU state for expedited IPI retry
    
    This commit improves the accuracy of the interaction between CPU hotplug
    operations and RCU's expedited grace periods by using RCU's online-CPU
    state to determine when failed IPIs should be retried.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit e6e7214fbbdab1f90254af68e0927bdb24708d22
Merge: ad83242a8f06 26f2c75cd2cf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 12 13:51:52 2016 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "Misc fixes: cputime fixes, two deadline scheduler fixes and a cgroups
      scheduling fix"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/cputime: Fix omitted ticks passed in parameter
      sched/cputime: Fix steal time accounting
      sched/deadline: Fix lock pinning warning during CPU hotplug
      sched/cputime: Mitigate performance regression in times()/clock_gettime()
      sched/fair: Fix typo in sync_throttle()
      sched/deadline: Fix wrap-around in DL heap

commit d52c0569bab4edc888832df44dc7ac28517134f6
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Aug 11 16:08:35 2016 +0200

    x86/apic/x2apic, smp/hotplug: Don't use before alloc in x2apic_cluster_probe()
    
    I made a mistake while converting the driver to the hotplug state
    machine and as a result x2apic_cluster_probe() was accessing
    cpus_in_cluster before allocating it.
    
    This patch fixes it by setting the cpumask after the allocation the
    memory succeeded.
    
    While at it, I marked two functions static which are only used within
    this file.
    
    Reported-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 6b2c28471de5 ("x86/x2apic: Convert to CPU hotplug state machine")
    Link: http://lkml.kernel.org/r/1470924515-9444-1-git-send-email-bigeasy@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit c0c8c9fa210c9a042060435f17e40ba4a76d6d6f
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Thu Aug 4 09:42:20 2016 +0800

    sched/deadline: Fix lock pinning warning during CPU hotplug
    
    The following warning can be triggered by hot-unplugging the CPU
    on which an active SCHED_DEADLINE task is running on:
    
      WARNING: CPU: 0 PID: 0 at kernel/locking/lockdep.c:3531 lock_release+0x690/0x6a0
      releasing a pinned lock
      Call Trace:
       dump_stack+0x99/0xd0
       __warn+0xd1/0xf0
       ? dl_task_timer+0x1a1/0x2b0
       warn_slowpath_fmt+0x4f/0x60
       ? sched_clock+0x13/0x20
       lock_release+0x690/0x6a0
       ? enqueue_pushable_dl_task+0x9b/0xa0
       ? enqueue_task_dl+0x1ca/0x480
       _raw_spin_unlock+0x1f/0x40
       dl_task_timer+0x1a1/0x2b0
       ? push_dl_task.part.31+0x190/0x190
      WARNING: CPU: 0 PID: 0 at kernel/locking/lockdep.c:3649 lock_unpin_lock+0x181/0x1a0
      unpinning an unpinned lock
      Call Trace:
       dump_stack+0x99/0xd0
       __warn+0xd1/0xf0
       warn_slowpath_fmt+0x4f/0x60
       lock_unpin_lock+0x181/0x1a0
       dl_task_timer+0x127/0x2b0
       ? push_dl_task.part.31+0x190/0x190
    
    As per the comment before this code, its safe to drop the RQ lock
    here, and since we (potentially) change rq, unpin and repin to avoid
    the splat.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    [ Rewrote changelog. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luca Abeni <luca.abeni@unitn.it>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1470274940-17976-1-git-send-email-wanpeng.li@hotmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 71f79fb3179e69b0c1448a2101a866d871c66e7f
Author: Gabriel Krisman Bertazi <krisman@linux.vnet.ibm.com>
Date:   Mon Aug 1 08:23:39 2016 -0600

    blk-mq: Allow timeouts to run while queue is freezing
    
    In case a submitted request gets stuck for some reason, the block layer
    can prevent the request starvation by starting the scheduled timeout work.
    If this stuck request occurs at the same time another thread has started
    a queue freeze, the blk_mq_timeout_work will not be able to acquire the
    queue reference and will return silently, thus not issuing the timeout.
    But since the request is already holding a q_usage_counter reference and
    is unable to complete, it will never release its reference, preventing
    the queue from completing the freeze started by first thread.  This puts
    the request_queue in a hung state, forever waiting for the freeze
    completion.
    
    This was observed while running IO to a NVMe device at the same time we
    toggled the CPU hotplug code. Eventually, once a request got stuck
    requiring a timeout during a queue freeze, we saw the CPU Hotplug
    notification code get stuck inside blk_mq_freeze_queue_wait, as shown in
    the trace below.
    
    [c000000deaf13690] [c000000deaf13738] 0xc000000deaf13738 (unreliable)
    [c000000deaf13860] [c000000000015ce8] __switch_to+0x1f8/0x350
    [c000000deaf138b0] [c000000000ade0e4] __schedule+0x314/0x990
    [c000000deaf13940] [c000000000ade7a8] schedule+0x48/0xc0
    [c000000deaf13970] [c0000000005492a4] blk_mq_freeze_queue_wait+0x74/0x110
    [c000000deaf139e0] [c00000000054b6a8] blk_mq_queue_reinit_notify+0x1a8/0x2e0
    [c000000deaf13a40] [c0000000000e7878] notifier_call_chain+0x98/0x100
    [c000000deaf13a90] [c0000000000b8e08] cpu_notify_nofail+0x48/0xa0
    [c000000deaf13ac0] [c0000000000b92f0] _cpu_down+0x2a0/0x400
    [c000000deaf13b90] [c0000000000b94a8] cpu_down+0x58/0xa0
    [c000000deaf13bc0] [c0000000006d5dcc] cpu_subsys_offline+0x2c/0x50
    [c000000deaf13bf0] [c0000000006cd244] device_offline+0x104/0x140
    [c000000deaf13c30] [c0000000006cd40c] online_store+0x6c/0xc0
    [c000000deaf13c80] [c0000000006c8c78] dev_attr_store+0x68/0xa0
    [c000000deaf13cc0] [c0000000003974d0] sysfs_kf_write+0x80/0xb0
    [c000000deaf13d00] [c0000000003963e8] kernfs_fop_write+0x188/0x200
    [c000000deaf13d50] [c0000000002e0f6c] __vfs_write+0x6c/0xe0
    [c000000deaf13d90] [c0000000002e1ca0] vfs_write+0xc0/0x230
    [c000000deaf13de0] [c0000000002e2cdc] SyS_write+0x6c/0x110
    [c000000deaf13e30] [c000000000009204] system_call+0x38/0xb4
    
    The fix is to allow the timeout work to execute in the window between
    dropping the initial refcount reference and the release of the last
    reference, which actually marks the freeze completion.  This can be
    achieved with percpu_refcount_tryget, which does not require the counter
    to be alive.  This way the timeout work can do it's job and terminate a
    stuck request even during a freeze, returning its reference and avoiding
    the deadlock.
    
    Allowing the timeout to run is just a part of the fix, since for some
    devices, we might get stuck again inside the device driver's timeout
    handler, should it attempt to allocate a new request in that path -
    which is a quite common action for Abort commands, which need to be sent
    after a timeout.  In NVMe, for instance, we call blk_mq_alloc_request
    from inside the timeout handler, which will fail during a freeze, since
    it also tries to acquire a queue reference.
    
    I considered a similar change to blk_mq_alloc_request as a generic
    solution for further device driver hangs, but we can't do that, since it
    would allow new requests to disturb the freeze process.  I thought about
    creating a new function in the block layer to support unfreezable
    requests for these occasions, but after working on it for a while, I
    feel like this should be handled in a per-driver basis.  I'm now
    experimenting with changes to the NVMe timeout path, but I'm open to
    suggestions of ways to make this generic.
    
    Signed-off-by: Gabriel Krisman Bertazi <krisman@linux.vnet.ibm.com>
    Cc: Brian King <brking@linux.vnet.ibm.com>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: linux-nvme@lists.infradead.org
    Cc: linux-block@vger.kernel.org
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit a6408f6cb63ac0958fee7dbce7861ffb540d8a49
Merge: 1a81a8f2a591 4fae16dffb81
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 29 13:55:30 2016 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull smp hotplug updates from Thomas Gleixner:
     "This is the next part of the hotplug rework.
    
       - Convert all notifiers with a priority assigned
    
       - Convert all CPU_STARTING/DYING notifiers
    
         The final removal of the STARTING/DYING infrastructure will happen
         when the merge window closes.
    
      Another 700 hundred line of unpenetrable maze gone :)"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (70 commits)
      timers/core: Correct callback order during CPU hot plug
      leds/trigger/cpu: Move from CPU_STARTING to ONLINE level
      powerpc/numa: Convert to hotplug state machine
      arm/perf: Fix hotplug state machine conversion
      irqchip/armada: Avoid unused function warnings
      ARC/time: Convert to hotplug state machine
      clocksource/atlas7: Convert to hotplug state machine
      clocksource/armada-370-xp: Convert to hotplug state machine
      clocksource/exynos_mct: Convert to hotplug state machine
      clocksource/arm_global_timer: Convert to hotplug state machine
      rcu: Convert rcutree to hotplug state machine
      KVM/arm/arm64/vgic-new: Convert to hotplug state machine
      smp/cfd: Convert core to hotplug state machine
      x86/x2apic: Convert to CPU hotplug state machine
      profile: Convert to hotplug state machine
      timers/core: Convert to hotplug state machine
      hrtimer: Convert to hotplug state machine
      x86/tboot: Convert to hotplug state machine
      arm64/armv8 deprecated: Convert to hotplug state machine
      hwtracing/coresight-etm4x: Convert to hotplug state machine
      ...

commit 926963160ca4d6267957541a85591b7c426066d6
Author: James Hogan <james.hogan@imgtec.com>
Date:   Wed Jul 13 14:12:46 2016 +0100

    MIPS: SMP: Drop stop_this_cpu() cpu_foreign_map hack
    
    Commit cccf34e9411c ("MIPS: c-r4k: Fix cache flushing for MT cores")
    added the cpu_foreign_map cpumask containing a single VPE from each
    online core, and recalculated it when secondary CPUs are brought up.
    
    stop_this_cpu() was also updated to recalculate cpu_foreign_map, but
    with an additional hack before marking the CPU as offline to copy
    cpu_online_mask into cpu_foreign_map and perform an SMP memory barrier.
    
    This appears to have been intended to prevent cache management IPIs
    being missed when the VPE representing the core in cpu_foreign_map is
    taken offline while other VPEs remain online. Unfortunately there is
    nothing in this hack to prevent r4k_on_each_cpu() from reading the old
    cpu_foreign_map, and smp_call_function_many() from reading that new
    cpu_online_mask with the core's representative VPE marked offline. It
    then wouldn't send an IPI to any online VPEs of that core.
    
    stop_this_cpu() is only actually called in panic and system shutdown /
    halt / reboot situations, in which case all CPUs are going down and we
    don't really need to care about cache management, so drop this hack.
    
    Note that the __cpu_disable() case for CPU hotplug is handled in the
    previous commit, and no synchronisation is needed there due to the use
    of stop_machine() which prevents hotplug from taking place while any CPU
    has disabled preemption (as r4k_on_each_cpu() does).
    
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Leonid Yegoshin <leonid.yegoshin@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/13796/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 826e99be6ab5189dbfb096389016ffb8d20a683e
Author: James Hogan <james.hogan@imgtec.com>
Date:   Wed Jul 13 14:12:45 2016 +0100

    MIPS: SMP: Update cpu_foreign_map on CPU disable
    
    When a CPU is disabled via CPU hotplug, cpu_foreign_map is not updated.
    This could result in cache management SMP calls being sent to offline
    CPUs instead of online siblings in the same core.
    
    Add a call to calculate_cpu_foreign_map() in the various MIPS cpu
    disable callbacks after set_cpu_online(). All cases are updated for
    consistency and to keep cpu_foreign_map strictly up to date, not just
    those which may support hardware multithreading.
    
    Fixes: cccf34e9411c ("MIPS: c-r4k: Fix cache flushing for MT cores")
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Kevin Cernekee <cernekee@gmail.com>
    Cc: Florian Fainelli <f.fainelli@gmail.com>
    Cc: Huacai Chen <chenhc@lemote.com>
    Cc: Hongliang Tao <taohl@lemote.com>
    Cc: Hua Yan <yanh@lemote.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/13799/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit cca08cd66ce6cc37812b6b36986ba7eaabd33e0b
Merge: 7e4dc77b2869 748c7201e622
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 13:59:34 2016 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - introduce and use task_rcu_dereference()/try_get_task_struct() to fix
       and generalize task_struct handling (Oleg Nesterov)
    
     - do various per entity load tracking (PELT) fixes and optimizations
       (Peter Zijlstra)
    
     - cputime virt-steal time accounting enhancements/fixes (Wanpeng Li)
    
     - introduce consolidated cputime output file cpuacct.usage_all and
       related refactorings (Zhao Lei)
    
     - ... plus misc fixes and enhancements
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/core: Panic on scheduling while atomic bugs if kernel.panic_on_warn is set
      sched/cpuacct: Introduce cpuacct.usage_all to show all CPU stats together
      sched/cpuacct: Use loop to consolidate code in cpuacct_stats_show()
      sched/cpuacct: Merge cpuacct_usage_index and cpuacct_stat_index enums
      sched/fair: Rework throttle_count sync
      sched/core: Fix sched_getaffinity() return value kerneldoc comment
      sched/fair: Reorder cgroup creation code
      sched/fair: Apply more PELT fixes
      sched/fair: Fix PELT integrity for new tasks
      sched/cgroup: Fix cpu_cgroup_fork() handling
      sched/fair: Fix PELT integrity for new groups
      sched/fair: Fix and optimize the fork() path
      sched/cputime: Add steal time support to full dynticks CPU time accounting
      sched/cputime: Fix prev steal time accouting during CPU hotplug
      KVM: Fix steal clock warp during guest CPU hotplug
      sched/debug: Always show 'nr_migrations'
      sched/fair: Use task_rcu_dereference()
      sched/api: Introduce task_rcu_dereference() and try_get_task_struct()
      sched/idle: Optimize the generic idle loop
      sched/fair: Fix the wrong throttled clock time for cfs_rq_clock_task()

commit 0d2808f338c7cb0ccf6b087dd7be0e4fa0c865e0
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Thu Jul 7 08:50:39 2016 +0100

    MIPS: smp-cps: Add support for CPU hotplug of MIPSr6 processors
    
    Introduce support for hotplug of Virtual Processors in MIPSr6 systems.
    The method is simpler than the VPE parallel from the now-deprecated MT
    ASE, it can now simply write the VP_STOP register with the mask of VPs
    to halt, and use the VP_RUNNING register to determine when the VP has
    halted.
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Reviewed-by: Paul Burton <paul.burton@imgtec.com>
    Cc: Matt Redfearn <matt.redfearn@imgtec.com>
    Cc: Qais Yousef <qais.yousef@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/13752/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 9736c6152ef6fbb688c05c75b250304787fc9ff7
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Thu Jul 7 08:50:38 2016 +0100

    MIPS: smp-cps: Allow booting of CPU other than VP0 within a core
    
    The boot_core function was hardcoded to always start VP0 when starting
    a core via the CPC. When hotplugging a CPU this may not be the desired
    behaviour.
    
    Make boot_core receive the VP ID to start running on the core, such that
    alternate VPs can be started via CPU hotplug.
    Also ensure that all other VPs within the core are stopped before
    bringing the core out of reset so that only the desired VP starts.
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Reviewed-by: Paul Burton <paul.burton@imgtec.com>
    Cc: Matt Redfearn <matt.redfearn@imgtec.com>
    Cc: Qais Yousef <qais.yousef@imgtec.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/13750/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit b7d9eb397b8764c1f1c53d504aa70f85ce0e212f
Author: John Allen <jallen@linux.vnet.ibm.com>
Date:   Thu Jul 7 10:03:44 2016 -0500

    powerpc/pseries: Add support for hotplug interrupt source
    
    Add handler for new hotplug interrupt. For memory and CPU hotplug events,
    we will add the hotplug errorlog to the hotplug workqueue. Since PCI
    hotplug is not currently supported in the kernel, PCI hotplug events are
    written to the rtas_log_bug and are handled by rtas_errd.
    
    Signed-off-by: John Allen <jallen@linux.vnet.ibm.com>
    Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 6b2c28471de550308784560206c3365e5179d42f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jul 13 17:17:00 2016 +0000

    x86/x2apic: Convert to CPU hotplug state machine
    
    Install the callbacks via the state machine and let the core invoke
    the callbacks on the already online CPUs.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathias Krause <minipli@googlemail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160713153337.736898691@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d83a4c116c4e723840bf9efc47c33ea40c70691b
Merge: f97d10454e4d d60585c5766e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 15 15:02:49 2016 +0900

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fix from Ingo Molnar:
     "Fix a CPU hotplug related corruption of the load average that got
      introduced in this merge window"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/core: Correct off by one bug in load migration calculation

commit 4b9bc86d5a999e344098303882d6395d39e36c13
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Apr 12 17:16:54 2016 +0200

    fcoe: convert to kworker
    
    The driver creates its own per-CPU threads which are updated based on
    CPU hotplug events. It is also possible to use kworkers and remove some
    of the kthread infrastrucure.
    
    The code checked ->thread to decide if there is an active per-CPU
    thread. By using the kworker infrastructure this is no longer
    possible (or required). The thread pointer is saved in `kthread' instead
    of `thread' so anything trying to use thread is caught by the
    compiler. Currently only the bnx2fc driver is using struct fcoe_percpu_s
    and the kthread member.
    
    After a CPU went offline, we may still enqueue items on the "offline"
    CPU. This isn't much of a problem. The work will be done on a random
    CPU. The allocated crc_eof_page page won't be cleaned up. It is probably
    expected that the CPU comes up at some point so it should not be a
    problem. The crc_eof_page memory is released of course once the module
    is removed.
    
    This patch was only compile-tested due to -ENODEV.
    
    Cc: Vasu Dev <vasu.dev@intel.com>
    Cc: "James E.J. Bottomley" <jejb@linux.vnet.ibm.com>
    Cc: "Martin K. Petersen" <martin.petersen@oracle.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: fcoe-devel@open-fcoe.org
    Cc: linux-scsi@vger.kernel.org
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Tested-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

commit ea00f4f4f00cc2bc3b63ad512a4e6df3b20832b9
Author: Lianwei Wang <lianwei.wang@gmail.com>
Date:   Sun Jun 19 23:52:27 2016 -0700

    PM / sleep: make PM notifiers called symmetrically
    
    This makes pm notifier PREPARE/POST symmetrical: if PREPARE
    fails, we will only undo what ever happened on PREPARE.
    
    It fixes the unbalanced CPU hotplug enable in CPU PM notifier.
    
    Signed-off-by: Lianwei Wang <lianwei.wang@gmail.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit feb245e304f343cf5e4f9123db36354144dce8a4
Author: Tejun Heo <htejun@gmail.com>
Date:   Thu Jun 16 15:35:04 2016 -0400

    sched/core: Allow kthreads to fall back to online && !active cpus
    
    During CPU hotplug, CPU_ONLINE callbacks are run while the CPU is
    online but not active.  A CPU_ONLINE callback may create or bind a
    kthread so that its cpus_allowed mask only allows the CPU which is
    being brought online.  The kthread may start executing before the CPU
    is made active and can end up in select_fallback_rq().
    
    In such cases, the expected behavior is selecting the CPU which is
    coming online; however, because select_fallback_rq() only chooses from
    active CPUs, it determines that the task doesn't have any viable CPU
    in its allowed mask and ends up overriding it to cpu_possible_mask.
    
    CPU_ONLINE callbacks should be able to put kthreads on the CPU which
    is coming online.  Update select_fallback_rq() so that it follows
    cpu_online() rather than cpu_active() for kthreads.
    
    Reported-by: Gautham R Shenoy <ego@linux.vnet.ibm.com>
    Tested-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Abdul Haleem <abdhalee@linux.vnet.ibm.com>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kernel-team@fb.com
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/20160616193504.GB3262@mtj.duckdns.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 3d89e5478bf550a50c99e93adf659369798263b0
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Mon Jun 13 18:32:45 2016 +0800

    sched/cputime: Fix prev steal time accouting during CPU hotplug
    
    Commit:
    
      e9532e69b8d1 ("sched/cputime: Fix steal time accounting vs. CPU hotplug")
    
    ... set rq->prev_* to 0 after a CPU hotplug comes back, in order to
    fix the case where (after CPU hotplug) steal time is smaller than
    rq->prev_steal_time.
    
    However, this should never happen. Steal time was only smaller because of the
    KVM-specific bug fixed by the previous patch.  Worse, the previous patch
    triggers a bug on CPU hot-unplug/plug operation: because
    rq->prev_steal_time is cleared, all of the CPU's past steal time will be
    accounted again on hot-plug.
    
    Since the root cause has been fixed, we can just revert commit e9532e69b8d1.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 'commit e9532e69b8d1 ("sched/cputime: Fix steal time accounting vs. CPU hotplug")'
    Link: http://lkml.kernel.org/r/1465813966-3116-3-git-send-email-wanpeng.li@hotmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 2348140d58f4f4245e9635ea8f1a77e940a4d877
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Mon Jun 13 18:32:44 2016 +0800

    KVM: Fix steal clock warp during guest CPU hotplug
    
    Sometimes, after CPU hotplug you can observe a spike in stolen time
    (100%) followed by the CPU being marked as 100% idle when it's actually
    busy with a CPU hog task.  The trace looks like the following:
    
     cpuhp/1-12    [001] d.h1   167.461657: account_process_tick: steal = 1291385514, prev_steal_time = 0
     cpuhp/1-12    [001] d.h1   167.461659: account_process_tick: steal_jiffies = 1291
      <idle>-0     [001] d.h1   167.462663: account_process_tick: steal = 18732255, prev_steal_time = 1291000000
      <idle>-0     [001] d.h1   167.462664: account_process_tick: steal_jiffies = 18446744072437
    
    The sudden decrease of "steal" causes steal_jiffies to underflow.
    The root cause is kvm_steal_time being reset to 0 after hot-plugging
    back in a CPU.  Instead, the preexisting value can be used, which is
    what the core scheduler code expects.
    
    John Stultz also reported a similar issue after guest S3.
    
    Suggested-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1465813966-3116-2-git-send-email-wanpeng.li@hotmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 0579a12791e483fcaaad76748fb163ec855102a4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 4 15:59:42 2016 +0100

    sched/cputime: Fix steal time accounting vs. CPU hotplug
    
    commit e9532e69b8d1d1284e8ecf8d2586de34aec61244 upstream.
    
    On CPU hotplug the steal time accounting can keep a stale rq->prev_steal_time
    value over CPU down and up. So after the CPU comes up again the delta
    calculation in steal_account_process_tick() wreckages itself due to the
    unsigned math:
    
             u64 steal = paravirt_steal_clock(smp_processor_id());
    
             steal -= this_rq()->prev_steal_time;
    
    So if steal is smaller than rq->prev_steal_time we end up with an insane large
    value which then gets added to rq->prev_steal_time, resulting in a permanent
    wreckage of the accounting. As a consequence the per CPU stats in /proc/stat
    become stale.
    
    Nice trick to tell the world how idle the system is (100%) while the CPU is
    100% busy running tasks. Though we prefer realistic numbers.
    
    None of the accounting values which use a previous value to account for
    fractions is reset at CPU hotplug time. update_rq_clock_task() has a sanity
    check for prev_irq_time and prev_steal_time_rq, but that sanity check solely
    deals with clock warps and limits the /proc/stat visible wreckage. The
    prev_time values are still wrong.
    
    Solution is simple: Reset rq->prev_*_time when the CPU is plugged in again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: <stable@vger.kernel.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Fixes: commit 095c0aa83e52 "sched: adjust scheduler cpu power for stolen time"
    Fixes: commit aa483808516c "sched: Remove irq time from available CPU power"
    Fixes: commit e6e6685accfa "KVM guest: Steal time accounting"
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1603041539490.3686@nanos
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Willy Tarreau <w@1wt.eu>

commit a53eaff8c1192bb5bdfda5deb484bc8f415c5dfd
Author: NeilBrown <neilb@suse.com>
Date:   Fri May 20 16:58:53 2016 -0700

    MM: increase safety margin provided by PF_LESS_THROTTLE
    
    When nfsd is exporting a filesystem over NFS which is then NFS-mounted
    on the local machine there is a risk of deadlock.  This happens when
    there are lots of dirty pages in the NFS filesystem and they cause NFSD
    to be throttled, either in throttle_vm_writeout() or in
    balance_dirty_pages().
    
    To avoid this problem the PF_LESS_THROTTLE flag is set for NFSD threads
    and it provides a 25% increase to the limits that affect NFSD.  Any
    process writing to an NFS filesystem will be throttled well before the
    number of dirty NFS pages reaches the limit imposed on NFSD, so NFSD
    will not deadlock on pages that it needs to write out.  At least it
    shouldn't.
    
    All processes are allowed a small excess margin to avoid performing too
    many calculations: ratelimit_pages.
    
    ratelimit_pages is set so that if a thread on every CPU uses the entire
    margin, the total will only go 3% over the limit, and this is much less
    than the 25% bonus that PF_LESS_THROTTLE provides, so this margin
    shouldn't be a problem.  But it is.
    
    The "total memory" that these 3% and 25% are calculated against are not
    really total memory but are "global_dirtyable_memory()" which doesn't
    include anonymous memory, just free memory and page-cache memory.
    
    The "ratelimit_pages" number is based on whatever the
    global_dirtyable_memory was on the last CPU hot-plug, which might not be
    what you expect, but is probably close to the total freeable memory.
    
    The throttle threshold uses the global_dirtable_memory at the moment
    when the throttling happens, which could be much less than at the last
    CPU hotplug.  So if lots of anonymous memory has been allocated, thus
    pushing out lots of page-cache pages, then NFSD might end up being
    throttled due to dirty NFS pages because the "25%" bonus it gets is
    calculated against a rather small amount of dirtyable memory, while the
    "3%" margin that other processes are allowed to dirty without penalty is
    calculated against a much larger number.
    
    To remove this possibility of deadlock we need to make sure that the
    margin granted to PF_LESS_THROTTLE exceeds that rate-limit margin.
    Simply adding ratelimit_pages isn't enough as that should be multiplied
    by the number of cpus.
    
    So add "global_wb_domain.dirty_limit / 32" as that more accurately
    reflects the current total over-shoot margin.  This ensures that the
    number of dirty NFS pages never gets so high that nfsd will be throttled
    waiting for them to be written.
    
    Link: http://lkml.kernel.org/r/87futgowwv.fsf@notabene.neil.brown.name
    Signed-off-by: NeilBrown <neilb@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 825a3b2605c3aa193e0075d0f9c72e33c17ab16a
Merge: cf6ed9a6682d ef0491ea17f8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 14:47:16 2016 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - massive CPU hotplug rework (Thomas Gleixner)
    
     - improve migration fairness (Peter Zijlstra)
    
     - CPU load calculation updates/cleanups (Yuyang Du)
    
     - cpufreq updates (Steve Muckle)
    
     - nohz optimizations (Frederic Weisbecker)
    
     - switch_mm() micro-optimization on x86 (Andy Lutomirski)
    
     - ... lots of other enhancements, fixes and cleanups.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (66 commits)
      ARM: Hide finish_arch_post_lock_switch() from modules
      sched/core: Provide a tsk_nr_cpus_allowed() helper
      sched/core: Use tsk_cpus_allowed() instead of accessing ->cpus_allowed
      sched/loadavg: Fix loadavg artifacts on fully idle and on fully loaded systems
      sched/fair: Correct unit of load_above_capacity
      sched/fair: Clean up scale confusion
      sched/nohz: Fix affine unpinned timers mess
      sched/fair: Fix fairness issue on migration
      sched/core: Kill sched_class::task_waking to clean up the migration logic
      sched/fair: Prepare to fix fairness problems on migration
      sched/fair: Move record_wakee()
      sched/core: Fix comment typo in wake_q_add()
      sched/core: Remove unused variable
      sched: Make hrtick_notifier an explicit call
      sched/fair: Make ilb_notifier an explicit call
      sched/hotplug: Make activate() the last hotplug step
      sched/hotplug: Move migration CPU_DYING to sched_cpu_dying()
      sched/migration: Move CPU_ONLINE into scheduler state
      sched/migration: Move calc_load_migrate() into CPU_DYING
      sched/migration: Move prepare transition to SCHED_STARTING state
      ...

commit da92223908f309d5ba63ce7dd568dfb6a7e5d7d2
Merge: 65643e3abe71 f7c17d26f43d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 13 16:16:51 2016 -0700

    Merge branch 'for-4.6-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    Pull workqueue fix from Tejun Heo:
     "CPU hotplug callbacks can invoke DOWN_FAILED w/o preceding
      DOWN_PREPARE which can trigger a WARN_ON() in workqueue.
    
      The bug has been there for a very long time.  It only triggers if CPU
      down fails at a specific point and I don't think it has adverse
      effects other than the warning messages.  The fix is very low impact"
    
    * 'for-4.6-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq:
      workqueue: fix rebind bound workers warning

commit 21b30c00f3067001519eda166675c1958b163c91
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Wed Feb 3 18:14:50 2016 -0800

    MIPS: BMIPS: Add Whirlwind (BMIPS5200) initialization code
    
    Import bmips_5xxx_init.S from the stblinux-3.3 tree, and to make sure that this
    would work nicely with a BMIPS multiplatform kernel (with BMIPS330, BMIPS43XX
    and BMIPS5000 enabled), update soft_reset to check for the BMIPS5200 processor
    id (PRID_IMP_BMIPS5200) and execute bmips_5xxx_init for these processors to
    bring them online.
    
    Tested on 7425, 7429 and 7435 with CPU hotplug. 7435 SMP still needs some
    additional changes in the L1 interrupt area to work properly with interrupt
    affinity.
    
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Cc: linux-mips@linux-mips.org
    Cc: john@phrozen.org
    Cc: cernekee@gmail.com
    Cc: jon.fraser@broadcom.com
    Cc: jaedon.shin@gmail.com
    Cc: dragan.stancevic@gmail.com
    Cc: jogo@openwrt.org
    Patchwork: https://patchwork.linux-mips.org/patch/12377/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 7aa5e217b800f21d381a02cd439f341fc752663d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 4 15:59:42 2016 +0100

    sched/cputime: Fix steal time accounting vs. CPU hotplug
    
    commit e9532e69b8d1d1284e8ecf8d2586de34aec61244 upstream.
    
    On CPU hotplug the steal time accounting can keep a stale rq->prev_steal_time
    value over CPU down and up. So after the CPU comes up again the delta
    calculation in steal_account_process_tick() wreckages itself due to the
    unsigned math:
    
             u64 steal = paravirt_steal_clock(smp_processor_id());
    
             steal -= this_rq()->prev_steal_time;
    
    So if steal is smaller than rq->prev_steal_time we end up with an insane large
    value which then gets added to rq->prev_steal_time, resulting in a permanent
    wreckage of the accounting. As a consequence the per CPU stats in /proc/stat
    become stale.
    
    Nice trick to tell the world how idle the system is (100%) while the CPU is
    100% busy running tasks. Though we prefer realistic numbers.
    
    None of the accounting values which use a previous value to account for
    fractions is reset at CPU hotplug time. update_rq_clock_task() has a sanity
    check for prev_irq_time and prev_steal_time_rq, but that sanity check solely
    deals with clock warps and limits the /proc/stat visible wreckage. The
    prev_time values are still wrong.
    
    Solution is simple: Reset rq->prev_*_time when the CPU is plugged in again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Fixes: commit 095c0aa83e52 "sched: adjust scheduler cpu power for stolen time"
    Fixes: commit aa483808516c "sched: Remove irq time from available CPU power"
    Fixes: commit e6e6685accfa "KVM guest: Steal time accounting"
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1603041539490.3686@nanos
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit a41e182b6aa057417054cb61bbae7ec891397484
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 4 15:59:42 2016 +0100

    sched/cputime: Fix steal time accounting vs. CPU hotplug
    
    commit e9532e69b8d1d1284e8ecf8d2586de34aec61244 upstream.
    
    On CPU hotplug the steal time accounting can keep a stale rq->prev_steal_time
    value over CPU down and up. So after the CPU comes up again the delta
    calculation in steal_account_process_tick() wreckages itself due to the
    unsigned math:
    
             u64 steal = paravirt_steal_clock(smp_processor_id());
    
             steal -= this_rq()->prev_steal_time;
    
    So if steal is smaller than rq->prev_steal_time we end up with an insane large
    value which then gets added to rq->prev_steal_time, resulting in a permanent
    wreckage of the accounting. As a consequence the per CPU stats in /proc/stat
    become stale.
    
    Nice trick to tell the world how idle the system is (100%) while the CPU is
    100% busy running tasks. Though we prefer realistic numbers.
    
    None of the accounting values which use a previous value to account for
    fractions is reset at CPU hotplug time. update_rq_clock_task() has a sanity
    check for prev_irq_time and prev_steal_time_rq, but that sanity check solely
    deals with clock warps and limits the /proc/stat visible wreckage. The
    prev_time values are still wrong.
    
    Solution is simple: Reset rq->prev_*_time when the CPU is plugged in again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Fixes: commit 095c0aa83e52 "sched: adjust scheduler cpu power for stolen time"
    Fixes: commit aa483808516c "sched: Remove irq time from available CPU power"
    Fixes: commit e6e6685accfa "KVM guest: Steal time accounting"
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1603041539490.3686@nanos
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    [bwh: Backported to 3.2: adjust filenames]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 82b23cb94b8cee25781c209d9b1df3b144c8bb5f
Merge: 0e11d256512c a19cad6d6682 3b9d6da67e11 16eeed7e5558
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Apr 23 11:45:52 2016 -0700

    Merge branches 'perf-urgent-for-linus', 'smp-urgent-for-linus' and 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf, cpu hotplug and timer fixes from Ingo Molnar:
     "perf:
       - A single tooling fix for a user-triggerable segfault.
    
      CPU hotplug:
       - Fix a CPU hotplug corner case regression, introduced by the recent
         hotplug rework
    
      timers:
       - Fix a boot hang in the ARM based Tango SoC clocksource driver"
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf intel-pt: Fix segfault tracing transactions
    
    * 'smp-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cpu/hotplug: Fix rollback during error-out in __cpu_disable()
    
    * 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      clocksource/drivers/tango-xtal: Fix boot hang due to incorrect test

commit 9120abdcdb7fd3a51d3135ff37c9dad358dde9ee
Author: Keerthy <j-keerthy@ti.com>
Date:   Wed Apr 13 08:54:52 2016 -0700

    ARM: configs: keystone: Add CPU Hotplug related options
    
    Add the config options needed to get CPU hotplug functional.
    
    Signed-off-by: Keerthy <j-keerthy@ti.com>
    Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>

commit fac411b9a9a663bb6dc41cd82780cb490a064b8d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 4 15:59:42 2016 +0100

    sched/cputime: Fix steal time accounting vs. CPU hotplug
    
    commit e9532e69b8d1d1284e8ecf8d2586de34aec61244 upstream.
    
    On CPU hotplug the steal time accounting can keep a stale rq->prev_steal_time
    value over CPU down and up. So after the CPU comes up again the delta
    calculation in steal_account_process_tick() wreckages itself due to the
    unsigned math:
    
             u64 steal = paravirt_steal_clock(smp_processor_id());
    
             steal -= this_rq()->prev_steal_time;
    
    So if steal is smaller than rq->prev_steal_time we end up with an insane large
    value which then gets added to rq->prev_steal_time, resulting in a permanent
    wreckage of the accounting. As a consequence the per CPU stats in /proc/stat
    become stale.
    
    Nice trick to tell the world how idle the system is (100%) while the CPU is
    100% busy running tasks. Though we prefer realistic numbers.
    
    None of the accounting values which use a previous value to account for
    fractions is reset at CPU hotplug time. update_rq_clock_task() has a sanity
    check for prev_irq_time and prev_steal_time_rq, but that sanity check solely
    deals with clock warps and limits the /proc/stat visible wreckage. The
    prev_time values are still wrong.
    
    Solution is simple: Reset rq->prev_*_time when the CPU is plugged in again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Fixes: commit 095c0aa83e52 "sched: adjust scheduler cpu power for stolen time"
    Fixes: commit aa483808516c "sched: Remove irq time from available CPU power"
    Fixes: commit e6e6685accfa "KVM guest: Steal time accounting"
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1603041539490.3686@nanos
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2a8225ef46968444fb1c4c632ec28e4cc2be633f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 4 15:59:42 2016 +0100

    sched/cputime: Fix steal time accounting vs. CPU hotplug
    
    commit e9532e69b8d1d1284e8ecf8d2586de34aec61244 upstream.
    
    On CPU hotplug the steal time accounting can keep a stale rq->prev_steal_time
    value over CPU down and up. So after the CPU comes up again the delta
    calculation in steal_account_process_tick() wreckages itself due to the
    unsigned math:
    
             u64 steal = paravirt_steal_clock(smp_processor_id());
    
             steal -= this_rq()->prev_steal_time;
    
    So if steal is smaller than rq->prev_steal_time we end up with an insane large
    value which then gets added to rq->prev_steal_time, resulting in a permanent
    wreckage of the accounting. As a consequence the per CPU stats in /proc/stat
    become stale.
    
    Nice trick to tell the world how idle the system is (100%) while the CPU is
    100% busy running tasks. Though we prefer realistic numbers.
    
    None of the accounting values which use a previous value to account for
    fractions is reset at CPU hotplug time. update_rq_clock_task() has a sanity
    check for prev_irq_time and prev_steal_time_rq, but that sanity check solely
    deals with clock warps and limits the /proc/stat visible wreckage. The
    prev_time values are still wrong.
    
    Solution is simple: Reset rq->prev_*_time when the CPU is plugged in again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Fixes: commit 095c0aa83e52 "sched: adjust scheduler cpu power for stolen time"
    Fixes: commit aa483808516c "sched: Remove irq time from available CPU power"
    Fixes: commit e6e6685accfa "KVM guest: Steal time accounting"
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1603041539490.3686@nanos
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a9a3cef503366cfaf0ac6c98b27a80f9164c186a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 4 15:59:42 2016 +0100

    sched/cputime: Fix steal time accounting vs. CPU hotplug
    
    commit e9532e69b8d1d1284e8ecf8d2586de34aec61244 upstream.
    
    On CPU hotplug the steal time accounting can keep a stale rq->prev_steal_time
    value over CPU down and up. So after the CPU comes up again the delta
    calculation in steal_account_process_tick() wreckages itself due to the
    unsigned math:
    
             u64 steal = paravirt_steal_clock(smp_processor_id());
    
             steal -= this_rq()->prev_steal_time;
    
    So if steal is smaller than rq->prev_steal_time we end up with an insane large
    value which then gets added to rq->prev_steal_time, resulting in a permanent
    wreckage of the accounting. As a consequence the per CPU stats in /proc/stat
    become stale.
    
    Nice trick to tell the world how idle the system is (100%) while the CPU is
    100% busy running tasks. Though we prefer realistic numbers.
    
    None of the accounting values which use a previous value to account for
    fractions is reset at CPU hotplug time. update_rq_clock_task() has a sanity
    check for prev_irq_time and prev_steal_time_rq, but that sanity check solely
    deals with clock warps and limits the /proc/stat visible wreckage. The
    prev_time values are still wrong.
    
    Solution is simple: Reset rq->prev_*_time when the CPU is plugged in again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Fixes: commit 095c0aa83e52 "sched: adjust scheduler cpu power for stolen time"
    Fixes: commit aa483808516c "sched: Remove irq time from available CPU power"
    Fixes: commit e6e6685accfa "KVM guest: Steal time accounting"
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1603041539490.3686@nanos
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 6ef98c94504eb928b37fb084f45f3da39c9d6f68
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 4 15:59:42 2016 +0100

    sched/cputime: Fix steal time accounting vs. CPU hotplug
    
    commit e9532e69b8d1d1284e8ecf8d2586de34aec61244 upstream.
    
    On CPU hotplug the steal time accounting can keep a stale rq->prev_steal_time
    value over CPU down and up. So after the CPU comes up again the delta
    calculation in steal_account_process_tick() wreckages itself due to the
    unsigned math:
    
             u64 steal = paravirt_steal_clock(smp_processor_id());
    
             steal -= this_rq()->prev_steal_time;
    
    So if steal is smaller than rq->prev_steal_time we end up with an insane large
    value which then gets added to rq->prev_steal_time, resulting in a permanent
    wreckage of the accounting. As a consequence the per CPU stats in /proc/stat
    become stale.
    
    Nice trick to tell the world how idle the system is (100%) while the CPU is
    100% busy running tasks. Though we prefer realistic numbers.
    
    None of the accounting values which use a previous value to account for
    fractions is reset at CPU hotplug time. update_rq_clock_task() has a sanity
    check for prev_irq_time and prev_steal_time_rq, but that sanity check solely
    deals with clock warps and limits the /proc/stat visible wreckage. The
    prev_time values are still wrong.
    
    Solution is simple: Reset rq->prev_*_time when the CPU is plugged in again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Fixes: commit 095c0aa83e52 "sched: adjust scheduler cpu power for stolen time"
    Fixes: commit aa483808516c "sched: Remove irq time from available CPU power"
    Fixes: commit e6e6685accfa "KVM guest: Steal time accounting"
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1603041539490.3686@nanos
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit 06a71a24bae57a07afee9cda6b00495347d8a448
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Mon Apr 4 14:46:51 2016 +0100

    arm64: KVM: unregister notifiers in hyp mode teardown path
    
    Commit 1e947bad0b63 ("arm64: KVM: Skip HYP setup when already running
    in HYP") re-organized the hyp init code and ended up leaving the CPU
    hotplug and PM notifier even if hyp mode initialization fails.
    
    Since KVM is not yet supported with ACPI, the above mentioned commit
    breaks CPU hotplug in ACPI boot.
    
    This patch fixes teardown_hyp_mode to properly unregister both CPU
    hotplug and PM notifiers in the teardown path.
    
    Fixes: 1e947bad0b63 ("arm64: KVM: Skip HYP setup when already running in HYP")
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

commit 93e2aeaca520743d66ec66b757db3a3e27936e91
Merge: e865f4965ff6 101ecde56696
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 4 16:38:36 2016 -0700

    Merge tag 'for-linus-4.6-rc2-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen fixes from David Vrabel:
     "Regression and bug fixes for 4.6-rc2:
    
       - safely migrate event channels between CPUs
       - fix CPU hotplug
       - maintainer changes"
    
    * tag 'for-linus-4.6-rc2-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      MAINTAINERS: xen: Konrad to step down and Juergen to pick up
      xen/events: Mask a moving irq
      Xen on ARM and ARM64: update MAINTAINERS info
      xen/x86: Call cpu_startup_entry(CPUHP_AP_ONLINE_IDLE) from xen_play_dead()
      xen/apic: Provide Xen-specific version of cpu_present_to_apicid APIC op

commit 49de0493e5f67a8023fa6fa5c89097c1f77de74e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Mar 20 18:59:02 2016 +0000

    x86/perf/intel/cstate: Make cstate hotplug handling actually work
    
    The current implementation aside of being an incomprehensible mess is broken.
    
      # cat /sys/bus/event_source/devices/cstate_core/cpumask
      0-17
    
    That's on a quad socket machine with 72 physical cores! Qualitee stuff.
    
    So it's not a surprise that event migration in case of CPU hotplug does not
    work either.
    
      # perf stat -e cstate_core/c6-residency/ -C 1 sleep 60 &
      # echo 0 >/sys/devices/system/cpu/cpu1/online
    
    Tracing cstate_pmu_event_update gives me:
    
     [001] cstate_pmu_event_update <-event_sched_out
    
    After the fix it properly moves the event:
    
     [001] cstate_pmu_event_update <-event_sched_out
     [073] cstate_pmu_event_update <-__perf_event_read
     [073] cstate_pmu_event_update <-event_sched_out
    
    The migration of pkg events does not work either. Not that I'm surprised.
    
    I really could not be bothered to decode that loop mess and simply replaced it
    by querying the proper cpumasks which give us the answer in a comprehensible
    way.
    
    This also requires to direct the event to the current active reader CPU in
    cstate_pmu_event_init() otherwise the hotplug logic can't work.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ Added event->cpu < 0 test to not explode]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/20160320185623.422519970@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 477a64e62f2816229d755553dd575a6e05539817
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Apr 12 08:06:55 2015 -0700

    cpu: Defer smpboot kthread unparking until CPU known to scheduler
    
    commit 00df35f991914db6b8bde8cf09808e19a9cffc3d upstream.
    
    Currently, smpboot_unpark_threads() is invoked before the incoming CPU
    has been added to the scheduler's runqueue structures.  This might
    potentially cause the unparked kthread to run on the wrong CPU, since the
    correct CPU isn't fully set up yet.
    
    That causes a sporadic, hard to debug boot crash triggering on some
    systems, reported by Borislav Petkov, and bisected down to:
    
      2a442c9c6453 ("x86: Use common outgoing-CPU-notification code")
    
    This patch places smpboot_unpark_threads() in a CPU hotplug
    notifier with priority set so that these kthreads are unparked just after
    the CPU has been added to the runqueues.
    
    Reported-and-tested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Cc: Owen Hofmann <osh@google.com>
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit be53f58fa0fcd97c62a84f2eb98cff528f8b2443
Merge: 19d6f04cd374 73e6aafd9ea8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 09:42:50 2016 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "Misc fixes: a cgroup fix, a fair-scheduler migration accounting fix, a
      cputime fix and two cpuacct cleanups"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/cpuacct: Simplify the cpuacct code
      sched/cpuacct: Rename parameter in cpuusage_write() for readability
      sched/fair: Add comments to explain select_idle_sibling()
      sched/fair: Fix fairness issue on migration
      sched/cgroup: Fix/cleanup cgroup teardown/init
      sched/cputime: Fix steal time accounting vs. CPU hotplug

commit 7a25d91214cb22e642b9ed6e4434bfaf74adad28
Author: Scott Wood <oss@buserror.net>
Date:   Tue Mar 15 01:47:38 2016 -0500

    powerpc/book3e-64: Use hardcoded mttmr opcode
    
    This preserves the ability to build using older binutils (reportedly <=
    2.22).
    
    Fixes: 6becef7ea04a ("powerpc/mpc85xx: Add CPU hotplug support for E6500")
    Signed-off-by: Scott Wood <oss@buserror.net>
    Cc: chenhui.zhao@freescale.com
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit e1b77c92981a522223bd1ac118fdcade6b7ad086
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Mar 9 14:08:18 2016 -0800

    sched/kasan: remove stale KASAN poison after hotplug
    
    Functions which the compiler has instrumented for KASAN place poison on
    the stack shadow upon entry and remove this poision prior to returning.
    
    In the case of CPU hotplug, CPUs exit the kernel a number of levels deep
    in C code.  Any instrumented functions on this critical path will leave
    portions of the stack shadow poisoned.
    
    When a CPU is subsequently brought back into the kernel via a different
    path, depending on stackframe, layout calls to instrumented functions
    may hit this stale poison, resulting in (spurious) KASAN splats to the
    console.
    
    To avoid this, clear any stale poison from the idle thread for a CPU
    prior to bringing a CPU online.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 323ee64aa175a67fbbe744e809777d17e6fb42d7
Author: Jacob Pan <jacob.jun.pan@linux.intel.com>
Date:   Wed Feb 24 13:31:38 2016 -0800

    powercap/rapl: track lead cpu per package
    
    RAPL driver operates on MSRs that are under package/socket
    scope instead of core scope. However, the current code does not
    keep track of which CPUs are available on each package for MSR
    access. Therefore it has to search for an active CPU on a given
    package each time.
    
    This patch optimizes the package level operations by tracking a
    per package lead CPU during initialization and CPU hotplug. The
    runtime search for active CPU is avoided.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit e9532e69b8d1d1284e8ecf8d2586de34aec61244
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 4 15:59:42 2016 +0100

    sched/cputime: Fix steal time accounting vs. CPU hotplug
    
    On CPU hotplug the steal time accounting can keep a stale rq->prev_steal_time
    value over CPU down and up. So after the CPU comes up again the delta
    calculation in steal_account_process_tick() wreckages itself due to the
    unsigned math:
    
             u64 steal = paravirt_steal_clock(smp_processor_id());
    
             steal -= this_rq()->prev_steal_time;
    
    So if steal is smaller than rq->prev_steal_time we end up with an insane large
    value which then gets added to rq->prev_steal_time, resulting in a permanent
    wreckage of the accounting. As a consequence the per CPU stats in /proc/stat
    become stale.
    
    Nice trick to tell the world how idle the system is (100%) while the CPU is
    100% busy running tasks. Though we prefer realistic numbers.
    
    None of the accounting values which use a previous value to account for
    fractions is reset at CPU hotplug time. update_rq_clock_task() has a sanity
    check for prev_irq_time and prev_steal_time_rq, but that sanity check solely
    deals with clock warps and limits the /proc/stat visible wreckage. The
    prev_time values are still wrong.
    
    Solution is simple: Reset rq->prev_*_time when the CPU is plugged in again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: <stable@vger.kernel.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Fixes: commit 095c0aa83e52 "sched: adjust scheduler cpu power for stolen time"
    Fixes: commit aa483808516c "sched: Remove irq time from available CPU power"
    Fixes: commit e6e6685accfa "KVM guest: Steal time accounting"
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1603041539490.3686@nanos
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6becef7ea04a695f64299238fe13d41e41607469
Author: chenhui zhao <chenhui.zhao@freescale.com>
Date:   Fri Nov 20 17:14:02 2015 +0800

    powerpc/mpc85xx: Add CPU hotplug support for E6500
    
    Support Freescale E6500 core-based platforms, like t4240.
    Support disabling/enabling individual CPU thread dynamically.
    
    Signed-off-by: Chenhui Zhao <chenhui.zhao@freescale.com>

commit fff4dc84e72419196623f118312f571a2e057196
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 19 12:18:41 2016 -0500

    cpuset: make mm migration asynchronous
    
    commit e93ad19d05648397ef3bcb838d26aec06c245dc0 upstream.
    
    If "cpuset.memory_migrate" is set, when a process is moved from one
    cpuset to another with a different memory node mask, pages in used by
    the process are migrated to the new set of nodes.  This was performed
    synchronously in the ->attach() callback, which is synchronized
    against process management.  Recently, the synchronization was changed
    from per-process rwsem to global percpu rwsem for simplicity and
    optimization.
    
    Combined with the synchronous mm migration, this led to deadlocks
    because mm migration could schedule a work item which may in turn try
    to create a new worker blocking on the process management lock held
    from cgroup process migration path.
    
    This heavy an operation shouldn't be performed synchronously from that
    deep inside cgroup migration in the first place.  This patch punts the
    actual migration to an ordered workqueue and updates cgroup process
    migration and cpuset config update paths to flush the workqueue after
    all locks are released.  This way, the operations still seem
    synchronous to userland without entangling mm migration with process
    management synchronization.  CPU hotplug can also invoke mm migration
    but there's no reason for it to wait for mm migrations and thus
    doesn't synchronize against their completions.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-and-tested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 06b74c658c595e4ef9fd5746f927a1cb6c3a59d5
Merge: e6a1c1e9ddcc 059fcd8cd166
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 20 09:30:42 2016 -0800

    Merge branch 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf fixes from Ingo Molnar:
     "A handful of CPU hotplug related fixes"
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf/core: Plug potential memory leak in CPU_UP_PREPARE
      perf/core: Remove the bogus and dangerous CPU_DOWN_FAILED hotplug state
      perf/core: Remove bogus UP_CANCELED hotplug state
      perf/x86/amd/uncore: Plug reference leak

commit 77da2b229d4a5c98284ca01dfee31314df153119
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Jan 8 11:18:29 2016 +0100

    vmstat: allocate vmstat_wq before it is used
    
    commit 751e5f5c753e8d447bcf89f9e96b9616ac081628 upstream.
    
    kernel test robot has reported the following crash:
    
      BUG: unable to handle kernel NULL pointer dereference at 00000100
      IP: [<c1074df6>] __queue_work+0x26/0x390
      *pdpt = 0000000000000000 *pde = f000ff53f000ff53 *pde = f000ff53f000ff53
      Oops: 0000 [#1] PREEMPT PREEMPT SMP SMP
      CPU: 0 PID: 24 Comm: kworker/0:1 Not tainted 4.4.0-rc4-00139-g373ccbe #1
      Workqueue: events vmstat_shepherd
      task: cb684600 ti: cb7ba000 task.ti: cb7ba000
      EIP: 0060:[<c1074df6>] EFLAGS: 00010046 CPU: 0
      EIP is at __queue_work+0x26/0x390
      EAX: 00000046 EBX: cbb37800 ECX: cbb37800 EDX: 00000000
      ESI: 00000000 EDI: 00000000 EBP: cb7bbe68 ESP: cb7bbe38
       DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
      CR0: 8005003b CR2: 00000100 CR3: 01fd5000 CR4: 000006b0
      Stack:
      Call Trace:
        __queue_delayed_work+0xa1/0x160
        queue_delayed_work_on+0x36/0x60
        vmstat_shepherd+0xad/0xf0
        process_one_work+0x1aa/0x4c0
        worker_thread+0x41/0x440
        kthread+0xb0/0xd0
        ret_from_kernel_thread+0x21/0x40
    
    The reason is that start_shepherd_timer schedules the shepherd work item
    which uses vmstat_wq (vmstat_shepherd) before setup_vmstat allocates
    that workqueue so if the further initialization takes more than HZ we
    might end up scheduling on a NULL vmstat_wq.  This is really unlikely
    but not impossible.
    
    Fixes: 373ccbe59270 ("mm, vmstat: allow WQ concurrency to discover memory reclaim doesn't make any progress")
    Reported-by: kernel test robot <ying.huang@linux.intel.com>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Tested-by: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ben Hutchings <ben@decadent.org.uk>
    [ luis: backported to 3.16: based on Ben's backport to 3.2:
      - as with 3.2, there's a similar race but with the CPU hotplug code ]
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit db0dbd57d59ad02c8343c69e8c73e749c0515ec3
Author: Huacai Chen <chenhc@lemote.com>
Date:   Thu Jan 21 21:09:51 2016 +0800

    MIPS: sync-r4k: reduce skew while synchronization
    
    While synchronization, count register will go backwards for the master.
    If synchronise_count_master() runs before synchronise_count_slave(),
    skew becomes even more. The skew is very harmful for CPU hotplug (CPU0
    do synchronization with CPU1, then CPU0 do synchronization with CPU2
    and CPU0's count goes backwards, so it will be out of sync with CPU1).
    
    After the commit cf9bfe55f24973a8f40e2 (MIPS: Synchronize MIPS count one
    CPU at a time), we needn't evaluate count_reference at the beginning of
    synchronise_count_master() any more. Thus, we evaluate the initcount (It
    seems like count_reference is redundant) in the 2nd loop. Since we write
    the count register in the last loop, we don't need additional barriers
    (the existing memory barriers are enough).
    
    Moreover, I think we loop 3 times is enough to get a primed instruction
    cache, this can also get less skew than looping 5 times.
    
    Comments are also updated in this patch.
    
    Signed-off-by: Huacai Chen <chenhc@lemote.com>
    Cc: Aurelien Jarno <aurelien@aurel32.net>
    Cc: Steven J. Hill <Steven.Hill@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Cc: Fuxin Zhang <zhangfx@lemote.com>
    Cc: Zhangjin Wu <wuzhangjin@gmail.com>
    Patchwork: https://patchwork.linux-mips.org/patch/12163/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 065bc97edc4bc250ec4d6dcc716c9a3e1e1c602c
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Jan 8 11:18:29 2016 +0100

    vmstat: allocate vmstat_wq before it is used
    
    commit 751e5f5c753e8d447bcf89f9e96b9616ac081628 upstream.
    
    kernel test robot has reported the following crash:
    
      BUG: unable to handle kernel NULL pointer dereference at 00000100
      IP: [<c1074df6>] __queue_work+0x26/0x390
      *pdpt = 0000000000000000 *pde = f000ff53f000ff53 *pde = f000ff53f000ff53
      Oops: 0000 [#1] PREEMPT PREEMPT SMP SMP
      CPU: 0 PID: 24 Comm: kworker/0:1 Not tainted 4.4.0-rc4-00139-g373ccbe #1
      Workqueue: events vmstat_shepherd
      task: cb684600 ti: cb7ba000 task.ti: cb7ba000
      EIP: 0060:[<c1074df6>] EFLAGS: 00010046 CPU: 0
      EIP is at __queue_work+0x26/0x390
      EAX: 00000046 EBX: cbb37800 ECX: cbb37800 EDX: 00000000
      ESI: 00000000 EDI: 00000000 EBP: cb7bbe68 ESP: cb7bbe38
       DS: 007b ES: 007b FS: 00d8 GS: 00e0 SS: 0068
      CR0: 8005003b CR2: 00000100 CR3: 01fd5000 CR4: 000006b0
      Stack:
      Call Trace:
        __queue_delayed_work+0xa1/0x160
        queue_delayed_work_on+0x36/0x60
        vmstat_shepherd+0xad/0xf0
        process_one_work+0x1aa/0x4c0
        worker_thread+0x41/0x440
        kthread+0xb0/0xd0
        ret_from_kernel_thread+0x21/0x40
    
    The reason is that start_shepherd_timer schedules the shepherd work item
    which uses vmstat_wq (vmstat_shepherd) before setup_vmstat allocates
    that workqueue so if the further initialization takes more than HZ we
    might end up scheduling on a NULL vmstat_wq.  This is really unlikely
    but not impossible.
    
    Fixes: 373ccbe59270 ("mm, vmstat: allow WQ concurrency to discover memory reclaim doesn't make any progress")
    Reported-by: kernel test robot <ying.huang@linux.intel.com>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Tested-by: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    [bwh: Backported to 3.2: This precise race condition doesn't exist, but there
     is a similar potential race with CPU hotplug.  So move the alloc_workqueue()
     above register_cpu_notifier().]
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit e93ad19d05648397ef3bcb838d26aec06c245dc0
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jan 19 12:18:41 2016 -0500

    cpuset: make mm migration asynchronous
    
    If "cpuset.memory_migrate" is set, when a process is moved from one
    cpuset to another with a different memory node mask, pages in used by
    the process are migrated to the new set of nodes.  This was performed
    synchronously in the ->attach() callback, which is synchronized
    against process management.  Recently, the synchronization was changed
    from per-process rwsem to global percpu rwsem for simplicity and
    optimization.
    
    Combined with the synchronous mm migration, this led to deadlocks
    because mm migration could schedule a work item which may in turn try
    to create a new worker blocking on the process management lock held
    from cgroup process migration path.
    
    This heavy an operation shouldn't be performed synchronously from that
    deep inside cgroup migration in the first place.  This patch punts the
    actual migration to an ordered workqueue and updates cgroup process
    migration and cpuset config update paths to flush the workqueue after
    all locks are released.  This way, the operations still seem
    synchronous to userland without entangling mm migration with process
    management synchronization.  CPU hotplug can also invoke mm migration
    but there's no reason for it to wait for mm migrations and thus
    doesn't synchronize against their completions.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-and-tested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: stable@vger.kernel.org # v4.4+

commit e666ae0b10aaa1c961c928558bafc28bc049ac87
Author: Nathan Fontenot <nfont@linux.vnet.ibm.com>
Date:   Wed Dec 16 14:52:39 2015 -0600

    powerpc/pseries: Update CPU hotplug error recovery
    
    Update the cpu dlpar add/remove paths to do better error recovery when
    a failure occurs during the add/remove operation.
    
    Signed-off-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 183deeea5871a6f750ec64ab1cff85fb089d38df
Author: Nathan Fontenot <nfont@linux.vnet.ibm.com>
Date:   Wed Dec 16 14:50:21 2015 -0600

    powerpc/pseries: Consolidate CPU hotplug code to hotplug-cpu.c
    
    No functional changes, this patch is simply a move of the cpu hotplug
    code from pseries/dlpar.c to pseries/hotplug-cpu.c. This is in an effort
    to consolidate all of the cpu hotplug code in a common place.
    
    Signed-off-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 918af9f941af9995fcaa7ef1eb67c433a492e2b3
Author: Grygorii Strashko <grygorii.strashko@ti.com>
Date:   Mon Nov 16 19:38:53 2015 +0200

    ARM: OMAP4+: SMP: use lockless clkdm/pwrdm api in omap4_boot_secondary
    
    OMAP CPU hotplug uses cpu1's clocks and power domains for CPU1 wake up
    from low power states (or turn on CPU1). This part of code is also
    part of system suspend (disable_nonboot_cpus()).
    >From other side, cpu1's clocks and power domains are used by CPUIdle. All above
    functionality is mutually exclusive and, therefore, lockless clkdm/pwrdm api
    can be used in omap4_boot_secondary().
    
    This fixes below back-trace on -RT which is triggered by
    pwrdm_lock/unlock():
    
    BUG: sleeping function called from invalid context at kernel/locking/rtmutex.c:917
     in_atomic(): 1, irqs_disabled(): 0, pid: 118, name: sh
     9 locks held by sh/118:
      #0:  (sb_writers#4){.+.+.+}, at: [<c0144a6c>] vfs_write+0x13c/0x164
      #1:  (&of->mutex){+.+.+.}, at: [<c01b4c70>] kernfs_fop_write+0x48/0x19c
      #2:  (s_active#24){.+.+.+}, at: [<c01b4c78>] kernfs_fop_write+0x50/0x19c
      #3:  (device_hotplug_lock){+.+.+.}, at: [<c03cbff0>] lock_device_hotplug_sysfs+0xc/0x4c
      #4:  (&dev->mutex){......}, at: [<c03cd284>] device_online+0x14/0x88
      #5:  (cpu_add_remove_lock){+.+.+.}, at: [<c003af90>] cpu_up+0x50/0x1a0
      #6:  (cpu_hotplug.lock){++++++}, at: [<c003ae48>] cpu_hotplug_begin+0x0/0xc4
      #7:  (cpu_hotplug.lock#2){+.+.+.}, at: [<c003aec0>] cpu_hotplug_begin+0x78/0xc4
      #8:  (boot_lock){+.+...}, at: [<c002b254>] omap4_boot_secondary+0x1c/0x178
     Preemption disabled at:[<  (null)>]   (null)
    
     CPU: 0 PID: 118 Comm: sh Not tainted 4.1.12-rt11-01998-gb4a62c3-dirty #137
     Hardware name: Generic DRA74X (Flattened Device Tree)
     [<c0017574>] (unwind_backtrace) from [<c0013be8>] (show_stack+0x10/0x14)
     [<c0013be8>] (show_stack) from [<c05a8670>] (dump_stack+0x80/0x94)
     [<c05a8670>] (dump_stack) from [<c05ad158>] (rt_spin_lock+0x24/0x54)
     [<c05ad158>] (rt_spin_lock) from [<c0030dac>] (clkdm_wakeup+0x10/0x2c)
     [<c0030dac>] (clkdm_wakeup) from [<c002b2c0>] (omap4_boot_secondary+0x88/0x178)
     [<c002b2c0>] (omap4_boot_secondary) from [<c0015d00>] (__cpu_up+0xc4/0x164)
     [<c0015d00>] (__cpu_up) from [<c003b09c>] (cpu_up+0x15c/0x1a0)
     [<c003b09c>] (cpu_up) from [<c03cd2d4>] (device_online+0x64/0x88)
     [<c03cd2d4>] (device_online) from [<c03cd360>] (online_store+0x68/0x74)
     [<c03cd360>] (online_store) from [<c01b4ce0>] (kernfs_fop_write+0xb8/0x19c)
     [<c01b4ce0>] (kernfs_fop_write) from [<c0144124>] (__vfs_write+0x20/0xd8)
     [<c0144124>] (__vfs_write) from [<c01449c0>] (vfs_write+0x90/0x164)
     [<c01449c0>] (vfs_write) from [<c01451e4>] (SyS_write+0x44/0x9c)
     [<c01451e4>] (SyS_write) from [<c0010240>] (ret_fast_syscall+0x0/0x54)
     CPU1: smp_ops.cpu_die() returned, trying to resuscitate
    
    Cc: Tero Kristo <t-kristo@ti.com>
    Signed-off-by: Grygorii Strashko <grygorii.strashko@ti.com>
    Signed-off-by: Tony Lindgren <tony@atomide.com>

commit 56e0464980febfa50432a070261579415c72664e
Merge: a5e1d715a8d0 b1e4006aeda8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 10 14:56:23 2015 -0800

    Merge tag 'armsoc-soc' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull ARM SoC platform updates from Olof Johansson:
     "New and/or improved SoC support for this release:
    
      Marvell Berlin:
         - Enable standard DT-based cpufreq
         - Add CPU hotplug support
    
      Freescale:
         - Ethernet init for i.MX7D
         - Suspend/resume support for i.MX6UL
    
      Allwinner:
         - Support for R8 chipset (used on NTC's $9 C.H.I.P board)
    
      Mediatek:
         - SMP support for some platforms
    
      Uniphier:
         - L2 support
         - Cleaned up SMP support, etc.
    
      plus a handful of other patches around above functionality, and a few
      other smaller changes"
    
    * tag 'armsoc-soc' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (42 commits)
      ARM: uniphier: rework SMP operations to use trampoline code
      ARM: uniphier: add outer cache support
      Documentation: EXYNOS: Update bootloader interface on exynos542x
      ARM: mvebu: add broken-idle option
      ARM: orion5x: use mac_pton() helper
      ARM: at91: pm: at91_pm_suspend_in_sram() must be 8-byte aligned
      ARM: sunxi: Add R8 support
      ARM: digicolor: select pinctrl/gpio driver
      arm: berlin: add CPU hotplug support
      arm: berlin: use non-self-cleared reset register to reset cpu
      ARM: mediatek: add smp bringup code
      ARM: mediatek: enable gpt6 on boot up to make arch timer working
      soc: mediatek: Fix random hang up issue while kernel init
      soc: ti: qmss: make acc queue support optional in the driver
      soc: ti: add firmware file name as part of the driver
      Documentation: dt: soc: Add description for knav qmss driver
      ARM: S3C64XX: Use PWM lookup table for mach-smartq
      ARM: S3C64XX: Use PWM lookup table for mach-hmt
      ARM: S3C64XX: Use PWM lookup table for mach-crag6410
      ARM: S3C64XX: Use PWM lookup table for smdk6410
      ...

commit a2a45b85ec45db4b041ea5d93b21033dbc3cc0fc
Author: Ulrich Obergfell <uobergfe@redhat.com>
Date:   Thu Nov 5 18:44:53 2015 -0800

    kernel/watchdog.c: remove {get|put}_online_cpus() from watchdog_{park|unpark}_threads()
    
    watchdog_{park|unpark}_threads() are now called in code paths that protect
    themselves against CPU hotplug, so {get|put}_online_cpus() calls are
    redundant and can be removed.
    
    Signed-off-by: Ulrich Obergfell <uobergfe@redhat.com>
    Acked-by: Don Zickus <dzickus@redhat.com>
    Reviewed-by: Aaron Tomlin <atomlin@redhat.com>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 8614ddef82139d08234dbf681188f9bcddae9f03
Author: Ulrich Obergfell <uobergfe@redhat.com>
Date:   Thu Nov 5 18:44:50 2015 -0800

    kernel/watchdog.c: avoid races between /proc handlers and CPU hotplug
    
    The handler functions for watchdog parameters in /proc/sys/kernel do not
    protect themselves against races with CPU hotplug.  Hence, theoretically
    it is possible that a new watchdog thread is started on a hotplugged CPU
    while a parameter is being modified, and the thread could thus use a
    parameter value that is 'in transition'.
    
    For example, if 'watchdog_thresh' is being set to zero (note: this
    disables the lockup detectors) the thread would erroneously use the value
    zero as the sample period.
    
    To avoid such races and to keep the /proc handler code consistent,
    call
         {get|put}_online_cpus() in proc_watchdog_common()
         {get|put}_online_cpus() in proc_watchdog_thresh()
         {get|put}_online_cpus() in proc_watchdog_cpumask()
    
    Signed-off-by: Ulrich Obergfell <uobergfe@redhat.com>
    Acked-by: Don Zickus <dzickus@redhat.com>
    Reviewed-by: Aaron Tomlin <atomlin@redhat.com>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ee89e71eb091d3ef8ca2be8bd4ec77ccfa91334c
Author: Ulrich Obergfell <uobergfe@redhat.com>
Date:   Thu Nov 5 18:44:47 2015 -0800

    kernel/watchdog.c: avoid race between lockup detector suspend/resume and CPU hotplug
    
    The lockup detector suspend/resume interface that was introduced by
    commit 8c073d27d7ad ("watchdog: introduce watchdog_suspend() and
    watchdog_resume()") does not protect itself against races with CPU
    hotplug.  Hence, theoretically it is possible that a new watchdog thread
    is started on a hotplugged CPU while the lockup detector is suspended,
    and the thread could thus interfere unexpectedly with the code that
    requested to suspend the lockup detector.
    
    Avoid the race by calling
    
      get_online_cpus() in lockup_detector_suspend()
      put_online_cpus() in lockup_detector_resume()
    
    Signed-off-by: Ulrich Obergfell <uobergfe@redhat.com>
    Acked-by: Don Zickus <dzickus@redhat.com>
    Reviewed-by: Aaron Tomlin <atomlin@redhat.com>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 41ecf1404b34d9975eb97f5005d9e4274eaeb76a
Merge: 2dc10ad81fc0 abed7d0710e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 4 17:32:42 2015 -0800

    Merge tag 'for-linus-4.4-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from David Vrabel:
    
     - Improve balloon driver memory hotplug placement.
    
     - Use unpopulated hotplugged memory for foreign pages (if
       supported/enabled).
    
     - Support 64 KiB guest pages on arm64.
    
     - CPU hotplug support on arm/arm64.
    
    * tag 'for-linus-4.4-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip: (44 commits)
      xen: fix the check of e_pfn in xen_find_pfn_range
      x86/xen: add reschedule point when mapping foreign GFNs
      xen/arm: don't try to re-register vcpu_info on cpu_hotplug.
      xen, cpu_hotplug: call device_offline instead of cpu_down
      xen/arm: Enable cpu_hotplug.c
      xenbus: Support multiple grants ring with 64KB
      xen/grant-table: Add an helper to iterate over a specific number of grants
      xen/xenbus: Rename *RING_PAGE* to *RING_GRANT*
      xen/arm: correct comment in enlighten.c
      xen/gntdev: use types from linux/types.h in userspace headers
      xen/gntalloc: use types from linux/types.h in userspace headers
      xen/balloon: Use the correct sizeof when declaring frame_list
      xen/swiotlb: Add support for 64KB page granularity
      xen/swiotlb: Pass addresses rather than frame numbers to xen_arch_need_swiotlb
      arm/xen: Add support for 64KB page granularity
      xen/privcmd: Add support for Linux 64KB page granularity
      net/xen-netback: Make it running on 64KB page granularity
      net/xen-netfront: Make it running on 64KB page granularity
      block/xen-blkback: Make it running on 64KB page granularity
      block/xen-blkfront: Make it running on 64KB page granularity
      ...

commit 53528695ff6d8b77011bc818407c13e30914a946
Merge: b831ef2cad97 e73e85f05938
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 3 18:03:50 2015 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The main changes in this cycle were:
    
       - sched/fair load tracking fixes and cleanups (Byungchul Park)
    
       - Make load tracking frequency scale invariant (Dietmar Eggemann)
    
       - sched/deadline updates (Juri Lelli)
    
       - stop machine fixes, cleanups and enhancements for bugs triggered by
         CPU hotplug stress testing (Oleg Nesterov)
    
       - scheduler preemption code rework: remove PREEMPT_ACTIVE and related
         cleanups (Peter Zijlstra)
    
       - Rework the sched_info::run_delay code to fix races (Peter Zijlstra)
    
       - Optimize per entity utilization tracking (Peter Zijlstra)
    
       - ... misc other fixes, cleanups and smaller updates"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (57 commits)
      sched: Don't scan all-offline ->cpus_allowed twice if !CONFIG_CPUSETS
      sched: Move cpu_active() tests from stop_two_cpus() into migrate_swap_stop()
      sched: Start stopper early
      stop_machine: Kill cpu_stop_threads->setup() and cpu_stop_unpark()
      stop_machine: Kill smp_hotplug_thread->pre_unpark, introduce stop_machine_unpark()
      stop_machine: Change cpu_stop_queue_two_works() to rely on stopper->enabled
      stop_machine: Introduce __cpu_stop_queue_work() and cpu_stop_queue_two_works()
      stop_machine: Ensure that a queued callback will be called before cpu_stop_park()
      sched/x86: Fix typo in __switch_to() comments
      sched/core: Remove a parameter in the migrate_task_rq() function
      sched/core: Drop unlikely behind BUG_ON()
      sched/core: Fix task and run queue sched_info::run_delay inconsistencies
      sched/numa: Fix task_tick_fair() from disabling numa_balancing
      sched/core: Add preempt_count invariant check
      sched/core: More notrace annotations
      sched/core: Kill PREEMPT_ACTIVE
      sched/core, sched/x86: Kill thread_info::saved_preempt_count
      sched/core: Simplify preempt_count tests
      sched/core: Robustify preemption leak checks
      sched/core: Stop setting PREEMPT_ACTIVE
      ...

commit b831ef2cad979912850e34f82415c0c5d59de8cb
Merge: b02ac6b18cd4 dc34bdd2367f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 3 17:51:33 2015 -0800

    Merge branch 'ras-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RAS changes from Ingo Molnar:
     "The main system reliability related changes were from x86, but also
      some generic RAS changes:
    
       - AMD MCE error injection subsystem enhancements.  (Aravind
         Gopalakrishnan)
    
       - Fix MCE and CPU hotplug interaction bug.  (Ashok Raj)
    
       - kcrash bootup robustness fix.  (Baoquan He)
    
       - kcrash cleanups.  (Borislav Petkov)
    
       - x86 microcode driver rework: simplify it by unmodularizing it and
         other cleanups.  (Borislav Petkov)"
    
    * 'ras-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      x86/mce: Add a default case to the switch in __mcheck_cpu_ancient_init()
      x86/mce: Add a Scalable MCA vendor flags bit
      MAINTAINERS: Unify the microcode driver section
      x86/microcode/intel: Move #ifdef DEBUG inside the function
      x86/microcode/amd: Remove maintainers from comments
      x86/microcode: Remove modularization leftovers
      x86/microcode: Merge the early microcode loader
      x86/microcode: Unmodularize the microcode driver
      x86/mce: Fix thermal throttling reporting after kexec
      kexec/crash: Say which char is the unrecognized
      x86/setup/crash: Check memblock_reserve() retval
      x86/setup/crash: Cleanup some more
      x86/setup/crash: Remove alignment variable
      x86/setup: Cleanup crashkernel reservation functions
      x86/amd_nb, EDAC: Rename amd_get_node_id()
      x86/setup: Do not reserve crashkernel high memory if low reservation failed
      x86/microcode/amd: Do not overwrite final patch levels
      x86/microcode/amd: Extract current patch level read to a function
      x86/ras/mce_amd_inj: Inject bank 4 errors on the NBC
      x86/ras/mce_amd_inj: Trigger deferred and thresholding errors interrupts
      ...

commit 281422869942c19f05a08d4017c633d08d390938
Merge: f5a8160c1e05 b33e18f61bd1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 3 15:40:38 2015 -0800

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU changes from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Improvements to expedited grace periods (Paul E McKenney)
    
       - Performance improvements to and locktorture tests for percpu-rwsem
         (Oleg Nesterov, Paul E McKenney)
    
       - Torture-test changes (Paul E McKenney, Davidlohr Bueso)
    
       - Documentation updates (Paul E McKenney)
    
       - Miscellaneous fixes (Paul E McKenney, Boqun Feng, Oleg Nesterov,
         Patrick Marlier)"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (54 commits)
      fs/writeback, rcu: Don't use list_entry_rcu() for pointer offsetting in bdi_split_work_to_wbs()
      rcu: Better hotplug handling for synchronize_sched_expedited()
      rcu: Enable stall warnings for synchronize_rcu_expedited()
      rcu: Add tasks to expedited stall-warning messages
      rcu: Add online/offline info to expedited stall warning message
      rcu: Consolidate expedited CPU selection
      rcu: Prepare for consolidating expedited CPU selection
      cpu: Remove try_get_online_cpus()
      rcu: Stop excluding CPU hotplug in synchronize_sched_expedited()
      rcu: Stop silencing lockdep false positive for expedited grace periods
      rcu: Switch synchronize_sched_expedited() to IPI
      locktorture: Fix module unwind when bad torture_type specified
      torture: Forgive non-plural arguments
      rcutorture: Fix unused-function warning for torturing_tasks()
      rcutorture: Fix module unwind when bad torture_type specified
      rcu_sync: Cleanup the CONFIG_PROVE_RCU checks
      locking/percpu-rwsem: Clean up the lockdep annotations in percpu_down_read()
      locking/percpu-rwsem: Fix the comments outdated by rcu_sync
      locking/percpu-rwsem: Make use of the rcu_sync infrastructure
      locking/percpu-rwsem: Make percpu_free_rwsem() after kzalloc() safe
      ...

commit 6aa2fdb87cf01d7746955c600cbac352dc04d451
Merge: 7b2a4306f9e7 d9e4ad5badf4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 3 14:40:01 2015 -0800

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq updates from Thomas Gleixner:
     "The irq departement delivers:
    
       - Rework the irqdomain core infrastructure to accomodate ACPI based
         systems.  This is required to support ARM64 without creating
         artificial device tree nodes.
    
       - Sanitize the ACPI based ARM GIC initialization by making use of the
         new firmware independent irqdomain core
    
       - Further improvements to the generic MSI management
    
       - Generalize the irq migration on CPU hotplug
    
       - Improvements to the threaded interrupt infrastructure
    
       - Allow the migration of "chained" low level interrupt handlers
    
       - Allow optional force masking of interrupts in disable_irq[_nosysnc]
    
       - Support for two new interrupt chips - Sigh!
    
       - A larger set of errata fixes for ARM gicv3
    
       - The usual pile of fixes, updates, improvements and cleanups all
         over the place"
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (71 commits)
      Document that IRQ_NONE should be returned when IRQ not actually handled
      PCI/MSI: Allow the MSI domain to be device-specific
      PCI: Add per-device MSI domain hook
      of/irq: Use the msi-map property to provide device-specific MSI domain
      of/irq: Split of_msi_map_rid to reuse msi-map lookup
      irqchip/gic-v3-its: Parse new version of msi-parent property
      PCI/MSI: Use of_msi_get_domain instead of open-coded "msi-parent" parsing
      of/irq: Use of_msi_get_domain instead of open-coded "msi-parent" parsing
      of/irq: Add support code for multi-parent version of "msi-parent"
      irqchip/gic-v3-its: Add handling of PCI requester id.
      PCI/MSI: Add helper function pci_msi_domain_get_msi_rid().
      of/irq: Add new function of_msi_map_rid()
      Docs: dt: Add PCI MSI map bindings
      irqchip/gic-v2m: Add support for multiple MSI frames
      irqchip/gic-v3: Fix translation of LPIs after conversion to irq_fwspec
      irqchip/mxs: Add Alphascale ASM9260 support
      irqchip/mxs: Prepare driver for hardware with different offsets
      irqchip/mxs: Panic if ioremap or domain creation fails
      irqdomain: Documentation updates
      irqdomain/msi: Use fwnode instead of of_node
      ...

commit 980bbff018f64a22af0e75a12ee14cbfbc547e6a
Author: Linus Walleij <linus.walleij@linaro.org>
Date:   Wed Oct 28 10:39:55 2015 +0100

    ARM64: juno: disable NOR flash node by default
    
    After discussing on the mailing list it turns out that
    accessing the flash memory from the kernel can disrupt CPU
    sleep states and CPU hotplugging, so let's disable this
    DT node by default. Setups that want to access the flash
    can modify this entry to enable the flash again.
    
    Quoting Sudeep Holla: "the firmware assumes the flash is
    always in read mode while Linux leaves NOR flash in
    "read id" mode after initialization."
    
    Reported-by: Sudeep Holla <sudeep.holla@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Sudeep Holla <sudeep.holla@arm.com>
    Cc: Liviu Dudau <Liviu.Dudau@arm.com>
    Cc: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Leif Lindholm <leif.lindholm@arm.com>
    Cc: Ryan Harkin <ryan.harkin@linaro.org>
    Fixes: 5078f77e1443 "ARM64: juno: add NOR flash to device tree"
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Olof Johansson <olof@lixom.net>

commit 881ea7d3f55bf7b8834c81d86c4dcb3d5f786f15
Author: chenhui zhao <chenhui.zhao@freescale.com>
Date:   Thu Jul 23 11:55:45 2015 +0800

    powerpc/corenet: use the mixed mode of MPIC when enabling CPU hotplug
    
    Core reset may cause issue if using the proxy mode of MPIC.
    Use the mixed mode of MPIC if enabling CPU hotplug.
    
    Signed-off-by: Chenhui Zhao <chenhui.zhao@freescale.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

commit 5462b10af11d0b334fe37883998e0f38b14127bb
Merge: 8207e2edbdb5 a7b3d5a715f4
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Oct 15 22:55:53 2015 +0200

    Merge tag 'berlin-soc-for-4.4-2' of git://git.infradead.org/users/hesselba/linux-berlin into next/soc
    
    Merge "Marvell Berlin SoC for 4.4 take 2" from Sebastian Hesselbarth:
    
    - use the non-self-clearing reset register
    - add cpu hotplug support
    
    * tag 'berlin-soc-for-4.4-2' of git://git.infradead.org/users/hesselba/linux-berlin:
      arm: berlin: add CPU hotplug support
      arm: berlin: use non-self-cleared reset register to reset cpu

commit a7b3d5a715f489ee542e59d722281c9f16da50dc
Author: Jisheng Zhang <jszhang@marvell.com>
Date:   Mon Sep 14 14:47:45 2015 +0800

    arm: berlin: add CPU hotplug support
    
    Add cpu hotplug support for berlin SoCs such as BG2 and BG2Q. These SoC
    don't support power off cpu independently, but we also want cpu hotplug
    support in these SoCs. We achieve this goal by putting the dying CPU in
    WFI state after the coherency is disabled, then asserting the dying CPU
    reset bit to put the CPU in reset state.
    
    Signed-off-by: Jisheng Zhang <jszhang@marvell.com>
    Signed-off-by: Sebastian Hesselbarth <sebastian.hesselbarth@gmail.com>

commit 807226e2fbb504d82cd504b7b6114896db41ef63
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Aug 7 12:03:45 2015 -0700

    rcu: Stop excluding CPU hotplug in synchronize_sched_expedited()
    
    Now that synchronize_sched_expedited() uses IPIs, a hook in
    rcu_sched_qs(), and the ->expmask field in the rcu_node combining
    tree, it is no longer necessary to exclude CPU hotplug.  Any
    races with CPU hotplug will be detected when attempting to send
    the IPI.  This commit therefore removes the code excluding
    CPU hotplug operations.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 191988e0bd3875526464e3a601edf29f2a3118ae
Merge: 8a4683a5e06e f86428854480
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Sep 29 11:51:41 2015 -0700

    Merge branch 'mvneta_percpu_irq'
    
    Gregory CLEMENT says:
    
    ====================
    net: mvneta: Switch to per-CPU irq and make rxq_def useful
    
    As stated in the first version: "this patchset reworks the Marvell
    neta driver in order to really support its per-CPU interrupts, instead
    of faking them as SPI, and allow the use of any RX queue instead of
    the hardcoded RX queue 0 that we have currently."
    
    Following the review which has been done, Maxime started adding the
    CPU hotplug support. I continued his work a few weeks ago and here is
    the result.
    
    Since the 1st version the main change is this CPU hotplug support, in
    order to validate it I powered up and down the CPUs while performing
    iperf. I ran the tests during hours: the kernel didn't crash and the
    network interfaces were still usable. Of course it impacted the
    performance, but continuously power down and up the CPUs is not
    something we usually do.
    
    I also reorganized the series, the 3 first patches should go through
    the irq subsystem, whereas the 4 others should go to the network
    subsystem.
    
    However, there is a runtime dependency between the two parts. Patch 5
    depend on the patch 3 to be able to use the percpu irq.
    
    Thanks,
    
    Gregory
    
    PS: Thanks to Willy who gave me some pointers on how to deal with the
    NAPI.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit f864288544802ae68b20da293e1b861230852dba
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Fri Sep 25 18:09:38 2015 +0200

    net: mvneta: Statically assign queues to CPUs
    
    Since the switch to per-CPU interrupts, we lost the ability to set which
    CPU was going to receive our RX interrupt, which was now only the CPU on
    which the mvneta_open function was run.
    
    We can now assign our queues to their respective CPUs, and make sure only
    this CPU is going to handle our traffic.
    
    This also paves the road to be able to change that at runtime, and later on
    to support RSS.
    
    [gregory.clement@free-electrons.com]: hardened the CPU hotplug support.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Signed-off-by: Gregory CLEMENT <gregory.clement@free-electrons.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 9c67f424970dd732af60ed2378ff2f6bb7463edf
Author: Bharata B Rao <bharata@linux.vnet.ibm.com>
Date:   Mon Sep 7 15:52:40 2015 +0530

    powerpc/pseries: Release DRC when configure_connector fails
    
    commit daebaabb5cfbe4a6f09ca0e0f8b7673efc704960 upstream.
    
    Commit f32393c943e2 ("powerpc/pseries: Correct cpu affinity for
    dlpar added cpus") moved dlpar_acquire_drc() call to before
    dlpar_configure_connector() call in dlpar_cpu_probe(), but missed
    to release the DRC if dlpar_configure_connector() failed.
    During CPU hotplug, if configure-connector fails for any reason,
    then this will result in subsequent CPU hotplug attempts to fail.
    
    Release the acquired DRC if dlpar_configure_connector() call fails
    so that the DRC is left in right isolation and allocation state
    for the subsequent hotplug operation to succeed.
    
    Fixes: f32393c943e2 ("powerpc/pseries: Correct cpu affinity for dlpar added cpus")
    Signed-off-by: Bharata B Rao <bharata@linux.vnet.ibm.com>
    Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 60de074ba1e8f327db19bc33d8530131ac01695d
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Sun Sep 27 02:09:25 2015 +0900

    blk-mq: fix deadlock when reading cpu_list
    
    CPU hotplug handling for blk-mq (blk_mq_queue_reinit) acquires
    all_q_mutex in blk_mq_queue_reinit_notify() and then removes sysfs
    entries by blk_mq_sysfs_unregister().  Removing sysfs entry needs to
    be blocked until the active reference of the kernfs_node to be zero.
    
    On the other hand, reading blk_mq_hw_sysfs_cpu sysfs entry (e.g.
    /sys/block/nullb0/mq/0/cpu_list) acquires all_q_mutex in
    blk_mq_hw_sysfs_cpus_show().
    
    If these happen at the same time, a deadlock can happen.  Because one
    can wait for the active reference to be zero with holding all_q_mutex,
    and the other tries to acquire all_q_mutex with holding the active
    reference.
    
    The reason that all_q_mutex is acquired in blk_mq_hw_sysfs_cpus_show()
    is to avoid reading an imcomplete hctx->cpumask.  Since reading sysfs
    entry for blk-mq needs to acquire q->sysfs_lock, we can avoid deadlock
    and reading an imcomplete hctx->cpumask by protecting q->sysfs_lock
    while hctx->cpumask is being updated.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Reviewed-by: Ming Lei <tom.leiming@gmail.com>
    Cc: Ming Lei <tom.leiming@gmail.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit 0e6263682014d480b8d7b8c10287f4536066b54f
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Sun Sep 27 02:09:22 2015 +0900

    blk-mq: fix q->mq_usage_counter access race
    
    CPU hotplug handling for blk-mq (blk_mq_queue_reinit) accesses
    q->mq_usage_counter while freezing all request queues in all_q_list.
    On the other hand, q->mq_usage_counter is deinitialized in
    blk_mq_free_queue() before deleting the queue from all_q_list.
    
    So if CPU hotplug event occurs in the window, percpu_ref_kill() is
    called with q->mq_usage_counter which has already been marked dead,
    and it triggers warning.  Fix it by deleting the queue from all_q_list
    earlier than destroying q->mq_usage_counter.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Reviewed-by: Ming Lei <tom.leiming@gmail.com>
    Cc: Ming Lei <tom.leiming@gmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit a723bab3d7529133f71fc8a5e96f86e3639a0d13
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Sun Sep 27 02:09:21 2015 +0900

    blk-mq: Fix use after of free q->mq_map
    
    CPU hotplug handling for blk-mq (blk_mq_queue_reinit) updates
    q->mq_map by blk_mq_update_queue_map() for all request queues in
    all_q_list.  On the other hand, q->mq_map is released before deleting
    the queue from all_q_list.
    
    So if CPU hotplug event occurs in the window, invalid memory access
    can happen.  Fix it by releasing q->mq_map in blk_mq_release() to make
    it happen latter than removal from all_q_list.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Suggested-by: Ming Lei <tom.leiming@gmail.com>
    Reviewed-by: Ming Lei <tom.leiming@gmail.com>
    Cc: Ming Lei <tom.leiming@gmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit 4593fdbe7a2f44d5e64c627c715dd0bcec9bdf14
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Sun Sep 27 02:09:20 2015 +0900

    blk-mq: fix sysfs registration/unregistration race
    
    There is a race between cpu hotplug handling and adding/deleting
    gendisk for blk-mq, where both are trying to register and unregister
    the same sysfs entries.
    
    null_add_dev
        --> blk_mq_init_queue
            --> blk_mq_init_allocated_queue
                --> add to 'all_q_list' (*)
        --> add_disk
            --> blk_register_queue
                --> blk_mq_register_disk (++)
    
    null_del_dev
        --> del_gendisk
            --> blk_unregister_queue
                --> blk_mq_unregister_disk (--)
        --> blk_cleanup_queue
            --> blk_mq_free_queue
                --> del from 'all_q_list' (*)
    
    blk_mq_queue_reinit
        --> blk_mq_sysfs_unregister (-)
        --> blk_mq_sysfs_register (+)
    
    While the request queue is added to 'all_q_list' (*),
    blk_mq_queue_reinit() can be called for the queue anytime by CPU
    hotplug callback.  But blk_mq_sysfs_unregister (-) and
    blk_mq_sysfs_register (+) in blk_mq_queue_reinit must not be called
    before blk_mq_register_disk (++) and after blk_mq_unregister_disk (--)
    is finished.  Because '/sys/block/*/mq/' is not exists.
    
    There has already been BLK_MQ_F_SYSFS_UP flag in hctx->flags which can
    be used to track these sysfs stuff, but it is only fixing this issue
    partially.
    
    In order to fix it completely, we just need per-queue flag instead of
    per-hctx flag with appropriate locking.  So this introduces
    q->mq_sysfs_init_done which is properly protected with all_q_mutex.
    
    Also, we need to ensure that blk_mq_map_swqueue() is called with
    all_q_mutex is held.  Since hctx->nr_ctx is reset temporarily and
    updated in blk_mq_map_swqueue(), so we should avoid
    blk_mq_register_hctx() seeing the temporary hctx->nr_ctx value
    in CPU hotplug handling or adding/deleting gendisk .
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Reviewed-by: Ming Lei <tom.leiming@gmail.com>
    Cc: Ming Lei <tom.leiming@gmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit e56d82a116176f7af9d642b560abbbd3a2b68013
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Sep 11 15:31:24 2015 +0100

    arm64: cpu hotplug: ensure we mask out CPU_TASKS_FROZEN in notifiers
    
    We have a couple of CPU hotplug notifiers for resetting the CPU debug
    state to a sane value when a CPU comes online.
    
    This patch ensures that we mask out CPU_TASKS_FROZEN so that we don't
    miss any online events occuring due to suspend/resume.
    
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit daebaabb5cfbe4a6f09ca0e0f8b7673efc704960
Author: Bharata B Rao <bharata@linux.vnet.ibm.com>
Date:   Mon Sep 7 15:52:40 2015 +0530

    powerpc/pseries: Release DRC when configure_connector fails
    
    Commit f32393c943e2 ("powerpc/pseries: Correct cpu affinity for
    dlpar added cpus") moved dlpar_acquire_drc() call to before
    dlpar_configure_connector() call in dlpar_cpu_probe(), but missed
    to release the DRC if dlpar_configure_connector() failed.
    During CPU hotplug, if configure-connector fails for any reason,
    then this will result in subsequent CPU hotplug attempts to fail.
    
    Release the acquired DRC if dlpar_configure_connector() call fails
    so that the DRC is left in right isolation and allocation state
    for the subsequent hotplug operation to succeed.
    
    Fixes: f32393c943e2 ("powerpc/pseries: Correct cpu affinity for dlpar added cpus")
    Cc: stable@vger.kernel.org # 4.1+
    Signed-off-by: Bharata B Rao <bharata@linux.vnet.ibm.com>
    Reviewed-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 7073bc66126e3ab742cce9416ad6b4be8b03c4f7
Merge: d4c90396ed7e f612a7b1a7f1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 31 18:12:07 2015 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The main RCU changes in this cycle are:
    
       - the combination of tree geometry-initialization simplifications and
         OS-jitter-reduction changes to expedited grace periods.  These two
         are stacked due to the large number of conflicts that would
         otherwise result.
    
       - privatize smp_mb__after_unlock_lock().
    
         This commit moves the definition of smp_mb__after_unlock_lock() to
         kernel/rcu/tree.h, in recognition of the fact that RCU is the only
         thing using this, that nothing else is likely to use it, and that
         it is likely to go away completely.
    
       - documentation updates.
    
       - torture-test updates.
    
       - misc fixes"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (60 commits)
      rcu,locking: Privatize smp_mb__after_unlock_lock()
      rcu: Silence lockdep false positive for expedited grace periods
      rcu: Don't disable CPU hotplug during OOM notifiers
      scripts: Make checkpatch.pl warn on expedited RCU grace periods
      rcu: Update MAINTAINERS entry
      rcu: Clarify CONFIG_RCU_EQS_DEBUG help text
      rcu: Fix backwards RCU_LOCKDEP_WARN() in synchronize_rcu_tasks()
      rcu: Rename rcu_lockdep_assert() to RCU_LOCKDEP_WARN()
      rcu: Make rcu_is_watching() really notrace
      cpu: Wait for RCU grace periods concurrently
      rcu: Create a synchronize_rcu_mult()
      rcu: Fix obsolete priority-boosting comment
      rcu: Use WRITE_ONCE in RCU_INIT_POINTER
      rcu: Hide RCU_NOCB_CPU behind RCU_EXPERT
      rcu: Add RCU-sched flavors of get-state and cond-sync
      rcu: Add fastpath bypassing funnel locking
      rcu: Rename RCU_GP_DONE_FQS to RCU_GP_DOING_FQS
      rcu: Pull out wait_event*() condition into helper function
      documentation: Describe new expedited stall warnings
      rcu: Add stall warnings to synchronize_sched_expedited()
      ...

commit df0e5109cfea25f842bf99f567aac20ac9ef78fe
Author: Ming Lei <ming.lei@canonical.com>
Date:   Tue Apr 21 10:00:20 2015 +0800

    blk-mq: fix CPU hotplug handling
    
    [ Upstream commit 2a34c0872adf252f23a6fef2d051a169ac796cef ]
    
    hctx->tags has to be set as NULL in case that it is to be unmapped
    no matter if set->tags[hctx->queue_num] is NULL or not in blk_mq_map_swqueue()
    because shared tags can be freed already from another request queue.
    
    The same situation has to be considered during handling CPU online too.
    Unmapped hw queue can be remapped after CPU topo is changed, so we need
    to allocate tags for the hw queue in blk_mq_map_swqueue(). Then tags
    allocation for hw queue can be removed in hctx cpu online notifier, and it
    is reasonable to do that after mapping is updated.
    
    Cc: <stable@vger.kernel.org>
    Reported-by: Dongsu Park <dongsu.park@profitbricks.com>
    Tested-by: Dongsu Park <dongsu.park@profitbricks.com>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>

commit 8ec41987436d566f7c4559c6871738b869f7ef07
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 4 17:49:36 2015 +0100

    arm64: mm: ensure patched kernel text is fetched from PoU
    
    The arm64 booting document requires that the bootloader has cleaned the
    kernel image to the PoC. However, when a CPU re-enters the kernel due to
    either a CPU hotplug "on" event or resuming from a low-power state (e.g.
    cpuidle), the kernel text may in-fact be dirty at the PoU due to things
    like alternative patching or even module loading.
    
    Thanks to I-cache speculation with the MMU off, stale instructions could
    be fetched prior to enabling the MMU, potentially leading to crashes
    when executing regions of code that have been modified at runtime.
    
    This patch addresses the issue by ensuring that the local I-cache is
    invalidated immediately after a CPU has enabled its MMU but before
    jumping out of the identity mapping. Any stale instructions fetched from
    the PoC will then be discarded and refetched correctly from the PoU.
    Patching kernel text executed prior to the MMU being enabled is
    prohibited, so the early entry code will always be clean.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 586b7ccdb7143b6a9b975d2c6ad52b6ca5c162b9
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Jul 28 15:03:05 2015 +0200

    KVM: s390: Fix hang VCPU hang/loop regression
    
    commit 785dbef407d8 ("KVM: s390: optimize round trip time in request
    handling") introduced a regression. This regression was seen with
    CPU hotplug in the guest and switching between 1 or 2 CPUs. This will
    set/reset the IBS control via synced request.
    
    Whenever we make a synced request, we first set the vcpu->requests
    bit and then block the vcpu. The handler, on the other hand, unblocks
    itself, processes vcpu->requests (by clearing them) and unblocks itself
    once again.
    
    Now, if the requester sleeps between setting of vcpu->requests and
    blocking, the handler will clear the vcpu->requests bit and try to
    unblock itself (although no bit is set). When the requester wakes up,
    it blocks the VCPU and we have a blocked VCPU without requests.
    
    Solution is to always unset the block bit.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Fixes: 785dbef407d8 ("KVM: s390: optimize round trip time in request handling")

commit 454d3a2500a4eb33be85dde3bfba9e5f6b5efadc
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jul 22 17:59:11 2015 +0200

    cpufreq: Remove cpufreq_rwsem
    
    cpufreq_rwsem was introduced in commit 6eed9404ab3c4 ("cpufreq: Use
    rwsem for protecting critical sections) in order to replace
    try_module_get() on the cpu-freq driver. That try_module_get() worked
    well until the refcount was so heavily used that module removal became
    more or less impossible.
    
    Though when looking at the various (undocumented) protection
    mechanisms in that code, the randomly sprinkeled around cpufreq_rwsem
    locking sites are superfluous.
    
    The policy, which is acquired in cpufreq_cpu_get() and released in
    cpufreq_cpu_put() is sufficiently protected already.
    
      cpufreq_cpu_get(cpu)
        /* Protects against concurrent driver removal */
        read_lock_irqsave(&cpufreq_driver_lock, flags);
        policy = per_cpu(cpufreq_cpu_data, cpu);
        kobject_get(&policy->kobj);
        read_unlock_irqrestore(&cpufreq_driver_lock, flags);
    
    The reference on the policy serializes versus module unload already:
    
      cpufreq_unregister_driver()
        subsys_interface_unregister()
          __cpufreq_remove_dev_finish()
            per_cpu(cpufreq_cpu_data) = NULL;
            cpufreq_policy_put_kobj()
    
    If there is a reference held on the policy, i.e. obtained prior to the
    unregister call, then cpufreq_policy_put_kobj() will wait until that
    reference is dropped. So once subsys_interface_unregister() returns
    there is no policy pointer in flight and no new reference can be
    obtained. So that rwsem protection is useless.
    
    The other usage of cpufreq_rwsem in show()/store() of the sysfs
    interface is redundant as well because sysfs already does the proper
    kobject_get()/put() pairs.
    
    That leaves CPU hotplug versus module removal. The current
    down_write() around the write_lock() in cpufreq_unregister_driver() is
    silly at best as it protects actually nothing.
    
    The trivial solution to this is to prevent hotplug across
    cpufreq_unregister_driver completely.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 9a54f98e341d09793247a6e598012edefb5ae7cb
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 14 16:24:14 2015 -0700

    rcu: Don't disable CPU hotplug during OOM notifiers
    
    RCU's rcu_oom_notify() disables CPU hotplug in order to stabilize the
    list of online CPUs, which it traverses.  However, this is completely
    pointless because smp_call_function_single() will quietly fail if invoked
    on an offline CPU.  Because the count of requests is incremented in the
    rcu_oom_notify_cpu() function that is remotely invoked, everything works
    nicely even in the face of concurrent CPU-hotplug operations.
    
    Furthermore, in recent kernels, invoking get_online_cpus() from an OOM
    notifier can result in deadlock.  This commit therefore removes the
    call to get_online_cpus() and put_online_cpus() from rcu_oom_notify().
    
    Reported-by: Marcin lusarz <marcin.slusarz@gmail.com>
    Reported-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Tested-by: Marcin lusarz <marcin.slusarz@gmail.com>

commit 9b683874504a57cfa97558d403c75e286e20c9ce
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 11 14:50:22 2015 -0700

    rcu: Stop disabling CPU hotplug in synchronize_rcu_expedited()
    
    The fact that tasks could be migrated from leaf to root rcu_node
    structures meant that synchronize_rcu_expedited() had to disable
    CPU hotplug.  However, tasks now stay put, so this commit removes the
    CPU-hotplug disabling from synchronize_rcu_expedited().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 42846f9b74fdb743c0ed4be08dc67365f98a81d4
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Mon Sep 22 15:52:48 2014 +0100

    arm: kvm: fix CPU hotplug
    
    commit 37a34ac1d4775aafbc73b9db53c7daebbbc67e6a upstream.
    
    On some platforms with no power management capabilities, the hotplug
    implementation is allowed to return from a smp_ops.cpu_die() call as a
    function return. Upon a CPU onlining event, the KVM CPU notifier tries
    to reinstall the hyp stub, which fails on platform where no reset took
    place following a hotplug event, with the message:
    
    CPU1: smp_ops.cpu_die() returned, trying to resuscitate
    CPU1: Booted secondary processor
    Kernel panic - not syncing: unexpected prefetch abort in Hyp mode at: 0x80409540
    unexpected data abort in Hyp mode at: 0x80401fe8
    unexpected HVC/SVC trap in Hyp mode at: 0x805c6170
    
    since KVM code is trying to reinstall the stub on a system where it is
    already configured.
    
    To prevent this issue, this patch adds a check in the KVM hotplug
    notifier that detects if the HYP stub really needs re-installing when a
    CPU is onlined and skips the installation call if the stub is already in
    place, which means that the CPU has not been reset.
    
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit e3d8238d7f5c3f539a29f5ac596cd342d847e099
Merge: 4e241557fc1c 86dca36e6ba0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 24 10:02:15 2015 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Mostly refactoring/clean-up:
    
       - CPU ops and PSCI (Power State Coordination Interface) refactoring
         following the merging of the arm64 ACPI support, together with
         handling of Trusted (secure) OS instances
    
       - Using fixmap for permanent FDT mapping, removing the initial dtb
         placement requirements (within 512MB from the start of the kernel
         image).  This required moving the FDT self reservation out of the
         memreserve processing
    
       - Idmap (1:1 mapping used for MMU on/off) handling clean-up
    
       - Removing flush_cache_all() - not safe on ARM unless the MMU is off.
         Last stages of CPU power down/up are handled by firmware already
    
       - "Alternatives" (run-time code patching) refactoring and support for
         immediate branch patching, GICv3 CPU interface access
    
       - User faults handling clean-up
    
      And some fixes:
    
       - Fix for VDSO building with broken ELF toolchains
    
       - Fix another case of init_mm.pgd usage for user mappings (during
         ASID roll-over broadcasting)
    
       - Fix for FPSIMD reloading after CPU hotplug
    
       - Fix for missing syscall trace exit
    
       - Workaround for .inst asm bug
    
       - Compat fix for switching the user tls tpidr_el0 register"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (42 commits)
      arm64: use private ratelimit state along with show_unhandled_signals
      arm64: show unhandled SP/PC alignment faults
      arm64: vdso: work-around broken ELF toolchains in Makefile
      arm64: kernel: rename __cpu_suspend to keep it aligned with arm
      arm64: compat: print compat_sp instead of sp
      arm64: mm: Fix freeing of the wrong memmap entries with !SPARSEMEM_VMEMMAP
      arm64: entry: fix context tracking for el0_sp_pc
      arm64: defconfig: enable memtest
      arm64: mm: remove reference to tlb.S from comment block
      arm64: Do not attempt to use init_mm in reset_context()
      arm64: KVM: Switch vgic save/restore to alternative_insn
      arm64: alternative: Introduce feature for GICv3 CPU interface
      arm64: psci: fix !CONFIG_HOTPLUG_CPU build warning
      arm64: fix bug for reloading FPSIMD state after CPU hotplug.
      arm64: kernel thread don't need to save fpsimd context.
      arm64: fix missing syscall trace exit
      arm64: alternative: Work around .inst assembler bugs
      arm64: alternative: Merge alternative-asm.h into alternative.h
      arm64: alternative: Allow immediate branch as alternative instruction
      arm64: Rework alternate sequence for ARM erratum 845719
      ...

commit fc934d40178ad4e551a17e2733241d9f29fddd70
Merge: 052b398a43a7 085c789783f5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 14:01:01 2015 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
    
     - Continued initialization/Kconfig updates: hide most Kconfig options
       from unsuspecting users.
    
       There's now a single high level configuration option:
    
            *
            * RCU Subsystem
            *
            Make expert-level adjustments to RCU configuration (RCU_EXPERT) [N/y/?] (NEW)
    
       Which if answered in the negative, leaves us with a single
       interactive configuration option:
    
            Offload RCU callback processing from boot-selected CPUs (RCU_NOCB_CPU) [N/y/?] (NEW)
    
       All the rest of the RCU options are configured automatically.  Later
       on we'll remove this single leftover configuration option as well.
    
     - Remove all uses of RCU-protected array indexes: replace the
       rcu_[access|dereference]_index_check() APIs with READ_ONCE() and
       rcu_lockdep_assert()
    
     - RCU CPU-hotplug cleanups
    
     - Updates to Tiny RCU: a race fix and further code shrinkage.
    
     - RCU torture-testing updates: fixes, speedups, cleanups and
       documentation updates.
    
     - Miscellaneous fixes
    
     - Documentation updates
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (60 commits)
      rcutorture: Allow repetition factors in Kconfig-fragment lists
      rcutorture: Display "make oldconfig" errors
      rcutorture: Update TREE_RCU-kconfig.txt
      rcutorture: Make rcutorture scripts force RCU_EXPERT
      rcutorture: Update configuration fragments for rcutree.rcu_fanout_exact
      rcutorture: TASKS_RCU set directly, so don't explicitly set it
      rcutorture: Test SRCU cleanup code path
      rcutorture: Replace barriers with smp_store_release() and smp_load_acquire()
      locktorture: Change longdelay_us to longdelay_ms
      rcutorture: Allow negative values of nreaders to oversubscribe
      rcutorture: Exchange TREE03 and TREE08 NR_CPUS, speed up CPU hotplug
      rcutorture: Exchange TREE03 and TREE04 geometries
      locktorture: fix deadlock in 'rw_lock_irq' type
      rcu: Correctly handle non-empty Tiny RCU callback list with none ready
      rcutorture: Test both RCU-sched and RCU-bh for Tiny RCU
      rcu: Further shrink Tiny RCU by making empty functions static inlines
      rcu: Conditionally compile RCU's eqs warnings
      rcu: Remove prompt for RCU implementation
      rcu: Make RCU able to tolerate undefined CONFIG_RCU_KTHREAD_PRIO
      rcu: Make RCU able to tolerate undefined CONFIG_RCU_FANOUT_LEAF
      ...

commit 32365e64a20edcc783137ad17fdd951ab814a2fe
Author: Janet Liu <janet.liu@spreadtrum.com>
Date:   Thu Jun 11 12:02:45 2015 +0800

    arm64: fix bug for reloading FPSIMD state after CPU hotplug.
    
    Now FPSIMD don't handle HOTPLUG_CPU. This introduces bug after cpu down/up process.
    
    After cpu down/up process, the FPSMID hardware register is default value, not any
    process's fpsimd context. when CPU_DEAD set cpu's fpsimd_state to NULL, it will force
    to load the fpsimd context for the thread, to avoid the chance to skip to load the context.
    If process A is the last user process on CPU N before cpu down, and the first user process
    on the same CPU N after cpu up, A's fpsimd_state.cpu is the current cpu id,
    and per_cpu(fpsimd_last_state) points A's fpsimd_state, so kernel will not reload the
    context during it return to user space.
    
    Signed-off-by: Janet Liu <janet.liu@spreadtrum.com>
    Signed-off-by: Xiongshan An <xiongshan.an@spreadtrum.com>
    Signed-off-by: Chunyan Zhang <chunyan.zhang@spreadtrum.com>
    [catalin.marinas@arm.com: some mostly cosmetic clean-ups]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

commit 66dc54fd7d1022ef2d55f690d8501c2e2cbb6b4a
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Wed Mar 18 14:09:54 2015 +0100

    ARM: EXYNOS: make exynos_core_restart() less verbose
    
    There is a kernel message about secondary CPU bootup when
    exynos_core_restart() is called through CPU hotplug code-path (the
    only exynos_core_restart() user currently) so there is no need for
    an extra info on Exynos3250 SoC about software reset.  This also
    prepares exynos_core_restart() to be re-used in coupled cpuidle
    code-path in the future.
    
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Krzysztof Kozlowski <k.kozlowski@samsung.com>
    Signed-off-by: Kukjin Kim <kgene@kernel.org>

commit 733a572e66d2a23c852fdce34dba5bbd40667817
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 18:23:18 2015 -0400

    memcg: make mem_cgroup_read_{stat|event}() iterate possible cpus instead of online
    
    cpu_possible_mask represents the CPUs which are actually possible
    during that boot instance.  For systems which don't support CPU
    hotplug, this will match cpu_online_mask exactly in most cases.  Even
    for systems which support CPU hotplug, the number of possible CPU
    slots is highly unlikely to diverge greatly from the number of online
    CPUs.  The only cases where the difference between possible and online
    caused problems were when the boot code failed to initialize the
    possible mask and left it fully set at NR_CPUS - 1.
    
    As such, most per-cpu constructs allocate for all possible CPUs and
    often iterate over the possibles, which also has the benefit of
    avoiding the blocking CPU hotplug synchronization.
    
    memcg open codes per-cpu stat counting for mem_cgroup_read_stat() and
    mem_cgroup_read_events(), which iterates over online CPUs and handles
    CPU hotplug operations explicitly.  This complexity doesn't actually
    buy anything.  Switch to iterating over the possibles and drop the
    explicit CPU hotplug handling.
    
    Eventually, we want to convert memcg to use percpu_counter instead of
    its own custom implementation which also benefits from quick access
    w/o summing for cases where larger error margin is acceptable.
    
    This will allow mem_cgroup_read_stat() to be called from non-sleepable
    contexts which will be used by cgroup writeback.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit 6530b3f4c5043aa8ef66faa8296291b1ea6ba2eb
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Mar 12 11:42:48 2015 -0700

    rcutorture: Exchange TREE03 and TREE08 NR_CPUS, speed up CPU hotplug
    
    TREE03 has been especially effective at finding bugs lately.  This commit
    makes it even more effective by speeding up its CPU hotplug testing and
    increasing its NR_CPUs from 8 to 16.  TREE08's NR_CPUS is decreased from
    16 to 8 in order to maintain the same test duration.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit f96626beb7d2fc7c80ea7964031af708f7dcc246
Author: Ming Lei <ming.lei@canonical.com>
Date:   Tue Apr 21 10:00:20 2015 +0800

    blk-mq: fix CPU hotplug handling
    
    commit 2a34c0872adf252f23a6fef2d051a169ac796cef upstream.
    
    hctx->tags has to be set as NULL in case that it is to be unmapped
    no matter if set->tags[hctx->queue_num] is NULL or not in blk_mq_map_swqueue()
    because shared tags can be freed already from another request queue.
    
    The same situation has to be considered during handling CPU online too.
    Unmapped hw queue can be remapped after CPU topo is changed, so we need
    to allocate tags for the hw queue in blk_mq_map_swqueue(). Then tags
    allocation for hw queue can be removed in hctx cpu online notifier, and it
    is reasonable to do that after mapping is updated.
    
    Reported-by: Dongsu Park <dongsu.park@profitbricks.com>
    Tested-by: Dongsu Park <dongsu.park@profitbricks.com>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit 3e005dc47acc38c310d31a1109d13819072e81b3
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Mon Sep 22 15:52:48 2014 +0100

    arm: kvm: fix CPU hotplug
    
    commit 37a34ac1d4775aafbc73b9db53c7daebbbc67e6a upstream.
    
    On some platforms with no power management capabilities, the hotplug
    implementation is allowed to return from a smp_ops.cpu_die() call as a
    function return. Upon a CPU onlining event, the KVM CPU notifier tries
    to reinstall the hyp stub, which fails on platform where no reset took
    place following a hotplug event, with the message:
    
    CPU1: smp_ops.cpu_die() returned, trying to resuscitate
    CPU1: Booted secondary processor
    Kernel panic - not syncing: unexpected prefetch abort in Hyp mode at: 0x80409540
    unexpected data abort in Hyp mode at: 0x80401fe8
    unexpected HVC/SVC trap in Hyp mode at: 0x805c6170
    
    since KVM code is trying to reinstall the stub on a system where it is
    already configured.
    
    To prevent this issue, this patch adds a check in the KVM hotplug
    notifier that detects if the HYP stub really needs re-installing when a
    CPU is onlined and skips the installation call if the stub is already in
    place, which means that the CPU has not been reset.
    
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit e35f6f14148c09ad534d122ed32722dd431ac184
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Apr 25 04:34:48 2015 +0200

    x86/fpu: Split fpu__cpu_init() into early-boot and cpu-boot parts
    
    There are two kinds of FPU initialization sequences necessary to bring FPU
    functionality up: once per system bootup activities, such as detection,
    feature initialization, etc. of attributes that are shared by all CPUs
    in the system - and per cpu initialization sequences run when a CPU is
    brought online (either during bootup or during CPU hotplug onlining),
    such as CR0/CR4 register setting, etc.
    
    The FPU code is mixing these roles together, with no clear distinction.
    
    Start sorting this out by splitting the main FPU detection routine
    (fpu__cpu_init()) into two parts: fpu__init_system() for
    one per system init activities, and fpu__init_cpu() for the
    per CPU onlining init activities.
    
    Note that xstate_init() is called from both variants for the time being,
    because it has a dual nature as well. We'll fix that in upcoming patches.
    
    Just do the split and call it as we used to before, don't introduce any
    change in initialization behavior yet, beyond duplicate (and harmless)
    fpu__init_cpu() and xstate_init() calls - which we'll fix in later
    patches.
    
    Reviewed-by: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 69f3181a88e391dbde3ad0b0a1d8722e200e84ac
Author: Ming Lei <ming.lei@canonical.com>
Date:   Tue Apr 21 10:00:20 2015 +0800

    blk-mq: fix CPU hotplug handling
    
    commit 2a34c0872adf252f23a6fef2d051a169ac796cef upstream.
    
    hctx->tags has to be set as NULL in case that it is to be unmapped
    no matter if set->tags[hctx->queue_num] is NULL or not in blk_mq_map_swqueue()
    because shared tags can be freed already from another request queue.
    
    The same situation has to be considered during handling CPU online too.
    Unmapped hw queue can be remapped after CPU topo is changed, so we need
    to allocate tags for the hw queue in blk_mq_map_swqueue(). Then tags
    allocation for hw queue can be removed in hctx cpu online notifier, and it
    is reasonable to do that after mapping is updated.
    
    Reported-by: Dongsu Park <dongsu.park@profitbricks.com>
    Tested-by: Dongsu Park <dongsu.park@profitbricks.com>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f7bbf3add4e037d8fb5f6251b33dda6a029cbdac
Author: Ming Lei <ming.lei@canonical.com>
Date:   Tue Apr 21 10:00:19 2015 +0800

    blk-mq: fix race between timeout and CPU hotplug
    
    commit f054b56c951bf1731ba7314a4c7f1cc0b2977cc9 upstream.
    
    Firstly during CPU hotplug, even queue is freezed, timeout
    handler still may come and access hctx->tags, which may cause
    use after free, so this patch deactivates timeout handler
    inside CPU hotplug notifier.
    
    Secondly, tags can be shared by more than one queues, so we
    have to check if the hctx has been unmapped, otherwise
    still use-after-free on tags can be triggered.
    
    Reported-by: Dongsu Park <dongsu.park@profitbricks.com>
    Tested-by: Dongsu Park <dongsu.park@profitbricks.com>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 1daac193f21d6e3d0adc528a06a7e11522d4254d
Merge: 41c64bb19c74 0ff28d9f4674
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 8 19:49:35 2015 -0700

    Merge branch 'for-linus' of git://git.kernel.dk/linux-block
    
    Pull block fixes from Jens Axboe:
     "A collection of fixes since the merge window;
    
       - fix for a double elevator module release, from Chao Yu.  Ancient bug.
    
       - the splice() MORE flag fix from Christophe Leroy.
    
       - a fix for NVMe, fixing a patch that went in in the merge window.
         From Keith.
    
       - two fixes for blk-mq CPU hotplug handling, from Ming Lei.
    
       - bdi vs blockdev lifetime fix from Neil Brown, fixing and oops in md.
    
       - two blk-mq fixes from Shaohua, fixing a race on queue stop and a
         bad merge issue with FUA writes.
    
       - division-by-zero fix for writeback from Tejun.
    
       - a block bounce page accounting fix, making sure we inc/dec after
         bouncing so that pre/post IO pages match up.  From Wang YanQing"
    
    * 'for-linus' of git://git.kernel.dk/linux-block:
      splice: sendfile() at once fails for big files
      blk-mq: don't lose requests if a stopped queue restarts
      blk-mq: fix FUA request hang
      block: destroy bdi before blockdev is unregistered.
      block:bounce: fix call inc_|dec_zone_page_state on different pages confuse value of NR_BOUNCE
      elevator: fix double release of elevator module
      writeback: use |1 instead of +1 to protect against div by zero
      blk-mq: fix CPU hotplug handling
      blk-mq: fix race between timeout and CPU hotplug
      NVMe: Fix VPD B0 max sectors translation

commit 905cdf9dda5d89d843667b2f11da2308d1fd1c34
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Tue Mar 17 23:37:58 2015 -0400

    ARM: hisi/hip04: remove the MCPM overhead
    
    This platform is currently relying on the MCPM infrastructure for no
    apparent reason.  The MCPM concurrency handling brings no benefits here
    as there is no asynchronous CPU wake-ups to be concerned about (this is
    used for CPU hotplug and secondary boot only, not for CPU idle).
    
    This platform is also different from the other MCPM users because a given
    CPU can't shut itself down completely without the assistance of another
    CPU. This is at odds with the on-going MCPM backend refactoring.
    
    To simplify things, this is converted to hook directly into the
    smp_operations callbacks, bypassing the MCPM infrastructure.
    
    Tested-by: Wei Xu <xuwei5@hisilicon.com>
    Cc: Haojian Zhuang <haojian.zhuang@linaro.org>
    Signed-off-by: Nicolas Pitre <nico@linaro.org>

commit 296cffc0f8705d35c2bcb754154ee4e829de238b
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Mon Sep 22 15:52:48 2014 +0100

    arm: kvm: fix CPU hotplug
    
    commit 37a34ac1d4775aafbc73b9db53c7daebbbc67e6a upstream.
    
    On some platforms with no power management capabilities, the hotplug
    implementation is allowed to return from a smp_ops.cpu_die() call as a
    function return. Upon a CPU onlining event, the KVM CPU notifier tries
    to reinstall the hyp stub, which fails on platform where no reset took
    place following a hotplug event, with the message:
    
    CPU1: smp_ops.cpu_die() returned, trying to resuscitate
    CPU1: Booted secondary processor
    Kernel panic - not syncing: unexpected prefetch abort in Hyp mode at: 0x80409540
    unexpected data abort in Hyp mode at: 0x80401fe8
    unexpected HVC/SVC trap in Hyp mode at: 0x805c6170
    
    since KVM code is trying to reinstall the stub on a system where it is
    already configured.
    
    To prevent this issue, this patch adds a check in the KVM hotplug
    notifier that detects if the HYP stub really needs re-installing when a
    CPU is onlined and skips the installation call if the stub is already in
    place, which means that the CPU has not been reset.
    
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit 2a34c0872adf252f23a6fef2d051a169ac796cef
Author: Ming Lei <ming.lei@canonical.com>
Date:   Tue Apr 21 10:00:20 2015 +0800

    blk-mq: fix CPU hotplug handling
    
    hctx->tags has to be set as NULL in case that it is to be unmapped
    no matter if set->tags[hctx->queue_num] is NULL or not in blk_mq_map_swqueue()
    because shared tags can be freed already from another request queue.
    
    The same situation has to be considered during handling CPU online too.
    Unmapped hw queue can be remapped after CPU topo is changed, so we need
    to allocate tags for the hw queue in blk_mq_map_swqueue(). Then tags
    allocation for hw queue can be removed in hctx cpu online notifier, and it
    is reasonable to do that after mapping is updated.
    
    Cc: <stable@vger.kernel.org>
    Reported-by: Dongsu Park <dongsu.park@profitbricks.com>
    Tested-by: Dongsu Park <dongsu.park@profitbricks.com>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit f054b56c951bf1731ba7314a4c7f1cc0b2977cc9
Author: Ming Lei <ming.lei@canonical.com>
Date:   Tue Apr 21 10:00:19 2015 +0800

    blk-mq: fix race between timeout and CPU hotplug
    
    Firstly during CPU hotplug, even queue is freezed, timeout
    handler still may come and access hctx->tags, which may cause
    use after free, so this patch deactivates timeout handler
    inside CPU hotplug notifier.
    
    Secondly, tags can be shared by more than one queues, so we
    have to check if the hctx has been unmapped, otherwise
    still use-after-free on tags can be triggered.
    
    Cc: <stable@vger.kernel.org>
    Reported-by: Dongsu Park <dongsu.park@profitbricks.com>
    Tested-by: Dongsu Park <dongsu.park@profitbricks.com>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit bb0fd7ab0986105765d11baa82e619c618a235aa
Merge: bdfa54dfd9ee 4b2f8838479e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 14 21:03:26 2015 -0700

    Merge branch 'for-linus' of git://ftp.arm.linux.org.uk/~rmk/linux-arm
    
    Pull ARM updates from Russell King:
     "Included in this update are both some long term fixes and some new
      features.
    
      Fixes:
    
       - An integer overflow in the calculation of ELF_ET_DYN_BASE.
    
       - Avoiding OOMs for high-order IOMMU allocations
    
       - SMP requires the data cache to be enabled for synchronisation
         primitives to work, so prevent the CPU_DCACHE_DISABLE option being
         visible on SMP builds.
    
       - A bug going back 10+ years in the noMMU ARM94* CPU support code,
         where it corrupts registers.  Found by folk getting Linux running
         on their cameras.
    
       - Versatile Express needs an errata workaround enabled for CPU
         hot-unplug to work.
    
      Features:
    
       - Clean up module linker by handling out of range relocations
         separately from relocation cases we don't handle.
    
       - Fix a long term bug in the pci_mmap_page_range() code, which we
         hope won't impact userspace (we hope there's no users of the
         existing broken interface.)
    
       - Don't map DMA coherent allocations when we don't have a MMU.
    
       - Drop experimental status for SMP_ON_UP.
    
       - Warn when DT doesn't specify ePAPR mandatory cache properties.
    
       - Add documentation concerning how we find the start of physical
         memory for AUTO_ZRELADDR kernels, detailing why we have chosen the
         mask and the implications of changing it.
    
       - Updates from Ard Biesheuvel to address some issues with large
         kernels (such as allyesconfig) failing to link.
    
       - Allow hibernation to work on modern (ARMv7) CPUs - this appears to
         have never worked in the past on these CPUs.
    
       - Enable IRQ_SHOW_LEVEL, which changes the /proc/interrupts output
         format (hopefully without userspace breaking...  let's hope that if
         it causes someone a problem, they tell us.)
    
       - Fix tegra-ahb DT offsets.
    
       - Rework ARM errata 643719 code (and ARMv7 flush_cache_louis()/
         flush_dcache_all()) code to be more efficient, and enable this
         errata workaround by default for ARMv7+SMP CPUs.  This complements
         the Versatile Express fix above.
    
       - Rework ARMv7 context code for errata 430973, so that only Cortex A8
         CPUs are impacted by the branch target buffer flush when this
         errata is enabled.  Also update the help text to indicate that all
         r1p* A8 CPUs are impacted.
    
       - Switch ARM to the generic show_mem() implementation, it conveys all
         the information which we were already reporting.
    
       - Prevent slow timer sources being used for udelay() - timers running
         at less than 1MHz are not useful for this, and can cause udelay()
         to return immediately, without any wait.  Using such a slow timer
         is silly.
    
       - VDSO support for 32-bit ARM, mainly for gettimeofday() using the
         ARM architected timer.
    
       - Perf support for Scorpion performance monitoring units"
    
    vdso semantic conflict fixed up as per linux-next.
    
    * 'for-linus' of git://ftp.arm.linux.org.uk/~rmk/linux-arm: (52 commits)
      ARM: update errata 430973 documentation to cover Cortex A8 r1p*
      ARM: ensure delay timer has sufficient accuracy for delays
      ARM: switch to use the generic show_mem() implementation
      ARM: proc-v7: avoid errata 430973 workaround for non-Cortex A8 CPUs
      ARM: enable ARM errata 643719 workaround by default
      ARM: cache-v7: optimise test for Cortex A9 r0pX devices
      ARM: cache-v7: optimise branches in v7_flush_cache_louis
      ARM: cache-v7: consolidate initialisation of cache level index
      ARM: cache-v7: shift CLIDR to extract appropriate field before masking
      ARM: cache-v7: use movw/movt instructions
      ARM: allow 16-bit instructions in ALT_UP()
      ARM: proc-arm94*.S: fix setup function
      ARM: vexpress: fix CPU hotplug with CT9x4 tile.
      ARM: 8276/1: Make CPU_DCACHE_DISABLE depend on !SMP
      ARM: 8335/1: Documentation: DT bindings: Tegra AHB: document the legacy base address
      ARM: 8334/1: amba: tegra-ahb: detect and correct bogus base address
      ARM: 8333/1: amba: tegra-ahb: fix register offsets in the macros
      ARM: 8339/1: Enable CONFIG_GENERIC_IRQ_SHOW_LEVEL
      ARM: 8338/1: kexec: Relax SMP validation to improve DT compatibility
      ARM: 8337/1: mm: Do not invoke OOM for higher order IOMMU DMA allocations
      ...

commit 078838d56574694d0a4815d9c1b7f28e8844638b
Merge: eeee78cf77df 590ee7dbd569
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 14 13:36:04 2015 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU changes from Ingo Molnar:
     "The main changes in this cycle were:
    
       - changes permitting use of call_rcu() and friends very early in
         boot, for example, before rcu_init() is invoked.
    
       - add in-kernel API to enable and disable expediting of normal RCU
         grace periods.
    
       - improve RCU's handling of (hotplug-) outgoing CPUs.
    
       - NO_HZ_FULL_SYSIDLE fixes.
    
       - tiny-RCU updates to make it more tiny.
    
       - documentation updates.
    
       - miscellaneous fixes"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (58 commits)
      cpu: Provide smpboot_thread_init() on !CONFIG_SMP kernels as well
      cpu: Defer smpboot kthread unparking until CPU known to scheduler
      rcu: Associate quiescent-state reports with grace period
      rcu: Yet another fix for preemption and CPU hotplug
      rcu: Add diagnostics to grace-period cleanup
      rcutorture: Default to grace-period-initialization delays
      rcu: Handle outgoing CPUs on exit from idle loop
      cpu: Make CPU-offline idle-loop transition point more precise
      rcu: Eliminate ->onoff_mutex from rcu_node structure
      rcu: Process offlining and onlining only at grace-period start
      rcu: Move rcu_report_unblock_qs_rnp() to common code
      rcu: Rework preemptible expedited bitmask handling
      rcu: Remove event tracing from rcu_cpu_notify(), used by offline CPUs
      rcutorture: Enable slow grace-period initializations
      rcu: Provide diagnostic option to slow down grace-period initialization
      rcu: Detect stalls caused by failure to propagate up rcu_node tree
      rcu: Eliminate empty HOTPLUG_CPU ifdef
      rcu: Simplify sync_rcu_preempt_exp_init()
      rcu: Put all orphan-callback-related code under same comment
      rcu: Consolidate offline-CPU callback initialization
      ...

commit 49d2953c72c64182ef2dcac64f6979c0b4e25db7
Merge: cc76ee75a9d3 62a935b256f6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 13 10:47:34 2015 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "Major changes:
    
       - Reworked CPU capacity code, for better SMP load balancing on
         systems with assymetric CPUs. (Vincent Guittot, Morten Rasmussen)
    
       - Reworked RT task SMP balancing to be push based instead of pull
         based, to reduce latencies on large CPU count systems. (Steven
         Rostedt)
    
       - SCHED_DEADLINE support updates and fixes. (Juri Lelli)
    
       - SCHED_DEADLINE task migration support during CPU hotplug. (Wanpeng Li)
    
       - x86 mwait-idle optimizations and fixes. (Mike Galbraith, Len Brown)
    
       - sched/numa improvements. (Rik van Riel)
    
       - various cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (28 commits)
      sched/core: Drop debugging leftover trace_printk call
      sched/deadline: Support DL task migration during CPU hotplug
      sched/core: Check for available DL bandwidth in cpuset_cpu_inactive()
      sched/deadline: Always enqueue on previous rq when dl_task_timer() fires
      sched/core: Remove unused argument from init_[rt|dl]_rq()
      sched/deadline: Fix rt runtime corruption when dl fails its global constraints
      sched/deadline: Avoid a superfluous check
      sched: Improve load balancing in the presence of idle CPUs
      sched: Optimize freq invariant accounting
      sched: Move CFS tasks to CPUs with higher capacity
      sched: Add SD_PREFER_SIBLING for SMT level
      sched: Remove unused struct sched_group_capacity::capacity_orig
      sched: Replace capacity_factor by usage
      sched: Calculate CPU's usage statistic and put it into struct sg_lb_stats::group_usage
      sched: Add struct rq::cpu_capacity_orig
      sched: Make scale_rt invariant with frequency
      sched: Make sched entity usage tracking scale-invariant
      sched: Remove frequency scaling from cpu_capacity
      sched: Track group sched_entity usage contributions
      sched: Add sched_avg::utilization_avg_contrib
      ...

commit 00df35f991914db6b8bde8cf09808e19a9cffc3d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Apr 12 08:06:55 2015 -0700

    cpu: Defer smpboot kthread unparking until CPU known to scheduler
    
    Currently, smpboot_unpark_threads() is invoked before the incoming CPU
    has been added to the scheduler's runqueue structures.  This might
    potentially cause the unparked kthread to run on the wrong CPU, since the
    correct CPU isn't fully set up yet.
    
    That causes a sporadic, hard to debug boot crash triggering on some
    systems, reported by Borislav Petkov, and bisected down to:
    
      2a442c9c6453 ("x86: Use common outgoing-CPU-notification code")
    
    This patch places smpboot_unpark_threads() in a CPU hotplug
    notifier with priority set so that these kthreads are unparked just after
    the CPU has been added to the runqueues.
    
    Reported-and-tested-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f80c5f13970259005e64365c80c8d8f00d787c79
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Feb 24 17:58:02 2015 +1100

    powerpc/smp: Wait until secondaries are active & online
    
    commit 875ebe940d77a41682c367ad799b4f39f128d3fa upstream.
    
    Anton has a busy ppc64le KVM box where guests sometimes hit the infamous
    "kernel BUG at kernel/smpboot.c:134!" issue during boot:
    
      BUG_ON(td->cpu != smp_processor_id());
    
    Basically a per CPU hotplug thread scheduled on the wrong CPU. The oops
    output confirms it:
    
      CPU: 0
      Comm: watchdog/130
    
    The problem is that we aren't ensuring the CPU active bit is set for the
    secondary before allowing the master to continue on. The master unparks
    the secondary CPU's kthreads and the scheduler looks for a CPU to run
    on. It calls select_task_rq() and realises the suggested CPU is not in
    the cpus_allowed mask. It then ends up in select_fallback_rq(), and
    since the active bit isnt't set we choose some other CPU to run on.
    
    This seems to have been introduced by 6acbfb96976f "sched: Fix hotplug
    vs. set_cpus_allowed_ptr()", which changed from setting active before
    online to setting active after online. However that was in turn fixing a
    bug where other code assumed an active CPU was also online, so we can't
    just revert that fix.
    
    The simplest fix is just to spin waiting for both active & online to be
    set. We already have a barrier prior to set_cpu_online() (which also
    sets active), to ensure all other setup is completed before online &
    active are set.
    
    Fixes: 6acbfb96976f ("sched: Fix hotplug vs. set_cpus_allowed_ptr()")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit f6ac49ba29499387e12e864a22e6d4bf46dafe9b
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Apr 2 15:38:18 2015 +0100

    ARM: vexpress: fix CPU hotplug with CT9x4 tile.
    
    The Cortex A9 tile fails to unplug CPUs if errata 643719 is not enabled.
    This leads to random weird behaviours, but ultimately seem to lock the
    kernel one way or another when a CPU is hot unplugged.
    
    Symptoms range from a spinlock lockup in the scheduler, the entire
    system hanging, to dumping out the kernel printk buffer a few lines at
    a time, and other weird behaviours.
    
    This is caused by the outgoing CPU not having its inner caches properly
    flushed before it exits coherency - flush_cache_louis() is used to
    achieve this, but as a result of the hardware bug, this function ends
    up doing nothing without the errata workaround enabled.
    
    As the Versatile Express has an affected CPU, this errata must always
    be enabled.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 8def906044c02edcedac79aa3d6310ab4d90c4d8
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Tue Mar 31 20:49:01 2015 +0530

    timer: Don't initialize 'tvec_base' on hotplug
    
    There is no need to call init_timers_cpu() on every CPU hotplug event,
    there is not much we need to reset.
    
     - Timer-lists are already empty at the end of migrate_timers().
     - timer_jiffies will be refreshed while adding a new timer, after the
       CPU is online again.
     - active_timers and all_timers can be reset from migrate_timers().
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/54a1c30ea7b805af55beb220cadf5a07a21b0a4d.1427814611.git.viresh.kumar@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit fa9c9d10e97e38d9903fad1829535175ad261f45
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Fri Mar 27 07:08:35 2015 +0800

    sched/deadline: Support DL task migration during CPU hotplug
    
    I observed that DL tasks can't be migrated to other CPUs during CPU
    hotplug, in addition, task may/may not be running again if CPU is
    added back.
    
    The root cause which I found is that DL tasks will be throtted and
    removed from the DL rq after comsuming all their budget, which
    leads to the situation that stop task can't pick them up from the
    DL rq and migrate them to other CPUs during hotplug.
    
    The method to reproduce:
    
      schedtool -E -t 50000:100000 -e ./test
    
    Actually './test' is just a simple for loop. Then observe which CPU the
    test task is on and offline it:
    
      echo 0 > /sys/devices/system/cpu/cpuN/online
    
    This patch adds the DL task migration during CPU hotplug by finding a
    most suitable later deadline rq after DL timer fires if current rq is
    offline.
    
    If it fails to find a suitable later deadline rq then it falls back to
    any eligible online CPU in so that the deadline task will come back
    to us, and the push/pull mechanism should then move it around properly.
    
    Suggested-and-Acked-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1427411315-4298-1-git-send-email-wanpeng.li@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit fee3fd4fd2ad136b26226346c3f8b446cc120bf5
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Wed Apr 1 13:36:57 2015 +0100

    ARM: 8338/1: kexec: Relax SMP validation to improve DT compatibility
    
    When trying to kexec into a new kernel on a platform where multiple CPU
    cores are present, but no SMP bringup code is available yet, the
    kexec_load system call fails with:
    
        kexec_load failed: Invalid argument
    
    The SMP test added to machine_kexec_prepare() in commit 2103f6cba61a8b8b
    ("ARM: 7807/1: kexec: validate CPU hotplug support") wants to prohibit
    kexec on SMP platforms where it cannot disable secondary CPUs.
    However, this test is too strict: if the secondary CPUs couldn't be
    enabled in the first place, there's no need to disable them later at
    kexec time.  Hence skip the test in the absence of SMP bringup code.
    
    This allows to add all CPU cores to the DTS from the beginning, without
    having to implement SMP bringup first, improving DT compatibility.
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Acked-by: Stephen Warren <swarren@nvidia.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 2b86feb696b4c3ffa2ad60e9e154b168ec2acdd3
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Feb 24 17:58:02 2015 +1100

    powerpc/smp: Wait until secondaries are active & online
    
    commit 875ebe940d77a41682c367ad799b4f39f128d3fa upstream.
    
    Anton has a busy ppc64le KVM box where guests sometimes hit the infamous
    "kernel BUG at kernel/smpboot.c:134!" issue during boot:
    
      BUG_ON(td->cpu != smp_processor_id());
    
    Basically a per CPU hotplug thread scheduled on the wrong CPU. The oops
    output confirms it:
    
      CPU: 0
      Comm: watchdog/130
    
    The problem is that we aren't ensuring the CPU active bit is set for the
    secondary before allowing the master to continue on. The master unparks
    the secondary CPU's kthreads and the scheduler looks for a CPU to run
    on. It calls select_task_rq() and realises the suggested CPU is not in
    the cpus_allowed mask. It then ends up in select_fallback_rq(), and
    since the active bit isnt't set we choose some other CPU to run on.
    
    This seems to have been introduced by 6acbfb96976f "sched: Fix hotplug
    vs. set_cpus_allowed_ptr()", which changed from setting active before
    online to setting active after online. However that was in turn fixing a
    bug where other code assumed an active CPU was also online, so we can't
    just revert that fix.
    
    The simplest fix is just to spin waiting for both active & online to be
    set. We already have a barrier prior to set_cpu_online() (which also
    sets active), to ensure all other setup is completed before online &
    active are set.
    
    Fixes: 6acbfb96976f ("sched: Fix hotplug vs. set_cpus_allowed_ptr()")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit 7f18d6525496068dabd5f69cc3dd8d01e79c892b
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Feb 24 17:58:02 2015 +1100

    powerpc/smp: Wait until secondaries are active & online
    
    commit 875ebe940d77a41682c367ad799b4f39f128d3fa upstream.
    
    Anton has a busy ppc64le KVM box where guests sometimes hit the infamous
    "kernel BUG at kernel/smpboot.c:134!" issue during boot:
    
      BUG_ON(td->cpu != smp_processor_id());
    
    Basically a per CPU hotplug thread scheduled on the wrong CPU. The oops
    output confirms it:
    
      CPU: 0
      Comm: watchdog/130
    
    The problem is that we aren't ensuring the CPU active bit is set for the
    secondary before allowing the master to continue on. The master unparks
    the secondary CPU's kthreads and the scheduler looks for a CPU to run
    on. It calls select_task_rq() and realises the suggested CPU is not in
    the cpus_allowed mask. It then ends up in select_fallback_rq(), and
    since the active bit isnt't set we choose some other CPU to run on.
    
    This seems to have been introduced by 6acbfb96976f "sched: Fix hotplug
    vs. set_cpus_allowed_ptr()", which changed from setting active before
    online to setting active after online. However that was in turn fixing a
    bug where other code assumed an active CPU was also online, so we can't
    just revert that fix.
    
    The simplest fix is just to spin waiting for both active & online to be
    set. We already have a barrier prior to set_cpu_online() (which also
    sets active), to ensure all other setup is completed before online &
    active are set.
    
    Fixes: 6acbfb96976f ("sched: Fix hotplug vs. set_cpus_allowed_ptr()")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 915b529c7ee0d876f27a97ef83ed62039e2ade43
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Feb 24 17:58:02 2015 +1100

    powerpc/smp: Wait until secondaries are active & online
    
    commit 875ebe940d77a41682c367ad799b4f39f128d3fa upstream.
    
    Anton has a busy ppc64le KVM box where guests sometimes hit the infamous
    "kernel BUG at kernel/smpboot.c:134!" issue during boot:
    
      BUG_ON(td->cpu != smp_processor_id());
    
    Basically a per CPU hotplug thread scheduled on the wrong CPU. The oops
    output confirms it:
    
      CPU: 0
      Comm: watchdog/130
    
    The problem is that we aren't ensuring the CPU active bit is set for the
    secondary before allowing the master to continue on. The master unparks
    the secondary CPU's kthreads and the scheduler looks for a CPU to run
    on. It calls select_task_rq() and realises the suggested CPU is not in
    the cpus_allowed mask. It then ends up in select_fallback_rq(), and
    since the active bit isnt't set we choose some other CPU to run on.
    
    This seems to have been introduced by 6acbfb96976f "sched: Fix hotplug
    vs. set_cpus_allowed_ptr()", which changed from setting active before
    online to setting active after online. However that was in turn fixing a
    bug where other code assumed an active CPU was also online, so we can't
    just revert that fix.
    
    The simplest fix is just to spin waiting for both active & online to be
    set. We already have a barrier prior to set_cpu_online() (which also
    sets active), to ensure all other setup is completed before online &
    active are set.
    
    Fixes: 6acbfb96976f ("sched: Fix hotplug vs. set_cpus_allowed_ptr()")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 42528795ac1c8d7ba021797ec004904168956d64
Merge: ff382810590e 476276781095 9910affa89fe c136f991049f 654e95334049 5871968d531f 915e8a4fe45e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Mar 20 08:31:01 2015 -0700

    Merge branches 'doc.2015.02.26a', 'earlycb.2015.03.03a', 'fixes.2015.03.03a', 'gpexp.2015.02.26a', 'hotplug.2015.03.20a', 'sysidle.2015.02.26b' and 'tiny.2015.02.26a' into HEAD
    
    doc.2015.02.26a:  Documentation changes
    earlycb.2015.03.03a:  Permit early-boot RCU callbacks
    fixes.2015.03.03a:  Miscellaneous fixes
    gpexp.2015.02.26a:  In-kernel expediting of normal grace periods
    hotplug.2015.03.20a:  CPU hotplug fixes
    sysidle.2015.02.26b:  NO_HZ_FULL_SYSIDLE fixes
    tiny.2015.02.26a:  TINY_RCU fixes

commit 654e953340491e498871321d7e2c9b0a12821933
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Mar 15 09:19:35 2015 -0700

    rcu: Associate quiescent-state reports with grace period
    
    As noted in earlier commit logs, CPU hotplug operations running
    concurrently with grace-period initialization can result in a given
    leaf rcu_node structure having all CPUs offline and no blocked readers,
    but with this rcu_node structure nevertheless blocking the current
    grace period.  Therefore, the quiescent-state forcing code now checks
    for this situation and repairs it.
    
    Unfortunately, this checking can result in false positives, for example,
    when the last task has just removed itself from this leaf rcu_node
    structure, but has not yet started clearing the ->qsmask bits further
    up the structure.  This means that the grace-period kthread (which
    forces quiescent states) and some other task might be attempting to
    concurrently clear these ->qsmask bits.  This is usually not a problem:
    One of these tasks will be the first to acquire the upper-level rcu_node
    structure's lock and with therefore clear the bit, and the other task,
    seeing the bit already cleared, will stop trying to clear bits.
    
    Sadly, this means that the following unusual sequence of events -can-
    result in a problem:
    
    1.      The grace-period kthread wins, and clears the ->qsmask bits.
    
    2.      This is the last thing blocking the current grace period, so
            that the grace-period kthread clears ->qsmask bits all the way
            to the root and finds that the root ->qsmask field is now zero.
    
    3.      Another grace period is required, so that the grace period kthread
            initializes it, including setting all the needed qsmask bits.
    
    4.      The leaf rcu_node structure (the one that started this whole
            mess) is blocking this new grace period, either because it
            has at least one online CPU or because there is at least one
            task that had blocked within an RCU read-side critical section
            while running on one of this leaf rcu_node structure's CPUs.
            (And yes, that CPU might well have gone offline before the
            grace period in step (3) above started, which can mean that
            there is a task on the leaf rcu_node structure's ->blkd_tasks
            list, but ->qsmask equal to zero.)
    
    5.      The other kthread didn't get around to trying to clear the upper
            level ->qsmask bits until all the above had happened.  This means
            that it now sees bits set in the upper-level ->qsmask field, so it
            proceeds to clear them.  Too bad that it is doing so on behalf of
            a quiescent state that does not apply to the current grace period!
    
    This sequence of events can result in the new grace period being too
    short.  It can also result in the new grace period ending before the
    leaf rcu_node structure's ->qsmask bits have been cleared, which will
    result in splats during initialization of the next grace period.  In
    addition, it can result in tasks blocking the new grace period still
    being queued at the start of the next grace period, which will result
    in other splats.  Sasha's testing turned up another of these splats,
    as did rcutorture testing.  (And yes, rcutorture is being adjusted to
    make these splats show up more quickly.  Which probably is having the
    undesirable side effect of making other problems show up less quickly.
    Can't have everything!)
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: <stable@vger.kernel.org> # 4.0.x
    Tested-by: Sasha Levin <sasha.levin@oracle.com>

commit a77da14ce9afb338040b405f6ab8afddc310411d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Mar 8 14:52:27 2015 -0700

    rcu: Yet another fix for preemption and CPU hotplug
    
    As noted earlier, the following sequence of events can occur when
    running PREEMPT_RCU and HOTPLUG_CPU on a system with a multi-level
    rcu_node combining tree:
    
    1.      A group of tasks block on CPUs corresponding to a given leaf
            rcu_node structure while within RCU read-side critical sections.
    2.      All CPUs corrsponding to that rcu_node structure go offline.
    3.      The next grace period starts, but because there are still tasks
            blocked, the upper-level bits corresponding to this leaf rcu_node
            structure remain set.
    4.      All the tasks exit their RCU read-side critical sections and
            remove themselves from the leaf rcu_node structure's list,
            leaving it empty.
    5.      But because there now is code to check for this condition at
            force-quiescent-state time, the upper bits are cleared and the
            grace period completes.
    
    However, there is another complication that can occur following step 4 above:
    
    4a.     The grace period starts, and the leaf rcu_node structure's
            gp_tasks pointer is set to NULL because there are no tasks
            blocked on this structure.
    4b.     One of the CPUs corresponding to the leaf rcu_node structure
            comes back online.
    4b.     An endless stream of tasks are preempted within RCU read-side
            critical sections on this CPU, such that the ->blkd_tasks
            list is always non-empty.
    
    The grace period will never end.
    
    This commit therefore makes the force-quiescent-state processing check only
    for absence of tasks blocking the current grace period rather than absence
    of tasks altogether.  This will cause a quiescent state to be reported if
    the current leaf rcu_node structure is not blocking the current grace period
    and its parent thinks that it is, regardless of how RCU managed to get
    itself into this state.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: <stable@vger.kernel.org> # 4.0.x
    Tested-by: Sasha Levin <sasha.levin@oracle.com>

commit 0aa04b055e71bd3b8040dd71a126126c66b6f01e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jan 23 21:52:37 2015 -0800

    rcu: Process offlining and onlining only at grace-period start
    
    Races between CPU hotplug and grace periods can be difficult to resolve,
    so the ->onoff_mutex is used to exclude the two events.  Unfortunately,
    this means that it is impossible for an outgoing CPU to perform the
    last bits of its offlining from its last pass through the idle loop,
    because sleeplocks cannot be acquired in that context.
    
    This commit avoids these problems by buffering online and offline events
    in a new ->qsmaskinitnext field in the leaf rcu_node structures.  When a
    grace period starts, the events accumulated in this mask are applied to
    the ->qsmaskinit field, and, if needed, up the rcu_node tree.  The special
    case of all CPUs corresponding to a given leaf rcu_node structure being
    offline while there are still elements in that structure's ->blkd_tasks
    list is handled using a new ->wait_blkd_tasks field.  In this case,
    propagating the offline bits up the tree is deferred until the beginning
    of the grace period after all of the tasks have exited their RCU read-side
    critical sections and removed themselves from the list, at which point
    the ->wait_blkd_tasks flag is cleared.  If one of that leaf rcu_node
    structure's CPUs comes back online before the list empties, then the
    ->wait_blkd_tasks flag is simply cleared.
    
    This of course means that RCU's notion of which CPUs are offline can be
    out of date.  This is OK because RCU need only wait on CPUs that were
    online at the time that the grace period started.  In addition, RCU's
    force-quiescent-state actions will handle the case where a CPU goes
    offline after the grace period starts.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 875ebe940d77a41682c367ad799b4f39f128d3fa
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Feb 24 17:58:02 2015 +1100

    powerpc/smp: Wait until secondaries are active & online
    
    Anton has a busy ppc64le KVM box where guests sometimes hit the infamous
    "kernel BUG at kernel/smpboot.c:134!" issue during boot:
    
      BUG_ON(td->cpu != smp_processor_id());
    
    Basically a per CPU hotplug thread scheduled on the wrong CPU. The oops
    output confirms it:
    
      CPU: 0
      Comm: watchdog/130
    
    The problem is that we aren't ensuring the CPU active bit is set for the
    secondary before allowing the master to continue on. The master unparks
    the secondary CPU's kthreads and the scheduler looks for a CPU to run
    on. It calls select_task_rq() and realises the suggested CPU is not in
    the cpus_allowed mask. It then ends up in select_fallback_rq(), and
    since the active bit isnt't set we choose some other CPU to run on.
    
    This seems to have been introduced by 6acbfb96976f "sched: Fix hotplug
    vs. set_cpus_allowed_ptr()", which changed from setting active before
    online to setting active after online. However that was in turn fixing a
    bug where other code assumed an active CPU was also online, so we can't
    just revert that fix.
    
    The simplest fix is just to spin waiting for both active & online to be
    set. We already have a barrier prior to set_cpu_online() (which also
    sets active), to ensure all other setup is completed before online &
    active are set.
    
    Fixes: 6acbfb96976f ("sched: Fix hotplug vs. set_cpus_allowed_ptr()")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 23e8fe2e16587494268510c1bc9f6952f50f0311
Merge: 30d46827c274 f49028292c13
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 9 14:28:42 2015 -0800

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The main RCU changes in this cycle are:
    
       - Documentation updates.
    
       - Miscellaneous fixes.
    
       - Preemptible-RCU fixes, including fixing an old bug in the
         interaction of RCU priority boosting and CPU hotplug.
    
       - SRCU updates.
    
       - RCU CPU stall-warning updates.
    
       - RCU torture-test updates"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (54 commits)
      rcu: Initialize tiny RCU stall-warning timeouts at boot
      rcu: Fix RCU CPU stall detection in tiny implementation
      rcu: Add GP-kthread-starvation checks to CPU stall warnings
      rcu: Make cond_resched_rcu_qs() apply to normal RCU flavors
      rcu: Optionally run grace-period kthreads at real-time priority
      ksoftirqd: Use new cond_resched_rcu_qs() function
      ksoftirqd: Enable IRQs and call cond_resched() before poking RCU
      rcutorture: Add more diagnostics in rcu_barrier() test failure case
      torture: Flag console.log file to prevent holdovers from earlier runs
      torture: Add "-enable-kvm -soundhw pcspk" to qemu command line
      rcutorture: Handle different mpstat versions
      rcutorture: Check from beginning to end of grace period
      rcu: Remove redundant rcu_batches_completed() declaration
      rcutorture: Drop rcu_torture_completed() and friends
      rcu: Provide rcu_batches_completed_sched() for TINY_RCU
      rcutorture: Use unsigned for Reader Batch computations
      rcutorture: Make build-output parsing correctly flag RCU's warnings
      rcu: Make _batches_completed() functions return unsigned long
      rcutorture: Issue warnings on close calls due to Reader Batch blows
      documentation: Fix smp typo in memory-barriers.txt
      ...

commit 29f12c48df4e6cba9df39dbe9d99649be27fb346
Merge: 2af613d3739b 135818bf494e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 6 13:06:10 2015 -0800

    Merge branch 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core kernel fixes from Ingo Molnar:
     "Two liblockdep fixes and a CPU hotplug race fix"
    
    * 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      tools/liblockdep: don't include host headers
      tools/liblockdep: ignore generated .so file
      smpboot: Add missing get_online_cpus() in smpboot_register_percpu_thread()

commit ecf5636dcd59cd5508641f995cc4c2bafedbb995
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Thu Feb 5 13:44:48 2015 +0800

    ACPI: Add interfaces to parse IOAPIC ID for IOAPIC hotplug
    
    We need to parse APIC ID for IOAPIC registration for IOAPIC hotplug.
    ACPI _MAT method and MADT table are used to figure out IOAPIC ID, just
    like parsing CPU APIC ID for CPU hotplug.
    
    [ tglx: Fixed docbook comment ]
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Len Brown <lenb@kernel.org>
    Link: http://lkml.kernel.org/r/1414387308-27148-8-git-send-email-jiang.liu@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 712eddf70225ab5ae65e946e22d2dfe6b93e8dd1
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Sat Jan 24 14:05:50 2015 +0900

    cpuidle: exynos: add coupled cpuidle support for exynos4210
    
    The following patch adds coupled cpuidle support for Exynos4210 to
    an existing cpuidle-exynos driver.  As a result it enables AFTR mode
    to be used by default on Exynos4210 without the need to hot unplug
    CPU1 first.
    
    The patch is heavily based on earlier cpuidle-exynos4210 driver from
    Daniel Lezcano:
    
    http://www.spinics.net/lists/linux-samsung-soc/msg28134.html
    
    Changes from Daniel's code include:
    - porting code to current kernels
    - fixing it to work on my setup (by using S5P_INFORM register
      instead of S5P_VA_SYSRAM one on Revison 1.1 and retrying poking
      CPU1 out of the BOOT ROM if necessary)
    - fixing rare lockup caused by waiting for CPU1 to get stuck in
      the BOOT ROM (CPU hotplug code in arch/arm/mach-exynos/platsmp.c
      doesn't require this and works fine)
    - moving Exynos specific code to arch/arm/mach-exynos/pm.c
    - using cpu_boot_reg_base() helper instead of BOOT_VECTOR macro
    - using exynos_cpu_*() helpers instead of accessing registers
      directly
    - using arch_send_wakeup_ipi_mask() instead of dsb_sev()
      (this matches CPU hotplug code in arch/arm/mach-exynos/platsmp.c)
    - integrating separate exynos4210-cpuidle driver into existing
      exynos-cpuidle one
    
    Cc: Colin Cross <ccross@google.com>
    Cc: Kukjin Kim <kgene.kim@samsung.com>
    Cc: Krzysztof Kozlowski <k.kozlowski@samsung.com>
    Cc: Tomasz Figa <tomasz.figa@gmail.com>
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Kukjin Kim <kgene@kernel.org>

commit f49028292c13b958fdf4f36c8cc8119d0dde187b
Merge: eef8f4c2acac 78e691f4ae2d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Jan 21 06:12:21 2015 +0100

    Merge branch 'for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into core/rcu
    
    Pull RCU updates from Paul E. McKenney:
    
      - Documentation updates.
    
      - Miscellaneous fixes.
    
      - Preemptible-RCU fixes, including fixing an old bug in the
        interaction of RCU priority boosting and CPU hotplug.
    
      - SRCU updates.
    
      - RCU CPU stall-warning updates.
    
      - RCU torture-test updates.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit fa5e4747af360dc65fdded3cc0ff1a6b8c227a71
Author: Takashi Iwai <tiwai@suse.de>
Date:   Wed Dec 10 16:38:30 2014 +0100

    blk-mq: Fix uninitialized kobject at CPU hotplugging
    
    commit 06a41a99d13d8e919e9a00a4849e6b85ae492592 upstream.
    
    When a CPU is hotplugged, the current blk-mq spews a warning like:
    
      kobject '(null)' (ffffe8ffffc8b5d8): tried to add an uninitialized object, something is seriously wrong.
      CPU: 1 PID: 1386 Comm: systemd-udevd Not tainted 3.18.0-rc7-2.g088d59b-default #1
      Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.7.5-20140531_171129-lamiak 04/01/2014
       0000000000000000 0000000000000002 ffffffff81605f07 ffffe8ffffc8b5d8
       ffffffff8132c7a0 ffff88023341d370 0000000000000020 ffff8800bb05bd58
       ffff8800bb05bd08 000000000000a0a0 000000003f441940 0000000000000007
      Call Trace:
       [<ffffffff81005306>] dump_trace+0x86/0x330
       [<ffffffff81005644>] show_stack_log_lvl+0x94/0x170
       [<ffffffff81006d21>] show_stack+0x21/0x50
       [<ffffffff81605f07>] dump_stack+0x41/0x51
       [<ffffffff8132c7a0>] kobject_add+0xa0/0xb0
       [<ffffffff8130aee1>] blk_mq_register_hctx+0x91/0xb0
       [<ffffffff8130b82e>] blk_mq_sysfs_register+0x3e/0x60
       [<ffffffff81309298>] blk_mq_queue_reinit_notify+0xf8/0x190
       [<ffffffff8107cfdc>] notifier_call_chain+0x4c/0x70
       [<ffffffff8105fd23>] cpu_notify+0x23/0x50
       [<ffffffff81060037>] _cpu_up+0x157/0x170
       [<ffffffff810600d9>] cpu_up+0x89/0xb0
       [<ffffffff815fa5b5>] cpu_subsys_online+0x35/0x80
       [<ffffffff814323cd>] device_online+0x5d/0xa0
       [<ffffffff81432485>] online_store+0x75/0x80
       [<ffffffff81236a5a>] kernfs_fop_write+0xda/0x150
       [<ffffffff811c5532>] vfs_write+0xb2/0x1f0
       [<ffffffff811c5f42>] SyS_write+0x42/0xb0
       [<ffffffff8160c4ed>] system_call_fastpath+0x16/0x1b
       [<00007f0132fb24e0>] 0x7f0132fb24e0
    
    This is indeed because of an uninitialized kobject for blk_mq_ctx.
    The blk_mq_ctx kobjects are initialized in blk_mq_sysfs_init(), but it
    goes loop over hctx_for_each_ctx(), i.e. it initializes only for
    online CPUs.  Thus, when a CPU is hotplugged, the ctx for the newly
    onlined CPU is registered without initialization.
    
    This patch fixes the issue by initializing the all ctx kobjects
    belonging to each queue.
    
    Bugzilla: https://bugzilla.novell.com/show_bug.cgi?id=908794
    Signed-off-by: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 48a9ccb7ee8a7d141efe00d4ce621c5e974e1b3a
Author: Takashi Iwai <tiwai@suse.de>
Date:   Wed Dec 10 16:38:30 2014 +0100

    blk-mq: Fix uninitialized kobject at CPU hotplugging
    
    commit 06a41a99d13d8e919e9a00a4849e6b85ae492592 upstream.
    
    When a CPU is hotplugged, the current blk-mq spews a warning like:
    
      kobject '(null)' (ffffe8ffffc8b5d8): tried to add an uninitialized object, something is seriously wrong.
      CPU: 1 PID: 1386 Comm: systemd-udevd Not tainted 3.18.0-rc7-2.g088d59b-default #1
      Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.7.5-20140531_171129-lamiak 04/01/2014
       0000000000000000 0000000000000002 ffffffff81605f07 ffffe8ffffc8b5d8
       ffffffff8132c7a0 ffff88023341d370 0000000000000020 ffff8800bb05bd58
       ffff8800bb05bd08 000000000000a0a0 000000003f441940 0000000000000007
      Call Trace:
       [<ffffffff81005306>] dump_trace+0x86/0x330
       [<ffffffff81005644>] show_stack_log_lvl+0x94/0x170
       [<ffffffff81006d21>] show_stack+0x21/0x50
       [<ffffffff81605f07>] dump_stack+0x41/0x51
       [<ffffffff8132c7a0>] kobject_add+0xa0/0xb0
       [<ffffffff8130aee1>] blk_mq_register_hctx+0x91/0xb0
       [<ffffffff8130b82e>] blk_mq_sysfs_register+0x3e/0x60
       [<ffffffff81309298>] blk_mq_queue_reinit_notify+0xf8/0x190
       [<ffffffff8107cfdc>] notifier_call_chain+0x4c/0x70
       [<ffffffff8105fd23>] cpu_notify+0x23/0x50
       [<ffffffff81060037>] _cpu_up+0x157/0x170
       [<ffffffff810600d9>] cpu_up+0x89/0xb0
       [<ffffffff815fa5b5>] cpu_subsys_online+0x35/0x80
       [<ffffffff814323cd>] device_online+0x5d/0xa0
       [<ffffffff81432485>] online_store+0x75/0x80
       [<ffffffff81236a5a>] kernfs_fop_write+0xda/0x150
       [<ffffffff811c5532>] vfs_write+0xb2/0x1f0
       [<ffffffff811c5f42>] SyS_write+0x42/0xb0
       [<ffffffff8160c4ed>] system_call_fastpath+0x16/0x1b
       [<00007f0132fb24e0>] 0x7f0132fb24e0
    
    This is indeed because of an uninitialized kobject for blk_mq_ctx.
    The blk_mq_ctx kobjects are initialized in blk_mq_sysfs_init(), but it
    goes loop over hctx_for_each_ctx(), i.e. it initializes only for
    online CPUs.  Thus, when a CPU is hotplugged, the ctx for the newly
    onlined CPU is registered without initialization.
    
    This patch fixes the issue by initializing the all ctx kobjects
    belonging to each queue.
    
    Bugzilla: https://bugzilla.novell.com/show_bug.cgi?id=908794
    Signed-off-by: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Signed-off-by: Luis Henriques <luis.henriques@canonical.com>

commit d19fb8d1f3f66cc342d30aa48f090c70afb753ed
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Oct 31 12:56:16 2014 -0700

    rcu: Don't migrate blocked tasks even if all corresponding CPUs offline
    
    When the last CPU associated with a given leaf rcu_node structure
    goes offline, something must be done about the tasks queued on that
    rcu_node structure.  Each of these tasks has been preempted on one of
    the leaf rcu_node structure's CPUs while in an RCU read-side critical
    section that it have not yet exited.  Handling these tasks is the job of
    rcu_preempt_offline_tasks(), which migrates them from the leaf rcu_node
    structure to the root rcu_node structure.
    
    Unfortunately, this migration has to be done one task at a time because
    each tasks allegiance must be shifted from the original leaf rcu_node to
    the root, so that future attempts to deal with these tasks will acquire
    the root rcu_node structure's ->lock rather than that of the leaf.
    Worse yet, this migration must be done with interrupts disabled, which
    is not so good for realtime response, especially given that there is
    no bound on the number of tasks on a given rcu_node structure's list.
    (OK, OK, there is a bound, it is just that it is unreasonably large,
    especially on 64-bit systems.)  This was not considered a problem back
    when rcu_preempt_offline_tasks() was first written because realtime
    systems were assumed not to do CPU-hotplug operations while real-time
    applications were running.  This assumption has proved of dubious validity
    given that people are starting to run multiple realtime applications
    on a single SMP system and that it is common practice to offline then
    online a CPU before starting its real-time application in order to clear
    extraneous processing off of that CPU.  So we now need CPU hotplug
    operations to avoid undue latencies.
    
    This commit therefore avoids migrating these tasks, instead letting
    them be dequeued one by one from the original leaf rcu_node structure
    by rcu_read_unlock_special().  This means that the clearing of bits
    from the upper-level rcu_node structures must be deferred until the
    last such task has been dequeued, because otherwise subsequent grace
    periods won't wait on them.  This commit has the beneficial side effect
    of simplifying the CPU-hotplug code for TREE_PREEMPT_RCU, especially in
    CONFIG_RCU_BOOST builds.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 87af9e7ff9d909e70a006ca0974466e2a1d8db0a
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Dec 12 10:11:44 2014 +0100

    hotplugcpu: Avoid deadlocks by waking active_writer
    
    Commit b2c4623dcd07 ("rcu: More on deadlock between CPU hotplug and expedited
    grace periods") introduced another problem that can easily be reproduced by
    starting/stopping cpus in a loop.
    
    E.g.:
      for i in `seq 5000`; do
          echo 1 > /sys/devices/system/cpu/cpu1/online
          echo 0 > /sys/devices/system/cpu/cpu1/online
      done
    
    Will result in:
      INFO: task /cpu_start_stop:1 blocked for more than 120 seconds.
      Call Trace:
      ([<00000000006a028e>] __schedule+0x406/0x91c)
       [<0000000000130f60>] cpu_hotplug_begin+0xd0/0xd4
       [<0000000000130ff6>] _cpu_up+0x3e/0x1c4
       [<0000000000131232>] cpu_up+0xb6/0xd4
       [<00000000004a5720>] device_online+0x80/0xc0
       [<00000000004a57f0>] online_store+0x90/0xb0
      ...
    
    And a deadlock.
    
    Problem is that if the last ref in put_online_cpus() can't get the
    cpu_hotplug.lock the puts_pending count is incremented, but a sleeping
    active_writer might never be woken up, therefore never exiting the loop in
    cpu_hotplug_begin().
    
    This fix removes puts_pending and turns refcount into an atomic variable. We
    also introduce a wait queue for the active_writer, to avoid possible races and
    use-after-free. There is no need to take the lock in put_online_cpus() anymore.
    
    Can't reproduce it with this fix.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit caf292ae5bb9d57198ce001d8b762f7abae3a94d
Merge: 8f4385d590d4 fcbf6a087a7e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 13 14:14:23 2014 -0800

    Merge branch 'for-3.19/core' of git://git.kernel.dk/linux-block
    
    Pull block driver core update from Jens Axboe:
     "This is the pull request for the core block IO changes for 3.19.  Not
      a huge round this time, mostly lots of little good fixes:
    
       - Fix a bug in sysfs blktrace interface causing a NULL pointer
         dereference, when enabled/disabled through that API.  From Arianna
         Avanzini.
    
       - Various updates/fixes/improvements for blk-mq:
    
            - A set of updates from Bart, mostly fixing buts in the tag
              handling.
    
            - Cleanup/code consolidation from Christoph.
    
            - Extend queue_rq API to be able to handle batching issues of IO
              requests. NVMe will utilize this shortly. From me.
    
            - A few tag and request handling updates from me.
    
            - Cleanup of the preempt handling for running queues from Paolo.
    
            - Prevent running of unmapped hardware queues from Ming Lei.
    
            - Move the kdump memory limiting check to be in the correct
              location, from Shaohua.
    
            - Initialize all software queues at init time from Takashi. This
              prevents a kobject warning when CPUs are brought online that
              weren't online when a queue was registered.
    
       - Single writeback fix for I_DIRTY clearing from Tejun.  Queued with
         the core IO changes, since it's just a single fix.
    
       - Version X of the __bio_add_page() segment addition retry from
         Maurizio.  Hope the Xth time is the charm.
    
       - Documentation fixup for IO scheduler merging from Jan.
    
       - Introduce (and use) generic IO stat accounting helpers for non-rq
         drivers, from Gu Zheng.
    
       - Kill off artificial limiting of max sectors in a request from
         Christoph"
    
    * 'for-3.19/core' of git://git.kernel.dk/linux-block: (26 commits)
      bio: modify __bio_add_page() to accept pages that don't start a new segment
      blk-mq: Fix uninitialized kobject at CPU hotplugging
      blktrace: don't let the sysfs interface remove trace from running list
      blk-mq: Use all available hardware queues
      blk-mq: Micro-optimize bt_get()
      blk-mq: Fix a race between bt_clear_tag() and bt_get()
      blk-mq: Avoid that __bt_get_word() wraps multiple times
      blk-mq: Fix a use-after-free
      blk-mq: prevent unmapped hw queue from being scheduled
      blk-mq: re-check for available tags after running the hardware queue
      blk-mq: fix hang in bt_get()
      blk-mq: move the kdump check to blk_mq_alloc_tag_set
      blk-mq: cleanup tag free handling
      blk-mq: use 'nr_cpu_ids' as highest CPU ID count for hwq <-> cpu map
      blk: introduce generic io stat accounting help function
      blk-mq: handle the single queue case in blk_mq_hctx_next_cpu
      genhd: check for int overflow in disk_expand_part_tbl()
      blk-mq: add blk_mq_free_hctx_request()
      blk-mq: export blk_mq_free_request()
      blk-mq: use get_cpu/put_cpu instead of preempt_disable/preempt_enable
      ...

commit 06a41a99d13d8e919e9a00a4849e6b85ae492592
Author: Takashi Iwai <tiwai@suse.de>
Date:   Wed Dec 10 16:38:30 2014 +0100

    blk-mq: Fix uninitialized kobject at CPU hotplugging
    
    When a CPU is hotplugged, the current blk-mq spews a warning like:
    
      kobject '(null)' (ffffe8ffffc8b5d8): tried to add an uninitialized object, something is seriously wrong.
      CPU: 1 PID: 1386 Comm: systemd-udevd Not tainted 3.18.0-rc7-2.g088d59b-default #1
      Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.7.5-20140531_171129-lamiak 04/01/2014
       0000000000000000 0000000000000002 ffffffff81605f07 ffffe8ffffc8b5d8
       ffffffff8132c7a0 ffff88023341d370 0000000000000020 ffff8800bb05bd58
       ffff8800bb05bd08 000000000000a0a0 000000003f441940 0000000000000007
      Call Trace:
       [<ffffffff81005306>] dump_trace+0x86/0x330
       [<ffffffff81005644>] show_stack_log_lvl+0x94/0x170
       [<ffffffff81006d21>] show_stack+0x21/0x50
       [<ffffffff81605f07>] dump_stack+0x41/0x51
       [<ffffffff8132c7a0>] kobject_add+0xa0/0xb0
       [<ffffffff8130aee1>] blk_mq_register_hctx+0x91/0xb0
       [<ffffffff8130b82e>] blk_mq_sysfs_register+0x3e/0x60
       [<ffffffff81309298>] blk_mq_queue_reinit_notify+0xf8/0x190
       [<ffffffff8107cfdc>] notifier_call_chain+0x4c/0x70
       [<ffffffff8105fd23>] cpu_notify+0x23/0x50
       [<ffffffff81060037>] _cpu_up+0x157/0x170
       [<ffffffff810600d9>] cpu_up+0x89/0xb0
       [<ffffffff815fa5b5>] cpu_subsys_online+0x35/0x80
       [<ffffffff814323cd>] device_online+0x5d/0xa0
       [<ffffffff81432485>] online_store+0x75/0x80
       [<ffffffff81236a5a>] kernfs_fop_write+0xda/0x150
       [<ffffffff811c5532>] vfs_write+0xb2/0x1f0
       [<ffffffff811c5f42>] SyS_write+0x42/0xb0
       [<ffffffff8160c4ed>] system_call_fastpath+0x16/0x1b
       [<00007f0132fb24e0>] 0x7f0132fb24e0
    
    This is indeed because of an uninitialized kobject for blk_mq_ctx.
    The blk_mq_ctx kobjects are initialized in blk_mq_sysfs_init(), but it
    goes loop over hctx_for_each_ctx(), i.e. it initializes only for
    online CPUs.  Thus, when a CPU is hotplugged, the ctx for the newly
    onlined CPU is registered without initialization.
    
    This patch fixes the issue by initializing the all ctx kobjects
    belonging to each queue.
    
    Bugzilla: https://bugzilla.novell.com/show_bug.cgi?id=908794
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Takashi Iwai <tiwai@suse.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit 6cd94d5e57ab97ddd672b707ab4bb639672c1727
Merge: 6c9e92476bc9 842f7d2c4d39
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 9 14:38:28 2014 -0800

    Merge tag 'soc-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull ARM SoC platform changes from Arnd Bergmann:
     "New and updated SoC support, notable changes include:
    
       - bcm:
            brcmstb SMP support
            initial iproc/cygnus support
       - exynos:
            Exynos4415 SoC support
            PMU and suspend support for Exynos5420
            PMU support for Exynos3250
            pm related maintenance
       - imx:
            new LS1021A SoC support
            vybrid 610 global timer support
       - integrator:
            convert to using multiplatform configuration
       - mediatek:
            earlyprintk support for mt8127/mt8135
       - meson:
            meson8 soc and l2 cache controller support
       - mvebu:
            Armada 38x CPU hotplug support
            drop support for prerelease Armada 375 Z1 stepping
            extended suspend support, now works on Armada 370/XP
       - omap:
            hwmod related maintenance
            prcm cleanup
       - pxa:
            initial pxa27x DT handling
       - rockchip:
            SMP support for rk3288
            add cpu frequency scaling support
       - shmobile:
            r8a7740 power domain support
            various small restart, timer, pci apmu changes
       - sunxi:
            Allwinner A80 (sun9i) earlyprintk support
       - ux500:
            power domain support
    
      Overall, a significant chunk of changes, coming mostly from the usual
      suspects: omap, shmobile, samsung and mvebu, all of which already
      contain a lot of platform specific code in arch/arm"
    
    * tag 'soc-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (187 commits)
      ARM: mvebu: use the cpufreq-dt platform_data for independent clocks
      soc: integrator: Add terminating entry for integrator_cm_match
      ARM: mvebu: add SDRAM controller description for Armada XP
      ARM: mvebu: adjust mbus controller description on Armada 370/XP
      ARM: mvebu: add suspend/resume DT information for Armada XP GP
      ARM: mvebu: synchronize secondary CPU clocks on resume
      ARM: mvebu: make sure MMU is disabled in armada_370_xp_cpu_resume
      ARM: mvebu: Armada XP GP specific suspend/resume code
      ARM: mvebu: reserve the first 10 KB of each memory bank for suspend/resume
      ARM: mvebu: implement suspend/resume support for Armada XP
      clk: mvebu: add suspend/resume for gatable clocks
      bus: mvebu-mbus: provide a mechanism to save SDRAM window configuration
      bus: mvebu-mbus: suspend/resume support
      clocksource: time-armada-370-xp: add suspend/resume support
      irqchip: armada-370-xp: Add suspend/resume support
      ARM: add lolevel debug support for asm9260
      ARM: add mach-asm9260
      ARM: EXYNOS: use u8 for val[] in struct exynos_pmu_conf
      power: reset: imx-snvs-poweroff: add power off driver for i.mx6
      ARM: imx: temporarily remove CONFIG_SOC_FSL from LS1021A
      ...

commit 7c5c92ed56d932b2c19c3f8aea86369509407d33
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Dec 9 10:58:19 2014 +1100

    powerpc: Secondary CPUs must set cpu_callin_map after setting active and online
    
    I have a busy ppc64le KVM box where guests sometimes hit the infamous
    "kernel BUG at kernel/smpboot.c:134!" issue during boot:
    
      BUG_ON(td->cpu != smp_processor_id());
    
    Basically a per CPU hotplug thread scheduled on the wrong CPU. The oops
    output confirms it:
    
      CPU: 0
      Comm: watchdog/130
    
    The problem is that we aren't ensuring the CPU active and online bits are set
    before allowing the master to continue on. The master unparks the secondary
    CPUs kthreads and the scheduler looks for a CPU to run on. It calls
    select_task_rq and realises the suggested CPU is not in the cpus_allowed
    mask. It then ends up in select_fallback_rq, and since the active and
    online bits aren't set we choose some other CPU to run on.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit f4d8504c6629c83dd6eec43a2eb7f34b9bae09a7
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Wed Sep 24 16:38:05 2014 +0800

    sched: Fix unreleased llc_shared_mask bit during CPU hotplug
    
    commit 03bd4e1f7265548832a76e7919a81f3137c44fd1 upstream.
    
    The following bug can be triggered by hot adding and removing a large number of
    xen domain0's vcpus repeatedly:
    
            BUG: unable to handle kernel NULL pointer dereference at 0000000000000004 IP: [..] find_busiest_group
            PGD 5a9d5067 PUD 13067 PMD 0
            Oops: 0000 [#3] SMP
            [...]
            Call Trace:
            load_balance
            ? _raw_spin_unlock_irqrestore
            idle_balance
            __schedule
            schedule
            schedule_timeout
            ? lock_timer_base
            schedule_timeout_uninterruptible
            msleep
            lock_device_hotplug_sysfs
            online_store
            dev_attr_store
            sysfs_write_file
            vfs_write
            SyS_write
            system_call_fastpath
    
    Last level cache shared mask is built during CPU up and the
    build_sched_domain() routine takes advantage of it to setup
    the sched domain CPU topology.
    
    However, llc_shared_mask is not released during CPU disable,
    which leads to an invalid sched domainCPU topology.
    
    This patch fix it by releasing the llc_shared_mask correctly
    during CPU disable.
    
    Yasuaki also reported that this can happen on real hardware:
    
      https://lkml.org/lkml/2014/7/22/1018
    
    His case is here:
    
            ==
            Here is an example on my system.
            My system has 4 sockets and each socket has 15 cores and HT is
            enabled. In this case, each core of sockes is numbered as
            follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-44, 90-104
            Socket#3 | 45-59, 105-119
    
            Then llc_shared_mask of CPU#30 has 0x3fff80000001fffc0000000.
    
            It means that last level cache of Socket#2 is shared with
            CPU#30-44 and 90-104.
    
            When hot-removing socket#2 and #3, each core of sockets is
            numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
    
            But llc_shared_mask is not cleared. So llc_shared_mask of CPU#30
            remains having 0x3fff80000001fffc0000000.
    
            After that, when hot-adding socket#2 and #3, each core of
            sockets is numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-59
            Socket#3 | 90-119
    
            Then llc_shared_mask of CPU#30 becomes
            0x3fff8000fffffffc0000000. It means that last level cache of
            Socket#2 is shared with CPU#30-59 and 90-104. So the mask has
            the wrong value.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Tested-by: Linn Crosetto <linn@hp.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1411547885-48165-1-git-send-email-wanpeng.li@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Zefan Li <lizefan@huawei.com>

commit a51b4d7710d1a3593c3bdc4592fdecbbb8df4f16
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Wed Sep 3 15:04:28 2014 +0200

    ACPI / cpuidle: fix deadlock between cpuidle_lock and cpu_hotplug.lock
    
    commit 6726655dfdd2dc60c035c690d9f10cb69d7ea075 upstream.
    
    There is a following AB-BA dependency between cpu_hotplug.lock and
    cpuidle_lock:
    
    1) cpu_hotplug.lock -> cpuidle_lock
    enable_nonboot_cpus()
     _cpu_up()
      cpu_hotplug_begin()
       LOCK(cpu_hotplug.lock)
     cpu_notify()
      ...
      acpi_processor_hotplug()
       cpuidle_pause_and_lock()
        LOCK(cpuidle_lock)
    
    2) cpuidle_lock -> cpu_hotplug.lock
    acpi_os_execute_deferred() workqueue
     ...
     acpi_processor_cst_has_changed()
      cpuidle_pause_and_lock()
       LOCK(cpuidle_lock)
      get_online_cpus()
       LOCK(cpu_hotplug.lock)
    
    Fix this by reversing the order acpi_processor_cst_has_changed() does
    thigs -- let it first execute the protection against CPU hotplug by
    calling get_online_cpus() and obtain the cpuidle lock only after that (and
    perform the symmentric change when allowing CPUs hotplug again and
    dropping cpuidle lock).
    
    Spotted by lockdep.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Zefan Li <lizefan@huawei.com>

commit 77ea46d1331e5b46ff4dd98e7296eb17355cff75
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Fri Nov 21 17:00:09 2014 +0100

    ARM: mvebu: make sure MMU is disabled in armada_370_xp_cpu_resume
    
    The armada_370_xp_cpu_resume() until now was used only as the function
    called by the SoC when returning from a deep idle state (as used in
    cpuidle, or when the CPU is brought offline using CPU hotplug).
    
    However, it is now also used when exiting the suspend to RAM state. In
    this case, it is the bootloader that calls back into this function,
    with the MMU left enabled by the BootROM. Having the MMU enabled when
    entering this function confuses the kerrnel because we are not using
    the kernel page tables at this point, but in other mvebu functions we
    use the information on whether the MMU is enabled or not to find out
    whether we should talk to the coherency fabric using a physical
    address or a virtual address. To fix that, we simply disable the MMU
    when entering this function, so that the kernel is in an expected
    situation.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Acked-by: Gregory CLEMENT <gregory.clement@free-electrons.com>
    Link: https://lkml.kernel.org/r/1416585613-2113-13-git-send-email-thomas.petazzoni@free-electrons.com
    Signed-off-by: Jason Cooper <jason@lakedaemon.net>

commit 756f80cee766574ae282baa97fdcf9cc6d0cc70c
Merge: d5bd4e8df43c 626d686487bf
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Nov 28 22:25:11 2014 +0100

    Merge tag 'mvebu-soc-3.19' of git://git.infradead.org/linux-mvebu into next/soc
    
    Pull "mvebu SoC changes for v3.19" from Jason Cooper:
    
     - Armada 38x
        - Implement CPU hotplug support
    
     - Armada 375
        - Remove Z1 stepping support (limited dist. of SoC)
    
    * tag 'mvebu-soc-3.19' of git://git.infradead.org/linux-mvebu:
      ARM: mvebu: Implement the CPU hotplug support for the Armada 38x SoCs
      ARM: mvebu: Fix the secondary startup for Cortex A9 SoC
      ARM: mvebu: Move SCU power up in a function
      ARM: mvebu: Clean-up the Armada XP support
      ARM: mvebu: update comments in coherency.c
      ARM: mvebu: remove Armada 375 Z1 workaround for I/O coherency
      ARM: mvebu: remove unused register offset definition
      ARM: mvebu: disable I/O coherency on non-SMP situations on Armada 370/375/38x/XP
      ARM: mvebu: make the coherency_ll.S functions work with no coherency fabric
      ARM: mvebu: Remove thermal quirk for A375 Z1 revision
      ARM: mvebu: add missing of_node_put() call in coherency.c
      ARM: orion: Fix for certain sequence of request_irq can cause irq storm
      ARM: mvebu: armada xp: Generalize use of i2c quirk
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 626d686487bfd8136c4543bee7b6b2e52c33b3f8
Author: Gregory CLEMENT <gregory.clement@free-electrons.com>
Date:   Thu Oct 30 12:39:44 2014 +0100

    ARM: mvebu: Implement the CPU hotplug support for the Armada 38x SoCs
    
    This commit implements the CPU hotplug support for the Marvell Armada
    38x platform. Similarly to what was done for the Armada XP, this
    commit:
    
     * Implements the ->cpu_die() function of SMP operations by calling
       armada_38x_do_cpu_suspend() to enter the deep idle state for
       CPUs going offline.
    
     * Implements a dummy ->cpu_kill() function, simply needed for the
       kernel to know we have CPU hotplug support.
    
     * The mvebu_cortex_a9_boot_secondary() function makes sure to wake up
       the CPU if waiting in deep idle state by sending an IPI before
       deasserting the CPUs from reset. This is because
       mvebu_cortex_a9_boot_secondary() is now used in two different
       situations: for the initial boot of secondary CPUs (where CPU reset
       deassert is used to wake up CPUs) and for CPU hotplug (where an IPI
       is used to take CPU out of deep idle).
    
     * At boot time, we exit from the idle state in the
        ->smp_secondary_init() hook.
    
    This commit has been tested using CPU hotplug through sysfs
    (/sys/devices/system/cpu/cpuX/online) and using kexec.
    
    Signed-off-by: Gregory CLEMENT <gregory.clement@free-electrons.com>
    Tested-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Reviewed-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Link: https://lkml.kernel.org/r/1414669184-16785-5-git-send-email-gregory.clement@free-electrons.com
    Signed-off-by: Jason Cooper <jason@lakedaemon.net>

commit 226424eee809251ec23bd4b09d8efba09c10fc3c
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Nov 5 16:11:44 2014 +0000

    perf: Fix corruption of sibling list with hotplug
    
    When a CPU hotplugged out, we call perf_remove_from_context() (via
    perf_event_exit_cpu()) to rip each CPU-bound event out of its PMU's cpu
    context, but leave siblings grouped together. Freeing of these events is
    left to the mercy of the usual refcounting.
    
    When a CPU-bound event's refcount drops to zero we cross-call to
    __perf_remove_from_context() to clean it up, detaching grouped siblings.
    
    This works when the relevant CPU is online, but will fail if the CPU is
    currently offline, and we won't detach the event from its siblings
    before freeing the event, leaving the sibling list corrupt. If the
    sibling list is later walked (e.g. because the CPU cam online again
    before a remaining sibling's refcount drops to zero), we will walk the
    now corrupted siblings list, potentially dereferencing garbage values.
    
    Given that the events should never be scheduled again (as we removed
    them from their context), we can simply detatch siblings when the CPU
    goes down in the first place. If the CPU comes back online, the
    redundant call to __perf_remove_from_context() is safe.
    
    Reported-by: Drew Richardson <drew.richardson@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: vincent.weaver@maine.edu
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1415203904-25308-2-git-send-email-mark.rutland@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6ac94d3abcb85fdb5755a51b3eb2e28dde07ecc3
Merge: 661b99e95fa3 842dfc11ea9a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 7 18:08:02 2014 -0800

    Merge branch 'upstream' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus
    
    Pull MIPS updates from Ralf Baechle:
     "This weeks' round of MIPS bug fixes for 3.18:
    
       - wire up the bpf syscall
       - fix TLB dump output for R3000 class TLBs
       - fix strnlen_user return value if no NUL character was found.
       - fix build with binutils 2.24.51+.  While there is no binutils 2.25
         release yet, toolchains derived from binutils 2.24.51+ are already
         in common use.
       - the Octeon GPIO code forgot to offline GPIO IRQs.
       - fix build error for XLP.
       - fix possible BUG assertion with EVA for CMA"
    
    * 'upstream' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus:
      MIPS: Fix build with binutils 2.24.51+
      MIPS: R3000: Fix debug output for Virtual page number
      MIPS: Fix strnlen_user() return value in case of overlong strings.
      MIPS: CMA: Do not reserve memory if not required
      MIPS: Wire up bpf syscall.
      MIPS/Xlp: Remove the dead function destroy_irq() to fix build error
      MIPS: Octeon: Make Octeon GPIO IRQ chip CPU hotplug-aware

commit fdb7a04767162bc25bfa1dd31f9852b671a81f37
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Wed Sep 24 16:38:05 2014 +0800

    sched: Fix unreleased llc_shared_mask bit during CPU hotplug
    
    commit 03bd4e1f7265548832a76e7919a81f3137c44fd1 upstream.
    
    The following bug can be triggered by hot adding and removing a large number of
    xen domain0's vcpus repeatedly:
    
            BUG: unable to handle kernel NULL pointer dereference at 0000000000000004 IP: [..] find_busiest_group
            PGD 5a9d5067 PUD 13067 PMD 0
            Oops: 0000 [#3] SMP
            [...]
            Call Trace:
            load_balance
            ? _raw_spin_unlock_irqrestore
            idle_balance
            __schedule
            schedule
            schedule_timeout
            ? lock_timer_base
            schedule_timeout_uninterruptible
            msleep
            lock_device_hotplug_sysfs
            online_store
            dev_attr_store
            sysfs_write_file
            vfs_write
            SyS_write
            system_call_fastpath
    
    Last level cache shared mask is built during CPU up and the
    build_sched_domain() routine takes advantage of it to setup
    the sched domain CPU topology.
    
    However, llc_shared_mask is not released during CPU disable,
    which leads to an invalid sched domainCPU topology.
    
    This patch fix it by releasing the llc_shared_mask correctly
    during CPU disable.
    
    Yasuaki also reported that this can happen on real hardware:
    
      https://lkml.org/lkml/2014/7/22/1018
    
    His case is here:
    
            ==
            Here is an example on my system.
            My system has 4 sockets and each socket has 15 cores and HT is
            enabled. In this case, each core of sockes is numbered as
            follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-44, 90-104
            Socket#3 | 45-59, 105-119
    
            Then llc_shared_mask of CPU#30 has 0x3fff80000001fffc0000000.
    
            It means that last level cache of Socket#2 is shared with
            CPU#30-44 and 90-104.
    
            When hot-removing socket#2 and #3, each core of sockets is
            numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
    
            But llc_shared_mask is not cleared. So llc_shared_mask of CPU#30
            remains having 0x3fff80000001fffc0000000.
    
            After that, when hot-adding socket#2 and #3, each core of
            sockets is numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-59
            Socket#3 | 90-119
    
            Then llc_shared_mask of CPU#30 becomes
            0x3fff8000fffffffc0000000. It means that last level cache of
            Socket#2 is shared with CPU#30-59 and 90-104. So the mask has
            the wrong value.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Tested-by: Linn Crosetto <linn@hp.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1411547885-48165-1-git-send-email-wanpeng.li@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 5df1eb90953a86127ca130d90724819383f896da
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Wed Sep 3 15:04:28 2014 +0200

    ACPI / cpuidle: fix deadlock between cpuidle_lock and cpu_hotplug.lock
    
    commit 6726655dfdd2dc60c035c690d9f10cb69d7ea075 upstream.
    
    There is a following AB-BA dependency between cpu_hotplug.lock and
    cpuidle_lock:
    
    1) cpu_hotplug.lock -> cpuidle_lock
    enable_nonboot_cpus()
     _cpu_up()
      cpu_hotplug_begin()
       LOCK(cpu_hotplug.lock)
     cpu_notify()
      ...
      acpi_processor_hotplug()
       cpuidle_pause_and_lock()
        LOCK(cpuidle_lock)
    
    2) cpuidle_lock -> cpu_hotplug.lock
    acpi_os_execute_deferred() workqueue
     ...
     acpi_processor_cst_has_changed()
      cpuidle_pause_and_lock()
       LOCK(cpuidle_lock)
      get_online_cpus()
       LOCK(cpu_hotplug.lock)
    
    Fix this by reversing the order acpi_processor_cst_has_changed() does
    thigs -- let it first execute the protection against CPU hotplug by
    calling get_online_cpus() and obtain the cpuidle lock only after that (and
    perform the symmentric change when allowing CPUs hotplug again and
    dropping cpuidle lock).
    
    Spotted by lockdep.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit f3af020b9a8d298022b811a19719df0cf461efa5
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Nov 4 13:52:27 2014 -0500

    blk-mq: make mq_queue_reinit_notify() freeze queues in parallel
    
    q->mq_usage_counter is a percpu_ref which is killed and drained when
    the queue is frozen.  On a CPU hotplug event, blk_mq_queue_reinit()
    which involves freezing the queue is invoked on all existing queues.
    Because percpu_ref killing and draining involve a RCU grace period,
    doing the above on one queue after another may take a long time if
    there are many queues on the system.
    
    This patch splits out initiation of freezing and waiting for its
    completion, and updates blk_mq_queue_reinit_notify() so that the
    queues are frozen in parallel instead of one after another.  Note that
    freezing and unfreezing are moved from blk_mq_queue_reinit() to
    blk_mq_queue_reinit_notify().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Tested-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

commit aea4869f68b5869afbb308bfb7d777d725df8900
Merge: 0f4b06766bb6 21ee24bf5b43
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 31 12:43:52 2014 -0700

    Merge branch 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core fixes from Ingo Molnar:
     "The tree contains two RCU fixes and a compiler quirk comment fix"
    
    * 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      rcu: Make rcu_barrier() understand about missing rcuo kthreads
      compiler/gcc4+: Remove inaccurate comment about 'asm goto' miscompiles
      rcu: More on deadlock between CPU hotplug and expedited grace periods

commit 21ee24bf5b43ecaeec43a7d5c61edb3cd7f847bf
Merge: 5631b8fba640 d7e29933969e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Oct 30 07:37:37 2014 +0100

    Merge branch 'urgent-for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into core/urgent
    
    Pull two RCU fixes from Paul E. McKenney:
    
    " - Complete the work of commit dd56af42bd82 (rcu: Eliminate deadlock
        between CPU hotplug and expedited grace periods), which was
        intended to allow synchronize_sched_expedited() to be safely
        used when holding locks acquired by CPU-hotplug notifiers.
        This commit makes the put_online_cpus() avoid the deadlock
        instead of just handling the get_online_cpus().
    
      - Complete the work of commit 35ce7f29a44a (rcu: Create rcuo
        kthreads only for onlined CPUs), which was intended to allow
        RCU to avoid allocating unneeded kthreads on systems where the
        firmware says that there are more CPUs than are really present.
        This commit makes rcu_barrier() aware of the mismatch, so that
        it doesn't hang waiting for non-existent CPUs. "
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit cf355704d681ce7043c732e732b0a23c27d158a8
Author: Alexander Sverdlin <alexander.sverdlin@nsn.com>
Date:   Thu Oct 23 15:55:04 2014 +0200

    MIPS: Octeon: Make Octeon GPIO IRQ chip CPU hotplug-aware
    
    Make Octeon GPIO IRQ chip CPU hotplug-aware
    
    Seems that irq_cpu_offline callbacks were forgotten in v1 and v2 CIU
    GPIO chips. There is such a callback for octeon_irq_chip_ciu2_gpio,
    covering CIU2 chips. Without this callback GPIO IRQs are not being migrated
    during core offlining. Patch is tested on Octeon II.
    
    Signed-off-by: Alexander Sverdlin <alexander.sverdlin@nsn.com>
    Cc: David Daney <ddaney.cavm@gmail.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/8201/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit b2c4623dcd07af4b8ae3b56ae5f879e281c7b4f8
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Oct 22 10:00:05 2014 -0700

    rcu: More on deadlock between CPU hotplug and expedited grace periods
    
    Commit dd56af42bd82 (rcu: Eliminate deadlock between CPU hotplug and
    expedited grace periods) was incomplete.  Although it did eliminate
    deadlocks involving synchronize_sched_expedited()'s acquisition of
    cpu_hotplug.lock via get_online_cpus(), it did nothing about the similar
    deadlock involving acquisition of this same lock via put_online_cpus().
    This deadlock became apparent with testing involving hibernation.
    
    This commit therefore changes put_online_cpus() acquisition of this lock
    to be conditional, and increments a new cpu_hotplug.puts_pending field
    in case of acquisition failure.  Then cpu_hotplug_begin() checks for this
    new field being non-zero, and applies any changes to cpu_hotplug.refcount.
    
    Reported-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Jiri Kosina <jkosina@suse.cz>
    Tested-by: Borislav Petkov <bp@suse.de>

commit dc303408a716e865099fcb3f83a90d9c51184c02
Merge: c4301c326634 e89dafb5ca50
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 21 07:48:56 2014 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux
    
    Pull more powerpc updates from Michael Ellerman:
     "Here's some more updates for powerpc for 3.18.
    
      They are a bit late I know, though must are actually bug fixes.  In my
      defence I nearly cut the top of my finger off last weekend in a
      gruesome bike maintenance accident, so I spent a good part of the week
      waiting around for doctors.  True story, I can send photos if you like :)
    
      Probably the most interesting fix is the sys_call_table one, which
      enables syscall tracing for powerpc.  There's a fix for HMI handling
      for old firmware, more endian fixes for firmware interfaces, more EEH
      fixes, Anton fixed our routine that gets the current stack pointer,
      and a few other misc bits"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux: (22 commits)
      powerpc: Only do dynamic DMA zone limits on platforms that need it
      powerpc: sync pseries_le_defconfig with pseries_defconfig
      powerpc: Add printk levels to setup_system output
      powerpc/vphn: NUMA node code expects big-endian
      powerpc/msi: Use WARN_ON() in msi bitmap selftests
      powerpc/msi: Fix the msi bitmap alignment tests
      powerpc/eeh: Block CFG upon frozen Shiner adapter
      powerpc/eeh: Don't collect logs on PE with blocked config space
      powerpc/eeh: Block PCI config access upon frozen PE
      powerpc/pseries: Drop config requests in EEH accessors
      powerpc/powernv: Drop config requests in EEH accessors
      powerpc/eeh: Rename flag EEH_PE_RESET to EEH_PE_CFG_BLOCKED
      powerpc/eeh: Fix condition for isolated state
      powerpc/pseries: Make CPU hotplug path endian safe
      powerpc/pseries: Use dump_stack instead of show_stack
      powerpc: Rename __get_SP() to current_stack_pointer()
      powerpc: Reimplement __get_SP() as a function not a define
      powerpc/numa: Add ability to disable and debug topology updates
      powerpc/numa: check error return from proc_create
      powerpc/powernv: Fallback to old HMI handling behavior for old firmware
      ...

commit 8a5de18239e418fe7b1f36504834689f754d8ccc
Merge: 857b50f5d0ee 2df36a5dd679
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 18 14:32:31 2014 -0700

    Merge tag 'kvm-arm-for-3.18-take-2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm
    
    Pull second batch of changes for KVM/{arm,arm64} from Marc Zyngier:
     "The most obvious thing is the sizeable MMU changes to support 48bit
      VAs on arm64.
    
      Summary:
    
       - support for 48bit IPA and VA (EL2)
       - a number of fixes for devices mapped into guests
       - yet another VGIC fix for BE
       - a fix for CPU hotplug
       - a few compile fixes (disabled VGIC, strict mm checks)"
    
    [ I'm pulling directly from Marc at the request of Paolo Bonzini, whose
      backpack was stolen at Dsseldorf airport and will do new keys and
      rebuild his web of trust.    - Linus ]
    
    * tag 'kvm-arm-for-3.18-take-2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm:
      arm/arm64: KVM: Fix BE accesses to GICv2 EISR and ELRSR regs
      arm: kvm: STRICT_MM_TYPECHECKS fix for user_mem_abort
      arm/arm64: KVM: Ensure memslots are within KVM_PHYS_SIZE
      arm64: KVM: Implement 48 VA support for KVM EL2 and Stage-2
      arm/arm64: KVM: map MMIO regions at creation time
      arm64: kvm: define PAGE_S2_DEVICE as read-only by default
      ARM: kvm: define PAGE_S2_DEVICE as read-only by default
      arm/arm64: KVM: add 'writable' parameter to kvm_phys_addr_ioremap
      arm/arm64: KVM: fix potential NULL dereference in user_mem_abort()
      arm/arm64: KVM: use __GFP_ZERO not memset() to get zeroed pages
      ARM: KVM: fix vgic-disabled build
      arm: kvm: fix CPU hotplug

commit d6f1e7abdb95a7ea031e7604829e4b5514d7e2c1
Author: Bharata B Rao <bharata@linux.vnet.ibm.com>
Date:   Tue Sep 16 15:15:45 2014 -0500

    powerpc/pseries: Make CPU hotplug path endian safe
    
    - ibm,rtas-configure-connector should treat the RTAS data as big endian.
    - Treat ibm,ppc-interrupt-server#s as big-endian when setting
      smp_processor_id during hotplug.
    
    Signed-off-by: Bharata B Rao <bharata@linux.vnet.ibm.com>
    Signed-off-by: Thomas Falcon <tlfalcon@linux.vnet.ibm.com>
    Acked-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

commit 1ee07ef6b5db7235b133ee257a3adf507697e6b3
Merge: 77654908ff1a 0cccdda8d151
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 14 03:47:00 2014 +0200

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Martin Schwidefsky:
     "This patch set contains the main portion of the changes for 3.18 in
      regard to the s390 architecture.  It is a bit bigger than usual,
      mainly because of a new driver and the vector extension patches.
    
      The interesting bits are:
       - Quite a bit of work on the tracing front.  Uprobes is enabled and
         the ftrace code is reworked to get some of the lost performance
         back if CONFIG_FTRACE is enabled.
       - To improve boot time with CONFIG_DEBIG_PAGEALLOC, support for the
         IPTE range facility is added.
       - The rwlock code is re-factored to improve writer fairness and to be
         able to use the interlocked-access instructions.
       - The kernel part for the support of the vector extension is added.
       - The device driver to access the CD/DVD on the HMC is added, this
         will hopefully come in handy to improve the installation process.
       - Add support for control-unit initiated reconfiguration.
       - The crypto device driver is enhanced to enable the additional AP
         domains and to allow the new crypto hardware to be used.
       - Bug fixes"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (39 commits)
      s390/ftrace: simplify enabling/disabling of ftrace_graph_caller
      s390/ftrace: remove 31 bit ftrace support
      s390/kdump: add support for vector extension
      s390/disassembler: add vector instructions
      s390: add support for vector extension
      s390/zcrypt: Toleration of new crypto hardware
      s390/idle: consolidate idle functions and definitions
      s390/nohz: use a per-cpu flag for arch_needs_cpu
      s390/vtime: do not reset idle data on CPU hotplug
      s390/dasd: add support for control unit initiated reconfiguration
      s390/dasd: fix infinite loop during format
      s390/mm: make use of ipte range facility
      s390/setup: correct 4-level kernel page table detection
      s390/topology: call set_sched_topology early
      s390/uprobes: architecture backend for uprobes
      s390/uprobes: common library for kprobes and uprobes
      s390/rwlock: use the interlocked-access facility 1 instructions
      s390/rwlock: improve writer fairness
      s390/rwlock: remove interrupt-enabling rwlock variant.
      s390/mm: remove change bit override support
      ...

commit d6dd50e07c5bec00db2005969b1a01f8ca3d25ef
Merge: 5ff0b9e1a1da fd19bda49120
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 13 15:44:12 2014 +0200

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - changes related to No-CBs CPUs and NO_HZ_FULL
    
       - RCU-tasks implementation
    
       - torture-test updates
    
       - miscellaneous fixes
    
       - locktorture updates
    
       - RCU documentation updates"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (81 commits)
      workqueue: Use cond_resched_rcu_qs macro
      workqueue: Add quiescent state between work items
      locktorture: Cleanup header usage
      locktorture: Cannot hold read and write lock
      locktorture: Fix __acquire annotation for spinlock irq
      locktorture: Support rwlocks
      rcu: Eliminate deadlock between CPU hotplug and expedited grace periods
      locktorture: Document boot/module parameters
      rcutorture: Rename rcutorture_runnable parameter
      locktorture: Add test scenario for rwsem_lock
      locktorture: Add test scenario for mutex_lock
      locktorture: Make torture scripting account for new _runnable name
      locktorture: Introduce torture context
      locktorture: Support rwsems
      locktorture: Add infrastructure for torturing read locks
      torture: Address race in module cleanup
      locktorture: Make statistics generic
      locktorture: Teach about lock debugging
      locktorture: Support mutexes
      locktorture: Add documentation
      ...

commit 6b4238b59f17764ee24f7e1731ef9a037758394d
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Wed Sep 24 16:38:05 2014 +0800

    sched: Fix unreleased llc_shared_mask bit during CPU hotplug
    
    commit 03bd4e1f7265548832a76e7919a81f3137c44fd1 upstream.
    
    The following bug can be triggered by hot adding and removing a large number of
    xen domain0's vcpus repeatedly:
    
            BUG: unable to handle kernel NULL pointer dereference at 0000000000000004 IP: [..] find_busiest_group
            PGD 5a9d5067 PUD 13067 PMD 0
            Oops: 0000 [#3] SMP
            [...]
            Call Trace:
            load_balance
            ? _raw_spin_unlock_irqrestore
            idle_balance
            __schedule
            schedule
            schedule_timeout
            ? lock_timer_base
            schedule_timeout_uninterruptible
            msleep
            lock_device_hotplug_sysfs
            online_store
            dev_attr_store
            sysfs_write_file
            vfs_write
            SyS_write
            system_call_fastpath
    
    Last level cache shared mask is built during CPU up and the
    build_sched_domain() routine takes advantage of it to setup
    the sched domain CPU topology.
    
    However, llc_shared_mask is not released during CPU disable,
    which leads to an invalid sched domainCPU topology.
    
    This patch fix it by releasing the llc_shared_mask correctly
    during CPU disable.
    
    Yasuaki also reported that this can happen on real hardware:
    
      https://lkml.org/lkml/2014/7/22/1018
    
    His case is here:
    
            ==
            Here is an example on my system.
            My system has 4 sockets and each socket has 15 cores and HT is
            enabled. In this case, each core of sockes is numbered as
            follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-44, 90-104
            Socket#3 | 45-59, 105-119
    
            Then llc_shared_mask of CPU#30 has 0x3fff80000001fffc0000000.
    
            It means that last level cache of Socket#2 is shared with
            CPU#30-44 and 90-104.
    
            When hot-removing socket#2 and #3, each core of sockets is
            numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
    
            But llc_shared_mask is not cleared. So llc_shared_mask of CPU#30
            remains having 0x3fff80000001fffc0000000.
    
            After that, when hot-adding socket#2 and #3, each core of
            sockets is numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-59
            Socket#3 | 90-119
    
            Then llc_shared_mask of CPU#30 becomes
            0x3fff8000fffffffc0000000. It means that last level cache of
            Socket#2 is shared with CPU#30-59 and 90-104. So the mask has
            the wrong value.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Tested-by: Linn Crosetto <linn@hp.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1411547885-48165-1-git-send-email-wanpeng.li@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit a9b1649917f0d2058022eda06082f9d299a06354
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Wed Oct 1 10:44:40 2014 +0200

    s390/vtime: do not reset idle data on CPU hotplug
    
    The sysfs attributes /sys/devices/system/cpu/cpu0/idle_count and
    /sys/devices/system/cpu/cpu0/idle_time_us are reset to zero every
    time a CPU is set online. The idle and iowait fields in /proc/stat
    corresponding to idle_time_us are not reset. To make things
    consistent do not reset the data for the sys attributes.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit 90d52e914efdb0f22779888f181f232ce453592e
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Wed Sep 24 16:38:05 2014 +0800

    sched: Fix unreleased llc_shared_mask bit during CPU hotplug
    
    commit 03bd4e1f7265548832a76e7919a81f3137c44fd1 upstream.
    
    The following bug can be triggered by hot adding and removing a large number of
    xen domain0's vcpus repeatedly:
    
            BUG: unable to handle kernel NULL pointer dereference at 0000000000000004 IP: [..] find_busiest_group
            PGD 5a9d5067 PUD 13067 PMD 0
            Oops: 0000 [#3] SMP
            [...]
            Call Trace:
            load_balance
            ? _raw_spin_unlock_irqrestore
            idle_balance
            __schedule
            schedule
            schedule_timeout
            ? lock_timer_base
            schedule_timeout_uninterruptible
            msleep
            lock_device_hotplug_sysfs
            online_store
            dev_attr_store
            sysfs_write_file
            vfs_write
            SyS_write
            system_call_fastpath
    
    Last level cache shared mask is built during CPU up and the
    build_sched_domain() routine takes advantage of it to setup
    the sched domain CPU topology.
    
    However, llc_shared_mask is not released during CPU disable,
    which leads to an invalid sched domainCPU topology.
    
    This patch fix it by releasing the llc_shared_mask correctly
    during CPU disable.
    
    Yasuaki also reported that this can happen on real hardware:
    
      https://lkml.org/lkml/2014/7/22/1018
    
    His case is here:
    
            ==
            Here is an example on my system.
            My system has 4 sockets and each socket has 15 cores and HT is
            enabled. In this case, each core of sockes is numbered as
            follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-44, 90-104
            Socket#3 | 45-59, 105-119
    
            Then llc_shared_mask of CPU#30 has 0x3fff80000001fffc0000000.
    
            It means that last level cache of Socket#2 is shared with
            CPU#30-44 and 90-104.
    
            When hot-removing socket#2 and #3, each core of sockets is
            numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
    
            But llc_shared_mask is not cleared. So llc_shared_mask of CPU#30
            remains having 0x3fff80000001fffc0000000.
    
            After that, when hot-adding socket#2 and #3, each core of
            sockets is numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-59
            Socket#3 | 90-119
    
            Then llc_shared_mask of CPU#30 becomes
            0x3fff8000fffffffc0000000. It means that last level cache of
            Socket#2 is shared with CPU#30-59 and 90-104. So the mask has
            the wrong value.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Tested-by: Linn Crosetto <linn@hp.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1411547885-48165-1-git-send-email-wanpeng.li@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5d8890fc2dd88809457422c9721077dbf09d138c
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Wed Sep 24 16:38:05 2014 +0800

    sched: Fix unreleased llc_shared_mask bit during CPU hotplug
    
    commit 03bd4e1f7265548832a76e7919a81f3137c44fd1 upstream.
    
    The following bug can be triggered by hot adding and removing a large number of
    xen domain0's vcpus repeatedly:
    
            BUG: unable to handle kernel NULL pointer dereference at 0000000000000004 IP: [..] find_busiest_group
            PGD 5a9d5067 PUD 13067 PMD 0
            Oops: 0000 [#3] SMP
            [...]
            Call Trace:
            load_balance
            ? _raw_spin_unlock_irqrestore
            idle_balance
            __schedule
            schedule
            schedule_timeout
            ? lock_timer_base
            schedule_timeout_uninterruptible
            msleep
            lock_device_hotplug_sysfs
            online_store
            dev_attr_store
            sysfs_write_file
            vfs_write
            SyS_write
            system_call_fastpath
    
    Last level cache shared mask is built during CPU up and the
    build_sched_domain() routine takes advantage of it to setup
    the sched domain CPU topology.
    
    However, llc_shared_mask is not released during CPU disable,
    which leads to an invalid sched domainCPU topology.
    
    This patch fix it by releasing the llc_shared_mask correctly
    during CPU disable.
    
    Yasuaki also reported that this can happen on real hardware:
    
      https://lkml.org/lkml/2014/7/22/1018
    
    His case is here:
    
            ==
            Here is an example on my system.
            My system has 4 sockets and each socket has 15 cores and HT is
            enabled. In this case, each core of sockes is numbered as
            follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-44, 90-104
            Socket#3 | 45-59, 105-119
    
            Then llc_shared_mask of CPU#30 has 0x3fff80000001fffc0000000.
    
            It means that last level cache of Socket#2 is shared with
            CPU#30-44 and 90-104.
    
            When hot-removing socket#2 and #3, each core of sockets is
            numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
    
            But llc_shared_mask is not cleared. So llc_shared_mask of CPU#30
            remains having 0x3fff80000001fffc0000000.
    
            After that, when hot-adding socket#2 and #3, each core of
            sockets is numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-59
            Socket#3 | 90-119
    
            Then llc_shared_mask of CPU#30 becomes
            0x3fff8000fffffffc0000000. It means that last level cache of
            Socket#2 is shared with CPU#30-59 and 90-104. So the mask has
            the wrong value.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Tested-by: Linn Crosetto <linn@hp.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1411547885-48165-1-git-send-email-wanpeng.li@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f99234e13c7cec10453aecbda179c96a5b0778f5
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Wed Sep 24 16:38:05 2014 +0800

    sched: Fix unreleased llc_shared_mask bit during CPU hotplug
    
    commit 03bd4e1f7265548832a76e7919a81f3137c44fd1 upstream.
    
    The following bug can be triggered by hot adding and removing a large number of
    xen domain0's vcpus repeatedly:
    
            BUG: unable to handle kernel NULL pointer dereference at 0000000000000004 IP: [..] find_busiest_group
            PGD 5a9d5067 PUD 13067 PMD 0
            Oops: 0000 [#3] SMP
            [...]
            Call Trace:
            load_balance
            ? _raw_spin_unlock_irqrestore
            idle_balance
            __schedule
            schedule
            schedule_timeout
            ? lock_timer_base
            schedule_timeout_uninterruptible
            msleep
            lock_device_hotplug_sysfs
            online_store
            dev_attr_store
            sysfs_write_file
            vfs_write
            SyS_write
            system_call_fastpath
    
    Last level cache shared mask is built during CPU up and the
    build_sched_domain() routine takes advantage of it to setup
    the sched domain CPU topology.
    
    However, llc_shared_mask is not released during CPU disable,
    which leads to an invalid sched domainCPU topology.
    
    This patch fix it by releasing the llc_shared_mask correctly
    during CPU disable.
    
    Yasuaki also reported that this can happen on real hardware:
    
      https://lkml.org/lkml/2014/7/22/1018
    
    His case is here:
    
            ==
            Here is an example on my system.
            My system has 4 sockets and each socket has 15 cores and HT is
            enabled. In this case, each core of sockes is numbered as
            follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-44, 90-104
            Socket#3 | 45-59, 105-119
    
            Then llc_shared_mask of CPU#30 has 0x3fff80000001fffc0000000.
    
            It means that last level cache of Socket#2 is shared with
            CPU#30-44 and 90-104.
    
            When hot-removing socket#2 and #3, each core of sockets is
            numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
    
            But llc_shared_mask is not cleared. So llc_shared_mask of CPU#30
            remains having 0x3fff80000001fffc0000000.
    
            After that, when hot-adding socket#2 and #3, each core of
            sockets is numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-59
            Socket#3 | 90-119
    
            Then llc_shared_mask of CPU#30 becomes
            0x3fff8000fffffffc0000000. It means that last level cache of
            Socket#2 is shared with CPU#30-59 and 90-104. So the mask has
            the wrong value.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Tested-by: Linn Crosetto <linn@hp.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1411547885-48165-1-git-send-email-wanpeng.li@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 37a34ac1d4775aafbc73b9db53c7daebbbc67e6a
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Mon Sep 22 15:52:48 2014 +0100

    arm: kvm: fix CPU hotplug
    
    On some platforms with no power management capabilities, the hotplug
    implementation is allowed to return from a smp_ops.cpu_die() call as a
    function return. Upon a CPU onlining event, the KVM CPU notifier tries
    to reinstall the hyp stub, which fails on platform where no reset took
    place following a hotplug event, with the message:
    
    CPU1: smp_ops.cpu_die() returned, trying to resuscitate
    CPU1: Booted secondary processor
    Kernel panic - not syncing: unexpected prefetch abort in Hyp mode at: 0x80409540
    unexpected data abort in Hyp mode at: 0x80401fe8
    unexpected HVC/SVC trap in Hyp mode at: 0x805c6170
    
    since KVM code is trying to reinstall the stub on a system where it is
    already configured.
    
    To prevent this issue, this patch adds a check in the KVM hotplug
    notifier that detects if the HYP stub really needs re-installing when a
    CPU is onlined and skips the installation call if the stub is already in
    place, which means that the CPU has not been reset.
    
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

commit d2865c7d4ef818421941579e185c1e4743692f85
Merge: 8207649c41bf 03bd4e1f7265
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 26 08:38:09 2014 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "A CONFIG_STACK_GROWSUP=y fix, and a hotplug llc CPU mask fix"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Fix unreleased llc_shared_mask bit during CPU hotplug
      sched: Fix end_of_stack() and location of stack canary for architectures using CONFIG_STACK_GROWSUP

commit 03bd4e1f7265548832a76e7919a81f3137c44fd1
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Wed Sep 24 16:38:05 2014 +0800

    sched: Fix unreleased llc_shared_mask bit during CPU hotplug
    
    The following bug can be triggered by hot adding and removing a large number of
    xen domain0's vcpus repeatedly:
    
            BUG: unable to handle kernel NULL pointer dereference at 0000000000000004 IP: [..] find_busiest_group
            PGD 5a9d5067 PUD 13067 PMD 0
            Oops: 0000 [#3] SMP
            [...]
            Call Trace:
            load_balance
            ? _raw_spin_unlock_irqrestore
            idle_balance
            __schedule
            schedule
            schedule_timeout
            ? lock_timer_base
            schedule_timeout_uninterruptible
            msleep
            lock_device_hotplug_sysfs
            online_store
            dev_attr_store
            sysfs_write_file
            vfs_write
            SyS_write
            system_call_fastpath
    
    Last level cache shared mask is built during CPU up and the
    build_sched_domain() routine takes advantage of it to setup
    the sched domain CPU topology.
    
    However, llc_shared_mask is not released during CPU disable,
    which leads to an invalid sched domainCPU topology.
    
    This patch fix it by releasing the llc_shared_mask correctly
    during CPU disable.
    
    Yasuaki also reported that this can happen on real hardware:
    
      https://lkml.org/lkml/2014/7/22/1018
    
    His case is here:
    
            ==
            Here is an example on my system.
            My system has 4 sockets and each socket has 15 cores and HT is
            enabled. In this case, each core of sockes is numbered as
            follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-44, 90-104
            Socket#3 | 45-59, 105-119
    
            Then llc_shared_mask of CPU#30 has 0x3fff80000001fffc0000000.
    
            It means that last level cache of Socket#2 is shared with
            CPU#30-44 and 90-104.
    
            When hot-removing socket#2 and #3, each core of sockets is
            numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
    
            But llc_shared_mask is not cleared. So llc_shared_mask of CPU#30
            remains having 0x3fff80000001fffc0000000.
    
            After that, when hot-adding socket#2 and #3, each core of
            sockets is numbered as follows:
    
                     | CPU#
            Socket#0 | 0-14 , 60-74
            Socket#1 | 15-29, 75-89
            Socket#2 | 30-59
            Socket#3 | 90-119
    
            Then llc_shared_mask of CPU#30 becomes
            0x3fff8000fffffffc0000000. It means that last level cache of
            Socket#2 is shared with CPU#30-59 and 90-104. So the mask has
            the wrong value.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Tested-by: Linn Crosetto <linn@hp.com>
    Reviewed-by: Borislav Petkov <bp@suse.de>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: <stable@vger.kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1411547885-48165-1-git-send-email-wanpeng.li@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit eec317319ded3de245ca270a77cb83f523312575
Merge: 60f91268ee29 377fb3f5d9a3
Author: Olof Johansson <olof@lixom.net>
Date:   Tue Sep 23 22:04:19 2014 -0700

    Merge tag 'soc-for-v3.18' of git://git.kernel.org/pub/scm/linux/kernel/git/tmlind/linux-omap into next/soc
    
    SoC related changes for omaps for v3.18 merge window:
    
    - PM changes to make the code easier to use on newer SoCs
    - PM changes for newer SoCs suspend and resume and wake-up events
    - Minor clean-up to remove dead Kconfig options
    
    Note that these have a dependency to the fixes-v3.18-not-urgent
    tag and is based on a commit in that series.
    
    * tag 'soc-for-v3.18' of git://git.kernel.org/pub/scm/linux/kernel/git/tmlind/linux-omap: (514 commits)
      ARM: OMAP5+: Reuse OMAP4 PM code for OMAP5 and DRA7
      ARM: dts: OMAP3+: Add PRM interrupt
      ARM: omap: Remove stray ARCH_HAS_OPP references
      ARM: DRA7: Add hook in SoC initcalls to enable pm initialization
      ARM: OMAP5: Add hook in SoC initcalls to enable pm initialization
      ARM: OMAP5 / DRA7: Enable CPU RET on suspend
      ARM: OMAP5 / DRA7: PM: Provide a dummy startup function for CPU hotplug
      ARM: OMAP5 / DRA7: PM: Avoid all SAR saves
      ARM: OMAP5 / DRA7: PM: Enable Mercury retention mode on CPUx powerdomains
      ARM: OMAP5 / DRA7: PM / wakeupgen: Enables ES2 PM mode by default
      ARM: OMAP5 / DRA7: PM: Set MPUSS-EMIF clock-domain static dependency
      ARM: OMAP5 / DRA7: PM: Update CPU context register offset
      ARM: AM437x: use pdata quirks for pinctrl information
      ARM: DRA7: use pdata quirks for pinctrl information
      ARM: OMAP5: use pdata quirks for pinctrl information
      ARM: OMAP4+: PM: Use only valid low power state for CPU hotplug
      ARM: OMAP4+: PM: use only valid low power state for suspend
      ARM: OMAP4+: PM: Make logic state programmable
      ARM: OMAP2+: powerdomain: introduce logic for finding valid power domain
      ARM: OMAP2+: powerdomain: pwrdm_for_each_clkdm iterate only valid clkdms
      ...

commit 2fee8c1dd07260329e9788984b79b099456a9d11
Merge: 01100c022df5 31957609db52
Author: Olof Johansson <olof@lixom.net>
Date:   Tue Sep 23 22:03:03 2014 -0700

    Merge tag 'fixes-v3.18-not-urgent' of git://git.kernel.org/pub/scm/linux/kernel/git/tmlind/linux-omap into next/fixes-non-critical
    
    Merge "non-urgent omap fixes for v3.18 merge window" from Tony Lindgren:
    
    Fixes for omaps that were not considered urgent enough
    for the -rc cycle:
    
    - Fixes for .dts files to differentiate panda and beaglebone versions
    - Powerdomain fixes from Nishant Menon mostly for newer omaps
    - Fixes for __initconst and of_device_ids const usage
    
    * tag 'fixes-v3.18-not-urgent' of git://git.kernel.org/pub/scm/linux/kernel/git/tmlind/linux-omap:
      ARM: OMAP2+: make of_device_ids const
      ARM: omap2: make arrays containing machine compatible strings const
      ARM: OMAP4+: PM: Use only valid low power state for CPU hotplug
      ARM: OMAP4+: PM: use only valid low power state for suspend
      ARM: OMAP4+: PM: Make logic state programmable
      ARM: OMAP2+: powerdomain: introduce logic for finding valid power domain
      ARM: OMAP2+: powerdomain: pwrdm_for_each_clkdm iterate only valid clkdms
      ARM: OMAP5: powerdomain data: fix powerdomain powerstate
      ARM: OMAP: DRA7: powerdomain data: fix powerdomain powerstate
      ARM: dts: am335x-bone*: Fix model name and update compatibility information
      ARM: dts: omap4-panda: Fix model and SoC family details
      + Linux 3.17-rc3
    
    Signed-off-by: Olof Johansson <olof@lixom.net>

commit dd56af42bd829c6e770ed69812bd65a04eaeb1e4
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Aug 25 20:25:06 2014 -0700

    rcu: Eliminate deadlock between CPU hotplug and expedited grace periods
    
    Currently, the expedited grace-period primitives do get_online_cpus().
    This greatly simplifies their implementation, but means that calls
    to them holding locks that are acquired by CPU-hotplug notifiers (to
    say nothing of calls to these primitives from CPU-hotplug notifiers)
    can deadlock.  But this is starting to become inconvenient, as can be
    seen here: https://lkml.org/lkml/2014/8/5/754.  The problem in this
    case is that some developers need to acquire a mutex from a CPU-hotplug
    notifier, but also need to hold it across a synchronize_rcu_expedited().
    As noted above, this currently results in deadlock.
    
    This commit avoids the deadlock and retains the simplicity by creating
    a try_get_online_cpus(), which returns false if the get_online_cpus()
    reference count could not immediately be incremented.  If a call to
    try_get_online_cpus() returns true, the expedited primitives operate as
    before.  If a call returns false, the expedited primitives fall back to
    normal grace-period operations.  This falling back of course results in
    increased grace-period latency, but only during times when CPU hotplug
    operations are actually in flight.  The effect should therefore be
    negligible during normal operation.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Tested-by: Lan Tianyu <tianyu.lan@intel.com>

commit d1c062033bc96b74941a06ffa1922e50c24f1680
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Wed Sep 3 15:04:28 2014 +0200

    ACPI / cpuidle: fix deadlock between cpuidle_lock and cpu_hotplug.lock
    
    commit 6726655dfdd2dc60c035c690d9f10cb69d7ea075 upstream.
    
    There is a following AB-BA dependency between cpu_hotplug.lock and
    cpuidle_lock:
    
    1) cpu_hotplug.lock -> cpuidle_lock
    enable_nonboot_cpus()
     _cpu_up()
      cpu_hotplug_begin()
       LOCK(cpu_hotplug.lock)
     cpu_notify()
      ...
      acpi_processor_hotplug()
       cpuidle_pause_and_lock()
        LOCK(cpuidle_lock)
    
    2) cpuidle_lock -> cpu_hotplug.lock
    acpi_os_execute_deferred() workqueue
     ...
     acpi_processor_cst_has_changed()
      cpuidle_pause_and_lock()
       LOCK(cpuidle_lock)
      get_online_cpus()
       LOCK(cpu_hotplug.lock)
    
    Fix this by reversing the order acpi_processor_cst_has_changed() does
    thigs -- let it first execute the protection against CPU hotplug by
    calling get_online_cpus() and obtain the cpuidle lock only after that (and
    perform the symmentric change when allowing CPUs hotplug again and
    dropping cpuidle lock).
    
    Spotted by lockdep.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4df052078015216d52dd1357d82c131059ebfb18
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Wed Sep 3 15:04:28 2014 +0200

    ACPI / cpuidle: fix deadlock between cpuidle_lock and cpu_hotplug.lock
    
    commit 6726655dfdd2dc60c035c690d9f10cb69d7ea075 upstream.
    
    There is a following AB-BA dependency between cpu_hotplug.lock and
    cpuidle_lock:
    
    1) cpu_hotplug.lock -> cpuidle_lock
    enable_nonboot_cpus()
     _cpu_up()
      cpu_hotplug_begin()
       LOCK(cpu_hotplug.lock)
     cpu_notify()
      ...
      acpi_processor_hotplug()
       cpuidle_pause_and_lock()
        LOCK(cpuidle_lock)
    
    2) cpuidle_lock -> cpu_hotplug.lock
    acpi_os_execute_deferred() workqueue
     ...
     acpi_processor_cst_has_changed()
      cpuidle_pause_and_lock()
       LOCK(cpuidle_lock)
      get_online_cpus()
       LOCK(cpu_hotplug.lock)
    
    Fix this by reversing the order acpi_processor_cst_has_changed() does
    thigs -- let it first execute the protection against CPU hotplug by
    calling get_online_cpus() and obtain the cpuidle lock only after that (and
    perform the symmentric change when allowing CPUs hotplug again and
    dropping cpuidle lock).
    
    Spotted by lockdep.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4f6a1e6210f5aeed2832d69103fc6511c0ca7c2d
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Wed Sep 3 15:04:28 2014 +0200

    ACPI / cpuidle: fix deadlock between cpuidle_lock and cpu_hotplug.lock
    
    commit 6726655dfdd2dc60c035c690d9f10cb69d7ea075 upstream.
    
    There is a following AB-BA dependency between cpu_hotplug.lock and
    cpuidle_lock:
    
    1) cpu_hotplug.lock -> cpuidle_lock
    enable_nonboot_cpus()
     _cpu_up()
      cpu_hotplug_begin()
       LOCK(cpu_hotplug.lock)
     cpu_notify()
      ...
      acpi_processor_hotplug()
       cpuidle_pause_and_lock()
        LOCK(cpuidle_lock)
    
    2) cpuidle_lock -> cpu_hotplug.lock
    acpi_os_execute_deferred() workqueue
     ...
     acpi_processor_cst_has_changed()
      cpuidle_pause_and_lock()
       LOCK(cpuidle_lock)
      get_online_cpus()
       LOCK(cpu_hotplug.lock)
    
    Fix this by reversing the order acpi_processor_cst_has_changed() does
    thigs -- let it first execute the protection against CPU hotplug by
    calling get_online_cpus() and obtain the cpuidle lock only after that (and
    perform the symmentric change when allowing CPUs hotplug again and
    dropping cpuidle lock).
    
    Spotted by lockdep.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4cdd9a72256c461874df095fde4a6b96bd7f426d
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Wed Sep 3 15:04:28 2014 +0200

    ACPI / cpuidle: fix deadlock between cpuidle_lock and cpu_hotplug.lock
    
    commit 6726655dfdd2dc60c035c690d9f10cb69d7ea075 upstream.
    
    There is a following AB-BA dependency between cpu_hotplug.lock and
    cpuidle_lock:
    
    1) cpu_hotplug.lock -> cpuidle_lock
    enable_nonboot_cpus()
     _cpu_up()
      cpu_hotplug_begin()
       LOCK(cpu_hotplug.lock)
     cpu_notify()
      ...
      acpi_processor_hotplug()
       cpuidle_pause_and_lock()
        LOCK(cpuidle_lock)
    
    2) cpuidle_lock -> cpu_hotplug.lock
    acpi_os_execute_deferred() workqueue
     ...
     acpi_processor_cst_has_changed()
      cpuidle_pause_and_lock()
       LOCK(cpuidle_lock)
      get_online_cpus()
       LOCK(cpu_hotplug.lock)
    
    Fix this by reversing the order acpi_processor_cst_has_changed() does
    thigs -- let it first execute the protection against CPU hotplug by
    calling get_online_cpus() and obtain the cpuidle lock only after that (and
    perform the symmentric change when allowing CPUs hotplug again and
    dropping cpuidle lock).
    
    Spotted by lockdep.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit ce4b1b16502b182368cda20a61de2995762c8bcc
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Fri Jun 20 14:23:11 2014 +0200

    x86/smpboot: Initialize secondary CPU only if master CPU will wait for it
    
    Hang is observed on virtual machines during CPU hotplug,
    especially in big guests with many CPUs. (It reproducible
    more often if host is over-committed).
    
    It happens because master CPU gives up waiting on
    secondary CPU and allows it to run wild. As result
    AP causes locking or crashing system. For example
    as described here:
    
      https://lkml.org/lkml/2014/3/6/257
    
    If master CPU have sent STARTUP IPI successfully,
    and AP signalled to master CPU that it's ready
    to start initialization, make master CPU wait
    indefinitely till AP is onlined.
    
    To ensure that AP won't ever run wild, make it
    wait at early startup till master CPU confirms its
    intention to wait for AP. If AP doesn't respond in 10
    seconds, the master CPU will timeout and cancel
    AP onlining.
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Acked-by: Toshi Kani <toshi.kani@hp.com>
    Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1403266991-12233-1-git-send-email-imammedo@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 9925cc1396339da25d5ef477be1f8c41b2391918
Merge: 753a6cb7e4fc eb35bdd7bca2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 12 09:53:47 2014 -0700

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes from Will Deacon:
     "Just a couple of stragglers here:
    
       - fix an issue migrating interrupts on CPU hotplug
       - fix a potential information leak of TLS registers across an exec
         (Nathan has sent a corresponding patch for arch/arm/ to rmk)"
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: flush TLS registers during exec
      arm64: use irq_set_affinity with force=false when migrating irqs

commit e97c4eb342055b24da886b56377dc0093e835b4a
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Fri Jun 6 17:30:43 2014 -0500

    ARM: OMAP5 / DRA7: PM: Provide a dummy startup function for CPU hotplug
    
    Dont assume that all OMAP4+ code will be able to use OMAP4 hotplug
    logic. On OMAP5, DRA7, we do not need this in place yet, also,
    currently the CPU startup pointer is located in omap4_cpu_pm_info
    instead of cpu_pm_ops.
    
    So, isolate the function to hotplug_restart pointer in cpu_pm_ops
    where it should have belonged, initalize them as per valid startup
    pointers for OMAP4430/60 as in current logic, however provide
    dummy_cpu_resume to be the startup location as well.
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    [nm@ti.com: split this out of original code and isolate it]
    Signed-off-by: Nishanth Menon <nm@ti.com>
    Reviewed-by: Kevin Hilman <khilman@linaro.org>
    Tested-by: Kevin Hilman <khilman@linaro.org>

commit 3e6a1c945950140926dd6e2cc667893de0a7fe3b
Author: Nishanth Menon <nm@ti.com>
Date:   Thu Jul 24 10:24:19 2014 -0500

    ARM: OMAP4+: PM: Use only valid low power state for CPU hotplug
    
    Not all SoCs support OFF mode - for example DRA74/72. So, use valid
    power state during CPU hotplug.
    
    Signed-off-by: Nishanth Menon <nm@ti.com>
    Reviewed-by: Kevin Hilman <khilman@linaro.org>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

commit 6726655dfdd2dc60c035c690d9f10cb69d7ea075
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Wed Sep 3 15:04:28 2014 +0200

    ACPI / cpuidle: fix deadlock between cpuidle_lock and cpu_hotplug.lock
    
    There is a following AB-BA dependency between cpu_hotplug.lock and
    cpuidle_lock:
    
    1) cpu_hotplug.lock -> cpuidle_lock
    enable_nonboot_cpus()
     _cpu_up()
      cpu_hotplug_begin()
       LOCK(cpu_hotplug.lock)
     cpu_notify()
      ...
      acpi_processor_hotplug()
       cpuidle_pause_and_lock()
        LOCK(cpuidle_lock)
    
    2) cpuidle_lock -> cpu_hotplug.lock
    acpi_os_execute_deferred() workqueue
     ...
     acpi_processor_cst_has_changed()
      cpuidle_pause_and_lock()
       LOCK(cpuidle_lock)
      get_online_cpus()
       LOCK(cpu_hotplug.lock)
    
    Fix this by reversing the order acpi_processor_cst_has_changed() does
    thigs -- let it first execute the protection against CPU hotplug by
    calling get_online_cpus() and obtain the cpuidle lock only after that (and
    perform the symmentric change when allowing CPUs hotplug again and
    dropping cpuidle lock).
    
    Spotted by lockdep.
    
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>
    Cc: All applicable <stable@vger.kernel.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit c9d26423e56ce1ab4d786f92aebecf859d419293
Merge: a11c5c9ef6dc af5b7e84d022
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Aug 14 18:13:46 2014 -0600

    Merge tag 'pm+acpi-3.17-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull more ACPI and power management updates from Rafael Wysocki:
     "These are a couple of regression fixes, cpuidle menu governor
      optimizations, fixes for ACPI proccessor and battery drivers,
      hibernation fix to avoid problems related to the e820 memory map,
      fixes for a few cpufreq drivers and a new version of the suspend
      profiling tool analyze_suspend.py.
    
      Specifics:
    
       - Fix for an ACPI-based device hotplug regression introduced in 3.14
         that causes a kernel panic to trigger when memory hot-remove is
         attempted with CONFIG_ACPI_HOTPLUG_MEMORY unset from Tang Chen
    
       - Fix for a cpufreq regression introduced in 3.16 that triggers a
         "sleeping function called from invalid context" bug in
         dev_pm_opp_init_cpufreq_table() from Stephen Boyd
    
       - ACPI battery driver fix for a warning message added in 3.16 that
         prints silly stuff sometimes from Mariusz Ceier
    
       - Hibernation fix for safer handling of mismatches in the 820 memory
         map between the configurations during image creation and during the
         subsequent restore from Chun-Yi Lee
    
       - ACPI processor driver fix to handle CPU hotplug notifications
         correctly during system suspend/resume from Lan Tianyu
    
       - Series of four cpuidle menu governor cleanups that also should
         speed it up a bit from Mel Gorman
    
       - Fixes for the speedstep-smi, integrator, cpu0 and arm_big_little
         cpufreq drivers from Hans Wennborg, Himangi Saraogi, Markus
         Pargmann and Uwe Kleine-Knig
    
       - Version 3.0 of the analyze_suspend.py suspend profiling tool from
         Todd E Brandt"
    
    * tag 'pm+acpi-3.17-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm:
      ACPI / battery: Fix warning message in acpi_battery_get_state()
      PM / tools: analyze_suspend.py: update to v3.0
      cpufreq: arm_big_little: fix module license spec
      cpufreq: speedstep-smi: fix decimal printf specifiers
      ACPI / hotplug: Check scan handlers in acpi_scan_hot_remove()
      cpufreq: OPP: Avoid sleeping while atomic
      cpufreq: cpu0: Do not print error message when deferring
      cpufreq: integrator: Use set_cpus_allowed_ptr
      PM / hibernate: avoid unsafe pages in e820 reserved regions
      ACPI / processor: Make acpi_cpu_soft_notify() process CPU FROZEN events
      cpuidle: menu: Lookup CPU runqueues less
      cpuidle: menu: Call nr_iowait_cpu less times
      cpuidle: menu: Use ktime_to_us instead of reinventing the wheel
      cpuidle: menu: Use shifts when calculating averages where possible

commit 5d61a2172a0142c635ab6d7c3b1589af85a3603e
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Aug 8 18:44:01 2014 -0500

    powerpc/nohash: Split __early_init_mmu() into boot and secondary
    
    __early_init_mmu() does some things that are really only needed by the
    boot cpu.  On FSL booke, This includes calling
    memblock_enforce_memory_limit(), which is labelled __init.  Secondary
    cpu init code can't be __init as that would break CPU hotplug.
    
    While it's probably a bug that memblock_enforce_memory_limit() isn't
    __init_memblock instead, there's no reason why we should be doing this
    stuff for secondary cpus in the first place.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 44c916d58b9ef1f2c4aec2def57fa8289c716a60
Merge: 889fa782bf8e c4846a7823bf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 8 11:00:26 2014 -0700

    Merge tag 'cleanup-for-3.17' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull ARM SoC cleanups from Olof Johansson:
     "This merge window brings a good size of cleanups on various platforms.
      Among the bigger ones:
    
       - Removal of Samsung s5pc100 and s5p64xx platforms.  Both of these
         have lacked active support for quite a while, and after asking
         around nobody showed interest in keeping them around.  If needed,
         they could be resurrected in the future but it's more likely that
         we would prefer reintroduction of them as DT and
         multiplatform-enabled platforms instead.
    
       - OMAP4 controller code register define diet.  They defined a lot of
         registers that were never actually used, etc.
    
       - Move of some of the Tegra platform code (PMC, APBIO, fuse,
         powergate) to drivers/soc so it can be shared with 64-bit code.
         This also converts them over to traditional driver models where
         possible.
    
       - Removal of legacy gpio-samsung driver, since the last users have
         been removed (moved to pinctrl)
    
      Plus a bunch of smaller changes for various platforms that sort of
      dissapear in the diffstat for the above.  clps711x cleanups, shmobile
      header file refactoring/moves for multiplatform friendliness, some
      misc cleanups, etc"
    
    * tag 'cleanup-for-3.17' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (117 commits)
      drivers: CCI: Correct use of ! and &
      video: clcd-versatile: Depend on ARM
      video: fix up versatile CLCD helper move
      MAINTAINERS: Add sdhci-st file to ARCH/STI architecture
      ARM: EXYNOS: Fix build breakge with PM_SLEEP=n
      MAINTAINERS: Remove Kirkwood
      ARM: tegra: Convert PMC to a driver
      soc/tegra: fuse: Set up in early initcall
      ARM: tegra: Always lock the CPU reset vector
      ARM: tegra: Setup CPU hotplug in a pure initcall
      soc/tegra: Implement runtime check for Tegra SoCs
      soc/tegra: fuse: fix dummy functions
      soc/tegra: fuse: move APB DMA into Tegra20 fuse driver
      soc/tegra: Add efuse and apbmisc bindings
      soc/tegra: Add efuse driver for Tegra
      ARM: tegra: move fuse exports to soc/tegra/fuse.h
      ARM: tegra: export apb dma readl/writel
      ARM: tegra: Use a function to get the chip ID
      ARM: tegra: Sort includes alphabetically
      ARM: tegra: Move includes to include/soc/tegra
      ...

commit ea9c167d93e6e2f4697f5061756f26356793cd95
Author: Lan Tianyu <tianyu.lan@intel.com>
Date:   Thu Jul 31 22:28:50 2014 +0800

    ACPI / processor: Make acpi_cpu_soft_notify() process CPU FROZEN events
    
    CPU hotplug happens during S2RAM and CPU notify event will be CPU_XXX_FROZEN.
    acpi_cpu_soft_notify() ignores to check these events. This also may make
    acpi_cpu_soft_notify() fall into sleep during CPU_DYING/STARTING_FROZEN
    events which don't allow callbacks to sleep. This patch is to fix it.
    
    Signed-off-by: Lan Tianyu <tianyu.lan@intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 98959948a7ba33cf8c708626e0d2a1456397e1c6
Merge: ef35ad26f8ff cd3bd4e628a6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 16:23:30 2014 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - Move the nohz kick code out of the scheduler tick to a dedicated IPI,
       from Frederic Weisbecker.
    
      This necessiated quite some background infrastructure rework,
      including:
    
       * Clean up some irq-work internals
       * Implement remote irq-work
       * Implement nohz kick on top of remote irq-work
       * Move full dynticks timer enqueue notification to new kick
       * Move multi-task notification to new kick
       * Remove unecessary barriers on multi-task notification
    
     - Remove proliferation of wait_on_bit() action functions and allow
       wait_on_bit_action() functions to support a timeout.  (Neil Brown)
    
     - Another round of sched/numa improvements, cleanups and fixes.  (Rik
       van Riel)
    
     - Implement fast idling of CPUs when the system is partially loaded,
       for better scalability.  (Tim Chen)
    
     - Restructure and fix the CPU hotplug handling code that may leave
       cfs_rq and rt_rq's throttled when tasks are migrated away from a dead
       cpu.  (Kirill Tkhai)
    
     - Robustify the sched topology setup code.  (Peterz Zijlstra)
    
     - Improve sched_feat() handling wrt.  static_keys (Jason Baron)
    
     - Misc fixes.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (37 commits)
      sched/fair: Fix 'make xmldocs' warning caused by missing description
      sched: Use macro for magic number of -1 for setparam
      sched: Robustify topology setup
      sched: Fix sched_setparam() policy == -1 logic
      sched: Allow wait_on_bit_action() functions to support a timeout
      sched: Remove proliferation of wait_on_bit() action functions
      sched/numa: Revert "Use effective_load() to balance NUMA loads"
      sched: Fix static_key race with sched_feat()
      sched: Remove extra static_key*() function indirection
      sched/rt: Fix replenish_dl_entity() comments to match the current upstream code
      sched: Transform resched_task() into resched_curr()
      sched/deadline: Kill task_struct->pi_top_task
      sched: Rework check_for_tasks()
      sched/rt: Enqueue just unthrottled rt_rq back on the stack in __disable_runtime()
      sched/fair: Disable runtime_enabled on dying rq
      sched/numa: Change scan period code to match intent
      sched/numa: Rework best node setting in task_numa_migrate()
      sched/numa: Examine a task move when examining a task swap
      sched/numa: Simplify task_numa_compare()
      sched/numa: Use effective_load() to balance NUMA loads
      ...

commit 515afdcba0880528fa8ae6fa63a14de6b9018770
Author: Hanjun Guo <hanjun.guo@linaro.org>
Date:   Tue Jul 29 11:27:50 2014 +0800

    ACPI / processor: Make it possible to get local x2apic id via _MAT
    
    Logical processors with APIC ID values of 255 and greater are
    required to have a Processor Device object and must convey the
    processor's APIC information to OSPM using the Processor Local
    X2APIC structure, but not until ACPI 5.1, X2APIC structure was
    not supported in _MAT method.
    
    _MAT is needed for CPU hotplug and system with more than 255
    CPUs will definitely need X2APIC structure, so add its support
    in map_mat_entry() to make it possible to get local x2apic id
    via _MAT based on ACPI 5.1.
    
    Signed-off-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit fb4188bad02f4871b26cf19b98e8d92499ca5d31
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jun 9 01:16:32 2014 +0200

    KVM: PPC: Book3s PR: Disable AIL mode with OPAL
    
    When we're using PR KVM we must not allow the CPU to take interrupts
    in virtual mode, as the SLB does not contain host kernel mappings
    when running inside the guest context.
    
    To make sure we get good performance for non-KVM tasks but still
    properly functioning PR KVM, let's just disable AIL whenever a vcpu
    is scheduled in.
    
    This is fundamentally different from how we deal with AIL on pSeries
    type machines where we disable AIL for the whole machine as soon as
    a single KVM VM is up.
    
    The reason for that is easy - on pSeries we do not have control over
    per-cpu configuration of AIL. We also don't want to mess with CPU hotplug
    races and AIL configuration, so setting it per CPU is easier and more
    flexible.
    
    This patch fixes running PR KVM on POWER8 bare metal for me.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Paul Mackerras <paulus@samba.org>

commit bf72f5dee0656cfa9dc40bcb533f08d1d144e6ea
Merge: 2a2261553dd1 51cbe7e7c400
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Thu Jul 24 16:32:31 2014 -0700

    x86: Merge tag 'ras_urgent' into x86/urgent
    
    Promote one fix for 3.16
    
    This fix was necessary after
    
    9c15a24b038f ("x86/mce: Improve mcheck_init_device() error handling")
    
    went in. What this patch did was, among others, check the return value
    of misc_register and exit early if it encountered an error. Original
    code sloppily didn't do that.
    
    However,
    
            cef12ee52b05 ("xen/mce: Add mcelog support for Xen platform")
    
    made it so that xen's init routine xen_late_init_mcelog runs first. This
    was needed for the xen mcelog device which is supposed to be independent
    from the baremetal one.
    
    Initially it was reported that misc_register() fails often on xen and
    that's why it needed fixing. However, it is *supposed* to fail by
    design, when running in dom0 so that the xen mcelog device file gets
    registered first.
    
    And *then* you need the notifier *not* unregistered on the error path so
    that the timer does get deleted properly in the CPU hotplug notifier.
    
    Btw, this fix is needed also on baremetal in the unlikely event that
    misc_register(&mce_chrdev_device) fails there too.
    
    I was unsure whether to rush it in now and decided to delay it to 3.17.
    However, xen people wanted it promoted as it breaks xen when doing cpu
    hotplug there. So, after a bit of simmering in tip/master for initial
    smoke testing, let's move it to 3.16. It fixes a semi-regression which
    got introduced in 3.16 so no need for stable tagging.
    
    tip/x86/ras contains that exact same commit but we can't remove it
    there as it is not the last one. It won't cause any merge issues, as I
    confirmed locally but I should state here the special situation of this
    one fix explicitly anyway.
    
    Thanks.
    
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit 9ce35884bd961700d34f8a5d908645be2fd0ba76
Author: Gregory CLEMENT <gregory.clement@free-electrons.com>
Date:   Wed Jul 23 15:00:38 2014 +0200

    ARM: mvebu: split again armada_370_xp_pmsu_idle_enter() in PMSU code
    
    do_armada_370_xp_cpu_suspend() and armada_370_xp_pmsu_idle_prepare(),
    have been merged into a single function called
    armada_370_xp_pmsu_idle_enter() by the commit "bbb92284b6c8 ARM:
    mvebu: slightly refactor/rename PMSU idle related functions", in
    prepare for the introduction of the CPU hotplug support for Armada XP.
    
    But for cpuidle the prepare function will be common to all the mvebu
    SoCs that use the PMSU, while the suspend function will be specific to
    each SoC. Keeping the prepare function separate will help reducing
    code duplication while new SoC support is added.
    
    Signed-off-by: Gregory CLEMENT <gregory.clement@free-electrons.com>
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Link: https://lkml.kernel.org/r/1406120453-29291-2-git-send-email-thomas.petazzoni@free-electrons.com
    Signed-off-by: Jason Cooper <jason@lakedaemon.net>

commit 51cbe7e7c400def749950ab6b2c120624dbe21a7
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Jun 20 23:16:45 2014 +0200

    x86, MCE: Robustify mcheck_init_device
    
    BorisO reports that misc_register() fails often on xen. The current code
    unregisters the CPU hotplug notifier in that case. If then a CPU is
    offlined and onlined back again, we end up with a second timer running
    on that CPU, leading to soft lockups and system hangs.
    
    So let's leave the hotcpu notifier always registered - even if
    mce_device_create failed for some cores and never unreg it so that we
    can deal with the timer handling accordingly.
    
    Reported-and-Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Link: http://lkml.kernel.org/r/1403274493-1371-1-git-send-email-boris.ostrovsky@oracle.com
    Signed-off-by: Borislav Petkov <bp@suse.de>

commit c6b659c0050429c36e3e805529d68d8680956e1a
Merge: 23e892929e7c 7232398abc6a
Author: Olof Johansson <olof@lixom.net>
Date:   Sat Jul 19 12:31:22 2014 -0700

    Merge tag 'tegra-for-3.17-soc' of git://git.kernel.org/pub/scm/linux/kernel/git/tegra/linux into next/cleanup
    
    Merge "ARM: tegra: core code changes for 3.17" from Thierry Reding:
    
    Some of the code that's currently called from the Tegra machine setup
    code is moved to regular initcalls. To catch dependency violations, the
    various code paths now WARN if they're called to early. Not all of the
    potential candidates are converted yet, but those that were have been
    verified to work across all supported Tegra generations.
    
    A new function, soc_is_tegra(), is also provided to make sure that the
    initcalls can abort early if they aren't run on Tegra, which can happen
    for multi-platform builds.
    
    Finally this also moves out the PMC driver to drivers/soc/tegra so that
    it can be shared with 64-bit ARM.
    
    This is based on the for-3.17/fuse-move branch. The split is somewhat
    arbitrary but allows the dependents of the for-3.17/fuse-move to pull
    in as little code as necessary.
    
    * tag 'tegra-for-3.17-soc' of git://git.kernel.org/pub/scm/linux/kernel/git/tegra/linux:
      ARM: tegra: Convert PMC to a driver
      soc/tegra: fuse: Set up in early initcall
      ARM: tegra: Always lock the CPU reset vector
      ARM: tegra: Setup CPU hotplug in a pure initcall
      soc/tegra: Implement runtime check for Tegra SoCs
    
    Signed-off-by: Olof Johansson <olof@lixom.net>

commit 05ccf19602cc16fc96401b4f2617d1b8e20e642d
Author: Thierry Reding <treding@nvidia.com>
Date:   Fri Jul 11 11:00:37 2014 +0200

    ARM: tegra: Setup CPU hotplug in a pure initcall
    
    CPU hotplug support doesn't have to be set up until fairly late in the
    boot process, so it can be done in a regular initcall. To make sure that
    we don't miss any ordering problems in the future, output a warning if
    any of the functions are called before initialization has completed.
    
    This is part of untangling the boot order dependencies on Tegra so that
    more code can be shared between 32-bit and 64-bit ARM.
    
    Signed-off-by: Thierry Reding <treding@nvidia.com>

commit b3c20e983269667ecce75fa7f4b0353917a8d0d1
Merge: 98abaf1370c9 e65714740d65
Author: Olof Johansson <olof@lixom.net>
Date:   Mon Jul 7 22:27:00 2014 -0700

    Merge tag 'mvebu-soc-3.17' of git://git.infradead.org/linux-mvebu into next/soc
    
    Merge "mvebu SoC changes for v3.17" from Jason Cooper:
    
    - kirkwood
      * add setup file for netxbig LEDs (non-trivial DT binding doesn't exist yet)
    
    - mvebu
      * staticize where needed
      * add CPU hotplug for Armada XP
      * add public datasheet for Armada 370
      * don't apply thermal quirk by default
      * get SoC ID from the system controller when possible
    
    * tag 'mvebu-soc-3.17' of git://git.infradead.org/linux-mvebu:
      ARM: mvebu: Staticize mvebu_cpu_reset_init
      ARM: mvebu: Staticize armada_370_xp_cpu_pm_init
      ARM: mvebu: Staticize armada_375_smp_cpu1_enable_wa
      ARM: mvebu: Use system controller to get the soc id when possible
      ARM: mvebu: Use the a standard errno in mvebu_get_soc_id
      ARM: mvebu: Don't apply the thermal quirk if the SoC revision is unknown
      Documentation: arm: add URLs to public datasheets for the Marvell Armada 370 SoC
      ARM: mvebu: implement CPU hotplug support for Armada XP
      ARM: mvebu: export PMSU idle enter/exit functions
      ARM: mvebu: slightly refactor/rename PMSU idle related functions
      ARM: mvebu: remove stub implementation of CPU hotplug on Armada 375/38x
      ARM: Kirkwood: Add setup file for netxbig LEDs
      ARM: mvebu: mark armada_370_xp_pmsu_idle_prepare() as static
    
    Signed-off-by: Olof Johansson <olof@lixom.net>

commit a77353e5eb56b6c6098bfce59aff1f449451b0b7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 25 07:13:07 2014 +0200

    irq_work: Remove BUG_ON in irq_work_run()
    
    Because of a collision with 8d056c48e486 ("CPU hotplug, smp: flush any
    pending IPI callbacks before CPU offline"), which ends up calling
    hotplug_cfd()->flush_smp_call_function_queue()->irq_work_run(), which
    is not from IRQ context.
    
    And since that already calls irq_work_run() from the hotplug path,
    remove our entire hotplug handling.
    
    Reported-by: Stephen Warren <swarren@wwwdotorg.org>
    Tested-by: Stephen Warren <swarren@wwwdotorg.org>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-busatzs2gvz4v62258agipuf@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 27c934158c5be0bebfb2970da521b9d9efc0058b
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Jun 20 23:16:45 2014 +0200

    x86, MCE: Robustify mcheck_init_device
    
    BorisO reports that misc_register() fails often on xen. The current code
    unregisters the CPU hotplug notifier in that case. If then a CPU is
    offlined and onlined back again, we end up with a second timer running
    on that CPU, leading to soft lockups and system hangs.
    
    So let's leave the hotcpu notifier always registered - even if
    mce_device_create failed for some cores and never unreg it so that we
    can deal with the timer handling accordingly.
    
    Reported-and-Tested-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Link: http://lkml.kernel.org/r/1403274493-1371-1-git-send-email-boris.ostrovsky@oracle.com
    Signed-off-by: Borislav Petkov <bp@suse.de>

commit 8d056c48e486249e6487910b83e0f3be7c14acf7
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Mon Jun 23 13:22:02 2014 -0700

    CPU hotplug, smp: flush any pending IPI callbacks before CPU offline
    
    There is a race between the CPU offline code (within stop-machine) and
    the smp-call-function code, which can lead to getting IPIs on the
    outgoing CPU, *after* it has gone offline.
    
    Specifically, this can happen when using
    smp_call_function_single_async() to send the IPI, since this API allows
    sending asynchronous IPIs from IRQ disabled contexts.  The exact race
    condition is described below.
    
    During CPU offline, in stop-machine, we don't enforce any rule in the
    _DISABLE_IRQ stage, regarding the order in which the outgoing CPU and
    the other CPUs disable their local interrupts.  Due to this, we can
    encounter a situation in which an IPI is sent by one of the other CPUs
    to the outgoing CPU (while it is *still* online), but the outgoing CPU
    ends up noticing it only *after* it has gone offline.
    
                  CPU 1                                         CPU 2
              (Online CPU)                               (CPU going offline)
    
           Enter _PREPARE stage                          Enter _PREPARE stage
    
                                                         Enter _DISABLE_IRQ stage
    
                                                       =
           Got a device interrupt, and                 | Didn't notice the IPI
           the interrupt handler sent an               | since interrupts were
           IPI to CPU 2 using                          | disabled on this CPU.
           smp_call_function_single_async()            |
                                                       =
    
           Enter _DISABLE_IRQ stage
    
           Enter _RUN stage                              Enter _RUN stage
    
                                      =
           Busy loop with interrupts  |                  Invoke take_cpu_down()
           disabled.                  |                  and take CPU 2 offline
                                      =
    
           Enter _EXIT stage                             Enter _EXIT stage
    
           Re-enable interrupts                          Re-enable interrupts
    
                                                         The pending IPI is noted
                                                         immediately, but alas,
                                                         the CPU is offline at
                                                         this point.
    
    This of course, makes the smp-call-function IPI handler code running on
    CPU 2 unhappy and it complains about "receiving an IPI on an offline
    CPU".
    
    One real example of the scenario on CPU 1 is the block layer's
    complete-request call-path:
    
            __blk_complete_request() [interrupt-handler]
                raise_blk_irq()
                    smp_call_function_single_async()
    
    However, if we look closely, the block layer does check that the target
    CPU is online before firing the IPI.  So in this case, it is actually
    the unfortunate ordering/timing of events in the stop-machine phase that
    leads to receiving IPIs after the target CPU has gone offline.
    
    In reality, getting a late IPI on an offline CPU is not too bad by
    itself (this can happen even due to hardware latencies in IPI
    send-receive).  It is a bug only if the target CPU really went offline
    without executing all the callbacks queued on its list.  (Note that a
    CPU is free to execute its pending smp-call-function callbacks in a
    batch, without waiting for the corresponding IPIs to arrive for each one
    of those callbacks).
    
    So, fixing this issue can be broken up into two parts:
    
    1. Ensure that a CPU goes offline only after executing all the
       callbacks queued on it.
    
    2. Modify the warning condition in the smp-call-function IPI handler
       code such that it warns only if an offline CPU got an IPI *and* that
       CPU had gone offline with callbacks still pending in its queue.
    
    Achieving part 1 is straight-forward - just flush (execute) all the
    queued callbacks on the outgoing CPU in the CPU_DYING stage[1],
    including those callbacks for which the source CPU's IPIs might not have
    been received on the outgoing CPU yet.  Once we do this, an IPI that
    arrives late on the CPU going offline (either due to the race mentioned
    above, or due to hardware latencies) will be completely harmless, since
    the outgoing CPU would have executed all the queued callbacks before
    going offline.
    
    Overall, this fix (parts 1 and 2 put together) additionally guarantees
    that we will see a warning only when the *IPI-sender code* is buggy -
    that is, if it queues the callback _after_ the target CPU has gone
    offline.
    
    [1].  The CPU_DYING part needs a little more explanation: by the time we
    execute the CPU_DYING notifier callbacks, the CPU would have already
    been marked offline.  But we want to flush out the pending callbacks at
    this stage, ignoring the fact that the CPU is offline.  So restructure
    the IPI handler code so that we can by-pass the "is-cpu-offline?" check
    in this particular case.  (Of course, the right solution here is to fix
    CPU hotplug to mark the CPU offline _after_ invoking the CPU_DYING
    notifiers, but this requires a lot of audit to ensure that this change
    doesn't break any existing code; hence lets go with the solution
    proposed above until that is done).
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Suggested-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Gautham R Shenoy <ego@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Galbraith <mgalbraith@suse.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Sachin Kamat <sachin.kamat@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 38356c1fbd8cd0f44a32ede2c97f0eb639d06613
Author: Borislav Petkov <bp@suse.de>
Date:   Thu May 22 16:40:54 2014 +0200

    x86, MCE: Kill CPU_POST_DEAD
    
    In conjunction with cleaning up CPU hotplug, we want to get rid of
    CPU_POST_DEAD. Kill this instance here and rediscover CMCI banks at the
    end of CPU_DEAD.
    
    Link: http://lkml.kernel.org/r/http://lkml.kernel.org/r/1400750624-19238-1-git-send-email-bp@alien8.de
    Signed-off-by: Borislav Petkov <bp@suse.de>

commit 26337779465637b761624d9752f52d1ec88f71d9
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Fri May 30 22:18:17 2014 +0200

    ARM: mvebu: implement CPU hotplug support for Armada XP
    
    This commit implements CPU hotplug support for the Marvell Armada XP
    platform. The CPU hotplug stub functions from hotplug.c are moved into
    platsmp.c, as it doesn't make much sense to have a separate file just
    for these two functions.
    
    In addition, this commit:
    
     * Implements the ->cpu_die() function of SMP operations by calling
       armada_370_xp_pmsu_idle_enter() to enter the deep idle state for
       CPUs going offline.
    
     * Implements a dummy ->cpu_kill() function, simply needed for the
       kernel to know we have CPU hotplug support.
    
     * The armada_xp_boot_secondary() function makes sure to wake up the
       CPU if waiting in deep idle state by sending an IPI. This is
       because armada_xp_boot_secondary() is now used in two different
       situations: for the initial boot of secondary CPUs (where CPU reset
       deassert is used to wake up CPUs) and for CPU hotplug (where an IPI
       is used to take CPU out of deep idle).
    
     * At boot time, we exit from the idle state in the
       ->smp_secondary_init() hook.
    
    This commit has been tested using CPU hotplug through sysfs
    (/sys/devices/system/cpu/cpuX/online) and using kexec.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Link: https://lkml.kernel.org/r/1401481098-23326-5-git-send-email-thomas.petazzoni@free-electrons.com
    Signed-off-by: Jason Cooper <jason@lakedaemon.net>

commit 8ea875e72d2dd66eea393f22c6bf4707f92f4a50
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Fri May 30 22:18:16 2014 +0200

    ARM: mvebu: export PMSU idle enter/exit functions
    
    The PMSU idle enter/exit functions will be needed for the CPU hotplug
    implementation on Armada XP, so this commit removes their static
    qualifier, and adds the appropriate prototypes in armada-370-xp.h.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Link: https://lkml.kernel.org/r/1401481098-23326-4-git-send-email-thomas.petazzoni@free-electrons.com
    Signed-off-by: Jason Cooper <jason@lakedaemon.net>

commit bbb92284b6c821e9434223d437fbd10b8a24c294
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Fri May 30 22:18:15 2014 +0200

    ARM: mvebu: slightly refactor/rename PMSU idle related functions
    
    The CPU hotplug code will need to call into PMSU functions to enter
    and exit from deep idle states. However, the deep idle state is
    currently entered by a function called do_armada_370_xp_cpu_suspend()
    whose name really suggests it's an internal function, but we need to
    export it to other files in mach-mvebu.
    
    Therefore, this commit:
    
     * Merges the code of do_armada_370_xp_cpu_suspend() into
       armada_370_xp_pmsu_idle_prepare(), into a single function called
       armada_370_xp_pmsu_idle_enter(), which prepares the PMSU for deep
       idle, and then enters the deep idle state. This code will be common
       to both cpuidle and CPU hotplug.
    
     * For symetry, it renames the armada_370_xp_pmsu_idle_restore()
       function to armada_370_xp_pmsu_idle_exit().
    
    We also remove the 'noinline' qualifier for these functions, which
    apparently had no reason to be here.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Link: https://lkml.kernel.org/r/1401481098-23326-3-git-send-email-thomas.petazzoni@free-electrons.com
    Signed-off-by: Jason Cooper <jason@lakedaemon.net>

commit 3169455448ea6d021b1b761b4fd241810de8c335
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Fri May 30 22:18:14 2014 +0200

    ARM: mvebu: remove stub implementation of CPU hotplug on Armada 375/38x
    
    In preparation to the addition of CPU hotplug support for Armada XP,
    and therefore moving the existing stub functions for hotplug support,
    this commit removes the reference from the SMP implementation of
    Armada 375/38x to the armada_xp_cpu_die() function. Proper CPU hotplug
    support for Armada 375 and 38x will be implemented at a later point.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Link: https://lkml.kernel.org/r/1401481098-23326-2-git-send-email-thomas.petazzoni@free-electrons.com
    Signed-off-by: Jason Cooper <jason@lakedaemon.net>

commit 52fcc56753de91ae337aeaa0a664f72d93f19827
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Wed Jun 11 14:06:37 2014 +0200

    ARM: mvebu: select ARM_CPU_SUSPEND for Marvell EBU v7 platforms
    
    On Marvell Armada platforms, the PMSU (Power Management Service Unit)
    controls a number of power management related activities, needed for
    things like suspend/resume, CPU hotplug, cpuidle or even simply SMP.
    
    Since cpuidle support was added for Armada XP, the pmsu.c file in
    arch/arm/mach-mvebu/ calls the cpu_suspend() and cpu_resume() ARM
    functions, which are only available when
    CONFIG_ARM_CPU_SUSPEND=y. Therefore, configurations that have
    CONFIG_ARM_CPU_SUSPEND disabled due to PM_SLEEP being disabled no
    longer build properly, due to undefined references to cpu_suspend()
    and cpu_resume().
    
    To fix this, this patch simply ensures CONFIG_ARM_CPU_SUSPEND is
    always enabled for Marvell EBU v7 platforms. Doing things in a more
    fine-grained way would require a lot of #ifdef-ery in pmsu.c to
    isolate the parts that use cpu_suspend()/cpu_resume(), and those parts
    would anyway have been needed as soon as either one of suspend/resume,
    CPU hotplug or cpuidle was enabled.
    
    Reported-by: Ezequiel Garcia <ezequiel.garcia@free-electrons.com>
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Link: https://lkml.kernel.org/r/1402488397-31381-1-git-send-email-thomas.petazzoni@free-electrons.com
    Acked-by: Ezequiel Garcia <ezequiel.garcia@free-electrons.com>
    Signed-off-by: Jason Cooper <jason@lakedaemon.net>

commit b92ad209c26a1891c4e04cd75fc771dcb002603f
Author: Leela Krishna Amudala <leela.krishna@linaro.org>
Date:   Wed May 28 00:43:21 2014 +0900

    ARM: EXYNOS: Use wfi macro in platform_do_lowpower
    
    This patch is originally based on commit b3377d186572 ("ARM: 7064/1:
    vexpress: Use wfi macro in platform_do_lowpower.")
    
    Current Exynos CPU hotplug code includes a hardcoded WFI instruction,
    in ARM encoding. When the kernel is compiled in Thumb-2 mode, this
    is invalid and causes the machine to hang hard when a CPU is offlined.
    
    Use wfi macro instead of the hardcoded WFI instruction.
    
    Signed-off-by: Leela Krishna Amudala <leela.krishna@linaro.org>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Kukjin Kim <kgene.kim@samsung.com>

commit 20b96d740d84a465bf4c5fd2bff37a79f56bff56
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue May 27 16:25:34 2014 +0530

    powerpc, kexec: Fix "Processor X is stuck" issue during kexec from ST mode
    
    commit 011e4b02f1da156ac7fea28a9da878f3c23af739 upstream.
    
    If we try to perform a kexec when the machine is in ST (Single-Threaded) mode
    (ppc64_cpu --smt=off), the kexec operation doesn't succeed properly, and we
    get the following messages during boot:
    
    [    0.089866] POWER8 performance monitor hardware support registered
    [    0.089985] power8-pmu: PMAO restore workaround active.
    [    5.095419] Processor 1 is stuck.
    [   10.097933] Processor 2 is stuck.
    [   15.100480] Processor 3 is stuck.
    [   20.102982] Processor 4 is stuck.
    [   25.105489] Processor 5 is stuck.
    [   30.108005] Processor 6 is stuck.
    [   35.110518] Processor 7 is stuck.
    [   40.113369] Processor 9 is stuck.
    [   45.115879] Processor 10 is stuck.
    [   50.118389] Processor 11 is stuck.
    [   55.120904] Processor 12 is stuck.
    [   60.123425] Processor 13 is stuck.
    [   65.125970] Processor 14 is stuck.
    [   70.128495] Processor 15 is stuck.
    [   75.131316] Processor 17 is stuck.
    
    Note that only the sibling threads are stuck, while the primary threads (0, 8,
    16 etc) boot just fine. Looking closer at the previous step of kexec, we observe
    that kexec tries to wakeup (bring online) the sibling threads of all the cores,
    before performing kexec:
    
    [ 9464.131231] Starting new kernel
    [ 9464.148507] kexec: Waking offline cpu 1.
    [ 9464.148552] kexec: Waking offline cpu 2.
    [ 9464.148600] kexec: Waking offline cpu 3.
    [ 9464.148636] kexec: Waking offline cpu 4.
    [ 9464.148671] kexec: Waking offline cpu 5.
    [ 9464.148708] kexec: Waking offline cpu 6.
    [ 9464.148743] kexec: Waking offline cpu 7.
    [ 9464.148779] kexec: Waking offline cpu 9.
    [ 9464.148815] kexec: Waking offline cpu 10.
    [ 9464.148851] kexec: Waking offline cpu 11.
    [ 9464.148887] kexec: Waking offline cpu 12.
    [ 9464.148922] kexec: Waking offline cpu 13.
    [ 9464.148958] kexec: Waking offline cpu 14.
    [ 9464.148994] kexec: Waking offline cpu 15.
    [ 9464.149030] kexec: Waking offline cpu 17.
    
    Instrumenting this piece of code revealed that the cpu_up() operation actually
    fails with -EBUSY. Thus, only the primary threads of all the cores are online
    during kexec, and hence this is a sure-shot receipe for disaster, as explained
    in commit e8e5c2155b (powerpc/kexec: Fix orphaned offline CPUs across kexec),
    as well as in the comment above wake_offline_cpus().
    
    It turns out that cpu_up() was returning -EBUSY because the variable
    'cpu_hotplug_disabled' was set to 1; and this disabling of CPU hotplug was done
    by migrate_to_reboot_cpu() inside kernel_kexec().
    
    Now, migrate_to_reboot_cpu() was originally written with the assumption that
    any further code will not need to perform CPU hotplug, since we are anyway in
    the reboot path. However, kexec is clearly not such a case, since we depend on
    onlining CPUs, atleast on powerpc.
    
    So re-enable cpu-hotplug after returning from migrate_to_reboot_cpu() in the
    kexec path, to fix this regression in kexec on powerpc.
    
    Also, wrap the cpu_up() in powerpc kexec code within a WARN_ON(), so that we
    can catch such issues more easily in the future.
    
    Fixes: c97102ba963 (kexec: migrate to reboot cpu)
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>

commit 813895f8dcb31bc6b0e9f5fc35e8c687a467f3dd
Merge: d4c54919ed86 745c51673e28
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 7 14:50:38 2014 -0700

    Merge branch 'x86/urgent' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Peter Anvin:
     "A significantly larger than I'd like set of patches for just below the
      wire.  All of these, however, fix real problems.
    
      The one thing that is genuinely scary in here is the change of SMP
      initialization, but that *does* fix a confirmed hang when booting
      virtual machines.
    
      There is also a patch to actually do the right thing about not
      offlining a CPU when there are not enough interrupt vectors available
      in the system; the accounting was done incorrectly.  The worst case
      for that patch is that we fail to offline CPUs when we should (the new
      code is strictly more conservative than the old), so is not
      particularly risky.
    
      Most of the rest is minor stuff; the EFI patches are all about
      exporting correct information to boot loaders and kexec"
    
    * 'x86/urgent' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/boot: EFI_MIXED should not prohibit loading above 4G
      x86/smpboot: Initialize secondary CPU only if master CPU will wait for it
      x86/smpboot: Log error on secondary CPU wakeup failure at ERR level
      x86: Fix list/memory corruption on CPU hotplug
      x86: irq: Get correct available vectors for cpu disable
      x86/efi: Do not export efi runtime map in case old map
      x86/efi: earlyprintk=efi,keep fix

commit 26aa7dc4730b1931dc908790944ff618d98931f5
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue May 27 16:25:34 2014 +0530

    powerpc, kexec: Fix "Processor X is stuck" issue during kexec from ST mode
    
    commit 011e4b02f1da156ac7fea28a9da878f3c23af739 upstream.
    
    If we try to perform a kexec when the machine is in ST (Single-Threaded) mode
    (ppc64_cpu --smt=off), the kexec operation doesn't succeed properly, and we
    get the following messages during boot:
    
    [    0.089866] POWER8 performance monitor hardware support registered
    [    0.089985] power8-pmu: PMAO restore workaround active.
    [    5.095419] Processor 1 is stuck.
    [   10.097933] Processor 2 is stuck.
    [   15.100480] Processor 3 is stuck.
    [   20.102982] Processor 4 is stuck.
    [   25.105489] Processor 5 is stuck.
    [   30.108005] Processor 6 is stuck.
    [   35.110518] Processor 7 is stuck.
    [   40.113369] Processor 9 is stuck.
    [   45.115879] Processor 10 is stuck.
    [   50.118389] Processor 11 is stuck.
    [   55.120904] Processor 12 is stuck.
    [   60.123425] Processor 13 is stuck.
    [   65.125970] Processor 14 is stuck.
    [   70.128495] Processor 15 is stuck.
    [   75.131316] Processor 17 is stuck.
    
    Note that only the sibling threads are stuck, while the primary threads (0, 8,
    16 etc) boot just fine. Looking closer at the previous step of kexec, we observe
    that kexec tries to wakeup (bring online) the sibling threads of all the cores,
    before performing kexec:
    
    [ 9464.131231] Starting new kernel
    [ 9464.148507] kexec: Waking offline cpu 1.
    [ 9464.148552] kexec: Waking offline cpu 2.
    [ 9464.148600] kexec: Waking offline cpu 3.
    [ 9464.148636] kexec: Waking offline cpu 4.
    [ 9464.148671] kexec: Waking offline cpu 5.
    [ 9464.148708] kexec: Waking offline cpu 6.
    [ 9464.148743] kexec: Waking offline cpu 7.
    [ 9464.148779] kexec: Waking offline cpu 9.
    [ 9464.148815] kexec: Waking offline cpu 10.
    [ 9464.148851] kexec: Waking offline cpu 11.
    [ 9464.148887] kexec: Waking offline cpu 12.
    [ 9464.148922] kexec: Waking offline cpu 13.
    [ 9464.148958] kexec: Waking offline cpu 14.
    [ 9464.148994] kexec: Waking offline cpu 15.
    [ 9464.149030] kexec: Waking offline cpu 17.
    
    Instrumenting this piece of code revealed that the cpu_up() operation actually
    fails with -EBUSY. Thus, only the primary threads of all the cores are online
    during kexec, and hence this is a sure-shot receipe for disaster, as explained
    in commit e8e5c2155b (powerpc/kexec: Fix orphaned offline CPUs across kexec),
    as well as in the comment above wake_offline_cpus().
    
    It turns out that cpu_up() was returning -EBUSY because the variable
    'cpu_hotplug_disabled' was set to 1; and this disabling of CPU hotplug was done
    by migrate_to_reboot_cpu() inside kernel_kexec().
    
    Now, migrate_to_reboot_cpu() was originally written with the assumption that
    any further code will not need to perform CPU hotplug, since we are anyway in
    the reboot path. However, kexec is clearly not such a case, since we depend on
    onlining CPUs, atleast on powerpc.
    
    So re-enable cpu-hotplug after returning from migrate_to_reboot_cpu() in the
    kexec path, to fix this regression in kexec on powerpc.
    
    Also, wrap the cpu_up() in powerpc kexec code within a WARN_ON(), so that we
    can catch such issues more easily in the future.
    
    Fixes: c97102ba963 (kexec: migrate to reboot cpu)
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a219ccf4637396a2392bfbec7c12acbfe2b06b46
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Fri Jun 6 14:37:05 2014 -0700

    smp: print more useful debug info upon receiving IPI on an offline CPU
    
    There is a longstanding problem related to CPU hotplug which causes IPIs
    to be delivered to offline CPUs, and the smp-call-function IPI handler
    code prints out a warning whenever this is detected.  Every once in a
    while this (usually harmless) warning gets reported on LKML, but so far
    it has not been completely fixed.  Usually the solution involves finding
    out the IPI sender and fixing it by adding appropriate synchronization
    with CPU hotplug.
    
    However, while going through one such internal bug reports, I found that
    there is a significant bug in the receiver side itself (more
    specifically, in stop-machine) that can lead to this problem even when
    the sender code is perfectly fine.  This patchset fixes that
    synchronization problem in the CPU hotplug stop-machine code.
    
    Patch 1 adds some additional debug code to the smp-call-function
    framework, to help debug such issues easily.
    
    Patch 2 modifies the stop-machine code to ensure that any IPIs that were
    sent while the target CPU was online, would be noticed and handled by
    that CPU without fail before it goes offline.  Thus, this avoids
    scenarios where IPIs are received on offline CPUs (as long as the sender
    uses proper hotplug synchronization).
    
    In fact, I debugged the problem by using Patch 1, and found that the
    payload of the IPI was always the block layer's trigger_softirq()
    function.  But I was not able to find anything wrong with the block
    layer code.  That's when I started looking at the stop-machine code and
    realized that there is a race-window which makes the IPI _receiver_ the
    culprit, not the sender.  Patch 2 fixes that race and hence this should
    put an end to most of the hard-to-debug IPI-to-offline-CPU issues.
    
    This patch (of 2):
    
    Today the smp-call-function code just prints a warning if we get an IPI
    on an offline CPU.  This info is sufficient to let us know that
    something went wrong, but often it is very hard to debug exactly who
    sent the IPI and why, from this info alone.
    
    In most cases, we get the warning about the IPI to an offline CPU,
    immediately after the CPU going offline comes out of the stop-machine
    phase and reenables interrupts.  Since all online CPUs participate in
    stop-machine, the information regarding the sender of the IPI is already
    lost by the time we exit the stop-machine loop.  So even if we dump the
    stack on each CPU at this point, we won't find anything useful since all
    of them will show the stack-trace of the stopper thread.  So we need a
    better way to figure out who sent the IPI and why.
    
    To achieve this, when we detect an IPI targeted to an offline CPU, loop
    through the call-single-data linked list and print out the payload
    (i.e., the name of the function which was supposed to be executed by the
    target CPU).  This would give us an insight as to who might have sent
    the IPI and help us debug this further.
    
    [akpm@linux-foundation.org: correctly suppress warning output on second and later occurrences]
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Mike Galbraith <mgalbraith@suse.de>
    Cc: Gautham R Shenoy <ego@linux.vnet.ibm.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 3e1a878b7ccdb31da6d9d2b855c72ad87afeba3f
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Thu Jun 5 15:42:45 2014 +0200

    x86/smpboot: Initialize secondary CPU only if master CPU will wait for it
    
    Hang is observed on virtual machines during CPU hotplug,
    especially in big guests with many CPUs. (It reproducible
    more often if host is over-committed).
    
    It happens because master CPU gives up waiting on
    secondary CPU and allows it to run wild. As result
    AP causes locking or crashing system. For example
    as described here:
    
       https://lkml.org/lkml/2014/3/6/257
    
    If master CPU have sent STARTUP IPI successfully,
    and AP signalled to master CPU that it's ready
    to start initialization, make master CPU wait
    indefinitely till AP is onlined.
    To ensure that AP won't ever run wild, make it
    wait at early startup till master CPU confirms its
    intention to wait for AP. If AP doesn't respond in 10
    seconds, the master CPU will timeout and cancel
    AP onlining.
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Acked-by: Toshi Kani <toshi.kani@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1401975765-22328-4-git-send-email-imammedo@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 89f898c1e195fa6235c869bb457e500b7b3ac49d
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Thu Jun 5 15:42:43 2014 +0200

    x86: Fix list/memory corruption on CPU hotplug
    
    currently if AP wake up is failed, master CPU marks AP as not
    present in do_boot_cpu() by calling set_cpu_present(cpu, false).
    That leads to following list corruption on the next physical CPU
    hotplug:
    
    [  418.107336] WARNING: CPU: 1 PID: 45 at lib/list_debug.c:33 __list_add+0xbe/0xd0()
    [  418.115268] list_add corruption. prev->next should be next (ffff88003dc57600), but was ffff88003e20c3a0. (prev=ffff88003e20c3a0).
    [  418.123693] Modules linked in: nf_conntrack_netbios_ns nf_conntrack_broadcast ipt_MASQUERADE ip6t_REJECT ipt_REJECT cfg80211 xt_conntrack rfkill ee
    [  418.138979] CPU: 1 PID: 45 Comm: kworker/u10:1 Not tainted 3.14.0-rc6+ #387
    [  418.149989] Hardware name: Red Hat KVM, BIOS 0.5.1 01/01/2007
    [  418.165750] Workqueue: kacpi_hotplug acpi_hotplug_work_fn
    [  418.166433]  0000000000000021 ffff880038ca7988 ffffffff8159b22d 0000000000000021
    [  418.176460]  ffff880038ca79d8 ffff880038ca79c8 ffffffff8106942c ffff880038ca79e8
    [  418.177453]  ffff88003e20c3a0 ffff88003dc57600 ffff88003e20c3a0 00000000ffffffea
    [  418.178445] Call Trace:
    [  418.185811]  [<ffffffff8159b22d>] dump_stack+0x49/0x5c
    [  418.186440]  [<ffffffff8106942c>] warn_slowpath_common+0x8c/0xc0
    [  418.187192]  [<ffffffff81069516>] warn_slowpath_fmt+0x46/0x50
    [  418.191231]  [<ffffffff8136ef51>] ? acpi_ns_get_node+0xb7/0xc7
    [  418.193889]  [<ffffffff812f796e>] __list_add+0xbe/0xd0
    [  418.196649]  [<ffffffff812e2aa9>] kobject_add_internal+0x79/0x200
    [  418.208610]  [<ffffffff812e2e18>] kobject_add_varg+0x38/0x60
    [  418.213831]  [<ffffffff812e2ef4>] kobject_add+0x44/0x70
    [  418.229961]  [<ffffffff813e2c60>] device_add+0xd0/0x550
    [  418.234991]  [<ffffffff813f0e95>] ? pm_runtime_init+0xe5/0xf0
    [  418.250226]  [<ffffffff813e32be>] device_register+0x1e/0x30
    [  418.255296]  [<ffffffff813e82a3>] register_cpu+0xe3/0x130
    [  418.266539]  [<ffffffff81592be5>] arch_register_cpu+0x65/0x150
    [  418.285845]  [<ffffffff81355c0d>] acpi_processor_hotadd_init+0x5a/0x9b
    ...
    Which is caused by the fact that generic_processor_info() allocates
    logical CPU id by calling:
    
     cpu = cpumask_next_zero(-1, cpu_present_mask);
    
    which returns id of previously failed to wake up CPU, since its
    bit is cleared by do_boot_cpu() and as result register_cpu()
    tries to register another CPU with the same id as already
    present but failed to be onlined CPU.
    
    Taking in account that AP will not do anything if master CPU
    failed to wake it up, there is no reason to mark that AP as not
    present and break next cpu hotplug attempts. As a side effect of
    not marking AP as not present, user would be allowed to online
    it again later.
    
    Also fix memory corruption in acpi_unmap_lsapic()
    
    if during CPU hotplug master CPU failed to wake up AP
    it set percpu x86_cpu_to_apicid to BAD_APICID=0xFFFF for AP.
    
    However following attempt to unplug that CPU will lead to
    out of bound write access to __apicid_to_node[] which is
    32768 items long on x86_64 kernel.
    
    So with above fix of cpu_present_mask make sure that a present
    CPU has a valid APIC ID by not setting x86_cpu_to_apicid
    to BAD_APICID in do_boot_cpu() on failure and allow
    acpi_processor_remove()->acpi_unmap_lsapic() cleanly remove CPU.
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Acked-by: Toshi Kani <toshi.kani@hp.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1401975765-22328-2-git-send-email-imammedo@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e2186023f2d81ee7bb42d2a7dec3d889df7cdace
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri May 23 18:15:30 2014 +1000

    powerpc/powernv: Add support for POWER8 split core on powernv
    
    Upcoming POWER8 chips support a concept called split core. This is where the
    core can be split into subcores that although not full cores, are able to
    appear as full cores to a guest.
    
    The splitting & unsplitting procedure is mildly complicated, and explained at
    length in the comments within the patch.
    
    One notable detail is that when splitting or unsplitting we need to pull
    offline cpus out of their offline state to do work as part of the procedure.
    
    The interface for changing the split mode is via a sysfs file, eg:
    
     $ echo 2 > /sys/devices/system/cpu/subcores_per_core
    
    Currently supported values are '1', '2' and '4'. And indicate respectively that
    the core should be unsplit, split in half, and split in quarters. These modes
    correspond to threads_per_subcore of 8, 4 and 2.
    
    We do not allow changing the split mode while KVM VMs are active. This is to
    prevent the value changing while userspace is configuring the VM, and also to
    prevent the mode being changed in such a way that existing guests are unable to
    be run.
    
    CPU hotplug fixes by Srivatsa.  max_cpus fixes by Mahesh.  cpuset fixes by
    benh.  Fix for irq race by paulus.  The rest by mikey and mpe.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 011e4b02f1da156ac7fea28a9da878f3c23af739
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue May 27 16:25:34 2014 +0530

    powerpc, kexec: Fix "Processor X is stuck" issue during kexec from ST mode
    
    If we try to perform a kexec when the machine is in ST (Single-Threaded) mode
    (ppc64_cpu --smt=off), the kexec operation doesn't succeed properly, and we
    get the following messages during boot:
    
    [    0.089866] POWER8 performance monitor hardware support registered
    [    0.089985] power8-pmu: PMAO restore workaround active.
    [    5.095419] Processor 1 is stuck.
    [   10.097933] Processor 2 is stuck.
    [   15.100480] Processor 3 is stuck.
    [   20.102982] Processor 4 is stuck.
    [   25.105489] Processor 5 is stuck.
    [   30.108005] Processor 6 is stuck.
    [   35.110518] Processor 7 is stuck.
    [   40.113369] Processor 9 is stuck.
    [   45.115879] Processor 10 is stuck.
    [   50.118389] Processor 11 is stuck.
    [   55.120904] Processor 12 is stuck.
    [   60.123425] Processor 13 is stuck.
    [   65.125970] Processor 14 is stuck.
    [   70.128495] Processor 15 is stuck.
    [   75.131316] Processor 17 is stuck.
    
    Note that only the sibling threads are stuck, while the primary threads (0, 8,
    16 etc) boot just fine. Looking closer at the previous step of kexec, we observe
    that kexec tries to wakeup (bring online) the sibling threads of all the cores,
    before performing kexec:
    
    [ 9464.131231] Starting new kernel
    [ 9464.148507] kexec: Waking offline cpu 1.
    [ 9464.148552] kexec: Waking offline cpu 2.
    [ 9464.148600] kexec: Waking offline cpu 3.
    [ 9464.148636] kexec: Waking offline cpu 4.
    [ 9464.148671] kexec: Waking offline cpu 5.
    [ 9464.148708] kexec: Waking offline cpu 6.
    [ 9464.148743] kexec: Waking offline cpu 7.
    [ 9464.148779] kexec: Waking offline cpu 9.
    [ 9464.148815] kexec: Waking offline cpu 10.
    [ 9464.148851] kexec: Waking offline cpu 11.
    [ 9464.148887] kexec: Waking offline cpu 12.
    [ 9464.148922] kexec: Waking offline cpu 13.
    [ 9464.148958] kexec: Waking offline cpu 14.
    [ 9464.148994] kexec: Waking offline cpu 15.
    [ 9464.149030] kexec: Waking offline cpu 17.
    
    Instrumenting this piece of code revealed that the cpu_up() operation actually
    fails with -EBUSY. Thus, only the primary threads of all the cores are online
    during kexec, and hence this is a sure-shot receipe for disaster, as explained
    in commit e8e5c2155b (powerpc/kexec: Fix orphaned offline CPUs across kexec),
    as well as in the comment above wake_offline_cpus().
    
    It turns out that cpu_up() was returning -EBUSY because the variable
    'cpu_hotplug_disabled' was set to 1; and this disabling of CPU hotplug was done
    by migrate_to_reboot_cpu() inside kernel_kexec().
    
    Now, migrate_to_reboot_cpu() was originally written with the assumption that
    any further code will not need to perform CPU hotplug, since we are anyway in
    the reboot path. However, kexec is clearly not such a case, since we depend on
    onlining CPUs, atleast on powerpc.
    
    So re-enable cpu-hotplug after returning from migrate_to_reboot_cpu() in the
    kexec path, to fix this regression in kexec on powerpc.
    
    Also, wrap the cpu_up() in powerpc kexec code within a WARN_ON(), so that we
    can catch such issues more easily in the future.
    
    Fixes: c97102ba963 (kexec: migrate to reboot cpu)
    Cc: stable@vger.kernel.org
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit caffcdd8d27ba78730d5540396ce72ad022aff2c
Author: Dietmar Eggemann <Dietmar.Eggemann@arm.com>
Date:   Wed Apr 30 14:39:38 2014 +0100

    sched: Do not zero sg->cpumask and sg->sgp->power in build_sched_groups()
    
    There is no need to zero struct sched_group member cpumask and struct
    sched_group_power member power since both structures are already allocated
    as zeroed memory in __sdt_alloc().
    
    This patch has been tested with
    BUG_ON(!cpumask_empty(sched_group_cpus(sg))); and BUG_ON(sg->sgp->power);
    in build_sched_groups() on ARM TC2 and INTEL i5 M520 platform including
    CPU hotplug scenarios.
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1398865178-12577-1-git-send-email-dietmar.eggemann@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 478c7cf7a8ff7ad587bd76f8ce9cfeede0df45fb
Merge: 23de4a7af7bc 658a0f4e661a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 21 17:58:34 2014 +0900

    Merge tag 'pm+acpi-3.15-rc6' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull ACPI and power management fixes from Rafael Wysocki:
     "Still fixing regressions (partly by reverting commits that broke
      things for people), fixing other stable-candidate bugs and adding some
      blacklist entries for ACPI video and _OSI.
    
      Two ACPICA regression fixes (one recent and one for a 3.14 commit), a
      fix for an ACPI-related regression in TPM (introduced in 3.14), a
      revert of the ACPI AC driver conversion in 3.13 that went wrong for an
      unknown reason, two reverts of commits that attempted to remove an old
      user space interface in /proc and broke some utilities, in 3.13 too, a
      fix for a CPU hotplug bug in the ACPI processor driver (stable
      material), two (stable candidate) fixes for intel_pstate and a few new
      blacklist entries, mostly for systems that shipped with Windows 8.
    
      Specifics:
    
       - ACPICA fix for a stale pointer access introduced by a recent commit
         in the XSDT validation code from Lv Zheng.
    
       - ACPICA fix for the default value of the command line switch to
         favor 32-bit FADT addresses (in case there's a conflict between a
         64-bit and a 32-bit address).  The previous default was that the
         32-bit version would take precedence and we tried to change it to
         the other way around and it didn't work.  From Lv Zheng.
    
       - A TPM commit related to ACPI _DSM in 3.14 caused the driver to
         refuse to load if a specific _DSM was missing and that broke resume
         from system suspend on Chromebooks that require the TPM hardware to
         be restored to a working state during resume by the OS.  Restore
         the old behavior to load the driver if the _DSM in question is not
         present, but prevent it from using the feature the _DSM is for.
    
       - ACPI AC driver conversion in 3.13 broke thermal management on at
         least one machine and has to be reverted.  From Guenter Roeck.
    
       - Two reverts of 3.13 commits that attempted to remove the old ACPI
         battery interface in /proc, but turned out to break some utilities
         still using that interface.  From Lan Tianyu.
    
       - ACPI processor driver fix to prevent acpi_processor_add() from
         modifying the CPU device's .offline field which leads to breakage
         if the initial online of the CPU fails.  From Igor Mammedov.
    
       - Two intel_pstate fixes, one to take a BayTrail documentation update
         into account and one to avoid forcing the maximum P-state on init
         which causes CPU PM trouble on systems with P-states coordination
         when one of the CPU cores is initialized after an offline/online
         cycle triggered by user space.  Both stable candidates, from Dirk
         Brandewie.
    
       - Fix for the ACPI video DMI blacklist entry for Dell Inspiron 7520
         from Aaron Lu.
    
       - Two new ACPI video blacklist entries for machines shipping with
         Win8 that need to use native backlight so that it can be controlled
         in a usual way (which doesn't work otherwise due bugs in the ACPI
         tables) from Hans de Goede.
    
       - Two ACPI _OSI quirks for systems that need them to work correctly
         with Linux from Edward Lin and Hans de Goede"
    
    * tag 'pm+acpi-3.15-rc6' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm:
      ACPI / video: Revert native brightness quirk for ThinkPad T530
      intel_pstate: remove setting P state to MAX on init
      ACPICA: Tables: Restore old behavor to favor 32-bit FADT addresses.
      ACPI / video: correct DMI tag for Dell Inspiron 7520
      intel_pstate: Set turbo VID for BayTrail
      ACPI / TPM: Fix resume regression on Chromebooks
      ACPI / proc: Do not say when /proc interfaces will be deleted in Kconfig
      ACPI / processor: do not mark present at boot but not onlined CPU as onlined
      ACPI: Revert "ACPI / AC: convert ACPI ac driver to platform bus"
      ACPI / blacklist: Add dmi_enable_osi_linux quirk for Asus EEE PC 1015PX
      ACPI: blacklist win8 OSI for Dell Inspiron 7737
      ACPI / video: Add use_native_backlight quirks for more systems
      ACPI: Revert "ACPI / Battery: Remove battery's proc directory"
      ACPI: Revert "ACPI: Remove CONFIG_ACPI_PROCFS_POWER and cm_sbsc.c"
      ACPICA: Tables: Fix invalid pointer accesses in acpi_tb_parse_root_table().

commit d9e9e8e2fe832180f5c8f659a63def2e8fcaea4a
Merge: a8d706986c5e 8db6e5104b77
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 27 11:21:03 2014 -0700

    Merge branch 'irq-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq fixes from Thomas Gleixner:
     "A slighlty large fix for a subtle issue in the CPU hotplug code of
      certain ARM SoCs, where the not yet online cpu needs to setup the cpu
      local timer and needs to set the interrupt affinity to itself.
      Setting interrupt affinity to a not online cpu is prohibited and
      therefor the timer interrupt ends up on the wrong cpu, which leads to
      nasty complications.
    
      The SoC folks tried to hack around that in the SoC code in some more
      than nasty ways.  The proper solution is to have a way to enforce the
      affinity setting to a not online cpu.  The core patch to the genirq
      code provides that facility and the follow up patches make use of it
      in the GIC interrupt controller and the exynos timer driver.
    
      The change to the core code has no implications to existing users,
      except for the rename of the locked function and therefor the
      necessary fixup in mips/cavium.  Aside of that, no runtime impact is
      possible, as none of the existing interrupt chips implements anything
      which depends on the force argument of the irq_set_affinity()
      callback"
    
    * 'irq-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      clocksource: Exynos_mct: Register clock event after request_irq()
      clocksource: Exynos_mct: Use irq_force_affinity() in cpu bringup
      irqchip: Gic: Support forced affinity setting
      genirq: Allow forcing cpu affinity of interrupts

commit 3e8072d48b2dd0898e99698018b2045f8cd49965
Merge: a63b747b41d6 edd10d332838
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 11 16:45:59 2014 -0700

    Merge git://git.infradead.org/users/willy/linux-nvme
    
    Pull NVMe driver updates from Matthew Wilcox:
     "Various updates to the NVMe driver.  The most user-visible change is
      that drive hotplugging now works and CPU hotplug while an NVMe drive
      is installed should also work better"
    
    * git://git.infradead.org/users/willy/linux-nvme:
      NVMe: Retry failed commands with non-fatal errors
      NVMe: Add getgeo to block ops
      NVMe: Start-stop nvme_thread during device add-remove.
      NVMe: Make I/O timeout a module parameter
      NVMe: CPU hot plug notification
      NVMe: per-cpu io queues
      NVMe: Replace DEFINE_PCI_DEVICE_TABLE
      NVMe: Fix divide-by-zero in nvme_trans_io_get_num_cmds
      NVMe: IOCTL path RCU protect queue access
      NVMe: RCU protected access to io queues
      NVMe: Initialize device reference count earlier
      NVMe: Add CONFIG_PM_SLEEP to suspend/resume functions

commit eeb91e4f9d578a6a8cc25a9603d4d62f2ec00db5
Merge: 40e9963e622c 19ce7f3f3110
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 11 13:20:04 2014 -0700

    Merge tag 'pm+acpi-3.15-rc1-3' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull more ACPI and power management fixes and updates from Rafael Wysocki:
     "This is PM and ACPI material that has emerged over the last two weeks
      and one fix for a CPU hotplug regression introduced by the recent CPU
      hotplug notifiers registration series.
    
      Included are intel_idle and turbostat updates from Len Brown (these
      have been in linux-next for quite some time), a new cpufreq driver for
      powernv (that might spend some more time in linux-next, but BenH was
      asking me so nicely to push it for 3.15 that I couldn't resist), some
      cpufreq fixes and cleanups (including fixes for some silly breakage in
      a couple of cpufreq drivers introduced during the 3.14 cycle),
      assorted ACPI cleanups, wakeup framework documentation fixes, a new
      sysfs attribute for cpuidle and a new command line argument for power
      domains diagnostics.
    
      Specifics:
    
       - Fix for a recently introduced CPU hotplug regression in ARM KVM
         from Ming Lei.
    
       - Fixes for breakage in the at32ap, loongson2_cpufreq, and unicore32
         cpufreq drivers introduced during the 3.14 cycle (-stable material)
         from Chen Gang and Viresh Kumar.
    
       - New powernv cpufreq driver from Vaidyanathan Srinivasan, with bits
         from Gautham R Shenoy and Srivatsa S Bhat.
    
       - Exynos cpufreq driver fix preventing it from being included into
         multiplatform builds that aren't supported by it from Sachin Kamat.
    
       - cpufreq cleanups related to the usage of the driver_data field in
         struct cpufreq_frequency_table from Viresh Kumar.
    
       - cpufreq ppc driver cleanup from Sachin Kamat.
    
       - Intel BayTrail support for intel_idle and ACPI idle from Len Brown.
    
       - Intel CPU model 54 (Atom N2000 series) support for intel_idle from
         Jan Kiszka.
    
       - intel_idle fix for Intel Ivy Town residency targets from Len Brown.
    
       - turbostat updates (Intel Broadwell support and output cleanups)
         from Len Brown.
    
       - New cpuidle sysfs attribute for exporting C-states' target
         residency information to user space from Daniel Lezcano.
    
       - New kernel command line argument to prevent power domains enabled
         by the bootloader from being turned off even if they are not in use
         (for diagnostics purposes) from Tushar Behera.
    
       - Fixes for wakeup sysfs attributes documentation from Geert
         Uytterhoeven.
    
       - New ACPI video blacklist entry for ThinkPad Helix from Stephen
         Chandler Paul.
    
       - Assorted ACPI cleanups and a Kconfig help update from Jonghwan
         Choi, Zhihui Zhang, Hanjun Guo"
    
    * tag 'pm+acpi-3.15-rc1-3' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (28 commits)
      ACPI: Update the ACPI spec information in Kconfig
      arm, kvm: fix double lock on cpu_add_remove_lock
      cpuidle: sysfs: Export target residency information
      cpufreq: ppc: Remove duplicate inclusion of fsl_soc.h
      cpufreq: create another field .flags in cpufreq_frequency_table
      cpufreq: use kzalloc() to allocate memory for cpufreq_frequency_table
      cpufreq: don't print value of .driver_data from core
      cpufreq: ia64: don't set .driver_data to index
      cpufreq: powernv: Select CPUFreq related Kconfig options for powernv
      cpufreq: powernv: Use cpufreq_frequency_table.driver_data to store pstate ids
      cpufreq: powernv: cpufreq driver for powernv platform
      cpufreq: at32ap: don't declare local variable as static
      cpufreq: loongson2_cpufreq: don't declare local variable as static
      cpufreq: unicore32: fix typo issue for 'clk'
      cpufreq: exynos: Disable on multiplatform build
      PM / wakeup: Correct presence vs. emptiness of wakeup_* attributes
      PM / domains: Add pd_ignore_unused to keep power domains enabled
      ACPI / dock: Drop dock_device_ids[] table
      ACPI / video: Favor native backlight interface for ThinkPad Helix
      ACPI / thermal: Fix wrong variable usage in debug statement
      ...

commit 553f809e23f00976caea7a1ebdabaa58a6383e7d
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Mon Apr 7 01:36:08 2014 +0800

    arm, kvm: fix double lock on cpu_add_remove_lock
    
    Commit 8146875de7d4 (arm, kvm: Fix CPU hotplug callback registration)
    holds the lock before calling the two functions:
    
            kvm_vgic_hyp_init()
            kvm_timer_hyp_init()
    
    and both the two functions are calling register_cpu_notifier()
    to register cpu notifier, so cause double lock on cpu_add_remove_lock.
    
    Considered that both two functions are only called inside
    kvm_arch_init() with holding cpu_add_remove_lock, so simply use
    __register_cpu_notifier() to fix the problem.
    
    Fixes: 8146875de7d4 (arm, kvm: Fix CPU hotplug callback registration)
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 467a9e1633043810259a7f5368fbcc1e84746137
Merge: b8780c363d80 a0e247a80592
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 7 14:55:46 2014 -0700

    Merge tag 'cpu-hotplug-3.15-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull CPU hotplug notifiers registration fixes from Rafael Wysocki:
     "The purpose of this single series of commits from Srivatsa S Bhat
      (with a small piece from Gautham R Shenoy) touching multiple
      subsystems that use CPU hotplug notifiers is to provide a way to
      register them that will not lead to deadlocks with CPU online/offline
      operations as described in the changelog of commit 93ae4f978ca7f ("CPU
      hotplug: Provide lockless versions of callback registration
      functions").
    
      The first three commits in the series introduce the API and document
      it and the rest simply goes through the users of CPU hotplug notifiers
      and converts them to using the new method"
    
    * tag 'cpu-hotplug-3.15-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (52 commits)
      net/iucv/iucv.c: Fix CPU hotplug callback registration
      net/core/flow.c: Fix CPU hotplug callback registration
      mm, zswap: Fix CPU hotplug callback registration
      mm, vmstat: Fix CPU hotplug callback registration
      profile: Fix CPU hotplug callback registration
      trace, ring-buffer: Fix CPU hotplug callback registration
      xen, balloon: Fix CPU hotplug callback registration
      hwmon, via-cputemp: Fix CPU hotplug callback registration
      hwmon, coretemp: Fix CPU hotplug callback registration
      thermal, x86-pkg-temp: Fix CPU hotplug callback registration
      octeon, watchdog: Fix CPU hotplug callback registration
      oprofile, nmi-timer: Fix CPU hotplug callback registration
      intel-idle: Fix CPU hotplug callback registration
      clocksource, dummy-timer: Fix CPU hotplug callback registration
      drivers/base/topology.c: Fix CPU hotplug callback registration
      acpi-cpufreq: Fix CPU hotplug callback registration
      zsmalloc: Fix CPU hotplug callback registration
      scsi, fcoe: Fix CPU hotplug callback registration
      scsi, bnx2fc: Fix CPU hotplug callback registration
      scsi, bnx2i: Fix CPU hotplug callback registration
      ...

commit bdfc7cbdeef8cadba0e5793079ac0130b8e2220c
Merge: 62d1a3ba5adc ade63aada79c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 2 13:40:50 2014 -0700

    Merge branch 'mips-for-linux-next' of git://git.linux-mips.org/pub/scm/ralf/upstream-sfr
    
    Pull MIPS updates from Ralf Baechle:
     - Support for Imgtec's Aptiv family of MIPS cores.
     - Improved detection of BCM47xx configurations.
     - Fix hiberation for certain configurations.
     - Add support for the Chinese Loongson 3 CPU, a MIPS64 R2 core and
       systems.
     - Detection and support for the MIPS P5600 core.
     - A few more random fixes that didn't make 3.14.
     - Support for the EVA Extended Virtual Addressing
     - Switch Alchemy to the platform PATA driver
     - Complete unification of Alchemy support
     - Allow availability of I/O cache coherency to be runtime detected
     - Improvments to multiprocessing support for Imgtec platforms
     - A few microoptimizations
     - Cleanups of FPU support
     - Paul Gortmaker's fixes for the init stuff
     - Support for seccomp
    
    * 'mips-for-linux-next' of git://git.linux-mips.org/pub/scm/ralf/upstream-sfr: (165 commits)
      MIPS: CPC: Use __raw_ memory access functions
      MIPS: CM: use __raw_ memory access functions
      MIPS: Fix warning when including smp-ops.h with CONFIG_SMP=n
      MIPS: Malta: GIC IPIs may be used without MT
      MIPS: smp-mt: Use common GIC IPI implementation
      MIPS: smp-cmp: Remove incorrect core number probe
      MIPS: Fix gigaton of warning building with microMIPS.
      MIPS: Fix core number detection for MT cores
      MIPS: MT: core_nvpes function to retrieve VPE count
      MIPS: Provide empty mips_mt_set_cpuoptions when CONFIG_MIPS_MT=n
      MIPS: Lasat: Replace del_timer by del_timer_sync
      MIPS: Malta: Setup PM I/O region on boot
      MIPS: Loongson: Add a Loongson-3 default config file
      MIPS: Loongson 3: Add CPU hotplug support
      MIPS: Loongson 3: Add Loongson-3 SMP support
      MIPS: Loongson: Add Loongson-3 Kconfig options
      MIPS: Loongson: Add swiotlb to support All-Memory DMA
      MIPS: Loongson 3: Add serial port support
      MIPS: Loongson 3: Add IRQ init and dispatch support
      MIPS: Loongson 3: Add HT-linked PCI support
      ...

commit 778c4783df0ff6dcc120fe5c1dc9b6afa1980cd0
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Feb 6 03:42:45 2014 +0530

    md/raid5: Fix CPU hotplug callback registration
    
    commit 789b5e0315284463617e106baad360cb9e8db3ac upstream.
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Interestingly, the raid5 code can actually prevent double initialization and
    hence can use the following simplified form of callback registration:
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            put_online_cpus();
    
    A hotplug operation that occurs between registering the notifier and calling
    get_online_cpus(), won't disrupt anything, because the code takes care to
    perform the memory allocations only once.
    
    So reorganize the code in raid5 this way to fix the deadlock with callback
    registration.
    
    Cc: linux-raid@vger.kernel.org
    Fixes: 36d1c6476be51101778882897b315bd928c8c7b5
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    [Srivatsa: Fixed the unregister_cpu_notifier() deadlock, added the
    free_scratch_buffer() helper to condense code further and wrote the changelog.]
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 6ed7705167dd056c61b1954fe1366fcddb5bda98
Merge: 3a88fe3b748b ea7bdc65bca8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 31 11:58:45 2014 -0700

    Merge branch 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 apic changes from Ingo Molnar:
     "An xAPIC CPU hotplug race fix, plus cleanups and minor fixes"
    
    * 'x86-apic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/apic: Plug racy xAPIC access of CPU hotplug code
      x86/apic: Always define nox2apic and define it as initdata
      x86/apic: Remove unused function prototypes
      x86/apic: Switch wait_for_init_deassert() to a bool flag
      x86/apic: Only use default_wait_for_init_deassert()

commit 3a88fe3b748b5bcf1c2c739ad63dab8364fd5b9b
Merge: 971eae7c9921 896dc5064083
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 31 11:58:08 2014 -0700

    Merge branch 'x86-acpi-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 acpi numa fix from Ingo Molnar:
     "A single NUMA CPU hotplug fix"
    
    * 'x86-acpi-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, acpi: Fix bug in associating hot-added CPUs with corresponding NUMA node

commit c4a987db1b3cd89207cece4b8121c09cbfbc978a
Author: Huacai Chen <chenhc@lemote.com>
Date:   Fri Mar 21 18:44:09 2014 +0800

    MIPS: Loongson 3: Add CPU hotplug support
    
    Tips of Loongson's CPU hotplug:
    1, To fully shutdown a core in Loongson 3, the target core should go to
       CKSEG1 and flush all L1 cache entries at first. Then, another core
       (usually Core 0) can safely disable the clock of the target core. So
       play_dead() call loongson3_play_dead() via CKSEG1 (both uncached and
       unmmaped).
    2, The default clocksource of Loongson is MIPS. Since clock source is a
       global device, timekeeping need the CP0' Count registers of each core
       be synchronous. Thus, when a core is up, we use a SMP_ASK_C0COUNT IPI
       to ask Core-0's Count.
    
    Signed-off-by: Huacai Chen <chenhc@lemote.com>
    Signed-off-by: Hongliang Tao <taohl@lemote.com>
    Signed-off-by: Hua Yan <yanh@lemote.com>
    Tested-by: Alex Smith <alex.smith@imgtec.com>
    Reviewed-by: Alex Smith <alex.smith@imgtec.com>
    Cc: John Crispin <john@phrozen.org>
    Cc: Steven J. Hill <Steven.Hill@imgtec.com>
    Cc: Aurelien Jarno <aurelien@aurel32.net>
    Cc: linux-mips@linux-mips.org
    Cc: Fuxin Zhang <zhangfx@lemote.com>
    Cc: Zhangjin Wu <wuzhangjin@gmail.com>
    Patchwork: https://patchwork.linux-mips.org/patch/6639
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit a0e247a8059223593f9c5c3d5c1fd50eedf415c0
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:12:57 2014 +0530

    net/iucv/iucv.c: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the code in net/iucv/iucv.c by using this latter form of callback
    registration. Also, provide helper functions to perform the common memory
    allocations and frees, to condense repetitive code.
    
    Cc: Ursula Braun <ursula.braun@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit e30a293e8ad7e6048d6d88bcc114094f964bd67b
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:12:51 2014 +0530

    net/core/flow.c: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the code in net/core/flow.c by using this latter form of callback
    registration.
    
    Cc: Li RongQing <roy.qing.li@gmail.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 576378249c8e0a020aafeaa702c834dff81dd596
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:12:40 2014 +0530

    mm, zswap: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the zswap code by using this latter form of callback registration.
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 0be94bad0b601df94b8558c0cbd28f7e6633c9e8
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:12:27 2014 +0530

    mm, vmstat: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the vmstat code in the MM subsystem by using this latter form of callback
    registration.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Cody P Schafer <cody@linux.vnet.ibm.com>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit c270a817196a9374a2dc730624d1501dced40b4d
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:12:08 2014 +0530

    profile: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the profile code by using this latter form of callback registration.
    
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Mauro Carvalho Chehab <mchehab@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit d39ad278a3001c860da4d7c13e51259b1904bec5
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:11:56 2014 +0530

    trace, ring-buffer: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the tracing ring-buffer code by using this latter form of callback
    registration.
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 43ea9536499e80d858e26cecb7abcc893f86d222
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:11:45 2014 +0530

    xen, balloon: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    The xen balloon driver doesn't take get/put_online_cpus() around this code,
    but that is also buggy, since it can miss CPU hotplug events in between the
    initialization and callback registration:
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
                       ^
                       |  Race window; Can miss CPU hotplug events here.
                       v
            register_cpu_notifier(&foobar_cpu_notifier);
    
    Interestingly, the balloon code in xen can simply be reorganized as shown
    below, to have a race-free method to register hotplug callbacks, without even
    taking get/put_online_cpus(). This is because the initialization performed for
    already online CPUs is exactly the same as that performed for CPUs that come
    online later. Moreover, the code has checks in place to avoid double
    initialization.
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            put_online_cpus();
    
    A hotplug operation that occurs between registering the notifier and calling
    get_online_cpus(), won't disrupt anything, because the code takes care to
    perform the memory allocations only once.
    
    So reorganize the balloon code in xen this way to fix the issues with CPU
    hotplug callback registration.
    
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 2480b6a3e5e80d778455f8138ae6d6efb568cd59
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:11:33 2014 +0530

    hwmon, via-cputemp: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the hwmon via-cputemp code by using this latter form of callback
    registration.
    
    Cc: Jean Delvare <jdelvare@suse.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 3289705fe2b429569f37730ecf660719b8924420
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:11:11 2014 +0530

    hwmon, coretemp: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the hwmon coretemp code by using this latter form of callback
    registration.
    
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Jean Delvare <jdelvare@suse.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit cf0485a2ac70acb1bc83f6310a7ebef3070f0333
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:10:54 2014 +0530

    thermal, x86-pkg-temp: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the thermal x86-pkg-temp code by using this latter form of callback
    registration.
    
    Cc: Zhang Rui <rui.zhang@intel.com>
    Cc: Eduardo Valentin <eduardo.valentin@ti.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 99c3bf361a4afd2ad331e767b3e5e6fa3488a096
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:10:43 2014 +0530

    octeon, watchdog: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the watchdog code in octeon by using this latter form of callback
    registration.
    
    Cc: Wim Van Sebroeck <wim@iguana.be>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 180d86463257812dc17e5df912f3bddcc96abb00
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:10:36 2014 +0530

    oprofile, nmi-timer: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the nmi-timer code in oprofile by using this latter form of callback
    registration.
    
    Cc: Robert Richter <rric@kernel.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 07494d547e92bde6857522d2a92ff70896aecadb
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:10:30 2014 +0530

    intel-idle: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the intel-idle code by using this latter form of callback registration.
    
    Cc: Len Brown <lenb@kernel.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 8daa127f4e857427c66f365f650dda90f459aa54
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:10:23 2014 +0530

    clocksource, dummy-timer: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the clocksource dummy-timer code by using this latter form of callback
    registration.
    
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit e12b711196a158a475350ee67876a1e9e2601661
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:10:12 2014 +0530

    drivers/base/topology.c: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the topology code by using this latter form of callback registration.
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 0197fbd212461f25666272b9e4654f2ccd94cff8
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:10:06 2014 +0530

    acpi-cpufreq: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the acpi-cpufreq code by using this latter form of callback registration.
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit f0e71fcd0fa6f3f5495cd9ad3f1e4acd94446a55
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:09:59 2014 +0530

    zsmalloc: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the zsmalloc code by using this latter form of callback registration.
    
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit cd45ae38035d1b7f98dd92429b59bbc8ba9443e9
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:09:52 2014 +0530

    scsi, fcoe: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the fcoe code in scsi by using this latter form of callback registration.
    
    Cc: Robert Love <robert.w.love@intel.com>
    Cc: "James E.J. Bottomley" <JBottomley@parallels.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 7229b6d0b2a9e62d5eb4886dcd0137664c735718
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:09:45 2014 +0530

    scsi, bnx2fc: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the bnx2fc code in scsi by using this latter form of callback
    registration.
    
    Cc: Eddie Wai <eddie.wai@broadcom.com>
    Cc: "James E.J. Bottomley" <JBottomley@parallels.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit bc0003c93f0e68b5383fb36579f7efa267b7dc16
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:09:39 2014 +0530

    scsi, bnx2i: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the bnx2i code in scsi by using this latter form of callback registration.
    
    Cc: Eddie Wai <eddie.wai@broadcom.com>
    Cc: "James E.J. Bottomley" <JBottomley@parallels.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 009f225ef050d231ebb7b87264a21a7daac0f175
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:09:26 2014 +0530

    powercap, intel-rapl: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the intel-rapl code in the powercap driver by using this latter form
    of callback registration. But retain the calls to get/put_online_cpus(),
    since they also protect the function rapl_cleanup_data(). By nesting
    get/put_online_cpus() *inside* cpu_notifier_register_begin/done(), we avoid
    the ABBA deadlock possibility mentioned above.
    
    Cc: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Tested-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 4b0b68af37b930403cf9074c0cf504fc2387c2fa
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:09:20 2014 +0530

    arm64, debug-monitors: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the debug-monitors code in arm64 by using this latter form of callback
    registration.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 3d0dc643f81ed64e2a7f839e8491728addbc51b9
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:09:08 2014 +0530

    arm64, hw_breakpoint.c: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the hw-breakpoint code in arm64 by using this latter form of callback
    registration.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 460dd42e1187047427e265edc451cb115341b74b
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:09:01 2014 +0530

    x86, kvm: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the kvm code in x86 by using this latter form of callback registration.
    
    Cc: Gleb Natapov <gleb@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 76902e3d9ce88ebd70e362e4d3ff173afa5ce575
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:08:49 2014 +0530

    x86, oprofile, nmi: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the oprofile code in x86 by using this latter form of callback
    registration. But retain the calls to get/put_online_cpus(), since they are
    used in other places as well, to protect the variables 'nmi_enabled' and
    'ctr_running'. Strictly speaking, this is not necessary since
    cpu_notifier_register_begin/done() provide a stronger synchronization
    with CPU hotplug than get/put_online_cpus(). However, let's retain the
    calls to get/put_online_cpus() to be consistent with the other call-sites.
    
    By nesting get/put_online_cpus() *inside* cpu_notifier_register_begin/done(),
    we avoid the ABBA deadlock possibility mentioned above.
    
    Cc: Robert Richter <rric@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 9f668f6661698d320df552fccc7a3dab0de89d13
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:08:43 2014 +0530

    x86, pci, amd-bus: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the amd-bus code in x86 by using this latter form of callback
    registration.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Acked-by: Bjorn Helgaas <bhelgaas@google.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 9014ad2a500cb3d260987cb5d63fd940f6bbbbe1
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:08:36 2014 +0530

    x86, hpet: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the hpet code in x86 by using this latter form of callback registration.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit a8c17c2951f3e198982dd2416b068183fc9fcc7e
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:08:29 2014 +0530

    x86, amd, uncore: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the amd-uncore code in x86 by using this latter form of callback
    registration.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit fd537e56f65c61bb4cece99ac3a1a1bef6676df9
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:08:09 2014 +0530

    x86, intel, rapl: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the intel rapl code in x86 by using this latter form of callback
    registration.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 8c60ea146499b9d2a81ceb5e3e0bd215ef0b6287
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:07:57 2014 +0530

    x86, intel, cacheinfo: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the intel cacheinfo code in x86 by using this latter form of callback
    registration.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 047868ce2952bb6a86ccea3ecc6fd59faa9062a7
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:07:45 2014 +0530

    x86, amd, ibs: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the amd-ibs code in x86 by using this latter form of callback
    registration.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 7b7139d4abe8a94446db6a91882f1400920891d5
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:07:28 2014 +0530

    x86, therm_throt.c: Remove unused therm_cpu_lock
    
    After fixing the CPU hotplug callback registration code, the callbacks
    invoked for each online CPU, during the initialization phase in
    thermal_throttle_init_device(), can no longer race with the actual CPU
    hotplug notifier callbacks (in thermal_throttle_cpu_callback). Hence the
    therm_cpu_lock is unnecessary now. Remove it.
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 4e6192bbec2ec21d2a6685987674a1c73e810de1
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:07:21 2014 +0530

    x86, therm_throt.c: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the thermal throttle code in x86 by using this latter form of callback
    registration.
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 82a8f131aadf55ac7fbc8c6f65f34d83101160de
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:07:04 2014 +0530

    x86, mce: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the mce code in x86 by using this latter form of callback registration.
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 2c666adacc9e4b00b918553ca136b8eef0ec3c2c
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:06:57 2014 +0530

    x86, intel, uncore: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the uncore code in intel-x86 by using this latter form of callback
    registration.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 42112a0f5d34601e5d0a1bc7c769ba39adda0769
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:06:50 2014 +0530

    x86, vsyscall: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the vsyscall code in x86 by using this latter form of callback
    registration.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 4b660b384da7b47996436cc3151ca5995c121c76
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:06:44 2014 +0530

    x86, cpuid: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the cpuid code in x86 by using this latter form of callback registration.
    
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit de82a01befdd33f6a1c050c7f888e2fa9949f48e
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:06:37 2014 +0530

    x86, msr: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the msr code in x86 by using this latter form of callback registration.
    
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit d1a5511390b0eb242c70ab977abff28644f66a5a
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:06:31 2014 +0530

    powerpc, sysfs: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the sysfs code in powerpc by using this latter form of callback
    registration.
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Olof Johansson <olof@lixom.net>
    Cc: Wang Dongsheng <dongsheng.wang@freescale.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 1e0b0c4c6b9241b66c6032e14962a149f852f23a
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:06:09 2014 +0530

    sparc, sysfs: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the sysfs code in sparc by using this latter form of callback
    registration.
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit f4edbcd5d1f350b90583c8f3e0e7112b28f65e62
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:05:58 2014 +0530

    s390, smp: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the smp code in s390 by using this latter form of callback registration.
    
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 6575080e671f3675a4c47b61c38556d54e028ca8
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:05:46 2014 +0530

    s390, cacheinfo: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the cacheinfo code in s390 by using this latter form of callback
    registration.
    
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 8146875de7d47c632921ea8284097a1ff0601ee7
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 18 15:53:05 2014 +0530

    arm, kvm: Fix CPU hotplug callback registration
    
    On 03/15/2014 12:40 AM, Christoffer Dall wrote:
    > On Fri, Mar 14, 2014 at 11:13:29AM +0530, Srivatsa S. Bhat wrote:
    >> On 03/13/2014 04:51 AM, Christoffer Dall wrote:
    >>> On Tue, Mar 11, 2014 at 02:05:38AM +0530, Srivatsa S. Bhat wrote:
    >>>> Subsystems that want to register CPU hotplug callbacks, as well as perform
    >>>> initialization for the CPUs that are already online, often do it as shown
    >>>> below:
    >>>>
    [...]
    >>> Just so we're clear, the existing code was simply racy as not prone to
    >>> deadlocks, right?
    >>>
    >>> This makes it clear that the test above for compatible CPUs can be quite
    >>> easily evaded by using CPU hotplug, but we don't really have a good
    >>> solution for handling that yet...  Hmmm, grumble grumble, I guess if you
    >>> hotplug unsupported CPUs on a KVM/ARM system for now, stuff will break.
    >>>
    >>
    >> In this particular case, there was no deadlock possibility, rather the
    >> existing code had insufficient synchronization against CPU hotplug.
    >>
    >> init_hyp_mode() would invoke cpu_init_hyp_mode() on currently online CPUs
    >> using on_each_cpu(). If a CPU came online after this point and before calling
    >> register_cpu_notifier(), that CPU would remain uninitialized because this
    >> subsystem would miss the hot-online event. This patch fixes this bug and
    >> also uses the new synchronization method (instead of get/put_online_cpus())
    >> to ensure that we don't deadlock with CPU hotplug.
    >>
    >
    > Yes, that was my conclusion as well.  Thanks for clarifying.  (It could
    > be noted in the commit message as well if you should feel so inclined).
    >
    
    Please find the patch with updated changelog (and your Ack) below.
    (No changes in code).
    
    From: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Subject: [PATCH] arm, kvm: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    In the existing arm kvm code, there is no synchronization with CPU hotplug
    to avoid missing the hotplug events that might occur after invoking
    init_hyp_mode() and before calling register_cpu_notifier(). Fix this bug
    and also use the new synchronization method (instead of get/put_online_cpus())
    to ensure that we don't deadlock with CPU hotplug.
    
    Cc: Gleb Natapov <gleb@kernel.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit c5929bd3a9920432dfb485253c64163fdfc90faf
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:05:21 2014 +0530

    arm, hw-breakpoint: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the hw-breakpoint code in arm by using this latter form of callback
    registration.
    
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit f2e48a89053dea259d7f96cf0fd1cfc7c4b34d80
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:05:10 2014 +0530

    ia64, err-inject: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the error injection code in ia64 by using this latter form of callback
    registration.
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit f5a7d445ffb6b251dc41942ffe41f515e8d639a4
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:04:58 2014 +0530

    ia64, topology: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the topology code in ia64 by using this latter form of callback
    registration.
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 9f37bca9a9e97fcd7589492aba7b8a5310026c47
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:04:52 2014 +0530

    ia64, palinfo: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the palinfo code in ia64 by using this latter form of callback
    registration.
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit eff722b06bdd6de326894ea2353ec0eda3e5ed34
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:04:45 2014 +0530

    ia64, salinfo: Fix hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the salinfo code in ia64 by using this latter form of callback
    registration.
    
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit f0bdb5e0c72b7347c867da539367138ad95c6b24
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:04:39 2014 +0530

    CPU hotplug, perf: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the perf subsystem's hotplug notifier by using this latter form of
    callback registration.
    
    Also provide a bare-bones version of perf_cpu_notifier() that doesn't
    invoke the notifiers for the already online CPUs. This would be useful
    for subsystems that need to perform a different set of initialization
    for the already online CPUs, or don't need the initialization altogether.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 8489d90b5f32effa402be2a4ee7636ba0c652145
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:04:32 2014 +0530

    Doc/cpu-hotplug: Specify race-free way to register CPU hotplug callbacks
    
    Recommend the usage of the new CPU hotplug callback registration APIs
    (__register_cpu_notifier() etc), when subsystems need to also perform
    initialization for already online CPUs. Provide examples of correct
    and race-free ways of achieving this, and point out the kinds of code
    that are error-prone.
    
    Cc: Rob Landley <rob@landley.net>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 93ae4f978ca7f26d17df915ac7afc919c1dd0353
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:04:14 2014 +0530

    CPU hotplug: Provide lockless versions of callback registration functions
    
    The following method of CPU hotplug callback registration is not safe
    due to the possibility of an ABBA deadlock involving the cpu_add_remove_lock
    and the cpu_hotplug.lock.
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    The deadlock is shown below:
    
              CPU 0                                         CPU 1
              -----                                         -----
    
       Acquire cpu_hotplug.lock
       [via get_online_cpus()]
    
                                                  CPU online/offline operation
                                                  takes cpu_add_remove_lock
                                                  [via cpu_maps_update_begin()]
    
       Try to acquire
       cpu_add_remove_lock
       [via register_cpu_notifier()]
    
                                                  CPU online/offline operation
                                                  tries to acquire cpu_hotplug.lock
                                                  [via cpu_hotplug_begin()]
    
                                *** DEADLOCK! ***
    
    The problem here is that callback registration takes the locks in one order
    whereas the CPU hotplug operations take the same locks in the opposite order.
    To avoid this issue and to provide a race-free method to register CPU hotplug
    callbacks (along with initialization of already online CPUs), introduce new
    variants of the callback registration APIs that simply register the callbacks
    without holding the cpu_add_remove_lock during the registration. That way,
    we can avoid the ABBA scenario. However, we will need to hold the
    cpu_add_remove_lock throughout the entire critical section, to protect updates
    to the callback/notifier chain.
    
    This can be achieved by writing the callback registration code as follows:
    
            cpu_maps_update_begin(); [ or cpu_notifier_register_begin(); see below ]
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* This doesn't take the cpu_add_remove_lock */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_maps_update_done();  [ or cpu_notifier_register_done(); see below ]
    
    Note that we can't use get_online_cpus() here instead of cpu_maps_update_begin()
    because the cpu_hotplug.lock is dropped during the invocation of CPU_POST_DEAD
    notifiers, and hence get_online_cpus() cannot provide the necessary
    synchronization to protect the callback/notifier chains against concurrent
    reads and writes. On the other hand, since the cpu_add_remove_lock protects
    the entire hotplug operation (including CPU_POST_DEAD), we can use
    cpu_maps_update_begin/done() to guarantee proper synchronization.
    
    Also, since cpu_maps_update_begin/done() is like a super-set of
    get/put_online_cpus(), the former naturally protects the critical sections
    from concurrent hotplug operations.
    
    Since the names cpu_maps_update_begin/done() don't make much sense in CPU
    hotplug callback registration scenarios, we'll introduce new APIs named
    cpu_notifier_register_begin/done() and map them to cpu_maps_update_begin/done().
    
    In summary, introduce the lockless variants of un/register_cpu_notifier() and
    also export the cpu_notifier_register_begin/done() APIs for use by modules.
    This way, we provide a race-free way to register hotplug callbacks as well as
    perform initialization for the CPUs that are already online.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Toshi Kani <toshi.kani@hp.com>
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit a19423b98704aa85e84097be6d1d44a8615c2340
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:04:03 2014 +0530

    CPU hotplug: Add lockdep annotations to get/put_online_cpus()
    
    Add lockdep annotations for get/put_online_cpus() and
    cpu_hotplug_begin()/cpu_hotplug_end().
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit ea7bdc65bca8cf837a63e0ff7b75daed83222511
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Mon Jan 27 20:14:06 2014 +0100

    x86/apic: Plug racy xAPIC access of CPU hotplug code
    
    apic_icr_write() and its users in smpboot.c were apparently
    written under the assumption that this code would only run
    during early boot. But nowadays we also execute it when onlining
    a CPU later on while the system is fully running. That will make
    wakeup_cpu_via_init_nmi and, thus, also native_apic_icr_write
    run in plain process context. If we migrate the caller to a
    different CPU at the wrong time or interrupt it and write to
    ICR/ICR2 to send unrelated IPIs, we can end up sending INIT,
    SIPI or NMIs to wrong CPUs.
    
    Fix this by disabling interrupts during the write to the ICR
    halves and disable preemption around waiting for ICR
    availability and using it.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Tested-By: Igor Mammedov <imammedo@redhat.com>
    Link: http://lkml.kernel.org/r/52E6AFFE.3030004@siemens.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d2a36071ef8dd24dceb95c3d9b05aaeac987b447
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Dec 10 11:31:02 2013 +1100

    powerpc/pseries: Don't try to register pseries cpu hotplug on non-pseries
    
    This results in oddball messages at boot on other platforms telling us
    that CPU hotplug isn't supported even when it is.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 6fd67d8581106f59fdfe07b283477ffd7864dbb9
Author: Oleg Drokin <green@linuxhacker.ru>
Date:   Fri Feb 28 21:16:46 2014 -0500

    lustre/libcfs: warn if all HTs in a core are gone
    
    libcfs cpu partition can't support CPU hotplug, but it is safe
    when plug-in new CPU or enabling/disabling hyper-threading.
    It has potential risk only if plug-out CPU because it may break CPU
    affinity of Lustre threads.
    
    Current libcfs will print warning for all CPU notification, this
    patch changed this behavior and only output warning when we lost all
    HTs in a CPU core which may have broken affinity of Lustre threads.
    
    Signed-off-by: Liang Zhen <liang.zhen@intel.com>
    Reviewed-on: http://review.whamcloud.com/8770
    Intel-bug-id: https://jira.hpdd.intel.com/browse/LU-4454
    Reviewed-by: Bobi Jam <bobijam@gmail.com>
    Reviewed-by: Andreas Dilger <andreas.dilger@intel.com>
    Signed-off-by: Oleg Drokin <oleg.drokin@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit fdc1e9ab458fe3051228cb38632f5977e7066572
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Feb 6 03:42:45 2014 +0530

    md/raid5: Fix CPU hotplug callback registration
    
    commit 789b5e0315284463617e106baad360cb9e8db3ac upstream.
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Interestingly, the raid5 code can actually prevent double initialization and
    hence can use the following simplified form of callback registration:
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            put_online_cpus();
    
    A hotplug operation that occurs between registering the notifier and calling
    get_online_cpus(), won't disrupt anything, because the code takes care to
    perform the memory allocations only once.
    
    So reorganize the code in raid5 this way to fix the deadlock with callback
    registration.
    
    Cc: linux-raid@vger.kernel.org
    Fixes: 36d1c6476be51101778882897b315bd928c8c7b5
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    [Srivatsa: Fixed the unregister_cpu_notifier() deadlock, added the
    free_scratch_buffer() helper to condense code further and wrote the changelog.]
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 2d470eab8a6136e063773be8c28794336c1055b8
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Feb 6 03:42:45 2014 +0530

    md/raid5: Fix CPU hotplug callback registration
    
    commit 789b5e0315284463617e106baad360cb9e8db3ac upstream.
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Interestingly, the raid5 code can actually prevent double initialization and
    hence can use the following simplified form of callback registration:
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            put_online_cpus();
    
    A hotplug operation that occurs between registering the notifier and calling
    get_online_cpus(), won't disrupt anything, because the code takes care to
    perform the memory allocations only once.
    
    So reorganize the code in raid5 this way to fix the deadlock with callback
    registration.
    
    Cc: linux-raid@vger.kernel.org
    Fixes: 36d1c6476be51101778882897b315bd928c8c7b5
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    [Srivatsa: Fixed the unregister_cpu_notifier() deadlock, added the
    free_scratch_buffer() helper to condense code further and wrote the changelog.]
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 4d4ef86d4438e741960a9c9b615b2fb3e7c2f779
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Feb 6 03:42:45 2014 +0530

    md/raid5: Fix CPU hotplug callback registration
    
    commit 789b5e0315284463617e106baad360cb9e8db3ac upstream.
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Interestingly, the raid5 code can actually prevent double initialization and
    hence can use the following simplified form of callback registration:
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            put_online_cpus();
    
    A hotplug operation that occurs between registering the notifier and calling
    get_online_cpus(), won't disrupt anything, because the code takes care to
    perform the memory allocations only once.
    
    So reorganize the code in raid5 this way to fix the deadlock with callback
    registration.
    
    Cc: linux-raid@vger.kernel.org
    Fixes: 36d1c6476be51101778882897b315bd928c8c7b5
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    [Srivatsa: Fixed the unregister_cpu_notifier() deadlock, added the
    free_scratch_buffer() helper to condense code further and wrote the changelog.]
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit cd6f6b709eebe70d80a3e542d218228bcf189654
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Feb 6 03:42:45 2014 +0530

    md/raid5: Fix CPU hotplug callback registration
    
    commit 789b5e0315284463617e106baad360cb9e8db3ac upstream.
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Interestingly, the raid5 code can actually prevent double initialization and
    hence can use the following simplified form of callback registration:
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            put_online_cpus();
    
    A hotplug operation that occurs between registering the notifier and calling
    get_online_cpus(), won't disrupt anything, because the code takes care to
    perform the memory allocations only once.
    
    So reorganize the code in raid5 this way to fix the deadlock with callback
    registration.
    
    Cc: linux-raid@vger.kernel.org
    Fixes: 36d1c6476be51101778882897b315bd928c8c7b5
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    [Srivatsa: Fixed the unregister_cpu_notifier() deadlock, added the
    free_scratch_buffer() helper to condense code further and wrote the changelog.]
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 3d5f35bdfdef5fd627afe9b4bf9c4f32d17f4593
Author: Juri Lelli <juri.lelli@gmail.com>
Date:   Thu Feb 20 09:19:39 2014 +0100

    sched/deadline: Fix bad accounting of nr_running
    
    Rostedt writes:
    
    My test suite was locking up hard when enabling mmiotracer. This was due
    to the mmiotracer placing all but one CPU offline. I found this out
    when I was able to reproduce the bug with just my stress-cpu-hotplug
    test. This bug baffled me because it would not always trigger, and
    would only trigger on the first run after boot up. The
    stress-cpu-hotplug test would crash hard the first run, or never crash
    at all. But a new reboot may cause it to crash on the first run again.
    
    I spent all week bisecting this, as I couldn't find a consistent
    reproducer. I finally narrowed it down to the sched deadline patches,
    and even more peculiar, to the commit that added the sched
    deadline boot up self test to the latency tracer. Then it dawned on me
    to what the bug was.
    
    All it took was to run a task under sched deadline to screw up the CPU
    hot plugging. This explained why it would lock up only on the first run
    of the stress-cpu-hotplug test. The bug happened when the boot up self
    test of the schedule latency tracer would test a deadline task. The
    deadline task would corrupt something that would cause CPU hotplug to
    fail. If it didn't corrupt it, the stress test would always work
    (there's no other sched deadline tasks that would run to cause
    problems). If it did corrupt on boot up, the first test would lockup
    hard.
    
    I proved this theory by running my deadline test program on another box,
    and then run the stress-cpu-hotplug test, and it would now consistently
    lock up. I could run stress-cpu-hotplug over and over with no problem,
    but once I ran the deadline test, the next run of the
    stress-cpu-hotplug would lock hard.
    
    After adding lots of tracing to the code, I found the cause. The
    function tracer showed that migrate_tasks() was stuck in an infinite
    loop, where rq->nr_running never equaled 1 to break out of it. When I
    added a trace_printk() to see what that number was, it was 335 and
    never decrementing!
    
    Looking at the deadline code I found:
    
    static void __dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags) {
            dequeue_dl_entity(&p->dl);
            dequeue_pushable_dl_task(rq, p);
    }
    
    static void dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags) {
            update_curr_dl(rq);
            __dequeue_task_dl(rq, p, flags);
    
            dec_nr_running(rq);
    }
    
    And this:
    
            if (dl_runtime_exceeded(rq, dl_se)) {
                    __dequeue_task_dl(rq, curr, 0);
                    if (likely(start_dl_timer(dl_se, curr->dl.dl_boosted)))
                            dl_se->dl_throttled = 1;
                    else
                            enqueue_task_dl(rq, curr, ENQUEUE_REPLENISH);
    
                    if (!is_leftmost(curr, &rq->dl))
                            resched_task(curr);
            }
    
    Notice how we call __dequeue_task_dl() and in the else case we
    call enqueue_task_dl()? Also notice that dequeue_task_dl() has
    underscores where enqueue_task_dl() does not. The enqueue_task_dl()
    calls inc_nr_running(rq), but __dequeue_task_dl() does not. This is
    where we get nr_running out of sync.
    
    [snip]
    
    Another point where nr_running can get out of sync is when the dl_timer
    fires:
    
            dl_se->dl_throttled = 0;
            if (p->on_rq) {
                    enqueue_task_dl(rq, p, ENQUEUE_REPLENISH);
                    if (task_has_dl_policy(rq->curr))
                            check_preempt_curr_dl(rq, p, 0);
                    else
                            resched_task(rq->curr);
    
    This patch does two things:
    
     - correctly accounts for throttled tasks (that are now considered
       !running);
    
     - fixes the bug, updating nr_running from {inc,dec}_dl_tasks(),
       since we risk to update it twice in some situations (e.g., a
       task is dequeued while it has exceeded its budget).
    
    Cc: mingo@redhat.com
    Cc: torvalds@linux-foundation.org
    Cc: akpm@linux-foundation.org
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Tested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1392884379-13744-1-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit bd3813d52d567ea6e85e70ce205770bfdafac368
Merge: c1b8ae03c382 789b5e031528
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 14 12:48:16 2014 -0800

    Merge tag 'md/3.14-fixes' of git://neil.brown.name/md
    
    Pull md fixes from Neil Brown:
     "Two bugfixes for md
    
      both tagged for -stable"
    
    * tag 'md/3.14-fixes' of git://neil.brown.name/md:
      md/raid5: Fix CPU hotplug callback registration
      md/raid1: restore ability for check and repair to fix read errors.

commit 789b5e0315284463617e106baad360cb9e8db3ac
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Feb 6 03:42:45 2014 +0530

    md/raid5: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Interestingly, the raid5 code can actually prevent double initialization and
    hence can use the following simplified form of callback registration:
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            put_online_cpus();
    
    A hotplug operation that occurs between registering the notifier and calling
    get_online_cpus(), won't disrupt anything, because the code takes care to
    perform the memory allocations only once.
    
    So reorganize the code in raid5 this way to fix the deadlock with callback
    registration.
    
    Cc: linux-raid@vger.kernel.org
    Cc: stable@vger.kernel.org (v2.6.32+)
    Fixes: 36d1c6476be51101778882897b315bd928c8c7b5
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    [Srivatsa: Fixed the unregister_cpu_notifier() deadlock, added the
    free_scratch_buffer() helper to condense code further and wrote the changelog.]
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: NeilBrown <neilb@suse.de>

commit 5326f5fc83fecf025bf211f429373d7bed0f7582
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Mon Dec 30 17:05:34 2013 +0530

    powerpc: Fix the setup of CPU-to-Node mappings during CPU online
    
    commit d4edc5b6c480a0917e61d93d55531d7efa6230be upstream.
    
    On POWER platforms, the hypervisor can notify the guest kernel about dynamic
    changes in the cpu-numa associativity (VPHN topology update). Hence the
    cpu-to-node mappings that we got from the firmware during boot, may no longer
    be valid after such updates. This is handled using the arch_update_cpu_topology()
    hook in the scheduler, and the sched-domains are rebuilt according to the new
    mappings.
    
    But unfortunately, at the moment, CPU hotplug ignores these updated mappings
    and instead queries the firmware for the cpu-to-numa relationships and uses
    them during CPU online. So the kernel can end up assigning wrong NUMA nodes
    to CPUs during subsequent CPU hotplug online operations (after booting).
    
    Further, a particularly problematic scenario can result from this bug:
    On POWER platforms, the SMT mode can be switched between 1, 2, 4 (and even 8)
    threads per core. The switch to Single-Threaded (ST) mode is performed by
    offlining all except the first CPU thread in each core. Switching back to
    SMT mode involves onlining those other threads back, in each core.
    
    Now consider this scenario:
    
    1. During boot, the kernel gets the cpu-to-node mappings from the firmware
       and assigns the CPUs to NUMA nodes appropriately, during CPU online.
    
    2. Later on, the hypervisor updates the cpu-to-node mappings dynamically and
       communicates this update to the kernel. The kernel in turn updates its
       cpu-to-node associations and rebuilds its sched domains. Everything is
       fine so far.
    
    3. Now, the user switches the machine from SMT to ST mode (say, by running
       ppc64_cpu --smt=1). This involves offlining all except 1 thread in each
       core.
    
    4. The user then tries to switch back from ST to SMT mode (say, by running
       ppc64_cpu --smt=4), and this involves onlining those threads back. Since
       CPU hotplug ignores the new mappings, it queries the firmware and tries to
       associate the newly onlined sibling threads to the old NUMA nodes. This
       results in sibling threads within the same core getting associated with
       different NUMA nodes, which is incorrect.
    
       The scheduler's build-sched-domains code gets thoroughly confused with this
       and enters an infinite loop and causes soft-lockups, as explained in detail
       in commit 3be7db6ab (powerpc: VPHN topology change updates all siblings).
    
    So to fix this, use the numa_cpu_lookup_table to remember the updated
    cpu-to-node mappings, and use them during CPU hotplug online operations.
    Further, we also need to ensure that all threads in a core are assigned to a
    common NUMA node, irrespective of whether all those threads were online during
    the topology update. To achieve this, we take care not to use cpu_sibling_mask()
    since it is not hotplug invariant. Instead, we use cpu_first_sibling_thread()
    and set up the mappings manually using the 'threads_per_core' value for that
    particular platform. This helps us ensure that we don't hit this bug with any
    combination of CPU hotplug and SMT mode switching.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 634bcdd9a104b791eec3f808d6ee4f1e565303e2
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Mon Dec 30 17:05:34 2013 +0530

    powerpc: Fix the setup of CPU-to-Node mappings during CPU online
    
    commit d4edc5b6c480a0917e61d93d55531d7efa6230be upstream.
    
    On POWER platforms, the hypervisor can notify the guest kernel about dynamic
    changes in the cpu-numa associativity (VPHN topology update). Hence the
    cpu-to-node mappings that we got from the firmware during boot, may no longer
    be valid after such updates. This is handled using the arch_update_cpu_topology()
    hook in the scheduler, and the sched-domains are rebuilt according to the new
    mappings.
    
    But unfortunately, at the moment, CPU hotplug ignores these updated mappings
    and instead queries the firmware for the cpu-to-numa relationships and uses
    them during CPU online. So the kernel can end up assigning wrong NUMA nodes
    to CPUs during subsequent CPU hotplug online operations (after booting).
    
    Further, a particularly problematic scenario can result from this bug:
    On POWER platforms, the SMT mode can be switched between 1, 2, 4 (and even 8)
    threads per core. The switch to Single-Threaded (ST) mode is performed by
    offlining all except the first CPU thread in each core. Switching back to
    SMT mode involves onlining those other threads back, in each core.
    
    Now consider this scenario:
    
    1. During boot, the kernel gets the cpu-to-node mappings from the firmware
       and assigns the CPUs to NUMA nodes appropriately, during CPU online.
    
    2. Later on, the hypervisor updates the cpu-to-node mappings dynamically and
       communicates this update to the kernel. The kernel in turn updates its
       cpu-to-node associations and rebuilds its sched domains. Everything is
       fine so far.
    
    3. Now, the user switches the machine from SMT to ST mode (say, by running
       ppc64_cpu --smt=1). This involves offlining all except 1 thread in each
       core.
    
    4. The user then tries to switch back from ST to SMT mode (say, by running
       ppc64_cpu --smt=4), and this involves onlining those threads back. Since
       CPU hotplug ignores the new mappings, it queries the firmware and tries to
       associate the newly onlined sibling threads to the old NUMA nodes. This
       results in sibling threads within the same core getting associated with
       different NUMA nodes, which is incorrect.
    
       The scheduler's build-sched-domains code gets thoroughly confused with this
       and enters an infinite loop and causes soft-lockups, as explained in detail
       in commit 3be7db6ab (powerpc: VPHN topology change updates all siblings).
    
    So to fix this, use the numa_cpu_lookup_table to remember the updated
    cpu-to-node mappings, and use them during CPU hotplug online operations.
    Further, we also need to ensure that all threads in a core are assigned to a
    common NUMA node, irrespective of whether all those threads were online during
    the topology update. To achieve this, we take care not to use cpu_sibling_mask()
    since it is not hotplug invariant. Instead, we use cpu_first_sibling_thread()
    and set up the mappings manually using the 'threads_per_core' value for that
    particular platform. This helps us ensure that we don't hit this bug with any
    combination of CPU hotplug and SMT mode switching.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit df8042baf0473019037c5221c942087afb089df4
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Mon Dec 30 17:05:34 2013 +0530

    powerpc: Fix the setup of CPU-to-Node mappings during CPU online
    
    commit d4edc5b6c480a0917e61d93d55531d7efa6230be upstream.
    
    On POWER platforms, the hypervisor can notify the guest kernel about dynamic
    changes in the cpu-numa associativity (VPHN topology update). Hence the
    cpu-to-node mappings that we got from the firmware during boot, may no longer
    be valid after such updates. This is handled using the arch_update_cpu_topology()
    hook in the scheduler, and the sched-domains are rebuilt according to the new
    mappings.
    
    But unfortunately, at the moment, CPU hotplug ignores these updated mappings
    and instead queries the firmware for the cpu-to-numa relationships and uses
    them during CPU online. So the kernel can end up assigning wrong NUMA nodes
    to CPUs during subsequent CPU hotplug online operations (after booting).
    
    Further, a particularly problematic scenario can result from this bug:
    On POWER platforms, the SMT mode can be switched between 1, 2, 4 (and even 8)
    threads per core. The switch to Single-Threaded (ST) mode is performed by
    offlining all except the first CPU thread in each core. Switching back to
    SMT mode involves onlining those other threads back, in each core.
    
    Now consider this scenario:
    
    1. During boot, the kernel gets the cpu-to-node mappings from the firmware
       and assigns the CPUs to NUMA nodes appropriately, during CPU online.
    
    2. Later on, the hypervisor updates the cpu-to-node mappings dynamically and
       communicates this update to the kernel. The kernel in turn updates its
       cpu-to-node associations and rebuilds its sched domains. Everything is
       fine so far.
    
    3. Now, the user switches the machine from SMT to ST mode (say, by running
       ppc64_cpu --smt=1). This involves offlining all except 1 thread in each
       core.
    
    4. The user then tries to switch back from ST to SMT mode (say, by running
       ppc64_cpu --smt=4), and this involves onlining those threads back. Since
       CPU hotplug ignores the new mappings, it queries the firmware and tries to
       associate the newly onlined sibling threads to the old NUMA nodes. This
       results in sibling threads within the same core getting associated with
       different NUMA nodes, which is incorrect.
    
       The scheduler's build-sched-domains code gets thoroughly confused with this
       and enters an infinite loop and causes soft-lockups, as explained in detail
       in commit 3be7db6ab (powerpc: VPHN topology change updates all siblings).
    
    So to fix this, use the numa_cpu_lookup_table to remember the updated
    cpu-to-node mappings, and use them during CPU hotplug online operations.
    Further, we also need to ensure that all threads in a core are assigned to a
    common NUMA node, irrespective of whether all those threads were online during
    the topology update. To achieve this, we take care not to use cpu_sibling_mask()
    since it is not hotplug invariant. Instead, we use cpu_first_sibling_thread()
    and set up the mappings manually using the 'threads_per_core' value for that
    particular platform. This helps us ensure that we don't hit this bug with any
    combination of CPU hotplug and SMT mode switching.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 9b83d851a2bdd021e2135999e5bce3eb8fef94e6
Merge: 2d08cd0ef89a 9ed82c6866e2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 25 10:49:30 2014 -0800

    Merge tag 'xtensa-next-20140123' of git://github.com/czankel/xtensa-linux
    
    Pull Xtensa patches from Chris Zankel:
     "The major changes are adding support for SMP for Xtensa, fixing and
      cleaning up the ISS (simulator) network driver, and better support for
      device trees"
    
    * tag 'xtensa-next-20140123' of git://github.com/czankel/xtensa-linux: (40 commits)
      xtensa: implement ndelay
      xtensa: clean up udelay
      xtensa: enable HAVE_PERF_EVENTS
      xtensa: remap io area defined in device tree
      xtensa: support default device tree buses
      xtensa: initialize device tree clock sources
      xtensa: xtfpga: fix definitions of platform devices
      xtensa: standardize devicetree cpu compatible strings
      xtensa: avoid duplicate of IO range definitions
      xtensa: fix ATOMCTL register documentation
      xtensa: Enable irqs after cpu is set online
      xtensa: ISS: raise network polling rate to 10 times/sec
      xtensa: remove unused XTENSA_ISS_NETWORK Kconfig parameter
      xtensa: ISS: avoid simple_strtoul usage
      xtensa: Switch to sched_clock_register()
      xtensa: implement CPU hotplug
      xtensa: add SMP support
      xtensa: add MX irqchip
      xtensa: clear timer IRQ unconditionally in its handler
      xtensa: clean up do_interrupt/do_IRQ
      ...

commit 2cc3f16cad1561c6fc551aefff559e53726efc8b
Merge: ad3ab302fd82 c7a730fa4624
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 20 10:27:52 2014 -0800

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull IRQ changes from Ingo Molnar:
     "The only change in this cycle is a CPU hotplug related spurious
      warning fix"
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/irq: Fix kbuild warning in smp_irq_move_cleanup_interrupt()
      x86/irq: Fix do_IRQ() interrupt warning for cpu hotplug retriggered irqs

commit d4edc5b6c480a0917e61d93d55531d7efa6230be
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Mon Dec 30 17:05:34 2013 +0530

    powerpc: Fix the setup of CPU-to-Node mappings during CPU online
    
    On POWER platforms, the hypervisor can notify the guest kernel about dynamic
    changes in the cpu-numa associativity (VPHN topology update). Hence the
    cpu-to-node mappings that we got from the firmware during boot, may no longer
    be valid after such updates. This is handled using the arch_update_cpu_topology()
    hook in the scheduler, and the sched-domains are rebuilt according to the new
    mappings.
    
    But unfortunately, at the moment, CPU hotplug ignores these updated mappings
    and instead queries the firmware for the cpu-to-numa relationships and uses
    them during CPU online. So the kernel can end up assigning wrong NUMA nodes
    to CPUs during subsequent CPU hotplug online operations (after booting).
    
    Further, a particularly problematic scenario can result from this bug:
    On POWER platforms, the SMT mode can be switched between 1, 2, 4 (and even 8)
    threads per core. The switch to Single-Threaded (ST) mode is performed by
    offlining all except the first CPU thread in each core. Switching back to
    SMT mode involves onlining those other threads back, in each core.
    
    Now consider this scenario:
    
    1. During boot, the kernel gets the cpu-to-node mappings from the firmware
       and assigns the CPUs to NUMA nodes appropriately, during CPU online.
    
    2. Later on, the hypervisor updates the cpu-to-node mappings dynamically and
       communicates this update to the kernel. The kernel in turn updates its
       cpu-to-node associations and rebuilds its sched domains. Everything is
       fine so far.
    
    3. Now, the user switches the machine from SMT to ST mode (say, by running
       ppc64_cpu --smt=1). This involves offlining all except 1 thread in each
       core.
    
    4. The user then tries to switch back from ST to SMT mode (say, by running
       ppc64_cpu --smt=4), and this involves onlining those threads back. Since
       CPU hotplug ignores the new mappings, it queries the firmware and tries to
       associate the newly onlined sibling threads to the old NUMA nodes. This
       results in sibling threads within the same core getting associated with
       different NUMA nodes, which is incorrect.
    
       The scheduler's build-sched-domains code gets thoroughly confused with this
       and enters an infinite loop and causes soft-lockups, as explained in detail
       in commit 3be7db6ab (powerpc: VPHN topology change updates all siblings).
    
    So to fix this, use the numa_cpu_lookup_table to remember the updated
    cpu-to-node mappings, and use them during CPU hotplug online operations.
    Further, we also need to ensure that all threads in a core are assigned to a
    common NUMA node, irrespective of whether all those threads were online during
    the topology update. To achieve this, we take care not to use cpu_sibling_mask()
    since it is not hotplug invariant. Instead, we use cpu_first_sibling_thread()
    and set up the mappings manually using the 'threads_per_core' value for that
    particular platform. This helps us ensure that we don't hit this bug with any
    combination of CPU hotplug and SMT mode switching.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 49b424fedaf88d0fa9913082b8c1ccd012a8a972
Author: Max Filippov <jcmvbkbc@gmail.com>
Date:   Thu Oct 17 02:42:28 2013 +0400

    xtensa: implement CPU hotplug
    
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>
    Signed-off-by: Chris Zankel <chris@zankel.net>

commit b981513f806d26b2cc971eb65ced14bede67558b
Author: Jiang Liu <jiang.liu@linux.intel.com>
Date:   Thu Jan 9 16:15:19 2014 +0800

    ACPI / scan: bail out early if failed to parse APIC ID for CPU
    
    Enhance ACPI CPU hotplug driver to print clear error message and
    bail out early if BIOS returns wrong value in ACPI MADT table or
    _MAT method. Otherwise it will add the CPU device even if failed
    to get APIC ID and fails any operations against sysfs interface:
    /sys/devices/system/cpu/cpux/online
    
    Signed-off-by: Jiang Liu <jiang.liu@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 7ca380f60626d90cdd17cc60de5e756422e35b2a
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Fri Dec 20 19:47:26 2013 +0100

    ACPI / cpuidle: fix max idle state handling with hotplug CPU support
    
    acpi_processor_hotplug() calls acpi_processor_setup_cpuidle_cx()
    without calling acpi_processor_setup_cpuidle_states() first so it
    is possible that dev->state_count becomes different from
    drv->state_count (in case of SMP system with unsupported C2/C3
    states + enabled CPU hotplug and num_online_cpus() becoming > 1).
    
    The driver code assumes that cpuidle core will handle such cases
    but currently this is untrue (dev->state_count is used only for
    handling cpuidle state sysfs entries and drv->state_count is used
    for all other cases) and will not be fixed in the future as
    dev->state_count is planned to be removed.
    
    Fix the issue by checking for the max supported idle state in
    C2/C3 state's ->enter handler (acpi_idle_enter_simple() for C2/C3
    and acpi_idle_enter_bm() for C3 + bm_check flag set) and setting
    the C1 state (instead of higher states) when needed.
    
    Also remove no longer needed max idle state checks from
    acpi_processor_setup_cpuidle_[states,cx]().
    
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Len Brown <lenb@kernel.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 9e036d3ef0b9fcb34acce5a89d1f2157f4f7b4ab
Author: Joseph Lo <josephl@nvidia.com>
Date:   Wed Sep 25 17:27:51 2013 +0800

    clk: tegra124: add wait_for_reset and disable_clock for tegra_cpu_car_ops
    
    Hook the functions for CPU hotplug support. After the CPU is hot
    unplugged, the flow controller will handle to clock gate the CPU clock.
    But still need to implement an empty function to avoid warning message.
    
    Cc: Mike Turquette <mturquette@linaro.org>
    Signed-off-by: Joseph Lo <josephl@nvidia.com>

commit 05ad391dbc2bd27b8d868cf9c3ec1b68a2126a16
Merge: 8b5baa460b69 67317c268956
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 11 16:32:21 2013 +0900

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64
    
    Pull ARM64 update from Catalin Marinas:
     "Main features:
       - Ticket-based spinlock implementation and lockless lockref support
       - Big endian support
       - CPU hotplug support, currently for PSCI (Power State Coordination
         Interface) capable firmware
       - Virtual address space extended to 42-bit in the 64K page
         configuration (maximum VA space with 2 levels of page tables)
       - Compat (AArch32) kuser helpers updated to ARMv8 (make use of
         load-acquire/store-release instructions)
       - Code cleanup, defconfig update and minor fixes"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64: (43 commits)
      ARM64: /proc/interrupts: display IPIs of online CPUs only
      arm64: locks: Remove CONFIG_GENERIC_LOCKBREAK
      arm64: KVM: vgic: byteswap GICv2 access on world switch if BE
      arm64: KVM: initialize HYP mode following the kernel endianness
      arm64: compat: Clear the IT state independent of the 32-bit ARM or Thumb-2 mode
      arm64: Use 42-bit address space with 64K pages
      arm64: module: ensure instruction is little-endian before manipulation
      arm64: defconfig: Enable CONFIG_PREEMPT by default
      arm64: fix access to preempt_count from assembly code
      arm64: move enabling of GIC before CPUs are set online
      arm64: use generic RW_DATA_SECTION macro in linker script
      arm64: Slightly improve the warning on CPU0 enable-method
      ARM64: simplify cpu_read_bootcpu_ops using OF/DT helper
      ARM64: DT: define ARM64 specific arch_match_cpu_phys_id
      arm64: allow ioremap_cache() to use existing RAM mappings
      arm64: update 32-bit kuser helpers to ARMv8
      arm64: perf: fix event number mask
      arm64: kconfig: allow CPU_BIG_ENDIAN to be selected
      arm64: Fix the endianness of arch_spinlock_t
      arm64: big-endian: write CPU holding pen address as LE
      ...

commit 0de0d64675259bf21d06b18985318ffb66a5218f
Author: Dave Martin <dave.martin@linaro.org>
Date:   Tue Oct 1 19:58:17 2013 +0100

    ARM: 7848/1: mcpm: Implement cpu_kill() to synchronise on powerdown
    
    CPU hotplug and kexec rely on smp_ops.cpu_kill(), which is supposed
    to wait for the CPU to park or power down, and perform the last
    rites (such as disabling clocks etc., where the platform doesn't do
    this automatically).
    
    kexec in particular is unsafe without performing this
    synchronisation to park secondaries.  Without it, the secondaries
    might not be parked when kexec trashes the kernel.
    
    There is no generic way to do this synchronisation, so a new mcpm
    platform_ops method power_down_finish() is added by this patch.
    
    The new method is mandatory.  A platform which provides no way to
    detect when CPUs are parked is likely broken.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 4dcf03346af269579fdfdadb662d22035c9fceb6
Merge: abcba009e517 bd6a9ddcb9f2
Author: Kevin Hilman <khilman@linaro.org>
Date:   Fri Oct 25 03:46:38 2013 -0700

    Merge tag 'tegra-for-3.13-soc-v2' of git://git.kernel.org/pub/scm/linux/kernel/git/swarren/linux-tegra into next/soc
    
    From Stephen Warren:
    ARM: tegra: core SoC support changes for 3.13
    
    This branch includes:
    * SoC fuse values are used as device randomness at boot.
    * Initial support for the Tegra124 SoC is added. When coupled with an
      appropriate clock driver, which should also be merged for 3.13, we are
      able to boot to user-space using an initrd.
    * The powergate code gains support for Tegra114.
    
    This branch is based on previous pull request tegra-for-3.13-cleanup.
    
    * tag 'tegra-for-3.13-soc-v2' of git://git.kernel.org/pub/scm/linux/kernel/git/swarren/linux-tegra:
      ARM: tegra: Add Tegra114 powergate support
      ARM: tegra: Constify list of CPU domains
      ARM: tegra: Remove duplicate powergate defines
      ARM: tegra: add LP1 support code for Tegra124
      ARM: tegra: re-calculate the LP1 data for Tegra30/114
      ARM: tegra: enable CPU idle for Tegra124
      ARM: tegra: make tegra_resume can work with current and later chips
      ARM: tegra: CPU hotplug support for Tegra124
      ARM: tegra: add PMC compatible value for Tegra124
      ARM: tegra: add Tegra124 SoC support
      ARM: tegra: add fuses as device randomness
      ARM: tegra: fix ARCH_TEGRA_114_SOC select sort order
      ARM: tegra: make tegra_init_fuse() __init
      ARM: tegra: remove much of iomap.h
      ARM: tegra: move resume vector define to irammap.h
      ARM: tegra: delete gpio-names.h
      ARM: tegra: delete stale header content
      ARM: tegra: remove common.c
      ARM: tegra: split tegra_pmc_init() in two
    
    Signed-off-by: Kevin Hilman <khilman@linaro.org>

commit 831ccf79b46e02f31cfd16c66df9d5600f635155
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 24 20:30:19 2013 +0100

    arm64: add PSCI CPU_OFF-based hotplug support
    
    This patch adds support for using PSCI CPU_OFF calls for CPU hotplug.
    With this code it is possible to hot unplug CPUs with "psci" as their
    boot-method, as long as there's an appropriate cpu_off function id
    specified in the psci node.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

commit 9997e62682e0fe68566c88f70ed320ded4e16529
Author: Joseph Lo <josephl@nvidia.com>
Date:   Fri Oct 11 17:57:30 2013 +0800

    ARM: tegra: CPU hotplug support for Tegra124
    
    The procedure of CPU hotplug for Tegra124 is same with Tegra114. We
    re-use the same function with it.
    
    Signed-off-by: Joseph Lo <josephl@nvidia.com>
    Signed-off-by: Stephen Warren <swarren@nvidia.com>

commit 64270d82d4bf7fb8e5347c41ea7d0477aa551391
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Fri Sep 27 16:54:42 2013 +0100

    ARM: vexpress: tc2: fix hotplug/idle/kexec race on cluster power down
    
    On the TC2 testchip, when all CPUs in a cluster enter standbywfi
    and commit a power down request, the power controller will wait
    for standbywfil2 coming from L2 cache controller to shut the
    cluster down.
    By the time all CPUs in a cluster commit a power down request
    and enter wfi, the power controller cannot backtrack, or put it
    another way, a CPU must not be allowed to complete execution
    independently of the power controller, the only way for it to
    resume properly must be upon wake-up IRQ pending and subsequent
    reset triggered from the power controller.
    
    Current MCPM back-end for TC2 disables the GIC CPU IF only when
    power down is committed through the tc2_pm_suspend() method, that
    makes sense since a suspended CPU is still online and can receive
    interrupts whereas a hotplugged CPU, since it is offline,
    migrated all IRQs and shutdown the per-CPU peripherals, hence
    their PPIs.
    
    The flaw with this reasoning is the following. If all CPUs in
    a clusters are entering a power down state either through CPU
    idle or CPU hotplug, when the last man successfully completes
    the MCPM power down sequence (and executes wfi), power controller
    waits for L2 wfi signal to quiesce the cluster and shut it down.
    If, when all CPUs are sitting in wfi, an online CPU hotplugs back
    in one of the CPUs in the cluster being shutdown, that CPU
    receives an IPI that causes wfi to complete (since tc2_pm_down()
    method does not disable the GIC CPU IF in that case - CPU being
    hotplugged out, not idle) and the power controller will never see
    the stanbywfil2 signal coming from L2 that is required for
    shutdown to happen and the system deadlocks.
    
    Further to this issue, kexec hotplugs secondary CPUs out during
    kernel reload/restart.
    Because kexec may (deliberately) trash the old kernel text, it is
    not OK for CPUs to follow the MCPM soft reboot path, since
    instructions after the WFI may have been replaced by kexec.
    
    If tc2_pm_down() does not disable the GIC cpu interface, there is a
    race between CPU powerdown in the old kernel and the IPI from the
    new kernel that triggers secondary boot, particularly if the
    powerdown is slow (due to L2 cache cleaning for example).  If the
    new kernel wins the race, the affected CPU(s) will not really be
    reset and may execute garbage after the WFI.
    
    The only solution to this problem consists in disabling the GIC
    CPU IF on a CPU committed to power down regardless of the power
    down entry method (CPU hotplug or CPU idle). This way, CPU wake-up
    is under power controller control, which prevents unexpected wfi
    exit caused by a pending IRQ.
    
    This patch moves the GIC CPU IF disable call in the TC2 MCPM
    implementation from the tc2_pm_suspend() method to the
    tc2_pm_down() method to fix the mentioned race condition(s).
    
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com> (for kexec)
    Signed-off-by: Sudeep KarkadaNagesha <sudeep.karkadanagesha@arm.com>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Olof Johansson <olof@lixom.net>

commit 4fa92fb25ae5a2d79d872ab54df511c831b1f363
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Sep 10 12:57:17 2013 +0200

    KVM: cleanup (physical) CPU hotplug
    
    Remove the useless argument, and do not do anything if there are no
    VMs running at the time of the hotplug.
    
    Cc: kvm@vger.kernel.org
    Cc: gleb@redhat.com
    Cc: jan.kiszka@siemens.com
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

commit 1cad5e9a3978d182aa9b0e909fb0379da5ba45af
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Thu Aug 29 18:22:08 2013 -0600

    hotplug / x86: Disable ARCH_CPU_PROBE_RELEASE on x86
    
    Commit d7c53c9e enabled ARCH_CPU_PROBE_RELEASE on x86 in order to
    serialize CPU online/offline operations.  Although it is the config
    option to enable CPU hotplug test interfaces, probe & release, it is
    also the option to enable cpu_hotplug_driver_lock() as well.  Therefore,
    this option had to be enabled on x86 with dummy arch_cpu_probe() and
    arch_cpu_release().
    
    Since then, lock_device_hotplug() was introduced to serialize CPU
    online/offline & hotplug operations.  Therefore, this config option
    is no longer required for the serialization.  This patch disables
    this config option on x86 and revert the changes made by commit
    d7c53c9e.
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Acked-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 02b9735c12892e04d3e101b06e4c6d64a814f566
Merge: 75acebf2423a f1728fd15991
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 12 11:22:45 2013 -0700

    Merge tag 'pm+acpi-fixes-3.12-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull ACPI and power management fixes from Rafael Wysocki:
     "All of these commits are fixes that have emerged recently and some of
      them fix bugs introduced during this merge window.
    
      Specifics:
    
       1) ACPI-based PCI hotplug (ACPIPHP) fixes related to spurious events
    
          After the recent ACPIPHP changes we've seen some interesting
          breakage on a system that triggers device check notifications
          during boot for non-existing devices.  Although those
          notifications are really spurious, we should be able to deal with
          them nevertheless and that shouldn't introduce too much overhead.
          Four commits to make that work properly.
    
       2) Memory hotplug and hibernation mutual exclusion rework
    
          This was maent to be a cleanup, but it happens to fix a classical
          ABBA deadlock between system suspend/hibernation and ACPI memory
          hotplug which is possible if they are started roughly at the same
          time.  Three commits rework memory hotplug so that it doesn't
          acquire pm_mutex and make hibernation use device_hotplug_lock
          which prevents it from racing with memory hotplug.
    
       3) ACPI Intel LPSS (Low-Power Subsystem) driver crash fix
    
          The ACPI LPSS driver crashes during boot on Apple Macbook Air with
          Haswell that has slightly unusual BIOS configuration in which one
          of the LPSS device's _CRS method doesn't return all of the
          information expected by the driver.  Fix from Mika Westerberg, for
          stable.
    
       4) ACPICA fix related to Store->ArgX operation
    
          AML interpreter fix for obscure breakage that causes AML to be
          executed incorrectly on some machines (observed in practice).
          From Bob Moore.
    
       5) ACPI core fix for PCI ACPI device objects lookup
    
          There still are cases in which there is more than one ACPI device
          object matching a given PCI device and we don't choose the one
          that the BIOS expects us to choose, so this makes the lookup take
          more criteria into account in those cases.
    
       6) Fix to prevent cpuidle from crashing in some rare cases
    
          If the result of cpuidle_get_driver() is NULL, which can happen on
          some systems, cpuidle_driver_ref() will crash trying to use that
          pointer and the Daniel Fu's fix prevents that from happening.
    
       7) cpufreq fixes related to CPU hotplug
    
          Stephen Boyd reported a number of concurrency problems with
          cpufreq related to CPU hotplug which are addressed by a series of
          fixes from Srivatsa S Bhat and Viresh Kumar.
    
       8) cpufreq fix for time conversion in time_in_state attribute
    
          Time conversion carried out by cpufreq when user space attempts to
          read /sys/devices/system/cpu/cpu*/cpufreq/stats/time_in_state
          won't work correcty if cputime_t doesn't map directly to jiffies.
          Fix from Andreas Schwab.
    
       9) Revert of a troublesome cpufreq commit
    
          Commit 7c30ed5 (cpufreq: make sure frequency transitions are
          serialized) was intended to address some known concurrency
          problems in cpufreq related to the ordering of transitions, but
          unfortunately it introduced several problems of its own, so I
          decided to revert it now and address the original problems later
          in a more robust way.
    
      10) Intel Haswell CPU models for intel_pstate from Nell Hardcastle.
    
      11) cpufreq fixes related to system suspend/resume
    
          The recent cpufreq changes that made it preserve CPU sysfs
          attributes over suspend/resume cycles introduced a possible NULL
          pointer dereference that caused it to crash during the second
          attempt to suspend.  Three commits from Srivatsa S Bhat fix that
          problem and a couple of related issues.
    
      12) cpufreq locking fix
    
          cpufreq_policy_restore() should acquire the lock for reading, but
          it acquires it for writing.  Fix from Lan Tianyu"
    
    * tag 'pm+acpi-fixes-3.12-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (25 commits)
      cpufreq: Acquire the lock in cpufreq_policy_restore() for reading
      cpufreq: Prevent problems in update_policy_cpu() if last_cpu == new_cpu
      cpufreq: Restructure if/else block to avoid unintended behavior
      cpufreq: Fix crash in cpufreq-stats during suspend/resume
      intel_pstate: Add Haswell CPU models
      Revert "cpufreq: make sure frequency transitions are serialized"
      cpufreq: Use signed type for 'ret' variable, to store negative error values
      cpufreq: Remove temporary fix for race between CPU hotplug and sysfs-writes
      cpufreq: Synchronize the cpufreq store_*() routines with CPU hotplug
      cpufreq: Invoke __cpufreq_remove_dev_finish() after releasing cpu_hotplug.lock
      cpufreq: Split __cpufreq_remove_dev() into two parts
      cpufreq: Fix wrong time unit conversion
      cpufreq: serialize calls to __cpufreq_governor()
      cpufreq: don't allow governor limits to be changed when it is disabled
      ACPI / bind: Prefer device objects with _STA to those without it
      ACPI / hotplug / PCI: Avoid parent bus rescans on spurious device checks
      ACPI / hotplug / PCI: Use _OST to notify firmware about notify status
      ACPI / hotplug / PCI: Avoid doing too much for spurious notifies
      ACPICA: Fix for a Store->ArgX when ArgX contains a reference to a field.
      ACPI / hotplug / PCI: Don't trim devices before scanning the namespace
      ...

commit 0f1cfe9d0d06fe44c2b310401d2db101968e8c58
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Wed Sep 11 14:21:50 2013 -0700

    mm/hotplug: remove stop_machine() from try_offline_node()
    
    lock_device_hotplug() serializes hotplug & online/offline operations.  The
    lock is held in common sysfs online/offline interfaces and ACPI hotplug
    code paths.
    
    And here are the code paths:
    
    - CPU & Mem online/offline via sysfs online
            store_online()->lock_device_hotplug()
    
    - Mem online via sysfs state:
            store_mem_state()->lock_device_hotplug()
    
    - ACPI CPU & Mem hot-add:
            acpi_scan_bus_device_check()->lock_device_hotplug()
    
    - ACPI CPU & Mem hot-delete:
            acpi_scan_hot_remove()->lock_device_hotplug()
    
    try_offline_node() off-lines a node if all memory sections and cpus are
    removed on the node.  It is called from acpi_processor_remove() and
    acpi_memory_remove_memory()->remove_memory() paths, both of which are in
    the ACPI hotplug code.
    
    try_offline_node() calls stop_machine() to stop all cpus while checking
    all cpu status with the assumption that the caller is not protected from
    CPU hotplug or CPU online/offline operations.  However, the caller is
    always serialized with lock_device_hotplug().  Also, the code needs to be
    properly serialized with a lock, not by stopping all cpus at a random
    place with stop_machine().
    
    This patch removes the use of stop_machine() in try_offline_node() and
    adds comments to try_offline_node() and remove_memory() that
    lock_device_hotplug() is required.
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 0df03a30c333d67ee9b4c37f32d423624f48fe05
Merge: 0a733e6effb4 6cdcdb793791
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Wed Sep 11 15:23:15 2013 +0200

    Merge branch 'pm-cpufreq'
    
    * pm-cpufreq:
      intel_pstate: Add Haswell CPU models
      Revert "cpufreq: make sure frequency transitions are serialized"
      cpufreq: Use signed type for 'ret' variable, to store negative error values
      cpufreq: Remove temporary fix for race between CPU hotplug and sysfs-writes
      cpufreq: Synchronize the cpufreq store_*() routines with CPU hotplug
      cpufreq: Invoke __cpufreq_remove_dev_finish() after releasing cpu_hotplug.lock
      cpufreq: Split __cpufreq_remove_dev() into two parts
      cpufreq: Fix wrong time unit conversion
      cpufreq: serialize calls to __cpufreq_governor()
      cpufreq: don't allow governor limits to be changed when it is disabled

commit 56d07db274b7b15ca38b60ea4a762d40de093000
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Sat Sep 7 01:23:55 2013 +0530

    cpufreq: Remove temporary fix for race between CPU hotplug and sysfs-writes
    
    Commit "cpufreq: serialize calls to __cpufreq_governor()" had been a temporary
    and partial solution to the race condition between writing to a cpufreq sysfs
    file and taking a CPU offline. Now that we have a proper and complete solution
    to that problem, remove the temporary fix.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 4f750c930822b92df74327a4d1364eff87701360
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Sat Sep 7 01:23:43 2013 +0530

    cpufreq: Synchronize the cpufreq store_*() routines with CPU hotplug
    
    The functions that are used to write to cpufreq sysfs files (such as
    store_scaling_max_freq()) are not hotplug safe. They can race with CPU
    hotplug tasks and lead to problems such as trying to acquire an already
    destroyed timer-mutex etc.
    
    Eg:
    
        __cpufreq_remove_dev()
         __cpufreq_governor(policy, CPUFREQ_GOV_STOP);
           policy->governor->governor(policy, CPUFREQ_GOV_STOP);
            cpufreq_governor_dbs()
             case CPUFREQ_GOV_STOP:
              mutex_destroy(&cpu_cdbs->timer_mutex)
              cpu_cdbs->cur_policy = NULL;
          <PREEMPT>
        store()
         __cpufreq_set_policy()
          __cpufreq_governor(policy, CPUFREQ_GOV_LIMITS);
            policy->governor->governor(policy, CPUFREQ_GOV_LIMITS);
             case CPUFREQ_GOV_LIMITS:
              mutex_lock(&cpu_cdbs->timer_mutex); <-- Warning (destroyed mutex)
               if (policy->max < cpu_cdbs->cur_policy->cur) <- cur_policy == NULL
    
    So use get_online_cpus()/put_online_cpus() in the store_*() functions, to
    synchronize with CPU hotplug. However, there is an additional point to note
    here: some parts of the CPU teardown in the cpufreq subsystem are done in
    the CPU_POST_DEAD stage, with cpu_hotplug.lock *released*. So, using the
    get/put_online_cpus() functions alone is insufficient; we should also ensure
    that we don't race with those latter steps in the hotplug sequence. We can
    easily achieve this by checking if the CPU is online before proceeding with
    the store, since the CPU would have been marked offline by the time the
    CPU_POST_DEAD notifiers are executed.
    
    Reported-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 1aee40ac9c86759c05f2ceb4523642b22fc8ea36
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Sat Sep 7 01:23:27 2013 +0530

    cpufreq: Invoke __cpufreq_remove_dev_finish() after releasing cpu_hotplug.lock
    
    __cpufreq_remove_dev_finish() handles the kobject cleanup for a CPU going
    offline. But because we destroy the kobject towards the end of the CPU offline
    phase, there are certain race windows where a task can try to write to a
    cpufreq sysfs file (eg: using store_scaling_max_freq()) while we are taking
    that CPU offline, and this can bump up the kobject refcount, which in turn might
    hinder the CPU offline task from running to completion. (It can also cause
    other more serious problems such as trying to acquire a destroyed timer-mutex
    etc., depending on the exact stage of the cleanup at which the task managed to
    take a new refcount).
    
    To fix the race window, we will need to synchronize those store_*() call-sites
    with CPU hotplug, using get_online_cpus()/put_online_cpus(). However, that
    in turn can cause a total deadlock because it can end up waiting for the
    CPU offline task to complete, with incremented refcount!
    
    Write to sysfs                            CPU offline task
    --------------                            ----------------
    kobj_refcnt++
    
                                              Acquire cpu_hotplug.lock
    
    get_online_cpus();
    
                                              Wait for kobj_refcnt to drop to zero
    
                         **DEADLOCK**
    
    A simple way to avoid this problem is to perform the kobject cleanup in the
    CPU offline path, with the cpu_hotplug.lock *released*. That is, we can
    perform the wait-for-kobj-refcnt-to-drop as well as the subsequent cleanup
    in the CPU_POST_DEAD stage of CPU offline, which is run with cpu_hotplug.lock
    released. Doing this helps us avoid deadlocks due to holding kobject refcounts
    and waiting on each other on the cpu_hotplug.lock.
    
    (Note: We can't move all of the cpufreq CPU offline steps to the
    CPU_POST_DEAD stage, because certain things such as stopping the governors
    have to be done before the outgoing CPU is marked offline. So retain those
    parts in the CPU_DOWN_PREPARE stage itself).
    
    Reported-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 2620bf06f168527e8d5159d6c21ea80e60b663fd
Merge: 359d16ca1bd6 2a2822475d0e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 16 16:52:29 2013 -0700

    Merge branch 'fixes' of git://git.linaro.org/people/rmk/linux-arm
    
    Pull ARM fixes from Russell King:
     "The usual collection of random fixes.  Also some further fixes to the
      last set of security fixes, and some more from Will (which you may
      already have in a slightly different form)"
    
    * 'fixes' of git://git.linaro.org/people/rmk/linux-arm:
      ARM: 7807/1: kexec: validate CPU hotplug support
      ARM: 7812/1: rwlocks: retry trylock operation if strex fails on free lock
      ARM: 7811/1: locks: use early clobber in arch_spin_trylock
      ARM: 7810/1: perf: Fix array out of bounds access in armpmu_map_hw_event()
      ARM: 7809/1: perf: fix event validation for software group leaders
      ARM: Fix FIQ code on VIVT CPUs
      ARM: Fix !kuser helpers case
      ARM: Fix the world famous typo with is_gate_vma()

commit e0bb3964c95b6068bd68ee332b20fc24a76ad2aa
Merge: 3b2f64d00c46 e607b0f985f5
Author: Olof Johansson <olof@lixom.net>
Date:   Tue Aug 13 22:07:52 2013 -0700

    Merge tag 'tc2-pm' of git://git.linaro.org/people/pawelmoll/linux into next/soc
    
    From Pawel Moll and Nicolas Pitre:
    - Fixes to the existing Vexpress DCSCB backend.
    
    - Lorenzo's minimal SPC driver required by the TC2 MCPM backend.
    
    - The MCPM backend enabling SMP secondary boot and CPU hotplug
      on the VExpress TC2 big.LITTLE platform.
    
    - MCPM suspend method to the TC2 backend allowing basic CPU
      idle/suspend.  The cpuidle driver that hooks into this will be
      submitted separately.
    
    * tag 'tc2-pm' of git://git.linaro.org/people/pawelmoll/linux:
      ARM: vexpress/TC2: implement PM suspend method
      ARM: vexpress/TC2: basic PM support
      ARM: vexpress: Add SCC to V2P-CA15_A7's device tree
      ARM: vexpress/TC2: add Serial Power Controller (SPC) support
      ARM: vexpress/dcscb: fix cache disabling sequences
    
    Signed-off-by: Olof Johansson <olof@lixom.net>

commit 2103f6cba61a8b8bea3fc1b63661d830a2125e76
Author: Stephen Warren <swarren@nvidia.com>
Date:   Fri Aug 2 20:52:49 2013 +0100

    ARM: 7807/1: kexec: validate CPU hotplug support
    
    Architectures should fully validate whether kexec is possible as part of
    machine_kexec_prepare(), so that user-space's kexec_load() operation can
    report any problems. Performing validation in machine_kexec() itself is
    too late, since it is not allowed to return.
    
    Prior to this patch, ARM's machine_kexec() was testing after-the-fact
    whether machine_kexec_prepare() was able to disable all but one CPU.
    Instead, modify machine_kexec_prepare() to validate all conditions
    necessary for machine_kexec_prepare()'s to succeed. BUG if the validation
    succeeded, yet disabling the CPUs didn't actually work.
    
    Signed-off-by: Stephen Warren <swarren@nvidia.com>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit b9d10be7a8e88fdcb12540387c219cdde87b0795
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Mon Aug 12 09:45:53 2013 -0600

    ACPI / processor: Acquire writer lock to update CPU maps
    
    CPU system maps are protected with reader/writer locks.  The reader
    lock, get_online_cpus(), assures that the maps are not updated while
    holding the lock.  The writer lock, cpu_hotplug_begin(), is used to
    udpate the cpu maps along with cpu_maps_update_begin().
    
    However, the ACPI processor handler updates the cpu maps without
    holding the the writer lock.
    
    acpi_map_lsapic() is called from acpi_processor_hotadd_init() to
    update cpu_possible_mask and cpu_present_mask.  acpi_unmap_lsapic()
    is called from acpi_processor_remove() to update cpu_possible_mask.
    Currently, they are either unprotected or protected with the reader
    lock, which is not correct.
    
    For example, the get_online_cpus() below is supposed to assure that
    cpu_possible_mask is not changed while the code is iterating with
    for_each_possible_cpu().
    
            get_online_cpus();
            for_each_possible_cpu(cpu) {
                    :
            }
            put_online_cpus();
    
    However, this lock has no protection with CPU hotplug since the ACPI
    processor handler does not use the writer lock when it updates
    cpu_possible_mask.  The reader lock does not serialize within the
    readers.
    
    This patch protects them with the writer lock with cpu_hotplug_begin()
    along with cpu_maps_update_begin(), which must be held before calling
    cpu_hotplug_begin().  It also protects arch_register_cpu() /
    arch_unregister_cpu(), which creates / deletes a sysfs cpu device
    interface.  For this purpose it changes cpu_hotplug_begin() and
    cpu_hotplug_done() to global and exports them in cpu.h.
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 5302c3fb2e62f4ca5e43e060491ba299f58c5231
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Jul 30 04:25:25 2013 +0530

    cpufreq: Perform light-weight init/teardown during suspend/resume
    
    Now that we have the infrastructure to perform a light-weight init/tear-down,
    use that in the cpufreq CPU hotplug notifier when invoked from the
    suspend/resume path.
    
    This also ensures that the file permissions of the cpufreq sysfs files are
    preserved across suspend/resume, something which commit a66b2e (cpufreq:
    Preserve sysfs files across suspend/resume) originally intended to do, but
    had to be reverted due to other problems.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 8414809c6a1e8479e331e09254adb58b33a36d25
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Jul 30 04:25:10 2013 +0530

    cpufreq: Preserve policy structure across suspend/resume
    
    To perform light-weight cpu-init and teardown in the cpufreq subsystem
    during suspend/resume, we need to separate out the 2 main functionalities
    of the cpufreq CPU hotplug callbacks, as outlined below:
    
    1. Init/tear-down of core cpufreq and CPU-specific components, which are
       critical to the correct functioning of the cpufreq subsystem.
    
    2. Init/tear-down of cpufreq sysfs files during suspend/resume.
    
    The first part requires accurate updates to the policy structure such as
    its ->cpus and ->related_cpus masks, whereas the second part requires that
    the policy->kobj structure is not released or re-initialized during
    suspend/resume.
    
    To handle both these requirements, we need to allow updates to the policy
    structure throughout suspend/resume, but prevent the structure from getting
    freed up. Also, we must have a mechanism by which the cpu-up callbacks can
    restore the policy structure, without allocating things afresh. (That also
    helps avoid memory leaks).
    
    To achieve this, we use 2 schemes:
    a. Use a fallback per-cpu storage area for preserving the policy structures
       during suspend, so that they can be restored during resume appropriately.
    
    b. Use the 'frozen' flag to determine when to free or allocate the policy
       structure vs when to restore the policy from the saved fallback storage.
       Thus we can successfully preserve the structure across suspend/resume.
    
    Effectively, this helps us complete the separation of the 'light-weight'
    and the 'full' init/tear-down sequences in the cpufreq subsystem, so that
    this can be made use of in the suspend/resume scenario.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 23d328994b548d6822b88fe7e1903652afc354e0
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Jul 30 04:23:56 2013 +0530

    cpufreq: Fix misplaced call to cpufreq_update_policy()
    
    The call to cpufreq_update_policy() is placed in the CPU hotplug callback
    of cpufreq_stats, which has a higher priority than the CPU hotplug callback
    of cpufreq-core. As a result, during CPU_ONLINE/CPU_ONLINE_FROZEN, we end up
    calling cpufreq_update_policy() *before* calling cpufreq_add_dev() !
    And for uninitialized CPUs, it just returns silently, not doing anything.
    
    To add to that, cpufreq_stats is not even the right place to call
    cpufreq_update_policy() to begin with. The cpufreq core ought to handle
    this in its own callback, from an elegance/relevance perspective.
    
    So move the invocation of cpufreq_update_policy() to cpufreq_cpu_callback,
    and place it *after* cpufreq_add_dev().
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 11b277eabe7005f5c6f2c200b1e26a237badb114
Author: Nicolas Pitre <nico@linaro.org>
Date:   Tue Aug 6 19:10:08 2013 +0100

    ARM: vexpress/TC2: basic PM support
    
    This is the MCPM backend for the Virtual Express A15x2 A7x3 CoreTile
    aka TC2.  This provides cluster management for SMP secondary boot and
    CPU hotplug.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Pawel Moll <pawel.moll@arm.com>
    Reviewed-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    [PM: made it drive SCC registers directly and provide base for SPC]
    Signed-off-by: Pawel Moll <pawel.moll@arm.com>

commit 272614351423ce8c37ff730efc130e5b73fe64f5
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Mon Nov 26 22:48:55 2012 -0500

    ARM: bL_switcher: filter CPU hotplug requests when the switcher is active
    
    Trying to support both the switcher and CPU hotplug at the same time
    is tricky due to ambiguous semantics.  So let's at least prevent users
    from messing around with those logical CPUs the switcher has removed
    and those which were not active when the switcher was activated.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>

commit f21407179ccd0dec35f4580052c26ea923c28ac9
Author: Julien Grall <julien.grall@linaro.org>
Date:   Mon Jul 22 22:40:58 2013 +0100

    xen/arm64: Don't compile cpu hotplug
    
    On ARM64, when CONFIG_XEN=y, the compilation will fail because CPU hotplug is
    not yet supported with XEN. For now, disable it.
    
    Signed-off-by: Julien Grall <julien.grall@linaro.org>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>

commit 916f4dbc2a827677212a3bf3ffcc22745fa6e0b1
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Jul 16 22:46:48 2013 +0200

    cpufreq: Revert commit 2f7021a8 to fix CPU hotplug regression
    
    commit e8d05276f236ee6435e78411f62be9714e0b9377 upstream.
    
    commit 2f7021a8 "cpufreq: protect 'policy->cpus' from offlining
    during __gov_queue_work()" caused a regression in CPU hotplug,
    because it lead to a deadlock between cpufreq governor worker thread
    and the CPU hotplug writer task.
    
    Lockdep splat corresponding to this deadlock is shown below:
    
    [   60.277396] ======================================================
    [   60.277400] [ INFO: possible circular locking dependency detected ]
    [   60.277407] 3.10.0-rc7-dbg-01385-g241fd04-dirty #1744 Not tainted
    [   60.277411] -------------------------------------------------------
    [   60.277417] bash/2225 is trying to acquire lock:
    [   60.277422]  ((&(&j_cdbs->work)->work)){+.+...}, at: [<ffffffff810621b5>] flush_work+0x5/0x280
    [   60.277444] but task is already holding lock:
    [   60.277449]  (cpu_hotplug.lock){+.+.+.}, at: [<ffffffff81042d8b>] cpu_hotplug_begin+0x2b/0x60
    [   60.277465] which lock already depends on the new lock.
    
    [   60.277472] the existing dependency chain (in reverse order) is:
    [   60.277477] -> #2 (cpu_hotplug.lock){+.+.+.}:
    [   60.277490]        [<ffffffff810ac6d4>] lock_acquire+0xa4/0x200
    [   60.277503]        [<ffffffff815b6157>] mutex_lock_nested+0x67/0x410
    [   60.277514]        [<ffffffff81042cbc>] get_online_cpus+0x3c/0x60
    [   60.277522]        [<ffffffff814b842a>] gov_queue_work+0x2a/0xb0
    [   60.277532]        [<ffffffff814b7891>] cs_dbs_timer+0xc1/0xe0
    [   60.277543]        [<ffffffff8106302d>] process_one_work+0x1cd/0x6a0
    [   60.277552]        [<ffffffff81063d31>] worker_thread+0x121/0x3a0
    [   60.277560]        [<ffffffff8106ae2b>] kthread+0xdb/0xe0
    [   60.277569]        [<ffffffff815bb96c>] ret_from_fork+0x7c/0xb0
    [   60.277580] -> #1 (&j_cdbs->timer_mutex){+.+...}:
    [   60.277592]        [<ffffffff810ac6d4>] lock_acquire+0xa4/0x200
    [   60.277600]        [<ffffffff815b6157>] mutex_lock_nested+0x67/0x410
    [   60.277608]        [<ffffffff814b785d>] cs_dbs_timer+0x8d/0xe0
    [   60.277616]        [<ffffffff8106302d>] process_one_work+0x1cd/0x6a0
    [   60.277624]        [<ffffffff81063d31>] worker_thread+0x121/0x3a0
    [   60.277633]        [<ffffffff8106ae2b>] kthread+0xdb/0xe0
    [   60.277640]        [<ffffffff815bb96c>] ret_from_fork+0x7c/0xb0
    [   60.277649] -> #0 ((&(&j_cdbs->work)->work)){+.+...}:
    [   60.277661]        [<ffffffff810ab826>] __lock_acquire+0x1766/0x1d30
    [   60.277669]        [<ffffffff810ac6d4>] lock_acquire+0xa4/0x200
    [   60.277677]        [<ffffffff810621ed>] flush_work+0x3d/0x280
    [   60.277685]        [<ffffffff81062d8a>] __cancel_work_timer+0x8a/0x120
    [   60.277693]        [<ffffffff81062e53>] cancel_delayed_work_sync+0x13/0x20
    [   60.277701]        [<ffffffff814b89d9>] cpufreq_governor_dbs+0x529/0x6f0
    [   60.277709]        [<ffffffff814b76a7>] cs_cpufreq_governor_dbs+0x17/0x20
    [   60.277719]        [<ffffffff814b5df8>] __cpufreq_governor+0x48/0x100
    [   60.277728]        [<ffffffff814b6b80>] __cpufreq_remove_dev.isra.14+0x80/0x3c0
    [   60.277737]        [<ffffffff815adc0d>] cpufreq_cpu_callback+0x38/0x4c
    [   60.277747]        [<ffffffff81071a4d>] notifier_call_chain+0x5d/0x110
    [   60.277759]        [<ffffffff81071b0e>] __raw_notifier_call_chain+0xe/0x10
    [   60.277768]        [<ffffffff815a0a68>] _cpu_down+0x88/0x330
    [   60.277779]        [<ffffffff815a0d46>] cpu_down+0x36/0x50
    [   60.277788]        [<ffffffff815a2748>] store_online+0x98/0xd0
    [   60.277796]        [<ffffffff81452a28>] dev_attr_store+0x18/0x30
    [   60.277806]        [<ffffffff811d9edb>] sysfs_write_file+0xdb/0x150
    [   60.277818]        [<ffffffff8116806d>] vfs_write+0xbd/0x1f0
    [   60.277826]        [<ffffffff811686fc>] SyS_write+0x4c/0xa0
    [   60.277834]        [<ffffffff815bbbbe>] tracesys+0xd0/0xd5
    [   60.277842] other info that might help us debug this:
    
    [   60.277848] Chain exists of:
      (&(&j_cdbs->work)->work) --> &j_cdbs->timer_mutex --> cpu_hotplug.lock
    
    [   60.277864]  Possible unsafe locking scenario:
    
    [   60.277869]        CPU0                    CPU1
    [   60.277873]        ----                    ----
    [   60.277877]   lock(cpu_hotplug.lock);
    [   60.277885]                                lock(&j_cdbs->timer_mutex);
    [   60.277892]                                lock(cpu_hotplug.lock);
    [   60.277900]   lock((&(&j_cdbs->work)->work));
    [   60.277907]  *** DEADLOCK ***
    
    [   60.277915] 6 locks held by bash/2225:
    [   60.277919]  #0:  (sb_writers#6){.+.+.+}, at: [<ffffffff81168173>] vfs_write+0x1c3/0x1f0
    [   60.277937]  #1:  (&buffer->mutex){+.+.+.}, at: [<ffffffff811d9e3c>] sysfs_write_file+0x3c/0x150
    [   60.277954]  #2:  (s_active#61){.+.+.+}, at: [<ffffffff811d9ec3>] sysfs_write_file+0xc3/0x150
    [   60.277972]  #3:  (x86_cpu_hotplug_driver_mutex){+.+...}, at: [<ffffffff81024cf7>] cpu_hotplug_driver_lock+0x17/0x20
    [   60.277990]  #4:  (cpu_add_remove_lock){+.+.+.}, at: [<ffffffff815a0d32>] cpu_down+0x22/0x50
    [   60.278007]  #5:  (cpu_hotplug.lock){+.+.+.}, at: [<ffffffff81042d8b>] cpu_hotplug_begin+0x2b/0x60
    [   60.278023] stack backtrace:
    [   60.278031] CPU: 3 PID: 2225 Comm: bash Not tainted 3.10.0-rc7-dbg-01385-g241fd04-dirty #1744
    [   60.278037] Hardware name: Acer             Aspire 5741G    /Aspire 5741G    , BIOS V1.20 02/08/2011
    [   60.278042]  ffffffff8204e110 ffff88014df6b9f8 ffffffff815b3d90 ffff88014df6ba38
    [   60.278055]  ffffffff815b0a8d ffff880150ed3f60 ffff880150ed4770 3871c4002c8980b2
    [   60.278068]  ffff880150ed4748 ffff880150ed4770 ffff880150ed3f60 ffff88014df6bb00
    [   60.278081] Call Trace:
    [   60.278091]  [<ffffffff815b3d90>] dump_stack+0x19/0x1b
    [   60.278101]  [<ffffffff815b0a8d>] print_circular_bug+0x2b6/0x2c5
    [   60.278111]  [<ffffffff810ab826>] __lock_acquire+0x1766/0x1d30
    [   60.278123]  [<ffffffff81067e08>] ? __kernel_text_address+0x58/0x80
    [   60.278134]  [<ffffffff810ac6d4>] lock_acquire+0xa4/0x200
    [   60.278142]  [<ffffffff810621b5>] ? flush_work+0x5/0x280
    [   60.278151]  [<ffffffff810621ed>] flush_work+0x3d/0x280
    [   60.278159]  [<ffffffff810621b5>] ? flush_work+0x5/0x280
    [   60.278169]  [<ffffffff810a9b14>] ? mark_held_locks+0x94/0x140
    [   60.278178]  [<ffffffff81062d77>] ? __cancel_work_timer+0x77/0x120
    [   60.278188]  [<ffffffff810a9cbd>] ? trace_hardirqs_on_caller+0xfd/0x1c0
    [   60.278196]  [<ffffffff81062d8a>] __cancel_work_timer+0x8a/0x120
    [   60.278206]  [<ffffffff81062e53>] cancel_delayed_work_sync+0x13/0x20
    [   60.278214]  [<ffffffff814b89d9>] cpufreq_governor_dbs+0x529/0x6f0
    [   60.278225]  [<ffffffff814b76a7>] cs_cpufreq_governor_dbs+0x17/0x20
    [   60.278234]  [<ffffffff814b5df8>] __cpufreq_governor+0x48/0x100
    [   60.278244]  [<ffffffff814b6b80>] __cpufreq_remove_dev.isra.14+0x80/0x3c0
    [   60.278255]  [<ffffffff815adc0d>] cpufreq_cpu_callback+0x38/0x4c
    [   60.278265]  [<ffffffff81071a4d>] notifier_call_chain+0x5d/0x110
    [   60.278275]  [<ffffffff81071b0e>] __raw_notifier_call_chain+0xe/0x10
    [   60.278284]  [<ffffffff815a0a68>] _cpu_down+0x88/0x330
    [   60.278292]  [<ffffffff81024cf7>] ? cpu_hotplug_driver_lock+0x17/0x20
    [   60.278302]  [<ffffffff815a0d46>] cpu_down+0x36/0x50
    [   60.278311]  [<ffffffff815a2748>] store_online+0x98/0xd0
    [   60.278320]  [<ffffffff81452a28>] dev_attr_store+0x18/0x30
    [   60.278329]  [<ffffffff811d9edb>] sysfs_write_file+0xdb/0x150
    [   60.278337]  [<ffffffff8116806d>] vfs_write+0xbd/0x1f0
    [   60.278347]  [<ffffffff81185950>] ? fget_light+0x320/0x4b0
    [   60.278355]  [<ffffffff811686fc>] SyS_write+0x4c/0xa0
    [   60.278364]  [<ffffffff815bbbbe>] tracesys+0xd0/0xd5
    [   60.280582] smpboot: CPU 1 is now offline
    
    The intention of that commit was to avoid warnings during CPU
    hotplug, which indicated that offline CPUs were getting IPIs from the
    cpufreq governor's work items.  But the real root-cause of that
    problem was commit a66b2e5 (cpufreq: Preserve sysfs files across
    suspend/resume) because it totally skipped all the cpufreq callbacks
    during CPU hotplug in the suspend/resume path, and hence it never
    actually shut down the cpufreq governor's worker threads during CPU
    offline in the suspend/resume path.
    
    Reflecting back, the reason why we never suspected that commit as the
    root-cause earlier, was that the original issue was reported with
    just the halt command and nobody had brought in suspend/resume to the
    equation.
    
    The reason for _that_ in turn, as it turns out, is that earlier
    halt/shutdown was being done by disabling non-boot CPUs while tasks
    were frozen, just like suspend/resume....  but commit cf7df378a
    (reboot: migrate shutdown/reboot to boot cpu) which came somewhere
    along that very same time changed that logic: shutdown/halt no longer
    takes CPUs offline.  Thus, the test-cases for reproducing the bug
    were vastly different and thus we went totally off the trail.
    
    Overall, it was one hell of a confusion with so many commits
    affecting each other and also affecting the symptoms of the problems
    in subtle ways.  Finally, now since the original problematic commit
    (a66b2e5) has been completely reverted, revert this intermediate fix
    too (2f7021a8), to fix the CPU hotplug deadlock.  Phew!
    
    Reported-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Reported-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Tested-by: Peter Wu <lekensteyn@gmail.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b7356abb9fb952d385caef6d58d7e7aff17a478e
Merge: ecb2cf1a6b63 5a8d28155930
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 19 09:59:06 2013 -0700

    Merge tag 'pm+acpi-3.11-rc2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management and ACPI fixes from Rafael Wysocki:
     "These are fixes collected over the last week, most importnatly two
      cpufreq reverts fixing regressions introduced in 3.10, an autoseelp
      fix preventing systems using it from crashing during shutdown and two
      ACPI scan fixes related to hotplug.
    
      Specifics:
    
       - Two cpufreq commits from the 3.10 cycle introduced regressions.
         The first of them was buggy (it did way much more than it needed to
         do) and the second one attempted to fix an issue introduced by the
         first one.  Fixes from Srivatsa S Bhat revert both.
    
       - If autosleep triggers during system shutdown and the shutdown
         callbacks of some device drivers have been called already, it may
         crash the system.  Fix from Liu Shuo prevents that from happening
         by making try_to_suspend() check system_state.
    
       - The ACPI memory hotplug driver doesn't clear its driver_data on
         errors which may cause a NULL poiter dereference to happen later.
         Fix from Toshi Kani.
    
       - The ACPI namespace scanning code should not try to attach scan
         handlers to device objects that have them already, which may
         confuse things quite a bit, and it should rescan the whole
         namespace branch starting at the given node after receiving a bus
         check notify event even if the device at that particular node has
         been discovered already.  Fixes from Rafael J Wysocki.
    
       - New ACPI video blacklist entry for a system whose initial backlight
         setting from the BIOS doesn't make sense.  From Lan Tianyu.
    
       - Garbage string output avoindance for ACPI PNP from Liu Shuo.
    
       - Two Kconfig fixes for issues introduced recently in the s3c24xx
         cpufreq driver (when moving the driver to drivers/cpufreq) from
         Paul Bolle.
    
       - Trivial comment fix in pm_wakeup.h from Chanwoo Choi"
    
    * tag 'pm+acpi-3.11-rc2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm:
      ACPI / video: ignore BIOS initial backlight value for Fujitsu E753
      PNP / ACPI: avoid garbage in resource name
      cpufreq: Revert commit 2f7021a8 to fix CPU hotplug regression
      cpufreq: s3c24xx: fix "depends on ARM_S3C24XX" in Kconfig
      cpufreq: s3c24xx: rename CONFIG_CPU_FREQ_S3C24XX_DEBUGFS
      PM / Sleep: Fix comment typo in pm_wakeup.h
      PM / Sleep: avoid 'autosleep' in shutdown progress
      cpufreq: Revert commit a66b2e to fix suspend/resume regression
      ACPI / memhotplug: Fix a stale pointer in error path
      ACPI / scan: Always call acpi_bus_scan() for bus check notifications
      ACPI / scan: Do not try to attach scan handlers to devices having them

commit ac2527bfc21739b77d687df1bfc4e973103fef7b
Author: Joseph Lo <josephl@nvidia.com>
Date:   Wed Jul 3 17:50:38 2013 +0800

    ARM: tegra: add a flag for tegra_disable_clean_inv_dcache to do LoUIS or ALL
    
    Adding a flag for tegra_disable_clean_inv_dcache to flush cache as LoUIS
    or ALL. After this patch, the v7_flush_dcache_louis is used for CPU hotplug
    and CPU suspend in CPU power down (e.g. CPU idle power-down mode) case. And
    the v7_flush_dcache_all is used for CPU cluster power down (e.g. suspend to
    LP2 mode).
    
    Signed-off-by: Joseph Lo <josephl@nvidia.com>
    Signed-off-by: Stephen Warren <swarren@nvidia.com>

commit 5a8d2815593007d4eb59f337ef919c871c2649ab
Merge: fc0ad6c7bb8c e8d05276f236
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Jul 18 21:47:00 2013 +0200

    Merge branch 'pm-fixes'
    
    * pm-fixes:
      cpufreq: Revert commit 2f7021a8 to fix CPU hotplug regression
      cpufreq: s3c24xx: fix "depends on ARM_S3C24XX" in Kconfig
      cpufreq: s3c24xx: rename CONFIG_CPU_FREQ_S3C24XX_DEBUGFS
      PM / Sleep: Fix comment typo in pm_wakeup.h
      PM / Sleep: avoid 'autosleep' in shutdown progress
      cpufreq: Revert commit a66b2e to fix suspend/resume regression

commit e8d05276f236ee6435e78411f62be9714e0b9377
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Jul 16 22:46:48 2013 +0200

    cpufreq: Revert commit 2f7021a8 to fix CPU hotplug regression
    
    commit 2f7021a8 "cpufreq: protect 'policy->cpus' from offlining
    during __gov_queue_work()" caused a regression in CPU hotplug,
    because it lead to a deadlock between cpufreq governor worker thread
    and the CPU hotplug writer task.
    
    Lockdep splat corresponding to this deadlock is shown below:
    
    [   60.277396] ======================================================
    [   60.277400] [ INFO: possible circular locking dependency detected ]
    [   60.277407] 3.10.0-rc7-dbg-01385-g241fd04-dirty #1744 Not tainted
    [   60.277411] -------------------------------------------------------
    [   60.277417] bash/2225 is trying to acquire lock:
    [   60.277422]  ((&(&j_cdbs->work)->work)){+.+...}, at: [<ffffffff810621b5>] flush_work+0x5/0x280
    [   60.277444] but task is already holding lock:
    [   60.277449]  (cpu_hotplug.lock){+.+.+.}, at: [<ffffffff81042d8b>] cpu_hotplug_begin+0x2b/0x60
    [   60.277465] which lock already depends on the new lock.
    
    [   60.277472] the existing dependency chain (in reverse order) is:
    [   60.277477] -> #2 (cpu_hotplug.lock){+.+.+.}:
    [   60.277490]        [<ffffffff810ac6d4>] lock_acquire+0xa4/0x200
    [   60.277503]        [<ffffffff815b6157>] mutex_lock_nested+0x67/0x410
    [   60.277514]        [<ffffffff81042cbc>] get_online_cpus+0x3c/0x60
    [   60.277522]        [<ffffffff814b842a>] gov_queue_work+0x2a/0xb0
    [   60.277532]        [<ffffffff814b7891>] cs_dbs_timer+0xc1/0xe0
    [   60.277543]        [<ffffffff8106302d>] process_one_work+0x1cd/0x6a0
    [   60.277552]        [<ffffffff81063d31>] worker_thread+0x121/0x3a0
    [   60.277560]        [<ffffffff8106ae2b>] kthread+0xdb/0xe0
    [   60.277569]        [<ffffffff815bb96c>] ret_from_fork+0x7c/0xb0
    [   60.277580] -> #1 (&j_cdbs->timer_mutex){+.+...}:
    [   60.277592]        [<ffffffff810ac6d4>] lock_acquire+0xa4/0x200
    [   60.277600]        [<ffffffff815b6157>] mutex_lock_nested+0x67/0x410
    [   60.277608]        [<ffffffff814b785d>] cs_dbs_timer+0x8d/0xe0
    [   60.277616]        [<ffffffff8106302d>] process_one_work+0x1cd/0x6a0
    [   60.277624]        [<ffffffff81063d31>] worker_thread+0x121/0x3a0
    [   60.277633]        [<ffffffff8106ae2b>] kthread+0xdb/0xe0
    [   60.277640]        [<ffffffff815bb96c>] ret_from_fork+0x7c/0xb0
    [   60.277649] -> #0 ((&(&j_cdbs->work)->work)){+.+...}:
    [   60.277661]        [<ffffffff810ab826>] __lock_acquire+0x1766/0x1d30
    [   60.277669]        [<ffffffff810ac6d4>] lock_acquire+0xa4/0x200
    [   60.277677]        [<ffffffff810621ed>] flush_work+0x3d/0x280
    [   60.277685]        [<ffffffff81062d8a>] __cancel_work_timer+0x8a/0x120
    [   60.277693]        [<ffffffff81062e53>] cancel_delayed_work_sync+0x13/0x20
    [   60.277701]        [<ffffffff814b89d9>] cpufreq_governor_dbs+0x529/0x6f0
    [   60.277709]        [<ffffffff814b76a7>] cs_cpufreq_governor_dbs+0x17/0x20
    [   60.277719]        [<ffffffff814b5df8>] __cpufreq_governor+0x48/0x100
    [   60.277728]        [<ffffffff814b6b80>] __cpufreq_remove_dev.isra.14+0x80/0x3c0
    [   60.277737]        [<ffffffff815adc0d>] cpufreq_cpu_callback+0x38/0x4c
    [   60.277747]        [<ffffffff81071a4d>] notifier_call_chain+0x5d/0x110
    [   60.277759]        [<ffffffff81071b0e>] __raw_notifier_call_chain+0xe/0x10
    [   60.277768]        [<ffffffff815a0a68>] _cpu_down+0x88/0x330
    [   60.277779]        [<ffffffff815a0d46>] cpu_down+0x36/0x50
    [   60.277788]        [<ffffffff815a2748>] store_online+0x98/0xd0
    [   60.277796]        [<ffffffff81452a28>] dev_attr_store+0x18/0x30
    [   60.277806]        [<ffffffff811d9edb>] sysfs_write_file+0xdb/0x150
    [   60.277818]        [<ffffffff8116806d>] vfs_write+0xbd/0x1f0
    [   60.277826]        [<ffffffff811686fc>] SyS_write+0x4c/0xa0
    [   60.277834]        [<ffffffff815bbbbe>] tracesys+0xd0/0xd5
    [   60.277842] other info that might help us debug this:
    
    [   60.277848] Chain exists of:
      (&(&j_cdbs->work)->work) --> &j_cdbs->timer_mutex --> cpu_hotplug.lock
    
    [   60.277864]  Possible unsafe locking scenario:
    
    [   60.277869]        CPU0                    CPU1
    [   60.277873]        ----                    ----
    [   60.277877]   lock(cpu_hotplug.lock);
    [   60.277885]                                lock(&j_cdbs->timer_mutex);
    [   60.277892]                                lock(cpu_hotplug.lock);
    [   60.277900]   lock((&(&j_cdbs->work)->work));
    [   60.277907]  *** DEADLOCK ***
    
    [   60.277915] 6 locks held by bash/2225:
    [   60.277919]  #0:  (sb_writers#6){.+.+.+}, at: [<ffffffff81168173>] vfs_write+0x1c3/0x1f0
    [   60.277937]  #1:  (&buffer->mutex){+.+.+.}, at: [<ffffffff811d9e3c>] sysfs_write_file+0x3c/0x150
    [   60.277954]  #2:  (s_active#61){.+.+.+}, at: [<ffffffff811d9ec3>] sysfs_write_file+0xc3/0x150
    [   60.277972]  #3:  (x86_cpu_hotplug_driver_mutex){+.+...}, at: [<ffffffff81024cf7>] cpu_hotplug_driver_lock+0x17/0x20
    [   60.277990]  #4:  (cpu_add_remove_lock){+.+.+.}, at: [<ffffffff815a0d32>] cpu_down+0x22/0x50
    [   60.278007]  #5:  (cpu_hotplug.lock){+.+.+.}, at: [<ffffffff81042d8b>] cpu_hotplug_begin+0x2b/0x60
    [   60.278023] stack backtrace:
    [   60.278031] CPU: 3 PID: 2225 Comm: bash Not tainted 3.10.0-rc7-dbg-01385-g241fd04-dirty #1744
    [   60.278037] Hardware name: Acer             Aspire 5741G    /Aspire 5741G    , BIOS V1.20 02/08/2011
    [   60.278042]  ffffffff8204e110 ffff88014df6b9f8 ffffffff815b3d90 ffff88014df6ba38
    [   60.278055]  ffffffff815b0a8d ffff880150ed3f60 ffff880150ed4770 3871c4002c8980b2
    [   60.278068]  ffff880150ed4748 ffff880150ed4770 ffff880150ed3f60 ffff88014df6bb00
    [   60.278081] Call Trace:
    [   60.278091]  [<ffffffff815b3d90>] dump_stack+0x19/0x1b
    [   60.278101]  [<ffffffff815b0a8d>] print_circular_bug+0x2b6/0x2c5
    [   60.278111]  [<ffffffff810ab826>] __lock_acquire+0x1766/0x1d30
    [   60.278123]  [<ffffffff81067e08>] ? __kernel_text_address+0x58/0x80
    [   60.278134]  [<ffffffff810ac6d4>] lock_acquire+0xa4/0x200
    [   60.278142]  [<ffffffff810621b5>] ? flush_work+0x5/0x280
    [   60.278151]  [<ffffffff810621ed>] flush_work+0x3d/0x280
    [   60.278159]  [<ffffffff810621b5>] ? flush_work+0x5/0x280
    [   60.278169]  [<ffffffff810a9b14>] ? mark_held_locks+0x94/0x140
    [   60.278178]  [<ffffffff81062d77>] ? __cancel_work_timer+0x77/0x120
    [   60.278188]  [<ffffffff810a9cbd>] ? trace_hardirqs_on_caller+0xfd/0x1c0
    [   60.278196]  [<ffffffff81062d8a>] __cancel_work_timer+0x8a/0x120
    [   60.278206]  [<ffffffff81062e53>] cancel_delayed_work_sync+0x13/0x20
    [   60.278214]  [<ffffffff814b89d9>] cpufreq_governor_dbs+0x529/0x6f0
    [   60.278225]  [<ffffffff814b76a7>] cs_cpufreq_governor_dbs+0x17/0x20
    [   60.278234]  [<ffffffff814b5df8>] __cpufreq_governor+0x48/0x100
    [   60.278244]  [<ffffffff814b6b80>] __cpufreq_remove_dev.isra.14+0x80/0x3c0
    [   60.278255]  [<ffffffff815adc0d>] cpufreq_cpu_callback+0x38/0x4c
    [   60.278265]  [<ffffffff81071a4d>] notifier_call_chain+0x5d/0x110
    [   60.278275]  [<ffffffff81071b0e>] __raw_notifier_call_chain+0xe/0x10
    [   60.278284]  [<ffffffff815a0a68>] _cpu_down+0x88/0x330
    [   60.278292]  [<ffffffff81024cf7>] ? cpu_hotplug_driver_lock+0x17/0x20
    [   60.278302]  [<ffffffff815a0d46>] cpu_down+0x36/0x50
    [   60.278311]  [<ffffffff815a2748>] store_online+0x98/0xd0
    [   60.278320]  [<ffffffff81452a28>] dev_attr_store+0x18/0x30
    [   60.278329]  [<ffffffff811d9edb>] sysfs_write_file+0xdb/0x150
    [   60.278337]  [<ffffffff8116806d>] vfs_write+0xbd/0x1f0
    [   60.278347]  [<ffffffff81185950>] ? fget_light+0x320/0x4b0
    [   60.278355]  [<ffffffff811686fc>] SyS_write+0x4c/0xa0
    [   60.278364]  [<ffffffff815bbbbe>] tracesys+0xd0/0xd5
    [   60.280582] smpboot: CPU 1 is now offline
    
    The intention of that commit was to avoid warnings during CPU
    hotplug, which indicated that offline CPUs were getting IPIs from the
    cpufreq governor's work items.  But the real root-cause of that
    problem was commit a66b2e5 (cpufreq: Preserve sysfs files across
    suspend/resume) because it totally skipped all the cpufreq callbacks
    during CPU hotplug in the suspend/resume path, and hence it never
    actually shut down the cpufreq governor's worker threads during CPU
    offline in the suspend/resume path.
    
    Reflecting back, the reason why we never suspected that commit as the
    root-cause earlier, was that the original issue was reported with
    just the halt command and nobody had brought in suspend/resume to the
    equation.
    
    The reason for _that_ in turn, as it turns out, is that earlier
    halt/shutdown was being done by disabling non-boot CPUs while tasks
    were frozen, just like suspend/resume....  but commit cf7df378a
    (reboot: migrate shutdown/reboot to boot cpu) which came somewhere
    along that very same time changed that logic: shutdown/halt no longer
    takes CPUs offline.  Thus, the test-cases for reproducing the bug
    were vastly different and thus we went totally off the trail.
    
    Overall, it was one hell of a confusion with so many commits
    affecting each other and also affecting the symptoms of the problems
    in subtle ways.  Finally, now since the original problematic commit
    (a66b2e5) has been completely reverted, revert this intermediate fix
    too (2f7021a8), to fix the CPU hotplug deadlock.  Phew!
    
    Reported-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Reported-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Tested-by: Peter Wu <lekensteyn@gmail.com>
    Cc: 3.10+ <stable@vger.kernel.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 728ce22b696f9f1404a74d7b2279a65933553a1b
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Wed Jun 12 15:08:51 2013 +0200

    cpuidle: Make cpuidle's sysfs directory dynamically allocated
    
    The cpuidle sysfs code is designed to have a single instance of per
    CPU cpuidle directory.  It is not possible to remove the sysfs entry
    and create it again.  This is not a problem with the current code but
    future changes will add CPU hotplug support to enable/disable the
    device, so it will need to remove the sysfs entry like other
    subsystems do.  That won't be possible without this change, because
    the kobj is a static object which can't be reused for
    kobj_init_and_add().
    
    Add cpuidle_device_kobj to be allocated dynamically when
    adding/removing a sysfs entry which is consistent with the other
    cpuidle's sysfs entries.
    
    An added benefit is that the sysfs code is now more self-contained
    and the includes needed for sysfs can be moved from cpuidle.h
    directly into sysfs.c so as to reduce the total number of headers
    dragged along with cpuidle.h.
    
    [rjw: Changelog]
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 0da273668657a70155f3d4ae121dc19277a05778
Merge: 560ae37178b1 b0ec636c93dd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 13 15:36:09 2013 -0700

    Merge branch 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     - watchdog fixes for full dynticks
     - improved debug output for full dynticks
     - remove an obsolete full dynticks check
     - two ARM SoC clocksource drivers for sharing across SoCs
     - tick broadcast fix for CPU hotplug
    
    * 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      tick: broadcast: Check broadcast mode on CPU hotplug
      clocksource: arm_global_timer: Add ARM global timer support
      clocksource: Add Marvell Orion SoC timer
      nohz: Remove obsolete check for full dynticks CPUs to be RCU nocbs
      watchdog: Boot-disable by default on full dynticks
      watchdog: Rename confusing state variable
      watchdog: Register / unregister watchdog kthreads on sysctl control
      nohz: Warn if the machine can not perform nohz_full

commit a272dcca1802a7e265a56e60b0d0a6715b0a8ac2
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Thu Jul 11 07:00:59 2013 -0700

    tick: broadcast: Check broadcast mode on CPU hotplug
    
    On ARM systems the dummy clockevent is registered with the cpu
    hotplug notifier chain before any other per-cpu clockevent. This
    has the side-effect of causing the dummy clockevent to be
    registered first in every hotplug sequence. Because the dummy is
    first, we'll try to turn the broadcast source on but the code in
    tick_device_uses_broadcast() assumes the broadcast source is in
    periodic mode and calls tick_broadcast_start_periodic()
    unconditionally.
    
    On boot this isn't a problem because we typically haven't
    switched into oneshot mode yet (if at all). During hotplug, if
    the broadcast source isn't in periodic mode we'll replace the
    broadcast oneshot handler with the broadcast periodic handler and
    start emulating oneshot mode when we shouldn't. Due to the way
    the broadcast oneshot handler programs the next_event it's
    possible for it to contain KTIME_MAX and cause us to hang the
    system when the periodic handler tries to program the next tick.
    Fix this by using the appropriate function to start the broadcast
    source.
    
    Reported-by: Stephen Warren <swarren@nvidia.com>
    Tested-by: Stephen Warren <swarren@nvidia.com>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Cc: Mark Rutland <Mark.Rutland@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: ARM kernel mailing list <linux-arm-kernel@lists.infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Joseph Lo <josephl@nvidia.com>
    Link: http://lkml.kernel.org/r/20130711140059.GA27430@codeaurora.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit f991fae5c6d42dfc5029150b05a78cf3f6c18cc9
Merge: d4141531f63a 2c843bd92ec2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 3 14:35:40 2013 -0700

    Merge tag 'pm+acpi-3.11-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management and ACPI updates from Rafael Wysocki:
     "This time the total number of ACPI commits is slightly greater than
      the number of cpufreq commits, but Viresh Kumar (who works on cpufreq)
      remains the most active patch submitter.
    
      To me, the most significant change is the addition of offline/online
      device operations to the driver core (with the Greg's blessing) and
      the related modifications of the ACPI core hotplug code.  Next are the
      freezer updates from Colin Cross that should make the freezing of
      tasks a bit less heavy weight.
    
      We also have a couple of regression fixes, a number of fixes for
      issues that have not been identified as regressions, two new drivers
      and a bunch of cleanups all over.
    
      Highlights:
    
       - Hotplug changes to support graceful hot-removal failures.
    
         It sometimes is necessary to fail device hot-removal operations
         gracefully if they cannot be carried out completely.  For example,
         if memory from a memory module being hot-removed has been allocated
         for the kernel's own use and cannot be moved elsewhere, it's
         desirable to fail the hot-removal operation in a graceful way
         rather than to crash the kernel, but currenty a success or a kernel
         crash are the only possible outcomes of an attempted memory
         hot-removal.  Needless to say, that is not a very attractive
         alternative and it had to be addressed.
    
         However, in order to make it work for memory, I first had to make
         it work for CPUs and for this purpose I needed to modify the ACPI
         processor driver.  It's been split into two parts, a resident one
         handling the low-level initialization/cleanup and a modular one
         playing the actual driver's role (but it binds to the CPU system
         device objects rather than to the ACPI device objects representing
         processors).  That's been sort of like a live brain surgery on a
         patient who's riding a bike.
    
         So this is a little scary, but since we found and fixed a couple of
         regressions it caused to happen during the early linux-next testing
         (a month ago), nobody has complained.
    
         As a bonus we remove some duplicated ACPI hotplug code, because the
         ACPI-based CPU hotplug is now going to use the common ACPI hotplug
         code.
    
       - Lighter weight freezing of tasks.
    
         These changes from Colin Cross and Mandeep Singh Baines are
         targeted at making the freezing of tasks a bit less heavy weight
         operation.  They reduce the number of tasks woken up every time
         during the freezing, by using the observation that the freezer
         simply doesn't need to wake up some of them and wait for them all
         to call refrigerator().  The time needed for the freezer to decide
         to report a failure is reduced too.
    
         Also reintroduced is the check causing a lockdep warining to
         trigger when try_to_freeze() is called with locks held (which is
         generally unsafe and shouldn't happen).
    
       - cpufreq updates
    
         First off, a commit from Srivatsa S Bhat fixes a resume regression
         introduced during the 3.10 cycle causing some cpufreq sysfs
         attributes to return wrong values to user space after resume.  The
         fix is kind of fresh, but also it's pretty obvious once Srivatsa
         has identified the root cause.
    
         Second, we have a new freqdomain_cpus sysfs attribute for the
         acpi-cpufreq driver to provide information previously available via
         related_cpus.  From Lan Tianyu.
    
         Finally, we fix a number of issues, mostly related to the
         CPUFREQ_POSTCHANGE notifier and cpufreq Kconfig options and clean
         up some code.  The majority of changes from Viresh Kumar with bits
         from Jacob Shin, Heiko Stbner, Xiaoguang Chen, Ezequiel Garcia,
         Arnd Bergmann, and Tang Yuantian.
    
       - ACPICA update
    
         A usual bunch of updates from the ACPICA upstream.
    
         During the 3.4 cycle we introduced support for ACPI 5 extended
         sleep registers, but they are only supposed to be used if the
         HW-reduced mode bit is set in the FADT flags and the code attempted
         to use them without checking that bit.  That caused suspend/resume
         regressions to happen on some systems.  Fix from Lv Zheng causes
         those registers to be used only if the HW-reduced mode bit is set.
    
         Apart from this some other ACPICA bugs are fixed and code cleanups
         are made by Bob Moore, Tomasz Nowicki, Lv Zheng, Chao Guan, and
         Zhang Rui.
    
       - cpuidle updates
    
         New driver for Xilinx Zynq processors is added by Michal Simek.
    
         Multidriver support simplification, addition of some missing
         kerneldoc comments and Kconfig-related fixes come from Daniel
         Lezcano.
    
       - ACPI power management updates
    
         Changes to make suspend/resume work correctly in Xen guests from
         Konrad Rzeszutek Wilk, sparse warning fix from Fengguang Wu and
         cleanups and fixes of the ACPI device power state selection
         routine.
    
       - ACPI documentation updates
    
         Some previously missing pieces of ACPI documentation are added by
         Lv Zheng and Aaron Lu (hopefully, that will help people to
         uderstand how the ACPI subsystem works) and one outdated doc is
         updated by Hanjun Guo.
    
       - Assorted ACPI updates
    
         We finally nailed down the IA-64 issue that was the reason for
         reverting commit 9f29ab11ddbf ("ACPI / scan: do not match drivers
         against objects having scan handlers"), so we can fix it and move
         the ACPI scan handler check added to the ACPI video driver back to
         the core.
    
         A mechanism for adding CMOS RTC address space handlers is
         introduced by Lan Tianyu to allow some EC-related breakage to be
         fixed on some systems.
    
         A spec-compliant implementation of acpi_os_get_timer() is added by
         Mika Westerberg.
    
         The evaluation of _STA is added to do_acpi_find_child() to avoid
         situations in which a pointer to a disabled device object is
         returned instead of an enabled one with the same _ADR value.  From
         Jeff Wu.
    
         Intel BayTrail PCH (Platform Controller Hub) support is added to
         the ACPI driver for Intel Low-Power Subsystems (LPSS) and that
         driver is modified to work around a couple of known BIOS issues.
         Changes from Mika Westerberg and Heikki Krogerus.
    
         The EC driver is fixed by Vasiliy Kulikov to use get_user() and
         put_user() instead of dereferencing user space pointers blindly.
    
         Code cleanups are made by Bjorn Helgaas, Nicholas Mazzuca and Toshi
         Kani.
    
       - Assorted power management updates
    
         The "runtime idle" helper routine is changed to take the return
         values of the callbacks executed by it into account and to call
         rpm_suspend() if they return 0, which allows us to reduce the
         overall code bloat a bit (by dropping some code that's not
         necessary any more after that modification).
    
         The runtime PM documentation is updated by Alan Stern (to reflect
         the "runtime idle" behavior change).
    
         New trace points for PM QoS are added by Sahara
         (<keun-o.park@windriver.com>).
    
         PM QoS documentation is updated by Lan Tianyu.
    
         Code cleanups are made and minor issues are addressed by Bernie
         Thompson, Bjorn Helgaas, Julius Werner, and Shuah Khan.
    
       - devfreq updates
    
         New driver for the Exynos5-bus device from Abhilash Kesavan.
    
         Minor cleanups, fixes and MAINTAINERS update from MyungJoo Ham,
         Abhilash Kesavan, Paul Bolle, Rajagopal Venkat, and Wei Yongjun.
    
       - OMAP power management updates
    
         Adaptive Voltage Scaling (AVS) SmartReflex voltage control driver
         updates from Andrii Tseglytskyi and Nishanth Menon."
    
    * tag 'pm+acpi-3.11-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (162 commits)
      cpufreq: Fix cpufreq regression after suspend/resume
      ACPI / PM: Fix possible NULL pointer deref in acpi_pm_device_sleep_state()
      PM / Sleep: Warn about system time after resume with pm_trace
      cpufreq: don't leave stale policy pointer in cdbs->cur_policy
      acpi-cpufreq: Add new sysfs attribute freqdomain_cpus
      cpufreq: make sure frequency transitions are serialized
      ACPI: implement acpi_os_get_timer() according the spec
      ACPI / EC: Add HP Folio 13 to ec_dmi_table in order to skip DSDT scan
      ACPI: Add CMOS RTC Operation Region handler support
      ACPI / processor: Drop unused variable from processor_perflib.c
      cpufreq: tegra: call CPUFREQ_POSTCHANGE notfier in error cases
      cpufreq: s3c64xx: call CPUFREQ_POSTCHANGE notfier in error cases
      cpufreq: omap: call CPUFREQ_POSTCHANGE notfier in error cases
      cpufreq: imx6q: call CPUFREQ_POSTCHANGE notfier in error cases
      cpufreq: exynos: call CPUFREQ_POSTCHANGE notfier in error cases
      cpufreq: dbx500: call CPUFREQ_POSTCHANGE notfier in error cases
      cpufreq: davinci: call CPUFREQ_POSTCHANGE notfier in error cases
      cpufreq: arm-big-little: call CPUFREQ_POSTCHANGE notfier in error cases
      cpufreq: powernow-k8: call CPUFREQ_POSTCHANGE notfier in error cases
      cpufreq: pcc: call CPUFREQ_POSTCHANGE notfier in error cases
      ...

commit 3e34131a65127e73fbae68c82748f32c8af7e4a4
Merge: f3acb96f38bb 0b0c002c340e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 3 13:12:42 2013 -0700

    Merge tag 'stable/for-linus-3.11-rc0-tag-two' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull Xen bugfixes from Konrad Rzeszutek Wilk:
     - Fix memory leak when CPU hotplugging.
     - Compile bugs with various #ifdefs
     - Fix state changes in Xen PCI front not dealing well with new
       toolstack.
     - Cleanups in code (use pr_*, fix 80 characters splits, etc)
     - Long standing bug in double-reporting the steal time
    
    * tag 'stable/for-linus-3.11-rc0-tag-two' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen/time: remove blocked time accounting from xen "clockchip"
      xen: Convert printks to pr_<level>
      xen: ifdef CONFIG_HIBERNATE_CALLBACKS xen_*_suspend
      xen/pcifront: Deal with toolstack missing 'XenbusStateClosing' state.
      xen/time: Free onlined per-cpu data structure if we want to online it again.
      xen/time: Check that the per_cpu data structure has data before freeing.
      xen/time: Don't leak interrupt name when offlining.
      xen/time: Encapsulate the struct clock_event_device in another structure.
      xen/spinlock: Don't leak interrupt name when offlining.
      xen/smp: Don't leak interrupt name when offlining.
      xen/smp: Set the per-cpu IRQ number to a valid default.
      xen/smp: Introduce a common structure to contain the IRQ name and interrupt line.
      xen/smp: Coalesce the free_irq calls in one function.
      xen-pciback: fix error return code in pcistub_irq_handler_switch()

commit 8b8b2412994fffd5a8ab3b9209fa0aa9f0fcee4c
Author: James Hogan <james.hogan@imgtec.com>
Date:   Mon Jul 1 15:36:38 2013 +0100

    metag: cpu hotplug: route_irq: preserve irq mask
    
    The route_irq() function needs to preserve the irq mask by using the
    _irqsave/irqrestore variants of raw spin lock functions instead of the
    _irq variants. This is because it is called from __cpu_disable() (via
    migrate_irqs()), which is called with IRQs disabled, so using the _irq
    variants re-enables IRQs.
    
    This appears to have been causing occasional hits of the
    BUG_ON(!irqs_disabled()) in __irq_work_run() during CPU hotplug soak
    testing:
      BUG: failure at kernel/irq_work.c:122/__irq_work_run()!
    
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>

commit e8b6cb3947430d62538d88f615c007a51aeb23fe
Merge: bdc8f09685a2 73e797f79e5e
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Jun 28 13:00:45 2013 +0200

    Merge branch 'acpi-doc'
    
    * acpi-doc:
      Documentation / CPU hotplug: Rephrase the outdated description for MADT entries
      ACPI / video: update video_extension.txt for backlight control
      ACPI / video: move video_extension.txt to Documentation/acpi
      ACPI / video: add description for brightness_switch_enabled
      ACPI: Add ACPI namespace documentation
      ACPI: Add sysfs ABI documentation
      ACPI: Update MAINTAINERS file to include Documentation/acpi

commit 419e172145cf6c51d436a8bf4afcd17511f0ff79
Author: Jacob Shin <jacob.shin@amd.com>
Date:   Thu Jun 27 22:02:12 2013 +0200

    cpufreq: don't leave stale policy pointer in cdbs->cur_policy
    
    Clear ->cur_policy when stopping a governor, or the ->cur_policy
    pointer may be stale on systems with have_governor_per_policy when a
    new policy is allocated due to CPU hotplug offline/online.
    
    [rjw: Changelog]
    Suggested-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 73e797f79e5edb3d000d1911a2d859f15c446c1a
Author: Hanjun Guo <hanjun.guo@linaro.org>
Date:   Fri Jun 21 17:09:18 2013 +0800

    Documentation / CPU hotplug: Rephrase the outdated description for MADT entries
    
    More than 256 entries in ACPI MADT is supported from ACPI 3.0, so the
    information in should be Documentation/cpu-hotplug.txt updated.
    
    [rjw: Changelog]
    Suggested-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit b0a52d36f57da68631c4500ed83aadaf7eb0d3c9
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Wed Jun 12 14:04:36 2013 -0700

    CPU hotplug: provide a generic helper to disable/enable CPU hotplug
    
    commit 16e53dbf10a2d7e228709a7286310e629ede5e45 upstream.
    
    There are instances in the kernel where we would like to disable CPU
    hotplug (from sysfs) during some important operation.  Today the freezer
    code depends on this and the code to do it was kinda tailor-made for
    that.
    
    Restructure the code and make it generic enough to be useful for other
    usecases too.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Robin Holt <holt@sgi.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Russ Anderson <rja@sgi.com>
    Cc: Robin Holt <holt@sgi.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit b3cba474228862814480d40554f77e98483f41ed
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Wed Jun 12 14:04:36 2013 -0700

    CPU hotplug: provide a generic helper to disable/enable CPU hotplug
    
    commit 16e53dbf10a2d7e228709a7286310e629ede5e45 upstream.
    
    There are instances in the kernel where we would like to disable CPU
    hotplug (from sysfs) during some important operation.  Today the freezer
    code depends on this and the code to do it was kinda tailor-made for
    that.
    
    Restructure the code and make it generic enough to be useful for other
    usecases too.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Robin Holt <holt@sgi.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Russ Anderson <rja@sgi.com>
    Cc: Robin Holt <holt@sgi.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 39a421ff0b7cb056e687894a9d5f57aa1303e1c8
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Mar 20 19:06:12 2013 -0500

    powerpc/mm/nohash: Ignore NULL stale_map entries
    
    This happens with threads that are offline due to CPU hotplug
    (including threads that were never "plugged in" to begin with because
    SMT is disabled).
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 1b3b08bec4f7bdf1c8b3a822439b070862179415
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Wed Jun 12 14:04:36 2013 -0700

    CPU hotplug: provide a generic helper to disable/enable CPU hotplug
    
    commit 16e53dbf10a2d7e228709a7286310e629ede5e45 upstream.
    
    There are instances in the kernel where we would like to disable CPU
    hotplug (from sysfs) during some important operation.  Today the freezer
    code depends on this and the code to do it was kinda tailor-made for
    that.
    
    Restructure the code and make it generic enough to be useful for other
    usecases too.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Robin Holt <holt@sgi.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Russ Anderson <rja@sgi.com>
    Cc: Robin Holt <holt@sgi.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit f07cb6a089644e2334c3523af589d9233cc18c74
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Mon Jun 10 12:20:17 2013 -0400

    ARM: keystone: Enable SMP support on Keystone machines
    
    Add basic SMP support for Keystone machines. This does not
    include support for CPU hotplug for now.
    
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: arm@kernel.org
    
    Acked-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

commit 19ab428f4b7988ef3ac727c680efc193ef53ce14
Author: Stephen Warren <swarren@nvidia.com>
Date:   Fri Jun 14 16:14:14 2013 +0100

    ARM: 7759/1: decouple CPU offlining from reboot/shutdown
    
    Add comments to machine_shutdown()/halt()/power_off()/restart() that
    describe their purpose and/or requirements re: CPUs being active/not.
    
    In machine_shutdown(), replace the call to smp_send_stop() with a call to
    disable_nonboot_cpus(). This completely disables all but one CPU, thus
    satisfying the requirement that only a single CPU be active for kexec.
    Adjust Kconfig dependencies for this change.
    
    In machine_halt()/power_off()/restart(), call smp_send_stop() directly,
    rather than via machine_shutdown(); these functions don't need to
    completely de-activate all CPUs using hotplug, but rather just quiesce
    them.
    
    Remove smp_kill_cpus(), and its call from smp_send_stop().
    smp_kill_cpus() was indirectly calling smp_ops.cpu_kill() without calling
    smp_ops.cpu_die() on the target CPUs first. At least some implementations
    of smp_ops had issues with this; it caused cpu_kill() to hang on Tegra,
    for example. Since smp_send_stop() is only used for shutdown, halt, and
    power-off, there is no need to attempt any kind of CPU hotplug here.
    
    Adjust Kconfig to reflect that machine_shutdown() (and hence kexec)
    relies upon disable_nonboot_cpus(). However, this alone doesn't guarantee
    that hotplug will work, or even that hotplug is implemented for a
    particular piece of HW that a multi-platform zImage runs on. Hence, add
    error-checking to machine_kexec() to determine whether it did work.
    
    Suggested-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Stephen Warren <swarren@nvidia.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Tested-by:  Zhangfei Gao <zhangfei.gao@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 7bf15412250747277cc53301d550d4894f749b12
Merge: 677b5c48bd52 8f6a0b652882
Author: Olof Johansson <olof@lixom.net>
Date:   Fri Jun 14 18:11:31 2013 -0700

    Merge tag 'tegra-for-3.11-soc' of git://git.kernel.org/pub/scm/linux/kernel/git/swarren/linux-tegra into next/soc
    
    From Stephen Warren:
    ARM: tegra: core SoC support enhancements
    
    This branch contains fixes and enhancement for core Tegra Soc support:
    * CPU hotplug support for Tegra114.
    * Some preliminary work on Tegra114 CPU sleep modes.
    * Minor fix for EMC table DT parsing.
    
    * tag 'tegra-for-3.11-soc' of git://git.kernel.org/pub/scm/linux/kernel/git/swarren/linux-tegra:
      ARM: tegra: don't pass CPU ID to tegra_{set,clear}_cpu_in_lp2
      ARM: tegra: cpuidle: using IS_ENABLED for multi SoCs management in init func
      ARM: tegra: hook tegra_tear_down_cpu function in the PM suspend init function
      ARM: tegra: cpuidle: move the init function behind the suspend init function
      ARM: tegra: remove ifdef in the tegra_resume
      ARM: tegra: add cpu_disable for hotplug
      ARM: tegra114: add CPU hotplug support
      clk: tegra114: implement wait_for_reset and disable_clock for tegra_cpu_car_ops
      ARM: tegra114: add power up sequence for warm boot CPU
      ARM: tegra: make tegra_resume can work for Tegra114
      ARM: tegra: skip SCU and PL310 code when CPU is not Cortex-A9
      ARM: tegra: add an assembly marco to check Tegra SoC ID
      ARM: tegra: emc: correction of ram-code parsing from dt
    
    Signed-off-by: Olof Johansson <olof@lixom.net>

commit cb7e9704d58dab4b1b4284903e6bf973ade3863e
Merge: dcae7f2dfcc6 971394f38999
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 13 12:36:42 2013 -0700

    Merge branch 'rcu/urgent' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu
    
    Pull RCU fixes from Paul McKenney:
     "I must confess that this past merge window was not RCU's best showing.
      This series contains three more fixes for RCU regressions:
    
       1.   A fix to __DECLARE_TRACE_RCU() that causes it to act as an
            interrupt from idle rather than as a task switch from idle.
            This change is needed due to the recent use of _rcuidle()
            tracepoints that can be invoked from interrupt handlers as well
            as from idle.  Without this fix, invoking _rcuidle() tracepoints
            from interrupt handlers results in splats and (more seriously)
            confusion on RCU's part as to whether a given CPU is idle or not.
            This confusion can in turn result in too-short grace periods and
            therefore random memory corruption.
    
       2.   A fix to a subtle deadlock that could result due to RCU doing
            a wakeup while holding one of its rcu_node structure's locks.
            Although the probability of occurrence is low, it really
            does happen.  The fix, courtesy of Steven Rostedt, uses
            irq_work_queue() to avoid the deadlock.
    
       3.   A fix to a silent deadlock (invisible to lockdep) due to the
            interaction of timeouts posted by RCU debug code enabled by
            CONFIG_PROVE_RCU_DELAY=y, grace-period initialization, and CPU
            hotplug operations.  This will not occur in production kernels,
            but really does occur in randconfig testing.  Diagnosis courtesy
            of Steven Rostedt"
    
    * 'rcu/urgent' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu:
      rcu: Fix deadlock with CPU hotplug, RCU GP init, and timer migration
      rcu: Don't call wakeup() with rcu_node structure ->lock held
      trace: Allow idle-safe tracepoints to be called from irq

commit 16e53dbf10a2d7e228709a7286310e629ede5e45
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Wed Jun 12 14:04:36 2013 -0700

    CPU hotplug: provide a generic helper to disable/enable CPU hotplug
    
    There are instances in the kernel where we would like to disable CPU
    hotplug (from sysfs) during some important operation.  Today the freezer
    code depends on this and the code to do it was kinda tailor-made for
    that.
    
    Restructure the code and make it generic enough to be useful for other
    usecases too.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Robin Holt <holt@sgi.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Russ Anderson <rja@sgi.com>
    Cc: Robin Holt <holt@sgi.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Shawn Guo <shawn.guo@linaro.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 971394f389992f8462c4e5ae0e3b49a10a9534a3
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Jun 2 07:13:57 2013 -0700

    rcu: Fix deadlock with CPU hotplug, RCU GP init, and timer migration
    
    In Steven Rostedt's words:
    
    > I've been debugging the last couple of days why my tests have been
    > locking up. One of my tracing tests, runs all available tracers. The
    > lockup always happened with the mmiotrace, which is used to trace
    > interactions between priority drivers and the kernel. But to do this
    > easily, when the tracer gets registered, it disables all but the boot
    > CPUs. The lockup always happened after it got done disabling the CPUs.
    >
    > Then I decided to try this:
    >
    > while :; do
    >       for i in 1 2 3; do
    >               echo 0 > /sys/devices/system/cpu/cpu$i/online
    >       done
    >       for i in 1 2 3; do
    >               echo 1 > /sys/devices/system/cpu/cpu$i/online
    >       done
    > done
    >
    > Well, sure enough, that locked up too, with the same users. Doing a
    > sysrq-w (showing all blocked tasks):
    >
    > [ 2991.344562]   task                        PC stack   pid father
    > [ 2991.344562] rcu_preempt     D ffff88007986fdf8     0    10      2 0x00000000
    > [ 2991.344562]  ffff88007986fc98 0000000000000002 ffff88007986fc48 0000000000000908
    > [ 2991.344562]  ffff88007986c280 ffff88007986ffd8 ffff88007986ffd8 00000000001d3c80
    > [ 2991.344562]  ffff880079248a40 ffff88007986c280 0000000000000000 00000000fffd4295
    > [ 2991.344562] Call Trace:
    > [ 2991.344562]  [<ffffffff815437ba>] schedule+0x64/0x66
    > [ 2991.344562]  [<ffffffff81541750>] schedule_timeout+0xbc/0xf9
    > [ 2991.344562]  [<ffffffff8154bec0>] ? ftrace_call+0x5/0x2f
    > [ 2991.344562]  [<ffffffff81049513>] ? cascade+0xa8/0xa8
    > [ 2991.344562]  [<ffffffff815417ab>] schedule_timeout_uninterruptible+0x1e/0x20
    > [ 2991.344562]  [<ffffffff810c980c>] rcu_gp_kthread+0x502/0x94b
    > [ 2991.344562]  [<ffffffff81062791>] ? __init_waitqueue_head+0x50/0x50
    > [ 2991.344562]  [<ffffffff810c930a>] ? rcu_gp_fqs+0x64/0x64
    > [ 2991.344562]  [<ffffffff81061cdb>] kthread+0xb1/0xb9
    > [ 2991.344562]  [<ffffffff81091e31>] ? lock_release_holdtime.part.23+0x4e/0x55
    > [ 2991.344562]  [<ffffffff81061c2a>] ? __init_kthread_worker+0x58/0x58
    > [ 2991.344562]  [<ffffffff8154c1dc>] ret_from_fork+0x7c/0xb0
    > [ 2991.344562]  [<ffffffff81061c2a>] ? __init_kthread_worker+0x58/0x58
    > [ 2991.344562] kworker/0:1     D ffffffff81a30680     0    47      2 0x00000000
    > [ 2991.344562] Workqueue: events cpuset_hotplug_workfn
    > [ 2991.344562]  ffff880078dbbb58 0000000000000002 0000000000000006 00000000000000d8
    > [ 2991.344562]  ffff880078db8100 ffff880078dbbfd8 ffff880078dbbfd8 00000000001d3c80
    > [ 2991.344562]  ffff8800779ca5c0 ffff880078db8100 ffffffff81541fcf 0000000000000000
    > [ 2991.344562] Call Trace:
    > [ 2991.344562]  [<ffffffff81541fcf>] ? __mutex_lock_common+0x3d4/0x609
    > [ 2991.344562]  [<ffffffff815437ba>] schedule+0x64/0x66
    > [ 2991.344562]  [<ffffffff81543a39>] schedule_preempt_disabled+0x18/0x24
    > [ 2991.344562]  [<ffffffff81541fcf>] __mutex_lock_common+0x3d4/0x609
    > [ 2991.344562]  [<ffffffff8103d11b>] ? get_online_cpus+0x3c/0x50
    > [ 2991.344562]  [<ffffffff8103d11b>] ? get_online_cpus+0x3c/0x50
    > [ 2991.344562]  [<ffffffff815422ff>] mutex_lock_nested+0x3b/0x40
    > [ 2991.344562]  [<ffffffff8103d11b>] get_online_cpus+0x3c/0x50
    > [ 2991.344562]  [<ffffffff810af7e6>] rebuild_sched_domains_locked+0x6e/0x3a8
    > [ 2991.344562]  [<ffffffff810b0ec6>] rebuild_sched_domains+0x1c/0x2a
    > [ 2991.344562]  [<ffffffff810b109b>] cpuset_hotplug_workfn+0x1c7/0x1d3
    > [ 2991.344562]  [<ffffffff810b0ed9>] ? cpuset_hotplug_workfn+0x5/0x1d3
    > [ 2991.344562]  [<ffffffff81058e07>] process_one_work+0x2d4/0x4d1
    > [ 2991.344562]  [<ffffffff81058d3a>] ? process_one_work+0x207/0x4d1
    > [ 2991.344562]  [<ffffffff8105964c>] worker_thread+0x2e7/0x3b5
    > [ 2991.344562]  [<ffffffff81059365>] ? rescuer_thread+0x332/0x332
    > [ 2991.344562]  [<ffffffff81061cdb>] kthread+0xb1/0xb9
    > [ 2991.344562]  [<ffffffff81061c2a>] ? __init_kthread_worker+0x58/0x58
    > [ 2991.344562]  [<ffffffff8154c1dc>] ret_from_fork+0x7c/0xb0
    > [ 2991.344562]  [<ffffffff81061c2a>] ? __init_kthread_worker+0x58/0x58
    > [ 2991.344562] bash            D ffffffff81a4aa80     0  2618   2612 0x10000000
    > [ 2991.344562]  ffff8800379abb58 0000000000000002 0000000000000006 0000000000000c2c
    > [ 2991.344562]  ffff880077fea140 ffff8800379abfd8 ffff8800379abfd8 00000000001d3c80
    > [ 2991.344562]  ffff8800779ca5c0 ffff880077fea140 ffffffff81541fcf 0000000000000000
    > [ 2991.344562] Call Trace:
    > [ 2991.344562]  [<ffffffff81541fcf>] ? __mutex_lock_common+0x3d4/0x609
    > [ 2991.344562]  [<ffffffff815437ba>] schedule+0x64/0x66
    > [ 2991.344562]  [<ffffffff81543a39>] schedule_preempt_disabled+0x18/0x24
    > [ 2991.344562]  [<ffffffff81541fcf>] __mutex_lock_common+0x3d4/0x609
    > [ 2991.344562]  [<ffffffff81530078>] ? rcu_cpu_notify+0x2f5/0x86e
    > [ 2991.344562]  [<ffffffff81530078>] ? rcu_cpu_notify+0x2f5/0x86e
    > [ 2991.344562]  [<ffffffff815422ff>] mutex_lock_nested+0x3b/0x40
    > [ 2991.344562]  [<ffffffff81530078>] rcu_cpu_notify+0x2f5/0x86e
    > [ 2991.344562]  [<ffffffff81091c99>] ? __lock_is_held+0x32/0x53
    > [ 2991.344562]  [<ffffffff81548912>] notifier_call_chain+0x6b/0x98
    > [ 2991.344562]  [<ffffffff810671fd>] __raw_notifier_call_chain+0xe/0x10
    > [ 2991.344562]  [<ffffffff8103cf64>] __cpu_notify+0x20/0x32
    > [ 2991.344562]  [<ffffffff8103cf8d>] cpu_notify_nofail+0x17/0x36
    > [ 2991.344562]  [<ffffffff815225de>] _cpu_down+0x154/0x259
    > [ 2991.344562]  [<ffffffff81522710>] cpu_down+0x2d/0x3a
    > [ 2991.344562]  [<ffffffff81526351>] store_online+0x4e/0xe7
    > [ 2991.344562]  [<ffffffff8134d764>] dev_attr_store+0x20/0x22
    > [ 2991.344562]  [<ffffffff811b3c5f>] sysfs_write_file+0x108/0x144
    > [ 2991.344562]  [<ffffffff8114c5ef>] vfs_write+0xfd/0x158
    > [ 2991.344562]  [<ffffffff8114c928>] SyS_write+0x5c/0x83
    > [ 2991.344562]  [<ffffffff8154c494>] tracesys+0xdd/0xe2
    >
    > As well as held locks:
    >
    > [ 3034.728033] Showing all locks held in the system:
    > [ 3034.728033] 1 lock held by rcu_preempt/10:
    > [ 3034.728033]  #0:  (rcu_preempt_state.onoff_mutex){+.+...}, at: [<ffffffff810c9471>] rcu_gp_kthread+0x167/0x94b
    > [ 3034.728033] 4 locks held by kworker/0:1/47:
    > [ 3034.728033]  #0:  (events){.+.+.+}, at: [<ffffffff81058d3a>] process_one_work+0x207/0x4d1
    > [ 3034.728033]  #1:  (cpuset_hotplug_work){+.+.+.}, at: [<ffffffff81058d3a>] process_one_work+0x207/0x4d1
    > [ 3034.728033]  #2:  (cpuset_mutex){+.+.+.}, at: [<ffffffff810b0ec1>] rebuild_sched_domains+0x17/0x2a
    > [ 3034.728033]  #3:  (cpu_hotplug.lock){+.+.+.}, at: [<ffffffff8103d11b>] get_online_cpus+0x3c/0x50
    > [ 3034.728033] 1 lock held by mingetty/2563:
    > [ 3034.728033]  #0:  (&ldata->atomic_read_lock){+.+...}, at: [<ffffffff8131e28a>] n_tty_read+0x252/0x7e8
    > [ 3034.728033] 1 lock held by mingetty/2565:
    > [ 3034.728033]  #0:  (&ldata->atomic_read_lock){+.+...}, at: [<ffffffff8131e28a>] n_tty_read+0x252/0x7e8
    > [ 3034.728033] 1 lock held by mingetty/2569:
    > [ 3034.728033]  #0:  (&ldata->atomic_read_lock){+.+...}, at: [<ffffffff8131e28a>] n_tty_read+0x252/0x7e8
    > [ 3034.728033] 1 lock held by mingetty/2572:
    > [ 3034.728033]  #0:  (&ldata->atomic_read_lock){+.+...}, at: [<ffffffff8131e28a>] n_tty_read+0x252/0x7e8
    > [ 3034.728033] 1 lock held by mingetty/2575:
    > [ 3034.728033]  #0:  (&ldata->atomic_read_lock){+.+...}, at: [<ffffffff8131e28a>] n_tty_read+0x252/0x7e8
    > [ 3034.728033] 7 locks held by bash/2618:
    > [ 3034.728033]  #0:  (sb_writers#5){.+.+.+}, at: [<ffffffff8114bc3f>] file_start_write+0x2a/0x2c
    > [ 3034.728033]  #1:  (&buffer->mutex#2){+.+.+.}, at: [<ffffffff811b3b93>] sysfs_write_file+0x3c/0x144
    > [ 3034.728033]  #2:  (s_active#54){.+.+.+}, at: [<ffffffff811b3c3e>] sysfs_write_file+0xe7/0x144
    > [ 3034.728033]  #3:  (x86_cpu_hotplug_driver_mutex){+.+.+.}, at: [<ffffffff810217c2>] cpu_hotplug_driver_lock+0x17/0x19
    > [ 3034.728033]  #4:  (cpu_add_remove_lock){+.+.+.}, at: [<ffffffff8103d196>] cpu_maps_update_begin+0x17/0x19
    > [ 3034.728033]  #5:  (cpu_hotplug.lock){+.+.+.}, at: [<ffffffff8103cfd8>] cpu_hotplug_begin+0x2c/0x6d
    > [ 3034.728033]  #6:  (rcu_preempt_state.onoff_mutex){+.+...}, at: [<ffffffff81530078>] rcu_cpu_notify+0x2f5/0x86e
    > [ 3034.728033] 1 lock held by bash/2980:
    > [ 3034.728033]  #0:  (&ldata->atomic_read_lock){+.+...}, at: [<ffffffff8131e28a>] n_tty_read+0x252/0x7e8
    >
    > Things looked a little weird. Also, this is a deadlock that lockdep did
    > not catch. But what we have here does not look like a circular lock
    > issue:
    >
    > Bash is blocked in rcu_cpu_notify():
    >
    > 1961          /* Exclude any attempts to start a new grace period. */
    > 1962          mutex_lock(&rsp->onoff_mutex);
    >
    >
    > kworker is blocked in get_online_cpus(), which makes sense as we are
    > currently taking down a CPU.
    >
    > But rcu_preempt is not blocked on anything. It is simply sleeping in
    > rcu_gp_kthread (really rcu_gp_init) here:
    >
    > 1453  #ifdef CONFIG_PROVE_RCU_DELAY
    > 1454                  if ((prandom_u32() % (rcu_num_nodes * 8)) == 0 &&
    > 1455                      system_state == SYSTEM_RUNNING)
    > 1456                          schedule_timeout_uninterruptible(2);
    > 1457  #endif /* #ifdef CONFIG_PROVE_RCU_DELAY */
    >
    > And it does this while holding the onoff_mutex that bash is waiting for.
    >
    > Doing a function trace, it showed me where it happened:
    >
    > [  125.940066] rcu_pree-10      3.... 28384115273: schedule_timeout_uninterruptible <-rcu_gp_kthread
    > [...]
    > [  125.940066] rcu_pree-10      3d..3 28384202439: sched_switch: prev_comm=rcu_preempt prev_pid=10 prev_prio=120 prev_state=D ==> next_comm=watchdog/3 next_pid=38 next_prio=120
    >
    > The watchdog ran, and then:
    >
    > [  125.940066] watchdog-38      3d..3 28384692863: sched_switch: prev_comm=watchdog/3 prev_pid=38 prev_prio=120 prev_state=P ==> next_comm=modprobe next_pid=2848 next_prio=118
    >
    > Not sure what modprobe was doing, but shortly after that:
    >
    > [  125.940066] modprobe-2848    3d..3 28385041749: sched_switch: prev_comm=modprobe prev_pid=2848 prev_prio=118 prev_state=R+ ==> next_comm=migration/3 next_pid=40 next_prio=0
    >
    > Where the migration thread took down the CPU:
    >
    > [  125.940066] migratio-40      3d..3 28389148276: sched_switch: prev_comm=migration/3 prev_pid=40 prev_prio=0 prev_state=P ==> next_comm=swapper/3 next_pid=0 next_prio=120
    >
    > which finally did:
    >
    > [  125.940066]   <idle>-0       3...1 28389282142: arch_cpu_idle_dead <-cpu_startup_entry
    > [  125.940066]   <idle>-0       3...1 28389282548: native_play_dead <-arch_cpu_idle_dead
    > [  125.940066]   <idle>-0       3...1 28389282924: play_dead_common <-native_play_dead
    > [  125.940066]   <idle>-0       3...1 28389283468: idle_task_exit <-play_dead_common
    > [  125.940066]   <idle>-0       3...1 28389284644: amd_e400_remove_cpu <-play_dead_common
    >
    >
    > CPU 3 is now offline, the rcu_preempt thread that ran on CPU 3 is still
    > doing a schedule_timeout_uninterruptible() and it registered it's
    > timeout to the timer base for CPU 3. You would think that it would get
    > migrated right? The issue here is that the timer migration happens at
    > the CPU notifier for CPU_DEAD. The problem is that the rcu notifier for
    > CPU_DOWN is blocked waiting for the onoff_mutex to be released, which is
    > held by the thread that just put itself into a uninterruptible sleep,
    > that wont wake up until the CPU_DEAD notifier of the timer
    > infrastructure is called, which wont happen until the rcu notifier
    > finishes. Here's our deadlock!
    
    This commit breaks this deadlock cycle by substituting a shorter udelay()
    for the previous schedule_timeout_uninterruptible(), while at the same
    time increasing the probability of the delay.  This maintains the intensity
    of the testing.
    
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Steven Rostedt <rostedt@goodmis.org>

commit db9bde2fa518fa67d28ffa287d1209871bcdc789
Merge: 6678e38959f9 033a899c9b06
Author: Olof Johansson <olof@lixom.net>
Date:   Fri May 31 23:39:35 2013 -0700

    Merge branch 'VExpress_DCSCB' of git://git.linaro.org/people/nico/linux into next/soc
    
    From Nicolas Pitre:
    
    This is the first MCPM backend submission for VExpress running on RTSM
    aka Fast Models implementing the big.LITTLE system architecture.  This
    enables SMP secondary boot as well as CPU hotplug on this platform.
    
    A big prerequisite for this support is the CCI driver from Lorenzo
    included in this pull request.
    
    Also included is Rob Herring's set_auxcr/get_auxcr allowing nicer code.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    
    * 'VExpress_DCSCB' of git://git.linaro.org/people/nico/linux:
      ARM: vexpress: Select multi-cluster SMP operation if required
      ARM: vexpress/dcscb: handle platform coherency exit/setup and CCI
      ARM: vexpress/dcscb: do not hardcode number of CPUs per cluster
      ARM: vexpress/dcscb: add CPU use counts to the power up/down API implementation
      ARM: vexpress: introduce DCSCB support
      ARM: introduce common set_auxcr/get_auxcr functions
      drivers/bus: arm-cci: function to enable CCI ports from early boot code
      drivers: bus: add ARM CCI support

commit 289ca1f20ef7f965b4e34a2644800ee6b95f8cab
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Sun May 5 09:30:09 2013 -0400

    xen/vcpu/pvhvm: Fix vcpu hotplugging hanging.
    
    commit 7f1fc268c47491fd5e63548f6415fc8604e13003 upstream.
    
    If a user did:
    
            echo 0 > /sys/devices/system/cpu/cpu1/online
            echo 1 > /sys/devices/system/cpu/cpu1/online
    
    we would (this a build with DEBUG enabled) get to:
    smpboot: ++++++++++++++++++++=_---CPU UP  1
    .. snip..
    smpboot: Stack at about ffff880074c0ff44
    smpboot: CPU1: has booted.
    
    and hang. The RCU mechanism would kick in an try to IPI the CPU1
    but the IPIs (and all other interrupts) would never arrive at the
    CPU1. At first glance at least. A bit digging in the hypervisor
    trace shows that (using xenanalyze):
    
    [vla] d4v1 vec 243 injecting
       0.043163027 --|x d4v1 intr_window vec 243 src 5(vector) intr f3
    ]  0.043163639 --|x d4v1 vmentry cycles 1468
    ]  0.043164913 --|x d4v1 vmexit exit_reason PENDING_INTERRUPT eip ffffffff81673254
       0.043164913 --|x d4v1 inj_virq vec 243  real
      [vla] d4v1 vec 243 injecting
       0.043164913 --|x d4v1 intr_window vec 243 src 5(vector) intr f3
    ]  0.043165526 --|x d4v1 vmentry cycles 1472
    ]  0.043166800 --|x d4v1 vmexit exit_reason PENDING_INTERRUPT eip ffffffff81673254
       0.043166800 --|x d4v1 inj_virq vec 243  real
      [vla] d4v1 vec 243 injecting
    
    there is a pending event (subsequent debugging shows it is the IPI
    from the VCPU0 when smpboot.c on VCPU1 has done
    "set_cpu_online(smp_processor_id(), true)") and the guest VCPU1 is
    interrupted with the callback IPI (0xf3 aka 243) which ends up calling
    __xen_evtchn_do_upcall.
    
    The __xen_evtchn_do_upcall seems to do *something* but not acknowledge
    the pending events. And the moment the guest does a 'cli' (that is the
    ffffffff81673254 in the log above) the hypervisor is invoked again to
    inject the IPI (0xf3) to tell the guest it has pending interrupts.
    This repeats itself forever.
    
    The culprit was the per_cpu(xen_vcpu, cpu) pointer. At the bootup
    we set each per_cpu(xen_vcpu, cpu) to point to the
    shared_info->vcpu_info[vcpu] but later on use the VCPUOP_register_vcpu_info
    to register per-CPU  structures (xen_vcpu_setup).
    This is used to allow events for more than 32 VCPUs and for performance
    optimizations reasons.
    
    When the user performs the VCPU hotplug we end up calling the
    the xen_vcpu_setup once more. We make the hypercall which returns
    -EINVAL as it does not allow multiple registration calls (and
    already has re-assigned where the events are being set). We pick
    the fallback case and set per_cpu(xen_vcpu, cpu) to point to the
    shared_info->vcpu_info[vcpu] (which is a good fallback during bootup).
    However the hypervisor is still setting events in the register
    per-cpu structure (per_cpu(xen_vcpu_info, cpu)).
    
    As such when the events are set by the hypervisor (such as timer one),
    and when we iterate in __xen_evtchn_do_upcall we end up reading stale
    events from the shared_info->vcpu_info[vcpu] instead of the
    per_cpu(xen_vcpu_info, cpu) structures. Hence we never acknowledge the
    events that the hypervisor has set and the hypervisor keeps on reminding
    us to ack the events which we never do.
    
    The fix is simple. Don't on the second time when xen_vcpu_setup is
    called over-write the per_cpu(xen_vcpu, cpu) if it points to
    per_cpu(xen_vcpu_info).
    
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit da9d83cf6e1f64c5dafc7c47f6ce93788bc1263a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 20 11:36:03 2013 -0700

    x86: Fix bit corruption at CPU resume time
    
    commit 5e427ec2d066b48a5c27b3a5a3315f7e4e729077 upstream.
    
    In commit 78d77df71510 ("x86-64, init: Do not set NX bits on non-NX
    capable hardware") we added the early_pmd_flags that gets the NX bit set
    when a CPU supports NX. However, the new variable was marked __initdata,
    because the main _use_ of this is in an __init routine.
    
    However, the bit setting happens from secondary_startup_64(), which is
    called not only at bootup, but on every secondary CPU start.  Including
    resuming from STR and at CPU hotplug time.  So the value cannot be
    __initdata.
    
    Reported-bisected-and-tested-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Peter Anvin <hpa@linux.intel.com>
    Cc: Fernando Luis Vzquez Cao <fernando@oss.ntt.co.jp>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 33d5c01915ccca298a5fda7e0cb33199d225e03a
Author: Joseph Lo <josephl@nvidia.com>
Date:   Mon May 20 18:39:29 2013 +0800

    ARM: tegra114: add CPU hotplug support
    
    The Tegra114 is a quad cores SoC. Each core can be hotplugged including
    CPU0. The hotplug sequence can be controlled by setting event trigger in
    flow controller. Then the flow controller will take care all the power
    sequence that include CPU up and down.
    
    Signed-off-by: Joseph Lo <josephl@nvidia.com>
    Signed-off-by: Stephen Warren <swarren@nvidia.com>

commit 31972fd95527a5942b777e89404501d5421a0df0
Author: Joseph Lo <josephl@nvidia.com>
Date:   Mon May 20 18:39:28 2013 +0800

    clk: tegra114: implement wait_for_reset and disable_clock for tegra_cpu_car_ops
    
    The conventional CPU hotplug sequence on the other Tegra chips, we will also
    clock gate the CPU in tegra_cpu_kill() after the CPU was power gated. For
    Tegra114, the flow controller will clock gate the CPU after the power down
    sequence. But we still need to implement a empty function for disable_clock
    to avoid kernel warning message.
    
    Signed-off-by: Joseph Lo <josephl@nvidia.com>
    Acked-by: Mike Turquette <mturquette@linaro.org>
    Signed-off-by: Stephen Warren <swarren@nvidia.com>

commit 18901e9f4fc9c99667516974d82c437453f83348
Author: Joseph Lo <josephl@nvidia.com>
Date:   Mon May 20 18:39:27 2013 +0800

    ARM: tegra114: add power up sequence for warm boot CPU
    
    For Tegra114, once the CPUs were powered up by PMC in cold boot flow. The
    flow controller will maintain the power state and control power sequence
    for each CPU by setting event trigger (e.g. CPU hotplug ,idle and
    suspend power down/up).
    
    Signed-off-by: Joseph Lo <josephl@nvidia.com>
    Signed-off-by: Stephen Warren <swarren@nvidia.com>

commit 1c4e2d70afb132058bfe979f6854c1d0a732b556
Author: Igor Mammedov <imammedo@redhat.com>
Date:   Tue May 14 16:46:07 2013 +0200

    cpu: make sure that cpu/online file created before KOBJ_ADD is emitted
    
    It fixes race between udev and hotplugged CPU registration by defining
    "online" attribute statically, so that device_add() would create it
    before notifying udev about new CPU.
    
    Original issue report is at https://lkml.org/lkml/2012/4/30/198
    "
    > On Mon, Apr 30, 2012 at 11:36:23AM -0400, Konrad Rzeszutek Wilk wrote:
    > > Hey Greg,
    > >
    > > Hoping you can help with some guidance on how to fix this.
    > >
    > > The issue is with CPU hotplug is that when a CPU goes up
    > > it calls 'arch_register_cpu' which eventually calls
    > > register_cpu. That function does these two things:
    > >
    > > 251         error = device_register(&cpu->dev);
    > > 252         if (!error && cpu->hotpluggable)
    > > 253                 register_cpu_control(cpu);
    > >
    > > and the device_register creates a nice little SysFS directory:
    > >
    > > /sys/devices/system/cpu/cpu2/ which at line 251 has the 'add' attribute
    > > but no 'online' attribute. udev then tries to echo 1 to the 'online'
    > > and it we get:
    > > udevd-work[2421]: error opening ATTR{/sys/devices/system/cpu/cpu2/online} for writing: No such file or directory
    > >
    > > Line 253 creates said 'online' and at that time udev [or the system admin]
    > > can write 1 to 'online' and the CPU goes up.
    > >
    > > So .. any thoughts? Is there some way to inhibit from uevent being sent
    > > until line 253 has run?
    "
    
    Signed-off-by: Igor Mammedov <imammedo@redhat.com>
    Tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5e427ec2d066b48a5c27b3a5a3315f7e4e729077
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 20 11:36:03 2013 -0700

    x86: Fix bit corruption at CPU resume time
    
    In commit 78d77df71510 ("x86-64, init: Do not set NX bits on non-NX
    capable hardware") we added the early_pmd_flags that gets the NX bit set
    when a CPU supports NX. However, the new variable was marked __initdata,
    because the main _use_ of this is in an __init routine.
    
    However, the bit setting happens from secondary_startup_64(), which is
    called not only at bootup, but on every secondary CPU start.  Including
    resuming from STR and at CPU hotplug time.  So the value cannot be
    __initdata.
    
    Reported-bisected-and-tested-by: Michal Hocko <mhocko@suse.cz>
    Cc: stable@vger.kernel.org # v3.9
    Acked-by: Peter Anvin <hpa@linux.intel.com>
    Cc: Fernando Luis Vzquez Cao <fernando@oss.ntt.co.jp>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit db9f69dc2d86546eb31c29be637fb70633672fea
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Sun May 5 09:30:09 2013 -0400

    xen/vcpu/pvhvm: Fix vcpu hotplugging hanging.
    
    commit 7f1fc268c47491fd5e63548f6415fc8604e13003 upstream.
    
    If a user did:
    
            echo 0 > /sys/devices/system/cpu/cpu1/online
            echo 1 > /sys/devices/system/cpu/cpu1/online
    
    we would (this a build with DEBUG enabled) get to:
    smpboot: ++++++++++++++++++++=_---CPU UP  1
    .. snip..
    smpboot: Stack at about ffff880074c0ff44
    smpboot: CPU1: has booted.
    
    and hang. The RCU mechanism would kick in an try to IPI the CPU1
    but the IPIs (and all other interrupts) would never arrive at the
    CPU1. At first glance at least. A bit digging in the hypervisor
    trace shows that (using xenanalyze):
    
    [vla] d4v1 vec 243 injecting
       0.043163027 --|x d4v1 intr_window vec 243 src 5(vector) intr f3
    ]  0.043163639 --|x d4v1 vmentry cycles 1468
    ]  0.043164913 --|x d4v1 vmexit exit_reason PENDING_INTERRUPT eip ffffffff81673254
       0.043164913 --|x d4v1 inj_virq vec 243  real
      [vla] d4v1 vec 243 injecting
       0.043164913 --|x d4v1 intr_window vec 243 src 5(vector) intr f3
    ]  0.043165526 --|x d4v1 vmentry cycles 1472
    ]  0.043166800 --|x d4v1 vmexit exit_reason PENDING_INTERRUPT eip ffffffff81673254
       0.043166800 --|x d4v1 inj_virq vec 243  real
      [vla] d4v1 vec 243 injecting
    
    there is a pending event (subsequent debugging shows it is the IPI
    from the VCPU0 when smpboot.c on VCPU1 has done
    "set_cpu_online(smp_processor_id(), true)") and the guest VCPU1 is
    interrupted with the callback IPI (0xf3 aka 243) which ends up calling
    __xen_evtchn_do_upcall.
    
    The __xen_evtchn_do_upcall seems to do *something* but not acknowledge
    the pending events. And the moment the guest does a 'cli' (that is the
    ffffffff81673254 in the log above) the hypervisor is invoked again to
    inject the IPI (0xf3) to tell the guest it has pending interrupts.
    This repeats itself forever.
    
    The culprit was the per_cpu(xen_vcpu, cpu) pointer. At the bootup
    we set each per_cpu(xen_vcpu, cpu) to point to the
    shared_info->vcpu_info[vcpu] but later on use the VCPUOP_register_vcpu_info
    to register per-CPU  structures (xen_vcpu_setup).
    This is used to allow events for more than 32 VCPUs and for performance
    optimizations reasons.
    
    When the user performs the VCPU hotplug we end up calling the
    the xen_vcpu_setup once more. We make the hypercall which returns
    -EINVAL as it does not allow multiple registration calls (and
    already has re-assigned where the events are being set). We pick
    the fallback case and set per_cpu(xen_vcpu, cpu) to point to the
    shared_info->vcpu_info[vcpu] (which is a good fallback during bootup).
    However the hypervisor is still setting events in the register
    per-cpu structure (per_cpu(xen_vcpu_info, cpu)).
    
    As such when the events are set by the hypervisor (such as timer one),
    and when we iterate in __xen_evtchn_do_upcall we end up reading stale
    events from the shared_info->vcpu_info[vcpu] instead of the
    per_cpu(xen_vcpu_info, cpu) structures. Hence we never acknowledge the
    events that the hypervisor has set and the hypervisor keeps on reminding
    us to ack the events which we never do.
    
    The fix is simple. Don't on the second time when xen_vcpu_setup is
    called over-write the per_cpu(xen_vcpu, cpu) if it points to
    per_cpu(xen_vcpu_info).
    
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 5a95901e93b435c0ab9013704b9943d39aef7539
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Sun May 5 09:30:09 2013 -0400

    xen/vcpu/pvhvm: Fix vcpu hotplugging hanging.
    
    commit 7f1fc268c47491fd5e63548f6415fc8604e13003 upstream.
    
    If a user did:
    
            echo 0 > /sys/devices/system/cpu/cpu1/online
            echo 1 > /sys/devices/system/cpu/cpu1/online
    
    we would (this a build with DEBUG enabled) get to:
    smpboot: ++++++++++++++++++++=_---CPU UP  1
    .. snip..
    smpboot: Stack at about ffff880074c0ff44
    smpboot: CPU1: has booted.
    
    and hang. The RCU mechanism would kick in an try to IPI the CPU1
    but the IPIs (and all other interrupts) would never arrive at the
    CPU1. At first glance at least. A bit digging in the hypervisor
    trace shows that (using xenanalyze):
    
    [vla] d4v1 vec 243 injecting
       0.043163027 --|x d4v1 intr_window vec 243 src 5(vector) intr f3
    ]  0.043163639 --|x d4v1 vmentry cycles 1468
    ]  0.043164913 --|x d4v1 vmexit exit_reason PENDING_INTERRUPT eip ffffffff81673254
       0.043164913 --|x d4v1 inj_virq vec 243  real
      [vla] d4v1 vec 243 injecting
       0.043164913 --|x d4v1 intr_window vec 243 src 5(vector) intr f3
    ]  0.043165526 --|x d4v1 vmentry cycles 1472
    ]  0.043166800 --|x d4v1 vmexit exit_reason PENDING_INTERRUPT eip ffffffff81673254
       0.043166800 --|x d4v1 inj_virq vec 243  real
      [vla] d4v1 vec 243 injecting
    
    there is a pending event (subsequent debugging shows it is the IPI
    from the VCPU0 when smpboot.c on VCPU1 has done
    "set_cpu_online(smp_processor_id(), true)") and the guest VCPU1 is
    interrupted with the callback IPI (0xf3 aka 243) which ends up calling
    __xen_evtchn_do_upcall.
    
    The __xen_evtchn_do_upcall seems to do *something* but not acknowledge
    the pending events. And the moment the guest does a 'cli' (that is the
    ffffffff81673254 in the log above) the hypervisor is invoked again to
    inject the IPI (0xf3) to tell the guest it has pending interrupts.
    This repeats itself forever.
    
    The culprit was the per_cpu(xen_vcpu, cpu) pointer. At the bootup
    we set each per_cpu(xen_vcpu, cpu) to point to the
    shared_info->vcpu_info[vcpu] but later on use the VCPUOP_register_vcpu_info
    to register per-CPU  structures (xen_vcpu_setup).
    This is used to allow events for more than 32 VCPUs and for performance
    optimizations reasons.
    
    When the user performs the VCPU hotplug we end up calling the
    the xen_vcpu_setup once more. We make the hypercall which returns
    -EINVAL as it does not allow multiple registration calls (and
    already has re-assigned where the events are being set). We pick
    the fallback case and set per_cpu(xen_vcpu, cpu) to point to the
    shared_info->vcpu_info[vcpu] (which is a good fallback during bootup).
    However the hypervisor is still setting events in the register
    per-cpu structure (per_cpu(xen_vcpu_info, cpu)).
    
    As such when the events are set by the hypervisor (such as timer one),
    and when we iterate in __xen_evtchn_do_upcall we end up reading stale
    events from the shared_info->vcpu_info[vcpu] instead of the
    per_cpu(xen_vcpu_info, cpu) structures. Hence we never acknowledge the
    events that the hypervisor has set and the hypervisor keeps on reminding
    us to ack the events which we never do.
    
    The fix is simple. Don't on the second time when xen_vcpu_setup is
    called over-write the per_cpu(xen_vcpu, cpu) if it points to
    per_cpu(xen_vcpu_info).
    
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a3b5b07e0d750c300d771a0a8e5ad24898bcdd9b
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Sun May 5 09:30:09 2013 -0400

    xen/vcpu/pvhvm: Fix vcpu hotplugging hanging.
    
    commit 7f1fc268c47491fd5e63548f6415fc8604e13003 upstream.
    
    If a user did:
    
            echo 0 > /sys/devices/system/cpu/cpu1/online
            echo 1 > /sys/devices/system/cpu/cpu1/online
    
    we would (this a build with DEBUG enabled) get to:
    smpboot: ++++++++++++++++++++=_---CPU UP  1
    .. snip..
    smpboot: Stack at about ffff880074c0ff44
    smpboot: CPU1: has booted.
    
    and hang. The RCU mechanism would kick in an try to IPI the CPU1
    but the IPIs (and all other interrupts) would never arrive at the
    CPU1. At first glance at least. A bit digging in the hypervisor
    trace shows that (using xenanalyze):
    
    [vla] d4v1 vec 243 injecting
       0.043163027 --|x d4v1 intr_window vec 243 src 5(vector) intr f3
    ]  0.043163639 --|x d4v1 vmentry cycles 1468
    ]  0.043164913 --|x d4v1 vmexit exit_reason PENDING_INTERRUPT eip ffffffff81673254
       0.043164913 --|x d4v1 inj_virq vec 243  real
      [vla] d4v1 vec 243 injecting
       0.043164913 --|x d4v1 intr_window vec 243 src 5(vector) intr f3
    ]  0.043165526 --|x d4v1 vmentry cycles 1472
    ]  0.043166800 --|x d4v1 vmexit exit_reason PENDING_INTERRUPT eip ffffffff81673254
       0.043166800 --|x d4v1 inj_virq vec 243  real
      [vla] d4v1 vec 243 injecting
    
    there is a pending event (subsequent debugging shows it is the IPI
    from the VCPU0 when smpboot.c on VCPU1 has done
    "set_cpu_online(smp_processor_id(), true)") and the guest VCPU1 is
    interrupted with the callback IPI (0xf3 aka 243) which ends up calling
    __xen_evtchn_do_upcall.
    
    The __xen_evtchn_do_upcall seems to do *something* but not acknowledge
    the pending events. And the moment the guest does a 'cli' (that is the
    ffffffff81673254 in the log above) the hypervisor is invoked again to
    inject the IPI (0xf3) to tell the guest it has pending interrupts.
    This repeats itself forever.
    
    The culprit was the per_cpu(xen_vcpu, cpu) pointer. At the bootup
    we set each per_cpu(xen_vcpu, cpu) to point to the
    shared_info->vcpu_info[vcpu] but later on use the VCPUOP_register_vcpu_info
    to register per-CPU  structures (xen_vcpu_setup).
    This is used to allow events for more than 32 VCPUs and for performance
    optimizations reasons.
    
    When the user performs the VCPU hotplug we end up calling the
    the xen_vcpu_setup once more. We make the hypercall which returns
    -EINVAL as it does not allow multiple registration calls (and
    already has re-assigned where the events are being set). We pick
    the fallback case and set per_cpu(xen_vcpu, cpu) to point to the
    shared_info->vcpu_info[vcpu] (which is a good fallback during bootup).
    However the hypervisor is still setting events in the register
    per-cpu structure (per_cpu(xen_vcpu_info, cpu)).
    
    As such when the events are set by the hypervisor (such as timer one),
    and when we iterate in __xen_evtchn_do_upcall we end up reading stale
    events from the shared_info->vcpu_info[vcpu] instead of the
    per_cpu(xen_vcpu_info, cpu) structures. Hence we never acknowledge the
    events that the hypervisor has set and the hypervisor keeps on reminding
    us to ack the events which we never do.
    
    The fix is simple. Don't on the second time when xen_vcpu_setup is
    called over-write the per_cpu(xen_vcpu, cpu) if it points to
    per_cpu(xen_vcpu_info).
    
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit a66b2e503fc79fff6632d02ef5a0ee47c1d2553d
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Wed May 15 21:47:17 2013 +0200

    cpufreq: Preserve sysfs files across suspend/resume
    
    The file permissions of cpufreq per-cpu sysfs files are not preserved
    across suspend/resume because we internally go through the CPU
    Hotplug path which reinitializes the file permissions on CPU online.
    
    But the user is not supposed to know that we are using CPU hotplug
    internally within suspend/resume (IOW, the kernel should not silently
    wreck the user-set file permissions across a suspend cycle).
    Therefore, we need to preserve the file permissions as they are
    across suspend/resume.
    
    The simplest way to achieve that is to just not touch the sysfs files
    at all - ie., just ignore the CPU hotplug notifications in the
    suspend/resume path (_FROZEN) in the cpufreq hotplug callback.
    
    Reported-by: Robert Jarzmik <robert.jarzmik@intel.com>
    Reported-by: Durgadoss R <durgadoss.r@intel.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 0bd5ff02f874986f30455d41222d53bd1846aa0e
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Apr 16 15:18:00 2013 -0400

    xen/time: Fix kasprintf splat when allocating timer%d IRQ line.
    
    commit 7918c92ae9638eb8a6ec18e2b4a0de84557cccc8 upstream.
    
    When we online the CPU, we get this splat:
    
    smpboot: Booting Node 0 Processor 1 APIC 0x2
    installing Xen timer for CPU 1
    BUG: sleeping function called from invalid context at /home/konrad/ssd/konrad/linux/mm/slab.c:3179
    in_atomic(): 1, irqs_disabled(): 0, pid: 0, name: swapper/1
    Pid: 0, comm: swapper/1 Not tainted 3.9.0-rc6upstream-00001-g3884fad #1
    Call Trace:
     [<ffffffff810c1fea>] __might_sleep+0xda/0x100
     [<ffffffff81194617>] __kmalloc_track_caller+0x1e7/0x2c0
     [<ffffffff81303758>] ? kasprintf+0x38/0x40
     [<ffffffff813036eb>] kvasprintf+0x5b/0x90
     [<ffffffff81303758>] kasprintf+0x38/0x40
     [<ffffffff81044510>] xen_setup_timer+0x30/0xb0
     [<ffffffff810445af>] xen_hvm_setup_cpu_clockevents+0x1f/0x30
     [<ffffffff81666d0a>] start_secondary+0x19c/0x1a8
    
    The solution to that is use kasprintf in the CPU hotplug path
    that 'online's the CPU. That is, do it in in xen_hvm_cpu_notify,
    and remove the call to in xen_hvm_setup_cpu_clockevents.
    
    Unfortunatly the later is not a good idea as the bootup path
    does not use xen_hvm_cpu_notify so we would end up never allocating
    timer%d interrupt lines when booting. As such add the check for
    atomic() to continue.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 0902a9044fa5b7a0456ea4daacec2c2b3189ba8c
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri May 3 00:25:49 2013 +0200

    Driver core: Use generic offline/online for CPU offline/online
    
    Rework the CPU hotplug code in drivers/base/cpu.c to use the
    generic offline/online support introduced previously instead of
    its own CPU-specific code.
    
    For this purpose, modify cpu_subsys to provide offline and online
    callbacks for CONFIG_HOTPLUG_CPU set and remove the code handling
    the CPU-specific 'online' sysfs attribute.
    
    This modification is not supposed to change the user-observable
    behavior of the kernel (i.e. the 'online' attribute will be present
    in exactly the same place in sysfs and should trigger exactly the
    same actions as before).
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: Toshi Kani <toshi.kani@hp.com>

commit 3672f12220a0994d9e37d6e2bb93ac7ec1669d60
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Apr 16 15:18:00 2013 -0400

    xen/time: Fix kasprintf splat when allocating timer%d IRQ line.
    
    commit 7918c92ae9638eb8a6ec18e2b4a0de84557cccc8 upstream.
    
    When we online the CPU, we get this splat:
    
    smpboot: Booting Node 0 Processor 1 APIC 0x2
    installing Xen timer for CPU 1
    BUG: sleeping function called from invalid context at /home/konrad/ssd/konrad/linux/mm/slab.c:3179
    in_atomic(): 1, irqs_disabled(): 0, pid: 0, name: swapper/1
    Pid: 0, comm: swapper/1 Not tainted 3.9.0-rc6upstream-00001-g3884fad #1
    Call Trace:
     [<ffffffff810c1fea>] __might_sleep+0xda/0x100
     [<ffffffff81194617>] __kmalloc_track_caller+0x1e7/0x2c0
     [<ffffffff81303758>] ? kasprintf+0x38/0x40
     [<ffffffff813036eb>] kvasprintf+0x5b/0x90
     [<ffffffff81303758>] kasprintf+0x38/0x40
     [<ffffffff81044510>] xen_setup_timer+0x30/0xb0
     [<ffffffff810445af>] xen_hvm_setup_cpu_clockevents+0x1f/0x30
     [<ffffffff81666d0a>] start_secondary+0x19c/0x1a8
    
    The solution to that is use kasprintf in the CPU hotplug path
    that 'online's the CPU. That is, do it in in xen_hvm_cpu_notify,
    and remove the call to in xen_hvm_setup_cpu_clockevents.
    
    Unfortunatly the later is not a good idea as the bootup path
    does not use xen_hvm_cpu_notify so we would end up never allocating
    timer%d interrupt lines when booting. As such add the check for
    atomic() to continue.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 037f6cd3f80e2777b758839055faaba00aab6e3d
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Apr 16 15:18:00 2013 -0400

    xen/time: Fix kasprintf splat when allocating timer%d IRQ line.
    
    commit 7918c92ae9638eb8a6ec18e2b4a0de84557cccc8 upstream.
    
    When we online the CPU, we get this splat:
    
    smpboot: Booting Node 0 Processor 1 APIC 0x2
    installing Xen timer for CPU 1
    BUG: sleeping function called from invalid context at /home/konrad/ssd/konrad/linux/mm/slab.c:3179
    in_atomic(): 1, irqs_disabled(): 0, pid: 0, name: swapper/1
    Pid: 0, comm: swapper/1 Not tainted 3.9.0-rc6upstream-00001-g3884fad #1
    Call Trace:
     [<ffffffff810c1fea>] __might_sleep+0xda/0x100
     [<ffffffff81194617>] __kmalloc_track_caller+0x1e7/0x2c0
     [<ffffffff81303758>] ? kasprintf+0x38/0x40
     [<ffffffff813036eb>] kvasprintf+0x5b/0x90
     [<ffffffff81303758>] kasprintf+0x38/0x40
     [<ffffffff81044510>] xen_setup_timer+0x30/0xb0
     [<ffffffff810445af>] xen_hvm_setup_cpu_clockevents+0x1f/0x30
     [<ffffffff81666d0a>] start_secondary+0x19c/0x1a8
    
    The solution to that is use kasprintf in the CPU hotplug path
    that 'online's the CPU. That is, do it in in xen_hvm_cpu_notify,
    and remove the call to in xen_hvm_setup_cpu_clockevents.
    
    Unfortunatly the later is not a good idea as the bootup path
    does not use xen_hvm_cpu_notify so we would end up never allocating
    timer%d interrupt lines when booting. As such add the check for
    atomic() to continue.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit f7cfcd277732f50bbdaf56880546faddbb2a73ba
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Apr 16 15:18:00 2013 -0400

    xen/time: Fix kasprintf splat when allocating timer%d IRQ line.
    
    commit 7918c92ae9638eb8a6ec18e2b4a0de84557cccc8 upstream.
    
    When we online the CPU, we get this splat:
    
    smpboot: Booting Node 0 Processor 1 APIC 0x2
    installing Xen timer for CPU 1
    BUG: sleeping function called from invalid context at /home/konrad/ssd/konrad/linux/mm/slab.c:3179
    in_atomic(): 1, irqs_disabled(): 0, pid: 0, name: swapper/1
    Pid: 0, comm: swapper/1 Not tainted 3.9.0-rc6upstream-00001-g3884fad #1
    Call Trace:
     [<ffffffff810c1fea>] __might_sleep+0xda/0x100
     [<ffffffff81194617>] __kmalloc_track_caller+0x1e7/0x2c0
     [<ffffffff81303758>] ? kasprintf+0x38/0x40
     [<ffffffff813036eb>] kvasprintf+0x5b/0x90
     [<ffffffff81303758>] kasprintf+0x38/0x40
     [<ffffffff81044510>] xen_setup_timer+0x30/0xb0
     [<ffffffff810445af>] xen_hvm_setup_cpu_clockevents+0x1f/0x30
     [<ffffffff81666d0a>] start_secondary+0x19c/0x1a8
    
    The solution to that is use kasprintf in the CPU hotplug path
    that 'online's the CPU. That is, do it in in xen_hvm_cpu_notify,
    and remove the call to in xen_hvm_setup_cpu_clockevents.
    
    Unfortunatly the later is not a good idea as the bootup path
    does not use xen_hvm_cpu_notify so we would end up never allocating
    timer%d interrupt lines when booting. As such add the check for
    atomic() to continue.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0c6ad85215e151e96a0af63f7012d6642c23e3bd
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Apr 16 15:18:00 2013 -0400

    xen/time: Fix kasprintf splat when allocating timer%d IRQ line.
    
    commit 7918c92ae9638eb8a6ec18e2b4a0de84557cccc8 upstream.
    
    When we online the CPU, we get this splat:
    
    smpboot: Booting Node 0 Processor 1 APIC 0x2
    installing Xen timer for CPU 1
    BUG: sleeping function called from invalid context at /home/konrad/ssd/konrad/linux/mm/slab.c:3179
    in_atomic(): 1, irqs_disabled(): 0, pid: 0, name: swapper/1
    Pid: 0, comm: swapper/1 Not tainted 3.9.0-rc6upstream-00001-g3884fad #1
    Call Trace:
     [<ffffffff810c1fea>] __might_sleep+0xda/0x100
     [<ffffffff81194617>] __kmalloc_track_caller+0x1e7/0x2c0
     [<ffffffff81303758>] ? kasprintf+0x38/0x40
     [<ffffffff813036eb>] kvasprintf+0x5b/0x90
     [<ffffffff81303758>] kasprintf+0x38/0x40
     [<ffffffff81044510>] xen_setup_timer+0x30/0xb0
     [<ffffffff810445af>] xen_hvm_setup_cpu_clockevents+0x1f/0x30
     [<ffffffff81666d0a>] start_secondary+0x19c/0x1a8
    
    The solution to that is use kasprintf in the CPU hotplug path
    that 'online's the CPU. That is, do it in in xen_hvm_cpu_notify,
    and remove the call to in xen_hvm_setup_cpu_clockevents.
    
    Unfortunatly the later is not a good idea as the bootup path
    does not use xen_hvm_cpu_notify so we would end up never allocating
    timer%d interrupt lines when booting. As such add the check for
    atomic() to continue.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 224786779d04bbcd5f61eaafc86bf8fee350388a
Author: Huacai Chen <chenhc@lemote.com>
Date:   Sun Mar 17 11:50:14 2013 +0000

    MIPS: Init new mmu_context for each possible CPU to avoid memory corruption
    
    Currently, init_new_context() only for each online CPU, this may cause
    memory corruption when CPU hotplug and fork() happens at the same time.
    To avoid this, we make init_new_context() cover each possible CPU.
    
    Scenario:
    1, CPU#1 is being offline;
    2, On CPU#0, do_fork() call dup_mm() and copy a mm_struct to the child;
    3, On CPU#0, dup_mm() call init_new_context(), since CPU#1 is offline
       and init_new_context() only covers the online CPUs, child has the
       same asid as its parent on CPU#1 (however, child's asid should be 0);
    4, CPU#1 is being online;
    5, Now, if both parent and child run on CPU#1, memory corruption (e.g.
       segfault, bus error, etc.) will occur.
    
    Signed-off-by: Huacai Chen <chenhc@lemote.com>
    Acked-by: David Daney <david.daney@cavium.com>
    Patchwork: http://patchwork.linux-mips.org/patch/4995/
    Acked-by: John Crispin <blogic@openwrt.org>

commit 8759934e2b6bdb3a08a81fc14a6588f3321719b1
Author: Huacai Chen <chenhc@lemote.com>
Date:   Sun Mar 17 11:49:38 2013 +0000

    MIPS: Build uasm-generated code only once to avoid CPU Hotplug problem
    
    This and the next patch resolve memory corruption problems while CPU
    hotplug. Without these patches, memory corruption can triggered easily
    as below:
    
    On a quad-core MIPS platform, use "spawn" of UnixBench-5.1.3 (http://
    code.google.com/p/byte-unixbench/) and a CPU hotplug script like this
    (hotplug.sh):
    while true; do
    echo 0 >/sys/devices/system/cpu/cpu1/online
    echo 0 >/sys/devices/system/cpu/cpu2/online
    echo 0 >/sys/devices/system/cpu/cpu3/online
    sleep 1
    echo 1 >/sys/devices/system/cpu/cpu1/online
    echo 1 >/sys/devices/system/cpu/cpu2/online
    echo 1 >/sys/devices/system/cpu/cpu3/online
    sleep 1
    done
    
    Run "hotplug.sh" and then run "spawn 10000", spawn will get segfault
    after a few minutes.
    
    This patch:
    Currently, clear_page()/copy_page() are generated by Micro-assembler
    dynamically. But they are unavailable until uasm_resolve_relocs() has
    finished because jump labels are illegal before that. Since these
    functions are shared by every CPU, we only call build_clear_page()/
    build_copy_page() only once at boot time. Without this patch, programs
    will get random memory corruption (segmentation fault, bus error, etc.)
    while CPU Hotplug (e.g. one CPU is using clear_page() while another is
    generating it in cpu_cache_init()).
    
    For similar reasons we modify build_tlb_refill_handler()'s invocation.
    
    V2:
    1, Rework the code to make CPU#0 can be online/offline.
    2, Introduce cpu_has_local_ebase feature since some types of MIPS CPU
       need a per-CPU tlb_refill_handler().
    
    Signed-off-by: Huacai Chen <chenhc@lemote.com>
    Signed-off-by: Hongbing Hu <huhb@lemote.com>
    Acked-by: David Daney <david.daney@cavium.com>
    Patchwork: http://patchwork.linux-mips.org/patch/4994/
    Acked-by: John Crispin <blogic@openwrt.org>

commit 7f1fc268c47491fd5e63548f6415fc8604e13003
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Sun May 5 09:30:09 2013 -0400

    xen/vcpu/pvhvm: Fix vcpu hotplugging hanging.
    
    If a user did:
    
            echo 0 > /sys/devices/system/cpu/cpu1/online
            echo 1 > /sys/devices/system/cpu/cpu1/online
    
    we would (this a build with DEBUG enabled) get to:
    smpboot: ++++++++++++++++++++=_---CPU UP  1
    .. snip..
    smpboot: Stack at about ffff880074c0ff44
    smpboot: CPU1: has booted.
    
    and hang. The RCU mechanism would kick in an try to IPI the CPU1
    but the IPIs (and all other interrupts) would never arrive at the
    CPU1. At first glance at least. A bit digging in the hypervisor
    trace shows that (using xenanalyze):
    
    [vla] d4v1 vec 243 injecting
       0.043163027 --|x d4v1 intr_window vec 243 src 5(vector) intr f3
    ]  0.043163639 --|x d4v1 vmentry cycles 1468
    ]  0.043164913 --|x d4v1 vmexit exit_reason PENDING_INTERRUPT eip ffffffff81673254
       0.043164913 --|x d4v1 inj_virq vec 243  real
      [vla] d4v1 vec 243 injecting
       0.043164913 --|x d4v1 intr_window vec 243 src 5(vector) intr f3
    ]  0.043165526 --|x d4v1 vmentry cycles 1472
    ]  0.043166800 --|x d4v1 vmexit exit_reason PENDING_INTERRUPT eip ffffffff81673254
       0.043166800 --|x d4v1 inj_virq vec 243  real
      [vla] d4v1 vec 243 injecting
    
    there is a pending event (subsequent debugging shows it is the IPI
    from the VCPU0 when smpboot.c on VCPU1 has done
    "set_cpu_online(smp_processor_id(), true)") and the guest VCPU1 is
    interrupted with the callback IPI (0xf3 aka 243) which ends up calling
    __xen_evtchn_do_upcall.
    
    The __xen_evtchn_do_upcall seems to do *something* but not acknowledge
    the pending events. And the moment the guest does a 'cli' (that is the
    ffffffff81673254 in the log above) the hypervisor is invoked again to
    inject the IPI (0xf3) to tell the guest it has pending interrupts.
    This repeats itself forever.
    
    The culprit was the per_cpu(xen_vcpu, cpu) pointer. At the bootup
    we set each per_cpu(xen_vcpu, cpu) to point to the
    shared_info->vcpu_info[vcpu] but later on use the VCPUOP_register_vcpu_info
    to register per-CPU  structures (xen_vcpu_setup).
    This is used to allow events for more than 32 VCPUs and for performance
    optimizations reasons.
    
    When the user performs the VCPU hotplug we end up calling the
    the xen_vcpu_setup once more. We make the hypercall which returns
    -EINVAL as it does not allow multiple registration calls (and
    already has re-assigned where the events are being set). We pick
    the fallback case and set per_cpu(xen_vcpu, cpu) to point to the
    shared_info->vcpu_info[vcpu] (which is a good fallback during bootup).
    However the hypervisor is still setting events in the register
    per-cpu structure (per_cpu(xen_vcpu_info, cpu)).
    
    As such when the events are set by the hypervisor (such as timer one),
    and when we iterate in __xen_evtchn_do_upcall we end up reading stale
    events from the shared_info->vcpu_info[vcpu] instead of the
    per_cpu(xen_vcpu_info, cpu) structures. Hence we never acknowledge the
    events that the hypervisor has set and the hypervisor keeps on reminding
    us to ack the events which we never do.
    
    The fix is simple. Don't on the second time when xen_vcpu_setup is
    called over-write the per_cpu(xen_vcpu, cpu) if it points to
    per_cpu(xen_vcpu_info).
    
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    CC: stable@vger.kernel.org
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit 5a5a1bf099d6942399ea0b34a62e5f0bc4c5c36e
Merge: 74c7d2f5200a 5379f8c0d72c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 08:42:45 2013 -0700

    Merge branch 'x86-ras-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 RAS changes from Ingo Molnar:
    
     - Add an Intel CMCI hotplug fix
    
     - Add AMD family 16h EDAC support
    
     - Make the AMD MCE banks code more flexible for virtual environments
    
    * 'x86-ras-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      amd64_edac: Add Family 16h support
      x86/mce: Rework cmci_rediscover() to play well with CPU hotplug
      x86, MCE, AMD: Use MCG_CAP MSR to find out number of banks on AMD
      x86, MCE, AMD: Replace shared_bank array with is_shared_bank() helper

commit 46d9be3e5eb01f71fc02653755d970247174b400
Merge: ce8aa4892944 cece95dfe5aa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 29 19:07:40 2013 -0700

    Merge branch 'for-3.10' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    Pull workqueue updates from Tejun Heo:
     "A lot of activities on workqueue side this time.  The changes achieve
      the followings.
    
       - WQ_UNBOUND workqueues - the workqueues which are per-cpu - are
         updated to be able to interface with multiple backend worker pools.
         This involved a lot of churning but the end result seems actually
         neater as unbound workqueues are now a lot closer to per-cpu ones.
    
       - The ability to interface with multiple backend worker pools are
         used to implement unbound workqueues with custom attributes.
         Currently the supported attributes are the nice level and CPU
         affinity.  It may be expanded to include cgroup association in
         future.  The attributes can be specified either by calling
         apply_workqueue_attrs() or through /sys/bus/workqueue/WQ_NAME/* if
         the workqueue in question is exported through sysfs.
    
         The backend worker pools are keyed by the actual attributes and
         shared by any workqueues which share the same attributes.  When
         attributes of a workqueue are changed, the workqueue binds to the
         worker pool with the specified attributes while leaving the work
         items which are already executing in its previous worker pools
         alone.
    
         This allows converting custom worker pool implementations which
         want worker attribute tuning to use workqueues.  The writeback pool
         is already converted in block tree and there are a couple others
         are likely to follow including btrfs io workers.
    
       - WQ_UNBOUND's ability to bind to multiple worker pools is also used
         to make it NUMA-aware.  Because there's no association between work
         item issuer and the specific worker assigned to execute it, before
         this change, using unbound workqueue led to unnecessary cross-node
         bouncing and it couldn't be helped by autonuma as it requires tasks
         to have implicit node affinity and workers are assigned randomly.
    
         After these changes, an unbound workqueue now binds to multiple
         NUMA-affine worker pools so that queued work items are executed in
         the same node.  This is turned on by default but can be disabled
         system-wide or for individual workqueues.
    
         Crypto was requesting NUMA affinity as encrypting data across
         different nodes can contribute noticeable overhead and doing it
         per-cpu was too limiting for certain cases and IO throughput could
         be bottlenecked by one CPU being fully occupied while others have
         idle cycles.
    
      While the new features required a lot of changes including
      restructuring locking, it didn't complicate the execution paths much.
      The unbound workqueue handling is now closer to per-cpu ones and the
      new features are implemented by simply associating a workqueue with
      different sets of backend worker pools without changing queue,
      execution or flush paths.
    
      As such, even though the amount of change is very high, I feel
      relatively safe in that it isn't likely to cause subtle issues with
      basic correctness of work item execution and handling.  If something
      is wrong, it's likely to show up as being associated with worker pools
      with the wrong attributes or OOPS while workqueue attributes are being
      changed or during CPU hotplug.
    
      While this creates more backend worker pools, it doesn't add too many
      more workers unless, of course, there are many workqueues with unique
      combinations of attributes.  Assuming everything else is the same,
      NUMA awareness costs an extra worker pool per NUMA node with online
      CPUs.
    
      There are also a couple things which are being routed outside the
      workqueue tree.
    
       - block tree pulled in workqueue for-3.10 so that writeback worker
         pool can be converted to unbound workqueue with sysfs control
         exposed.  This simplifies the code, makes writeback workers
         NUMA-aware and allows tuning nice level and CPU affinity via sysfs.
    
       - The conversion to workqueue means that there's no 1:1 association
         between a specific worker, which makes writeback folks unhappy as
         they want to be able to tell which filesystem caused a problem from
         backtrace on systems with many filesystems mounted.  This is
         resolved by allowing work items to set debug info string which is
         printed when the task is dumped.  As this change involves unifying
         implementations of dump_stack() and friends in arch codes, it's
         being routed through Andrew's -mm tree."
    
    * 'for-3.10' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq: (84 commits)
      workqueue: use kmem_cache_free() instead of kfree()
      workqueue: avoid false negative WARN_ON() in destroy_workqueue()
      workqueue: update sysfs interface to reflect NUMA awareness and a kernel param to disable NUMA affinity
      workqueue: implement NUMA affinity for unbound workqueues
      workqueue: introduce put_pwq_unlocked()
      workqueue: introduce numa_pwq_tbl_install()
      workqueue: use NUMA-aware allocation for pool_workqueues
      workqueue: break init_and_link_pwq() into two functions and introduce alloc_unbound_pwq()
      workqueue: map an unbound workqueues to multiple per-node pool_workqueues
      workqueue: move hot fields of workqueue_struct to the end
      workqueue: make workqueue->name[] fixed len
      workqueue: add workqueue->unbound_attrs
      workqueue: determine NUMA node of workers accourding to the allowed cpumask
      workqueue: drop 'H' from kworker names of unbound worker pools
      workqueue: add wq_numa_tbl_len and wq_numa_possible_cpumask[]
      workqueue: move pwq_pool_locking outside of get/put_unbound_pool()
      workqueue: fix memory leak in apply_workqueue_attrs()
      workqueue: fix unbound workqueue attrs hashing / comparison
      workqueue: fix race condition in unbound workqueue free path
      workqueue: remove pwq_lock which is no longer used
      ...

commit 9d2da7af909e1cf529f3cac582aaae05b107aa1e
Merge: c1be5a5b1b35 18c0025b692a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 29 08:16:51 2013 -0700

    Merge tag 'stable/for-linus-3.10-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull Xen updates from Konrad Rzeszutek Wilk:
     "Features:
       - Populate the boot_params with EDD data.
       - Cleanups in the IRQ code.
      Bug-fixes:
       - CPU hotplug offline/online in PVHVM mode.
       - Re-upload processor PM data after ACPI S3 suspend/resume cycle."
    
    And Konrad gets a gold star for sending the pull request early when he
    thought he'd be away for the first week of the merge window (but because
    of 3.9 dragging out to -rc8 he then re-sent the reminder on the first
    day of the merge window anyway)
    
    * tag 'stable/for-linus-3.10-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen: resolve section mismatch warnings in xen-acpi-processor
      xen: Re-upload processor PM data to hypervisor after S3 resume (v2)
      xen/smp: Unifiy some of the PVs and PVHVM offline CPU path
      xen/smp/pvhvm: Don't initialize IRQ_WORKER as we are using the native one.
      xen/spinlock: Disable IRQ spinlock (PV) allocation on PVHVM
      xen/spinlock:  Check against default value of -1 for IRQ line.
      xen/time: Add default value of -1 for IRQ and check for that.
      xen/events: Check that IRQ value passed in is valid.
      xen/time: Fix kasprintf splat when allocating timer%d IRQ line.
      xen/smp/spinlock: Fix leakage of the spinlock interrupt line for every CPU online/offline
      xen/smp: Fix leakage of timer interrupt line for every CPU online/offline.
      xen kconfig: fix select INPUT_XEN_KBDDEV_FRONTEND
      xen: drop tracking of IRQ vector
      x86/xen: populate boot_params with EDD data

commit 5a677ce044f18a341ab942e23516e52ad89f7687
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Apr 12 19:12:06 2013 +0100

    ARM: KVM: switch to a dual-step HYP init code
    
    Our HYP init code suffers from two major design issues:
    - it cannot support CPU hotplug, as we tear down the idmap very early
    - it cannot perform a TLB invalidation when switching from init to
      runtime mappings, as pages are manipulated from PL1 exclusively
    
    The hotplug problem mandates that we keep two sets of page tables
    (boot and runtime). The TLB problem mandates that we're able to
    transition from one PGD to another while in HYP, invalidating the TLBs
    in the process.
    
    To be able to do this, we need to share a page between the two page
    tables. A page that will have the same VA in both configurations. All we
    need is a VA that has the following properties:
    - This VA can't be used to represent a kernel mapping.
    - This VA will not conflict with the physical address of the kernel text
    
    The vectors page seems to satisfy this requirement:
    - The kernel never maps anything else there
    - The kernel text being copied at the beginning of the physical memory,
      it is unlikely to use the last 64kB (I doubt we'll ever support KVM
      on a system with something like 4MB of RAM, but patches are very
      welcome).
    
    Let's call this VA the trampoline VA.
    
    Now, we map our init page at 3 locations:
    - idmap in the boot pgd
    - trampoline VA in the boot pgd
    - trampoline VA in the runtime pgd
    
    The init scenario is now the following:
    - We jump in HYP with four parameters: boot HYP pgd, runtime HYP pgd,
      runtime stack, runtime vectors
    - Enable the MMU with the boot pgd
    - Jump to a target into the trampoline page (remember, this is the same
      physical page!)
    - Now switch to the runtime pgd (same VA, and still the same physical
      page!)
    - Invalidate TLBs
    - Set stack and vectors
    - Profit! (or eret, if you only care about the code).
    
    Note that we keep the boot mapping permanently (it is not strictly an
    idmap anymore) to allow for CPU hotplug in later patches.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@cs.columbia.edu>

commit 9ff221bad8869f73141c6a3c187afe2e933c991f
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Mon Nov 19 16:02:17 2012 -0500

    ARM: mcpm: generic SMP secondary bringup and hotplug support
    
    Now that the cluster power API is in place, we can use it for SMP secondary
    bringup and CPU hotplug in a generic fashion.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>

commit 51e0eaf98d48dc6fc385d1aee9e208afe3ce0cf1
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue Apr 23 02:14:51 2013 +0000

    powerpc/cell: Only iterate over online nodes in cbe_init_pm_irq()
    
    None of the cell platforms support CPU hotplug, so we should iterate
    only over online nodes when setting PMU interrupts.
    
    This also fixes a warning during boot when NODES_SHIFT is large enough:
    
    WARNING: at /scratch/michael/src/kmk/linus/kernel/irq/irqdomain.c:766
    ...
    NIP [c0000000000db278] .irq_linear_revmap+0x30/0x58
    LR [c0000000000dc2a0] .irq_create_mapping+0x38/0x1a8
    Call Trace:
    [c0000003fc9c3af0] [c0000000000dc2a0] .irq_create_mapping+0x38/0x1a8 (unreliable)
    [c0000003fc9c3b80] [c000000000655c1c] .__machine_initcall_cell_cbe_init_pm_irq+0x84/0x158
    [c0000003fc9c3c20] [c00000000000afb4] .do_one_initcall+0x5c/0x1e0
    [c0000003fc9c3cd0] [c000000000644580] .kernel_init_freeable+0x238/0x328
    [c0000003fc9c3db0] [c00000000000b784] .kernel_init+0x1c/0x120
    [c0000003fc9c3e30] [c000000000009fb8] .ret_from_kernel_thread+0x64/0xac
    
    This is caused by us overflowing our linear revmap because we're
    requesting too many interrupts.
    
    Reported-by: Dennis Schridde <devurandom@gmx.net>
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 7918c92ae9638eb8a6ec18e2b4a0de84557cccc8
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Apr 16 15:18:00 2013 -0400

    xen/time: Fix kasprintf splat when allocating timer%d IRQ line.
    
    When we online the CPU, we get this splat:
    
    smpboot: Booting Node 0 Processor 1 APIC 0x2
    installing Xen timer for CPU 1
    BUG: sleeping function called from invalid context at /home/konrad/ssd/konrad/linux/mm/slab.c:3179
    in_atomic(): 1, irqs_disabled(): 0, pid: 0, name: swapper/1
    Pid: 0, comm: swapper/1 Not tainted 3.9.0-rc6upstream-00001-g3884fad #1
    Call Trace:
     [<ffffffff810c1fea>] __might_sleep+0xda/0x100
     [<ffffffff81194617>] __kmalloc_track_caller+0x1e7/0x2c0
     [<ffffffff81303758>] ? kasprintf+0x38/0x40
     [<ffffffff813036eb>] kvasprintf+0x5b/0x90
     [<ffffffff81303758>] kasprintf+0x38/0x40
     [<ffffffff81044510>] xen_setup_timer+0x30/0xb0
     [<ffffffff810445af>] xen_hvm_setup_cpu_clockevents+0x1f/0x30
     [<ffffffff81666d0a>] start_secondary+0x19c/0x1a8
    
    The solution to that is use kasprintf in the CPU hotplug path
    that 'online's the CPU. That is, do it in in xen_hvm_cpu_notify,
    and remove the call to in xen_hvm_setup_cpu_clockevents.
    
    Unfortunatly the later is not a good idea as the bootup path
    does not use xen_hvm_cpu_notify so we would end up never allocating
    timer%d interrupt lines when booting. As such add the check for
    atomic() to continue.
    
    CC: stable@vger.kernel.org
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit 7a0c819d28f5c91955854e048766d6afef7c8a3d
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Wed Mar 20 15:31:29 2013 +0530

    x86/mce: Rework cmci_rediscover() to play well with CPU hotplug
    
    Dave Jones reports that offlining a CPU leads to this trace:
    
    numa_remove_cpu cpu 1 node 0: mask now 0,2-3
    smpboot: CPU 1 is now offline
    BUG: using smp_processor_id() in preemptible [00000000] code:
    cpu-offline.sh/10591
    caller is cmci_rediscover+0x6a/0xe0
    Pid: 10591, comm: cpu-offline.sh Not tainted 3.9.0-rc3+ #2
    Call Trace:
     [<ffffffff81333bbd>] debug_smp_processor_id+0xdd/0x100
     [<ffffffff8101edba>] cmci_rediscover+0x6a/0xe0
     [<ffffffff815f5b9f>] mce_cpu_callback+0x19d/0x1ae
     [<ffffffff8160ea66>] notifier_call_chain+0x66/0x150
     [<ffffffff8107ad7e>] __raw_notifier_call_chain+0xe/0x10
     [<ffffffff8104c2e3>] cpu_notify+0x23/0x50
     [<ffffffff8104c31e>] cpu_notify_nofail+0xe/0x20
     [<ffffffff815ef082>] _cpu_down+0x302/0x350
     [<ffffffff815ef106>] cpu_down+0x36/0x50
     [<ffffffff815f1c9d>] store_online+0x8d/0xd0
     [<ffffffff813edc48>] dev_attr_store+0x18/0x30
     [<ffffffff81226eeb>] sysfs_write_file+0xdb/0x150
     [<ffffffff811adfb2>] vfs_write+0xa2/0x170
     [<ffffffff811ae16c>] sys_write+0x4c/0xa0
     [<ffffffff81613019>] system_call_fastpath+0x16/0x1b
    
    However, a look at cmci_rediscover shows that it can be simplified quite
    a bit, apart from solving the above issue. It invokes functions that
    take spin locks with interrupts disabled, and hence it can run in atomic
    context. Also, it is run in the CPU_POST_DEAD phase, so the dying CPU
    is already dead and out of the cpu_online_mask. So take these points into
    account and simplify the code, and thereby also fix the above issue.
    
    Reported-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>

commit 118c9a45fdacc6fe57910fa1d048e2d5bbc193f4
Merge: f8e9248dbb2b 06d1d8c85700
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 2 08:35:03 2013 -0700

    Merge tag 'fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull ARM SoC bug fixes from Arnd Bergmann:
     "After a quiet set of fixes for 3.9-rc4, a lot of people woke up and
      sent urgent fixes for 3.9.  I pushed back on a number of them that got
      deferred to 3.10, but these are the ones that seemed important.
    
      Regression in 3.9:
    
       - Multiple regressions in OMAP2+ clock cleanup
       - SH-Mobile frame buffer bug fix that merged here because of
         maintainer MIA
       - ux500 prcmu changes broke DT booting
       - MMCI duplicated regulator setup on ux500
       - New ux500 clock driver broke ethernet on snowball
       - Local interrupt driver for mvebu broke ethernet
       - MVEBU GPIO driver did not get set up right on Orion DT
       - incorrect interrupt number on Orion crypto for DT
    
      Long-standing bugs, including candidates for stable:
    
       - Kirkwood MMC needs to disable invalid card detect pins
       - MV SDIO pinmux was wrong on Mirabox
       - GoFlex Net board file needs to set NAND chip delay
       - MSM timer restart race
       - ep93xx early debug code broke in 3.7
       - i.MX CPU hotplug race
       - Incorrect clock setup for OMAP1 USB
       - Workaround for bad clock setup by some old OMAP4 boot loaders
       - Static I/O mappings on cns3xxx since 3.2"
    
    * tag 'fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc:
      ARM: cns3xxx: fix mapping of private memory region
      arm: mvebu: Fix pinctrl for Armada 370 Mirabox SDIO port.
      arm: orion5x: correct IRQ used in dtsi for mv_cesa
      arm: orion5x: fix orion5x.dtsi gpio parameters
      ARM: Kirkwood: fix unused mvsdio gpio pins
      arm: mvebu: Use local interrupt only for the timer 0
      ARM: kirkwood: Fix chip-delay for GoFlex Net
      ARM: ux500: Enable the clock controlling Ethernet on Snowball
      ARM: ux500: Stop passing ios_handler() as an MMCI power controlling call-back
      ARM: ux500: Apply the TCPM and TCDM locations and sizes to dbx5x0 DT
      fbdev: sh_mobile_lcdc: fixup B side hsync adjust settings
      ARM: OMAP: clocks: Delay clk inits atleast until slab is initialized
      ARM: imx: fix sync issue between imx_cpu_die and imx_cpu_kill
      ARM: msm: Stop counting before reprogramming clockevent
      ARM: ep93xx: Fix wait for UART FIFO to be empty
      ARM: OMAP4: PM: fix PM regression introduced by recent clock cleanup
      ARM: OMAP3: hwmod data: keep MIDLEMODE in force-standby for musb
      ARM: OMAP4: clock data: lock USB DPLL on boot
      ARM: OMAP1: fix USB host on 1710

commit 4c16bd327c74d6678858706211a0c6e4e53eb3e6
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Apr 1 11:23:36 2013 -0700

    workqueue: implement NUMA affinity for unbound workqueues
    
    Currently, an unbound workqueue has single current, or first, pwq
    (pool_workqueue) to which all new work items are queued.  This often
    isn't optimal on NUMA machines as workers may jump around across node
    boundaries and work items get assigned to workers without any regard
    to NUMA affinity.
    
    This patch implements NUMA affinity for unbound workqueues.  Instead
    of mapping all entries of numa_pwq_tbl[] to the same pwq,
    apply_workqueue_attrs() now creates a separate pwq covering the
    intersecting CPUs for each NUMA node which has online CPUs in
    @attrs->cpumask.  Nodes which don't have intersecting possible CPUs
    are mapped to pwqs covering whole @attrs->cpumask.
    
    As CPUs come up and go down, the pool association is changed
    accordingly.  Changing pool association may involve allocating new
    pools which may fail.  To avoid failing CPU_DOWN, each workqueue
    always keeps a default pwq which covers whole attrs->cpumask which is
    used as fallback if pool creation fails during a CPU hotplug
    operation.
    
    This ensures that all work items issued on a NUMA node is executed on
    the same node as long as the workqueue allows execution on the CPUs of
    the node.
    
    As this maps a workqueue to multiple pwqs and max_active is per-pwq,
    this change the behavior of max_active.  The limit is now per NUMA
    node instead of global.  While this is an actual change, max_active is
    already per-cpu for per-cpu workqueues and primarily used as safety
    mechanism rather than for active concurrency control.  Concurrency is
    usually limited from workqueue users by the number of concurrently
    active work items and this change shouldn't matter much.
    
    v2: Fixed pwq freeing in apply_workqueue_attrs() error path.  Spotted
        by Lai.
    
    v3: The previous version incorrectly made a workqueue spanning
        multiple nodes spread work items over all online CPUs when some of
        its nodes don't have any desired cpus.  Reimplemented so that NUMA
        affinity is properly updated as CPUs go up and down.  This problem
        was spotted by Lai Jiangshan.
    
    v4: destroy_workqueue() was putting wq->dfl_pwq and then clearing it;
        however, wq may be freed at any time after dfl_pwq is put making
        the clearing use-after-free.  Clear wq->dfl_pwq before putting it.
    
    v5: apply_workqueue_attrs() was leaking @tmp_attrs, @new_attrs and
        @pwq_tbl after success.  Fixed.
    
        Retry loop in wq_update_unbound_numa_attrs() isn't necessary as
        application of new attrs is excluded via CPU hotplug.  Removed.
    
        Documentation on CPU affinity guarantee on CPU_DOWN added.
    
        All changes are suggested by Lai Jiangshan.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit 76fc253723add627cf28c09c79fb67e71f9e4782
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Mar 22 10:15:47 2013 -0400

    xen/acpi-stub: Disable it b/c the acpi_processor_add is no longer called.
    
    With the Xen ACPI stub code (CONFIG_XEN_STUB=y) enabled, the power
    C and P states are no longer uploaded to the hypervisor.
    
    The reason is that the Xen CPU hotplug code: xen-acpi-cpuhotplug.c
    and the xen-acpi-stub.c register themselves as the "processor" type object.
    
    That means the generic processor (processor_driver.c) stops
    working and it does not call (acpi_processor_add) which populates the
    
             per_cpu(processors, pr->id) = pr;
    
    structure. The 'pr' is gathered from the acpi_processor_get_info function
    which does the job of finding the C-states and figuring out PBLK address.
    
    The 'processors->pr' is then later used by xen-acpi-processor.c (the one that
    uploads C and P states to the hypervisor). Since it is NULL, we end
    skip the gathering of _PSD, _PSS, _PCT, etc and never upload the power
    management data.
    
    The end result is that enabling the CONFIG_XEN_STUB in the build means that
    xen-acpi-processor is not working anymore.
    
    This temporary patch fixes it by marking the XEN_STUB driver as
    BROKEN until this can be properly fixed.
    
    CC: jinsong.liu@intel.com
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit bd7c089eb25b26d2e03fd34f97e5517a4463f871
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 19 13:45:21 2013 -0700

    workqueue: relocate rebind_workers()
    
    rebind_workers() will be reimplemented in a way which makes it mostly
    decoupled from the rest of worker management.  Move rebind_workers()
    so that it's located with other CPU hotplug related functions.
    
    This patch is pure function relocation.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>

commit bc3a1afc92aea46d6df18d38e5d15867b17c69f6
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 13 19:47:39 2013 -0700

    workqueue: rename worker_pool->assoc_mutex to ->manager_mutex
    
    Manager operations are currently governed by two mutexes -
    pool->manager_arb and ->assoc_mutex.  The former is used to decide who
    gets to be the manager and the latter to exclude the actual manager
    operations including creation and destruction of workers.  Anyone who
    grabs ->manager_arb must perform manager role; otherwise, the pool
    might stall.
    
    Grabbing ->assoc_mutex blocks everyone else from performing manager
    operations but doesn't require the holder to perform manager duties as
    it's merely blocking manager operations without becoming the manager.
    
    Because the blocking was necessary when [dis]associating per-cpu
    workqueues during CPU hotplug events, the latter was named
    assoc_mutex.  The mutex is scheduled to be used for other purposes, so
    this patch gives it a more fitting generic name - manager_mutex - and
    updates / adds comments to explain synchronization around the manager
    role and operations.
    
    This patch is pure rename / doc update.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit d62242d7f63d6c874f783d8a691534080df1cb59
Author: Magnus Damm <damm@opensource.se>
Date:   Wed Feb 13 00:44:57 2013 +0900

    ARM: shmobile: Remove partial CPU Hotplug from EMEV2
    
    Remove partial CPU hotplug support from EMEV2 SMP code.
    
    The upstream EMEV2 SMP support code has no CPU shutdown or
    reset ability so we cannot reboot the secondary CPU cores.
    
    Regular SMP operation is however still working as expected.
    
    Signed-off-by: Magnus Damm <damm@opensource.se>
    Signed-off-by: Simon Horman <horms+renesas@verge.net.au>

commit 8e8b180a5f1b237345f6e2d960bcceb8b6bc3793
Merge: 56a79b7b021b 45e27161c622
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 3 14:22:53 2013 -0800

    Merge tag 'stable/for-linus-3.9-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull Xen bug-fixes from Konrad Rzeszutek Wilk:
     - Update the Xen ACPI memory and CPU hotplug locking mechanism.
     - Fix PAT issues wherein various applications would not start
     - Fix handling of multiple MSI as AHCI now does it.
     - Fix ARM compile failures.
    
    * tag 'stable/for-linus-3.9-rc1-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xenbus: fix compile failure on ARM with Xen enabled
      xen/pci: We don't do multiple MSI's.
      xen/pat: Disable PAT using pat_enabled value.
      xen/acpi: xen cpu hotplug minor updates
      xen/acpi: xen memory hotplug minor updates

commit 77be36de8b07027a70fbc8f02703ccd76cd16d09
Merge: 89f883372fa6 c81611c4e96f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 24 16:06:13 2013 -0800

    Merge tag 'stable/for-linus-3.9-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    Pull Xen update from Konrad Rzeszutek Wilk:
     "This has two new ACPI drivers for Xen - a physical CPU offline/online
      and a memory hotplug.  The way this works is that ACPI kicks the
      drivers and they make the appropiate hypercall to the hypervisor to
      tell it that there is a new CPU or memory.  There also some changes to
      the Xen ARM ABIs and couple of fixes.  One particularly nasty bug in
      the Xen PV spinlock code was fixed by Stefan Bader - and has been
      there since the 2.6.32!
    
      Features:
       - Xen ACPI memory and CPU hotplug drivers - allowing Xen hypervisor
         to be aware of new CPU and new DIMMs
       - Cleanups
      Bug-fixes:
       - Fixes a long-standing bug in the PV spinlock wherein we did not
         kick VCPUs that were in a tight loop.
       - Fixes in the error paths for the event channel machinery"
    
    Fix up a few semantic conflicts with the ACPI interface changes in
    drivers/xen/xen-acpi-{cpu,mem}hotplug.c.
    
    * tag 'stable/for-linus-3.9-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      xen: event channel arrays are xen_ulong_t and not unsigned long
      xen: Send spinlock IPI to all waiters
      xen: introduce xen_remap, use it instead of ioremap
      xen: close evtchn port if binding to irq fails
      xen-evtchn: correct comment and error output
      xen/tmem: Add missing %s in the printk statement.
      xen/acpi: move xen_acpi_get_pxm under CONFIG_XEN_DOM0
      xen/acpi: ACPI cpu hotplug
      xen/acpi: Move xen_acpi_get_pxm to Xen's acpi.h
      xen/stub: driver for CPU hotplug
      xen/acpi: ACPI memory hotplug
      xen/stub: driver for memory hotplug
      xen: implement updated XENMEM_add_to_physmap_range ABI
      xen/smp: Move the common CPU init code a bit to prep for PVH patch.

commit 40a58637a4fa10a2faea71f0f30ff0b3d74c6e00
Author: Liu Jinsong <jinsong.liu@intel.com>
Date:   Fri Jan 25 15:42:31 2013 +0800

    xen/acpi: Move xen_acpi_get_pxm to Xen's acpi.h
    
    So that it could be reused by Xen CPU hotplug logic.
    
    Signed-off-by: Liu, Jinsong <jinsong.liu@intel.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit b22ff77b82598ff131d215627533e09e4a472220
Author: Liu Jinsong <jinsong.liu@intel.com>
Date:   Thu Jan 24 22:12:30 2013 +0800

    xen/stub: driver for CPU hotplug
    
    Add Xen stub driver for CPU hotplug, early occupy to block native,
    will be replaced later by real Xen processor driver module.
    
    Signed-off-by: Liu Jinsong <jinsong.liu@intel.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit 01ac5e342f3b87a9b83b991230d96c22c4167ec9
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Jan 21 19:36:16 2013 -0500

    ARM: KVM: VGIC initialisation code
    
    Add the init code for the hypervisor, the virtual machine, and
    the virtual CPUs.
    
    An interrupt handler is also wired to allow the VGIC maintenance
    interrupts, used to deal with level triggered interrupts and LR
    underflows.
    
    A CPU hotplug notifier is registered to disable/enable the interrupt
    as requested.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Christoffer Dall <c.dall@virtualopensystems.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

commit 72640d880350165c3a7b8f2e4fb4bf852ea7b606
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Jan 21 17:23:26 2013 +0000

    powerpc/pasemi: Fix crash on reboot
    
    commit f96972f2dc "kernel/sys.c: call disable_nonboot_cpus() in
    kernel_restart()"
    
    added a call to disable_nonboot_cpus() on kernel_restart(), which tries
    to shutdown all the CPUs except the first one. The issue with the PA
    Semi, is that it does not support CPU hotplug.
    
    When the call is made to __cpu_down(), it calls the notifiers
    CPU_DOWN_PREPARE, and then tries to take the CPU down.
    
    One of the notifiers to the CPU hotplug code, is the cpufreq. The
    DOWN_PREPARE will call __cpufreq_remove_dev() which calls
    cpufreq_driver->exit. The PA Semi exit handler unmaps regions of I/O
    that is used by an interrupt that goes off constantly
    (system_reset_common, but it goes off during normal system operations
    too). I'm not sure exactly what this interrupt does.
    
    Running a simple function trace, you can see it goes off quite a bit:
    
    # tracer: function
    #
    #           TASK-PID    CPU#    TIMESTAMP  FUNCTION
    #              | |       |          |         |
              <idle>-0     [001]  1558.859363: .pasemi_system_reset_exception <-.system_reset_exception
              <idle>-0     [000]  1558.860112: .pasemi_system_reset_exception <-.system_reset_exception
              <idle>-0     [000]  1558.861109: .pasemi_system_reset_exception <-.system_reset_exception
              <idle>-0     [001]  1558.861361: .pasemi_system_reset_exception <-.system_reset_exception
              <idle>-0     [000]  1558.861437: .pasemi_system_reset_exception <-.system_reset_exception
    
    When the region is unmapped, the system crashes with:
    
    Disabling non-boot CPUs ...
    Error taking CPU1 down: -38
    Unable to handle kernel paging request for data at address 0xd0000800903a0100
    Faulting instruction address: 0xc000000000055fcc
    Oops: Kernel access of bad area, sig: 11 [#1]
    PREEMPT SMP NR_CPUS=64 NUMA PA Semi PWRficient
    Modules linked in: shpchp
    NIP: c000000000055fcc LR: c000000000055fb4 CTR: c0000000000df1fc
    REGS: c0000000012175d0 TRAP: 0300   Not tainted  (3.8.0-rc4-test-dirty)
    MSR: 9000000000009032 <SF,HV,EE,ME,IR,DR,RI>  CR: 24000088  XER: 00000000
    SOFTE: 0
    DAR: d0000800903a0100, DSISR: 42000000
    TASK = c0000000010e9008[0] 'swapper/0' THREAD: c000000001214000 CPU: 0
    GPR00: d0000800903a0000 c000000001217850 c0000000012167e0 0000000000000000
    GPR04: 0000000000000000 0000000000000724 0000000000000724 0000000000000000
    GPR08: 0000000000000000 0000000000000000 0000000000000001 0000000000a70000
    GPR12: 0000000024000080 c00000000fff0000 ffffffffffffffff 000000003ffffae0
    GPR16: ffffffffffffffff 0000000000a21198 0000000000000060 0000000000000000
    GPR20: 00000000008fdd35 0000000000a21258 000000003ffffaf0 0000000000000417
    GPR24: 0000000000a226d0 c000000000000000 0000000000000000 0000000000000000
    GPR28: c00000000138b358 0000000000000000 c000000001144818 d0000800903a0100
    NIP [c000000000055fcc] .set_astate+0x5c/0xa4
    LR [c000000000055fb4] .set_astate+0x44/0xa4
    Call Trace:
    [c000000001217850] [c000000000055fb4] .set_astate+0x44/0xa4 (unreliable)
    [c0000000012178f0] [c00000000005647c] .restore_astate+0x2c/0x34
    [c000000001217980] [c000000000054668] .pasemi_system_reset_exception+0x6c/0x88
    [c000000001217a00] [c000000000019ef0] .system_reset_exception+0x48/0x84
    [c000000001217a80] [c000000000001e40] system_reset_common+0x140/0x180
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 57886616ca7bff844a6427436d0c8faf74653f73
Author: Joseph Lo <josephl@nvidia.com>
Date:   Thu Jan 3 14:42:59 2013 +0800

    ARM: tegra: update the cache maintenance order for CPU shutdown
    
    Updating the cache maintenance order before CPU shutdown when doing CPU
    hotplug.
    The old order:
    * clean L1 by flush_cache_all
    * exit SMP
    * CPU shutdown
    Adapt to:
    * disable L1 data cache by clear C bit
    * clean L1 by v7_flush_dcache_louis
    * exit SMP
    * CPU shutdown
    
    For CPU hotplug case, it's no need to do "flush_cache_all". And we should
    disable L1 data cache before clean L1 data cache. Then leaving the SMP
    coherency.
    
    Signed-off-by: Joseph Lo <josephl@nvidia.com>
    Acked-by: Peter De Schrijver <pdeschrijver@nvidia.com>
    Signed-off-by: Stephen Warren <swarren@nvidia.com>

commit 5dfd486c4750c9278c63fa96e6e85bdd2fb58e9d
Author: Dave Hansen <dave@linux.vnet.ibm.com>
Date:   Tue Jan 22 13:24:35 2013 -0800

    x86, kvm: Fix kvm's use of __pa() on percpu areas
    
    In short, it is illegal to call __pa() on an address holding
    a percpu variable.  This replaces those __pa() calls with
    slow_virt_to_phys().  All of the cases in this patch are
    in boot time (or CPU hotplug time at worst) code, so the
    slow pagetable walking in slow_virt_to_phys() is not expected
    to have a performance impact.
    
    The times when this actually matters are pretty obscure
    (certain 32-bit NUMA systems), but it _does_ happen.  It is
    important to keep KVM guests working on these systems because
    the real hardware is getting harder and harder to find.
    
    This bug manifested first by me seeing a plain hang at boot
    after this message:
    
            CPU 0 irqstacks, hard=f3018000 soft=f301a000
    
    or, sometimes, it would actually make it out to the console:
    
    [    0.000000] BUG: unable to handle kernel paging request at ffffffff
    
    I eventually traced it down to the KVM async pagefault code.
    This can be worked around by disabling that code either at
    compile-time, or on the kernel command-line.
    
    The kvm async pagefault code was injecting page faults in
    to the guest which the guest misinterpreted because its
    "reason" was not being properly sent from the host.
    
    The guest passes a physical address of an per-cpu async page
    fault structure via an MSR to the host.  Since __pa() is
    broken on percpu data, the physical address it sent was
    bascially bogus and the host went scribbling on random data.
    The guest never saw the real reason for the page fault (it
    was injected by the host), assumed that the kernel had taken
    a _real_ page fault, and panic()'d.  The behavior varied,
    though, depending on what got corrupted by the bad write.
    
    Signed-off-by: Dave Hansen <dave@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20130122212435.4905663F@kernel.stglabs.ibm.com
    Acked-by: Rik van Riel <riel@redhat.com>
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit b791ca8fdc546678386a98a17be6f872472f50a4
Author: Bjrn Mork <bjorn@mork.no>
Date:   Wed Dec 19 20:51:31 2012 +0100

    watchdog: Fix disable/enable regression
    
    commit 3935e89505a1c3ab3f3b0c7ef0eae54124f48905 upstream.
    
    Commit 8d4516904b39 ("watchdog: Fix CPU hotplug regression") causes an
    oops or hard lockup when doing
    
     echo 0 > /proc/sys/kernel/nmi_watchdog
     echo 1 > /proc/sys/kernel/nmi_watchdog
    
    and the kernel is booted with nmi_watchdog=1 (default)
    
    Running laptop-mode-tools and disconnecting/connecting AC power will
    cause this to trigger, making it a common failure scenario on laptops.
    
    Instead of bailing out of watchdog_disable() when !watchdog_enabled we
    can initialize the hrtimer regardless of watchdog_enabled status.  This
    makes it safe to call watchdog_disable() in the nmi_watchdog=0 case,
    without the negative effect on the enabled => disabled => enabled case.
    
    All these tests pass with this patch:
    - nmi_watchdog=1
      echo 0 > /proc/sys/kernel/nmi_watchdog
      echo 1 > /proc/sys/kernel/nmi_watchdog
    
    - nmi_watchdog=0
      echo 0 > /sys/devices/system/cpu/cpu1/online
    
    - nmi_watchdog=0
      echo mem > /sys/power/state
    
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=51661
    
    Signed-off-by: Bjrn Mork <bjorn@mork.no>
    Cc: Norbert Warmuth <nwarmuth@t-online.de>
    Cc: Joseph Salisbury <joseph.salisbury@canonical.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit d55bf532d72b3cfdfe84e696ace995067324c96c
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Jan 15 22:40:26 2013 -0500

    Revert "xen/smp: Fix CPU online/offline bug triggering a BUG: scheduling while atomic."
    
    This reverts commit 41bd956de3dfdc3a43708fe2e0c8096c69064a1e.
    
    The fix is incorrect and not appropiate for the latest kernels.
    In fact it _causes_ the BUG: scheduling while atomic while
    doing vCPU hotplug.
    
    Suggested-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit fb9125680d0e7c23eae7c6000acc91ea26acab9c
Author: Li Zhong <zhong@linux.vnet.ibm.com>
Date:   Wed Oct 17 21:30:13 2012 +0000

    powerpc: Fix a lazy irq related WARING in arch_local_irq_restore()
    
    The pseries CPU hotplug code uses cede_processor without properly
    synchronizing the SW and HW interrupt enable state. This fixes
    it using the same helpers that were written for the idle code.
    
    Signed-off-by: Li Zhong <zhong@linux.vnet.ibm.com>
    
    =======================
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 93f3b2ee0abff5438e74cc90cf816429248cc8eb
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Jan 2 16:54:12 2013 +0100

    s390/irq: count cpu restart events
    
    Count CPU Restart events and make them visible via /proc/interrupts.
    Every CPU hotplug (online) event will increase the per cpu counter.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit 3a5a6d0c2b0391e159fa5bf1dddb9bf1f35178a0
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Jan 7 08:51:07 2013 -0800

    cpuset: don't nest cgroup_mutex inside get_online_cpus()
    
    CPU / memory hotplug path currently grabs cgroup_mutex from hotplug
    event notifications.  We want to separate cpuset locking from cgroup
    core and make cgroup_mutex outer to hotplug synchronization so that,
    among other things, mechanisms which depend on get_online_cpus() can
    be used from cgroup callbacks.  In general, we want to keep
    cgroup_mutex the outermost lock to minimize locking interactions among
    different controllers.
    
    Convert cpuset_handle_hotplug() to cpuset_hotplug_workfn() and
    schedule it from the hotplug notifications.  As the function can
    already handle multiple mixed events without any input, converting it
    to a work function is mostly trivial; however, one complication is
    that cpuset_update_active_cpus() needs to update sched domains
    synchronously to reflect an offlined cpu to avoid confusing the
    scheduler.  This is worked around by falling back to the the default
    single sched domain synchronously before scheduling the actual hotplug
    work.  This makes sched domain rebuilt twice per CPU hotplug event but
    the operation isn't that heavy and a lot of the second operation would
    be noop for systems w/ single sched domain, which is the common case.
    
    This decouples cpuset hotplug handling from the notification callbacks
    and there can be an arbitrary delay between the actual event and
    updates to cpusets.  Scheduler and mm can handle it fine but moving
    tasks out of an empty cpuset may race against writes to the cpuset
    restoring execution resources which can lead to confusing behavior.
    Flush hotplug work item from cpuset_write_resmask() to avoid such
    confusions.
    
    v2: Synchronous sched domain rebuilding using the fallback sched
        domain added.  This fixes various issues caused by confused
        scheduler putting tasks on a dead CPU, including the one reported
        by Li Zefan.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizefan@huawei.com>

commit 3935e89505a1c3ab3f3b0c7ef0eae54124f48905
Author: Bjrn Mork <bjorn@mork.no>
Date:   Wed Dec 19 20:51:31 2012 +0100

    watchdog: Fix disable/enable regression
    
    Commit 8d4516904b39 ("watchdog: Fix CPU hotplug regression") causes an
    oops or hard lockup when doing
    
     echo 0 > /proc/sys/kernel/nmi_watchdog
     echo 1 > /proc/sys/kernel/nmi_watchdog
    
    and the kernel is booted with nmi_watchdog=1 (default)
    
    Running laptop-mode-tools and disconnecting/connecting AC power will
    cause this to trigger, making it a common failure scenario on laptops.
    
    Instead of bailing out of watchdog_disable() when !watchdog_enabled we
    can initialize the hrtimer regardless of watchdog_enabled status.  This
    makes it safe to call watchdog_disable() in the nmi_watchdog=0 case,
    without the negative effect on the enabled => disabled => enabled case.
    
    All these tests pass with this patch:
    - nmi_watchdog=1
      echo 0 > /proc/sys/kernel/nmi_watchdog
      echo 1 > /proc/sys/kernel/nmi_watchdog
    
    - nmi_watchdog=0
      echo 0 > /sys/devices/system/cpu/cpu1/online
    
    - nmi_watchdog=0
      echo mem > /sys/power/state
    
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=51661
    
    Cc: <stable@vger.kernel.org> # v3.7
    Cc: Norbert Warmuth <nwarmuth@t-online.de>
    Cc: Joseph Salisbury <joseph.salisbury@canonical.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Bjrn Mork <bjorn@mork.no>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit cfd1f032f98e5ab3a04f23a0adbd53ff8744827d
Merge: 27d7c2a006a8 8d4516904b39
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 6 08:27:11 2012 -0800

    Merge branch 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull watchdog fix from Thomas Gleixner:
     "Trivial CPU hotplug regression fix for the watchdog code"
    
    * 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      watchdog: Fix CPU hotplug regression

commit 8d4516904b39507458bee8115793528e12b1d8dd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 4 18:59:34 2012 +0100

    watchdog: Fix CPU hotplug regression
    
    Norbert reported:
    "3.7-rc6 booted with nmi_watchdog=0 fails to suspend to RAM or
     offline CPUs. It's reproducable with a KVM guest and physical
     system."
    
    The reason is that commit bcd951cf(watchdog: Use hotplug thread
    infrastructure) missed to take this into account. So the cpu offline
    code gets stuck in the teardown function because it accesses non
    initialized data structures.
    
    Add a check for watchdog_enabled into that path to cure the issue.
    
    Reported-and-tested-by: Norbert Warmuth <nwarmuth@t-online.de>
    Tested-by: Joseph Salisbury <joseph.salisbury@canonical.com>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1211231033230.2701@ionos
    Link: http://bugs.launchpad.net/bugs/1079534
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit d4c091f13dc4d30e4af43c0ccf8c82b3277574ca
Merge: 08ab72980a77 261cba2deb7d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Nov 29 21:43:06 2012 +0100

    Merge branch 'acpi-general'
    
    * acpi-general: (38 commits)
      ACPI / thermal: _TMP and _CRT/_HOT/_PSV/_ACx dependency fix
      ACPI: drop unnecessary local variable from acpi_system_write_wakeup_device()
      ACPI: Fix logging when no pci_irq is allocated
      ACPI: Update Dock hotplug error messages
      ACPI: Update Container hotplug error messages
      ACPI: Update Memory hotplug error messages
      ACPI: Update CPU hotplug error messages
      ACPI: Add acpi_handle_<level>() interfaces
      ACPI: remove use of __devexit
      ACPI / PM: Add Sony Vaio VPCEB1S1E to nonvs blacklist.
      ACPI / battery: Correct battery capacity values on Thinkpads
      Revert "ACPI / x86: Add quirk for "CheckPoint P-20-00" to not use bridge _CRS_ info"
      ACPI: create _SUN sysfs file
      ACPI / memhotplug: bind the memory device when the driver is being loaded
      ACPI / memhotplug: don't allow to eject the memory device if it is being used
      ACPI / memhotplug: free memory device if acpi_memory_enable_device() failed
      ACPI / memhotplug: fix memory leak when memory device is unbound from acpi_memhotplug
      ACPI / memhotplug: deal with eject request in hotplug queue
      ACPI / memory-hotplug: add memory offline code to acpi_memory_device_remove()
      ACPI / memory-hotplug: call acpi_bus_trim() to remove memory device
      ...
    
    Conflicts:
            include/linux/acpi.h (two additions at the end of the same file)

commit 47db4547ffe5aa2eb5b053e6c02f0561fbbfa221
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Tue Nov 20 23:42:27 2012 +0000

    ACPI: Update CPU hotplug error messages
    
    Updated CPU hotplug error messages with acpi_handle_<level>(),
    dev_<level>() and pr_<level>().  Modified some messages for
    clarity.  Added error status / id info to the messages where
    needed.
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Tested-by: Vijay Mohan Pandarathil <vijaymohan.pandarathil@hp.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

commit 45f5984a8a528f7507f3ec860d297934d4449ad1
Author: Gregory CLEMENT <gregory.clement@free-electrons.com>
Date:   Wed Nov 14 22:51:08 2012 +0100

    arm: mvebu: Add SMP support for Armada XP
    
    This enables SMP support on the Armada XP processor. It adds the
    mandatory functions to support SMP such as: the SMP initialization
    functions in platsmp.c, the secondary CPU entry point in headsmp.S and
    the CPU hotplug initial support in hotplug.c.
    
    Signed-off-by: Yehuda Yitschak <yehuday@marvell.com>
    Signed-off-by: Gregory CLEMENT <gregory.clement@free-electrons.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>

commit aac1cda34b84a9411d6b8d18c3658f094c834911
Merge: 2c5594df344c d484a215139c 351573a86d0e cda4dc813071 c896054f75f9 7bd8f2a74bcb af71befa282d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Nov 16 09:59:58 2012 -0800

    Merge branches 'urgent.2012.10.27a', 'doc.2012.11.16a', 'fixes.2012.11.13a', 'srcu.2012.10.27a', 'stall.2012.11.13a', 'tracing.2012.11.08a' and 'idle.2012.10.24a' into HEAD
    
    urgent.2012.10.27a: Fix for RCU user-mode transition (already in -tip).
    
    doc.2012.11.08a: Documentation updates, most notably codifying the
            memory-barrier guarantees inherent to grace periods.
    
    fixes.2012.11.13a: Miscellaneous fixes.
    
    srcu.2012.10.27a: Allow statically allocated and initialized srcu_struct
            structures (courtesy of Lai Jiangshan).
    
    stall.2012.11.13a: Add more diagnostic information to RCU CPU stall
            warnings, also decrease from 60 seconds to 21 seconds.
    
    hotplug.2012.11.08a: Minor updates to CPU hotplug handling.
    
    tracing.2012.11.08a: Improved debugfs tracing, courtesy of Michael Wang.
    
    idle.2012.10.24a: Updates to RCU idle/adaptive-idle handling, including
            a boot parameter that maps normal grace periods to expedited.
    
    Resolved conflict in kernel/rcutree.c due to side-by-side change.

commit 6f5298c2139b06925037490367906f3d73955b86
Author: Fenghua Yu <fenghua.yu@intel.com>
Date:   Tue Nov 13 11:32:50 2012 -0800

    x86/i387.c: Initialize thread xstate only on CPU0 only once
    
    init_thread_xstate() is only called once to avoid overriding xstate_size during
    boot time or during CPU hotplug.
    
    Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
    Link: http://lkml.kernel.org/r/1352835171-3958-14-git-send-email-fenghua.yu@intel.com
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit a3920a6efa158b445b8a39080b463b9b29337425
Merge: 18a022de47bc d1d4a81b842d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 13 11:27:59 2012 +0900

    Merge branch 'release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux
    
    Pull ACPI & Thermal updates from Len Brown:
     "The generic Linux thermal layer is gaining some new capabilities
      (generic cooling via cpufreq) and some new customers (ARM).
    
      Also, an ACPI EC bug fix plus a regression fix."
    
    * 'release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux: (30 commits)
      tools/power/acpi/acpidump: remove duplicated include from acpidump.c
      ACPI idle, CPU hotplug: Fix NULL pointer dereference during hotplug
      cpuidle / ACPI: fix potential NULL pointer dereference
      ACPI: EC: Add a quirk for CLEVO M720T/M730T laptop
      ACPI: EC: Make the GPE storm threshold a module parameter
      thermal: Exynos: Fix NULL pointer dereference in exynos_unregister_thermal()
      Thermal: Fix bug on cpu_cooling, cooling device's id conflict problem.
      thermal: exynos: Use devm_* functions
      ARM: exynos: add thermal sensor driver platform data support
      thermal: exynos: register the tmu sensor with the kernel thermal layer
      thermal: exynos5: add exynos5250 thermal sensor driver support
      hwmon: exynos4: move thermal sensor driver to driver/thermal directory
      thermal: add generic cpufreq cooling implementation
      Fix a build error.
      thermal: Fix potential NULL pointer accesses
      thermal: add Renesas R-Car thermal sensor support
      thermal: fix potential out-of-bounds memory access
      Thermal: Introduce locking for cdev.thermal_instances list.
      Thermal: Unify the code for both active and passive cooling
      Thermal: Introduce simple arbitrator for setting device cooling state
      ...

commit 224cae7fedfe7dcc4c377ac235eda59eec1e6edf
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 19:46:26 2012 +0530

    CPU hotplug, cpusets, suspend: Don't modify cpusets during suspend/resume
    
    commit d35be8bab9b0ce44bed4b9453f86ebf64062721e upstream.
    
    In the event of CPU hotplug, the kernel modifies the cpusets' cpus_allowed
    masks as and when necessary to ensure that the tasks belonging to the cpusets
    have some place (online CPUs) to run on. And regular CPU hotplug is
    destructive in the sense that the kernel doesn't remember the original cpuset
    configurations set by the user, across hotplug operations.
    
    However, suspend/resume (which uses CPU hotplug) is a special case in which
    the kernel has the responsibility to restore the system (during resume), to
    exactly the same state it was in before suspend.
    
    In order to achieve that, do the following:
    
    1. Don't modify cpusets during suspend/resume. At all.
       In particular, don't move the tasks from one cpuset to another, and
       don't modify any cpuset's cpus_allowed mask. So, simply ignore cpusets
       during the CPU hotplug operations that are carried out in the
       suspend/resume path.
    
    2. However, cpusets and sched domains are related. We just want to avoid
       altering cpusets alone. So, to keep the sched domains updated, build
       a single sched domain (containing all active cpus) during each of the
       CPU hotplug operations carried out in s/r path, effectively ignoring
       the cpusets' cpus_allowed masks.
    
       (Since userspace is frozen while doing all this, it will go unnoticed.)
    
    3. During the last CPU online operation during resume, build the sched
       domains by looking up the (unaltered) cpusets' cpus_allowed masks.
       That will bring back the system to the same original state as it was in
       before suspend.
    
    Ultimately, this will not only solve the cpuset problem related to suspend
    resume (ie., restores the cpusets to exactly what it was before suspend, by
    not touching it at all) but also speeds up suspend/resume because we avoid
    running cpuset update code for every CPU being offlined/onlined.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120524141611.3692.20155.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit c62f9945efea31db203fd4fb77e830ddffdcabf6
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 19:46:26 2012 +0530

    CPU hotplug, cpusets, suspend: Don't modify cpusets during suspend/resume
    
    commit d35be8bab9b0ce44bed4b9453f86ebf64062721e upstream.
    
    In the event of CPU hotplug, the kernel modifies the cpusets' cpus_allowed
    masks as and when necessary to ensure that the tasks belonging to the cpusets
    have some place (online CPUs) to run on. And regular CPU hotplug is
    destructive in the sense that the kernel doesn't remember the original cpuset
    configurations set by the user, across hotplug operations.
    
    However, suspend/resume (which uses CPU hotplug) is a special case in which
    the kernel has the responsibility to restore the system (during resume), to
    exactly the same state it was in before suspend.
    
    In order to achieve that, do the following:
    
    1. Don't modify cpusets during suspend/resume. At all.
       In particular, don't move the tasks from one cpuset to another, and
       don't modify any cpuset's cpus_allowed mask. So, simply ignore cpusets
       during the CPU hotplug operations that are carried out in the
       suspend/resume path.
    
    2. However, cpusets and sched domains are related. We just want to avoid
       altering cpusets alone. So, to keep the sched domains updated, build
       a single sched domain (containing all active cpus) during each of the
       CPU hotplug operations carried out in s/r path, effectively ignoring
       the cpusets' cpus_allowed masks.
    
       (Since userspace is frozen while doing all this, it will go unnoticed.)
    
    3. During the last CPU online operation during resume, build the sched
       domains by looking up the (unaltered) cpusets' cpus_allowed masks.
       That will bring back the system to the same original state as it was in
       before suspend.
    
    Ultimately, this will not only solve the cpuset problem related to suspend
    resume (ie., restores the cpusets to exactly what it was before suspend, by
    not touching it at all) but also speeds up suspend/resume because we avoid
    running cpuset update code for every CPU being offlined/onlined.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120524141611.3692.20155.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 8f48f1a28ee27909afbba8a3c2c653a15f810c3e
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 19:46:26 2012 +0530

    CPU hotplug, cpusets, suspend: Don't modify cpusets during suspend/resume
    
    commit d35be8bab9b0ce44bed4b9453f86ebf64062721e upstream.
    
    In the event of CPU hotplug, the kernel modifies the cpusets' cpus_allowed
    masks as and when necessary to ensure that the tasks belonging to the cpusets
    have some place (online CPUs) to run on. And regular CPU hotplug is
    destructive in the sense that the kernel doesn't remember the original cpuset
    configurations set by the user, across hotplug operations.
    
    However, suspend/resume (which uses CPU hotplug) is a special case in which
    the kernel has the responsibility to restore the system (during resume), to
    exactly the same state it was in before suspend.
    
    In order to achieve that, do the following:
    
    1. Don't modify cpusets during suspend/resume. At all.
       In particular, don't move the tasks from one cpuset to another, and
       don't modify any cpuset's cpus_allowed mask. So, simply ignore cpusets
       during the CPU hotplug operations that are carried out in the
       suspend/resume path.
    
    2. However, cpusets and sched domains are related. We just want to avoid
       altering cpusets alone. So, to keep the sched domains updated, build
       a single sched domain (containing all active cpus) during each of the
       CPU hotplug operations carried out in s/r path, effectively ignoring
       the cpusets' cpus_allowed masks.
    
       (Since userspace is frozen while doing all this, it will go unnoticed.)
    
    3. During the last CPU online operation during resume, build the sched
       domains by looking up the (unaltered) cpusets' cpus_allowed masks.
       That will bring back the system to the same original state as it was in
       before suspend.
    
    Ultimately, this will not only solve the cpuset problem related to suspend
    resume (ie., restores the cpusets to exactly what it was before suspend, by
    not touching it at all) but also speeds up suspend/resume because we avoid
    running cpuset update code for every CPU being offlined/onlined.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120524141611.3692.20155.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

commit 0588f1f934791b79d0a1e9b327be9b6eb361d2b8
Merge: 9d55ab71b735 301a5cba2887
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 12 22:13:05 2012 +0900

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "A CPU hotplug related crash fix and a nohz accounting fixlet."
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Update sched_domains_numa_masks[][] when new cpus are onlined
      sched: Ensure 'sched_domains_numa_levels' is safe to use in other functions
      nohz: Fix one jiffy count too far in idle cputime

commit 40924754f2cabd5d9af4bcd4dcecc362b5e0baa1
Merge: cbd8aca47213 2f60d628ffd0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 12 10:46:03 2012 +0900

    Merge branch 'writeback-for-next' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux
    
    Pull writeback fixes from Fengguang Wu:
     "Three trivial writeback fixes"
    
    * 'writeback-for-next' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux:
      CPU hotplug, writeback: Don't call writeback_set_ratelimit() too often during hotplug
      writeback: correct comment for move_expired_inodes()
      backing-dev: use kstrto* in preference to simple_strtoul

commit 93034429eba1b31cc4c017c28c9677e199931dcf
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 19:46:26 2012 +0530

    CPU hotplug, cpusets, suspend: Don't modify cpusets during suspend/resume
    
    commit d35be8bab9b0ce44bed4b9453f86ebf64062721e upstream.
    
    In the event of CPU hotplug, the kernel modifies the cpusets' cpus_allowed
    masks as and when necessary to ensure that the tasks belonging to the cpusets
    have some place (online CPUs) to run on. And regular CPU hotplug is
    destructive in the sense that the kernel doesn't remember the original cpuset
    configurations set by the user, across hotplug operations.
    
    However, suspend/resume (which uses CPU hotplug) is a special case in which
    the kernel has the responsibility to restore the system (during resume), to
    exactly the same state it was in before suspend.
    
    In order to achieve that, do the following:
    
    1. Don't modify cpusets during suspend/resume. At all.
       In particular, don't move the tasks from one cpuset to another, and
       don't modify any cpuset's cpus_allowed mask. So, simply ignore cpusets
       during the CPU hotplug operations that are carried out in the
       suspend/resume path.
    
    2. However, cpusets and sched domains are related. We just want to avoid
       altering cpusets alone. So, to keep the sched domains updated, build
       a single sched domain (containing all active cpus) during each of the
       CPU hotplug operations carried out in s/r path, effectively ignoring
       the cpusets' cpus_allowed masks.
    
       (Since userspace is frozen while doing all this, it will go unnoticed.)
    
    3. During the last CPU online operation during resume, build the sched
       domains by looking up the (unaltered) cpusets' cpus_allowed masks.
       That will bring back the system to the same original state as it was in
       before suspend.
    
    Ultimately, this will not only solve the cpuset problem related to suspend
    resume (ie., restores the cpusets to exactly what it was before suspend, by
    not touching it at all) but also speeds up suspend/resume because we avoid
    running cpuset update code for every CPU being offlined/onlined.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120524141611.3692.20155.stgit@srivatsabhat.in.ibm.com
    [Preeti U Murthy: Please apply this patch to the stable tree 3.0.y]
    Signed-off-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>

commit 075663d19885eb3738fd2d7dbdb8947e12563b68
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Mon Oct 8 16:28:20 2012 -0700

    CPU hotplug, debug: detect imbalance between get_online_cpus() and put_online_cpus()
    
    The synchronization between CPU hotplug readers and writers is achieved
    by means of refcounting, safeguarded by the cpu_hotplug.lock.
    
    get_online_cpus() increments the refcount, whereas put_online_cpus()
    decrements it.  If we ever hit an imbalance between the two, we end up
    compromising the guarantees of the hotplug synchronization i.e, for
    example, an extra call to put_online_cpus() can end up allowing a
    hotplug reader to execute concurrently with a hotplug writer.
    
    So, add a WARN_ON() in put_online_cpus() to detect such cases where the
    refcount can go negative, and also attempt to fix it up, so that we can
    continue to run.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit cf31cd1a0c692a1445c80756055875088fa29982
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Mon Oct 8 13:43:08 2012 +0530

    ACPI idle, CPU hotplug: Fix NULL pointer dereference during hotplug
    
    On a KVM guest, when a CPU is taken offline and brought back online, we hit
    the following NULL pointer dereference:
    
    [   45.400843] Unregister pv shared memory for cpu 1
    [   45.412331] smpboot: CPU 1 is now offline
    [   45.529894] SMP alternatives: lockdep: fixing up alternatives
    [   45.533472] smpboot: Booting Node 0 Processor 1 APIC 0x1
    [   45.411526] kvm-clock: cpu 1, msr 0:7d14601, secondary cpu clock
    [   45.571370] KVM setup async PF for cpu 1
    [   45.572331] kvm-stealtime: cpu 1, msr 7d0e040
    [   45.575031] BUG: unable to handle kernel NULL pointer dereference at           (null)
    [   45.576017] IP: [<ffffffff81519f98>] cpuidle_disable_device+0x18/0x80
    [   45.576017] PGD 5dfb067 PUD 5da8067 PMD 0
    [   45.576017] Oops: 0000 [#1] SMP
    [   45.576017] Modules linked in:
    [   45.576017] CPU 0
    [   45.576017] Pid: 607, comm: stress_cpu_hotp Not tainted 3.6.0-padata-tp-debug #3 Bochs Bochs
    [   45.576017] RIP: 0010:[<ffffffff81519f98>]  [<ffffffff81519f98>] cpuidle_disable_device+0x18/0x80
    [   45.576017] RSP: 0018:ffff880005d93ce8  EFLAGS: 00010286
    [   45.576017] RAX: ffff880005d93fd8 RBX: 0000000000000000 RCX: 0000000000000006
    [   45.576017] RDX: 0000000000000006 RSI: 2222222222222222 RDI: 0000000000000000
    [   45.576017] RBP: ffff880005d93cf8 R08: 2222222222222222 R09: 2222222222222222
    [   45.576017] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000000
    [   45.576017] R13: 0000000000000000 R14: ffffffff81c8cca0 R15: 0000000000000001
    [   45.576017] FS:  00007f91936ae700(0000) GS:ffff880007c00000(0000) knlGS:0000000000000000
    [   45.576017] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
    [   45.576017] CR2: 0000000000000000 CR3: 0000000005db3000 CR4: 00000000000006f0
    [   45.576017] DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    [   45.576017] DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
    [   45.576017] Process stress_cpu_hotp (pid: 607, threadinfo ffff880005d92000, task ffff8800066bbf40)
    [   45.576017] Stack:
    [   45.576017]  ffff880007a96400 0000000000000000 ffff880005d93d28 ffffffff813ac689
    [   45.576017]  ffff880007a96400 ffff880007a96400 0000000000000002 ffffffff81cd8d01
    [   45.576017]  ffff880005d93d58 ffffffff813aa498 0000000000000001 00000000ffffffdd
    [   45.576017] Call Trace:
    [   45.576017]  [<ffffffff813ac689>] acpi_processor_hotplug+0x55/0x97
    [   45.576017]  [<ffffffff813aa498>] acpi_cpu_soft_notify+0x93/0xce
    [   45.576017]  [<ffffffff816ae47d>] notifier_call_chain+0x5d/0x110
    [   45.576017]  [<ffffffff8109730e>] __raw_notifier_call_chain+0xe/0x10
    [   45.576017]  [<ffffffff81069050>] __cpu_notify+0x20/0x40
    [   45.576017]  [<ffffffff81069085>] cpu_notify+0x15/0x20
    [   45.576017]  [<ffffffff816978f1>] _cpu_up+0xee/0x137
    [   45.576017]  [<ffffffff81697983>] cpu_up+0x49/0x59
    [   45.576017]  [<ffffffff8168758d>] store_online+0x9d/0xe0
    [   45.576017]  [<ffffffff8140a9f8>] dev_attr_store+0x18/0x30
    [   45.576017]  [<ffffffff812322c0>] sysfs_write_file+0xe0/0x150
    [   45.576017]  [<ffffffff811b389c>] vfs_write+0xac/0x180
    [   45.576017]  [<ffffffff811b3be2>] sys_write+0x52/0xa0
    [   45.576017]  [<ffffffff816b31e9>] system_call_fastpath+0x16/0x1b
    [   45.576017] Code: 48 c7 c7 40 e5 ca 81 e8 07 d0 18 00 5d c3 0f 1f 44 00 00 0f 1f 44 00 00 55 48 89 e5 48 83 ec 10 48 89 5d f0 4c 89 65 f8 48 89 fb <f6> 07 02 75 13 48 8b 5d f0 4c 8b 65 f8 c9 c3 66 0f 1f 84 00 00
    [   45.576017] RIP  [<ffffffff81519f98>] cpuidle_disable_device+0x18/0x80
    [   45.576017]  RSP <ffff880005d93ce8>
    [   45.576017] CR2: 0000000000000000
    [   45.656079] ---[ end trace 433d6c9ac0b02cef ]---
    
    Analysis:
    Commit 3d339dc (cpuidle / ACPI : move cpuidle_device field out of the
    acpi_processor_power structure()) made the allocation of the dev structure
    (struct cpuidle) of a CPU dynamic, whereas previously it was statically
    allocated. And this dynamic allocation occurs in acpi_processor_power_init()
    if pr->flags.power evaluates to non-zero.
    
    On KVM guests, pr->flags.power evaluates to zero, hence dev is never
    allocated. This causes the NULL pointer (dev) dereference in
    cpuidle_disable_device() during a subsequent CPU online operation. Fix this
    by ensuring that dev is non-NULL before dereferencing.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

commit 033d9959ed2dc1029217d4165f80a71702dc578e
Merge: 974a847e00cf 7c6e72e46c9e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 09:54:49 2012 -0700

    Merge branch 'for-3.7' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    Pull workqueue changes from Tejun Heo:
     "This is workqueue updates for v3.7-rc1.  A lot of activities this
      round including considerable API and behavior cleanups.
    
       * delayed_work combines a timer and a work item.  The handling of the
         timer part has always been a bit clunky leading to confusing
         cancelation API with weird corner-case behaviors.  delayed_work is
         updated to use new IRQ safe timer and cancelation now works as
         expected.
    
       * Another deficiency of delayed_work was lack of the counterpart of
         mod_timer() which led to cancel+queue combinations or open-coded
         timer+work usages.  mod_delayed_work[_on]() are added.
    
         These two delayed_work changes make delayed_work provide interface
         and behave like timer which is executed with process context.
    
       * A work item could be executed concurrently on multiple CPUs, which
         is rather unintuitive and made flush_work() behavior confusing and
         half-broken under certain circumstances.  This problem doesn't
         exist for non-reentrant workqueues.  While non-reentrancy check
         isn't free, the overhead is incurred only when a work item bounces
         across different CPUs and even in simulated pathological scenario
         the overhead isn't too high.
    
         All workqueues are made non-reentrant.  This removes the
         distinction between flush_[delayed_]work() and
         flush_[delayed_]_work_sync().  The former is now as strong as the
         latter and the specified work item is guaranteed to have finished
         execution of any previous queueing on return.
    
       * In addition to the various bug fixes, Lai redid and simplified CPU
         hotplug handling significantly.
    
       * Joonsoo introduced system_highpri_wq and used it during CPU
         hotplug.
    
      There are two merge commits - one to pull in IRQ safe timer from
      tip/timers/core and the other to pull in CPU hotplug fixes from
      wq/for-3.6-fixes as Lai's hotplug restructuring depended on them."
    
    Fixed a number of trivial conflicts, but the more interesting conflicts
    were silent ones where the deprecated interfaces had been used by new
    code in the merge window, and thus didn't cause any real data conflicts.
    
    Tejun pointed out a few of them, I fixed a couple more.
    
    * 'for-3.7' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq: (46 commits)
      workqueue: remove spurious WARN_ON_ONCE(in_irq()) from try_to_grab_pending()
      workqueue: use cwq_set_max_active() helper for workqueue_set_max_active()
      workqueue: introduce cwq_set_max_active() helper for thaw_workqueues()
      workqueue: remove @delayed from cwq_dec_nr_in_flight()
      workqueue: fix possible stall on try_to_grab_pending() of a delayed work item
      workqueue: use hotcpu_notifier() for workqueue_cpu_down_callback()
      workqueue: use __cpuinit instead of __devinit for cpu callbacks
      workqueue: rename manager_mutex to assoc_mutex
      workqueue: WORKER_REBIND is no longer necessary for idle rebinding
      workqueue: WORKER_REBIND is no longer necessary for busy rebinding
      workqueue: reimplement idle worker rebinding
      workqueue: deprecate __cancel_delayed_work()
      workqueue: reimplement cancel_delayed_work() using try_to_grab_pending()
      workqueue: use mod_delayed_work() instead of __cancel + queue
      workqueue: use irqsafe timer for delayed_work
      workqueue: clean up delayed_work initializers and add missing one
      workqueue: make deferrable delayed_work initializer names consistent
      workqueue: cosmetic whitespace updates for macro definitions
      workqueue: deprecate system_nrt[_freezable]_wq
      workqueue: deprecate flush[_delayed]_work_sync()
      ...

commit 11801e9de26992d37cb869cc74f389b6a7677e0e
Merge: 1a58ddfc0fcf b6e3b5c2fea9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 18:24:44 2012 -0700

    Merge tag 'soc' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull ARM soc-specific updates from Olof Johansson:
     "Most notable here is probably the addition of basic support for the
      BCM2835, an SoC used in some of the Roku 2 players as well as the
      much-hyped Raspberry Pi, cleaned up and contributed by Stephen Warren.
      It's still early days on mainline support, with just the basics
      working.  But it has to start somewhere!
    
      Beyond that there's some conversions of clock infrastructure on tegra
      to common clock, misc updates for several other platforms, and OMAP
      now has its own bus (under drivers/bus) to manage its devices through.
    
      This branch adds two new directories outside of arch/arm:
      drivers/irqchip for new irq controllers, and drivers/bus for the above
      OMAP bus.  It's expected that some of the other platforms will migrate
      parts of their platforms to those directories over time as well."
    
    Fix up trivial conflicts with the clk infrastructure changes.
    
    * tag 'soc' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (62 commits)
      ARM: shmobile: add new __iomem annotation for new code
      ARM: LPC32xx: Support GPI 28
      ARM: LPC32xx: Platform update for devicetree completion of spi-pl022
      ARM: LPC32xx: Board cleanup
      irqchip: fill in empty Kconfig
      ARM: SAMSUNG: Add check for NULL in clock interface
      ARM: EXYNOS: Put PCM, Slimbus, Spdif clocks to off state
      ARM: EXYNOS: Add bus clock for FIMD
      ARM: SAMSUNG: Fix HDMI related warnings
      ARM: S3C24XX: Add .get_rate callback for "camif-upll" clock
      ARM: EXYNOS: Fix incorrect help text
      ARM: EXYNOS: Turn off clocks for NAND, OneNAND and TSI controllers
      ARM: OMAP: AM33xx hwmod: fixup SPI after platform_data move
      MAINTAINERS: add an entry for the BCM2835 ARM sub-architecture
      ARM: bcm2835: instantiate console UART
      ARM: bcm2835: add stub clock driver
      ARM: bcm2835: add system timer
      ARM: bcm2835: add interrupt controller driver
      ARM: add infra-structure for BCM2835 and Raspberry Pi
      ARM: tegra20: add CPU hotplug support
      ...

commit da8347969f324db5f572581397d9b3a8e108cda4
Merge: 80749df4a149 c416ddf5b909
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 10:46:27 2012 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86/asm changes from Ingo Molnar:
     "The one change that stands out is the alternatives patching change
      that prevents us from ever patching back instructions from SMP to UP:
      this simplifies things and speeds up CPU hotplug.
    
      Other than that it's smaller fixes, cleanups and improvements."
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86: Unspaghettize do_trap()
      x86_64: Work around old GAS bug
      x86: Use REP BSF unconditionally
      x86: Prefer TZCNT over BFS
      x86/64: Adjust types of temporaries used by ffs()/fls()/fls64()
      x86: Drop unnecessary kernel_eflags variable on 64-bit
      x86/smp: Don't ever patch back to UP if we unplug cpus

commit 2f60d628ffd042e65e0b1d3431fb3e38d6f7c1be
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Fri Sep 28 20:27:49 2012 +0800

    CPU hotplug, writeback: Don't call writeback_set_ratelimit() too often during hotplug
    
    The CPU hotplug callback related to writeback calls writeback_set_ratelimit()
    during every state change in the hotplug sequence. This is unnecessary
    since num_online_cpus() changes only once during the entire hotplug operation.
    
    So invoke the function only once per hotplug, thereby avoiding the
    unnecessary repetition of those costly calculations.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>

commit b5bd6a0e5fa8c0376d9746c566fe3daaa51ec825
Author: Jan Kara <jack@suse.cz>
Date:   Mon Sep 24 17:17:35 2012 -0700

    lib/flex_proportions.c: fix corruption of denominator in flexible proportions
    
    When racing with CPU hotplug, percpu_counter_sum() can return negative
    values for the number of observed events.
    
    This confuses fprop_new_period(), which uses unsigned type and as a
    result number of events is set to big *positive* number.  From that
    moment on, things go pear shaped and can result e.g.  in division by
    zero as denominator is later truncated to 32-bits.
    
    This bug causes a divide-by-zero oops in bdi_dirty_limit() in Borislav's
    3.6.0-rc6 based kernel.
    
    Fix the issue by using a signed type in fprop_new_period().  That makes
    us bail out from the function without doing anything (mistakenly)
    thinking there are no events to age.  That makes aging somewhat
    inaccurate but getting accurate data would be rather hard.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Reported-by: Borislav Petkov <bp@amd64.org>
    Reported-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 6323fa2256baa73d6a960ee57ec086b66aeecd0b
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Mon Sep 10 15:07:26 2012 +0530

    ARM: mm: update __v7_setup() to the new LoUIS cache maintenance API
    
    The ARMv7 processor setup function __v7_setup() cleans and invalidates the
    CPU cache before enabling MMU to start the CPU with a clean CPU local cache.
    
    But on ARMv7 architectures like Cortex-[A15/A8], this code will end
    up flushing the L2 caches(up to level of Coherency) which is undesirable
    and expensive. The setup functions are used in the CPU hotplug scenario too
    and hence flushing all cache levels should be avoided.
    
    This patch replaces the cache flushing call with the newly introduced
    v7 dcache LoUIS API where only cache levels up to LoUIS are cleaned and
    invalidated when a processors executes __v7_setup which is the expected
    behavior.
    
    For processors like A9 and A5 where the L2 cache is an outer one the
    behavior should be unchanged.
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Shawn Guo <shawn.guo@linaro.org>

commit 0737c8d7aedfe590010e38c3e3bbdc87affa8c3d
Merge: 0bf7a7056c7b 5f0ecb907deb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Sep 23 14:50:15 2012 -0700

    Merge branch 'hwmon-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jdelvare/staging
    
    Pull hwmon subsystem fixes from Jean Delvare.
    
    * 'hwmon-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jdelvare/staging:
      hwmon: (fam15h_power) Tweak runavg_range on resume
      hwmon: (coretemp) Use get_online_cpus to avoid races involving CPU hotplug
      hwmon: (via-cputemp) Use get_online_cpus to avoid races involving CPU hotplug

commit 641f14560035bbb86500ea4b3a27ad27f034725b
Author: Silas Boyd-Wickizer <sbw@mit.edu>
Date:   Sun Sep 23 20:27:32 2012 +0200

    hwmon: (coretemp) Use get_online_cpus to avoid races involving CPU hotplug
    
    coretemp_init loops with for_each_online_cpu, adding platform_devices
    and sysfs interfaces, then calls register_hotcpu_notifier.  There is a
    race if a CPU is offlined or onlined after the loop, but before
    register_hotcpu_notifier.  The race might result in the absence of a
    platform_device+sysfs interface for an online CPU, or the presence of
    a platform_device+sysfs interface for an offline CPU.  A similar race
    occurs during coretemp_exit, after the module calls
    unregister_hotcpu_notifier, but before it unregisters all devices, a
    CPU might offline and a device for an offline CPU will exist for a
    short while.
    
    This fix surrounds for_each_online_cpu and register_hotcpu_notifier
    with get_online_cpus+put_online_cpus; and surrounds
    unregister_hotcpu_notifier and device unregistering with
    get_online_cpus+put_online_cpus.
    
    Build tested.
    
    Signed-off-by: Silas Boyd-Wickizer <sbw@mit.edu>
    Signed-off-by: Jean Delvare <khali@linux-fr.org>

commit 1ec3ddfd27a77db55b8c0e80bcd27c656473fb96
Author: Silas Boyd-Wickizer <sbw@mit.edu>
Date:   Sun Sep 23 20:27:32 2012 +0200

    hwmon: (via-cputemp) Use get_online_cpus to avoid races involving CPU hotplug
    
    via_cputemp_init loops with for_each_online_cpu, adding
    platform_devices, then calls register_hotcpu_notifier.  If a CPU is
    offlined between the loop and register_hotcpu_notifier, then later
    onlined, via_cputemp_device_add will attempt to add platform devices
    with the same ID.  A similar race occurs during via_cputemp_exit,
    after the module calls unregister_hotcpu_notifier, a CPU might offline
    and a device will exist for a CPU that is offline.
    
    This fix surrounds for_each_online_cpu and register_hotcpu_notifier
    with get_online_cpus+put_online_cpus; and surrounds
    unregister_hotcpu_notifier and device unregistering with
    get_online_cpus+put_online_cpus.
    
    Build tested.
    
    Signed-off-by: Silas Boyd-Wickizer <sbw@mit.edu>
    Acked-by: Harald Welte <laforge@gnumonks.org>
    Signed-off-by: Jean Delvare <khali@linux-fr.org>

commit 429227bbe55647aa42f8f63cac61e4544e248629
Author: Silas Boyd-Wickizer <sbw@mit.edu>
Date:   Fri Aug 3 12:34:50 2012 -0700

    Use get_online_cpus to avoid races involving CPU hotplug
    
    If arch/x86/kernel/cpuid.c is a module, a CPU might offline or online
    between the for_each_online_cpu() loop and the call to
    register_hotcpu_notifier in cpuid_init or the call to
    unregister_hotcpu_notifier in cpuid_exit.  The potential races can
    lead to leaks/duplicates, attempts to destroy non-existant devices, or
    random pointer dereferences.
    
    For example, in cpuid_exit if:
    
            for_each_online_cpu(cpu)
                    cpuid_device_destroy(cpu);
            class_destroy(cpuid_class);
            __unregister_chrdev(CPUID_MAJOR, 0, NR_CPUS, "cpu/cpuid");
            <----- CPU onlines
            unregister_hotcpu_notifier(&cpuid_class_cpu_notifier);
    
    the hotcpu notifier will attempt to create a device for the
    cpuid_class, which the module already destroyed.
    
    This fix surrounds for_each_online_cpu and register_hotcpu_notifier or
    unregister_hotcpu_notifier with get_online_cpus+put_online_cpus.
    
    Tested on a VM.
    
    Signed-off-by: Silas Boyd-Wickizer <sbw@mit.edu>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit a2db672aa305a045404615e5222ba681bab6cf58
Author: Silas Boyd-Wickizer <sbw@mit.edu>
Date:   Fri Aug 3 12:33:27 2012 -0700

    Use get_online_cpus to avoid races involving CPU hotplug
    
    If arch/x86/kernel/msr.c is a module, a CPU might offline or online
    between the for_each_online_cpu(i) loop and the call to
    register_hotcpu_notifier in msr_init or the call to
    unregister_hotcpu_notifier in msr_exit. The potential races can lead
    to leaks/duplicates, attempts to destroy non-existant devices, or
    random pointer dereferences.
    
    For example, in msr_init if:
    
            for_each_online_cpu(i) {
                    err = msr_device_create(i);
                    if (err != 0)
                            goto out_class;
            }
            <----- CPU offlines
            register_hotcpu_notifier(&msr_class_cpu_notifier);
    
    and the CPU never onlines before msr_exit, then the module will never
    call msr_device_destroy for the associated CPU.
    
    This fix surrounds for_each_online_cpu and register_hotcpu_notifier or
    unregister_hotcpu_notifier with get_online_cpus+put_online_cpus.
    
    Tested on a VM.
    
    Signed-off-by: Silas Boyd-Wickizer <sbw@mit.edu>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 1331e7a1bbe1f11b19c4327ba0853bee2a606543
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Aug 2 17:43:50 2012 -0700

    rcu: Remove _rcu_barrier() dependency on __stop_machine()
    
    Currently, _rcu_barrier() relies on preempt_disable() to prevent
    any CPU from going offline, which in turn depends on CPU hotplug's
    use of __stop_machine().
    
    This patch therefore makes _rcu_barrier() use get_online_cpus() to
    block CPU-hotplug operations.  This has the added benefit of removing
    the need for _rcu_barrier() to adopt callbacks:  Because CPU-hotplug
    operations are excluded, there can be no callbacks to adopt.  This
    commit simplifies the code accordingly.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit b612a85792192b70e6497619521772c38ace758e
Merge: ea832c41dacb 3aec092eed5d
Author: Olof Johansson <olof@lixom.net>
Date:   Thu Sep 20 21:27:41 2012 -0700

    Merge branch 'next/soc' into next/multiplatform
    
    * next/soc: (50 commits)
      ARM: OMAP: AM33xx hwmod: fixup SPI after platform_data move
      MAINTAINERS: add an entry for the BCM2835 ARM sub-architecture
      ARM: bcm2835: instantiate console UART
      ARM: bcm2835: add stub clock driver
      ARM: bcm2835: add system timer
      ARM: bcm2835: add interrupt controller driver
      ARM: add infra-structure for BCM2835 and Raspberry Pi
      ARM: tegra20: add CPU hotplug support
      ARM: tegra30: add CPU hotplug support
      ARM: tegra: clean up the common assembly macros into sleep.h
      ARM: tegra: replace the CPU CAR access code by tegra_cpu_car_ops
      ARM: tegra: introduce tegra_cpu_car_ops structures
      ARM: Tegra: Add smp_twd clock for Tegra20
      ARM: AM33XX: clock: Add dcan clock aliases for device-tree
      ARM: OMAP2+: dpll: Add missing soc_is_am33xx() check for common functions
      ARM: OMAP: omap_device: idle devices with no driver bound
      ARM: OMAP: omap_device: don't attempt late suspend if no driver bound
      ARM: OMAP: omap_device: keep track of driver bound status
      ARM: OMAP3+: hwmod: Add AM33XX HWMOD data
      ARM: OMAP2+: hwmod: Hook-up am33xx support in omap_hwmod framework
      ...
    
    Change/remove conflict in arch/arm/mach-ux500/clock.c resolved.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>

commit eab6d82843ee1df244f8847d1bf8bb89160ec4aa
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Sep 18 09:59:22 2012 -0700

    workqueue: WORKER_REBIND is no longer necessary for busy rebinding
    
    Because the old unbind/rebinding implementation wasn't atomic w.r.t.
    GCWQ_DISASSOCIATED manipulation which is protected by
    global_cwq->lock, we had to use two flags, WORKER_UNBOUND and
    WORKER_REBIND, to avoid incorrectly losing all NOT_RUNNING bits with
    back-to-back CPU hotplug operations; otherwise, completion of
    rebinding while another unbinding is in progress could clear UNBIND
    prematurely.
    
    Now that both unbind/rebinding are atomic w.r.t. GCWQ_DISASSOCIATED,
    there's no need to use two flags.  Just one is enough.  Don't use
    WORKER_REBIND for busy rebinding.
    
    tj: Updated description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 6c1423ba5dbdab45bcd8c1bc3bc6e07fe3f6a470
Merge: 136b5721d75a 960bd11bf2da
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Sep 17 16:07:34 2012 -0700

    Merge branch 'for-3.6-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq into for-3.7
    
    This merge is necessary as Lai's CPU hotplug restructuring series
    depends on the CPU hotplug bug fixes in for-3.6-fixes.
    
    The merge creates one trivial conflict between the following two
    commits.
    
     96e65306b8 "workqueue: UNBOUND -> REBIND morphing in rebind_workers() should be atomic"
     e2b6a6d570 "workqueue: use system_highpri_wq for highpri workers in rebind_workers()"
    
    Both add local variable definitions to the same block and can be
    merged in any order.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit cccc277ba840f32d252a785ab1df686fdc0f49f5
Merge: ea2abb670b71 453689e407f2
Author: Olof Johansson <olof@lixom.net>
Date:   Sun Sep 16 18:33:23 2012 -0700

    Merge tag 'tegra-for-3.7-cpu-hotplug' of git://git.kernel.org/pub/scm/linux/kernel/git/swarren/linux-tegra into next/soc
    
    From Stephen Warren:
    
    ARM: tegra: implement CPU hotplug
    
    This branch implements CPU hot-plugging support for both Tegra20 and
    Tegra30. Portions of the implementation are contained in the clock
    driver, hence this branch is based on the common clock conversion in
    order to avoid duplicating work.
    
    By Joseph Lo
    via Stephen Warren
    * tag 'tegra-for-3.7-cpu-hotplug' of git://git.kernel.org/pub/scm/linux/kernel/git/swarren/linux-tegra:
      ARM: tegra20: add CPU hotplug support
      ARM: tegra30: add CPU hotplug support
      ARM: tegra: clean up the common assembly macros into sleep.h
      ARM: tegra: replace the CPU CAR access code by tegra_cpu_car_ops
      ARM: tegra: introduce tegra_cpu_car_ops structures

commit 2d8b21d95f44989e09fd9b36ca9f061ad5bc567e
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Sep 8 13:15:22 2011 +0100

    ARM: SoC: convert spear13xx to SMP operations
    
    Convert the spear13xx platform to use struct smp_operations to provide
    its SMP and CPU hotplug operations.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Shiraz Hashim <shiraz.hashim@st.com>
    Cc: spear-devel@list.st.com
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit e4f2d97920f2256e5af035281e8ac35030493bf8
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 8 13:15:22 2011 +0100

    ARM: SoC: convert imx6q to SMP operations
    
    Convert the imx6q platform to use struct smp_operations to provide
    its SMP and CPU hotplug operations.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Cc: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 7ad71b61e744e7c565eec9e7aa734b0c42d93b16
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Sep 5 14:36:18 2012 +0000

    ARM: SoC: convert highbank to SMP operations
    
    Convert the highbank platform to use struct smp_operations to provide
    its SMP and CPU hotplug operations.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Rob Herring <rob.herring@calxeda.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit a62580e58065dc00430b16ced6e7a00837b8323f
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 8 13:15:22 2011 +0100

    ARM: SoC: convert shmobile SMP to SMP operations
    
    Convert shmobile SMP platforms to use struct smp_operations to provide
    their SMP and CPU hotplug operations.
    
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 5ac21a943e4052ef6743b09b6a06fbb683a3519d
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 8 13:15:22 2011 +0100

    ARM: SoC: convert ux500 to SMP operations
    
    Convert ux500 platforms to use struct smp_operations to provide
    their SMP and CPU hotplug operations.
    
    Cc: Linus Walleij <linus.walleij@stericsson.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: srinidhi kasagar <srinidhi.kasagar@stericsson.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 44ea349f5b7e0b4865de9ca6b4437c746eede40c
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 8 13:15:22 2011 +0100

    ARM: SoC: convert MSM to SMP operations
    
    Convert MSM SMP platforms to use struct smp_operations to provide
    their SMP and CPU hotplug operations.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Cc: David Brown <davidb@codeaurora.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 06853ae475e1386d6d11d139ba7bf2c97b3d1768
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 8 13:15:22 2011 +0100

    ARM: SoC: convert Exynos4 to SMP operations
    
    Convert Exynos4 to use struct smp_operations to provide its SMP
    and CPU hotplug operations.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Kyungmin Park <kyungmin.park@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Acked-by: Kukjin Kim <kgene.kim@samsung.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 453689e407f2b7c0a72a2e6fb2ef84c20475773b
Author: Joseph Lo <josephl@nvidia.com>
Date:   Thu Aug 16 17:31:52 2012 +0800

    ARM: tegra20: add CPU hotplug support
    
    Hotplug function put CPU in offline or online mode at runtime.
    When the CPU been put into offline, it was been clock gated. The
    offline CPU can be power gated, when the remaining CPU goes into
    LP2.
    
    Based on the worked by:
    Colin Cross <ccross@android.com>
    Gary King <gking@nvidia.com>
    
    Signed-off-by: Joseph Lo <josephl@nvidia.com>
    Signed-off-by: Stephen Warren <swarren@nvidia.com>

commit 59b0f6825c15d24859e22b1024440ae2a094983e
Author: Joseph Lo <josephl@nvidia.com>
Date:   Thu Aug 16 17:31:51 2012 +0800

    ARM: tegra30: add CPU hotplug support
    
    Hotplug function put CPUs in offline or online state at runtime.
    When the CPU been put in the offline state, it was been clock and
    power gated. Except primary CPU other CPUs can be hotplugged.
    
    Based on the work by:
    Scott Williams <scwilliams@nvidia.com>
    Colin Cross <ccross@android.com>
    Gary King <gking@nvidia.com>
    
    Signed-off-by: Joseph Lo <josephl@nvidia.com>
    Signed-off-by: Stephen Warren <swarren@nvidia.com>

commit a17257322f5e6ca376c15908b55423369274fcad
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 8 13:15:22 2011 +0100

    ARM: SoC: convert Tegra to SMP operations
    
    Convert Tegra to use struct smp_operations to provide its SMP
    and CPU hotplug operations.
    
    Tested on Harmony.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Stephen Warren <swarren@nvidia.com>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 06915321e7935d2eb778f0a7f333b2603c1404df
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 8 13:15:22 2011 +0100

    ARM: SoC: convert OMAP4 to SMP operations
    
    Convert OMAP4 to use struct smp_operations to provide its SMP
    and CPU hotplug operations.
    
    Tested on both Panda and IGEPv2 (MULTI_OMAP kernel)
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 3695adc2fdaf3ad1881e0dd3e3422e5e141abd7d
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 8 13:15:22 2011 +0100

    ARM: SoC: convert VExpress/RealView to SMP operations
    
    Convert both Realview and VExpress to use struct smp_operations to
    provide their SMP and CPU hotplug operation.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Nicolas Pitre <nico@fluxnic.net>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 0bd1189e239c76eb3a50e458548fbe7e4a5dfff1
Merge: 274a2f5ddb3e ee378aa49b59
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 12 07:16:54 2012 +0800

    Merge branch 'for-3.6-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    Pull workqueue fixes from Tejun Heo:
     "It's later than I'd like but well the timing just didn't work out this
      time.
    
      There are three bug fixes.  One from before 3.6-rc1 and two from the
      new CPU hotplug code.  Kudos to Lai for discovering all of them and
      providing fixes.
    
       * Atomicity bug when clearing a flag and setting another.  The two
         operation should have been atomic but wasn't.  This bug has existed
         for a long time but is unlikely to have actually happened.  Fix is
         safe.  Marked for -stable.
    
       * If CPU hotplug cycles happen back-to-back before workers finish the
         previous cycle, the states could get out of sync and it could get
         stuck.  Fixed by waiting for workers to complete before finishing
         hotplug cycle.
    
       * While CPU hotplug is in progress, idle workers could be depleted
         which can then lead to deadlock.  I think both happening together
         is highly unlikely but still better to fix it and the fix isn't too
         scary.
    
      There's another workqueue related regression which reported a few days
      ago:
    
        https://bugzilla.kernel.org/show_bug.cgi?id=47301
    
      It's a bit of head scratcher but there is a semi-reliable reproduce
      case, so I'm hoping to resolve it soonish."
    
    * 'for-3.6-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq:
      workqueue: fix possible idle worker depletion across CPU hotplug
      workqueue: restore POOL_MANAGING_WORKERS
      workqueue: fix possible deadlock in idle worker rebinding
      workqueue: move WORKER_REBIND clearing in rebind_workers() to the end of the function
      workqueue: UNBOUND -> REBIND morphing in rebind_workers() should be atomic

commit ee378aa49b594da9bda6a2c768cc5b2ad585f911
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Mon Sep 10 10:03:44 2012 -0700

    workqueue: fix possible idle worker depletion across CPU hotplug
    
    To simplify both normal and CPU hotplug paths, worker management is
    prevented while CPU hoplug is in progress.  This is achieved by CPU
    hotplug holding the same exclusion mechanism used by workers to ensure
    there's only one manager per pool.
    
    If someone else seems to be performing the manager role, workers
    proceed to execute work items.  CPU hotplug using the same mechanism
    can lead to idle worker depletion because all workers could proceed to
    execute work items while CPU hotplug is in progress and CPU hotplug
    itself wouldn't actually perform the worker management duty - it
    doesn't guarantee that there's an idle worker left when it releases
    management.
    
    This idle worker depletion, under extreme circumstances, can break
    forward-progress guarantee and thus lead to deadlock.
    
    This patch fixes the bug by using separate mechanisms for manager
    exclusion among workers and hotplug exclusion.  For manager exclusion,
    POOL_MANAGING_WORKERS which was restored by the previous patch is
    used.  pool->manager_mutex is now only used for exclusion between the
    elected manager and CPU hotplug.  The elected manager won't proceed
    without holding pool->manager_mutex.
    
    This ensures that the worker which won the manager position can't skip
    managing while CPU hotplug is in progress.  It will block on
    manager_mutex and perform management after CPU hotplug is complete.
    
    Note that hotplug may happen while waiting for manager_mutex.  A
    manager isn't either on idle or busy list and thus the hoplug code
    can't unbind/rebind it.  Make the manager handle its own un/rebinding.
    
    tj: Updated comment and description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 552a37e9360a293cd20e7f8ff1fb326a244c5f1e
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Mon Sep 10 10:03:33 2012 -0700

    workqueue: restore POOL_MANAGING_WORKERS
    
    This patch restores POOL_MANAGING_WORKERS which was replaced by
    pool->manager_mutex by 6037315269 "workqueue: use mutex for global_cwq
    manager exclusion".
    
    There's a subtle idle worker depletion bug across CPU hotplug events
    and we need to distinguish an actual manager and CPU hotplug
    preventing management.  POOL_MANAGING_WORKERS will be used for the
    former and manager_mutex the later.
    
    This patch just lays POOL_MANAGING_WORKERS on top of the existing
    manager_mutex and doesn't introduce any synchronization changes.  The
    next patch will update it.
    
    Note that this patch fixes a non-critical anomaly where
    too_many_workers() may return %true spuriously while CPU hotplug is in
    progress.  While the issue could schedule idle timer spuriously, it
    didn't trigger any actual misbehavior.
    
    tj: Rewrote patch description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit ec58815ab0409a921a7c9744eb4ca44866b14d71
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Sep 4 23:16:32 2012 -0700

    workqueue: fix possible deadlock in idle worker rebinding
    
    Currently, rebind_workers() and idle_worker_rebind() are two-way
    interlocked.  rebind_workers() waits for idle workers to finish
    rebinding and rebound idle workers wait for rebind_workers() to finish
    rebinding busy workers before proceeding.
    
    Unfortunately, this isn't enough.  The second wait from idle workers
    is implemented as follows.
    
            wait_event(gcwq->rebind_hold, !(worker->flags & WORKER_REBIND));
    
    rebind_workers() clears WORKER_REBIND, wakes up the idle workers and
    then returns.  If CPU hotplug cycle happens again before one of the
    idle workers finishes the above wait_event(), rebind_workers() will
    repeat the first part of the handshake - set WORKER_REBIND again and
    wait for the idle worker to finish rebinding - and this leads to
    deadlock because the idle worker would be waiting for WORKER_REBIND to
    clear.
    
    This is fixed by adding another interlocking step at the end -
    rebind_workers() now waits for all the idle workers to finish the
    above WORKER_REBIND wait before returning.  This ensures that all
    rebinding steps are complete on all idle workers before the next
    hotplug cycle can happen.
    
    This problem was diagnosed by Lai Jiangshan who also posted a patch to
    fix the issue, upon which this patch is based.
    
    This is the minimal fix and further patches are scheduled for the next
    merge window to simplify the CPU hotplug path.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Original-patch-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    LKML-Reference: <1346516916-1991-3-git-send-email-laijs@cn.fujitsu.com>

commit 375f561a4131a0f501c8845a2a20f2ca1abc8f7a
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Jul 26 18:51:09 2012 +0000

    powerpc/powernv: Always go into nap mode when CPU is offline
    
    The CPU hotplug code for the powernv platform currently only puts
    offline CPUs into nap mode if the powersave_nap variable is set.
    However, HV-style KVM on this platform requires secondary CPU threads
    to be offline and in nap mode.  Since we know nap mode works just
    fine on all POWER7 machines, and the only machines that support the
    powernv platform are POWER7 machines, this changes the code to
    always put offline CPUs into nap mode, regardless of powersave_nap.
    Powersave_nap still controls whether or not CPUs go into nap mode
    when idle, as before.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 19e4ab54e7f5fe46a3c931afa7e786f11d57b558
Merge: 0d7614f09c1e fa8bbb13ab49
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Aug 13 16:45:48 2012 +0200

    Merge branch 'for-arm-soc' of git://git.kernel.org/pub/scm/linux/kernel/git/cooloney/linux-leds into next/drivers
    
    From Bryan Wu <bryan.wu@canonical.com>:
    
    Based on Linus Walleij's ARM LED consolidation work, this patchset introduce a
    new generic led trigger for CPU not only for ARM but also for others.
    
    For enabling CPU idle event, CPU arch code should call ledtrig_cpu() stub to
    trigger idle start or idle end event.
    
    These patches convert old style LED driver in arch/arm to gpio_led or new led
    driver interface. Against 3.5 release and build successfully for all the machines.
    
    Test ledtrig-cpu driver on OMAP4 Panda board.
    
    v9 --> v10
     * fix compiling issue on versatile_defconfig reported by Russell King
     * rebase to 3.5 kernel and move patches to new git tree
    
    v8 --> v9:
     * use mutex to replace rw_sema pointed out by Tim Gardner
     * add a new struct led_trigger_cpu
     * add lock_is_inited to record mutex lock initialization
    
    v6 --> v7:
     * add a patch to unify the led-trigger name
     * fix some typo pointed
     * use BUG_ON to detect CPU numbers during building stage
    
    v5 --> v6:
     * replace  __get_cpu_var() to per_cpu()
     * remove smp_processor_id() which is wrong with for_each_possible_cpu()
     * test on real OMAP4 Panda board
     * add comments about CPU hotplug in the CPU LED trigger driver
    
    v4 --> v5:
     * rebase all the patches on top of latest linux-next
     * replace on_each_cpu() with for_each_possible_cpu()
     * add some description of ledtrig_cpu() API
     * remove old leds code from driver nwflash.c, which should use a new led trigger then
     * this trigger driver can be built as module now
    
    v3 --> v4:
     * fix a typo pointed by Jochen Friedrich
     * fix some building errors
     * add Reviewed-by and Tested-by into patch log
    
    v2 --> v3:
     * almost rewrote the whole ledtrig-cpu driver, which is more simple
     * every CPU will have a per-CPU trigger
     * cpu trigger can be assigned to any leds
     * fix a lockdep issue in led-trigger common code
     * other fix according to review
    
    v1 --> v2:
     * remove select operations in Kconfig of every machines
     * add back supporting of led in core module of mach-integrator
     * solidate name scheme in ledtrig-cpu.c
     * add comments of CPU_LED_* cpu led events
     * fold patches of RealView and Versatile together
     * add machine_is_ check during assabet led driver init
     * add some Acked-by in patch logs
     * remove code for simpad machine in machine-sa11000, since Jochen Friedrich
       introduced gpiolib and gpio-led driver for simpad
     * on Assabet and Netwinder machine, LED operations is reversed like:
       setting bit means turn off leds
       clearing bit means turn on leds
     * add a new function to read CM_CTRL register for led driver
    
    * 'for-arm-soc' of git://git.kernel.org/pub/scm/linux/kernel/git/cooloney/linux-leds:
      ARM: use new LEDS CPU trigger stub to replace old one
      ARM: mach-sa1100: retire custom LED code
      ARM: mach-omap1: retire custom LED code
      ARM: mach-pnx4008: remove including old leds event API header file
      ARM: plat-samsung: remove including old leds event API header file
      ARM: mach-pxa: retire custom LED code
      char: nwflash: remove old led event code
      ARM: mach-footbridge: retire custom LED code
      ARM: mach-ebsa110: retire custom LED code
      ARM: mach-clps711x: retire custom LED code of P720T machine
      ARM: mach-integrator: retire custom LED code
      ARM: mach-integrator: move CM_CTRL to header file for accessing by other functions
      ARM: mach-orion5x: convert custom LED code to gpio_led and LED CPU trigger
      ARM: mach-shark: retire custom LED code
      ARM: mach-ks8695: remove leds driver, since nobody use it
      ARM: mach-realview and mach-versatile: retire custom LED code
      ARM: at91: convert old leds drivers to gpio_led and led_trigger drivers
      led-triggers: create a trigger for CPU activity
    
    Conflicts:
            arch/arm/mach-clps711x/p720t.c
            arch/arm/mach-sa1100/leds-cerf.c
            arch/arm/mach-sa1100/leds-lart.c
    
    Let's hope this is the last time we pull this and it doesn't cause
    more trouble. I have verified that version 10 causes no build
    warnings or errors any more, and the patches still look good.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit 300d3739e873d50d4c6e3656f89007a217fb1d29
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Tue Aug 7 13:50:22 2012 +0200

    Revert "NMI watchdog: fix for lockup detector breakage on resume"
    
    Revert commit 45226e9 (NMI watchdog: fix for lockup detector breakage
    on resume) which breaks resume from system suspend on my SH7372
    Mackerel board (by causing a NULL pointer dereference to happen) and
    is generally wrong, because it abuses the CPU hotplug functionality
    in a shamelessly blatant way.
    
    The original issue should be addressed through appropriate syscore
    resume callback instead.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 1ca0049f2c8b28611f30798aa75858763c9fcbea
Merge: ddc5057c1c1f eaf4ce6c5fed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 3 10:59:36 2012 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar:
     "Various fixes"
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86-64, kcmp: The kcmp system call can be common
      arch/x86/kernel/kdebugfs.c: Ensure a consistent return value in error case
      x86/mce: Add quirk for instruction recovery on Sandy Bridge processors
      x86/mce: Move MCACOD defines from mce-severity.c to <asm/mce.h>
      x86/ioapic: Fix NULL pointer dereference on CPU hotplug after disabling irqs
      x86, nops: Missing break resulting in incorrect selection on Intel
      x86: CONFIG_CC_STACKPROTECTOR=y is no longer experimental

commit d89dffa976bcd13fd87eb76e02e3b71c3a7868e3
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Mon Jul 30 14:43:17 2012 -0700

    fault-injection: add selftests for cpu and memory hotplug
    
    This adds two selftests
    
    * tools/testing/selftests/cpu-hotplug/on-off-test.sh is testing script
    for CPU hotplug
    
    1. Online all hot-pluggable CPUs
    2. Offline all hot-pluggable CPUs
    3. Online all hot-pluggable CPUs again
    4. Exit if cpu-notifier-error-inject.ko is not available
    5. Offline all hot-pluggable CPUs in preparation for testing
    6. Test CPU hot-add error handling by injecting notifier errors
    7. Online all hot-pluggable CPUs in preparation for testing
    8. Test CPU hot-remove error handling by injecting notifier errors
    
    * tools/testing/selftests/memory-hotplug/on-off-test.sh is doing the
    similar thing for memory hotplug.
    
    1. Online all hot-pluggable memory
    2. Offline 10% of hot-pluggable memory
    3. Online all hot-pluggable memory again
    4. Exit if memory-notifier-error-inject.ko is not available
    5. Offline 10% of hot-pluggable memory in preparation for testing
    6. Test memory hot-add error handling by injecting notifier errors
    7. Online all hot-pluggable memory in preparation for testing
    8. Test memory hot-remove error handling by injecting notifier errors
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Suggested-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Greg KH <greg@kroah.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Cc: Dave Jones <davej@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 79071638ce655c1f78a50d05c7dae0ad04a3e92a
Merge: 44a6b8442190 8323f26ce342
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 26 13:08:01 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The biggest change is a performance improvement on SMP systems:
    
      | 4 socket 40 core + SMT Westmere box, single 30 sec tbench
      | runs, higher is better:
      |
      | clients     1       2       4        8       16       32       64      128
      |..........................................................................
      | pre        30      41     118      645     3769     6214    12233    14312
      | post      299     603    1211     2418     4697     6847    11606    14557
      |
      | A nice increase in performance.
    
      which speedup is particularly noticeable on heavily interacting
      few-tasks workloads, so the changes should help desktop-style Xorg
      workloads and interactivity as well, on multi-core CPUs.
    
      There are also cpuset suspend behavior fixes/restructuring and various
      smaller tweaks."
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Fix race in task_group()
      sched: Improve balance_cpu() to consider other cpus in its group as target of (pinned) task
      sched: Reset loop counters if all tasks are pinned and we need to redo load balance
      sched: Reorder 'struct lb_env' members to reduce its size
      sched: Improve scalability via 'CPU buddies', which withstand random perturbations
      cpusets: Remove/update outdated comments
      cpusets, hotplug: Restructure functions that are invoked during hotplug
      cpusets, hotplug: Implement cpuset tree traversal in a helper function
      CPU hotplug, cpusets, suspend: Don't modify cpusets during suspend/resume
      sched/x86: Remove broken power estimation

commit 1d44b30f35a9873a65b320dd5300088fa995fd94
Author: Tomoki Sekiyama <tomoki.sekiyama.qu@hitachi.com>
Date:   Thu Jul 26 19:47:32 2012 +0900

    x86/ioapic: Fix NULL pointer dereference on CPU hotplug after disabling irqs
    
    In the current kernel, percpu variable `vector_irq' is not always
    cleared when a CPU is offlined. If the CPU that has the disabled
    irqs in vector_irq is hotplugged again, __setup_vector_irq()
    hits invalid irq vector and may crash.
    
    This bug can be reproduced as following;
    
     # echo 0 > /sys/devices/system/cpu/cpu7/online
     # modprobe -r some_driver_using_interrupts     # vector_irq@cpu7 uncleared
     # echo 1 > /sys/devices/system/cpu/cpu7/online # kernel may crash
    
    To fix this problem, this patch clears vector_irq in
    __fixup_irqs() when the CPU is offlined.
    
    This also reverts commit f6175f5bfb4c, which partially fixes
    this bug by clearing vector in __clear_irq_vector(). But in
    environments with IOMMU IRQ remapper, it could fail because
    cfg->domain doesn't contain offlined CPUs. With this patch, the
    fix in __clear_irq_vector() can be reverted because every
    vector_irq is already cleared in __fixup_irqs() on offlined CPUs.
    
    Signed-off-by: Tomoki Sekiyama <tomoki.sekiyama.qu@hitachi.com>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: yrl.pp-manager.tt@hitachi.com
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Alexander Gordeev <agordeev@redhat.com>
    Link: http://lkml.kernel.org/r/20120726104732.2889.19144.stgit@kvmdev
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a08489c569dc174cff97d2cb165aa81e3f1501cc
Merge: 08d9329c29ec 6fec10a1a586
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 24 17:46:16 2012 -0700

    Merge branch 'for-3.6' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    Pull workqueue changes from Tejun Heo:
     "There are three major changes.
    
       - WQ_HIGHPRI has been reimplemented so that high priority work items
         are served by worker threads with -20 nice value from dedicated
         highpri worker pools.
    
       - CPU hotplug support has been reimplemented such that idle workers
         are kept across CPU hotplug events.  This makes CPU hotplug cheaper
         (for PM) and makes the code simpler.
    
       - flush_kthread_work() has been reimplemented so that a work item can
         be freed while executing.  This removes an annoying behavior
         difference between kthread_worker and workqueue."
    
    * 'for-3.6' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq:
      workqueue: fix spurious CPU locality WARN from process_one_work()
      kthread_worker: reimplement flush_kthread_work() to allow freeing the work item being executed
      kthread_worker: reorganize to prepare for flush_kthread_work() reimplementation
      workqueue: simplify CPU hotplug code
      workqueue: remove CPU offline trustee
      workqueue: don't butcher idle workers on an offline CPU
      workqueue: reimplement CPU online rebinding to handle idle workers
      workqueue: drop @bind from create_worker()
      workqueue: use mutex for global_cwq manager exclusion
      workqueue: ROGUE workers are UNBOUND workers
      workqueue: drop CPU_DYING notifier operation
      workqueue: perform cpu down operations from low priority cpu_notifier()
      workqueue: reimplement WQ_HIGHPRI using a separate worker_pool
      workqueue: introduce NR_WORKER_POOLS and for_each_worker_pool()
      workqueue: separate out worker_pool flags
      workqueue: use @pool instead of @gcwq or @cpu where applicable
      workqueue: factor out worker_pool from global_cwq
      workqueue: don't use WQ_HIGHPRI for unbound workqueues

commit d35be8bab9b0ce44bed4b9453f86ebf64062721e
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 19:46:26 2012 +0530

    CPU hotplug, cpusets, suspend: Don't modify cpusets during suspend/resume
    
    In the event of CPU hotplug, the kernel modifies the cpusets' cpus_allowed
    masks as and when necessary to ensure that the tasks belonging to the cpusets
    have some place (online CPUs) to run on. And regular CPU hotplug is
    destructive in the sense that the kernel doesn't remember the original cpuset
    configurations set by the user, across hotplug operations.
    
    However, suspend/resume (which uses CPU hotplug) is a special case in which
    the kernel has the responsibility to restore the system (during resume), to
    exactly the same state it was in before suspend.
    
    In order to achieve that, do the following:
    
    1. Don't modify cpusets during suspend/resume. At all.
       In particular, don't move the tasks from one cpuset to another, and
       don't modify any cpuset's cpus_allowed mask. So, simply ignore cpusets
       during the CPU hotplug operations that are carried out in the
       suspend/resume path.
    
    2. However, cpusets and sched domains are related. We just want to avoid
       altering cpusets alone. So, to keep the sched domains updated, build
       a single sched domain (containing all active cpus) during each of the
       CPU hotplug operations carried out in s/r path, effectively ignoring
       the cpusets' cpus_allowed masks.
    
       (Since userspace is frozen while doing all this, it will go unnoticed.)
    
    3. During the last CPU online operation during resume, build the sched
       domains by looking up the (unaltered) cpusets' cpus_allowed masks.
       That will bring back the system to the same original state as it was in
       before suspend.
    
    Ultimately, this will not only solve the cpuset problem related to suspend
    resume (ie., restores the cpusets to exactly what it was before suspend, by
    not touching it at all) but also speeds up suspend/resume because we avoid
    running cpuset update code for every CPU being offlined/onlined.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120524141611.3692.20155.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 8db25e7891a47e03db6f04344a9c92be16e391bb
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 17 12:39:28 2012 -0700

    workqueue: simplify CPU hotplug code
    
    With trustee gone, CPU hotplug code can be simplified.
    
    * gcwq_claim/release_management() now grab and release gcwq lock too
      respectively and gained _and_lock and _and_unlock postfixes.
    
    * All CPU hotplug logic was implemented in workqueue_cpu_callback()
      which was called by workqueue_cpu_up/down_callback() for the correct
      priority.  This was because up and down paths shared a lot of logic,
      which is no longer true.  Remove workqueue_cpu_callback() and move
      all hotplug logic into the two actual callbacks.
    
    This patch doesn't make any functional changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>

commit 3ce63377305b694f53e7dd0c72907591c5344224
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 17 12:39:27 2012 -0700

    workqueue: don't butcher idle workers on an offline CPU
    
    Currently, during CPU offlining, after all pending work items are
    drained, the trustee butchers all workers.  Also, on CPU onlining
    failure, workqueue_cpu_callback() ensures that the first idle worker
    is destroyed.  Combined, these guarantee that an offline CPU doesn't
    have any worker for it once all the lingering work items are finished.
    
    This guarantee isn't really necessary and makes CPU on/offlining more
    expensive than needs to be, especially for platforms which use CPU
    hotplug for powersaving.
    
    This patch lets offline CPUs removes idle worker butchering from the
    trustee and let a CPU which failed onlining keep the created first
    worker.  The first worker is created if the CPU doesn't have any
    during CPU_DOWN_PREPARE and started right away.  If onlining succeeds,
    the rebind_workers() call in CPU_ONLINE will rebind it like any other
    workers.  If onlining fails, the worker is left alone till the next
    try.
    
    This makes CPU hotplugs cheaper by allowing global_cwqs to keep
    workers across them and simplifies code.
    
    Note that trustee doesn't re-arm idle timer when it's done and thus
    the disassociated global_cwq will keep all workers until it comes back
    online.  This will be improved by further patches.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>

commit 25511a477657884d2164f338341fa89652610507
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 17 12:39:27 2012 -0700

    workqueue: reimplement CPU online rebinding to handle idle workers
    
    Currently, if there are left workers when a CPU is being brough back
    online, the trustee kills all idle workers and scheduled rebind_work
    so that they re-bind to the CPU after the currently executing work is
    finished.  This works for busy workers because concurrency management
    doesn't try to wake up them from scheduler callbacks, which require
    the target task to be on the local run queue.  The busy worker bumps
    concurrency counter appropriately as it clears WORKER_UNBOUND from the
    rebind work item and it's bound to the CPU before returning to the
    idle state.
    
    To reduce CPU on/offlining overhead (as many embedded systems use it
    for powersaving) and simplify the code path, workqueue is planned to
    be modified to retain idle workers across CPU on/offlining.  This
    patch reimplements CPU online rebinding such that it can also handle
    idle workers.
    
    As noted earlier, due to the local wakeup requirement, rebinding idle
    workers is tricky.  All idle workers must be re-bound before scheduler
    callbacks are enabled.  This is achieved by interlocking idle
    re-binding.  Idle workers are requested to re-bind and then hold until
    all idle re-binding is complete so that no bound worker starts
    executing work item.  Only after all idle workers are re-bound and
    parked, CPU_ONLINE proceeds to release them and queue rebind work item
    to busy workers thus guaranteeing scheduler callbacks aren't invoked
    until all idle workers are ready.
    
    worker_rebind_fn() is renamed to busy_worker_rebind_fn() and
    idle_worker_rebind() for idle workers is added.  Rebinding logic is
    moved to rebind_workers() and now called from CPU_ONLINE after
    flushing trustee.  While at it, add CPU sanity check in
    worker_thread().
    
    Note that now a worker may become idle or the manager between trustee
    release and rebinding during CPU_ONLINE.  As the previous patch
    updated create_worker() so that it can be used by regular manager
    while unbound and this patch implements idle re-binding, this is safe.
    
    This prepares for removal of trustee and keeping idle workers across
    CPU hotplugs.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>

commit bc2ae0f5bb2f39e6db06a62f9d353e4601a332a1
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 17 12:39:27 2012 -0700

    workqueue: drop @bind from create_worker()
    
    Currently, create_worker()'s callers are responsible for deciding
    whether the newly created worker should be bound to the associated CPU
    and create_worker() sets WORKER_UNBOUND only for the workers for the
    unbound global_cwq.  Creation during normal operation is always via
    maybe_create_worker() and @bind is true.  For workers created during
    hotplug, @bind is false.
    
    Normal operation path is planned to be used even while the CPU is
    going through hotplug operations or offline and this static decision
    won't work.
    
    Drop @bind from create_worker() and decide whether to bind by looking
    at GCWQ_DISASSOCIATED.  create_worker() will also set WORKER_UNBOUND
    autmatically if disassociated.  To avoid flipping GCWQ_DISASSOCIATED
    while create_worker() is in progress, the flag is now allowed to be
    changed only while holding all manager_mutexes on the global_cwq.
    
    This requires that GCWQ_DISASSOCIATED is not cleared behind trustee's
    back.  CPU_ONLINE no longer clears DISASSOCIATED before flushing
    trustee, which clears DISASSOCIATED before rebinding remaining workers
    if asked to release.  For cases where trustee isn't around, CPU_ONLINE
    clears DISASSOCIATED after flushing trustee.  Also, now, first_idle
    has UNBOUND set on creation which is explicitly cleared by CPU_ONLINE
    while binding it.  These convolutions will soon be removed by further
    simplification of CPU hotplug path.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: "Rafael J. Wysocki" <rjw@sisk.pl>

commit 0b35d326f813a654f0cd40f513bd95e3935921c1
Merge: 106544d81d88 eeaaa96a3a21
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 8 09:26:55 2012 -0700

    Merge branch 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 fixes from Ingo Molnar.
    
    * 'x86-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/nmi: Fix section mismatch warnings on 32-bit
      x86/uv: Fix UV2 BAU legacy mode
      x86/mm: Only add extra pages count for the first memory range during pre-allocation early page table space
      x86, efi stub: Add .reloc section back into image
      x86/ioapic: Fix NULL pointer dereference on CPU hotplug after disabling irqs
      x86/reboot: Fix a warning message triggered by stop_other_cpus()
      x86/intel/moorestown: Change intel_scu_devices_create() to __devinit
      x86/numa: Set numa_nodes_parsed at acpi_numa_memory_affinity_init()
      x86/gart: Fix kmemleak warning
      x86: mce: Add the dropped timer interval init back
      x86/mce: Fix the MCE poll timer logic

commit 8f5af6f1f2d09fe5eac86a5dc1731a5917c1503a
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Fri May 4 08:31:53 2012 -0700

    rcu: RCU_FAST_NO_HZ detection of callback adoption
    
    In the present implementations of CPU hotplug, the outgoing CPU is
    guaranteed to run its stop-machine process on the way out, which
    will guarantee that RCU_FAST_NO_HZ forces the CPU out of dyntick-idle
    mode.
    
    However, new versions of CPU hotplug might not work this way.  This
    commit therefore removes this design constraint by explicitly notifying
    CPUs when they adopt non-lazy RCU callbacks.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Tested-by: Pascal Chapperon <pascal.chapperon@wanadoo.fr>

commit f6175f5bfb4c9f2ed32758c95f765b529b1a7f15
Author: Tomoki Sekiyama <tomoki.sekiyama.qu@hitachi.com>
Date:   Mon May 28 18:09:18 2012 +0900

    x86/ioapic: Fix NULL pointer dereference on CPU hotplug after disabling irqs
    
    In current Linux, percpu variable `vector_irq' is not cleared on
    offlined cpus while disabling devices' irqs. If the cpu that has
    the disabled irqs in vector_irq is hotplugged,
    __setup_vector_irq() hits invalid irq vector and may crash.
    
    This bug can be reproduced as following;
    
      # echo 0 > /sys/devices/system/cpu/cpu7/online
      # modprobe -r some_driver_using_interrupts      # vector_irq@cpu7 uncleared
      # echo 1 > /sys/devices/system/cpu/cpu7/online  # kernel may crash
    
    This patch fixes this bug by clearing vector_irq in
    __clear_irq_vector() even if the cpu is offlined.
    
    Signed-off-by: Tomoki Sekiyama <tomoki.sekiyama.qu@hitachi.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: yrl.pp-manager.tt@hitachi.com
    Cc: ltc-kernel@ml.yrl.intra.hitachi.co.jp
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Alexander Gordeev <agordeev@redhat.com>
    Link: http://lkml.kernel.org/r/4FC340BE.7080101@hitachi.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 51af3b9202ccffe0476899d5c29f0ae7e6bfdcea
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Wed May 23 20:25:21 2012 -0600

    ACPI: Add _OST support for ACPI CPU hotplug
    
    Changed acpi_processor_hotplug_notify() to call ACPI _OST method
    when ACPI CPU hotplug operation has completed.
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

commit 275c58d77062bbb85dbeb3843ba04f34aa50cf8e
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Wed May 23 20:25:19 2012 -0600

    ACPI: Add an interface to evaluate _OST
    
    Added acpi_evaluate_hotplug_opt(). All ACPI hotplug handlers must call
    this function when evaluating _OST for hotplug operations. If the
    platform does not support _OST, this function returns AE_NOT_FOUND and
    has no effect on the platform.
    
    ACPI_HOTPLUG_OST is defined when all relevant ACPI hotplug operations,
    such as CPU, memory and container hotplug, are enabled. This assures
    consistent behavior among the hotplug operations with regarding the
    _OST support. When ACPI_HOTPLUG_OST is not defined, this function is
    a no-op.
    
    ACPI PCI hotplug is not enhanced to support _OST at this time since it
    is a legacy method being replaced by PCIe native hotplug. _OST support
    for ACPI PCI hotplug may be added in future if necessary.
    
    Some platforms may require the OS to support _OST in order to support
    ACPI hotplug operations. For example, if a platform has the management
    console where user can request a hotplug operation from, this _OST
    support would be required for the management console to show the result
    of the hotplug request to user.
    
    Added macro definitions of _OST source events and status codes.
    Also renamed OSC_SB_CPUHP_OST_SUPPORT to OSC_SB_HOTPLUG_OST_SUPPORT
    since this _OSC bit is not specific to CPU hotplug. This bit is
    defined in Table 6-147 of ACPI 5.0 as follows.
    
      Bits:       3
      Field Name: Insertion / Ejection _OST Processing Support
      Definition: This bit is set if OSPM will evaluate the _OST
                  object defined under a device when processing
                  insertion and ejection source event codes.
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

commit 73863ab028579ed98c4f1f36d016536b1b415344
Author: Anton Vorontsov <anton.vorontsov@linaro.org>
Date:   Thu May 31 16:26:23 2012 -0700

    powerpc: use clear_tasks_mm_cpumask()
    
    Current CPU hotplug code has some task->mm handling issues:
    
    1. Working with task->mm w/o getting mm or grabing the task lock is
       dangerous as ->mm might disappear (exit_mm() assigns NULL under
       task_lock(), so tasklist lock is not enough).
    
       We can't use get_task_mm()/mmput() pair as mmput() might sleep,
       so we must take the task lock while handle its mm.
    
    2. Checking for process->mm is not enough because process' main
       thread may exit or detach its mm via use_mm(), but other threads
       may still have a valid mm.
    
       To fix this we would need to use find_lock_task_mm(), which would
       walk up all threads and returns an appropriate task (with task
       lock held).
    
    clear_tasks_mm_cpumask() has all the issues fixed, so let's use it.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit f8dccff375994b99f448841c75f5a180d45262c6
Author: Jacob Shin <jacob.shin@amd.com>
Date:   Wed Apr 27 13:32:11 2011 -0500

    CPU hotplug, re-create sysfs directory and symlinks
    
    commit 27ecddc2a9f99ce4ac9a59a0acd77f7100b6d034 upstream.
    
    When we discover CPUs that are affected by each other's
    frequency/voltage transitions, the first CPU gets a sysfs directory
    created, and rest of the siblings get symlinks. Currently, when we
    hotplug off only the first CPU, all of the symlinks and the sysfs
    directory gets removed. Even though rest of the siblings are still
    online and functional, they are orphaned, and no longer governed by
    cpufreq.
    
    This patch, given the above scenario, creates a sysfs directory for
    the first sibling and symlinks for the rest of the siblings.
    
    Please note the recursive call, it was rather too ugly to roll it
    out. And the removal of redundant NULL setting (it is already taken
    care of near the top of the function).
    
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Acked-by: Mark Langsdorf <mark.langsdorf@amd.com>
    Reviewed-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

commit 568b44559d7ca269d367e694c74eb4436e7e3ccf
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Wed May 16 00:32:37 2012 +0530

    mn10300/CPU hotplug: Add missing call to notify_cpu_starting()
    
    The scheduler depends on receiving the CPU_STARTING notification, without
    which we end up into a lot of trouble. So add the missing call to
    notify_cpu_starting() in the bringup code.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit ec2e0f9811a2c667d06feecb413c57f74c6b84f4
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Wed May 16 00:32:17 2012 +0530

    parisc/CPU hotplug: Add missing call to notify_cpu_starting()
    
    The scheduler depends on receiving the CPU_STARTING notification, without
    which we end up into a lot of trouble. So add the missing call to
    notify_cpu_starting() in the bringup code.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-and-Tested-by: Mikulas Patocka <mpatocka@redhat.com>
    Acked-and-Tested-by: Tobias Ulmer <tobiasu@tmux.org>
    Tested-by: John David Anglin <dave.anglin@bell.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 6fa99b7f80b4a7ed2cf616eae393bb6d9d51ba8f
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Apr 27 12:51:43 2012 +0100

    ARM: 7405/1: kexec: call platform_cpu_kill on the killer rather than the victim
    
    When performing a kexec on an SMP system, the secondary cores are stopped
    by calling machine_shutdown(), which in turn issues IPIs to offline the
    other CPUs. Unfortunately, this isn't enough to reboot the cores into
    a new kernel (since they are just executing a cpu_relax loop somewhere
    in memory) so we make use of platform_cpu_kill, part of the CPU hotplug
    implementation, to place the cores somewhere safe. This function expects
    to be called on the killing CPU for each core that it takes out.
    
    This patch moves the platform_cpu_kill callback out of the IPI handler
    and into smp_send_stop, therefore ensuring that it executes on the
    killing CPU rather than on the victim, matching what the hotplug code
    requires.
    
    Cc: stable@vger.kernel.org
    Reported-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 38498a67aa2cf8c80754b8d304bfacc10bc582b5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:44 2012 +0000

    smp: Add generic smpboot facility
    
    Start a new file, which will hold SMP and CPU hotplug related generic
    infrastructure.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/20120420124557.035417523@linutronix.de

commit ce1ee9f5b99fa4ba332692194aa57b604112762f
Merge: 4d634ca35a8b e00574b7f318
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 23 19:50:48 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rkuo/linux-hexagon-kernel
    
    Pull Hexagon fixes from Richard Kuo:
     "It's mostly compile fixes and the Hexagon portion of a CPU hotplug
      patch set."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rkuo/linux-hexagon-kernel:
      hexagon: add missing cpu.h include
      hexagon/CPU hotplug: Add missing call to notify_cpu_starting()
      hexagon:  use renamed tick_nohz_idle_* functions
      Hexagon: misc compile warning/error cleanup due to missing headers

commit 57f27cca7ab59cec05adc85cef97e9b4f7d28d78
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu Mar 22 16:58:25 2012 +0530

    hexagon/CPU hotplug: Add missing call to notify_cpu_starting()
    
    The scheduler depends on receiving the CPU_STARTING notification, without
    which we end up into a lot of trouble. So add the missing call to
    notify_cpu_starting() in the bringup code.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Richard Kuo <rkuo@codeaurora.org>

commit 70f33fa5863458ec9f1ae3aabdb9c2401598ec3d
Merge: a1ada0860621 e72d5c7e9c83
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 11 11:08:20 2012 -0700

    Merge branch 'stable' of git://git.kernel.org/pub/scm/linux/kernel/git/cmetcalf/linux-tile
    
    Pull arch/tile fixes from Chris Metcalf:
     "This is one important change from Srivatsa Bhat that got dropped when
      I put together my pull request for -rc2, plus a trivial change to
      remove a compiler warning."
    
    * 'stable' of git://git.kernel.org/pub/scm/linux/kernel/git/cmetcalf/linux-tile:
      arch/tile: avoid unused variable warning in proc.c for tilegx
      tile/CPU hotplug: Add missing call to notify_cpu_starting()

commit d1640130cda146ed925f12434bfe579ee7d80a1c
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu Mar 22 16:59:11 2012 +0530

    tile/CPU hotplug: Add missing call to notify_cpu_starting()
    
    The scheduler depends on receiving the CPU_STARTING notification, without
    which we end up into a lot of trouble. So add the missing call to
    notify_cpu_starting() in the bringup code.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Chris Metcalf <cmetcalf@tilera.com>

commit 8f6b7676ceecc1f40df77d5a4d6a8bae87594a2d
Merge: 143418d0c87f 5219a5342ab1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 2 09:40:24 2012 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6
    
    Pull crypto fixes from Herbert Xu:
     - Fix for CPU hotplug hang in padata.
     - Avoid using cpu_active inappropriately in pcrypt and padata.
     - Fix for user-space algorithm lookup hang with IV generators.
     - Fix for netlink dump of algorithms where stuff went missing due to
       incorrect calculation of message size.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6:
      crypto: user - Fix size of netlink dump message
      crypto: user - Fix lookup of algorithms with IV generator
      crypto: pcrypt - Use the online cpumask as the default
      padata: Fix cpu hotplug
      padata: Use the online cpumask as the default
      padata: Add a reference to the api documentation

commit 9f324bda970c599ca35f7be89d9d1bcb96d6053c
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Mon Mar 19 13:08:02 2012 -0600

    ACPI: Add CPU hotplug support for processor device objects
    
    acpi_processor_install_hotplug_notify() registers processor objects to
    receive ACPI CPU hotplug event notifications. This patch additionally
    registers processor device objects (ACPI0007) to receive the notifications
    as well.
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Reviewed-by: Bjorn Helgaas <bhelgaas@google.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

commit 106b44388d8f76373149c4ea144f717b6d4d9a6d
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Mar 21 13:03:45 2012 -0400

    xen/smp: Fix bringup bug in AP code.
    
    The CPU hotplug code has now a callback to help bring up the CPU.
    Without the call we end up getting:
    
     BUG: soft lockup - CPU#0 stuck for 29s! [migration/0:6]
    Modules linked in:
    CPU ] Pid: 6, comm: migration/0 Not tainted 3.3.0upstream-01180-ged378a5 #1 Dell Inc. PowerEdge T105 /0RR825
    RIP: e030:[<ffffffff810d3b8b>]  [<ffffffff810d3b8b>] stop_machine_cpu_stop+0x7b/0xf0
    RSP: e02b:ffff8800ceaabdb0  EFLAGS: 00000293
    .. snip..
    Call Trace:
     [<ffffffff810d3b10>] ? stop_one_cpu_nowait+0x50/0x50
     [<ffffffff810d3841>] cpu_stopper_thread+0xf1/0x1c0
     [<ffffffff815a9776>] ? __schedule+0x3c6/0x760
     [<ffffffff815aa749>] ? _raw_spin_unlock_irqrestore+0x19/0x30
     [<ffffffff810d3750>] ? res_counter_charge+0x150/0x150
     [<ffffffff8108dc76>] kthread+0x96/0xa0
     [<ffffffff815b27e4>] kernel_thread_helper+0x4/0x10
     [<ffffffff815aacbc>] ? retint_restore_ar
    
    Thix fixes it.
    
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit 4293f20c19f44ca66e5ac836b411d25e14b9f185
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 7 08:21:19 2012 -0800

    Revert "CPU hotplug, cpusets, suspend: Don't touch cpusets during suspend/resume"
    
    This reverts commit 8f2f748b0656257153bcf0941df8d6060acc5ca6.
    
    It causes some odd regression that we have not figured out, and it's too
    late in the -rc series to try to figure it out now.
    
    As reported by Konstantin Khlebnikov, it causes consistent hangs on his
    laptop (Thinkpad x220: 2x cores + HT).  They can be avoided by adding
    calls to "rebuild_sched_domains();" in cpuset_cpu_[in]active() for the
    CPU_{ONLINE/DOWN_FAILED/DOWN_PREPARE}_FROZEN cases, but it's not at all
    clear why, and it makes no sense.
    
    Konstantin's config doesn't even have CONFIG_CPUSETS enabled, just to
    make things even more interesting.  So it's not the cpusets, it's just
    the scheduling domains.
    
    So until this is understood, revert.
    
    Bisected-reported-and-tested-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 2273d5ccb882106a74c7b780a6bfa16fb210cd24
Merge: 5189fa19a4b2 847854f5988a 5d85d97c9f69 8f2f748b0656
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 2 11:38:43 2012 -0800

    Merge branches 'core-urgent-for-linus', 'perf-urgent-for-linus' and 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pulling latest branches from Ingo:
    
    * 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      memblock: Fix size aligning of memblock_alloc_base_nid()
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf probe: Ensure offset provided is not greater than function length without DWARF info too
      perf tools: Ensure comm string is properly terminated
      perf probe: Ensure offset provided is not greater than function length
      perf evlist: Return first evsel for non-sample event on old kernel
      perf/hwbp: Fix a possible memory leak
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      CPU hotplug, cpusets, suspend: Don't touch cpusets during suspend/resume

commit 8f2f748b0656257153bcf0941df8d6060acc5ca6
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu Feb 23 15:27:15 2012 +0530

    CPU hotplug, cpusets, suspend: Don't touch cpusets during suspend/resume
    
    Currently, during CPU hotplug, the cpuset callbacks modify the cpusets
    to reflect the state of the system, and this handling is asymmetric.
    That is, upon CPU offline, that CPU is removed from all cpusets. However
    when it comes back online, it is put back only to the root cpuset.
    
    This gives rise to a significant problem during suspend/resume. During
    suspend, we offline all non-boot cpus and during resume we online them back.
    Which means, after a resume, all cpusets (except the root cpuset) will be
    restricted to just one single CPU (the boot cpu). But the whole point of
    suspend/resume is to restore the system to a state which is as close as
    possible to how it was before suspend.
    
    So to fix this, don't touch cpusets during suspend/resume. That is, modify
    the cpuset-related CPU hotplug callback to just ignore CPU hotplug when it
    is initiated as part of the suspend/resume sequence.
    
    Reported-by: Prashanth Nageshappa <prashanth@linux.vnet.ibm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/4F460D7B.1020703@linux.vnet.ibm.com
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a50c3af910e06f35bc0c68f89d8fef98c0fec0ea
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Tue Jan 10 17:52:31 2012 -0800

    rcu: Don't make callbacks go through second full grace period
    
    RCU's current CPU-offline code path dumps all of the outgoing CPU's
    callbacks onto the RCU_NEXT_TAIL portion of the surviving CPU's
    callback list.  This means that all the ready-to-invoke callbacks from
    the outgoing CPU must wait for another full RCU grace period.  This was
    just fine when CPU-hotplug events were rare, but there is increasing
    evidence that users are planning to make increasing use of CPU hotplug.
    
    Therefore, this commit changes the callback-dumping procedure so that
    callbacks that are ready to invoke are moved to the RCU_DONE_TAIL
    portion of the surviving CPU's callback list.  This avoids running
    these callbacks through a second unnecessary grace period.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 5b02aa1e6e7cf7d3bbf8027dc7e05e0eb2f1e828
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Feb 1 16:07:41 2012 -0500

    xen/bootup: During bootup suppress XENBUS: Unable to read cpu state
    
    When the initial domain starts, it prints (depending on the
    amount of CPUs) a slew of
    XENBUS: Unable to read cpu state
    XENBUS: Unable to read cpu state
    XENBUS: Unable to read cpu state
    XENBUS: Unable to read cpu state
    
    which provide no useful information - as the error is a valid
    issue - but not on the initial domain. The reason is that the
    XenStore is not accessible at that time (it is after all the
    first guest) so the CPU hotplug watch cannot parse "availability/cpu"
    attribute.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

commit a3301b751b19f0efbafddc4034f8e7ce6bf3007b
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Sat Jan 14 08:11:31 2012 +0530

    x86/mce: Fix CPU hotplug and suspend regression related to MCE
    
    Commit 8a25a2fd126c ("cpu: convert 'cpu' and 'machinecheck' sysdev_class
    to a regular subsystem") changed how things are dealt with in the MCE
    subsystem.  Some of the things that got broken due to this are CPU
    hotplug and suspend/hibernate.
    
    MCE uses per_cpu allocations of struct device.  So, when a CPU goes
    offline and comes back online, in order to ensure that we start from a
    clean slate with respect to the MCE subsystem, zero out the entire
    per_cpu device structure to 0 before using it.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit f71989e20764812bb63fb2df49e37bce3b17ed58
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu Nov 3 00:59:25 2011 +0100

    PM / Sleep: Fix race between CPU hotplug and freezer
    
    commit 79cfbdfa87e84992d509e6c1648a18e1d7e68c20 upstream.
    
    The CPU hotplug notifications sent out by the _cpu_up() and _cpu_down()
    functions depend on the value of the 'tasks_frozen' argument passed to them
    (which indicates whether tasks have been frozen or not).
    (Examples for such CPU hotplug notifications: CPU_ONLINE, CPU_ONLINE_FROZEN,
    CPU_DEAD, CPU_DEAD_FROZEN).
    
    Thus, it is essential that while the callbacks for those notifications are
    running, the state of the system with respect to the tasks being frozen or
    not remains unchanged, *throughout that duration*. Hence there is a need for
    synchronizing the CPU hotplug code with the freezer subsystem.
    
    Since the freezer is involved only in the Suspend/Hibernate call paths, this
    patch hooks the CPU hotplug code to the suspend/hibernate notifiers
    PM_[SUSPEND|HIBERNATE]_PREPARE and PM_POST_[SUSPEND|HIBERNATE] to prevent
    the race between CPU hotplug and freezer, thus ensuring that CPU hotplug
    notifications will always be run with the state of the system really being
    what the notifications indicate, _throughout_ their execution time.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 4ae84455f98af3b3c3a23f8b6458fcefc9ff62bf
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu Nov 3 00:59:25 2011 +0100

    PM / Sleep: Fix race between CPU hotplug and freezer
    
    commit 79cfbdfa87e84992d509e6c1648a18e1d7e68c20 upstream.
    
    The CPU hotplug notifications sent out by the _cpu_up() and _cpu_down()
    functions depend on the value of the 'tasks_frozen' argument passed to them
    (which indicates whether tasks have been frozen or not).
    (Examples for such CPU hotplug notifications: CPU_ONLINE, CPU_ONLINE_FROZEN,
    CPU_DEAD, CPU_DEAD_FROZEN).
    
    Thus, it is essential that while the callbacks for those notifications are
    running, the state of the system with respect to the tasks being frozen or
    not remains unchanged, *throughout that duration*. Hence there is a need for
    synchronizing the CPU hotplug code with the freezer subsystem.
    
    Since the freezer is involved only in the Suspend/Hibernate call paths, this
    patch hooks the CPU hotplug code to the suspend/hibernate notifiers
    PM_[SUSPEND|HIBERNATE]_PREPARE and PM_POST_[SUSPEND|HIBERNATE] to prevent
    the race between CPU hotplug and freezer, thus ensuring that CPU hotplug
    notifications will always be run with the state of the system really being
    what the notifications indicate, _throughout_ their execution time.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit d12b918be6b7af2824d55940a5fbf9cfe7116052
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu Nov 3 00:59:25 2011 +0100

    PM / Sleep: Fix race between CPU hotplug and freezer
    
    commit 79cfbdfa87e84992d509e6c1648a18e1d7e68c20 upstream.
    
    The CPU hotplug notifications sent out by the _cpu_up() and _cpu_down()
    functions depend on the value of the 'tasks_frozen' argument passed to them
    (which indicates whether tasks have been frozen or not).
    (Examples for such CPU hotplug notifications: CPU_ONLINE, CPU_ONLINE_FROZEN,
    CPU_DEAD, CPU_DEAD_FROZEN).
    
    Thus, it is essential that while the callbacks for those notifications are
    running, the state of the system with respect to the tasks being frozen or
    not remains unchanged, *throughout that duration*. Hence there is a need for
    synchronizing the CPU hotplug code with the freezer subsystem.
    
    Since the freezer is involved only in the Suspend/Hibernate call paths, this
    patch hooks the CPU hotplug code to the suspend/hibernate notifiers
    PM_[SUSPEND|HIBERNATE]_PREPARE and PM_POST_[SUSPEND|HIBERNATE] to prevent
    the race between CPU hotplug and freezer, thus ensuring that CPU hotplug
    notifications will always be run with the state of the system really being
    what the notifications indicate, _throughout_ their execution time.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit f40aaf6da14a4043d6361e0c7d74ada18e866059
Author: Magnus Damm <damm@opensource.se>
Date:   Tue Jan 10 17:44:39 2012 +0900

    ARM: mach-shmobile: r8a7779 SMP support V3
    
    This patch contains r8a7779 SMP support V3 - now including
    CPU hotplug offine and online support. The r8a7779 power
    domain code is tied together with SMP glue code which allows
    us to control the power domains via CPU hotplug.
    
    At this point the kernel boots with the 4 Cortex-A9 cores in
    SMP mode and all CPU cores except CPU0 can be hotplugged.
    
    The code in platsmp.c is quite far from pretty, but it is
    kept like that intentionally to avoid creating layers of
    code that will go away in the near future anyway. The code
    needs to be updated when some per-SoC handling code will be
    added to the ARM architecture, see the following patch for
    more information:
     "[RFC PATCH 0/3] Per SoC descriptor"
    
    Signed-off-by: Magnus Damm <damm@opensource.se>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

commit 1229835ccb6d7ea2b36230121205be95c88eca88
Author: Magnus Damm <damm@opensource.se>
Date:   Wed Dec 28 16:53:16 2011 +0900

    ARM: mach-shmobile: Flush caches in platform_cpu_die()
    
    Add cache flushing code to the SH-Mobile specific CPU hotplug
    implementation. While at it, add a cpu mask to make sure the
    cache flushing code is finished in platform_cpu_die() before
    letting the SoC-specific code in shmobile_platform_cpu_kill()
    proceed with turning off power.
    
    Without this code CPU hotplug offline fails when cache is
    enabled on Cortex-A9 based SoCs.
    
    Signed-off-by: Magnus Damm <damm@opensource.se>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

commit f0eab3889e4b5b71e1609c9850f652f06a812cf5
Author: Magnus Damm <damm@opensource.se>
Date:   Wed Dec 28 16:44:06 2011 +0900

    ARM: mach-shmobile: Fix headsmp.S code to use CPUINIT
    
    Convert the low level SMP assembly code for SH-Mobile ARM
    from using the INIT to the CPUINIT section. This unbreaks
    onlining of CPUs using the CPU hotplug interface:
    
    echo 1 > /sys/devices/system/cpu/cpu1/online
    
    Without this fix the reset vector code used by CPU hotplug
    will be freed as init section data and CPU cores cannot
    be brought online.
    
    Signed-off-by: Magnus Damm <damm@opensource.se>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

commit 27c57858531c4829a1446ebb5fd606d07846b2e5
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu Dec 22 02:45:29 2011 +0530

    VFS: Fix race between CPU hotplug and lglocks
    
    commit e30e2fdfe56288576ee9e04dbb06b4bd5f282203 upstream.
    
    Currently, the *_global_[un]lock_online() routines are not at all synchronized
    with CPU hotplug. Soft-lockups detected as a consequence of this race was
    reported earlier at https://lkml.org/lkml/2011/8/24/185. (Thanks to Cong Meng
    for finding out that the root-cause of this issue is the race condition
    between br_write_[un]lock() and CPU hotplug, which results in the lock states
    getting messed up).
    
    Fixing this race by just adding {get,put}_online_cpus() at appropriate places
    in *_global_[un]lock_online() is not a good option, because, then suddenly
    br_write_[un]lock() would become blocking, whereas they have been kept as
    non-blocking all this time, and we would want to keep them that way.
    
    So, overall, we want to ensure 3 things:
    1. br_write_lock() and br_write_unlock() must remain as non-blocking.
    2. The corresponding lock and unlock of the per-cpu spinlocks must not happen
       for different sets of CPUs.
    3. Either prevent any new CPU online operation in between this lock-unlock, or
       ensure that the newly onlined CPU does not proceed with its corresponding
       per-cpu spinlock unlocked.
    
    To achieve all this:
    (a) We introduce a new spinlock that is taken by the *_global_lock_online()
        routine and released by the *_global_unlock_online() routine.
    (b) We register a callback for CPU hotplug notifications, and this callback
        takes the same spinlock as above.
    (c) We maintain a bitmap which is close to the cpu_online_mask, and once it is
        initialized in the lock_init() code, all future updates to it are done in
        the callback, under the above spinlock.
    (d) The above bitmap is used (instead of cpu_online_mask) while locking and
        unlocking the per-cpu locks.
    
    The callback takes the spinlock upon the CPU_UP_PREPARE event. So, if the
    br_write_lock-unlock sequence is in progress, the callback keeps spinning,
    thus preventing the CPU online operation till the lock-unlock sequence is
    complete. This takes care of requirement (3).
    
    The bitmap that we maintain remains unmodified throughout the lock-unlock
    sequence, since all updates to it are managed by the callback, which takes
    the same spinlock as the one taken by the lock code and released only by the
    unlock routine. Combining this with (d) above, satisfies requirement (2).
    
    Overall, since we use a spinlock (mentioned in (a)) to prevent CPU hotplug
    operations from racing with br_write_lock-unlock, requirement (1) is also
    taken care of.
    
    By the way, it is to be noted that a CPU offline operation can actually run
    in parallel with our lock-unlock sequence, because our callback doesn't react
    to notifications earlier than CPU_DEAD (in order to maintain our bitmap
    properly). And this means, since we use our own bitmap (which is stale, on
    purpose) during the lock-unlock sequence, we could end up unlocking the
    per-cpu lock of an offline CPU (because we had locked it earlier, when the
    CPU was online), in order to satisfy requirement (2). But this is harmless,
    though it looks a bit awkward.
    
    Debugged-by: Cong Meng <mc@linux.vnet.ibm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit f3545737cf06d342d34483b7a8421d0bb90b9c1b
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu Dec 22 02:45:29 2011 +0530

    VFS: Fix race between CPU hotplug and lglocks
    
    commit e30e2fdfe56288576ee9e04dbb06b4bd5f282203 upstream.
    
    Currently, the *_global_[un]lock_online() routines are not at all synchronized
    with CPU hotplug. Soft-lockups detected as a consequence of this race was
    reported earlier at https://lkml.org/lkml/2011/8/24/185. (Thanks to Cong Meng
    for finding out that the root-cause of this issue is the race condition
    between br_write_[un]lock() and CPU hotplug, which results in the lock states
    getting messed up).
    
    Fixing this race by just adding {get,put}_online_cpus() at appropriate places
    in *_global_[un]lock_online() is not a good option, because, then suddenly
    br_write_[un]lock() would become blocking, whereas they have been kept as
    non-blocking all this time, and we would want to keep them that way.
    
    So, overall, we want to ensure 3 things:
    1. br_write_lock() and br_write_unlock() must remain as non-blocking.
    2. The corresponding lock and unlock of the per-cpu spinlocks must not happen
       for different sets of CPUs.
    3. Either prevent any new CPU online operation in between this lock-unlock, or
       ensure that the newly onlined CPU does not proceed with its corresponding
       per-cpu spinlock unlocked.
    
    To achieve all this:
    (a) We introduce a new spinlock that is taken by the *_global_lock_online()
        routine and released by the *_global_unlock_online() routine.
    (b) We register a callback for CPU hotplug notifications, and this callback
        takes the same spinlock as above.
    (c) We maintain a bitmap which is close to the cpu_online_mask, and once it is
        initialized in the lock_init() code, all future updates to it are done in
        the callback, under the above spinlock.
    (d) The above bitmap is used (instead of cpu_online_mask) while locking and
        unlocking the per-cpu locks.
    
    The callback takes the spinlock upon the CPU_UP_PREPARE event. So, if the
    br_write_lock-unlock sequence is in progress, the callback keeps spinning,
    thus preventing the CPU online operation till the lock-unlock sequence is
    complete. This takes care of requirement (3).
    
    The bitmap that we maintain remains unmodified throughout the lock-unlock
    sequence, since all updates to it are managed by the callback, which takes
    the same spinlock as the one taken by the lock code and released only by the
    unlock routine. Combining this with (d) above, satisfies requirement (2).
    
    Overall, since we use a spinlock (mentioned in (a)) to prevent CPU hotplug
    operations from racing with br_write_lock-unlock, requirement (1) is also
    taken care of.
    
    By the way, it is to be noted that a CPU offline operation can actually run
    in parallel with our lock-unlock sequence, because our callback doesn't react
    to notifications earlier than CPU_DEAD (in order to maintain our bitmap
    properly). And this means, since we use our own bitmap (which is stale, on
    purpose) during the lock-unlock sequence, we could end up unlocking the
    per-cpu lock of an offline CPU (because we had locked it earlier, when the
    CPU was online), in order to satisfy requirement (2). But this is harmless,
    though it looks a bit awkward.
    
    Debugged-by: Cong Meng <mc@linux.vnet.ibm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit b4d48c942c17ce3d3a330ad91e109e522bc97378
Merge: 1a5cd29631a6 5f0a6e2d5038
Author: Mauro Carvalho Chehab <mchehab@redhat.com>
Date:   Fri Dec 30 13:59:37 2011 -0200

    Merge tag 'v3.2-rc7' into staging/for_v3.3
    
    Linux 3.2-rc7
    
    * tag 'v3.2-rc7': (1304 commits)
      Linux 3.2-rc7
      netfilter: xt_connbytes: handle negation correctly
      Btrfs: call d_instantiate after all ops are setup
      Btrfs: fix worker lock misuse in find_worker
      net: relax rcvbuf limits
      rps: fix insufficient bounds checking in store_rps_dev_flow_table_cnt()
      net: introduce DST_NOPEER dst flag
      mqprio: Avoid panic if no options are provided
      bridge: provide a mtu() method for fake_dst_ops
      md/bitmap: It is OK to clear bits during recovery.
      md: don't give up looking for spares on first failure-to-add
      md/raid5: ensure correct assessment of drives during degraded reshape.
      md/linear: fix hot-add of devices to linear arrays.
      sparc64: Fix MSIQ HV call ordering in pci_sun4v_msiq_build_irq().
      pata_of_platform: Add missing CONFIG_OF_IRQ dependency.
      ipv4: using prefetch requires including prefetch.h
      VFS: Fix race between CPU hotplug and lglocks
      vfs: __read_cache_page should use gfp argument rather than GFP_KERNEL
      USB: Fix usb/isp1760 build on sparc
      net: Add a flow_cache_flush_deferred function
      ...
    
    Conflicts:
            drivers/media/common/tuners/tda18218.c
            drivers/media/video/omap3isp/ispccdc.c
            drivers/staging/media/as102/as102_drv.h

commit a22681fabb1564d00d54e804ec95ba9330d857ed
Merge: 6d451c578c72 e30e2fdfe562
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 23 21:47:28 2011 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      VFS: Fix race between CPU hotplug and lglocks

commit e30e2fdfe56288576ee9e04dbb06b4bd5f282203
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu Dec 22 02:45:29 2011 +0530

    VFS: Fix race between CPU hotplug and lglocks
    
    Currently, the *_global_[un]lock_online() routines are not at all synchronized
    with CPU hotplug. Soft-lockups detected as a consequence of this race was
    reported earlier at https://lkml.org/lkml/2011/8/24/185. (Thanks to Cong Meng
    for finding out that the root-cause of this issue is the race condition
    between br_write_[un]lock() and CPU hotplug, which results in the lock states
    getting messed up).
    
    Fixing this race by just adding {get,put}_online_cpus() at appropriate places
    in *_global_[un]lock_online() is not a good option, because, then suddenly
    br_write_[un]lock() would become blocking, whereas they have been kept as
    non-blocking all this time, and we would want to keep them that way.
    
    So, overall, we want to ensure 3 things:
    1. br_write_lock() and br_write_unlock() must remain as non-blocking.
    2. The corresponding lock and unlock of the per-cpu spinlocks must not happen
       for different sets of CPUs.
    3. Either prevent any new CPU online operation in between this lock-unlock, or
       ensure that the newly onlined CPU does not proceed with its corresponding
       per-cpu spinlock unlocked.
    
    To achieve all this:
    (a) We introduce a new spinlock that is taken by the *_global_lock_online()
        routine and released by the *_global_unlock_online() routine.
    (b) We register a callback for CPU hotplug notifications, and this callback
        takes the same spinlock as above.
    (c) We maintain a bitmap which is close to the cpu_online_mask, and once it is
        initialized in the lock_init() code, all future updates to it are done in
        the callback, under the above spinlock.
    (d) The above bitmap is used (instead of cpu_online_mask) while locking and
        unlocking the per-cpu locks.
    
    The callback takes the spinlock upon the CPU_UP_PREPARE event. So, if the
    br_write_lock-unlock sequence is in progress, the callback keeps spinning,
    thus preventing the CPU online operation till the lock-unlock sequence is
    complete. This takes care of requirement (3).
    
    The bitmap that we maintain remains unmodified throughout the lock-unlock
    sequence, since all updates to it are managed by the callback, which takes
    the same spinlock as the one taken by the lock code and released only by the
    unlock routine. Combining this with (d) above, satisfies requirement (2).
    
    Overall, since we use a spinlock (mentioned in (a)) to prevent CPU hotplug
    operations from racing with br_write_lock-unlock, requirement (1) is also
    taken care of.
    
    By the way, it is to be noted that a CPU offline operation can actually run
    in parallel with our lock-unlock sequence, because our callback doesn't react
    to notifications earlier than CPU_DEAD (in order to maintain our bitmap
    properly). And this means, since we use our own bitmap (which is stale, on
    purpose) during the lock-unlock sequence, we could end up unlocking the
    per-cpu lock of an offline CPU (because we had locked it earlier, when the
    CPU was online), in order to satisfy requirement (2). But this is harmless,
    though it looks a bit awkward.
    
    Debugged-by: Cong Meng <mc@linux.vnet.ibm.com>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Cc: stable@vger.kernel.org

commit 290130a17718c1451bb8a77a5e2510e0279bd5f3
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jun 6 12:28:54 2011 +0100

    ARM: reset: implement soft_restart for jumping to a physical address
    
    Tools such as kexec and CPU hotplug require a way to reset the processor
    and branch to some code in physical space. This requires various bits of
    jiggery pokery with the caches and MMU which, when it goes wrong, tends
    to lock up the system.
    
    This patch fleshes out the soft_restart implementation so that it
    branches to the reset code using the identity mapping. This requires us
    to change to a temporary stack, held within the kernel image as a static
    array, to avoid conflicting with the new view of memory.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit f220242af98a5248209426f36d93226c3e0f2391
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Sat Dec 3 15:09:28 2011 -0800

    rcu: Make rcutorture test for hotpluggability before offlining CPUs
    
    The rcutorture test now can automatically exercise CPU hotplug and
    collect success statistics, which can be correlated with other rcutorture
    activity.  This permits rcutorture to completely exercise RCU regardless
    of what sort of userspace and filesystem layout is in use.  Unfortunately,
    rcutorture is happy to attempt to offline CPUs that cannot be offlined,
    for example, CPU 0 in both the x86 and ARM architectures.  Although this
    allows rcutorture testing to proceed normally, it confounds attempts at
    error analysis due to the resulting flood of spurious CPU-hotplug errors.
    
    Therefore, this commit uses the new cpu_is_hotpluggable() function to
    avoid attempting to offline CPUs that are not hotpluggable, which in
    turn avoids spurious CPU-hotplug errors.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 2987557f52b97f679f0c324d8f51b8d66e1f2084
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Sat Dec 3 13:06:50 2011 -0800

    driver-core/cpu: Expose hotpluggability to the rest of the kernel
    
    When architectures register CPUs, they indicate whether the CPU allows
    hotplugging; notably, x86 and ARM don't allow hotplugging CPU 0.
    Userspace can easily query the hotpluggability of a CPU via sysfs;
    however, the kernel has no convenient way of accessing that property in
    an architecture-independent way.  While the kernel can simply try it and
    see, some code needs to distinguish between "hotplug failed" and
    "hotplug has no hope of working on this CPU"; for example, rcutorture's
    CPU hotplug tests want to avoid drowning out real hotplug failures with
    expected failures.
    
    Expose this property via a new cpu_is_hotpluggable function, so that the
    rest of the kernel can access it in an architecture-independent way.
    
    Signed-off-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit b58bdccaa8d908e0f71dae396468a0d3f7bb3125
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Wed Nov 16 17:48:21 2011 -0800

    rcu: Add rcutorture CPU-hotplug capability
    
    Running CPU-hotplug operations concurrently with rcutorture has
    historically been a good way to find bugs in both RCU and CPU hotplug.
    This commit therefore adds an rcutorture module parameter called
    "onoff_interval" that causes a randomly selected CPU-hotplug operation to
    be executed at the specified interval, in seconds.  The default value of
    "onoff_interval" is zero, which disables rcutorture-instigated CPU-hotplug
    operations.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit a6e48358d15fec2f3f9e86a6d6fc62422141a3a9
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Sun Sep 4 13:10:32 2011 +0530

    ARM: OMAP4: Remove __INIT from omap_secondary_startup() to re-use it for hotplug.
    
    Remove the __INIT from omap_secondary_startup() so that it can
    be re-used for CPU hotplug.
    
    While at this, remove the un-used AUXBOOT register reference.
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Jean Pihet <j-pihet@ti.com>
    Reviewed-by: Kevin Hilman <khilman@ti.com>
    Tested-by: Vishwanath BS <vishwanath.bs@ti.com>
    Signed-off-by: Kevin Hilman <khilman@ti.com>

commit eaa142ca3dee1478aca1a645bb0e5a249a33241a
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 9 12:24:07 2011 +0100

    ARM: mach-imx: convert logical CPU numbers to physical numbers
    
    This patch uses the new cpu_logical_map() macro for converting logical
    CPU numbers into physical numbers when releasing CPUs during the SMP
    boot and CPU hotplug paths.
    
    Cc: Sascha Hauer <s.hauer@pengutronix.de>
    Acked-by: Shawn Guo <shawn.guo@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit bf14fc54d6386ccd1ef3f1b0ff69e7a765cf8ded
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jun 6 12:35:46 2011 +0100

    ARM: highbank: convert logical CPU numbers to physical numbers
    
    This patch uses the new cpu_logical_map() macro for converting logical
    CPU numbers into physical numbers when releasing CPUs during the SMP
    boot and CPU hotplug paths.
    
    Acked-by: Rob Herring <rob.herring@calxeda.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit ff0ff78068dd8a962358dbbdafa9d6f24540d3e5
Merge: b1914cb2f35c 4af92e7a68af
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 15 22:44:45 2011 -0200

    Merge branch 'upstream' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus
    
    * 'upstream' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus:
      MIPS: lantiq: use export.h in favour of module.h
      MAINTAINERS: The MIPS git tree has moved.
      MIPS: Fix build error due to missing inclusion of <linux/export.h>.
      MIPS: ASID conflict after CPU hotplug
      MIPS: Octeon: Fix compile error in arch/mips/cavium-octeon/flash_setup.c
      MIPS: errloongson2_clock: Fix build error by including linux/module.h
      STAGING: octeon-ethernet: Fix compile error caused by skb_frag_struct change
      MIPS: Hook up process_vm_readv and process_vm_writev system calls.
      MIPS: Kernel hangs occasionally during boot.
      MIPS: Octeon: Mark SMP-IPI interrupt as IRQF_NO_THREAD
      MIPS: BCM47xx: fix build with GENERIC_GPIO configuration
      MIPS: NXP: Remove unused source files.
      MIPS: Yosemite, Emma: Fix off-by-two in arcs_cmdline buffer size check

commit 5c200197130e307de6eba72fc335c83c9dd6a5bc
Author: Maksim Rayskiy <maksim.rayskiy@gmail.com>
Date:   Thu Nov 10 17:59:45 2011 +0000

    MIPS: ASID conflict after CPU hotplug
    
    I am running SMP Linux 2.6.37-rc1 on BMIPS5000 (single core dual thread)
    and observe some abnormalities when doing system suspend/resume which I
    narrowed down to cpu hotplugging. The suspend brings the second thread
    processor down and then restarts it, after which I see memory corruption
    in userspace. I started digging and found out that problem occurs because
    while doing execve() the child process is getting the same ASID as the
    parent, which obviously corrupts parent's address space.
    
    Further digging showed that activate_mm() calls get_new_mmu_context() to
    get a new ASID, but at this time ASID field in entryHi is 1, and
    asid_cache(cpu) is 0x100 (it was just reset to ASID_FIRST_VERSION when
    the secondary TP was booting).
    
    So, get_new_mmu_context() increments the asid_cache(cpu) value to
    0x101, and thus puts 0x01 into entryHi. The result - ASID field does
    not get changed as it was supposed to.
    
    My solution is very simple - do not reset asid_cache(cpu) on TP warm
    restart.
    
    Patchwork: https://patchwork.linux-mips.org/patch/1797/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 8110efc64c4790cd1bf7e30f080e5ba3faa7cb85
Merge: 06d8eb1b7d1e d6cc76856d35
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 5 17:54:18 2011 -0700

    Merge branch 'pm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    * 'pm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm:
      PM / Freezer: Revert 27920651fe "PM / Freezer: Make fake_signal_wake_up() wake TASK_KILLABLE tasks too"
      PM / Freezer: Reimplement wait_event_freezekillable using freezer_do_not_count/freezer_count
      USB: Update last_busy time after autosuspend fails
      PM / Runtime: Automatically retry failed autosuspends
      PM / QoS: Remove redundant check
      PM / OPP: Fix build when CONFIG_PM_OPP is not set
      PM / Runtime: Fix runtime accounting calculation error
      PM / Sleep: Update freezer documentation
      PM / Sleep: Remove unused symbol 'suspend_cpu_hotplug'
      PM / Sleep: Fix race between CPU hotplug and freezer
      ACPI / PM: Add Sony VPCEB17FX to nonvs blacklist

commit 79cfbdfa87e84992d509e6c1648a18e1d7e68c20
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu Nov 3 00:59:25 2011 +0100

    PM / Sleep: Fix race between CPU hotplug and freezer
    
    The CPU hotplug notifications sent out by the _cpu_up() and _cpu_down()
    functions depend on the value of the 'tasks_frozen' argument passed to them
    (which indicates whether tasks have been frozen or not).
    (Examples for such CPU hotplug notifications: CPU_ONLINE, CPU_ONLINE_FROZEN,
    CPU_DEAD, CPU_DEAD_FROZEN).
    
    Thus, it is essential that while the callbacks for those notifications are
    running, the state of the system with respect to the tasks being frozen or
    not remains unchanged, *throughout that duration*. Hence there is a need for
    synchronizing the CPU hotplug code with the freezer subsystem.
    
    Since the freezer is involved only in the Suspend/Hibernate call paths, this
    patch hooks the CPU hotplug code to the suspend/hibernate notifiers
    PM_[SUSPEND|HIBERNATE]_PREPARE and PM_POST_[SUSPEND|HIBERNATE] to prevent
    the race between CPU hotplug and freezer, thus ensuring that CPU hotplug
    notifications will always be run with the state of the system really being
    what the notifications indicate, _throughout_ their execution time.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 4536e4d1d21c8172402a2217b0fa1880665ace36
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 3 07:44:04 2011 -0700

    Revert "perf: Add PM notifiers to fix CPU hotplug races"
    
    This reverts commit 144060fee07e9c22e179d00819c83c86fbcbf82c.
    
    It causes a resume regression for Andi on his Acer Aspire 1830T post
    3.1.  The screen just stays black after wakeup.
    
    Also, it really looks like the wrong way to suspend and resume perf
    events: I think they should be done as part of the CPU suspend and
    resume, rather than as a notifier that does smp_call_function().
    
    Reported-by: Andi Kleen <andi@firstfloor.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 7e0bb71e75020348bee523720a0c2f04cc72f540
Merge: b9e2780d576a 0ab1e79b825a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 25 15:18:39 2011 +0200

    Merge branch 'pm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    * 'pm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (63 commits)
      PM / Clocks: Remove redundant NULL checks before kfree()
      PM / Documentation: Update docs about suspend and CPU hotplug
      ACPI / PM: Add Sony VGN-FW21E to nonvs blacklist.
      ARM: mach-shmobile: sh7372 A4R support (v4)
      ARM: mach-shmobile: sh7372 A3SP support (v4)
      PM / Sleep: Mark devices involved in wakeup signaling during suspend
      PM / Hibernate: Improve performance of LZO/plain hibernation, checksum image
      PM / Hibernate: Do not initialize static and extern variables to 0
      PM / Freezer: Make fake_signal_wake_up() wake TASK_KILLABLE tasks too
      PM / Hibernate: Add resumedelay kernel param in addition to resumewait
      MAINTAINERS: Update linux-pm list address
      PM / ACPI: Blacklist Vaio VGN-FW520F machine known to require acpi_sleep=nonvs
      PM / ACPI: Blacklist Sony Vaio known to require acpi_sleep=nonvs
      PM / Hibernate: Add resumewait param to support MMC-like devices as resume file
      PM / Hibernate: Fix typo in a kerneldoc comment
      PM / Hibernate: Freeze kernel threads after preallocating memory
      PM: Update the policy on default wakeup settings
      PM / VT: Cleanup #if defined uglyness and fix compile error
      PM / Suspend: Off by one in pm_suspend()
      PM / Hibernate: Include storage keys in hibernation image on s390
      ...

commit 7fef9fc83fbd7293ea9fe665d14046422ebf4219
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Wed Oct 19 23:59:05 2011 +0200

    PM / Documentation: Update docs about suspend and CPU hotplug
    
    Update the documentation about the interaction between the suspend (S3) call
    path and the CPU hotplug infrastructure.
    This patch focusses only on the activities of the freezer, cpu hotplug and
    the notifications involved. It outlines how regular CPU hotplug differs from
    the way it is invoked during suspend and also tries to explain the locking
    involved. In addition to that, it discusses the issue of microcode update
    during CPU hotplug operations.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 4a139b64703115e41e1a4e16ebf7eb93d0a0e349
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 9 12:24:07 2011 +0100

    ARM: versatile: convert logical CPU numbers to physical numbers
    
    This patch uses the new cpu_logical_map() macro for converting logical
    CPU numbers into physical numbers when dealing with the pen_release
    variable in the SMP boot and CPU hotplug paths.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 28763487b19eacca70374c1f62b3b8da2bba464f
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 9 12:21:36 2011 +0100

    ARM: ux500: convert logical CPU numbers to physical numbers
    
    This patch uses the new cpu_logical_map() macro for converting logical
    CPU numbers into physical numbers when dealing with the pen_release
    variable in the SMP boot and CPU hotplug paths.
    
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 1d3cfb34f9605578b4f733acba4affa751b2e4a2
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 9 12:02:27 2011 +0100

    ARM: msm: convert logical CPU numbers to physical numbers
    
    This patch uses the new cpu_logical_map() macro for converting logical
    CPU numbers into physical numbers when dealing with the pen_release
    variable in the SMP boot and CPU hotplug paths.
    
    Tested-and-acked-by: David Brown <davidb@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 2f41c36b07af71bb308ff8f1739481b022476080
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 9 11:29:19 2011 +0100

    ARM: exynos4: convert logical CPU numbers to physical numbers
    
    This patch uses the new cpu_logical_map() macro for converting logical
    CPU numbers into physical numbers when dealing with the pen_release
    variable in the SMP boot and CPU hotplug paths.
    
    Cc: Kukjin Kim <kgene.kim@samsung.com>
    Tested-and-acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit b3377d1865723cea5334954e6ffcb2182b6689c8
Author: Nick Bowler <nbowler@elliptictech.com>
Date:   Wed Aug 24 18:55:37 2011 +0100

    ARM: 7064/1: vexpress: Use wfi macro in platform_do_lowpower.
    
    Current Versatile Express CPU hotplug code includes a hardcoded WFI
    instruction, in ARM encoding.  When the kernel is compiled in Thumb-2
    mode, this is invalid and causes the machine to hang hard when a CPU
    is offlined.
    
    Using the wfi macro (which uses the appropriate assembler mnemonic)
    causes the correct instruction to be emitted in either case.  As a
    consequence of this change, an apparently vestigial "cc" clobber is
    dropped from the asm (the macro uses "memory" only).
    
    Signed-off-by: Nick Bowler <nbowler@elliptictech.com>
    Reviewed-by: Jamie Iles <jamie@jamieiles.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 70989449daccf545214b4840b112558e25c2b3fc
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Sun Oct 9 15:42:00 2011 -0400

    x86, microcode: Don't request microcode from userspace unnecessarily
    
    Requesting the microcode from userspace *every time* when onlining CPUs
    (during a CPU hotplug operation) is unnecessary. Thus, ensure that
    once the kernel gets the microcode after booting, it is not freed nor
    invalidated when a CPU goes offline, so that it can be reused when that
    CPU comes back online, without requesting userspace for it again. As a
    result, the CPU hotplug operations become faster as well.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Acked-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Link: http://lkml.kernel.org/r/4E91F908.5010006@linux.vnet.ibm.com
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>

commit 344eb010b2e399069bac474a9fd0ba04908a2601
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Sep 19 17:44:54 2011 +0000

    powerpc/powernv: Add CPU hotplug support
    
    Unplugged CPU go into NAP mode in a loop until woken up
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit fb82b83970a32263698e54a8779d2ce88cd3b060
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Sep 19 17:44:49 2011 +0000

    powerpc/smp: More generic support for "soft hotplug"
    
    This adds more generic support for doing CPU hotplug with a simple
    idle loop and no actual reset of the processors. The generic
    smp_generic_kick_cpu() does the hotplug bringup trick if the PACA
    shows that the CPU has already been started at boot and we provide
    an accessor for the CPU state.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 0b390e2126e03b6ec41f96fb0550b1526d00e203
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Jul 27 15:18:59 2011 +0100

    ARM: perf: use cpumask_t to record active IRQs
    
    Commit 5dfc54e0 ("ARM: GIC: avoid routing interrupts to offline CPUs")
    prevents the GIC from setting the affinity of an IRQ to a CPU with
    id >= nr_cpu_ids. This was previously abused by perf on some platforms
    where more IRQs were registered than possible CPUs.
    
    This patch fixes the problem by using a cpumask_t to keep track of the
    active (requested) interrupts in perf. The same effect could be achieved
    by limiting the number of IRQs to the number of CPUs, but using a mask
    instead will be useful for adding extended CPU hotplug support in the
    future.
    
    Acked-by: Jamie Iles <jamie@jamieiles.com>
    Reviewed-by: Jean Pihet <j-pihet@ti.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit 144060fee07e9c22e179d00819c83c86fbcbf82c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Aug 1 12:49:14 2011 +0200

    perf: Add PM notifiers to fix CPU hotplug races
    
    Francis reports that s2r gets him spurious NMIs, this is because the
    suspend code leaves the boot cpu up and running.
    
    Cure this by adding a suspend notifier. The problem is that hotplug
    and suspend are completely un-serialized and the PM notifiers run
    before the suspend cpu unplug of all but the boot cpu.
    
    This leaves a window where the user can initialize another hotplug
    operation (either remove or add a cpu) resulting in either one too
    many or one too few hotplug ops. Thus we cannot use the hotplug code
    for the suspend case.
    
    There's another reason to not use the hotplug code, which is that the
    hotplug code totally destroys the perf state, we can do better for
    suspend and simply remove all counters from the PMU so that we can
    re-instate them on resume.
    
    Reported-by: Francis Moreau <francis.moro@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-1cvevybkgmv4s6v5y37t4847@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit f8c02e3305bdf8656219da21fde9f0b8b31c0508
Author: Jacob Shin <jacob.shin@amd.com>
Date:   Wed Apr 27 13:32:11 2011 -0500

    CPU hotplug, re-create sysfs directory and symlinks
    
    commit 27ecddc2a9f99ce4ac9a59a0acd77f7100b6d034 upstream.
    
    When we discover CPUs that are affected by each other's
    frequency/voltage transitions, the first CPU gets a sysfs directory
    created, and rest of the siblings get symlinks. Currently, when we
    hotplug off only the first CPU, all of the symlinks and the sysfs
    directory gets removed. Even though rest of the siblings are still
    online and functional, they are orphaned, and no longer governed by
    cpufreq.
    
    This patch, given the above scenario, creates a sysfs directory for
    the first sibling and symlinks for the rest of the siblings.
    
    Please note the recursive call, it was rather too ugly to roll it
    out. And the removal of redundant NULL setting (it is already taken
    care of near the top of the function).
    
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Acked-by: Mark Langsdorf <mark.langsdorf@amd.com>
    Reviewed-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

commit b6844e8f64920cdee620157252169ba63afb0c89
Merge: 2f175074e681 3ad55155b222
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 24 10:20:54 2011 -0700

    Merge branch 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm
    
    * 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm: (237 commits)
      ARM: 7004/1: fix traps.h compile warnings
      ARM: 6998/2: kernel: use proper memory barriers for bitops
      ARM: 6997/1: ep93xx: increase NR_BANKS to 16 for support of 128MB RAM
      ARM: Fix build errors caused by adding generic macros
      ARM: CPU hotplug: ensure we migrate all IRQs off a downed CPU
      ARM: CPU hotplug: pass in proper affinity mask on IRQ migration
      ARM: GIC: avoid routing interrupts to offline CPUs
      ARM: CPU hotplug: fix abuse of irqdesc->node
      ARM: 6981/2: mmci: adjust calculation of f_min
      ARM: 7000/1: LPAE: Use long long printk format for displaying the pud
      ARM: 6999/1: head, zImage: Always Enter the kernel in ARM state
      ARM: btc: avoid invalidating the branch target cache on kernel TLB maintanence
      ARM: ARM_DMA_ZONE_SIZE is no more
      ARM: mach-shark: move ARM_DMA_ZONE_SIZE to mdesc->dma_zone_size
      ARM: mach-sa1100: move ARM_DMA_ZONE_SIZE to mdesc->dma_zone_size
      ARM: mach-realview: move from ARM_DMA_ZONE_SIZE to mdesc->dma_zone_size
      ARM: mach-pxa: move from ARM_DMA_ZONE_SIZE to mdesc->dma_zone_size
      ARM: mach-ixp4xx: move from ARM_DMA_ZONE_SIZE to mdesc->dma_zone_size
      ARM: mach-h720x: move from ARM_DMA_ZONE_SIZE to mdesc->dma_zone_size
      ARM: mach-davinci: move from ARM_DMA_ZONE_SIZE to mdesc->dma_zone_size
      ...

commit 78359cb86b8c4c8946f6732eac2757fa5e1d4de4
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jul 21 15:14:21 2011 +0100

    ARM: CPU hotplug: ensure we migrate all IRQs off a downed CPU
    
    Our selection of interrupts to consider for IRQ migration is sub-
    standard.  We were potentially including per-CPU interrupts in our
    migration strategy, but omitting chained interrupts.  This caused
    some interrupts to remain on a downed CPU.
    
    We were also trying to migrate interrupts which were not migratable,
    resulting in an OOPS.
    
    Instead, iterate over all interrupts, skipping per-CPU interrupts
    or interrupts whose affinity does not include the downed CPU, and
    attempt to set the affinity for every one else if their chip
    implements irq_set_affinity().
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit ca15af19ac07908c8ca386f6d944a18aa343b868
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jul 21 15:07:56 2011 +0100

    ARM: CPU hotplug: pass in proper affinity mask on IRQ migration
    
    Now that the GIC takes care of selecting a target interrupt from the
    affinity mask, we don't need all this complexity in the core code
    anymore.  Just detect when we need to break affinity.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 2ef75701d1711a1feee2a82b42a2597ddc05f88b
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jul 21 14:51:13 2011 +0100

    ARM: CPU hotplug: fix abuse of irqdesc->node
    
    irqdesc's node member is supposed to mark the numa node number for the
    interrupt.  Our use of it is non-standard.  Remove this, replacing the
    functionality with a test of the affinity mask.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit ac619f4eba45da10053fc991f8a5d47b3be79fa3
Author: Jan Beulich <JBeulich@novell.com>
Date:   Tue Jul 19 11:39:03 2011 +0100

    x86: Serialize SMP bootup CMOS accesses on rtc_lock
    
    With CPU hotplug, there is a theoretical race between other CMOS
    (namely RTC) accesses and those done in the SMP secondary
    processor bringup path.
    
    I am unware of the problem having been noticed by anyone in practice,
    but it would very likely be rather spurious and very hard to reproduce.
    So to be on the safe side, acquire rtc_lock around those accesses.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Link: http://lkml.kernel.org/r/4E257AE7020000780004E2FF@nat28.tlf.novell.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 1237d7b76c365f5026e9c55534dcd7126fa014ed
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 10 21:26:13 2011 -0500

    ftrace: Fix memory leak with function graph and cpu hotplug
    
    commit 868baf07b1a259f5f3803c1dc2777b6c358f83cf upstream.
    
    When the fuction graph tracer starts, it needs to make a special
    stack for each task to save the real return values of the tasks.
    All running tasks have this stack created, as well as any new
    tasks.
    
    On CPU hot plug, the new idle task will allocate a stack as well
    when init_idle() is called. The problem is that cpu hotplug does
    not create a new idle_task. Instead it uses the idle task that
    existed when the cpu went down.
    
    ftrace_graph_init_task() will add a new ret_stack to the task
    that is given to it. Because a clone will make the task
    have a stack of its parent it does not check if the task's
    ret_stack is already NULL or not. When the CPU hotplug code
    starts a CPU up again, it will allocate a new stack even
    though one already existed for it.
    
    The solution is to treat the idle_task specially. In fact, the
    function_graph code already does, just not at init_idle().
    Instead of using the ftrace_graph_init_task() for the idle task,
    which that function expects the task to be a clone, have a
    separate ftrace_graph_init_idle_task(). Also, we will create a
    per_cpu ret_stack that is used by the idle task. When we call
    ftrace_graph_init_idle_task() it will check if the idle task's
    ret_stack is NULL, if it is, then it will assign it the per_cpu
    ret_stack.
    
    Reported-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

commit 117839a2b8925123c88ac808f5bbd9a706593e60
Author: Matt Evans <matt@ozlabs.org>
Date:   Thu Jul 29 18:49:08 2010 +0000

    powerpc/kexec: Fix orphaned offline CPUs across kexec
    
    commit e8e5c2155b0035b6e04f29be67f6444bc914005b upstream.
    
    When CPU hotplug is used, some CPUs may be offline at the time a kexec is
    performed.  The subsequent kernel may expect these CPUs to be already running,
    and will declare them stuck.  On pseries, there's also a soft-offline (cede)
    state that CPUs may be in; this can also cause problems as the kexeced kernel
    may ask RTAS if they're online -- and RTAS would say they are.  The CPU will
    either appear stuck, or will cause a crash as we replace its cede loop beneath
    it.
    
    This patch kicks each present offline CPU awake before the kexec, so that
    none are forever lost to these assumptions in the subsequent kernel.
    
    Now, the behaviour is that all available CPUs that were offlined are now
    online & usable after the kexec.  This mimics the behaviour of a full reboot
    (on which all CPUs will be restarted).
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

commit 761a310ee4c08a0c38a73663822001c67806acd8
Author: Jacob Shin <jacob.shin@amd.com>
Date:   Wed Apr 27 13:32:11 2011 -0500

    CPU hotplug, re-create sysfs directory and symlinks
    
    commit 27ecddc2a9f99ce4ac9a59a0acd77f7100b6d034 upstream.
    
    When we discover CPUs that are affected by each other's
    frequency/voltage transitions, the first CPU gets a sysfs directory
    created, and rest of the siblings get symlinks. Currently, when we
    hotplug off only the first CPU, all of the symlinks and the sysfs
    directory gets removed. Even though rest of the siblings are still
    online and functional, they are orphaned, and no longer governed by
    cpufreq.
    
    This patch, given the above scenario, creates a sysfs directory for
    the first sibling and symlinks for the rest of the siblings.
    
    Please note the recursive call, it was rather too ugly to roll it
    out. And the removal of redundant NULL setting (it is already taken
    care of near the top of the function).
    
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Acked-by: Mark Langsdorf <mark.langsdorf@amd.com>
    Reviewed-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 1c6d087337f7ab5624f8322a4001d8ec81b589a4
Author: Jacob Shin <jacob.shin@amd.com>
Date:   Wed Apr 27 13:32:11 2011 -0500

    CPU hotplug, re-create sysfs directory and symlinks
    
    commit 27ecddc2a9f99ce4ac9a59a0acd77f7100b6d034 upstream.
    
    When we discover CPUs that are affected by each other's
    frequency/voltage transitions, the first CPU gets a sysfs directory
    created, and rest of the siblings get symlinks. Currently, when we
    hotplug off only the first CPU, all of the symlinks and the sysfs
    directory gets removed. Even though rest of the siblings are still
    online and functional, they are orphaned, and no longer governed by
    cpufreq.
    
    This patch, given the above scenario, creates a sysfs directory for
    the first sibling and symlinks for the rest of the siblings.
    
    Please note the recursive call, it was rather too ugly to roll it
    out. And the removal of redundant NULL setting (it is already taken
    care of near the top of the function).
    
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Acked-by: Mark Langsdorf <mark.langsdorf@amd.com>
    Reviewed-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 15ae4738537b75bb8f9ba737bcb18c8cb0cb1e07
Author: Jacob Shin <jacob.shin@amd.com>
Date:   Wed Apr 27 13:32:11 2011 -0500

    CPU hotplug, re-create sysfs directory and symlinks
    
    commit 27ecddc2a9f99ce4ac9a59a0acd77f7100b6d034 upstream.
    
    When we discover CPUs that are affected by each other's
    frequency/voltage transitions, the first CPU gets a sysfs directory
    created, and rest of the siblings get symlinks. Currently, when we
    hotplug off only the first CPU, all of the symlinks and the sysfs
    directory gets removed. Even though rest of the siblings are still
    online and functional, they are orphaned, and no longer governed by
    cpufreq.
    
    This patch, given the above scenario, creates a sysfs directory for
    the first sibling and symlinks for the rest of the siblings.
    
    Please note the recursive call, it was rather too ugly to roll it
    out. And the removal of redundant NULL setting (it is already taken
    care of near the top of the function).
    
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Acked-by: Mark Langsdorf <mark.langsdorf@amd.com>
    Reviewed-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 5e982fede3ac5a43fa5e13c3b4fbe86488f22c05
Author: Jacob Shin <jacob.shin@amd.com>
Date:   Wed Apr 27 13:32:11 2011 -0500

    CPU hotplug, re-create sysfs directory and symlinks
    
    commit 27ecddc2a9f99ce4ac9a59a0acd77f7100b6d034 upstream.
    
    When we discover CPUs that are affected by each other's
    frequency/voltage transitions, the first CPU gets a sysfs directory
    created, and rest of the siblings get symlinks. Currently, when we
    hotplug off only the first CPU, all of the symlinks and the sysfs
    directory gets removed. Even though rest of the siblings are still
    online and functional, they are orphaned, and no longer governed by
    cpufreq.
    
    This patch, given the above scenario, creates a sysfs directory for
    the first sibling and symlinks for the rest of the siblings.
    
    Please note the recursive call, it was rather too ugly to roll it
    out. And the removal of redundant NULL setting (it is already taken
    care of near the top of the function).
    
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Acked-by: Mark Langsdorf <mark.langsdorf@amd.com>
    Reviewed-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 6e9101aeec39961308176e0f59e73ac5d37d243a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue May 24 05:43:18 2011 +0200

    watchdog: Fix non-standard prototype of get_softlockup_thresh()
    
    This build warning slipped through:
    
      kernel/watchdog.c:102: warning: function declaration isn't a prototype
    
    As reported by Stephen Rothwell.
    
    Also address an unused variable warning that GCC 4.6.0 reports:
    we cannot do anything about failed watchdog ops during CPU hotplug
    (it's not serious enough to return an error from the notifier),
    so ignore them.
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Mandeep Singh Baines <msb@chromium.org>
    Cc: Marcin Slusarz <marcin.slusarz@gmail.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/20110524134129.8da27016.sfr@canb.auug.org.au
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    LKML-Reference: <20110517071642.GF22305@elte.hu>

commit f8223b17550c427ac249124ff7f25722221f4591
Merge: 98a38a5d60a6 1a8e1463a49a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 19 15:57:29 2011 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/davej/cpufreq
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/davej/cpufreq:
      [CPUFREQ] remove redundant sprintf from request_module call.
      [CPUFREQ] cpufreq_stats.c: Fixed brace coding style issue
      [CPUFREQ] Fix memory leak in cpufreq_stat
      [CPUFREQ] cpufreq.h: Fix some checkpatch.pl coding style issues.
      [CPUFREQ] use dynamic debug instead of custom infrastructure
      [CPUFREQ] CPU hotplug, re-create sysfs directory and symlinks
      [CPUFREQ] Fix _OSC UUID in pcc-cpufreq

commit e3995a25fa361ce987a7d0ade00b17e3151519d7
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Mon Apr 18 15:31:26 2011 -0700

    rcu: put per-CPU kthread at non-RT priority during CPU hotplug operations
    
    If you are doing CPU hotplug operations, it is best not to have
    CPU-bound realtime tasks running CPU-bound on the outgoing CPU.
    So this commit makes per-CPU kthreads run at non-realtime priority
    during that time.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

commit 27ecddc2a9f99ce4ac9a59a0acd77f7100b6d034
Author: Jacob Shin <jacob.shin@amd.com>
Date:   Wed Apr 27 13:32:11 2011 -0500

    [CPUFREQ] CPU hotplug, re-create sysfs directory and symlinks
    
    When we discover CPUs that are affected by each other's
    frequency/voltage transitions, the first CPU gets a sysfs directory
    created, and rest of the siblings get symlinks. Currently, when we
    hotplug off only the first CPU, all of the symlinks and the sysfs
    directory gets removed. Even though rest of the siblings are still
    online and functional, they are orphaned, and no longer governed by
    cpufreq.
    
    This patch, given the above scenario, creates a sysfs directory for
    the first sibling and symlinks for the rest of the siblings.
    
    Please note the recursive call, it was rather too ugly to roll it
    out. And the removal of redundant NULL setting (it is already taken
    care of near the top of the function).
    
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Acked-by: Mark Langsdorf <mark.langsdorf@amd.com>
    Reviewed-by: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Dave Jones <davej@redhat.com>
    Cc: stable@kernel.org

commit b4ca174c59cf2132dc51eb1e8c6e23f4156e02ca
Author: Matt Evans <matt@ozlabs.org>
Date:   Mon Mar 7 17:26:04 2011 +0530

    powerpc/kexec: Fix orphaned offline CPUs across kexec
    
    Commit: e8e5c2155b0035b6e04f29be67f6444bc914005b upstream
    
    When CPU hotplug is used, some CPUs may be offline at the time a kexec is
    performed.  The subsequent kernel may expect these CPUs to be already running,
    and will declare them stuck.  On pseries, there's also a soft-offline (cede)
    state that CPUs may be in; this can also cause problems as the kexeced kernel
    may ask RTAS if they're online -- and RTAS would say they are.  The CPU will
    either appear stuck, or will cause a crash as we replace its cede loop beneath
    it.
    
    This patch kicks each present offline CPU awake before the kexec, so that
    none are forever lost to these assumptions in the subsequent kernel.
    
    Now, the behaviour is that all available CPUs that were offlined are now
    online & usable after the kexec.  This mimics the behaviour of a full reboot
    (on which all CPUs will be restarted).
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Kamalesh babulal <kamalesh@linux.vnet.ibm.com>
    cc: Anton Blanchard <anton@samba.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 5d5b1b9f79ebad81215d11e208e9bfa9679a4ddd
Merge: adff377bb101 7b84b29b8c27
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 18 12:24:24 2011 -0700

    Merge branch 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc:
      powerpc/powermac: Build fix with SMP and CPU hotplug
      powerpc/perf_event: Skip updating kernel counters if register value shrinks
      powerpc: Don't write protect kernel text with CONFIG_DYNAMIC_FTRACE enabled
      powerpc: Fix oops if scan_dispatch_log is called too early
      powerpc/pseries: Use a kmem cache for DTL buffers
      powerpc/kexec: Fix regression causing compile failure on UP
      powerpc/85xx: disable Suspend support if SMP enabled
      powerpc/e500mc: Remove CPU_FTR_MAYBE_CAN_NAP/CPU_FTR_MAYBE_CAN_DOZE
      powerpc/book3e: Fix CPU feature handling on 64-bit e5500
      powerpc: Check device status before adding serial device
      powerpc/85xx: Don't add disabled PCIe devices

commit 7b84b29b8c2711fe64e0dba4db22f02ce0f16015
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Apr 18 15:46:35 2011 +1000

    powerpc/powermac: Build fix with SMP and CPU hotplug
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 2f7c1a711119518a21feb2d077c8015e9f32875d
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Aug 16 14:38:33 2010 +0200

    x86-32: Separate 1:1 pagetables from swapper_pg_dir
    
    commit fd89a137924e0710078c3ae855e7cec1c43cb845 upstream.
    
    This patch fixes machine crashes which occur when heavily exercising the
    CPU hotplug codepaths on a 32-bit kernel. These crashes are caused by
    AMD Erratum 383 and result in a fatal machine check exception. Here's
    the scenario:
    
    1. On 32-bit, the swapper_pg_dir page table is used as the initial page
    table for booting a secondary CPU.
    
    2. To make this work, swapper_pg_dir needs a direct mapping of physical
    memory in it (the low mappings). By adding those low, large page (2M)
    mappings (PAE kernel), we create the necessary conditions for Erratum
    383 to occur.
    
    3. Other CPUs which do not participate in the off- and onlining game may
    use swapper_pg_dir while the low mappings are present (when leave_mm is
    called). For all steps below, the CPU referred to is a CPU that is using
    swapper_pg_dir, and not the CPU which is being onlined.
    
    4. The presence of the low mappings in swapper_pg_dir can result
    in TLB entries for addresses below __PAGE_OFFSET to be established
    speculatively. These TLB entries are marked global and large.
    
    5. When the CPU with such TLB entry switches to another page table, this
    TLB entry remains because it is global.
    
    6. The process then generates an access to an address covered by the
    above TLB entry but there is a permission mismatch - the TLB entry
    covers a large global page not accessible to userspace.
    
    7. Due to this permission mismatch a new 4kb, user TLB entry gets
    established. Further, Erratum 383 provides for a small window of time
    where both TLB entries are present. This results in an uncorrectable
    machine check exception signalling a TLB multimatch which panics the
    machine.
    
    There are two ways to fix this issue:
    
            1. Always do a global TLB flush when a new cr3 is loaded and the
            old page table was swapper_pg_dir. I consider this a hack hard
            to understand and with performance implications
    
            2. Do not use swapper_pg_dir to boot secondary CPUs like 64-bit
            does.
    
    This patch implements solution 2. It introduces a trampoline_pg_dir
    which has the same layout as swapper_pg_dir with low_mappings. This page
    table is used as the initial page table of the booting CPU. Later in the
    bringup process, it switches to swapper_pg_dir and does a global TLB
    flush. This fixes the crashes in our test cases.
    
    -v2: switch to swapper_pg_dir right after entering start_secondary() so
    that we are able to access percpu data which might not be mapped in the
    trampoline page table.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    LKML-Reference: <20100816123833.GB28147@aftab>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

commit ccd00d1091fa7fff151127f26f6900be2ac8cc10
Merge: afdef69c7ff7 c0bb9e45f3a7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 1 08:57:02 2011 -0700

    Merge branch 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc:
      kdump: Allow shrinking of kdump region to be overridden
      powerpc/pmac/smp: Remove no-longer needed preempt workaround
      powerpc/smp: Increase vdso_data->processorCount, not just decrease it
      powerpc/smp: Create idle threads on demand and properly reset them
      powerpc/smp: Don't expose per-cpu "cpu_state" array
      powerpc/pmac/smp: Fix CPU hotplug crashes on some machines
      powerpc/smp: Add a smp_ops->bringup_up() done callback
      powerpc/pmac: Rename cpu_state in therm_pm72 to avoid collision
      powerpc/pmac/smp: Properly NAP offlined CPU on G5
      powerpc/pmac/smp: Remove HMT changes for PowerMac offline code
      powerpc/pmac/smp: Consolidate 32-bit and 64-bit PowerMac cpu_die in one file
      powerpc/pmac/smp: Fixup smp_core99_cpu_disable() and use it on 64-bit
      powerpc/pmac/smp: Rename fixup_irqs() to migrate_irqs() and use it on ppc32
      powerpc/pmac/smp: Fix 32-bit PowerMac cpu_die
      powerpc/smp: Remove unused smp_ops->cpu_enable()
      powerpc/smp: Remove unused generic_cpu_enable()
      powerpc/smp: Fix generic_mach_cpu_die()
      powerpc/smp: soft-replugged CPUs must go back to start_secondary
      powerpc: Make decrementer interrupt robust against offlined CPUs

commit 734796f12351f9a0f38c47b981414f82d852f222
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Mar 8 13:54:50 2011 +1100

    powerpc/pmac/smp: Fix CPU hotplug crashes on some machines
    
    On some machines that use i2c to synchronize the timebases (such
    as PowerMac7,2/7,3 G5 machines), hotplug CPU would crash when
    putting back a new CPU online due to the underlying i2c bus being
    closed.
    
    This uses the newly added bringup_done() callback to move the close
    along with other housekeeping calls, and adds a CPU notifier to
    re-open the i2c bus around subsequent hotplug operations
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 02966761c2632f7a15ecfc83bcd32b935ae84d3d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 10 21:26:13 2011 -0500

    ftrace: Fix memory leak with function graph and cpu hotplug
    
    commit 868baf07b1a259f5f3803c1dc2777b6c358f83cf upstream.
    
    When the fuction graph tracer starts, it needs to make a special
    stack for each task to save the real return values of the tasks.
    All running tasks have this stack created, as well as any new
    tasks.
    
    On CPU hot plug, the new idle task will allocate a stack as well
    when init_idle() is called. The problem is that cpu hotplug does
    not create a new idle_task. Instead it uses the idle task that
    existed when the cpu went down.
    
    ftrace_graph_init_task() will add a new ret_stack to the task
    that is given to it. Because a clone will make the task
    have a stack of its parent it does not check if the task's
    ret_stack is already NULL or not. When the CPU hotplug code
    starts a CPU up again, it will allocate a new stack even
    though one already existed for it.
    
    The solution is to treat the idle_task specially. In fact, the
    function_graph code already does, just not at init_idle().
    Instead of using the ftrace_graph_init_task() for the idle task,
    which that function expects the task to be a clone, have a
    separate ftrace_graph_init_idle_task(). Also, we will create a
    per_cpu ret_stack that is used by the idle task. When we call
    ftrace_graph_init_idle_task() it will check if the idle task's
    ret_stack is NULL, if it is, then it will assign it the per_cpu
    ret_stack.
    
    Reported-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>

commit 125124738e4704bf30ac47a59c96f7e3cbfb8161
Author: Matt Evans <matt@ozlabs.org>
Date:   Mon Mar 7 17:26:04 2011 +0530

    powerpc/kexec: Fix orphaned offline CPUs across kexec
    
    Commit: e8e5c2155b0035b6e04f29be67f6444bc914005b upstream
    
    When CPU hotplug is used, some CPUs may be offline at the time a kexec is
    performed.  The subsequent kernel may expect these CPUs to be already running,
    and will declare them stuck.  On pseries, there's also a soft-offline (cede)
    state that CPUs may be in; this can also cause problems as the kexeced kernel
    may ask RTAS if they're online -- and RTAS would say they are.  The CPU will
    either appear stuck, or will cause a crash as we replace its cede loop beneath
    it.
    
    This patch kicks each present offline CPU awake before the kexec, so that
    none are forever lost to these assumptions in the subsequent kernel.
    
    Now, the behaviour is that all available CPUs that were offlined are now
    online & usable after the kexec.  This mimics the behaviour of a full reboot
    (on which all CPUs will be restarted).
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Kamalesh babulal <kamalesh@linux.vnet.ibm.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    cc: Anton Blanchard <anton@samba.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit e00e56dfd3cf1d209ce630a2b440c91e4a30bbd3
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed Mar 23 22:16:32 2011 +0100

    cpufreq: Use syscore_ops for boot CPU suspend/resume (v2)
    
    The cpufreq subsystem uses sysdev suspend and resume for
    executing cpufreq_suspend() and cpufreq_resume(), respectively,
    during system suspend, after interrupts have been switched off on the
    boot CPU, and during system resume, while interrupts are still off on
    the boot CPU.  In both cases the other CPUs are off-line at the
    relevant point (either they have been switched off via CPU hotplug
    during suspend, or they haven't been switched on yet during resume).
    For this reason, although it may seem that cpufreq_suspend() and
    cpufreq_resume() are executed for all CPUs in the system, they are
    only called for the boot CPU in fact, which is quite confusing.
    
    To remove the confusion and to prepare for elimiating sysdev
    suspend and resume operations from the kernel enirely, convernt
    cpufreq to using a struct syscore_ops object for the boot CPU
    suspend and resume and rename the callbacks so that their names
    reflect their purpose.  In addition, put some explanatory remarks
    into their kerneldoc comments.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

commit 4db6055ac0d0bfe40daec0448b4d2824c991ad07
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 10 21:26:13 2011 -0500

    ftrace: Fix memory leak with function graph and cpu hotplug
    
    commit 868baf07b1a259f5f3803c1dc2777b6c358f83cf upstream.
    
    When the fuction graph tracer starts, it needs to make a special
    stack for each task to save the real return values of the tasks.
    All running tasks have this stack created, as well as any new
    tasks.
    
    On CPU hot plug, the new idle task will allocate a stack as well
    when init_idle() is called. The problem is that cpu hotplug does
    not create a new idle_task. Instead it uses the idle task that
    existed when the cpu went down.
    
    ftrace_graph_init_task() will add a new ret_stack to the task
    that is given to it. Because a clone will make the task
    have a stack of its parent it does not check if the task's
    ret_stack is already NULL or not. When the CPU hotplug code
    starts a CPU up again, it will allocate a new stack even
    though one already existed for it.
    
    The solution is to treat the idle_task specially. In fact, the
    function_graph code already does, just not at init_idle().
    Instead of using the ftrace_graph_init_task() for the idle task,
    which that function expects the task to be a clone, have a
    separate ftrace_graph_init_idle_task(). Also, we will create a
    per_cpu ret_stack that is used by the idle task. When we call
    ftrace_graph_init_idle_task() it will check if the idle task's
    ret_stack is NULL, if it is, then it will assign it the per_cpu
    ret_stack.
    
    Reported-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 1883e22e58194ef67349ff23333defebaaafa530
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 10 21:26:13 2011 -0500

    ftrace: Fix memory leak with function graph and cpu hotplug
    
    commit 868baf07b1a259f5f3803c1dc2777b6c358f83cf upstream.
    
    When the fuction graph tracer starts, it needs to make a special
    stack for each task to save the real return values of the tasks.
    All running tasks have this stack created, as well as any new
    tasks.
    
    On CPU hot plug, the new idle task will allocate a stack as well
    when init_idle() is called. The problem is that cpu hotplug does
    not create a new idle_task. Instead it uses the idle task that
    existed when the cpu went down.
    
    ftrace_graph_init_task() will add a new ret_stack to the task
    that is given to it. Because a clone will make the task
    have a stack of its parent it does not check if the task's
    ret_stack is already NULL or not. When the CPU hotplug code
    starts a CPU up again, it will allocate a new stack even
    though one already existed for it.
    
    The solution is to treat the idle_task specially. In fact, the
    function_graph code already does, just not at init_idle().
    Instead of using the ftrace_graph_init_task() for the idle task,
    which that function expects the task to be a clone, have a
    separate ftrace_graph_init_idle_task(). Also, we will create a
    per_cpu ret_stack that is used by the idle task. When we call
    ftrace_graph_init_idle_task() it will check if the idle task's
    ret_stack is NULL, if it is, then it will assign it the per_cpu
    ret_stack.
    
    Reported-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 8d94010a78908645e0cd99d2381053ab42fe7abb
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 10 21:26:13 2011 -0500

    ftrace: Fix memory leak with function graph and cpu hotplug
    
    commit 868baf07b1a259f5f3803c1dc2777b6c358f83cf upstream.
    
    When the fuction graph tracer starts, it needs to make a special
    stack for each task to save the real return values of the tasks.
    All running tasks have this stack created, as well as any new
    tasks.
    
    On CPU hot plug, the new idle task will allocate a stack as well
    when init_idle() is called. The problem is that cpu hotplug does
    not create a new idle_task. Instead it uses the idle task that
    existed when the cpu went down.
    
    ftrace_graph_init_task() will add a new ret_stack to the task
    that is given to it. Because a clone will make the task
    have a stack of its parent it does not check if the task's
    ret_stack is already NULL or not. When the CPU hotplug code
    starts a CPU up again, it will allocate a new stack even
    though one already existed for it.
    
    The solution is to treat the idle_task specially. In fact, the
    function_graph code already does, just not at init_idle().
    Instead of using the ftrace_graph_init_task() for the idle task,
    which that function expects the task to be a clone, have a
    separate ftrace_graph_init_idle_task(). Also, we will create a
    per_cpu ret_stack that is used by the idle task. When we call
    ftrace_graph_init_idle_task() it will check if the idle task's
    ret_stack is NULL, if it is, then it will assign it the per_cpu
    ret_stack.
    
    Reported-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit b0e080d125f14ce2cde9a2ab50b2c2d0fb5609df
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 10 21:26:13 2011 -0500

    ftrace: Fix memory leak with function graph and cpu hotplug
    
    commit 868baf07b1a259f5f3803c1dc2777b6c358f83cf upstream.
    
    When the fuction graph tracer starts, it needs to make a special
    stack for each task to save the real return values of the tasks.
    All running tasks have this stack created, as well as any new
    tasks.
    
    On CPU hot plug, the new idle task will allocate a stack as well
    when init_idle() is called. The problem is that cpu hotplug does
    not create a new idle_task. Instead it uses the idle task that
    existed when the cpu went down.
    
    ftrace_graph_init_task() will add a new ret_stack to the task
    that is given to it. Because a clone will make the task
    have a stack of its parent it does not check if the task's
    ret_stack is already NULL or not. When the CPU hotplug code
    starts a CPU up again, it will allocate a new stack even
    though one already existed for it.
    
    The solution is to treat the idle_task specially. In fact, the
    function_graph code already does, just not at init_idle().
    Instead of using the ftrace_graph_init_task() for the idle task,
    which that function expects the task to be a clone, have a
    separate ftrace_graph_init_idle_task(). Also, we will create a
    per_cpu ret_stack that is used by the idle task. When we call
    ftrace_graph_init_idle_task() it will check if the idle task's
    ret_stack is NULL, if it is, then it will assign it the per_cpu
    ret_stack.
    
    Reported-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 9147532184cd5d39a677ad15c5b9a7fa9cea631a
Author: Matt Evans <matt@ozlabs.org>
Date:   Mon Mar 7 17:26:04 2011 +0530

    powerpc/kexec: Fix orphaned offline CPUs across kexec
    
    Commit: e8e5c2155b0035b6e04f29be67f6444bc914005b upstream
    
    When CPU hotplug is used, some CPUs may be offline at the time a kexec is
    performed.  The subsequent kernel may expect these CPUs to be already running,
    and will declare them stuck.  On pseries, there's also a soft-offline (cede)
    state that CPUs may be in; this can also cause problems as the kexeced kernel
    may ask RTAS if they're online -- and RTAS would say they are.  The CPU will
    either appear stuck, or will cause a crash as we replace its cede loop beneath
    it.
    
    This patch kicks each present offline CPU awake before the kexec, so that
    none are forever lost to these assumptions in the subsequent kernel.
    
    Now, the behaviour is that all available CPUs that were offlined are now
    online & usable after the kexec.  This mimics the behaviour of a full reboot
    (on which all CPUs will be restarted).
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Kamalesh babulal <kamalesh@linux.vnet.ibm.com>
    cc: Anton Blanchard <anton@samba.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 5b48d8480bad7420846b372e4eff9f3c0966d925
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Aug 16 14:38:33 2010 +0200

    x86-32: Separate 1:1 pagetables from swapper_pg_dir
    
    commit fd89a137924e0710078c3ae855e7cec1c43cb845 upstream.
    
    This patch fixes machine crashes which occur when heavily exercising the
    CPU hotplug codepaths on a 32-bit kernel. These crashes are caused by
    AMD Erratum 383 and result in a fatal machine check exception. Here's
    the scenario:
    
    1. On 32-bit, the swapper_pg_dir page table is used as the initial page
    table for booting a secondary CPU.
    
    2. To make this work, swapper_pg_dir needs a direct mapping of physical
    memory in it (the low mappings). By adding those low, large page (2M)
    mappings (PAE kernel), we create the necessary conditions for Erratum
    383 to occur.
    
    3. Other CPUs which do not participate in the off- and onlining game may
    use swapper_pg_dir while the low mappings are present (when leave_mm is
    called). For all steps below, the CPU referred to is a CPU that is using
    swapper_pg_dir, and not the CPU which is being onlined.
    
    4. The presence of the low mappings in swapper_pg_dir can result
    in TLB entries for addresses below __PAGE_OFFSET to be established
    speculatively. These TLB entries are marked global and large.
    
    5. When the CPU with such TLB entry switches to another page table, this
    TLB entry remains because it is global.
    
    6. The process then generates an access to an address covered by the
    above TLB entry but there is a permission mismatch - the TLB entry
    covers a large global page not accessible to userspace.
    
    7. Due to this permission mismatch a new 4kb, user TLB entry gets
    established. Further, Erratum 383 provides for a small window of time
    where both TLB entries are present. This results in an uncorrectable
    machine check exception signalling a TLB multimatch which panics the
    machine.
    
    There are two ways to fix this issue:
    
            1. Always do a global TLB flush when a new cr3 is loaded and the
            old page table was swapper_pg_dir. I consider this a hack hard
            to understand and with performance implications
    
            2. Do not use swapper_pg_dir to boot secondary CPUs like 64-bit
            does.
    
    This patch implements solution 2. It introduces a trampoline_pg_dir
    which has the same layout as swapper_pg_dir with low_mappings. This page
    table is used as the initial page table of the booting CPU. Later in the
    bringup process, it switches to swapper_pg_dir and does a global TLB
    flush. This fixes the crashes in our test cases.
    
    -v2: switch to swapper_pg_dir right after entering start_secondary() so
    that we are able to access percpu data which might not be mapped in the
    trampoline page table.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    LKML-Reference: <20100816123833.GB28147@aftab>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit cdfb2881023413742e545893e1b84ba96135d8b8
Author: Matt Evans <matt@ozlabs.org>
Date:   Mon Mar 7 17:26:04 2011 +0530

    powerpc/kexec: Fix orphaned offline CPUs across kexec
    
    Commit: e8e5c2155b0035b6e04f29be67f6444bc914005b upstream
    
    When CPU hotplug is used, some CPUs may be offline at the time a kexec is
    performed.  The subsequent kernel may expect these CPUs to be already running,
    and will declare them stuck.  On pseries, there's also a soft-offline (cede)
    state that CPUs may be in; this can also cause problems as the kexeced kernel
    may ask RTAS if they're online -- and RTAS would say they are.  The CPU will
    either appear stuck, or will cause a crash as we replace its cede loop beneath
    it.
    
    This patch kicks each present offline CPU awake before the kexec, so that
    none are forever lost to these assumptions in the subsequent kernel.
    
    Now, the behaviour is that all available CPUs that were offlined are now
    online & usable after the kexec.  This mimics the behaviour of a full reboot
    (on which all CPUs will be restarted).
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Kamalesh babulal <kamalesh@linux.vnet.ibm.com>
    cc: Anton Blanchard <anton@samba.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 868baf07b1a259f5f3803c1dc2777b6c358f83cf
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Feb 10 21:26:13 2011 -0500

    ftrace: Fix memory leak with function graph and cpu hotplug
    
    When the fuction graph tracer starts, it needs to make a special
    stack for each task to save the real return values of the tasks.
    All running tasks have this stack created, as well as any new
    tasks.
    
    On CPU hot plug, the new idle task will allocate a stack as well
    when init_idle() is called. The problem is that cpu hotplug does
    not create a new idle_task. Instead it uses the idle task that
    existed when the cpu went down.
    
    ftrace_graph_init_task() will add a new ret_stack to the task
    that is given to it. Because a clone will make the task
    have a stack of its parent it does not check if the task's
    ret_stack is already NULL or not. When the CPU hotplug code
    starts a CPU up again, it will allocate a new stack even
    though one already existed for it.
    
    The solution is to treat the idle_task specially. In fact, the
    function_graph code already does, just not at init_idle().
    Instead of using the ftrace_graph_init_task() for the idle task,
    which that function expects the task to be a clone, have a
    separate ftrace_graph_init_idle_task(). Also, we will create a
    per_cpu ret_stack that is used by the idle task. When we call
    ftrace_graph_init_idle_task() it will check if the idle task's
    ret_stack is NULL, if it is, then it will assign it the per_cpu
    ret_stack.
    
    Reported-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Stable Tree <stable@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 30b99d07b7e08d0e6bcc2f0b924828c03e67f881
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Jan 14 12:06:26 2011 +0000

    ARM: fix wrongly patched constants
    
    e3d9c625 (ARM: CPU hotplug: fix hard-coded control register constants)
    changed the wrong constants in the hotplug assembly code.  Fix this.
    
    Reported-by: viresh kumar <viresh.kumar@st.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit faabfa0816916b0a7cfc93f6a9be382830658c80
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Dec 20 16:58:19 2010 +0000

    ARM: SMP: ensure frame pointer is reinitialized for soft-CPU hotplug
    
    When we soft-CPU hotplug a CPU, we reset the stack pointer and
    jump back to start_secondary().  This allows us to restart as if
    the CPU was actually reset.
    
    However, we weren't resetting the frame pointer, which could cause
    problems with backtracing.  Reset the frame pointer to zero (which
    means no parent frame) just like the early assembly code also does.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit e3d9c625f5e4158014e041f492b46e38ad10987e
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Dec 19 11:36:33 2010 +0000

    ARM: CPU hotplug: fix hard-coded control register constants
    
    Use the definition we've provided in asm/system.h rather than
    numeric constants.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit e9882777d992b76e0b80deadd66ad886c25f5d1f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Dec 19 11:33:12 2010 +0000

    ARM: CPU hotplug: add Versatile Express hotplug CPU support
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit d4450261e546953c4a1ce8b48e29164a57c6ed33
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Dec 19 11:30:43 2010 +0000

    ARM: CPU hotplug: fix reporting of spurious wakeups
    
    The original scheme for reporting spurious wakeups was broken - it
    tried to use printk() from a context which wasn't coherent with the
    other CPUs, which risks corrupting the printk() data.
    
    Fix this by noting the number spurious wakeups, and only report them
    when we are properly woken - when we will be coherent with the rest
    of the system.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 90b44199e83ec780d6def11a602d825dc68438e3
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Dec 18 10:59:49 2010 +0000

    ARM: VFP: re-initialize VFP coprocessor access enables on CPU hotplug
    
    We can not guarantee that VFP will be enabled when CPU hotplug brings
    a CPU back online from a reset state.  Add a hotplug CPU notifier to
    ensure that the VFP coprocessor access is enabled whenever a CPU comes
    back online.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 3705ff6da538aff6dba535e2e9cbcbb9456d0d53
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Dec 18 10:53:12 2010 +0000

    ARM: Fix subtle race in CPU pen_release hotplug code
    
    There is a subtle race in the CPU hotplug code, where a CPU which has
    been offlined can online itself before being requested, which results
    in things going astray on the next online/offline cycle.
    
    What happens in the normal online/offline/online cycle is:
    
            CPU0                    CPU3
            requests boot of CPU3
            pen_release = 3
            flush cache line
                                    checks pen_release, reads 3
                                    starts boot
                                    pen_release = -1
            ... requests CPU3 offline ...
                                    ... dies ...
                                    checks pen_release, reads -1
            requests boot of CPU3
            pen_release = 3
            flush cache line
                                    checks pen_release, reads 3
                                    starts boot
                                    pen_release = -1
    
    However, as the write of -1 of pen_release is not fully flushed back to
    memory, and the checking of pen_release is done with caches disabled,
    this allows CPU3 the opportunity to read the old value of pen_release:
    
            CPU0                    CPU3
            requests boot of CPU3
            pen_release = 3
            flush cache line
                                    checks pen_release, reads 3
                                    starts boot
                                    pen_release = -1
            ... requests CPU3 offline ...
                                    ... dies ...
                                    checks pen_release, reads 3
                                    starts boot
                                    pen_release = -1
            requests boot of CPU3
            pen_release = 3
            flush cache line
    
    Fix this by grouping the write of pen_release along with its cache line
    flushing code to ensure that any update to pen_release is always pushed
    out to physical memory.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 86e62b93368cffca9111996e3ed9e5b7bf6f0af3
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Nov 30 18:24:57 2010 +0000

    ARM: SMP: remove smp_mpidr.h
    
    With "ARM: CPU hotplug: remove bug checks in platform_cpu_die()", we
    now do not use hard_smp_processor_id(), we no longer need to read the
    hardware processor ID.  Remove the include providing this function.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit f36d340122ae8744e64af0a92a6f77b97542c0a4
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Nov 30 12:21:30 2010 +0000

    ARM: CPU hotplug: ensure correct ordering of unplug
    
    Don't call idle_task_exit() with interrupts disabled, and ensure
    that we have a memory barrier after interrupts are disabled but
    before signalling that this CPU has shut down.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit bbc81fd4327f9ed4480b05981e38acd48b1d184a
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Nov 30 11:12:30 2010 +0000

    ARM: CPU hotplug: remove bug checks in platform_cpu_die()
    
    platform_cpu_die() is entered from the CPU's own idle thread, which
    can not be migrated to other CPUs.  Moreover, the 'cpu' argument
    comes from the thread info, which will always be the 'current'
    CPU.  So remove this useless bug check.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 3c030beabf937b1d3b4ecaedfd1fb2f1e2aa0c70
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Nov 30 11:07:35 2010 +0000

    ARM: CPU hotplug: move cpu_killed completion to core code
    
    We always need to wait for the dying CPU to reach a safe state before
    taking it down, irrespective of the requirements of the platform.
    Move the completion code into the ARM SMP hotplug code rather than
    having each platform re-implement this.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 1c51ed4fb9f11fa1e0873aa2d5b28f42a85ac299
Author: Magnus Damm <damm@opensource.se>
Date:   Tue Dec 14 16:56:55 2010 +0900

    ARM: mach-shmobile: SMP base support
    
    Add SMP base support for R-Mobile / SH-Mobile processors.
    
    This patch contains all base code to support CONFIG_SMP
    regardless of ARCH_SHMOBILE processor type. Both local timer
    and CPU hotplug are supported, but no processor specific
    code is included.
    
    At this point only the default behavior is in place, so
    a single core will always be used even though CONFIG_SMP
    is enabled on multicore systems.
    
    The SMP Kconfig entry for arch/arm/Kconfig is excluded from
    this patch to simplify merging.
    
    Signed-off-by: Magnus Damm <damm@opensource.se>
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

commit 6faa675c82698713380298a82cae5995bee4a4b7
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Aug 16 14:38:33 2010 +0200

    x86-32: Separate 1:1 pagetables from swapper_pg_dir
    
    commit fd89a137924e0710078c3ae855e7cec1c43cb845 upstream.
    
    This patch fixes machine crashes which occur when heavily exercising the
    CPU hotplug codepaths on a 32-bit kernel. These crashes are caused by
    AMD Erratum 383 and result in a fatal machine check exception. Here's
    the scenario:
    
    1. On 32-bit, the swapper_pg_dir page table is used as the initial page
    table for booting a secondary CPU.
    
    2. To make this work, swapper_pg_dir needs a direct mapping of physical
    memory in it (the low mappings). By adding those low, large page (2M)
    mappings (PAE kernel), we create the necessary conditions for Erratum
    383 to occur.
    
    3. Other CPUs which do not participate in the off- and onlining game may
    use swapper_pg_dir while the low mappings are present (when leave_mm is
    called). For all steps below, the CPU referred to is a CPU that is using
    swapper_pg_dir, and not the CPU which is being onlined.
    
    4. The presence of the low mappings in swapper_pg_dir can result
    in TLB entries for addresses below __PAGE_OFFSET to be established
    speculatively. These TLB entries are marked global and large.
    
    5. When the CPU with such TLB entry switches to another page table, this
    TLB entry remains because it is global.
    
    6. The process then generates an access to an address covered by the
    above TLB entry but there is a permission mismatch - the TLB entry
    covers a large global page not accessible to userspace.
    
    7. Due to this permission mismatch a new 4kb, user TLB entry gets
    established. Further, Erratum 383 provides for a small window of time
    where both TLB entries are present. This results in an uncorrectable
    machine check exception signalling a TLB multimatch which panics the
    machine.
    
    There are two ways to fix this issue:
    
            1. Always do a global TLB flush when a new cr3 is loaded and the
            old page table was swapper_pg_dir. I consider this a hack hard
            to understand and with performance implications
    
            2. Do not use swapper_pg_dir to boot secondary CPUs like 64-bit
            does.
    
    This patch implements solution 2. It introduces a trampoline_pg_dir
    which has the same layout as swapper_pg_dir with low_mappings. This page
    table is used as the initial page table of the booting CPU. Later in the
    bringup process, it switches to swapper_pg_dir and does a global TLB
    flush. This fixes the crashes in our test cases.
    
    -v2: switch to swapper_pg_dir right after entering start_secondary() so
    that we are able to access percpu data which might not be mapped in the
    trampoline page table.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    LKML-Reference: <20100816123833.GB28147@aftab>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit ba8f2de53b32dc1f13b60dab2f9fee563e5c73bd
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Aug 16 14:38:33 2010 +0200

    x86-32: Separate 1:1 pagetables from swapper_pg_dir
    
    commit fd89a137924e0710078c3ae855e7cec1c43cb845 upstream.
    
    This patch fixes machine crashes which occur when heavily exercising the
    CPU hotplug codepaths on a 32-bit kernel. These crashes are caused by
    AMD Erratum 383 and result in a fatal machine check exception. Here's
    the scenario:
    
    1. On 32-bit, the swapper_pg_dir page table is used as the initial page
    table for booting a secondary CPU.
    
    2. To make this work, swapper_pg_dir needs a direct mapping of physical
    memory in it (the low mappings). By adding those low, large page (2M)
    mappings (PAE kernel), we create the necessary conditions for Erratum
    383 to occur.
    
    3. Other CPUs which do not participate in the off- and onlining game may
    use swapper_pg_dir while the low mappings are present (when leave_mm is
    called). For all steps below, the CPU referred to is a CPU that is using
    swapper_pg_dir, and not the CPU which is being onlined.
    
    4. The presence of the low mappings in swapper_pg_dir can result
    in TLB entries for addresses below __PAGE_OFFSET to be established
    speculatively. These TLB entries are marked global and large.
    
    5. When the CPU with such TLB entry switches to another page table, this
    TLB entry remains because it is global.
    
    6. The process then generates an access to an address covered by the
    above TLB entry but there is a permission mismatch - the TLB entry
    covers a large global page not accessible to userspace.
    
    7. Due to this permission mismatch a new 4kb, user TLB entry gets
    established. Further, Erratum 383 provides for a small window of time
    where both TLB entries are present. This results in an uncorrectable
    machine check exception signalling a TLB multimatch which panics the
    machine.
    
    There are two ways to fix this issue:
    
            1. Always do a global TLB flush when a new cr3 is loaded and the
            old page table was swapper_pg_dir. I consider this a hack hard
            to understand and with performance implications
    
            2. Do not use swapper_pg_dir to boot secondary CPUs like 64-bit
            does.
    
    This patch implements solution 2. It introduces a trampoline_pg_dir
    which has the same layout as swapper_pg_dir with low_mappings. This page
    table is used as the initial page table of the booting CPU. Later in the
    bringup process, it switches to swapper_pg_dir and does a global TLB
    flush. This fixes the crashes in our test cases.
    
    -v2: switch to swapper_pg_dir right after entering start_secondary() so
    that we are able to access percpu data which might not be mapped in the
    trampoline page table.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    LKML-Reference: <20100816123833.GB28147@aftab>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 8cfdc0008542b57caadbfe013da163131a8293f4
Author: Zachary Amsden <zamsden@redhat.com>
Date:   Thu Aug 19 22:07:21 2010 -1000

    KVM: x86: Make cpu_tsc_khz updates use local CPU
    
    This simplifies much of the init code; we can now simply always
    call tsc_khz_changed, optionally passing it a new value, or letting
    it figure out the existing value (while interrupts are disabled, and
    thus, by inference from the rule, not raceful against CPU hotplug or
    frequency updates, which will issue IPIs to the local CPU to perform
    this very same task).
    
    Signed-off-by: Zachary Amsden <zamsden@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

commit 11adcc29f068229ca77fb91b42047f1a03cf500c
Author: Changhwan Youn <chaos.youn@samsung.com>
Date:   Fri Aug 20 18:17:51 2010 +0900

    ARM: S5PV310: Add CPU hotplug support for S5PV310
    
    This patch adds CPU hotplug support for S5PV310/S5PC210.
    
    Signed-off-by: Changhwan Youn <chaos.youn@samsung.com>
    Signed-off-by: Kukjin Kim <kgene.kim@samsung.com>

commit 269dcc1c2ec25864308ee03a3fa26ea819d9f5d0
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Sep 7 14:23:09 2010 -0700

    rcu: Add tracing data to support queueing models
    
    The current tracing data is not sufficient to deduce the average time
    that a callback spends waiting for a grace period to end.  Add three
    per-CPU counters recording the number of callbacks invoked (ci), the
    number of callbacks orphaned (co), and the number of callbacks adopted
    (ca).  Given the existing callback queue length (ql), the average wait
    time in absence of CPU hotplug operations is ql/ci.  The units of wait
    time will be in terms of the duration over which ci was measured.
    
    In the presence of CPU hotplug operations, there is room for argument,
    but ql/(ci-co+ca) won't steer you too far wrong.
    
    Also fixes a typo called out by Lucas De Marchi <lucas.de.marchi@gmail.com>.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

commit 9d704c04ff8ae61b60935d67ce334b18fc70f1b2
Author: Sundar Iyer <sundar.iyer@stericsson.com>
Date:   Wed Sep 15 10:45:51 2010 +0100

    ARM: 6391/1: ux500: add CPU hotplug support
    
    Acked-by: Linus Walleij <linus.walleij@stericsson.com>
    Signed-off-by: Sundar Iyer <sundar.iyer@stericsson.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit f2955b490b249ca56e465fd32cc355f84aedf8bd
Merge: 3d96406c7da1 9efdda310cb2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 10 07:31:24 2010 -0700

    Merge branch 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      tracing: t_start: reset FTRACE_ITER_HASH in case of seek/pread
      perf symbols: Fix multiple initialization of symbol system
      perf: Fix CPU hotplug
      perf, trace: Fix module leak
      tracing/kprobe: Fix handling of C-unlike argument names
      tracing/kprobes: Fix handling of argument names
      perf probe: Fix handling of arguments names
      perf probe: Fix return probe support
      tracing/kprobe: Fix a memory leak in error case
      tracing: Do not allow llseek to set_ftrace_filter

commit 5e11637e2c929e34dcc0fbbfb48bdb638937701a
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jun 11 13:35:08 2010 +0200

    perf: Fix CPU hotplug
    
    Since we have UP_PREPARE, we should also have UP_CANCELED.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit eac243355a99d6b9d41bbeba4fc83e7f735485f9
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Tue Aug 31 23:00:08 2010 -0400

    lockup_detector: Convert cpu notifier to return encapsulate errno value
    
    By the commit e6bde73b07edeb703d4c89c1daabc09c303de11f
    ("cpu-hotplug: return better errno on cpu hotplug failure"),
    the cpu notifier can return encapsulate errno value, resulting
    in more meaningful error codes for CPU hotplug failures.
    
    This converts the cpu notifier to return encapsulate errno value
    for the lockup_detector as well.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: peterz@infradead.org
    Cc: gorcunov@gmail.com
    Cc: fweisbec@gmail.com
    LKML-Reference: <1283310009-22168-3-git-send-email-dzickus@redhat.com>
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 6105020a90881ab1916a632532948255c9d28a8f
Author: Borislav Petkov <bp@amd64.org>
Date:   Thu Aug 19 20:10:29 2010 +0200

    x86, hotplug: Serialize CPU hotplug to avoid bringup concurrency issues
    
    commit d7c53c9e822a4fefa13a0cae76f3190bfd0d5c11 upstream.
    
    When testing cpu hotplug code on 32-bit we kept hitting the "CPU%d:
    Stuck ??" message due to multiple cores concurrently accessing the
    cpu_callin_mask, among others.
    
    Since these codepaths are not protected from concurrent access due to
    the fact that there's no sane reason for making an already complex
    code unnecessarily more complex - we hit the issue only when insanely
    switching cores off- and online - serialize hotplugging cores on the
    sysfs level and be done with it.
    
    [ v2.1: fix !HOTPLUG_CPU build ]
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <20100819181029.GC17171@aftab>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 858ba8a4118f7b1cef6cc70d85ffbf5fbaf9ec78
Author: Borislav Petkov <bp@amd64.org>
Date:   Thu Aug 19 20:10:29 2010 +0200

    x86, hotplug: Serialize CPU hotplug to avoid bringup concurrency issues
    
    commit d7c53c9e822a4fefa13a0cae76f3190bfd0d5c11 upstream.
    
    When testing cpu hotplug code on 32-bit we kept hitting the "CPU%d:
    Stuck ??" message due to multiple cores concurrently accessing the
    cpu_callin_mask, among others.
    
    Since these codepaths are not protected from concurrent access due to
    the fact that there's no sane reason for making an already complex
    code unnecessarily more complex - we hit the issue only when insanely
    switching cores off- and online - serialize hotplugging cores on the
    sysfs level and be done with it.
    
    [ v2.1: fix !HOTPLUG_CPU build ]
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <20100819181029.GC17171@aftab>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 0c113009da9fc094747cd3cb8bb2eb47f0c6ada2
Author: Borislav Petkov <bp@amd64.org>
Date:   Thu Aug 19 20:10:29 2010 +0200

    x86, hotplug: Serialize CPU hotplug to avoid bringup concurrency issues
    
    commit d7c53c9e822a4fefa13a0cae76f3190bfd0d5c11 upstream.
    
    When testing cpu hotplug code on 32-bit we kept hitting the "CPU%d:
    Stuck ??" message due to multiple cores concurrently accessing the
    cpu_callin_mask, among others.
    
    Since these codepaths are not protected from concurrent access due to
    the fact that there's no sane reason for making an already complex
    code unnecessarily more complex - we hit the issue only when insanely
    switching cores off- and online - serialize hotplugging cores on the
    sysfs level and be done with it.
    
    [ v2.1: fix !HOTPLUG_CPU build ]
    
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <20100819181029.GC17171@aftab>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 36423a5ed5e4ea95ceedb68fad52965033e11639
Merge: f6143a9b7329 05e407603e52
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 20 14:25:08 2010 -0700

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, apic: Fix apic=debug boot crash
      x86, hotplug: Serialize CPU hotplug to avoid bringup concurrency issues
      x86-32: Fix dummy trampoline-related inline stubs
      x86-32: Separate 1:1 pagetables from swapper_pg_dir
      x86, cpu: Fix regression in AMD errata checking code

commit d7c53c9e822a4fefa13a0cae76f3190bfd0d5c11
Author: Borislav Petkov <bp@amd64.org>
Date:   Thu Aug 19 20:10:29 2010 +0200

    x86, hotplug: Serialize CPU hotplug to avoid bringup concurrency issues
    
    When testing cpu hotplug code on 32-bit we kept hitting the "CPU%d:
    Stuck ??" message due to multiple cores concurrently accessing the
    cpu_callin_mask, among others.
    
    Since these codepaths are not protected from concurrent access due to
    the fact that there's no sane reason for making an already complex
    code unnecessarily more complex - we hit the issue only when insanely
    switching cores off- and online - serialize hotplugging cores on the
    sysfs level and be done with it.
    
    [ v2.1: fix !HOTPLUG_CPU build ]
    
    Cc: <stable@kernel.org>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    LKML-Reference: <20100819181029.GC17171@aftab>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit fd89a137924e0710078c3ae855e7cec1c43cb845
Author: Joerg Roedel <joerg.roedel@amd.com>
Date:   Mon Aug 16 14:38:33 2010 +0200

    x86-32: Separate 1:1 pagetables from swapper_pg_dir
    
    This patch fixes machine crashes which occur when heavily exercising the
    CPU hotplug codepaths on a 32-bit kernel. These crashes are caused by
    AMD Erratum 383 and result in a fatal machine check exception. Here's
    the scenario:
    
    1. On 32-bit, the swapper_pg_dir page table is used as the initial page
    table for booting a secondary CPU.
    
    2. To make this work, swapper_pg_dir needs a direct mapping of physical
    memory in it (the low mappings). By adding those low, large page (2M)
    mappings (PAE kernel), we create the necessary conditions for Erratum
    383 to occur.
    
    3. Other CPUs which do not participate in the off- and onlining game may
    use swapper_pg_dir while the low mappings are present (when leave_mm is
    called). For all steps below, the CPU referred to is a CPU that is using
    swapper_pg_dir, and not the CPU which is being onlined.
    
    4. The presence of the low mappings in swapper_pg_dir can result
    in TLB entries for addresses below __PAGE_OFFSET to be established
    speculatively. These TLB entries are marked global and large.
    
    5. When the CPU with such TLB entry switches to another page table, this
    TLB entry remains because it is global.
    
    6. The process then generates an access to an address covered by the
    above TLB entry but there is a permission mismatch - the TLB entry
    covers a large global page not accessible to userspace.
    
    7. Due to this permission mismatch a new 4kb, user TLB entry gets
    established. Further, Erratum 383 provides for a small window of time
    where both TLB entries are present. This results in an uncorrectable
    machine check exception signalling a TLB multimatch which panics the
    machine.
    
    There are two ways to fix this issue:
    
            1. Always do a global TLB flush when a new cr3 is loaded and the
            old page table was swapper_pg_dir. I consider this a hack hard
            to understand and with performance implications
    
            2. Do not use swapper_pg_dir to boot secondary CPUs like 64-bit
            does.
    
    This patch implements solution 2. It introduces a trampoline_pg_dir
    which has the same layout as swapper_pg_dir with low_mappings. This page
    table is used as the initial page table of the booting CPU. Later in the
    bringup process, it switches to swapper_pg_dir and does a global TLB
    flush. This fixes the crashes in our test cases.
    
    -v2: switch to swapper_pg_dir right after entering start_secondary() so
    that we are able to access percpu data which might not be mapped in the
    trampoline page table.
    
    Signed-off-by: Joerg Roedel <joerg.roedel@amd.com>
    LKML-Reference: <20100816123833.GB28147@aftab>
    Signed-off-by: Borislav Petkov <borislav.petkov@amd.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

commit 034260d6779087431a8b2f67589c68b919299e5c
Author: Kevin Cernekee <cernekee@gmail.com>
Date:   Thu Jun 3 22:11:25 2010 -0700

    printk: fix delayed messages from CPU hotplug events
    
    When a secondary CPU is being brought up, it is not uncommon for
    printk() to be invoked when cpu_online(smp_processor_id()) == 0.  The
    case that I witnessed personally was on MIPS:
    
    http://lkml.org/lkml/2010/5/30/4
    
    If (can_use_console() == 0), printk() will spool its output to log_buf
    and it will be visible in "dmesg", but that output will NOT be echoed to
    the console until somebody calls release_console_sem() from a CPU that
    is online.  Therefore, the boot time messages from the new CPU can get
    stuck in "limbo" for a long time, and might suddenly appear on the
    screen when a completely unrelated event (e.g. "eth0: link is down")
    occurs.
    
    This patch modifies the console code so that any pending messages are
    automatically flushed out to the console whenever a CPU hotplug
    operation completes successfully or aborts.
    
    The issue was seen on 2.6.34.
    
    Original patch by Kevin Cernekee with cleanups by akpm and additional fixes
    by Santosh Shilimkar.  This patch superseeds
    https://patchwork.linux-mips.org/patch/1357/.
    
    Signed-off-by: Kevin Cernekee <cernekee@gmail.com>
    To: <mingo@elte.hu>
    To: <akpm@linux-foundation.org>
    To: <simon.kagstrom@netinsight.net>
    To: <David.Woodhouse@intel.com>
    To: <lethal@linux-sh.org>
    Cc: <linux-kernel@vger.kernel.org>
    Cc: <linux-mips@linux-mips.org>
    Reviewed-by: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Kevin Cernekee <cernekee@gmail.com>
    Patchwork: https://patchwork.linux-mips.org/patch/1534/
    LKML-Reference: <ede63b5a20af951c755736f035d1e787772d7c28@localhost>
    LKML-Reference: <EAF47CD23C76F840A9E7FCE10091EFAB02C5DB6D1F@dbde02.ent.ti.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 3fd02a351fc8aa8602fe2b39d6a098ea2538db2e
Author: Alex Chiang <achiang@canonical.com>
Date:   Thu Jun 17 09:08:54 2010 -0600

    ACPI: processor: fix processor_physically_present on UP
    
    commit 856b185dd23da39e562983fbf28860f54e661b41 upstream.
    
    The commit 5d554a7bb06 (ACPI: processor: add internal
    processor_physically_present()) is broken on uniprocessor (UP)
    configurations, as acpi_get_cpuid() will always return -1.
    
    We use the value of num_possible_cpus() to tell us whether we got
    an invalid cpuid from acpi_get_cpuid() in the SMP case, or if
    instead, we are UP, in which case num_possible_cpus() is #defined
    as 1.
    
    We use num_possible_cpus() instead of num_online_cpus() to
    protect ourselves against the scenario of CPU hotplug, and we've
    taken down all the CPUs except one.
    
    Thanks to Jan Pogadl for initial report and analysis and Chen
    Gong for review.
    
    https://bugzilla.kernel.org/show_bug.cgi?id=16357
    
    Reported-by: Jan Pogadl <pogadl.jan@googlemail.com>:
    Reviewed-by: Chen Gong <gong.chen@linux.intel.com>
    Signed-off-by: Alex Chiang <achiang@canonical.com>
    Signed-off-by: Len Brown <len.brown@intel.com>
    Cc: Thomas Renninger <trenn@suse.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 7d35b8d09fb0cb0a89c8c265a5bfb52c2867b1d5
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Mon Aug 2 13:18:19 2010 +0300

    omap4: hotplug: Add basic CPU hotplug support
    
    This patch adds cpu hotplug support for OMAP4430. Only CPU inactive
    state is supported as a low power state in the basic hot-plug support
    
    Signed-off-by: Rajendra Nayak <rnayak@ti.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Cc: Kevin Hilman <khilman@deeprootsystems.com>
    Signed-off-by: Tony Lindgren <tony@atomide.com>

commit 3f9eaf0984952f69cb2d0c1b0f99b95b74742094
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Mon Aug 2 13:18:18 2010 +0300

    omap4: Add smc API to read AuxCoreBoot0 register
    
    This patch adds a secure API to read AuxCoreBoot0 register to
    check the cpu boot status. It also moves the other smc APIs
    to common omap44xx-smc.S. This APIs should not be marked as
    __INIT because we need these to be present for CPU hotplug
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Tony Lindgren <tony@atomide.com>

commit e8e5c2155b0035b6e04f29be67f6444bc914005b
Author: Matt Evans <matt@ozlabs.org>
Date:   Thu Jul 29 18:49:08 2010 +0000

    powerpc/kexec: Fix orphaned offline CPUs across kexec
    
    When CPU hotplug is used, some CPUs may be offline at the time a kexec is
    performed.  The subsequent kernel may expect these CPUs to be already running,
    and will declare them stuck.  On pseries, there's also a soft-offline (cede)
    state that CPUs may be in; this can also cause problems as the kexeced kernel
    may ask RTAS if they're online -- and RTAS would say they are.  The CPU will
    either appear stuck, or will cause a crash as we replace its cede loop beneath
    it.
    
    This patch kicks each present offline CPU awake before the kexec, so that
    none are forever lost to these assumptions in the subsequent kernel.
    
    Now, the behaviour is that all available CPUs that were offlined are now
    online & usable after the kexec.  This mimics the behaviour of a full reboot
    (on which all CPUs will be restarted).
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 856b185dd23da39e562983fbf28860f54e661b41
Author: Alex Chiang <achiang@canonical.com>
Date:   Thu Jun 17 09:08:54 2010 -0600

    ACPI: processor: fix processor_physically_present on UP
    
    The commit 5d554a7bb06 (ACPI: processor: add internal
    processor_physically_present()) is broken on uniprocessor (UP)
    configurations, as acpi_get_cpuid() will always return -1.
    
    We use the value of num_possible_cpus() to tell us whether we got
    an invalid cpuid from acpi_get_cpuid() in the SMP case, or if
    instead, we are UP, in which case num_possible_cpus() is #defined
    as 1.
    
    We use num_possible_cpus() instead of num_online_cpus() to
    protect ourselves against the scenario of CPU hotplug, and we've
    taken down all the CPUs except one.
    
    Thanks to Jan Pogadl for initial report and analysis and Chen
    Gong for review.
    
    https://bugzilla.kernel.org/show_bug.cgi?id=16357
    
    Reported-by: Jan Pogadl <pogadl.jan@googlemail.com>:
    Reviewed-by: Chen Gong <gong.chen@linux.intel.com>
    Signed-off-by: Alex Chiang <achiang@canonical.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

commit db7bccf45cb87522096b8f43144e31ca605a9f24
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:12 2010 +0200

    workqueue: reimplement CPU hotplugging support using trustee
    
    Reimplement CPU hotplugging support using trustee thread.  On CPU
    down, a trustee thread is created and each step of CPU down is
    executed by the trustee and workqueue_cpu_callback() simply drives and
    waits for trustee state transitions.
    
    CPU down operation no longer waits for works to be drained but trustee
    sticks around till all pending works have been completed.  If CPU is
    brought back up while works are still draining,
    workqueue_cpu_callback() tells trustee to step down and tell workers
    to rebind to the cpu.
    
    As it's difficult to tell whether cwqs are empty if it's freezing or
    frozen, trustee doesn't consider draining to be complete while a gcwq
    is freezing or frozen (tracked by new GCWQ_FREEZING flag).  Also,
    workers which get unbound from their cpu are marked with WORKER_ROGUE.
    
    Trustee based implementation doesn't bring any new feature at this
    point but it will be used to manage worker pool when dynamic shared
    worker pool is implemented.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 1537663f5763892cacf1409ac0efef1b4f332d1e
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:11 2010 +0200

    workqueue: kill cpu_populated_map
    
    Worker management is about to be overhauled.  Simplify things by
    removing cpu_populated_map, creating workers for all possible cpus and
    making single threaded workqueues behave more like multi threaded
    ones.
    
    After this patch, all cwqs are always initialized, all workqueues are
    linked on the workqueues list and workers for all possibles cpus
    always exist.  This also makes CPU hotplug support simpler - checking
    ->cpus_allowed before processing works in worker_thread() and flushing
    cwqs on CPU_POST_DEAD are enough.
    
    While at it, make get_cwq() always return the cwq for the specified
    cpu, add target_cwq() for cases where single thread distinction is
    necessary and drop all direct usage of per_cpu_ptr() on wq->cpu_wq.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 00b9b0af5887fed54e899e3b7f5c2ccf5e739def
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 27 10:32:08 2010 -0700

    Avoid warning when CPU hotplug isn't enabled
    
    Commit e9fb7631ebcd ("cpu-hotplug: introduce cpu_notify(),
    __cpu_notify(), cpu_notify_nofail()") also introduced this annoying
    warning:
    
      kernel/cpu.c:157: warning: 'cpu_notify_nofail' defined but not used
    
    when CONFIG_HOTPLUG_CPU wasn't set.
    
    So move that helper inside the #ifdef CONFIG_HOTPLUG_CPU region, and
    simplify it while at it.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit c081243f7a9278f093bbc87bc5c56f1f2d203904
Author: Adam Lackorzynski <adam@os.inf.tu-dresden.de>
Date:   Sat Feb 27 07:07:59 2010 +0000

    powerpc: Fix SMP build with disabled CPU hotplugging.
    
    commit 5b72d74ce2fccca2a301de60f31b16ddf5c93984 upstream.
    
    Compiling 2.6.33 with SMP enabled and HOTPLUG_CPU disabled gives me the
    following link errors:
    
      LD      init/built-in.o
      LD      .tmp_vmlinux1
    arch/powerpc/platforms/built-in.o: In function `.smp_xics_setup_cpu':
    smp.c:(.devinit.text+0x88): undefined reference to `.set_cpu_current_state'
    smp.c:(.devinit.text+0x94): undefined reference to `.set_default_offline_state'
    arch/powerpc/platforms/built-in.o: In function `.smp_pSeries_kick_cpu':
    smp.c:(.devinit.text+0x13c): undefined reference to `.set_preferred_offline_state'
    smp.c:(.devinit.text+0x148): undefined reference to `.get_cpu_current_state'
    smp.c:(.devinit.text+0x1a8): undefined reference to `.get_cpu_current_state'
    make: *** [.tmp_vmlinux1] Error 1
    
    The following change fixes that for me and seems to work as expected.
    
    Signed-off-by: Adam Lackorzynski <adam@os.inf.tu-dresden.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit e7dc951eecb708d4aef18db4dbf489ba282d16ff
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Apr 26 19:09:57 2010 +0900

    sh: CPU hotplug support for SH-X3 SMP.
    
    This wires up CPU hotplug for SH-X3 SMP CPUs. Presently only secondary
    cores can be hotplugged given that the boot CPU has to contend with the
    broadcast timer. When real local timers are implemented this restriction
    can be lifted.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

commit 763142d1efb56effe614d71185781796c4b83c78
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Apr 26 19:08:55 2010 +0900

    sh: CPU hotplug support.
    
    This adds preliminary support for CPU hotplug for SH SMP systems.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

commit 9715b8c7d55912fb6f5dd9b1c084d8eefcd0d848
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Mon Apr 26 18:49:58 2010 +0900

    sh: provide percpu CPU states for hotplug notifiers.
    
    This provides percpu CPU states in preparation for CPU hotplug and the
    associated notifier chains.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

commit a9079ca0cb15feda15e7a380092e02d5cd834148
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Apr 21 12:01:06 2010 +0900

    sh: Tidy CPU probing and fixup section annotations.
    
    This does a detect_cpu_and_cache_system() -> cpu_probe() rename, tidies
    up the unused return value, and stuffs it under __cpuinit in preparation
    for CPU hotplug.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

commit ae7c9b70dcb4313ea3dbcc9a2f240dae6c2b50c0
Author: Jacob Pan <jacob.jun.pan@linux.intel.com>
Date:   Mon Apr 19 11:23:43 2010 -0700

    x86, mrst: Conditionally register cpu hotplug notifier for apbt
    
    APB timer is used on Moorestown platforms but not on a standard PC.
    If APB timer code is compiled in but not initialized at run-time due
    to lack of FW reported SFI table, kernel would panic when the non-boot
    CPUs are offlined and notifier is called.
    
    https://bugzilla.kernel.org/show_bug.cgi?id=15786
    
    This patch ensures CPU hotplug notifier for APB timer is only registered
    when the APBT timer block is initialized.
    
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    LKML-Reference: <1271701423-1162-1-git-send-email-jacob.jun.pan@linux.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

commit dd5a2355ad5af18cc8fef4844e51e49197942c3a
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Feb 18 15:30:55 2010 -0800

    x86, apic: Don't use logical-flat mode when CPU hotplug may exceed 8 CPUs
    
    commit 681ee44d40d7c93b42118320e4620d07d8704fd6 upstream
    
    We need to fall back from logical-flat APIC mode to physical-flat mode
    when we have more than 8 CPUs.  However, in the presence of CPU
    hotplug(with bios listing not enabled but possible cpus as disabled cpus in
    MADT), we have to consider the number of possible CPUs rather than
    the number of current CPUs; otherwise we may cross the 8-CPU boundary
    when CPUs are added later.
    
    32bit apic code can use more cleanups (like the removal of vendor checks in
    32bit default_setup_apic_routing()) and more unifications with 64bit code.
    Yinghai has some patches in works already. This patch addresses the boot issue
    that is reported in the virtualization guest context.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: Shaohui Zheng <shaohui.zheng@intel.com>
    Reviewed-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit b6fedfd2a18a489d31246312f7279f82e3cc6b37
Merge: c32da02342b7 30124d11097e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 12 16:06:51 2010 -0800

    Merge branch 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc:
      powerpc/booke: Fix breakpoint/watchpoint one-shot behavior
      powerpc: Reduce printk from pseries_mach_cpu_die()
      powerpc: Move checks in pseries_mach_cpu_die()
      powerpc: Reset kernel stack on cpu online from cede state
      powerpc: Fix G5 thermal shutdown
      powerpc/pseries: Pass CPPR value to H_XIRR hcall
      powerpc/booke: Fix a couple typos in the advanced ptrace code
      powerpc: Fix SMP build with disabled CPU hotplugging.
      powerpc: Dynamically allocate pacas
      powerpc/perf: e500 support
      powerpc/perf: Build callchain code regardless of hardware event support.
      powerpc/cpm2: Checkpatch cleanup
      powerpc/86xx: Renaming following split of GE Fanuc joint venture
      powerpc/86xx: Convert gef_pic_lock to raw_spinlock
      powerpc/qe: Convert qe_ic_lock to raw_spinlock
      powerpc/82xx: Convert pci_pic_lock to raw_spinlock
      powerpc/85xx: Convert socrates_fpga_pic_lock to raw_spinlock

commit 0b39db28b953945232719e7ff6fb802aa8a2be5f
Author: Graf Yang <graf.yang@analog.com>
Date:   Mon Dec 28 11:13:51 2009 +0000

    Blackfin: SMP: add PM/CPU hotplug support
    
    Signed-off-by: Graf Yang <graf.yang@analog.com>
    Signed-off-by: Mike Frysinger <vapier@gentoo.org>

commit 5b72d74ce2fccca2a301de60f31b16ddf5c93984
Author: Adam Lackorzynski <adam@os.inf.tu-dresden.de>
Date:   Sat Feb 27 07:07:59 2010 +0000

    powerpc: Fix SMP build with disabled CPU hotplugging.
    
    Compiling 2.6.33 with SMP enabled and HOTPLUG_CPU disabled gives me the
    following link errors:
    
      LD      init/built-in.o
      LD      .tmp_vmlinux1
    arch/powerpc/platforms/built-in.o: In function `.smp_xics_setup_cpu':
    smp.c:(.devinit.text+0x88): undefined reference to `.set_cpu_current_state'
    smp.c:(.devinit.text+0x94): undefined reference to `.set_default_offline_state'
    arch/powerpc/platforms/built-in.o: In function `.smp_pSeries_kick_cpu':
    smp.c:(.devinit.text+0x13c): undefined reference to `.set_preferred_offline_state'
    smp.c:(.devinit.text+0x148): undefined reference to `.get_cpu_current_state'
    smp.c:(.devinit.text+0x1a8): undefined reference to `.get_cpu_current_state'
    make: *** [.tmp_vmlinux1] Error 1
    
    The following change fixes that for me and seems to work as expected.
    
    Signed-off-by: Adam Lackorzynski <adam@os.inf.tu-dresden.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit 5ea8d3759244590defd369828c965101c97b65e1
Merge: f2d6cff7f525 681ee44d40d7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 11 14:01:10 2010 -0800

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, apic: Don't use logical-flat mode when CPU hotplug may exceed 8 CPUs
      x86-32: Make AT_VECTOR_SIZE_ARCH=2
      x86/agp: Fix amd64-agp module initialization regression
      x86, doc: Fix minor spelling error in arch/x86/mm/gup.c

commit 681ee44d40d7c93b42118320e4620d07d8704fd6
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Tue Feb 9 18:01:44 2010 -0800

    x86, apic: Don't use logical-flat mode when CPU hotplug may exceed 8 CPUs
    
    We need to fall back from logical-flat APIC mode to physical-flat mode
    when we have more than 8 CPUs.  However, in the presence of CPU
    hotplug(with bios listing not enabled but possible cpus as disabled cpus in
    MADT), we have to consider the number of possible CPUs rather than
    the number of current CPUs; otherwise we may cross the 8-CPU boundary
    when CPUs are added later.
    
    32bit apic code can use more cleanups (like the removal of vendor checks in
    32bit default_setup_apic_routing()) and more unifications with 64bit code.
    Yinghai has some patches in works already. This patch addresses the boot issue
    that is reported in the virtualization guest context.
    
    [ hpa: incorporated function annotation feedback from Yinghai Lu ]
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    LKML-Reference: <1265767304.2833.19.camel@sbs-t61.sc.intel.com>
    Acked-by: Shaohui Zheng <shaohui.zheng@intel.com>
    Reviewed-by: Yinghai Lu <yinghai@kernel.org>
    Cc: <stable@kernel.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

commit fa3f5a5c1c8e6a2cbc7e21755ea7c215f8cf0577
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 10 15:35:10 2009 +0100

    clockevents: Prevent clockevent_devices list corruption on cpu hotplug
    
    commit bb6eddf7676e1c1f3e637aa93c5224488d99036f upstream.
    
    Xiaotian Feng triggered a list corruption in the clock events list on
    CPU hotplug and debugged the root cause.
    
    If a CPU registers more than one per cpu clock event device, then only
    the active clock event device is removed on CPU_DEAD. The unused
    devices are kept in the clock events device list.
    
    On CPU up the clock event devices are registered again, which means
    that we list_add an already enqueued list_head. That results in list
    corruption.
    
    Resolve this by removing all devices which are associated to the dead
    CPU on CPU_DEAD.
    
    Reported-by: Xiaotian Feng <dfeng@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Xiaotian Feng <dfeng@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 5c87f0c04abf876c8019c04ff581976a9a241206
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 10 15:35:10 2009 +0100

    clockevents: Prevent clockevent_devices list corruption on cpu hotplug
    
    commit bb6eddf7676e1c1f3e637aa93c5224488d99036f upstream.
    
    Xiaotian Feng triggered a list corruption in the clock events list on
    CPU hotplug and debugged the root cause.
    
    If a CPU registers more than one per cpu clock event device, then only
    the active clock event device is removed on CPU_DEAD. The unused
    devices are kept in the clock events device list.
    
    On CPU up the clock event devices are registered again, which means
    that we list_add an already enqueued list_head. That results in list
    corruption.
    
    Resolve this by removing all devices which are associated to the dead
    CPU on CPU_DEAD.
    
    Reported-by: Xiaotian Feng <dfeng@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Xiaotian Feng <dfeng@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 00afa758067ac1c947149ef766adcdfe30c44d7d
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Sun Dec 27 14:33:14 2009 +0200

    SLAB: Fix lockdep annotation breakage
    
    Commit ce79ddc8e2376a9a93c7d42daf89bfcbb9187e62 ("SLAB: Fix lockdep annotations
    for CPU hotplug") broke init_node_lock_keys() off-slab logic which causes
    lockdep false positives.
    
    Fix that up by reverting the logic back to original while keeping CPU hotplug
    fixes intact.
    
    Reported-and-tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Reported-and-tested-by: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

commit 8d0e7fb9d1581c4543ea917c2ea1a50db607a344
Merge: 702a7c7609be 355d79c87a53
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 12 11:37:39 2009 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/slab-2.6
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/penberg/slab-2.6:
      slab, kmemleak: pass the correct pointer to kmemleak_erase()
      slab, kmemleak: stop calling kmemleak_erase() unconditionally
      SLAB: Fix unlikely() annotation in __cache_alloc_node()
      SLAB: Fix lockdep annotations for CPU hotplug
      SLUB: Fix __GFP_ZERO unlikely() annotation
      slub: allow stats to be cleared

commit bb6eddf7676e1c1f3e637aa93c5224488d99036f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 10 15:35:10 2009 +0100

    clockevents: Prevent clockevent_devices list corruption on cpu hotplug
    
    Xiaotian Feng triggered a list corruption in the clock events list on
    CPU hotplug and debugged the root cause.
    
    If a CPU registers more than one per cpu clock event device, then only
    the active clock event device is removed on CPU_DEAD. The unused
    devices are kept in the clock events device list.
    
    On CPU up the clock event devices are registered again, which means
    that we list_add an already enqueued list_head. That results in list
    corruption.
    
    Resolve this by removing all devices which are associated to the dead
    CPU on CPU_DEAD.
    
    Reported-by: Xiaotian Feng <dfeng@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Xiaotian Feng <dfeng@redhat.com>
    Cc: stable@kernel.org

commit 607781762e7aae9c976f0a9a8829d4ba3e2da4ab
Merge: d0b093a8b5ae 8bfb2f8e655b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 5 09:52:14 2009 -0800

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (31 commits)
      rcu: Make RCU's CPU-stall detector be default
      rcu: Add expedited grace-period support for preemptible RCU
      rcu: Enable fourth level of TREE_RCU hierarchy
      rcu: Rename "quiet" functions
      rcu: Re-arrange code to reduce #ifdef pain
      rcu: Eliminate unneeded function wrapping
      rcu: Fix grace-period-stall bug on large systems with CPU hotplug
      rcu: Eliminate __rcu_pending() false positives
      rcu: Further cleanups of use of lastcomp
      rcu: Simplify association of forced quiescent states with grace periods
      rcu: Accelerate callback processing on CPUs not detecting GP end
      rcu: Mark init-time-only rcu_bootup_announce() as __init
      rcu: Simplify association of quiescent states with grace periods
      rcu: Rename dynticks_completed to completed_fqs
      rcu: Enable synchronize_sched_expedited() fastpath
      rcu: Remove inline from forward-referenced functions
      rcu: Fix note_new_gpnum() uses of ->gpnum
      rcu: Fix synchronization for rcu_process_gp_end() uses of ->completed counter
      rcu: Prepare for synchronization fixes: clean up for non-NO_HZ handling of ->completed counter
      rcu: Cleanup: balance rcu_irq_enter()/rcu_irq_exit() calls
      ...

commit ce79ddc8e2376a9a93c7d42daf89bfcbb9187e62
Author: Pekka Enberg <penberg@cs.helsinki.fi>
Date:   Mon Nov 23 22:01:15 2009 +0200

    SLAB: Fix lockdep annotations for CPU hotplug
    
    As reported by Paul McKenney:
    
      I am seeing some lockdep complaints in rcutorture runs that include
      frequent CPU-hotplug operations.  The tests are otherwise successful.
      My first thought was to send a patch that gave each array_cache
      structure's ->lock field its own struct lock_class_key, but you already
      have a init_lock_keys() that seems to be intended to deal with this.
    
      ------------------------------------------------------------------------
    
      =============================================
      [ INFO: possible recursive locking detected ]
      2.6.32-rc4-autokern1 #1
      ---------------------------------------------
      syslogd/2908 is trying to acquire lock:
       (&nc->lock){..-...}, at: [<c0000000001407f4>] .kmem_cache_free+0x118/0x2d4
    
      but task is already holding lock:
       (&nc->lock){..-...}, at: [<c0000000001411bc>] .kfree+0x1f0/0x324
    
      other info that might help us debug this:
      3 locks held by syslogd/2908:
       #0:  (&u->readlock){+.+.+.}, at: [<c0000000004556f8>] .unix_dgram_recvmsg+0x70/0x338
       #1:  (&nc->lock){..-...}, at: [<c0000000001411bc>] .kfree+0x1f0/0x324
       #2:  (&parent->list_lock){-.-...}, at: [<c000000000140f64>] .__drain_alien_cache+0x50/0xb8
    
      stack backtrace:
      Call Trace:
      [c0000000e8ccafc0] [c0000000000101e4] .show_stack+0x70/0x184 (unreliable)
      [c0000000e8ccb070] [c0000000000afebc] .validate_chain+0x6ec/0xf58
      [c0000000e8ccb180] [c0000000000b0ff0] .__lock_acquire+0x8c8/0x974
      [c0000000e8ccb280] [c0000000000b2290] .lock_acquire+0x140/0x18c
      [c0000000e8ccb350] [c000000000468df0] ._spin_lock+0x48/0x70
      [c0000000e8ccb3e0] [c0000000001407f4] .kmem_cache_free+0x118/0x2d4
      [c0000000e8ccb4a0] [c000000000140b90] .free_block+0x130/0x1a8
      [c0000000e8ccb540] [c000000000140f94] .__drain_alien_cache+0x80/0xb8
      [c0000000e8ccb5e0] [c0000000001411e0] .kfree+0x214/0x324
      [c0000000e8ccb6a0] [c0000000003ca860] .skb_release_data+0xe8/0x104
      [c0000000e8ccb730] [c0000000003ca2ec] .__kfree_skb+0x20/0xd4
      [c0000000e8ccb7b0] [c0000000003cf2c8] .skb_free_datagram+0x1c/0x5c
      [c0000000e8ccb830] [c00000000045597c] .unix_dgram_recvmsg+0x2f4/0x338
      [c0000000e8ccb920] [c0000000003c0f14] .sock_recvmsg+0xf4/0x13c
      [c0000000e8ccbb30] [c0000000003c28ec] .SyS_recvfrom+0xb4/0x130
      [c0000000e8ccbcb0] [c0000000003bfb78] .sys_recv+0x18/0x2c
      [c0000000e8ccbd20] [c0000000003ed388] .compat_sys_recv+0x14/0x28
      [c0000000e8ccbd90] [c0000000003ee1bc] .compat_sys_socketcall+0x178/0x220
      [c0000000e8ccbe30] [c0000000000085d4] syscall_exit+0x0/0x40
    
    This patch fixes the issue by setting up lockdep annotations during CPU
    hotplug.
    
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

commit b668c9cf3e58739dac54a1d6f42f2b4bdd980b3e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Nov 22 08:53:48 2009 -0800

    rcu: Fix grace-period-stall bug on large systems with CPU hotplug
    
    When the last CPU of a given leaf rcu_node structure goes
    offline, all of the tasks queued on that leaf rcu_node structure
    (due to having blocked in their current RCU read-side critical
    sections) are requeued onto the root rcu_node structure.  This
    requeuing is carried out by rcu_preempt_offline_tasks().
    However, it is possible that these queued tasks are the only
    thing preventing the leaf rcu_node structure from reporting a
    quiescent state up the rcu_node hierarchy.  Unfortunately, the
    old code would fail to do this reporting, resulting in a
    grace-period stall given the following sequence of events:
    
    1.      Kernel built for more than 32 CPUs on 32-bit systems or for more
            than 64 CPUs on 64-bit systems, so that there is more than one
            rcu_node structure.  (Or CONFIG_RCU_FANOUT is artificially set
            to a number smaller than CONFIG_NR_CPUS.)
    
    2.      The kernel is built with CONFIG_TREE_PREEMPT_RCU.
    
    3.      A task running on a CPU associated with a given leaf rcu_node
            structure blocks while in an RCU read-side critical section
            -and- that CPU has not yet passed through a quiescent state
            for the current RCU grace period.  This will cause the task
            to be queued on the leaf rcu_node's blocked_tasks[] array, in
            particular, on the element of this array corresponding to the
            current grace period.
    
    4.      Each of the remaining CPUs corresponding to this same leaf rcu_node
            structure pass through a quiescent state.  However, the task is
            still in its RCU read-side critical section, so these quiescent
            states cannot be reported further up the rcu_node hierarchy.
            Nevertheless, all bits in the leaf rcu_node structure's ->qsmask
            field are now zero.
    
    5.      Each of the remaining CPUs go offline.  (The events in step
            #4 and #5 can happen in any order as long as each CPU passes
            through a quiescent state before going offline.)
    
    6.      When the last CPU goes offline, __rcu_offline_cpu() will invoke
            rcu_preempt_offline_tasks(), which will move the task to the
            root rcu_node structure, but without reporting a quiescent state
            up the rcu_node hierarchy (and this failure to report a quiescent
            state is the bug).
    
            But because this leaf rcu_node structure's ->qsmask field is
            already zero and its ->block_tasks[] entries are all empty,
            force_quiescent_state() will skip this rcu_node structure.
    
            Therefore, grace periods are now hung.
    
    This patch abstracts some code out of rcu_read_unlock_special(),
    calling the result task_quiet() by analogy with cpu_quiet(), and
    invokes task_quiet() from both rcu_read_lock_special() and
    __rcu_offline_cpu().  Invoking task_quiet() from
    __rcu_offline_cpu() reports the quiescent state up the rcu_node
    hierarchy, fixing the bug.  This ends up requiring a separate
    lock_class_key per level of the rcu_node hierarchy, which this
    patch also provides.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <12589088301770-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e74f4c4564455c91a3b4075bb1721993c2a95dda
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 6 21:48:17 2009 -0700

    rcu: Make hot-unplugged CPU relinquish its own RCU callbacks
    
    The current interaction between RCU and CPU hotplug requires that
    RCU block in CPU notifiers waiting for callbacks to drain.
    
    This can be greatly simplified by having each CPU relinquish its
    own callbacks, and for both _rcu_barrier() and CPU_DEAD notifiers
    to adopt all callbacks that were previously relinquished.
    
    This change also eliminates the possibility of certain types of
    hangs due to the previous practice of waiting for callbacks to be
    invoked from within CPU notifiers.  If you don't every wait, you
    cannot hang.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <1254890898456-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 27569620c748ec13f801b4683b448a2ac2adaae4
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 15 09:53:46 2009 -0700

    rcu: Split hierarchical RCU initialization into boot-time and CPU-online pieces
    
    This patch divides the rcutree initialization into boot-time
    and hotplug-time components, so that the tree data structures
    are guaranteed to be fully linked at boot time regardless of
    what might happen in CPU hotplug operations.
    
    This makes RCU more resilient against CPU hotplug misbehavior
    (and vice versa), but more importantly, does a better job of
    compartmentalizing the code.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: josht@linux.vnet.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: hugh.dickins@tiscali.co.uk
    Cc: benh@kernel.crashing.org
    LKML-Reference: <1250355231152-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit c1815e074079838d36d89e45e92b7ee317190700
Author: Bjorn Helgaas <bjorn.helgaas@hp.com>
Date:   Mon Jun 22 20:41:04 2009 +0000

    ACPI: processor: remove KOBJ_ONLINE/KOBJ_OFFLINE events
    
    This patch removes the KOBJ_ONLINE/KOBJ_OFFLINE events the driver used
    to generate for CPU hotplug.  As far as I know, nobody consumes these.
    The driver core still generates KOBJ_ADD and KOBJ_REMOVE, of course.
    
    Signed-off-by: Bjorn Helgaas <bjorn.helgaas@hp.com>
    CC: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    CC: Zhao Yakui <yakui.zhao@intel.com>
    CC: Matthew Garrett <mjg@redhat.com>
    CC: Thomas Renninger <trenn@suse.de>
    CC: Dave Jones <davej@codemonkey.org.uk>
    CC: Kay Sievers <kay.sievers@vrfy.org>
    CC: Greg Kroah-Hartman <gregkh@suse.de>
    Signed-off-by: Len Brown <len.brown@intel.com>

commit c82e6d450fda56cb2d4f68534173d3cd11b32f9f
Merge: c3cb5e193937 a620c1632629
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 24 10:47:38 2009 -0700

    Merge branch 'upstream' of git://ftp.linux-mips.org/pub/scm/upstream-linus
    
    * 'upstream' of git://ftp.linux-mips.org/pub/scm/upstream-linus:
      Staging: octeon-ethernet: Fix race freeing transmit buffers.
      Staging: octeon-ethernet: Convert to use net_device_ops.
      MIPS: Cavium: Add CPU hotplugging code.
      MIPS: SMP: Allow suspend and hibernation if CPU hotplug is available
      MIPS: Add arch generic CPU hotplug
      DMA: txx9dmac: use dma_unmap_single if DMA_COMPL_{SRC,DEST}_UNMAP_SINGLE set
      MIPS: Sibyte: Fix build error if CONFIG_SERIAL_SB1250_DUART is undefined.
      MIPS: MIPSsim: Fix build error if MSC01E_INT_BASE is undefined.
      MIPS: Hibernation: Remove SMP TLB and cacheflushing code.
      MIPS: Build fix - include <linux/smp.h> into all smp_processor_id() users.
      MIPS: bug.h Build fix - include <linux/compiler.h>.

commit 773cb77d0e32f0a3c36edf5aaeb9642c18038cd2
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Jun 23 10:36:38 2009 +0100

    MIPS: Cavium: Add CPU hotplugging code.
    
    Thanks to Cavium Inc. for the code contribution and help.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 9801b321ecdb6708365b6825bf728c8e433fca00
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Jun 23 10:20:56 2009 +0100

    MIPS: SMP: Allow suspend and hibernation if CPU hotplug is available
    
    The SMP implementation of suspend and hibernate depends on CPU hotplugging.
    In the past we didn't have CPU hotplug so suspend and hibernation were not
    possible on SMP systems.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 1b2bc75c1bde6581d2694cb3ed7fb06b69685008
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Jun 23 10:00:31 2009 +0100

    MIPS: Add arch generic CPU hotplug
    
    Each platform has to add support for CPU hotplugging itself by providing
    suitable definitions for the cpu_disable and cpu_die of the smp_ops
    methods and setting SYS_SUPPORTS_HOTPLUG_CPU.  A platform should only set
    SYS_SUPPORTS_HOTPLUG_CPU once all it's smp_ops definitions have the
    necessary changes.  This patch contains the changes to the dummy smp_ops
    definition for uni-processor systems.
    
    Parts of the code contributed by Cavium Inc.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit 0c26d7cc31cd81a82be3b9d7687217d49fe9c47e
Merge: 936940a9c7e3 21ab01e2fcbf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 24 10:17:07 2009 -0700

    Merge branch 'release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-acpi-2.6
    
    * 'release' of git://git.kernel.org/pub/scm/linux/kernel/git/lenb/linux-acpi-2.6: (72 commits)
      asus-laptop: remove EXPERIMENTAL dependency
      asus-laptop: use pr_fmt and pr_<level>
      eeepc-laptop: cpufv updates
      eeepc-laptop: sync eeepc-laptop with asus_acpi
      asus_acpi: Deprecate in favor of asus-laptop
      acpi4asus: update MAINTAINER and KConfig links
      asus-laptop: platform dev as parent for led and backlight
      eeepc-laptop: enable camera by default
      ACPI: Rename ACPI processor device bus ID
      acerhdf: Acer Aspire One fan control
      ACPI: video: DMI workaround broken Acer 7720 BIOS enabling display brightness
      ACPI: run ACPI device hot removal in kacpi_hotplug_wq
      ACPI: Add the reference count to avoid unloading ACPI video bus twice
      ACPI: DMI to disable Vista compatibility on some Sony laptops
      ACPI: fix a deadlock in hotplug case
      Show the physical device node of backlight class device.
      ACPI: pdc init related memory leak with physical CPU hotplug
      ACPI: pci_root: remove unused dev/fn information
      ACPI: pci_root: simplify list traversals
      ACPI: pci_root: use driver data rather than list lookup
      ...

commit 7b768f07dce463a054c9dd84862d15ccc3d2b712
Author: Pallipadi, Venkatesh <venkatesh.pallipadi@intel.com>
Date:   Fri Jun 19 17:14:59 2009 -0700

    ACPI: pdc init related memory leak with physical CPU hotplug
    
    arch_acpi_processor_cleanup_pdc() in x86 and ia64 results in memory allocated
    for _PDC objects that is never freed and will cause memory leak in case of
    physical CPU remove and add. Patch fixes the memory leak by freeing the
    objects soon after _PDC is evaluated.
    
    Reported-by: Bjorn Helgaas <bjorn.helgaas@hp.com>
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

commit 363c55cae53742f3f685a1814912c6d4fda245b4
Author: Wu Zhangjin <wuzj@lemote.com>
Date:   Thu Jun 4 20:27:10 2009 +0800

    MIPS: Add hibernation support
    
    [Ralf: SMP support requires CPU hotplugging which MIPS currently doesn't
    support.  As implemented in this patch cache and tlb flushing will also be
    invoked with interrupts disabled so smp_call_function() will blow up in
    charming ways.  So limit to !SMP.]
    
    Reviewed-by: Pavel Machek <pavel@ucw.cz>
    Reviewed-by: Yan Hua <yanh@lemote.com>
    Reviewed-by: Arnaud Patard <apatard@mandriva.com>
    Reviewed-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Wu Zhangjin <wuzj@lemote.com>
    Signed-off-by: Hu Hongbing <huhb@lemote.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit f8c3301e8336334be62527c5314018f75f58ad4b
Merge: 437f7fdb607f 6329db8bd60f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 24 07:44:58 2009 -0700

    Merge branch 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc
    
    * 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc:
      powerpc: Fix modular build of ide-pmac when mediabay is built in
      powerpc/pasemi: Fix build error on UP
      powerpc: Make macintosh/mediabay driver depend on CONFIG_BLOCK
      maintainers: Fix PS3 patterns
      powerpc/ps3: Fix CONFIG_PS3_FLASH=n build warning
      powerpc/32: Don't clobber personality flags on exec
      powerpc: Fix crash on CPU hotplug
      powerpc/85xx: Remove defconfigs that mpc85xx_{smp_}defconfig cover
      powerpc/85xx: Added SMP defconfig
      powerpc/85xx: Enabled a bunch of FSL specific drivers/options
      powerpc/85xx: Updated generic mpc85xx_defconfig
      powerpc: don't disable SATA interrupts on Freescale MPC8610 HPCD
      fsl_rio: Pass the proper device to dma mapping routines
      powerpc: Fix of_node_put() exit path in of_irq_map_one()
      powerpc/5200: defconfig updates
      powerpc/5200: Add FLASH nodes to lite5200 device tree
      powerpc/device-tree: Document MTD nodes with multiple "reg" tuples
      powerpc/of-device-tree: Factor MTD physmap bindings out of booting-without-of
      powerpc/5200: Bring the legacy fsl_spi_platform_data hooks back

commit 24f1ce803c55f645429e6a7dd94763fbace84b0a
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Apr 16 04:47:32 2009 +0000

    powerpc: Fix crash on CPU hotplug
    
    early_init_mmu_secondary() is called at CPU hotplug time, so it
    must be marked as __cpuinit, not __init.
    
    Caused by 757c74d2 ("powerpc/mm: Introduce early_init_mmu() on 64-bit").
    
    Tested-by: Sachin Sant <sachinp@in.ibm.com>
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

commit 8976f424d43c80ea32b6e847226e1a8ccdb6e748
Author: Robert Love <robert.w.love@intel.com>
Date:   Tue Mar 17 11:41:46 2009 -0700

    [SCSI] fcoe: create/destroy fcoe Rx threads on CPU hotplug events
    
    This patch adds support for dynamically created Rx threads
    upon CPU hotplug events.
    
    There were existing synchronization problems that this patch
    attempts to resolve. The main problem had to do with fcoe_rcv()
    running in a different context than the hotplug notifications.
    This opened the possiblity that fcoe_rcv() would target a Rx
    thread for a skb. However, that thread could become NULL if
    the CPU was made offline.
    
    This patch uses the Rx queue's (a skb_queue) lock to protect
    the thread it's associated with and we use the 'thread' member
    of the fcoe_percpu_s to determine if the thread is ready to
    accept new skbs.
    
    The patch also attempts to do a better job of cleaning up, both
    if hotplug registration fails as well as when the module is
    removed.
    
    Contribution provided by Joe Eykholt <jeykholt@cisco.com> to
    fix incorrect use of __cpuinitdata.
    
    Signed-off-by: Yi Zou <yi.zou@intel.com>
    Signed-off-by: Robert Love <robert.w.love@intel.com>
    Signed-off-by: James Bottomley <James.Bottomley@HansenPartnership.com>

commit 59222efe2d184956464abe5b637bc842ff053b93
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Mar 12 11:46:03 2009 -0400

    ring-buffer: use CONFIG_HOTPLUG_CPU not CONFIG_HOTPLUG
    
    The hotplug code in the ring buffers is for use with CPU hotplug,
    not generic hotplug.
    
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

commit 2d542cf34264ac92e9e7ac55c0b096b066d569d2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Feb 25 08:40:09 2009 +0100

    tracing/hw-branch-tracing: convert bts-tracer mutex to a spinlock
    
    Impact: fix CPU hotplug lockup
    
    bts_hotcpu_handler() is called with irqs disabled, so using mutex_lock()
    is a no-no.
    
    All the BTS codepaths here are atomic (they do not schedule), so using
    a spinlock is the right solution.
    
    Cc: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit ec5b3d32437571b8a742069a4cfd04edb6b6eda5
Author: H. Peter Anvin <hpa@linux.intel.com>
Date:   Mon Feb 23 14:01:04 2009 -0800

    x86, mce: remove invalid __cpuinit/__cpuexit annotations
    
    Impact: Bug fix when CPU hotplug is disabled
    
    Correct the following broken __cpuinit/__cpuexit annotations:
    
    - mce_cpu_features() is called from mce_resume(), and so cannot be
      __cpuinit.
    - mce_disable_cpu() and mce_reenable_cpu() are called from
      mce_cpu_callback(), and so cannot be __cpuexit().
    
    Cc: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

commit cc3ca22063784076bd240fda87217387a8f2ae92
Author: H. Peter Anvin <hpa@zytor.com>
Date:   Fri Feb 20 23:35:51 2009 -0800

    x86, mce: remove incorrect __cpuinit for mce_cpu_features()
    
    Impact: Bug fix on UP
    
    Checkin 6ec68bff3c81e776a455f6aca95c8c5f1d630198:
        x86, mce: reinitialize per cpu features on resume
    
    introduced a call to mce_cpu_features() in the resume path, in order
    for the MCE machinery to get properly reinitialized after a resume.
    However, this function (and its successors) was flagged __cpuinit,
    which becomes __init on UP configurations (on SMP suspend/resume
    requires CPU hotplug and so this would not be seen.)
    
    Remove the offending __cpuinit annotations for mce_cpu_features() and
    its successor functions.
    
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

commit 94df7de0289bc2df3d6e85cd2ece52bf42682f45
Author: Sebastien Dugue <sebastien.dugue@bull.net>
Date:   Mon Dec 1 14:09:07 2008 +0100

    hrtimers: allow the hot-unplugging of all cpus
    
    Impact: fix CPU hotplug hang on Power6 testbox
    
    On architectures that support offlining all cpus (at least powerpc/pseries),
    hot-unpluging the tick_do_timer_cpu can result in a system hang.
    
    This comes from the fact that if the cpu going down happens to be the
    cpu doing the tick, then as the tick_do_timer_cpu handover happens after the
    cpu is dead (via the CPU_DEAD notification), we're left without ticks,
    jiffies are frozen and any task relying on timers (msleep, ...) is stuck.
    That's particularly the case for the cpu looping in __cpu_die() waiting
    for the dying cpu to be dead.
    
    This patch addresses this by having the tick_do_timer_cpu handover happen
    earlier during the CPU_DYING notification. For this, a new clockevent
    notification type is introduced (CLOCK_EVT_NOTIFY_CPU_DYING) which is triggered
    in hrtimer_cpu_notify().
    
    Signed-off-by: Sebastien Dugue <sebastien.dugue@bull.net>
    Cc: <stable@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit f5813d94279a18ff5936d675e24b44b44a571197
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Wed Jan 7 18:08:40 2009 -0800

    cpusets: set task's cpu_allowed to cpu_possible_map when attaching it into top cpuset
    
    I found a bug on my dual-cpu box.  I created a sub cpuset in top cpuset
    and assign 1 to its cpus.  And then we attach some tasks into this sub
    cpuset.  After this, we offline CPU1.  Now, the tasks in this new cpuset
    are moved into top cpuset automatically because there is no cpu in sub
    cpuset.  Then we online CPU1, we find all the tasks which doesn't belong
    to top cpuset originally just run on CPU0.
    
    We fix this bug by setting task's cpu_allowed to cpu_possible_map when
    attaching it into top cpuset.  This method needn't modify the current
    behavior of cpusets on CPU hotplug, and all of tasks in top cpuset use
    cpu_possible_map to initialize their cpu_allowed.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Cc: Paul Menage <menage@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 5762ba1873b0bb9faa631aaa02f533c2b9837f82
Author: Sebastien Dugue <sebastien.dugue@bull.net>
Date:   Mon Dec 1 14:09:07 2008 +0100

    hrtimers: allow the hot-unplugging of all cpus
    
    Impact: fix CPU hotplug hang on Power6 testbox
    
    On architectures that support offlining all cpus (at least powerpc/pseries),
    hot-unpluging the tick_do_timer_cpu can result in a system hang.
    
    This comes from the fact that if the cpu going down happens to be the
    cpu doing the tick, then as the tick_do_timer_cpu handover happens after the
    cpu is dead (via the CPU_DEAD notification), we're left without ticks,
    jiffies are frozen and any task relying on timers (msleep, ...) is stuck.
    That's particularly the case for the cpu looping in __cpu_die() waiting
    for the dying cpu to be dead.
    
    This patch addresses this by having the tick_do_timer_cpu handover happen
    earlier during the CPU_DYING notification. For this, a new clockevent
    notification type is introduced (CLOCK_EVT_NOTIFY_CPU_DYING) which is triggered
    in hrtimer_cpu_notify().
    
    Signed-off-by: Sebastien Dugue <sebastien.dugue@bull.net>
    Cc: <stable@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit cf9b303e55da810255638c0b616b1a3f7eda9320
Author: Andi Kleen <andi@firstfloor.org>
Date:   Mon Dec 15 23:33:10 2008 +0100

    x86: re-enable MCE on secondary CPUS after suspend/resume
    
    Impact: fix disabled MCE after resume
    
    Don't prevent multiple initialization of MCEs.
    
    Back from early prehistory mcheck_init() has a reentry check. Presumably
    that was needed in very old kernels to prevent it entering twice.
    
    But as Andreas points out this prevents CPU hotplug (and therefore resume)
    to correctly reinitialize MCEs when a AP boots again after being
    offlined.
    
    Just drop the check.
    
    Reported-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Tested-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 37810659ea7d9572c5ac284ade272f806ef8f788
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Dec 4 11:17:10 2008 +0100

    hrtimer: removing all ur callback modes, fix hotplug
    
    Impact: fix hrtimer locking (reported by lockdep) in the CPU hotplug case
    
    This addition fixes the hotplug locking issue on my machine
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 409832f5484cd1e2d8812c3236dffb33d01c359b
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Sat Nov 22 17:33:54 2008 +0000

    sparc32 cpuinit flase positives
    
    All noise since we don't have CPU hotplug there.  However, they
    did expose something very odd-looking in there - poke_viking()
    does a bunch of identical btfixup each time it's called (i.e.
    for each CPU).  That one is left alone for now; just the trivial
    misannotation fixes.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 7d6a8a1c487422b772201927c454930377d8cf7e
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Sat Nov 22 17:33:34 2008 +0000

    false __cpuinit positives on alpha
    
    pure noise - alpha doesn't have CPU hotplug
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit a5af4eb16618dba52321fe420e2c93e815fcd762
Author: Isaku Yamahata <yamahata@valinux.co.jp>
Date:   Tue Oct 14 17:50:43 2008 -0700

    xen: compilation fix fo xen CPU hotplugging
    
    This patch fixes compilation error on ia64.
    include asm/xen/hypervisor.h instead of asm-x86/xen/hypervisor.h
    use xen_pv_domain() instead of is_running_on_xen()
    
    >   CC      drivers/xen/cpu_hotplug.o
    > In file included from /linux-2.6/drivers/xen/cpu_hotplug.c:5:
    > /linux-2.6/include/asm-x86/xen/hypervisor.h:44:22: error: asm/desc.h: No such file or directory
    > /linux-2.6/include/asm-x86/xen/hypervisor.h:66:1: warning: "MULTI_UVMFLAGS_INDEX" redefined
    > In file included from /linux-2.6/include/asm-x86/xen/hypervisor.h:52,
    >                  from /linux-2.6/drivers/xen/cpu_hotplug.c:5:
    > /linux-2.6/arch/ia64/include/asm/xen/hypercall.h:233:1: warning: this is the location of the previous definition
    > /linux-2.6/drivers/xen/cpu_hotplug.c: In function 'setup_vcpu_hotplug_event':
    > /linux-2.6/drivers/xen/cpu_hotplug.c:81: error: implicit declaration of function 'is_running_on_xen'
    > make[4]: *** [drivers/xen/cpu_hotplug.o] Error 1
    > make[4]: *** Waiting for unfinished jobs....
    
    Signed-off-by: Isaku Yamahata <yamahata@valinux.co.jp>
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Cc: Isaku Yamahata <yamahata@valinux.co.jp>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 5f86515158ca86182c1dbecd546f1848121ba135
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Fri Oct 17 14:40:30 2008 +0800

    rcupdate: fix bug of rcu_barrier*()
    
    current rcu_barrier_bh() is like this:
    
    void rcu_barrier_bh(void)
    {
            BUG_ON(in_interrupt());
            /* Take cpucontrol mutex to protect against CPU hotplug */
            mutex_lock(&rcu_barrier_mutex);
            init_completion(&rcu_barrier_completion);
            atomic_set(&rcu_barrier_cpu_count, 0);
            /*
             * The queueing of callbacks in all CPUs must be atomic with
             * respect to RCU, otherwise one CPU may queue a callback,
             * wait for a grace period, decrement barrier count and call
             * complete(), while other CPUs have not yet queued anything.
             * So, we need to make sure that grace periods cannot complete
             * until all the callbacks are queued.
             */
            rcu_read_lock();
            on_each_cpu(rcu_barrier_func, (void *)RCU_BARRIER_BH, 1);
            rcu_read_unlock();
            wait_for_completion(&rcu_barrier_completion);
            mutex_unlock(&rcu_barrier_mutex);
    }
    
    The inconsistency of the code and the comments show a bug here.
    rcu_read_lock() cannot make sure that "grace periods for RCU_BH
    cannot complete until all the callbacks are queued".
    it only make sure that race periods for RCU cannot complete
    until all the callbacks are queued.
    
    so we must use rcu_read_lock_bh() for rcu_barrier_bh().
    like this:
    
    void rcu_barrier_bh(void)
    {
            ......
            rcu_read_lock_bh();
            on_each_cpu(rcu_barrier_func, (void *)RCU_BARRIER_BH, 1);
            rcu_read_unlock_bh();
            ......
    }
    
    and also rcu_barrier() rcu_barrier_sched() are implemented like this.
    it will bring a lot of duplicate code. My patch uses another way to
    fix this bug, please see the comment of my patch.
    Thank Paul E. McKenney for he rewrote the comment.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 7f2f49a58283110083a7358d2d98025a11653373
Author: Chuck Ebbert <cebbert@redhat.com>
Date:   Thu Oct 2 15:30:07 2008 -0400

    x86: allow number of additional hotplug CPUs to be set at compile time, V2
    
    x86: allow number of additional hotplug CPUs to be set at compile time, V2
    
    The default number of additional CPU IDs for hotplugging is determined
    by asking ACPI or mptables how many "disabled" CPUs there are in the
    system, but many systems get this wrong so that e.g. a uniprocessor
    machine gets an extra CPU allocated and never switches to single CPU
    mode.
    
    And sometimes CPU hotplugging is enabled only for suspend/hibernate
    anyway, so the additional CPU IDs are not wanted. Allow the number
    to be set to zero at compile time.
    
    Also, force the number of extra CPUs to zero if hotplugging is disabled
    which allows removing some conditional code.
    
    Tested on uniprocessor x86_64 that ACPI claims has a disabled processor,
    with CPU hotplugging configured.
    
    ("After" has the number of additional CPUs set to 0)
    Before: NR_CPUS: 512, nr_cpu_ids: 2, nr_node_ids 1
    After: NR_CPUS: 512, nr_cpu_ids: 1, nr_node_ids 1
    
    [Changed the name of the option and the prompt according to Ingo's
     suggestion.]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 14adf855baefad5ac3b545be23a64e6b61d6b74a
Author: Chuck Ebbert <cebbert@redhat.com>
Date:   Mon Sep 29 18:29:42 2008 -0400

    x86: move prefill_possible_map calling early, fix, V2
    
    Commit 4a701737 ("x86: move prefill_possible_map calling early, fix")
    is the wrong fix: prefill_possible_map() needs to be available
    even when CONFIG_HOTPLUG_CPU is not set. A followon patch will do that.
    
    Fix this correctly by making prefill_possible_map() available even when
    CONFIG_HOTPLUG_CPU is not set. The function is needed so that
    the number of possible CPUs can be determined.
    
    Tested on uniprocessor machine with CPU hotplug disabled.
    
    From boot log:
      Before: NR_CPUS: 512, nr_cpu_ids: 512, nr_node_ids 1
      After: NR_CPUS: 512, nr_cpu_ids: 1, nr_node_ids 1
    
    Signed-off-by: Chuck Ebbert <cebbert@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 95237b80a3021ce5abb4d9ad330355549026f9c3
Merge: cf4b0b2c9520 61e9916eba35
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 30 08:40:46 2008 -0700

    Merge branch 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc:
      powerpc: Fix failure to shutdown with CPU hotplug
      powerpc: Fix PCI in Holly device tree

commit 08115ab4d98cb577a83971ebd57cdfbcc6f50b68
Author: Chuck Ebbert <cebbert@redhat.com>
Date:   Mon Sep 29 18:24:23 2008 -0400

    xen: make CONFIG_XEN_SAVE_RESTORE depend on CONFIG_XEN
    
    Xen options need to depend on XEN.
    Also, add newline at end of file.
    
    Without this patch you need to disable CONFIG_PM in order to
    disable CPU hotplugging.
    
    Signed-off-by: Chuck Ebbert <cebbert@redhat.com>
    Acked-by Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 61e9916eba35dfb76d38013a5aae9a59cc50877a
Author: Johannes Berg <johannes@sipsolutions.net>
Date:   Wed Sep 24 22:56:25 2008 +0000

    powerpc: Fix failure to shutdown with CPU hotplug
    
    I tracked down the shutdown regression to CPUs not dying
    when being shut down during power-off. This turns out to
    be due to the system_state being SYSTEM_POWER_OFF, which
    this code doesn't take as a valid state for shutting off
    CPUs in.
    
    This has never made sense to me, but when I added hotplug
    code to implement hibernate I only "made it work" and did
    not question the need to check the system_state. Thomas
    Gleixner helped me dig, but the only thing we found is
    that it was added with the original commit that added CPU
    hotplug support.
    
    Signed-off-by: Johannes Berg <johannes@sipsolutions.net>
    Acked-by: Joel Schopp <jschopp@austin.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

commit e1d7bf14999469b16e86889ac71c94a9d0d2f5f4
Merge: e228c1b51ef5 291c54ff764d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 8 15:47:21 2008 -0700

    Merge branch 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      sched: arch_reinit_sched_domains() must destroy domains to force rebuild
      sched, cpuset: rework sched domains and CPU hotplug handling (v4)

commit 26fd10517e810dd59ea050b052de24a75ee6dc07
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Mon Sep 8 13:43:34 2008 +0100

    xen: make CPU hotplug functions static
    
    There's no need for these functions to be accessed from outside of xen/smp.c
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Acked-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 64f996f670e9477072a43b226294ea1cc153f6ac
Merge: f53252256587 23952a96ae73
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 6 19:36:23 2008 -0700

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: cpu_init(): fix memory leak when using CPU hotplug
      x86: pda_init(): fix memory leak when using CPU hotplug
      x86, xen: Use native_pte_flags instead of native_pte_val for .pte_flags
      x86: move mtrr cpu cap setting early in early_init_xxxx
      x86: delay early cpu initialization until cpuid is done
      x86: use X86_FEATURE_NOPL in alternatives
      x86: add NOPL as a synthetic CPU feature bit
      x86: boot: stub out unimplemented CPU feature words

commit 23952a96ae738277f3139b63d622e22984589031
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Wed Aug 6 10:29:37 2008 +0200

    x86: cpu_init(): fix memory leak when using CPU hotplug
    
    Exception stacks are allocated each time a CPU is set online.
    But the allocated space is never freed. Thus with one CPU hotplug
    offline/online cycle there is a memory leak of 24K (6 pages) for
    a CPU.
    
    Fix is to allocate exception stacks only once -- when the CPU is
    set online for the first time.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: akpm@linux-foundation.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit d04ec773d7ca1bbc05a2768be95c1cebe2b07757
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Wed Aug 6 10:27:30 2008 +0200

    x86: pda_init(): fix memory leak when using CPU hotplug
    
    pda->irqstackptr is allocated whenever a CPU is set online.
    But it is never freed. This results in a memory leak of 16K
    for each CPU offline/online cycle.
    
    Fix is to allocate pda->irqstackptr only once.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: akpm@linux-foundation.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit ec73adba51b4dae11134f7e6ffc84feade9f15fa
Merge: cc556c5c92a4 2a61812af2e5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 25 11:26:33 2008 -0700

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: add X86_FEATURE_XMM4_2 definitions
      x86: fix cpufreq + sched_clock() regression
      x86: fix HPET regression in 2.6.26 versus 2.6.25, check hpet against BAR, v3
      x86: do not enable TSC notifier if we don't need it
      x86 MCE: Fix CPU hotplug problem with multiple multicore AMD CPUs
      x86: fix: make PCI ECS for AMD CPUs hotplug capable
      x86: fix: do not run code in amd_bus.c on non-AMD CPUs

commit d68d82afd4c88e25763b23cd9cd4974573a3706f
Author: Alex Nixon <alex.nixon@citrix.com>
Date:   Fri Aug 22 11:52:15 2008 +0100

    xen: implement CPU hotplugging
    
    Note the changes from 2.6.18-xen CPU hotplugging:
    
    A vcpu_down request from the remote admin via Xenbus both hotunplugs the
    CPU, and disables it by removing it from the cpu_present map, and removing
    its entry in /sys.
    
    A vcpu_up request from the remote admin only re-enables the CPU, and does
    not immediately bring the CPU up. A udev event is emitted, which can be
    caught by the user if he wishes to automatically re-up CPUs when available,
    or implement a more complex policy.
    
    Signed-off-by: Alex Nixon <alex.nixon@citrix.com>
    Acked-by: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 8735728ef8dc935c4fb351f913758fdbb62c308d
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Fri Aug 22 22:23:09 2008 +0200

    x86 MCE: Fix CPU hotplug problem with multiple multicore AMD CPUs
    
    During CPU hot-remove the sysfs directory created by
    threshold_create_bank(), defined in
    arch/x86/kernel/cpu/mcheck/mce_amd_64.c, has to be removed before
    its parent directory, created by mce_create_device(), defined in
    arch/x86/kernel/cpu/mcheck/mce_64.c .  Moreover, when the CPU in
    question is hotplugged again, obviously the latter has to be created
    before the former.  At present, the right ordering is not enforced,
    because all of these operations are carried out by CPU hotplug
    notifiers which are not appropriately ordered with respect to each
    other.  This leads to serious problems on systems with two or more
    multicore AMD CPUs, among other things during suspend and hibernation.
    
    Fix the problem by placing threshold bank CPU hotplug callbacks in
    mce_cpu_callback(), so that they are invoked at the right places,
    if defined.  Additionally, use kobject_del() to remove the sysfs
    directory associated with the kobject created by
    kobject_create_and_add() in threshold_create_bank(), to prevent the
    kernel from crashing during CPU hotplug operations on systems with
    two or more multicore AMD CPUs.
    
    This patch fixes bug #11337.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Andi Kleen <andi@firstfloor.org>
    Tested-by: Mark Langsdorf <mark.langsdorf@amd.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit f607e3a03c90e8c050cb0c12ec9967c2925cc812
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 19 13:34:59 2008 -0700

    Revert "[CPUFREQ][2/2] preregister support for powernow-k8"
    
    This reverts commit 34ae7f35a21694aa5cb8829dc5142c39d73d6ba0, which has
    been reported to cause a number of problems.  During suspend and resume,
    it apparently causes a crash in a CPU hotplug notifier to happen,
    although the exact details are sketchy because of the inability to get
    good traces during the suspend sequence.
    
    See buzilla entries
    
            http://bugzilla.kernel.org/show_bug.cgi?id=11296
            http://bugzilla.kernel.org/show_bug.cgi?id=11339
    
    for more examples and details.
    
    [ Mark: "Revert the patch for now.  I'm still looking into getting a
      reliable reproduction and I do not have a fix at this time." ]
    
    Requested-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Mark Langsdorf <mark.langsdorf@amd.com>
    Acked-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@inux-foundation.org>

commit 1f49a2c2aeb22d5abc6d4ea574ff63d37ca55fbe
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Fri Aug 15 12:45:09 2008 -0400

    x86: revert replace LOCK_PREFIX in futex.h
    
    Since we now use DS prefixes instead of NOP to remove LOCK prefixes,
    there are no longer any issues with instruction boundaries moving around.
    
    Depends on :
    
    x86 alternatives : fix LOCK_PREFIX race with preemptible kernel and CPU hotplug
    
    On Thu, 14 Aug 2008, Mathieu Desnoyers wrote:
    >
    > Changing the 0x90 (single-byte nop) currently used into a 0x3E DS segment
    > override prefix should fix this issue. Since the default of the atomic
    > instructions is to use the DS segment anyway, it should not affect the
    > behavior.
    
    Ok, so I think this is an _excellent_ patch, but I'd like to also then use
    LOCK_PREFIX in include/asm-x86/futex.h.
    
    See commit 9d55b9923a1b7ea8193b8875c57ec940dc2ff027.
    
                    Linus
    
    Applies to 2.6.27-rc2 (and -rc3 unless hell broke loose in futex.h between rc2
    and rc3).
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    CC: Linus Torvalds <torvalds@linux-foundation.org>
    CC: H. Peter Anvin <hpa@zytor.com>
    CC: Jeremy Fitzhardinge <jeremy@goop.org>
    CC: Roland McGrath <roland@redhat.com>
    CC: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    CC: Steven Rostedt <srostedt@redhat.com>
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Andrew Morton <akpm@linux-foundation.org>
    CC: David Miller <davem@davemloft.net>
    CC: Ulrich Drepper <drepper@redhat.com>
    CC: Rusty Russell <rusty@rustcorp.com.au>
    CC: Gregory Haskins <ghaskins@novell.com>
    CC: Arnaldo Carvalho de Melo <acme@redhat.com>
    CC: "Luis Claudio R. Goncalves" <lclaudio@uudg.org>
    CC: Clark Williams <williams@redhat.com>
    CC: Christoph Lameter <cl@linux-foundation.org>
    CC: Andi Kleen <andi@firstfloor.org>
    CC: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

commit f88f07e0f0fd6376e081b10930d272a08fbf082f
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Thu Aug 14 16:58:15 2008 -0400

    x86: alternatives : fix LOCK_PREFIX race with preemptible kernel and CPU hotplug
    
    If a kernel thread is preempted in single-cpu mode right after the NOP (nop
    about to be turned into a lock prefix), then we CPU hotplug a CPU, and then the
    thread is scheduled back again, a SMP-unsafe atomic operation will be used on
    shared SMP variables, leading to corruption. No corruption would happen in the
    reverse case : going from SMP to UP is ok because we split a bit instruction
    into tiny pieces, which does not present this condition.
    
    Changing the 0x90 (single-byte nop) currently used into a 0x3E DS segment
    override prefix should fix this issue. Since the default of the atomic
    instructions is to use the DS segment anyway, it should not affect the
    behavior.
    
    The exception to this are references that use ESP/RSP and EBP/RBP as
    the base register (they will use the SS segment), however, in Linux
    (a) DS == SS at all times, and (b) we do not distinguish between
    segment violations reported as #SS as opposed to #GP, so there is no
    need to disassemble the instruction to figure out the suitable segment.
    
    This patch assumes that the 0x3E prefix will leave atomic operations as-is (thus
    assuming they normally touch data in the DS segment). Since there seem to be no
    obvious ill-use of other segment override prefixes for atomic operations, it
    should be safe. It can be verified with a quick
    
    grep -r LOCK_PREFIX include/asm-x86/
    grep -A 1 -r LOCK_PREFIX arch/x86/
    
    Taken from
    
    This source :
    AMD64 Architecture Programmer's Manual Volume 3: General-Purpose and System
    Instructions
    States
    "Instructions that Reference a Non-Stack SegmentIf an instruction encoding
    references any base register other than rBP or rSP, or if an instruction
    contains an immediate offset, the default segment is the data segment (DS).
    These instructions can use the segment-override prefix to select one of the
    non-default segments, as shown in Table 1-5."
    
    Therefore, forcing the DS segment on the atomic operations, which already use
    the DS segment, should not change.
    
    This source :
    http://wiki.osdev.org/X86_Instruction_Encoding
    States
    "In 64-bit the CS, SS, DS and ES segment overrides are ignored."
    
    Confirmed by "AMD 64-Bit Technology" A.7
    http://www.amd.com/us-en/assets/content_type/white_papers_and_tech_docs/x86-64_overview.pdf
    
    "In 64-bit mode, the DS, ES, SS and CS segment-override prefixes have no effect.
    These four prefixes are no longer treated as segment-override prefixes in the
    context of multipleprefix rules. Instead, they are treated as null prefixes."
    
    This patch applies to 2.6.27-rc2, but would also have to be applied to earlier
    kernels (2.6.26, 2.6.25, ...).
    
    Performance impact of the fix : tests done on "xaddq" and "xaddl" shows it
    actually improves performances on Intel Xeon, AMD64, Pentium M. It does not
    change the performance on Pentium II, Pentium 3 and Pentium 4.
    
    Xeon E5405 2.0GHz :
    NR_TESTS                                    10000000
    test empty cycles :                        162207948
    test test 1-byte nop xadd cycles :         170755422
    test test DS override prefix xadd cycles : 170000118 *
    test test LOCK xadd cycles :               472012134
    
    AMD64 2.0GHz :
    NR_TESTS                                    10000000
    test empty cycles :                        146674549
    test test 1-byte nop xadd cycles :         150273860
    test test DS override prefix xadd cycles : 149982382 *
    test test LOCK xadd cycles :               270000690
    
    Pentium 4 3.0GHz
    NR_TESTS                                    10000000
    test empty cycles :                        290001195
    test test 1-byte nop xadd cycles :         310000560
    test test DS override prefix xadd cycles : 310000575 *
    test test LOCK xadd cycles :              1050103740
    
    Pentium M 2.0GHz
    NR_TESTS 10000000
    test empty cycles :                        180000523
    test test 1-byte nop xadd cycles :         320000345
    test test DS override prefix xadd cycles : 310000374 *
    test test LOCK xadd cycles :               480000357
    
    Pentium 3 550MHz
    NR_TESTS                                    10000000
    test empty cycles :                        510000231
    test test 1-byte nop xadd cycles :         620000128
    test test DS override prefix xadd cycles : 620000110 *
    test test LOCK xadd cycles :               800000088
    
    Pentium II 350MHz
    NR_TESTS                                    10000000
    test empty cycles :                        200833494
    test test 1-byte nop xadd cycles :         340000130
    test test DS override prefix xadd cycles : 340000126 *
    test test LOCK xadd cycles :               530000078
    
    Speed test modules can be found at
    http://ltt.polymtl.ca/svn/trunk/tests/kernel/test-prefix-speed-32.c
    http://ltt.polymtl.ca/svn/trunk/tests/kernel/test-prefix-speed.c
    
    Macro-benchmarks
    
    2.0GHz E5405 Core 2 dual Quad-Core Xeon
    
    Summary
    
    * replace smp lock prefixes with DS segment selector prefixes
                      no lock prefix (s)   with lock prefix (s)    Speedup
    make -j1 kernel/      33.94 +/- 0.07         34.91 +/- 0.27      2.8 %
    hackbench 50           2.99 +/- 0.01          3.74 +/- 0.01     25.1 %
    
    * replace smp lock prefixes with 0x90 nops
                      no lock prefix (s)   with lock prefix (s)    Speedup
    make -j1 kernel/      34.16 +/- 0.32         34.91 +/- 0.27      2.2 %
    hackbench 50           3.00 +/- 0.01          3.74 +/- 0.01     24.7 %
    
    Detail :
    
    1 CPU, replace smp lock prefixes with DS segment selector prefixes
    
    make -j1 kernel/
    
    real    0m34.067s
    user    0m30.630s
    sys     0m2.980s
    
    real    0m33.867s
    user    0m30.582s
    sys     0m3.024s
    
    real    0m33.939s
    user    0m30.738s
    sys     0m2.876s
    
    real    0m33.913s
    user    0m30.806s
    sys     0m2.808s
    
    avg : 33.94s
    std. dev. : 0.07s
    
    hackbench 50
    
    Time: 2.978
    Time: 2.982
    Time: 3.010
    Time: 2.984
    Time: 2.982
    
    avg : 2.99
    std. dev. : 0.01
    
    1 CPU, noreplace-smp
    
    make -j1 kernel/
    
    real    0m35.326s
    user    0m30.630s
    sys     0m3.260s
    
    real    0m34.325s
    user    0m30.802s
    sys     0m3.084s
    
    real    0m35.568s
    user    0m30.722s
    sys     0m3.168s
    
    real    0m34.435s
    user    0m30.886s
    sys     0m2.996s
    
    avg.: 34.91s
    std. dev. : 0.27s
    
    hackbench 50
    
    Time: 3.733
    Time: 3.750
    Time: 3.761
    Time: 3.737
    Time: 3.741
    
    avg : 3.74
    std. dev. : 0.01
    
    1 CPU, replace smp lock prefixes with 0x90 nops
    
    make -j1 kernel/
    
    real    0m34.139s
    user    0m30.782s
    sys     0m2.820s
    
    real    0m34.010s
    user    0m30.630s
    sys     0m2.976s
    
    real    0m34.777s
    user    0m30.658s
    sys     0m2.916s
    
    real    0m33.924s
    user    0m30.634s
    sys     0m2.924s
    
    real    0m33.962s
    user    0m30.774s
    sys     0m2.800s
    
    real    0m34.141s
    user    0m30.770s
    sys     0m2.828s
    
    avg : 34.16
    std. dev. : 0.32
    
    hackbench 50
    
    Time: 2.999
    Time: 2.994
    Time: 3.004
    Time: 2.991
    Time: 2.988
    
    avg : 3.00
    std. dev. : 0.01
    
    I did more runs (20 runs of each) to compare the nop case to the DS
    prefix case. Results in seconds. They actually does not seems to show a
    significant difference.
    
    NOP
    
    34.155
    33.955
    34.012
    35.299
    35.679
    34.141
    33.995
    35.016
    34.254
    33.957
    33.957
    34.008
    35.013
    34.494
    33.893
    34.295
    34.314
    34.854
    33.991
    34.132
    
    DS
    
    34.080
    34.304
    34.374
    35.095
    34.291
    34.135
    33.940
    34.208
    35.276
    34.288
    33.861
    33.898
    34.610
    34.709
    33.851
    34.256
    35.161
    34.283
    33.865
    35.078
    
    Used http://www.graphpad.com/quickcalcs/ttest1.cfm?Format=C to do the
    T-test (yeah, I'm lazy) :
    
     Group      Group One (DS prefix)       Group Two (nops)
     Mean                    34.37815               34.37070
     SD                       0.46108                0.51905
     SEM                      0.10310                0.11606
     N                             20                     20
    
    P value and statistical significance:
      The two-tailed P value equals 0.9620
      By conventional criteria, this difference is considered to be not statistically significant.
    
    Confidence interval:
      The mean of Group One minus Group Two equals 0.00745
      95% confidence interval of this difference: From -0.30682 to 0.32172
    
    Intermediate values used in calculations:
      t = 0.0480
      df = 38
      standard error of difference = 0.155
    
    So, unless these calculus are completely bogus, the difference between the nop
    and the DS case seems not to be statistically significant.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    CC: Linus Torvalds <torvalds@linux-foundation.org>
    CC: Jeremy Fitzhardinge <jeremy@goop.org>
    CC: Roland McGrath <roland@redhat.com>
    CC: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    CC: Steven Rostedt <srostedt@redhat.com>
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Andrew Morton <akpm@linux-foundation.org>
    CC: David Miller <davem@davemloft.net>
    CC: Ulrich Drepper <drepper@redhat.com>
    CC: Rusty Russell <rusty@rustcorp.com.au>
    CC: Gregory Haskins <ghaskins@novell.com>
    CC: Arnaldo Carvalho de Melo <acme@redhat.com>
    CC: "Luis Claudio R. Goncalves" <lclaudio@uudg.org>
    CC: Clark Williams <williams@redhat.com>
    CC: Christoph Lameter <cl@linux-foundation.org>
    CC: Andi Kleen <andi@firstfloor.org>
    CC: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

commit cf417141cbb3a4ceb5cca15b2c1f099bd0a6603c
Author: Max Krasnyansky <maxk@qualcomm.com>
Date:   Mon Aug 11 14:33:53 2008 -0700

    sched, cpuset: rework sched domains and CPU hotplug handling (v4)
    
    This is an updated version of my previous cpuset patch on top of
    the latest mainline git.
    The patch fixes CPU hotplug handling issues in the current cpusets code.
    Namely circular locking in rebuild_sched_domains() and unsafe access to
    the cpu_online_map in the cpuset cpu hotplug handler.
    
    This version includes changes suggested by Paul Jackson (naming, comments,
    style, etc). I also got rid of the separate workqueue thread because it is
    now safe to call get_online_cpus() from workqueue callbacks.
    
    Here are some more details:
    
    rebuild_sched_domains() is the only way to rebuild sched domains
    correctly based on the current cpuset settings. What this means
    is that we need to be able to call it from different contexts,
    like cpu hotplug for example.
    Also latest scheduler code in -tip now calls rebuild_sched_domains()
    directly from functions like arch_reinit_sched_domains().
    
    In order to support that properly we need to rework cpuset locking
    rules to avoid circular dependencies, which is what this patch does.
    New lock nesting rules are explained in the comments.
    We can now safely call rebuild_sched_domains() from virtually any
    context. The only requirement is that it needs to be called under
    get_online_cpus(). This allows cpu hotplug handlers and the scheduler
    to call rebuild_sched_domains() directly.
    The rest of the cpuset code now offloads sched domains rebuilds to
    a workqueue (async_rebuild_sched_domains()).
    
    This version of the patch addresses comments from the previous review.
    I fixed all miss-formated comments and trailing spaces.
    
    I also factored out the code that builds domain masks and split up CPU and
    memory hotplug handling. This was needed to simplify locking, to avoid unsafe
    access to the cpu_online_map from mem hotplug handler, and in general to make
    things cleaner.
    
    The patch passes moderate testing (building kernel with -j 16, creating &
    removing domains and bringing cpus off/online at the same time) on the
    quad-core2 based machine.
    
    It passes lockdep checks, even with preemptable RCU enabled.
    This time I also tested in with suspend/resume path and everything is working
    as expected.
    
    Signed-off-by: Max Krasnyansky <maxk@qualcomm.com>
    Acked-by: Paul Jackson <pj@sgi.com>
    Cc: menage@google.com
    Cc: a.p.zijlstra@chello.nl
    Cc: vegard.nossum@gmail.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit b55793f7528ce1b73c25b3ac8a86a6cda2a0f9a4
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Wed Aug 6 10:29:37 2008 +0200

    x86: cpu_init(): fix memory leak when using CPU hotplug
    
    Exception stacks are allocated each time a CPU is set online.
    But the allocated space is never freed. Thus with one CPU hotplug
    offline/online cycle there is a memory leak of 24K (6 pages) for
    a CPU.
    
    Fix is to allocate exception stacks only once -- when the CPU is
    set online for the first time.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: akpm@linux-foundation.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 49800efcb17afdf973f33e8aa8807b7f83993cc6
Author: Andreas Herrmann <andreas.herrmann3@amd.com>
Date:   Wed Aug 6 10:27:30 2008 +0200

    x86: pda_init(): fix memory leak when using CPU hotplug
    
    pda->irqstackptr is allocated whenever a CPU is set online.
    But it is never freed. This results in a memory leak of 16K
    for each CPU offline/online cycle.
    
    Fix is to allocate pda->irqstackptr only once.
    
    Signed-off-by: Andreas Herrmann <andreas.herrmann3@amd.com>
    Cc: akpm@linux-foundation.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit c2147a5092cfe13dbf3210e54e8a622015edeecc
Author: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
Date:   Fri Jul 25 19:45:11 2008 -0700

    Better interface for hooking early initcalls
    
    Added early initcall (pre-SMP) support, using an identical interface to
    that of regular initcalls.  Functions called from do_pre_smp_initcalls()
    could be converted to use this cleaner interface.
    
    This is required by CPU hotplug, because early users have to register
    notifiers before going SMP.  One such CPU hotplug user is the relay
    interface with buffer-only channels, which needs to register such a
    notifier, to be usable in early code.  This in turn is used by kmemtrace.
    
    Signed-off-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 46c88e2962809828a5105478f2a51d2ad10499d8
Author: Dmitry Adamushko <dmitry.adamushko@gmail.com>
Date:   Fri Jul 11 01:20:02 2008 +0000

    slub: Fix use-after-preempt of per-CPU data structure
    
    commit bdb21928512a860a60e6a24a849dc5b63cbaf96a upstream
    
    Vegard Nossum reported a crash in kmem_cache_alloc():
    
            BUG: unable to handle kernel paging request at da87d000
            IP: [<c01991c7>] kmem_cache_alloc+0xc7/0xe0
            *pde = 28180163 *pte = 1a87d160
            Oops: 0002 [#1] PREEMPT SMP DEBUG_PAGEALLOC
            Pid: 3850, comm: grep Not tainted (2.6.26-rc9-00059-gb190333 #5)
            EIP: 0060:[<c01991c7>] EFLAGS: 00210203 CPU: 0
            EIP is at kmem_cache_alloc+0xc7/0xe0
            EAX: 00000000 EBX: da87c100 ECX: 1adad71a EDX: 6b6b6b6b
            ESI: 00200282 EDI: da87d000 EBP: f60bfe74 ESP: f60bfe54
            DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
    
    and analyzed it:
    
      "The register %ecx looks innocent but is very important here. The disassembly:
    
           mov    %edx,%ecx
           shr    $0x2,%ecx
           rep stos %eax,%es:(%edi) <-- the fault
    
       So %ecx has been loaded from %edx... which is 0x6b6b6b6b/POISON_FREE.
       (0x6b6b6b6b >> 2 == 0x1adadada.)
    
       %ecx is the counter for the memset, from here:
    
           memset(object, 0, c->objsize);
    
      i.e. %ecx was loaded from c->objsize, so "c" must have been freed.
      Where did "c" come from? Uh-oh...
    
           c = get_cpu_slab(s, smp_processor_id());
    
      This looks like it has very much to do with CPU hotplug/unplug. Is
      there a race between SLUB/hotplug since the CPU slab is used after it
      has been freed?"
    
    Good analysis.
    
    Yeah, it's possible that a caller of kmem_cache_alloc() -> slab_alloc()
    can be migrated on another CPU right after local_irq_restore() and
    before memset().  The inital cpu can become offline in the mean time (or
    a migration is a consequence of the CPU going offline) so its
    'kmem_cache_cpu' structure gets freed ( slab_cpuup_callback).
    
    At some point of time the caller continues on another CPU having an
    obsolete pointer...
    
    Signed-off-by: Dmitry Adamushko <dmitry.adamushko@gmail.com>
    Reported-by: Vegard Nossum <vegard.nossum@gmail.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 3e84050c81ffb4961ef43d20e1fb1d7607167d83
Author: Dmitry Adamushko <dmitry.adamushko@gmail.com>
Date:   Sun Jul 13 02:10:29 2008 +0200

    cpusets, hotplug, scheduler: fix scheduler domain breakage
    
    Commit f18f982ab ("sched: CPU hotplug events must not destroy scheduler
    domains created by the cpusets") introduced a hotplug-related problem as
    described below:
    
    Upon CPU_DOWN_PREPARE,
    
      update_sched_domains() -> detach_destroy_domains(&cpu_online_map)
    
    does the following:
    
    /*
     * Force a reinitialization of the sched domains hierarchy. The domains
     * and groups cannot be updated in place without racing with the balancing
     * code, so we temporarily attach all running cpus to the NULL domain
     * which will prevent rebalancing while the sched domains are recalculated.
     */
    
    The sched-domains should be rebuilt when a CPU_DOWN ops. has been
    completed, effectively either upon CPU_DEAD{_FROZEN} (upon success) or
    CPU_DOWN_FAILED{_FROZEN} (upon failure -- restore the things to their
    initial state). That's what update_sched_domains() also does but only
    for !CPUSETS case.
    
    With f18f982ab, sched-domains' reinitialization is delegated to
    CPUSETS code:
    
    cpuset_handle_cpuhp() -> common_cpu_mem_hotplug_unplug() ->
    rebuild_sched_domains()
    
    Being called for CPU_UP_PREPARE and if its callback is called after
    update_sched_domains()), it just negates all the work done by
    update_sched_domains() -- i.e. a soon-to-be-offline cpu is included in
    the sched-domains and that makes it visible for the load-balancer
    while the CPU_DOWN ops. is in progress.
    
    __migrate_live_tasks() moves the tasks off a 'dead' cpu (it's already
    "offline" when this function is called).
    
    try_to_wake_up() is called for one of these tasks from another CPU ->
    the load-balancer (wake_idle()) picks up a "dead" CPU and places the
    task on it. Then e.g. BUG_ON(rq->nr_running) detects this a bit later
    -> oops.
    
    Signed-off-by: Dmitry Adamushko <dmitry.adamushko@gmail.com>
    Tested-by: Vegard Nossum <vegard.nossum@gmail.com>
    Cc: Paul Menage <menage@google.com>
    Cc: Max Krasnyansky <maxk@qualcomm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: miaox@cn.fujitsu.com
    Cc: rostedt@goodmis.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit bdb21928512a860a60e6a24a849dc5b63cbaf96a
Author: Dmitry Adamushko <dmitry.adamushko@gmail.com>
Date:   Thu Jul 10 22:21:58 2008 +0200

    slub: Fix use-after-preempt of per-CPU data structure
    
    Vegard Nossum reported a crash in kmem_cache_alloc():
    
            BUG: unable to handle kernel paging request at da87d000
            IP: [<c01991c7>] kmem_cache_alloc+0xc7/0xe0
            *pde = 28180163 *pte = 1a87d160
            Oops: 0002 [#1] PREEMPT SMP DEBUG_PAGEALLOC
            Pid: 3850, comm: grep Not tainted (2.6.26-rc9-00059-gb190333 #5)
            EIP: 0060:[<c01991c7>] EFLAGS: 00210203 CPU: 0
            EIP is at kmem_cache_alloc+0xc7/0xe0
            EAX: 00000000 EBX: da87c100 ECX: 1adad71a EDX: 6b6b6b6b
            ESI: 00200282 EDI: da87d000 EBP: f60bfe74 ESP: f60bfe54
            DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
    
    and analyzed it:
    
      "The register %ecx looks innocent but is very important here. The disassembly:
    
           mov    %edx,%ecx
           shr    $0x2,%ecx
           rep stos %eax,%es:(%edi) <-- the fault
    
       So %ecx has been loaded from %edx... which is 0x6b6b6b6b/POISON_FREE.
       (0x6b6b6b6b >> 2 == 0x1adadada.)
    
       %ecx is the counter for the memset, from here:
    
           memset(object, 0, c->objsize);
    
      i.e. %ecx was loaded from c->objsize, so "c" must have been freed.
      Where did "c" come from? Uh-oh...
    
           c = get_cpu_slab(s, smp_processor_id());
    
      This looks like it has very much to do with CPU hotplug/unplug. Is
      there a race between SLUB/hotplug since the CPU slab is used after it
      has been freed?"
    
    Good analysis.
    
    Yeah, it's possible that a caller of kmem_cache_alloc() -> slab_alloc()
    can be migrated on another CPU right after local_irq_restore() and
    before memset().  The inital cpu can become offline in the mean time (or
    a migration is a consequence of the CPU going offline) so its
    'kmem_cache_cpu' structure gets freed ( slab_cpuup_callback).
    
    At some point of time the caller continues on another CPU having an
    obsolete pointer...
    
    Signed-off-by: Dmitry Adamushko <dmitry.adamushko@gmail.com>
    Reported-by: Vegard Nossum <vegard.nossum@gmail.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 0758c2f30b75419fbe5e0ec6dfc892bbc0687f57
Author: Dmitry Adamushko <dmitry.adamushko@gmail.com>
Date:   Mon Jun 30 18:22:34 2008 +0200

    sched: fix cpu hotplug
    
    Commit 79c537998d143b127c8c662a403c3356cb885f1c upstream
    
    the CPU hotplug problems (crashes under high-volume unplug+replug
    tests) seem to be related to migrate_dead_tasks().
    
    Firstly I added traces to see all tasks being migrated with
    migrate_live_tasks() and migrate_dead_tasks(). On my setup the problem
    pops up (the one with "se == NULL" in the loop of
    pick_next_task_fair()) shortly after the traces indicate that some has
    been migrated with migrate_dead_tasks()). btw., I can reproduce it
    much faster now with just a plain cpu down/up loop.
    
    [disclaimer] Well, unless I'm really missing something important in
    this late hour [/desclaimer] pick_next_task() is not something
    appropriate for migrate_dead_tasks() :-)
    
    the following change seems to eliminate the problem on my setup
    (although, I kept it running only for a few minutes to get a few
    messages indicating migrate_dead_tasks() does move tasks and the
    system is still ok)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 79c537998d143b127c8c662a403c3356cb885f1c
Author: Dmitry Adamushko <dmitry.adamushko@gmail.com>
Date:   Sun Jun 29 00:16:56 2008 +0200

    sched: fix cpu hotplug
    
    the CPU hotplug problems (crashes under high-volume unplug+replug
    tests) seem to be related to migrate_dead_tasks().
    
    Firstly I added traces to see all tasks being migrated with
    migrate_live_tasks() and migrate_dead_tasks(). On my setup the problem
    pops up (the one with "se == NULL" in the loop of
    pick_next_task_fair()) shortly after the traces indicate that some has
    been migrated with migrate_dead_tasks()). btw., I can reproduce it
    much faster now with just a plain cpu down/up loop.
    
    [disclaimer] Well, unless I'm really missing something important in
    this late hour [/desclaimer] pick_next_task() is not something
    appropriate for migrate_dead_tasks() :-)
    
    the following change seems to eliminate the problem on my setup
    (although, I kept it running only for a few minutes to get a few
    messages indicating migrate_dead_tasks() does move tasks and the
    system is still ok)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 961ccddd59d627b89bd3dc284b6517833bbdf25d
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Mon Jun 23 13:55:38 2008 +1000

    sched: add new API sched_setscheduler_nocheck: add a flag to control access checks
    
    Hidehiro Kawai noticed that sched_setscheduler() can fail in
    stop_machine: it calls sched_setscheduler() from insmod, which can
    have CAP_SYS_MODULE without CAP_SYS_NICE.
    
    Two cases could have failed, so are changed to sched_setscheduler_nocheck:
      kernel/softirq.c:cpu_callback()
            - CPU hotplug callback
      kernel/stop_machine.c:__stop_machine_run()
            - Called from various places, including modprobe()
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linux-mm@kvack.org
    Cc: sugita <yumiko.sugita.yf@hitachi.com>
    Cc: Satoshi OSHIMA <satoshi.oshima.fk@hitachi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e570dc2a503f8334b700e8483082c675394f53fd
Merge: b1ae8d3a0053 d4abc238c9f4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 20 12:36:55 2008 -0700

    Merge branch 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      sched, delay accounting: fix incorrect delay time when constantly waiting on runqueue
      sched: CPU hotplug events must not destroy scheduler domains created by the cpusets
      sched: rt-group: fix RR buglet
      sched: rt-group: heirarchy aware throttle
      sched: rt-group: fix hierarchy
      sched: NULL pointer dereference while setting sched_rt_period_us
      sched: fix defined-but-unused warning

commit f18f982abf183e91f435990d337164c7a43d1e6d
Author: Max Krasnyansky <maxk@qualcomm.com>
Date:   Thu May 29 11:17:01 2008 -0700

    sched: CPU hotplug events must not destroy scheduler domains created by the cpusets
    
    First issue is not related to the cpusets. We're simply leaking doms_cur.
    It's allocated in arch_init_sched_domains() which is called for every
    hotplug event. So we just keep reallocation doms_cur without freeing it.
    I introduced free_sched_domains() function that cleans things up.
    
    Second issue is that sched domains created by the cpusets are
    completely destroyed by the CPU hotplug events. For all CPU hotplug
    events scheduler attaches all CPUs to the NULL domain and then puts
    them all into the single domain thereby destroying domains created
    by the cpusets (partition_sched_domains).
    The solution is simple, when cpusets are enabled scheduler should not
    create default domain and instead let cpusets do that. Which is
    exactly what the patch does.
    
    Signed-off-by: Max Krasnyansky <maxk@qualcomm.com>
    Cc: pj@sgi.com
    Cc: menage@google.com
    Cc: rostedt@goodmis.org
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 7def2be1dc679984f4c4fb3ef19a8a081b2454ec
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jun 5 14:49:58 2008 +0200

    sched: fix hotplug cpus on ia64
    
    Cliff Wickman wrote:
    
    > I built an ia64 kernel from Andrew's tree (2.6.26-rc2-mm1)
    > and get a very predictable hotplug cpu problem.
    > billberry1:/tmp/cpw # ./dis
    > disabled cpu 17
    > enabled cpu 17
    > billberry1:/tmp/cpw # ./dis
    > disabled cpu 17
    > enabled cpu 17
    > billberry1:/tmp/cpw # ./dis
    >
    > The script that disables the cpu always hangs (unkillable)
    > on the 3rd attempt.
    >
    > And a bit further:
    > The kstopmachine thread always sits on the run queue (real time) for about
    > 30 minutes before running.
    
    this fix solves some (but not all) issues between CPU hotplug and
    RT bandwidth throttling.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 80f3186924d89e1e8f3dbe9d7efdd7921a355769
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed May 28 17:33:46 2008 -0400

    x86: prevent PGE flush from interruption/preemption
    
    upstream commit: b1979a5fda7869a790f4fd83fb06c78498d26ba1
    
    CR4 manipulation is not protected against interrupts and preemption,
    but KVM uses smp_function_call to manipulate the X86_CR4_VMXE bit
    either from the CPU hotplug code or from the kvm_init call.
    
    We need to protect the CR4 manipulation from both interrupts and
    preemption.
    
    Original bug report: http://lkml.org/lkml/2008/5/7/48
    Bugzilla entry: http://bugzilla.kernel.org/show_bug.cgi?id=10642
    
    This is not a regression from 2.6.25, it's a long standing and hard to
    trigger bug.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Chris Wright <chrisw@sous-sol.org>

commit 5c8e1ed1d204a6770ca2854cd3b3597070fe7e5a
Author: Max Krasnyansky <maxk@qualcomm.com>
Date:   Thu May 29 11:17:01 2008 -0700

    sched: CPU hotplug events must not destroy scheduler domains created by the cpusets
    
    First issue is not related to the cpusets. We're simply leaking doms_cur.
    It's allocated in arch_init_sched_domains() which is called for every
    hotplug event. So we just keep reallocation doms_cur without freeing it.
    I introduced free_sched_domains() function that cleans things up.
    
    Second issue is that sched domains created by the cpusets are
    completely destroyed by the CPU hotplug events. For all CPU hotplug
    events scheduler attaches all CPUs to the NULL domain and then puts
    them all into the single domain thereby destroying domains created
    by the cpusets (partition_sched_domains).
    The solution is simple, when cpusets are enabled scheduler should not
    create default domain and instead let cpusets do that. Which is
    exactly what the patch does.
    
    Signed-off-by: Max Krasnyansky <maxk@qualcomm.com>
    Cc: pj@sgi.com
    Cc: menage@google.com
    Cc: rostedt@goodmis.org
    Cc: mingo@elte.hu
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 72b59d67f80983f7bb587b086fb4cb1bc95263a4
Author: Pekka Paalanen <pq@iki.fi>
Date:   Mon May 12 21:21:01 2008 +0200

    x86_64: fix kernel rodata NX setting
    
    Without CONFIG_DYNAMIC_FTRACE, mark_rodata_ro() would mark a wrong
    number of pages as no-execute. The bug was introduced in the patch
    "ftrace: dont write protect kernel text". The symptom was machine reboot
    after a CPU hotplug.
    
    Signed-off-by: Pekka Paalanen <pq@iki.fi>
    Acked-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit b1979a5fda7869a790f4fd83fb06c78498d26ba1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:21:15 2008 +0200

    x86: prevent PGE flush from interruption/preemption
    
    CR4 manipulation is not protected against interrupts and preemption,
    but KVM uses smp_function_call to manipulate the X86_CR4_VMXE bit
    either from the CPU hotplug code or from the kvm_init call.
    
    We need to protect the CR4 manipulation from both interrupts and
    preemption.
    
    Original bug report: http://lkml.org/lkml/2008/5/7/48
    Bugzilla entry: http://bugzilla.kernel.org/show_bug.cgi?id=10642
    
    This is not a regression from 2.6.25, it's a long standing and hard to
    trigger bug.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 4446a36ff8c74ac3b32feb009b651048e129c6af
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon May 12 21:21:05 2008 +0200

    rcu: add call_rcu_sched()
    
    Fourth cut of patch to provide the call_rcu_sched().  This is again to
    synchronize_sched() as call_rcu() is to synchronize_rcu().
    
    Should be fine for experimental and -rt use, but not ready for inclusion.
    With some luck, I will be able to tell Andrew to come out of hiding on
    the next round.
    
    Passes multi-day rcutorture sessions with concurrent CPU hotplugging.
    
    Fixes since the first version include a bug that could result in
    indefinite blocking (spotted by Gautham Shenoy), better resiliency
    against CPU-hotplug operations, and other minor fixes.
    
    Fixes since the second version include reworking grace-period detection
    to avoid deadlocks that could happen when running concurrently with
    CPU hotplug, adding Mathieu's fix to avoid the softlockup messages,
    as well as Mathieu's fix to allow use earlier in boot.
    
    Fixes since the third version include a wrong-CPU bug spotted by
    Andrew, getting rid of the obsolete synchronize_kernel API that somehow
    snuck back in, merging spin_unlock() and local_irq_restore() in a
    few places, commenting the code that checks for quiescent states based
    on interrupting from user-mode execution or the idle loop, removing
    some inline attributes, and some code-style changes.
    
    Known/suspected shortcomings:
    
    o       I still do not entirely trust the sleep/wakeup logic.  Next step
            will be to use a private snapshot of the CPU online mask in
            rcu_sched_grace_period() -- if the CPU wasn't there at the start
            of the grace period, we don't need to hear from it.  And the
            bit about accounting for changes in online CPUs inside of
            rcu_sched_grace_period() is ugly anyway.
    
    o       It might be good for rcu_sched_grace_period() to invoke
            resched_cpu() when a given CPU wasn't responding quickly,
            but resched_cpu() is declared static...
    
    This patch also fixes a long-standing bug in the earlier preemptable-RCU
    implementation of synchronize_rcu() that could result in loss of
    concurrent external changes to a task's CPU affinity mask.  I still cannot
    remember who reported this...
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit e0ecfa7917cafe72f4a75f87e8bb5d8d51dc534f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jan 25 21:08:24 2008 +0100

    Preempt-RCU: fix rcu_barrier for preemptive environment.
    
    Fix rcu_barrier() to work properly in preemptive kernel environment.
    Also, the ordering of callback must be preserved while moving
    callbacks to another CPU during CPU hotplug.
    
    Signed-off-by: Gautham R Shenoy <ego@in.ibm.com>
    Signed-off-by: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 82a1fcb90287052aabfa235e7ffc693ea003fe69
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 25 21:08:02 2008 +0100

    softlockup: automatically detect hung TASK_UNINTERRUPTIBLE tasks
    
    this patch extends the soft-lockup detector to automatically
    detect hung TASK_UNINTERRUPTIBLE tasks. Such hung tasks are
    printed the following way:
    
     ------------------>
     INFO: task prctl:3042 blocked for more than 120 seconds.
     "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message
     prctl         D fd5e3793     0  3042   2997
            f6050f38 00000046 00000001 fd5e3793 00000009 c06d8264 c06dae80 00000286
            f6050f40 f6050f00 f7d34d90 f7d34fc8 c1e1be80 00000001 f6050000 00000000
            f7e92d00 00000286 f6050f18 c0489d1a f6050f40 00006605 00000000 c0133a5b
     Call Trace:
      [<c04883a5>] schedule_timeout+0x6d/0x8b
      [<c04883d8>] schedule_timeout_uninterruptible+0x15/0x17
      [<c0133a76>] msleep+0x10/0x16
      [<c0138974>] sys_prctl+0x30/0x1e2
      [<c0104c52>] sysenter_past_esp+0x5f/0xa5
      =======================
     2 locks held by prctl/3042:
     #0:  (&sb->s_type->i_mutex_key#5){--..}, at: [<c0197d11>] do_fsync+0x38/0x7a
     #1:  (jbd_handle){--..}, at: [<c01ca3d2>] journal_start+0xc7/0xe9
     <------------------
    
    the current default timeout is 120 seconds. Such messages are printed
    up to 10 times per bootup. If the system has crashed already then the
    messages are not printed.
    
    if lockdep is enabled then all held locks are printed as well.
    
    this feature is a natural extension to the softlockup-detector (kernel
    locked up without scheduling) and to the NMI watchdog (kernel locked up
    with IRQs disabled).
    
    [ Gautham R Shenoy <ego@in.ibm.com>: CPU hotplug fixes. ]
    [ Andrew Morton <akpm@linux-foundation.org>: build warning fix. ]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>

commit fd4be699b5b6b1744c9b0736d991ccfb35eea575
Merge: 417009f64f17 473980a99316
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Sun Jan 13 10:01:45 2008 -0800

    Merge branch 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc
    
    * 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc:
      [POWERPC] Fix CPU hotplug when using the SLB shadow buffer
      [POWERPC] efika: add phy-handle property for fec_mpc52xx

commit 473980a99316c0e788bca50996375a2815124ce1
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Jan 11 14:02:47 2008 +1100

    [POWERPC] Fix CPU hotplug when using the SLB shadow buffer
    
    Before we register the SLB shadow buffer, we need to invalidate the
    entries in the buffer, otherwise we can end up stale entries from when
    we previously offlined the CPU.
    
    This does this invalidate as well as unregistering the buffer with
    PHYP before we offline the cpu.  Tested and fixes crashes seen on
    970MP (thanks to tonyb) and POWER5.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

commit a263898f628dd21e59210b547986c154788f628e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Dec 30 11:58:17 2007 +0100

    CPU hotplug: fix cpu_is_offline() on !CONFIG_HOTPLUG_CPU
    
    make randconfig bootup testing found that the cpufreq code
    crashes on bootup, if the powernow-k8 driver is enabled and
    if maxcpus=1 passed on the boot line to a !CONFIG_HOTPLUG_CPU
    kernel.
    
    First lockdep found out that there's an inconsistent unlock
    sequence:
    
     =====================================
     [ BUG: bad unlock balance detected! ]
     -------------------------------------
     swapper/1 is trying to release lock (&per_cpu(cpu_policy_rwsem, cpu)) at:
     [<ffffffff806ffd8e>] unlock_policy_rwsem_write+0x3c/0x42
     but there are no more locks to release!
    
    Call Trace:
     [<ffffffff806ffd8e>] unlock_policy_rwsem_write+0x3c/0x42
     [<ffffffff80251c29>] print_unlock_inbalance_bug+0x104/0x12c
     [<ffffffff80252f3a>] mark_held_locks+0x56/0x94
     [<ffffffff806ffd8e>] unlock_policy_rwsem_write+0x3c/0x42
     [<ffffffff807008b6>] cpufreq_add_dev+0x2a8/0x5c4
     ...
    
    then shortly afterwards the cpufreq code crashed on an assert:
    
     ------------[ cut here ]------------
     kernel BUG at drivers/cpufreq/cpufreq.c:1068!
     invalid opcode: 0000 [1] SMP
     [...]
     Call Trace:
      [<ffffffff805145d6>] sysdev_driver_unregister+0x5b/0x91
      [<ffffffff806ff520>] cpufreq_register_driver+0x15d/0x1a2
      [<ffffffff80cc0596>] powernowk8_init+0x86/0x94
     [...]
     ---[ end trace 1e9219be2b4431de ]---
    
    the bug was caused by maxcpus=1 bootup, which brought up the
    secondary core as !cpu_online() but !cpu_is_offline() either,
    which on on !CONFIG_HOTPLUG_CPU is always 0 (include/linux/cpu.h):
    
      /* CPUs don't go offline once they're online w/o CONFIG_HOTPLUG_CPU */
      static inline int cpu_is_offline(int cpu) { return 0; }
    
    but the cpufreq code uses cpu_online() and cpu_is_offline() in
    a mixed way - the low-level drivers use cpu_online(), while
    the cpufreq core uses cpu_is_offline(). This opened up the
    possibility to add the non-initialized sysdev device of the
    secondary core:
    
     cpufreq-core: trying to register driver powernow-k8
     cpufreq-core: adding CPU 0
     powernow-k8: BIOS error - no PSB or ACPI _PSS objects
     cpufreq-core: initialization failed
     cpufreq-core: adding CPU 1
     cpufreq-core: initialization failed
    
    which then blew up. The fix is to make cpu_is_offline() always
    the negation of cpu_online(). With that fix applied the kernel
    boots up fine without crashing:
    
     Calling initcall 0xffffffff80cc0510: powernowk8_init+0x0/0x94()
     powernow-k8: Found 1 AMD Athlon(tm) 64 X2 Dual Core Processor 3800+ processors (1 cpu cores) (version 2.20.00)
     powernow-k8: BIOS error - no PSB or ACPI _PSS objects
     initcall 0xffffffff80cc0510: powernowk8_init+0x0/0x94() returned -19.
     initcall 0xffffffff80cc0510 ran for 19 msecs: powernowk8_init+0x0/0x94()
     Calling initcall 0xffffffff80cc328f: init_lapic_nmi_sysfs+0x0/0x39()
    
    We could fix this by making CPU enumeration aware of max_cpus, but that
    would be more fragile IMO, and the cpu_online(cpu) != cpu_is_offline(cpu)
    possibility was quite confusing and a continuous source of bugs too.
    
    Most distributions have kernels with CPU hotplug enabled, so this bug
    remained hidden for a long time.
    
    Bug forensics:
    
    The broken cpu_is_offline() API variant was introduced via:
    
     commit a59d2e4e6977e7b94e003c96a41f07e96cddc340
     Author: Rusty Russell <rusty@rustcorp.com.au>
     Date:   Mon Mar 8 06:06:03 2004 -0800
    
         [PATCH] minor cleanups for hotplug CPUs
    
    ( this predates linux-2.6.git, this commit is available from Thomas's
      historic git tree. )
    
    Then 1.5 years later the cpufreq code made use of it:
    
     commit c32b6b8e524d2c337767d312814484d9289550cf
     Author: Ashok Raj <ashok.raj@intel.com>
     Date:   Sun Oct 30 14:59:54 2005 -0800
    
         [PATCH] create and destroy cpufreq sysfs entries based on cpu notifiers
    
     +       if (cpu_is_offline(cpu))
     +               return 0;
    
    which is a correct use of the subtly broken new API. v2.6.15 then
    shipped with this bug included.
    
    then it took two more years for random-kernel qa to hit it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 4486c5f510463673d2ea57b46137086f5b21ef36
Merge: f941b168a4d7 ed5d4026ae6f
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Wed Dec 19 14:29:23 2007 -0800

    Merge branch 'release' of git://git.kernel.org/pub/scm/linux/kernel/git/aegl/linux-2.6
    
    * 'release' of git://git.kernel.org/pub/scm/linux/kernel/git/aegl/linux-2.6:
      [IA64] Adjust CMCI mask on CPU hotplug
      [IA64] make flush_tlb_kernel_range() an inline function
      [IA64] Guard elfcorehdr_addr with #if CONFIG_PROC_FS
      [IA64] Fix Altix BTE error return status
      [IA64] Remove assembler warnings on head.S
      [IA64] Remove compiler warinings about uninitialized variable in irq_ia64.c
      [IA64] set_thread_area fails in IA32 chroot
      [IA64] print kernel release in OOPS to make kerneloops.org happy
      [IA64] Two trivial spelling fixes
      [IA64] Avoid unnecessary TLB flushes when allocating memory
      [IA64] ia32 nopage
      [IA64] signal: remove redundant code in setup_sigcontext()
      IA64: Slim down __clear_bit_unlock

commit ed5d4026ae6f51bec25e03a891a7d59c492577ab
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Wed Dec 19 11:42:02 2007 -0800

    [IA64] Adjust CMCI mask on CPU hotplug
    
    Currently CMCI mask of hot-added CPU is always disabled after CPU hotplug.
    We should adjust this mask depending on CMC polling state.
    
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Signed-off-by: Satoru Takeuchi <takeuchi_satoru@jp.fujitsu.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>

commit 1e32b073f372f0fe903c4474fbd47c2ac61428c8
Author: Satyam Sharma <satyam@infradead.org>
Date:   Wed Oct 17 18:04:36 2007 +0200

    i386: misc cpuinit annotations
    
    cpuid_class_cpu_callback() is callback function of a CPU hotplug
    notifier_block (that is already marked as __cpuinitdata). Therefore
    it can safely be marked as __cpuinit.
    
    cpuid_device_create() is only referenced from other functions that
    are __cpuinit or __init. So it can also be safely marked __cpuinit.
    
    [ tglx: arch/x86 adaptation ]
    
    Signed-off-by: Satyam Sharma <satyam@infradead.org>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit db6a5cef7f474a5bad476a31f4e4c69a69fab8b1
Author: Satyam Sharma <satyam@infradead.org>
Date:   Tue Oct 2 13:39:45 2007 -0700

    [IA64] tree-wide: Misc __cpu{initdata, init, exit} annotations
    
    * palinfo.c:
    
    palinfo_cpu_notifier is a CPU hotplug notifier_block, and can be
    marked __cpuinitdata, and the callback function palinfo_cpu_callback()
    itself can be marked __cpuinit. create_palinfo_proc_entries() is only
    called from __cpuinit callback or general __init code, therefore a
    candidate for __cpuinit itself. remove_palinfo_proc_entries() is only
    called from __cpuinit callback or general __exit code, therefore a
    candidate for __cpuexit.
    
    * salinfo.c:
    
    The CPU hotplug notifier_block can be __cpuinitdata. The callback
    salinfo_cpu_callback() is incorrectly marked __devinit -- it must
    be __cpuinit instead.
    
    * topology.c:
    
    cache_sysfs_init() is only called at device_initcall() time so marking
    it as __cpuinit is wrong and wasteful. It should be unconditionally
    __init. Also cleanup reference to hotplug notifier callback function
    from this function and replace with cache_add_dev(), which could also
    enable us to use other tricks to replace __cpuinit{data} annotations,
    as recently discussed on this list.
    
    cache_shared_cpu_map_setup() is only ever called from __cpuinit-marked
    functions hence both its definitions (SMP or !SMP) are candidates for
    __cpuinit itself. Also all_cpu_cache_info can be __cpuinitdata because
    only referenced from __cpuinit code.
    
    Signed-off-by: Satyam Sharma <satyam@infradead.org>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Tony Luck <tony.luck@intel.com>

commit 59a35bafb223bbb0553ba1a3bb9280bda668a8d8
Author: Satyam Sharma <satyam@infradead.org>
Date:   Thu Aug 23 09:14:25 2007 +0530

    hwmon: (coretemp) Remove bogus __cpuinitdata etc cleanup
    
    The CPU hotplug notifier_block coretemp_cpu_notifier is already defined
    inside an #ifdef HOTPLUG_CPU, therefore marking it as __cpuinitdata is
    quite a pointless thing to do.
    
    Also, remove duplicate prototype of function coretemp_update_device()
    at the top of this file (another one already exists barely 10 lines
    above this one :-)
    
    Signed-off-by: Satyam Sharma <satyam@infradead.org>
    Acked-by: Rudolf Marek <r.marek@assembler.cz>
    Signed-off-by: Mark M. Hoffman <mhoffman@lightlink.com>

commit 428ab280a0754656fa09304017b0ce626744cc77
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Mon Aug 6 14:02:12 2007 +0100

    [MIPS] SMP: Scatter __cpuinit over the code as needed.
    
    MIPS doesn't do CPU hotplugging yet but since many of the functions don't
    even have an __init let's fix this right.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

commit dc559f7cd5d6d11a99b6c29402b31fbb3f3a1db0
Author: Olof Johansson <olof@lixom.net>
Date:   Wed Aug 22 12:26:43 2007 +1000

    [POWERPC] Rework SMP timebase handoff for pasemi
    
    Rework timebase handoff to play nice with configurations with more than
    2 cores, as well as with CPU hotplug.
    
    Previous scheme just pushed out the current timebase from the giving
    core to all cores without caring if they wanted it or not, nor checking
    if they'd taken it. The taking side didn't make sure the giving side
    had provided a value yet either. In other words, it was completely broken.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

commit 602033ed5907a59ce86f709082a35be047743a86
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Thu Jul 26 12:07:21 2007 -0700

    Revert most of "x86: Fix alternatives and kprobes to remap write-protected kernel text"
    
    This reverts most of commit 19d36ccdc34f5ed444f8a6af0cbfdb6790eb1177.
    
    The way to DEBUG_RODATA interactions with KPROBES and CPU hotplug is to
    just not mark the text as being write-protected in the first place.
    Both of those facilities depend on rewriting instructions.
    
    Having "helpful" debug facilities that just cause more problem is not
    being helpful.  It just adds complexity and bugs. Not worth it.
    
    Reported-by: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 91973de736bc97dc04156242c5a4b00993b6c902
Author: Peter Oruba <peter.oruba@amd.com>
Date:   Mon Jul 9 11:35:27 2007 -0700

    [CPUFREQ] bugfix cpufreq in combination with performance governor
    
    There is a frequency scaling issue that I encountered with the performance
    governor in combination with CPU hotplug.
    
    In cpufreq.c CPU frequency is reduced to its minimum before the CPU gets
    unregistered and set offline.  Does that have a particular reason?
    
    Since the (k8-)governor does not monitor CPU frequency that setting also
    applies then to the remaining CPU as well and lets the system run on the
    lowest frequency although performance is chose as the policy.
    
    Signed-off-by: Peter Oruba <peter.oruba@amd.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dave Jones <davej@redhat.com>

commit d09c6b809432668371b5de9102f4f9aa6a7c79cc
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Jun 14 15:13:16 2007 +0900

    mm: Fix memory/cpu hotplug section mismatch and oops.
    
    When building with memory hotplug enabled and cpu hotplug disabled, we
    end up with the following section mismatch:
    
    WARNING: mm/built-in.o(.text+0x4e58): Section mismatch: reference to
    .init.text: (between 'free_area_init_node' and '__build_all_zonelists')
    
    This happens as a result of:
    
            -> free_area_init_node()
              -> free_area_init_core()
                -> zone_pcp_init() <-- all __meminit up to this point
                  -> zone_batchsize() <-- marked as __cpuinit                     fo
    
    This happens because CONFIG_HOTPLUG_CPU=n sets __cpuinit to __init, but
    CONFIG_MEMORY_HOTPLUG=y unsets __meminit.
    
    Changing zone_batchsize() to __devinit fixes this.
    
    __devinit is the only thing that is common between CONFIG_HOTPLUG_CPU=y and
    CONFIG_MEMORY_HOTPLUG=y. In the long run, perhaps this should be moved to
    another section identifier completely. Without this, memory hot-add
    of offline nodes (via hotadd_new_pgdat()) will oops if CPU hotplug is
    not also enabled.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>
    Acked-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    
    --
    
     mm/page_alloc.c |    2 +-
     1 file changed, 1 insertion(+), 1 deletion(-)

commit 455c017ae3934797653549704c286e7bcc3a9397
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed May 9 02:35:11 2007 -0700

    microcode: use suspend-related CPU hotplug notifications
    
    Make the microcode driver use the suspend-related CPU hotplug notifications
    to handle the CPU hotplug events occuring during system-wide suspend and
    resume transitions.  Remove the global variable suspend_cpu_hotplug
    previously used for this purpose.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 8bb7844286fb8c9fce6f65d8288aeb09d03a5e0d
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed May 9 02:35:10 2007 -0700

    Add suspend-related notifications for CPU hotplug
    
    Since nonboot CPUs are now disabled after tasks and devices have been
    frozen and the CPU hotplug infrastructure is used for this purpose, we need
    special CPU hotplug notifications that will help the CPU-hotplug-aware
    subsystems distinguish normal CPU hotplug events from CPU hotplug events
    related to a system-wide suspend or resume operation in progress.  This
    patch introduces such notifications and causes them to be used during
    suspend and resume transitions.  It also changes all of the
    CPU-hotplug-aware subsystems to take these notifications into consideration
    (for now they are handled in the same way as the corresponding "normal"
    ones).
    
    [oleg@tv-sign.ru: cleanups]
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit b89deed32ccc96098bd6bc953c64bba6b847774f
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed May 9 02:33:52 2007 -0700

    implement flush_work()
    
    A basic problem with flush_scheduled_work() is that it blocks behind _all_
    presently-queued works, rather than just the work whcih the caller wants to
    flush.  If the caller holds some lock, and if one of the queued work happens
    to want that lock as well then accidental deadlocks can occur.
    
    One example of this is the phy layer: it wants to flush work while holding
    rtnl_lock().  But if a linkwatch event happens to be queued, the phy code will
    deadlock because the linkwatch callback function takes rtnl_lock.
    
    So we implement a new function which will flush a *single* work - just the one
    which the caller wants to free up.  Thus we avoid the accidental deadlocks
    which can arise from unrelated subsystems' callbacks taking shared locks.
    
    flush_work() non-blockingly dequeues the work_struct which we want to kill,
    then it waits for its handler to complete on all CPUs.
    
    Add ->current_work to the "struct cpu_workqueue_struct", it points to
    currently running "struct work_struct". When flush_work(work) detects
    ->current_work == work, it inserts a barrier at the _head_ of ->worklist
    (and thus right _after_ that work) and waits for completition. This means
    that the next work fired on that CPU will be this barrier, or another
    barrier queued by concurrent flush_work(), so the caller of flush_work()
    will be woken before any "regular" work has a chance to run.
    
    When wait_on_work() unlocks workqueue_mutex (or whatever we choose to protect
    against CPU hotplug), CPU may go away. But in that case take_over_work() will
    move a barrier we queued to another CPU, it will be fired sometime, and
    wait_on_work() will be woken.
    
    Actually, we are doing cleanup_workqueue_thread()->kthread_stop() before
    take_over_work(), so cwq->thread should complete its ->worklist (and thus
    the barrier), because currently we don't check kthread_should_stop() in
    run_workqueue(). But even if we did, everything should be ok.
    
    [akpm@osdl.org: cleanup]
    [akpm@osdl.org: add flush_work_keventd() wrapper]
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit e18f3ffb9c3ddfc1b4ad8f38f5f2acae8c16f0c9
Author: Andrew Morton <akpm@osdl.org>
Date:   Wed May 9 02:33:50 2007 -0700

    schedule_on_each_cpu(): use preempt_disable()
    
    We take workqueue_mutex in there to keep CPU hotplug away.  But
    preempt_disable() will suffice for that.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit d9333afd6a714760c13f76ba275a32ec7bd979c1
Author: Johannes Berg <johannes@sipsolutions.net>
Date:   Thu May 3 06:33:51 2007 +1000

    [POWERPC] powermac: Support G5 CPU hotplug
    
    This allows "hotplugging" of CPUs on G5 machines.  CPUs that are
    disabled are put into an idle loop with the decrementer frequency set
    to minimum.  To wake them up again we kick them just like when bringing
    them up.  To stop those CPUs from messing with any global state we stop
    them from entering the timer interrupt.
    
    Signed-off-by: Johannes Berg <johannes@sipsolutions.net>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

commit f18d397e6aa5cde638d164b1d519c3ee903f4867
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed May 2 19:27:04 2007 +0200

    [PATCH] x86-64: optimize & fix APIC mode setup
    
    Fix a couple of inconsistencies/problems I found while reviewing the x86_64
    genapic code (when I was chasing mysterious eth0 timeouts that would only
    trigger if CPU_HOTPLUG is enabled):
    
     - AMD systems defaulted to the slower flat-physical mode instead
       of the flat-logical mode. The only restriction on AMD systems
       is that they should not use clustered APIC mode.
    
     - removed the CPU hotplug hacks, switching the default for small
       systems back from phys-flat to logical-flat. The switching to logical
       flat mode on small systems fixed sporadic ethernet driver timeouts i
       was getting on a dual-core Athlon64 system:
    
        NETDEV WATCHDOG: eth0: transmit timed out
        eth0: Transmit timeout, status 0c 0005 c07f media 80.
        eth0: Tx queue start entry 32  dirty entry 28.
        eth0:  Tx descriptor 0 is 0008a04a. (queue head)
        eth0:  Tx descriptor 1 is 0008a04a.
        eth0:  Tx descriptor 2 is 0008a04a.
        eth0:  Tx descriptor 3 is 0008a04a.
        eth0: link up, 100Mbps, full-duplex, lpa 0xC5E1
    
     - The use of '<= 8' was a bug by itself (the valid APIC ids
       for logical flat mode go from 0 to 7, not 0 to 8). The new logic
       is to use logical flat mode on both AMD and Intel systems, and
       to only switch to physical mode when logical mode cannot be used.
       If CPU hotplug is racy wrt. APIC shutdown then CPU hotplug needs
       fixing, not the whole IRQ system be made inconsistent and slowed
       down.
    
     - minor cleanups: simplified some code constructs
    
    build & booted on a couple of AMD and Intel SMP systems.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: "Li, Shaohua" <shaohua.li@intel.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>

commit 1d64b9cb1dc2a7cd521444e3d908adeccd026356
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sun Apr 1 23:49:49 2007 -0700

    [PATCH] Fix microcode-related suspend problem
    
    Fix the regression resulting from the recent change of suspend code
    ordering that causes systems based on Intel x86 CPUs using the microcode
    driver to hang during the resume.
    
    The problem occurs since the microcode driver uses request_firmware() in
    its CPU hotplug notifier, which is called after tasks has been frozen and
    hangs.  It can be fixed by telling the microcode driver to use the
    microcode stored in memory during the resume instead of trying to load it
    from disk.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Adrian Bunk <bunk@stusta.de>
    Cc: Tigran Aivazian <tigran@aivazian.fsnet.co.uk>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Maxim <maximlevitsky@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit f64cd9de37bfd97c8d23a05a9f50759ff7195212
Merge: 6b3964cde70c 77d8e1efea0e
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Thu Mar 22 19:33:52 2007 -0700

    Merge branch 'for-linus' of master.kernel.org:/pub/scm/linux/kernel/git/roland/infiniband
    
    * 'for-linus' of master.kernel.org:/pub/scm/linux/kernel/git/roland/infiniband:
      IB/ipoib: Fix thinko in packet length checks
      IPoIB: Fix use-after-free in path_rec_completion()
      IB/ehca: Make scaling code work without CPU hotplug
      RDMA/cxgb3: Handle build_phys_page_list() failure in iwch_reregister_phys_mem()
      IB/ipath: Check return value of lookup_one_len
      IPoIB: Fix race in detaching from mcast group before attaching
      IPoIB/cm: Fix reaping of stale connections

commit 73b9e9870f5780cb554b68bbcfa47782b27a3e04
Author: Joachim Fenkes <fenkes@de.ibm.com>
Date:   Thu Mar 22 16:52:13 2007 +0100

    IB/ehca: Make scaling code work without CPU hotplug
    
    eHCA scaling code must not depend on register_cpu_notifier() if
    CONFIG_HOTPLUG_CPU is not set, so put all related code into #ifdefs.
    
    Signed-off-by: Joachim Fenkes <fenkes@de.ibm.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>

commit d04f41e35343f1d788551fd3f753f51794f4afcf
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Mar 7 18:12:31 2007 +0100

    [PATCH] CPU hotplug: call check_tsc_sync_source() with irqs off
    
    check_tsc_sync_source() depends on being called with irqs disabled (it
    checks whether the TSC is coherent across two specific CPUs). This is
    incidentally true during bootup, but not during cpu hotplug __cpu_up().
    This got found via smp_processor_id() debugging.
    
    disable irqs explicitly and remove the unconditional enabling of
    interrupts. Add touch_nmi_watchdog() to the cpu_online_map busy loop.
    
    this bug is present both on i386 and on x86_64.
    
    Reported-by: Michal Piotrowski <michal.k.k.piotrowski@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit f8657e1b55901e6c227094258d1fa3642fa242bd
Author: Vivek Goyal <vgoyal@in.ibm.com>
Date:   Tue Feb 13 13:26:22 2007 +0100

    [PATCH] i386: move startup_32() in text.head section
    
    o Entry startup_32 was in .text section but it was accessing some init
      data too and it prompts MODPOST to generate compilation warnings.
    
    WARNING: vmlinux - Section mismatch: reference to .init.data:boot_params from
    .text between '_text' (at offset 0xc0100029) and 'startup_32_smp'
    WARNING: vmlinux - Section mismatch: reference to .init.data:boot_params from
    .text between '_text' (at offset 0xc0100037) and 'startup_32_smp'
    WARNING: vmlinux - Section mismatch: reference to
    .init.data:init_pg_tables_end from .text between '_text' (at offset
    0xc0100099) and 'startup_32_smp'
    
    o Can't move startup_32 to .init.text as this entry point has to be at the
      start of bzImage. Hence moved startup_32 to a new section .text.head and
      instructed MODPOST to not to generate warnings if init data is being
      accessed from .text.head section. This code has been audited.
    
    o SMP boot up code (startup_32_smp) can go into .init.text if CPU hotplug
      is not supported. Otherwise it generates more warnings
    
    WARNING: vmlinux - Section mismatch: reference to .init.data:new_cpu_data from
    .text between 'checkCPUtype' (at offset 0xc0100126) and 'is486'
    WARNING: vmlinux - Section mismatch: reference to .init.data:new_cpu_data from
    .text between 'checkCPUtype' (at offset 0xc0100130) and 'is486'
    
    Signed-off-by: Vivek Goyal <vgoyal@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Andi Kleen <ak@suse.de>

commit 23c887522e912ca494950796a95df8dd210f4b01
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Sat Feb 10 01:45:05 2007 -0800

    [PATCH] Relay: add CPU hotplug support
    
    Mathieu originally needed to add this for tracing Xen, but it's something
    that's needed for any application that can be tracing while cpus are added.
    
    unplug isn't supported by this patch.  The thought was that at minumum a new
    buffer needs to be added when a cpu comes up, but it wasn't worth the effort
    to remove buffers on cpu down since they'd be freed soon anyway when the
    channel was closed.
    
    [zanussi@us.ibm.com: avoid lock_cpu_hotplug deadlock]
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Tom Zanussi <zanussi@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

commit 32b7d973f1bfaf221ad53957fd360187815d29c2
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Tue Jan 23 16:52:07 2007 +0100

    i386: fix CPU hotplug with 2GB VMSPLIT
    
    In VMSPLIT mode, kernel PGD might have more entries than user space
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

commit 4a5d107a9a79ef3404209a43866554cba451dda4
Author: Vivek Goyal <vgoyal@in.ibm.com>
Date:   Thu Jan 11 01:52:44 2007 +0100

    [PATCH] i386: cpu hotplug/smpboot misc MODPOST warning fixes
    
    o Misc smpboot/cpu hotplug path cleanups. I did those to supress the
      warnings generated by MODPOST. These warnings are visible only
      if CONFIG_RELOCATABLE=y.
    
    o CONFIG_RELOCATABLE compiles the kernel with --emit-relocs option. This
      option retains relocation information in vmlinux file and MODPOST
      is quick to spit out "Section mismatch" warnings.
    
    o This patch fixes some of those warnings. Many of the functions in
      smpboot case are __devinit type and they in turn accesses text/data which
      if of type __cpuinit. Now if CONFIG_HOTPLUG=y and CONFIG_HOTPLUG_CPU=n
      then we end up in cases where a function in .text segment is calling
      another function in .init.text segment and MODPOST emits warning.
    
    WARNING: vmlinux - Section mismatch: reference to .init.text:identify_cpu from .text between 'smp_store_cpu_info' (at offset 0xc011020d) and 'do_boot_cpu'
    WARNING: vmlinux - Section mismatch: reference to .init.text:init_gdt from .text between 'do_boot_cpu' (at offset 0xc01102ca) and '__cpu_up'
    WARNING: vmlinux - Section mismatch: reference to .init.text:print_cpu_info from .text between 'do_boot_cpu' (at offset 0xc01105d0) and '__cpu_up'
    
    o It also fixes the issues where CONFIG_HOTPLUG_CPU=y and start_secondary()
      is calling smp_callin() which in-turn calls synchronize_tsc_ap() which is
      of type __init. This should have meant broken CPU hotplug.
    
    WARNING: vmlinux - Section mismatch: reference to .init.data: from .text between 'start_secondary' (at offset 0xc011603f) and 'initialize_secondary'
    WARNING: vmlinux - Section mismatch: reference to .init.data: from .text between 'MP_processor_info' (at offset 0xc0116a4f) and 'mp_register_lapic'
    WARNING: vmlinux - Section mismatch: reference to .init.data: from .text between 'MP_processor_info' (at offset 0xc0116a4f) and 'mp_register_lapic'
    
    Signed-off-by: Vivek Goyal <vgoyal@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Andi Kleen <ak@suse.de>

commit abf56f5ea98c71722511e37d321982013123dbbf
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Sat Dec 23 21:39:08 2006 -0500

    [PATCH] i386: CPU hotplug broken with 2GB VMSPLIT
    
    In VMSPLIT mode, kernel PGD might have more entries than user space
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Signed-off-by: Chris Wright <chrisw@sous-sol.org>

commit 1119a33a962077570ab0c2ef4712c0e48acfc577
Author: Vivek Goyal <vgoyal@in.ibm.com>
Date:   Fri Jan 5 16:36:33 2007 -0800

    [PATCH] i386: fix modpost warning in SMP trampoline code
    
    o MODPOST generates warning for i386 if kernel is compiled with
      CONFIG_RELOCATABLE=y
    
    WARNING: vmlinux - Section mismatch: reference to .init.text:startup_32_smp
    from .data between 'trampoline_data' (at offset 0xc0519cf8) and 'boot_gdt'
    
    o trampoline code/data can go into init section is CPU hotplug is not
      enabled.
    
    Signed-off-by: Vivek Goyal <vgoyal@in.ibm.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 3b1bdf4e08d6a8d4fae5a30224ed2c55bf1e43fc
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Fri Dec 8 02:41:13 2006 -0800

    [PATCH] CPU hotplug broken with 2GB VMSPLIT
    
    In VMSPLIT mode, kernel PGD might have more entries than user space.
    
    Acked-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 8c131af1db510793f87dc43edbc8950a35370df3
Author: Andi Kleen <ak@suse.de>
Date:   Tue Nov 14 16:57:46 2006 +0100

    [PATCH] x86-64: Fix vgetcpu when CONFIG_HOTPLUG_CPU is disabled
    
    The vgetcpu per CPU initialization previously relied on CPU hotplug
    events for all CPUs to initialize the per CPU state. That only
    worked only on kernels with CONFIG_HOTPLUG_CPU enabled.  On the
    others some CPUs didn't get their state initialized properly
    and vgetcpu wouldn't work.
    
    Change the initialization sequence to instead run in a normal
    initcall (which runs after the normal CPU bootup) and initialize
    all running CPUs there. Later hotplug CPUs are still handled
    with an hotplug notifier.
    
    This actually simplifies the code somewhat.
    
    Signed-off-by: Andi Kleen <ak@suse.de>

commit 7f8c4c50bda13d27afc03679d25aa1fcac8df551
Author: Srinivasa Ds <srinivasa@in.ibm.com>
Date:   Wed Oct 18 17:34:49 2006 +0530

    [POWERPC] Fix build breakage with CONFIG_PPC32
    
    low_cpu_die is called from the CPU hotplug code on 32-bit powermacs,
    but it is only defined if CONFIG_PM || CONFIG_CPU_FREQ_PMAC.  This
    changes the ifdef so it is defined for CONFIG_HOTPLUG_CPU on 32-bit
    machines.
    
    Signed-off-by: Srinivasa DS <srinivasa@in.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

commit 967cc8158c385ac0087ae3455e8835fa7eb94fd0
Author: Dave Jones <davej@redhat.com>
Date:   Sat Sep 23 01:00:03 2006 +0000

    CPUFREQ: Fix some more CPU hotplug locking.
    
    [CPUFREQ] Fix some more CPU hotplug locking.
    
    Lukewarm IQ detected in hotplug locking
    BUG: warning at kernel/cpu.c:38/lock_cpu_hotplug()
    [<b0134a42>] lock_cpu_hotplug+0x42/0x65
    [<b02f8af1>] cpufreq_update_policy+0x25/0xad
    [<b0358756>] kprobe_flush_task+0x18/0x40
    [<b0355aab>] schedule+0x63f/0x68b
    [<b01377c2>] __link_module+0x0/0x1f
    [<b0119e7d>] __cond_resched+0x16/0x34
    [<b03560bf>] cond_resched+0x26/0x31
    [<b0355b0e>] wait_for_completion+0x17/0xb1
    [<f965c547>] cpufreq_stat_cpu_callback+0x13/0x20 [cpufreq_stats]
    [<f9670074>] cpufreq_stats_init+0x74/0x8b [cpufreq_stats]
    [<b0137872>] sys_init_module+0x91/0x174
    [<b0102c81>] sysenter_past_esp+0x56/0x79
    
    As there are other places that call cpufreq_update_policy without
    the hotplug lock, it seems better to keep the hotplug locking
    at the lower level for the time being until this is revamped.
    
    Signed-off-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 9a4b9efa1d39d7d31bed08fbe5a9b2a03b2759d4
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Wed Sep 27 01:50:53 2006 -0700

    [PATCH] x86 microcode: add sysfs and hotplug support
    
    Add sysfs support.  Currently each CPU has three microcode related
    attributes.  One is 'version' which shows current ucode version of CPU.
    Tools can use the attribute do validation or show CPU ucode status.  one is
    'reload' which allows manually reloading ucode.  Another is
    'processor_flags', which exports processor flags, so we can write tools to
    check if CPU has latest ucode.  Also add suspend/resume and CPU hotplug
    support.
    
    [akpm@osdl.org: cleanups, build fix]
    [bunk@stusta.de: Kconfig fixes]
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Acked-by: Tigran Aivazian <tigran@veritas.com>
    Cc: Greg KH <greg@kroah.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit e3920fb42c8ddfe63befb54d95c0e13eabacea9b
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Mon Sep 25 23:32:48 2006 -0700

    [PATCH] Disable CPU hotplug during suspend
    
    The current suspend code has to be run on one CPU, so we use the CPU
    hotplug to take the non-boot CPUs offline on SMP machines.  However, we
    should also make sure that these CPUs will not be enabled by someone else
    after we have disabled them.
    
    The functions disable_nonboot_cpus() and enable_nonboot_cpus() are moved to
    kernel/cpu.c, because they now refer to some stuff in there that should
    better be static.  Also it's better if disable_nonboot_cpus() returns an
    error instead of panicking if something goes wrong, and
    enable_nonboot_cpus() has no reason to panic(), because the CPUs may have
    been enabled by the userland before it tries to take them online.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 7ff6f08295d90ab20d25200ef485ebb45b1b8d71
Author: Martin Peschke <mp3@de.ibm.com>
Date:   Mon Sep 25 23:31:21 2006 -0700

    [PATCH] CPU hotplug compatible alloc_percpu()
    
    This patch splits alloc_percpu() up into two phases.  Likewise for
    free_percpu().  This allows clients to limit initial allocations to online
    cpu's, and to populate or depopulate per-cpu data at run time as needed:
    
      struct my_struct *obj;
    
      /* initial allocation for online cpu's */
      obj = percpu_alloc(sizeof(struct my_struct), GFP_KERNEL);
    
      ...
    
      /* populate per-cpu data for cpu coming online */
      ptr = percpu_populate(obj, sizeof(struct my_struct), GFP_KERNEL, cpu);
    
      ...
    
      /* access per-cpu object */
      ptr = percpu_ptr(obj, smp_processor_id());
    
      ...
    
      /* depopulate per-cpu data for cpu going offline */
      percpu_depopulate(obj, cpu);
    
      ...
    
      /* final removal */
      percpu_free(obj);
    
    Signed-off-by: Martin Peschke <mp3@de.ibm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 73fea175303926055440c06bc8894f0c5c58afc8
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Tue Sep 26 10:52:35 2006 +0200

    [PATCH] i386: Support physical cpu hotplug for x86_64
    
    This patch enables ACPI based physical CPU hotplug support for x86_64.
    Implements acpi_map_lsapic() and acpi_unmap_lsapic() to support physical cpu
    hotplug.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Cc: Andi Kleen <ak@muc.de>
    Cc: "Brown, Len" <len.brown@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>

commit a670fad0adb1cc6202a607d250f10bd380593905
Author: Vojtech Pavlik <vojtech@suse.cz>
Date:   Tue Sep 26 10:52:28 2006 +0200

    [PATCH] Add initalization of the RDTSCP auxilliary values
    
    This patch adds initalization of the RDTSCP auxilliary values to CPU numbers
    to time.c. If RDTSCP is available, the MSRs are written with the respective
    values. It can be later used to initalize per-cpu timekeeping variables.
    
    AK: Some cleanups. Move externs into headers and fix CPU hotplug.
    
    Signed-off-by: Vojtech Pavlik <vojtech@suse.cz>
    Signed-off-by: Andi Kleen <ak@suse.de>

commit 4038f901cf102a40715b900984ed7540a9fa637f
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Tue Sep 26 10:52:27 2006 +0200

    [PATCH] i386/x86-64: Fix NMI watchdog suspend/resume
    
    Making NMI suspend/resume work with SMP. We use CPU hotplug to offline
    APs in SMP suspend/resume. Only BSP executes sysdev's .suspend/.resume
    method. APs should follow CPU hotplug code path.
    
    And:
    
    +From: Don Zickus <dzickus@redhat.com>
    
    Makes the start/stop paths of nmi watchdog more robust to handle the
    suspend/resume cases more gracefully.
    
    AK: I merged the two patches together
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>

commit 2ee8099f2c2bc74a7c2fac7f83e12a5d651681d3
Merge: c03efdb202a4 24669f7d00d3
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Fri Sep 22 17:50:22 2006 -0700

    Merge master.kernel.org:/pub/scm/linux/kernel/git/davej/cpufreq
    
    * master.kernel.org:/pub/scm/linux/kernel/git/davej/cpufreq:
      [CPUFREQ] sw_any_bug_dmi_table can be used on resume, so it isn't initdata
      [CPUFREQ] Fix some more CPU hotplug locking.
      [CPUFREQ] Workaround for BIOS bug in software coordination of frequency
      [CPUFREQ] Longhaul - Add voltage scaling to driver
      [CPUFREQ] Fix sparse warning in ondemand
      [CPUFREQ] make drivers/cpufreq/cpufreq_ondemand.c:powersave_bias_target() static
      [CPUFREQ] Longhaul - Add ignore_latency option
      [CPUFREQ] Longhaul - Disable arbiter
      [CPUFREQ][2/2] ondemand: updated add powersave_bias tunable
      [CPUFREQ][1/2] ondemand: updated tune for hardware coordination
      [CPUFREQ] Fix typo.

commit ddad65df0048e210c93640b59b3bad12701febb6
Author: Dave Jones <davej@redhat.com>
Date:   Fri Sep 22 19:15:23 2006 -0400

    [CPUFREQ] Fix some more CPU hotplug locking.
    
    Lukewarm IQ detected in hotplug locking
    BUG: warning at kernel/cpu.c:38/lock_cpu_hotplug()
    [<b0134a42>] lock_cpu_hotplug+0x42/0x65
    [<b02f8af1>] cpufreq_update_policy+0x25/0xad
    [<b0358756>] kprobe_flush_task+0x18/0x40
    [<b0355aab>] schedule+0x63f/0x68b
    [<b01377c2>] __link_module+0x0/0x1f
    [<b0119e7d>] __cond_resched+0x16/0x34
    [<b03560bf>] cond_resched+0x26/0x31
    [<b0355b0e>] wait_for_completion+0x17/0xb1
    [<f965c547>] cpufreq_stat_cpu_callback+0x13/0x20 [cpufreq_stats]
    [<f9670074>] cpufreq_stats_init+0x74/0x8b [cpufreq_stats]
    [<b0137872>] sys_init_module+0x91/0x174
    [<b0102c81>] sysenter_past_esp+0x56/0x79
    
    As there are other places that call cpufreq_update_policy without
    the hotplug lock, it seems better to keep the hotplug locking
    at the lower level for the time being until this is revamped.
    
    Signed-off-by: Dave Jones <davej@redhat.com>

commit 4c4d50f7b39cc58f1064b93a61ad617451ae41df
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Aug 27 01:23:51 2006 -0700

    [PATCH] cpuset: top_cpuset tracks hotplug changes to cpu_online_map
    
    Change the list of cpus allowed to tasks in the top (root) cpuset to
    dynamically track what cpus are online, using a CPU hotplug notifier.  Make
    this top cpus file read-only.
    
    On systems that have cpusets configured in their kernel, but that aren't
    actively using cpusets (for some distros, this covers the majority of
    systems) all tasks end up in the top cpuset.
    
    If that system does support CPU hotplug, then these tasks cannot make use
    of CPUs that are added after system boot, because the CPUs are not allowed
    in the top cpuset.  This is a surprising regression over earlier kernels
    that didn't have cpusets enabled.
    
    In order to keep the behaviour of cpusets consistent between systems
    actively making use of them and systems not using them, this patch changes
    the behaviour of the 'cpus' file in the top (root) cpuset, making it read
    only, and making it automatically track the value of cpu_online_map.  Thus
    tasks in the top cpuset will have automatic use of hot plugged CPUs allowed
    by their cpuset.
    
    Thanks to Anton Blanchard and Nathan Lynch for reporting this problem,
    driving the fix, and earlier versions of this patch.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Cc: Nathan Lynch <ntl@pobox.com>
    Cc: Anton Blanchard <anton@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 81b73dd92b97423b8f5324a59044da478c04f4c4
Author: Haren Myneni <haren@us.ibm.com>
Date:   Thu Jul 27 14:29:00 2006 -0700

    [POWERPC] Fix might-sleep warning on removing cpus
    
    Noticing the following might_sleep warning (dump_stack()) during kdump
    testing when CONFIG_DEBUG_SPINLOCK_SLEEP is enabled. All secondary CPUs
    will be calling rtas_set_indicator with interrupts disabled to remove
    them from global interrupt queue.
    
    BUG: sleeping function called from invalid context at
    arch/powerpc/kernel/rtas.c:463
    in_atomic():1, irqs_disabled():1
    Call Trace:
    [C00000000FFFB970] [C000000000010234] .show_stack+0x68/0x1b0 (unreliable)
    [C00000000FFFBA10] [C000000000059354] .__might_sleep+0xd8/0xf4
    [C00000000FFFBA90] [C00000000001D1BC] .rtas_busy_delay+0x20/0x5c
    [C00000000FFFBB20] [C00000000001D8A8] .rtas_set_indicator+0x6c/0xcc
    [C00000000FFFBBC0] [C000000000048BF4] .xics_teardown_cpu+0x118/0x134
    [C00000000FFFBC40] [C00000000004539C]
    .pseries_kexec_cpu_down_xics+0x74/0x8c
    [C00000000FFFBCC0] [C00000000002DF08] .crash_ipi_callback+0x15c/0x188
    [C00000000FFFBD50] [C0000000000296EC] .smp_message_recv+0x84/0xdc
    [C00000000FFFBDC0] [C000000000048E08] .xics_ipi_dispatch+0xf0/0x130
    [C00000000FFFBE50] [C00000000009EF10] .handle_IRQ_event+0x7c/0xf8
    [C00000000FFFBF00] [C0000000000A0A14] .handle_percpu_irq+0x90/0x10c
    [C00000000FFFBF90] [C00000000002659C] .call_handle_irq+0x1c/0x2c
    [C00000000058B9C0] [C00000000000CA10] .do_IRQ+0xf4/0x1a4
    [C00000000058BA50] [C0000000000044EC] hardware_interrupt_entry+0xc/0x10
     --- Exception: 501 at .plpar_hcall_norets+0x14/0x1c
       LR = .pseries_dedicated_idle_sleep+0x190/0x1d4
    [C00000000058BD40] [C00000000058BDE0] 0xc00000000058bde0 (unreliable)
    [C00000000058BDF0] [C00000000001270C] .cpu_idle+0x10c/0x1e0
    [C00000000058BE70] [C000000000009274] .rest_init+0x44/0x5c
    
    To fix this issue, rtas_set_indicator_fast() is added so that will not
    wait for RTAS 'busy' delay and this new function is used for kdump (in
    xics_teardown_cpu()) and for CPU hotplug ( xics_migrate_irqs_away() and
    xics_setup_cpu()).
    
    Note that the platform architecture spec says that set-indicator
    on the indicator we're using here is not permitted to return the
    busy or extended busy status codes.
    
    Signed-off-by: Haren Myneni <haren@us.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

commit 5cb900a34f640771a4ab6a178659c75022f2971a
Author: Gerald Schaefer <geraldsc@de.ibm.com>
Date:   Mon Aug 7 18:13:09 2006 +0200

    [S390] add __cpuinit to appldata_cpu_notify
    
    Use __cpuinit for CPU hotplug notifier function.
    
    Signed-off-by: Gerald Schaefer <geraldsc@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

commit 46f5960fdbf359f0c75989854bbaebc1de7a1eb4
Merge: 90eb29efd0ca 29bbd72d6ee1
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Wed Aug 2 22:35:26 2006 -0700

    Merge master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6
    
    * master.kernel.org:/pub/scm/linux/kernel/git/davem/net-2.6: (32 commits)
      [NET]: Fix more per-cpu typos
      [SECURITY]: Fix build with CONFIG_SECURITY disabled.
      [I/OAT]: Remove CPU hotplug lock from net_dma_rebalance
      [DECNET]: Fix for routing bug
      [AF_UNIX]: Kernel memory leak fix for af_unix datagram getpeersec patch
      [NET]: skb_queue_lock_key() is no longer used.
      [NET]: Remove lockdep_set_class() call from skb_queue_head_init().
      [IPV6]: SNMPv2 "ipv6IfStatsOutFragCreates" counter error
      [IPV6]: SNMPv2 "ipv6IfStatsInHdrErrors" counter error
      [NET]: Kill the WARN_ON() calls for checksum fixups.
      [NETFILTER]: xt_hashlimit/xt_string: missing string validation
      [NETFILTER]: SIP helper: expect RTP streams in both directions
      [E1000]: Convert to netdev_alloc_skb
      [TG3]: Convert to netdev_alloc_skb
      [NET]: Add netdev_alloc_skb().
      [TCP]: Process linger2 timeout consistently.
      [SECURITY] secmark: nul-terminate secdata
      [NET] infiniband: Cleanup ib_addr module to use the netevents
      [NET]: Core net changes to generate netevents
      [NET]: Network Event Notifier Mechanism.
      ...

commit e6eb307d48c81d688804f8b39a0a3ddde3cd3458
Author: Chris Leech <christopher.leech@intel.com>
Date:   Wed Aug 2 14:21:19 2006 -0700

    [I/OAT]: Remove CPU hotplug lock from net_dma_rebalance
    
    Remove the lock_cpu_hotplug()/unlock_cpu_hotplug() calls from
    net_dma_rebalance
    
    The lock_cpu_hotplug()/unlock_cpu_hotplug() sequence in
    net_dma_rebalance is both incorrect (as pointed out by David Miller)
    because lock_cpu_hotplug() may sleep while the net_dma_event_lock
    spinlock is held, and unnecessary (as pointed out by Andrew Morton) as
    spin_lock() disables preemption which protects from CPU hotplug
    events.
    
    Signed-off-by: Chris Leech <christopher.leech@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit aa95387774039096c11803c04011f1aa42d85758
Author: Linus Torvalds <torvalds@macmini.osdl.org>
Date:   Sun Jul 23 12:12:16 2006 -0700

    cpu hotplug: simplify and hopefully fix locking
    
    The CPU hotplug locking was quite messy, with a recursive lock to
    handle the fact that both the actual up/down sequence wanted to
    protect itself from being re-entered, but the callbacks that it
    called also tended to want to protect themselves from CPU events.
    
    This splits the lock into two (one to serialize the whole hotplug
    sequence, the other to protect against the CPU present bitmaps
    changing). The latter still allows recursive usage because some
    subsystems (ondemand policy for cpufreq at least) had already gotten
    too used to the lax locking, but the locking mistakes are hopefully
    now less fundamental, and we now warn about recursive lock usage
    when we see it, in the hope that it can be fixed.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit efc30814a88bdbe2bfe4ac94de2eb089ad80bee3
Author: Kirill Korotaev <dev@sw.ru>
Date:   Tue Jun 27 02:54:32 2006 -0700

    [PATCH] sched: CPU hotplug race vs. set_cpus_allowed()
    
    There is a race between set_cpus_allowed() and move_task_off_dead_cpu().
    __migrate_task() doesn't report any err code, so task can be left on its
    runqueue if its cpus_allowed mask changed so that dest_cpu is not longer a
    possible target.  Also, chaning cpus_allowed mask requires rq->lock being
    held.
    
    Signed-off-by: Kirill Korotaev <dev@openvz.org>
    Acked-By: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit bd9e0b74f52dbac6241643fadca2393808b14c7a
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Tue Jun 27 02:53:43 2006 -0700

    [PATCH] x86: cpu_init(): avoid GFP_KERNEL allocation while atomic
    
    The patch fixes two issues:
    
    1.  cpu_init is called with interrupt disabled.  Allocating gdt table
       there isn't good at runtime.
    
    2. gdt table page cause memory leak in CPU hotplug case.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Cc: Ashok Raj <ashok.raj@intel.com>
    Cc: Zachary Amsden <zach@vmware.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 3e1792fa40be9dd0f530e2b32f439bb97c40bcee
Author: Brian Uhrain says <buhrain@rosettastone.com>
Date:   Mon Apr 10 22:53:16 2006 -0700

    [PATCH] alpha: SMP boot fixes
    
    I've encountered two problems with 2.6.16 and newer kernels on my API CS20
    (dual 833MHz Alpha 21264b processors).  The first is the kernel OOPSing
    because of a NULL pointer dereference while trying to populate SysFS with the
    CPU information.  The other is that only one processor was being brought up.
    I've included a small Alpha-specific patch that fixes both problems.
    
    The first problem was caused by the CPUs never being properly registered using
    register_cpu(), the way it's done on other architectures.
    
    The second problem has to do with the removal of hwrpb_cpu_present_mask in
    arch/alpha/kernel/smp.c.  In setup_smp() in the 2.6.15 kernel sources,
    hwrpb_cpu_present_mask has a bit set for each processor that is probed, and
    afterwards cpu_present_mask is set to the cpumask for the boot CPU.  In the
    same function of the same file in the 2.6.16 sources, instead of
    hwrpb_cpu_present_mask being set, cpu_possible_map is updated for each probed
    CPU.  cpu_present_mask is still set to the cpumask of the boot CPU afterwards.
     The problem lies in include/asm-alpha/smp.h, where cpu_possible_map is
    #define'd to be cpu_present_mask.
    
    Cleanups from: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    
     - cpu_present_mask and cpu_possible_map are essentially the same thing
       on alpha, as it doesn't support CPU hotplug;
     - allocate "struct cpu" only for present CPUs, like sparc64 does.
       Static array of "struct cpu" is just a waste of memory.
    
    Signed-off-by: Brian Uhrain <buhrain@rosettastone.com>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 3a62231e38fe628ec2540fb22276ea1c3af5ddd8
Author: Hirokazu Takata <takata@linux-m32r.org>
Date:   Mon Apr 10 22:53:18 2006 -0700

    [PATCH] m32r: Fix cpu_possible_map and cpu_present_map initialization for SMP kernel
    
    This patch fixes a boot problem of the m32r SMP kernel 2.6.16-rc1-mm3 or
    later.
    
    In this patch, cpu_possible_map is statically initialized, and cpu_present_map
    is also copied from cpu_possible_map in smp_prepare_cpus(), because the m32r
    architecture has not supported CPU hotplug yet.
    
    Signed-off-by: Hayato Fujiwara <fujiwara.hayato@renesas.com>
    Signed-off-by: Hirokazu Takata <takata@linux-m32r.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

commit 7c1c4e541888947947bc46a18a9a5543a259ed62
Author: Hirokazu Takata <takata@linux-m32r.org>
Date:   Mon Apr 10 22:53:18 2006 -0700

    [PATCH] m32r: Fix cpu_possible_map and cpu_present_map initialization for SMP kernel
    
    This patch fixes a boot problem of the m32r SMP kernel 2.6.16-rc1-mm3 or
    later.
    
    In this patch, cpu_possible_map is statically initialized, and cpu_present_map
    is also copied from cpu_possible_map in smp_prepare_cpus(), because the m32r
    architecture has not supported CPU hotplug yet.
    
    Signed-off-by: Hayato Fujiwara <fujiwara.hayato@renesas.com>
    Signed-off-by: Hirokazu Takata <takata@linux-m32r.org>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 917b1f78a9871a1985004df09ed1eb2e0dc3bf4f
Author: Brian Uhrain says <buhrain@rosettastone.com>
Date:   Mon Apr 10 22:53:16 2006 -0700

    [PATCH] alpha: SMP boot fixes
    
    I've encountered two problems with 2.6.16 and newer kernels on my API CS20
    (dual 833MHz Alpha 21264b processors).  The first is the kernel OOPSing
    because of a NULL pointer dereference while trying to populate SysFS with the
    CPU information.  The other is that only one processor was being brought up.
    I've included a small Alpha-specific patch that fixes both problems.
    
    The first problem was caused by the CPUs never being properly registered using
    register_cpu(), the way it's done on other architectures.
    
    The second problem has to do with the removal of hwrpb_cpu_present_mask in
    arch/alpha/kernel/smp.c.  In setup_smp() in the 2.6.15 kernel sources,
    hwrpb_cpu_present_mask has a bit set for each processor that is probed, and
    afterwards cpu_present_mask is set to the cpumask for the boot CPU.  In the
    same function of the same file in the 2.6.16 sources, instead of
    hwrpb_cpu_present_mask being set, cpu_possible_map is updated for each probed
    CPU.  cpu_present_mask is still set to the cpumask of the boot CPU afterwards.
     The problem lies in include/asm-alpha/smp.h, where cpu_possible_map is
    #define'd to be cpu_present_mask.
    
    Cleanups from: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    
     - cpu_present_mask and cpu_possible_map are essentially the same thing
       on alpha, as it doesn't support CPU hotplug;
     - allocate "struct cpu" only for present CPUs, like sparc64 does.
       Static array of "struct cpu" is just a waste of memory.
    
    Signed-off-by: Brian Uhrain <buhrain@rosettastone.com>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 51f62e186b78f8743246a349b09be370c8735479
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Sat Mar 25 16:29:28 2006 +0100

    [PATCH] x86_64: cleanup allocating logical cpu numbers in x86_64
    
    Minor cleanup to lend better for physical CPU hotplug.
    Earlier way of using num_processors as index doesnt
    fit if CPUs come and go. This makes the code little bit better
    to read, and helps physical hotplug use the same functions as boot.
    
    Reserving CPU0 for BSP is too late to be done in smp_prepare_boot_cpu().
    Since logical assignments from MADT is already done via
    setup_arch()->acpi_boot_init()->parse lapic
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit be9bf30c73184e0f1e4e0a50fb193d2a551bf75e
Merge: ace48471736a 84f0b1ef8c01
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Sat Mar 25 08:52:23 2006 -0800

    Merge master.kernel.org:/pub/scm/linux/kernel/git/davej/cpufreq
    
    * master.kernel.org:/pub/scm/linux/kernel/git/davej/cpufreq:
      [CPUFREQ] kzalloc conversion for gx-suspmod
      [CPUFREQ] Whitespace cleanup
      [CPUFREQ] Mark longhaul driver as broken.
      [PATCH] cpufreq: fix section mismatch warnings
      [CPUFREQ] Fix the p4-clockmod N60 errata workaround.
      [CPUFREQ] Fix handling for CPU hotplug
      [CPUFREQ] powernow-k8: Let cpufreq driver handle affected CPUs
      [CPUFREQ] Lots of whitespace & CodingStyle cleanup.
      [CPUFREQ] Remove duplicate cpuinfo struct
      [CPUFREQ] Silence powernow-k8 warning on k7's.

commit 9a0b5817ad97bb718ab85322759d19a238712b47
Author: Gerd Hoffmann <kraxel@suse.de>
Date:   Thu Mar 23 02:59:32 2006 -0800

    [PATCH] x86: SMP alternatives
    
    Implement SMP alternatives, i.e.  switching at runtime between different
    code versions for UP and SMP.  The code can patch both SMP->UP and UP->SMP.
    The UP->SMP case is useful for CPU hotplug.
    
    With CONFIG_CPU_HOTPLUG enabled the code switches to UP at boot time and
    when the number of CPUs goes down to 1, and switches to SMP when the number
    of CPUs goes up to 2.
    
    Without CONFIG_CPU_HOTPLUG or on non-SMP-capable systems the code is
    patched once at boot time (if needed) and the tables are released
    afterwards.
    
    The changes in detail:
    
      * The current alternatives bits are moved to a separate file,
        the SMP alternatives code is added there.
    
      * The patch adds some new elf sections to the kernel:
        .smp_altinstructions
            like .altinstructions, also contains a list
            of alt_instr structs.
        .smp_altinstr_replacement
            like .altinstr_replacement, but also has some space to
            save original instruction before replaving it.
        .smp_locks
            list of pointers to lock prefixes which can be nop'ed
            out on UP.
        The first two are used to replace more complex instruction
        sequences such as spinlocks and semaphores.  It would be possible
        to deal with the lock prefixes with that as well, but by handling
        them as special case the table sizes become much smaller.
    
     * The sections are page-aligned and padded up to page size, so they
       can be free if they are not needed.
    
     * Splitted the code to release init pages to a separate function and
       use it to release the elf sections if they are unused.
    
    Signed-off-by: Gerd Hoffmann <kraxel@suse.de>
    Signed-off-by: Chuck Ebbert <76306.1226@compuserve.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 8ff69732d484ea9ccbf242cc49b4fe9538e64c71
Author: Dave Jones <davej@redhat.com>
Date:   Sun Mar 5 03:37:23 2006 -0500

    [CPUFREQ] Fix handling for CPU hotplug
    
    This patch adds proper logic to cpufreq driver in order to handle
    CPU Hotplug.
    
    When CPUs go on/offline, the affected CPUs data, cpufreq_policy->cpus,
    is not updated properly. This causes sysfs directories and symlinks to
    be in an incorrect state after few CPU on/offlines.
    
    Signed-off-by: Jacob Shin <jacob.shin@amd.com>
    Signed-off-by: Dave Jones <davej@redhat.com>

commit 69aa234b918c0d9bc4a20cd6d4453aaa3418f457
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Tue Feb 14 15:01:11 2006 -0800

    [IA64] Dont set NR_CPUS for cpu_possible_map when CPU hotplug is enabled.
    
    Do not set cpu_possible_map for NR_CPUS when ACPI_CONFIG_HOTPLUG_CPU is set.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Tony Luck <tony.luck@intel.com>

commit fe38d8553ccb5237bf0eddda9e94fbca7288551c
Author: Chuck Ebbert <76306.1226@compuserve.com>
Date:   Sat Feb 4 23:28:03 2006 -0800

    [PATCH] i386 cpu hotplug: don't access freed memory
    
    i386 CPU init code accesses freed init memory when booting a newly-started
    processor after CPU hotplug.  The cpu_devs array is searched to find the
    vendor and it contains pointers to freed data.
    
    Fix that by:
    
            1. Zeroing entries for freed vendor data after bootup.
            2. Changing Transmeta, NSC and UMC to all __init[data].
            3. Printing a warning (once only) and setting this_cpu
               to a safe default when the vendor is not found.
    
    This does not change behavior for AMD systems.  They were broken already
    but no error was reported.
    
    Signed-off-by: Chuck Ebbert <76306.1226@compuserve.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit bd576c9523fbf23e94fb7dbe05d2ae1cf96864e4
Author: Chuck Ebbert <76306.1226@compuserve.com>
Date:   Sat Feb 4 23:27:42 2006 -0800

    [PATCH] sched: only print migration_cost once per boot
    
    migration_cost prints after every CPU hotplug event.  Make it print only
    once at boot.
    
    Signed-off-by: Chuck Ebbert <76306.1226@compuserve.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 7eb903f4a5c35c8310f0aa7b0e94aae0b826d837
Author: Andi Kleen <ak@suse.de>
Date:   Wed Jan 11 22:42:39 2006 +0100

    [PATCH] x86_64: Add documentation for CPU hotplug ACPI extension
    
    Cc: len.brown@intel.com, ashok.ray@intel.com
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit cffe632a25b017dac4b6f060cad31940c6c167b4
Author: akpm@osdl.org <akpm@osdl.org>
Date:   Mon Jan 9 20:51:48 2006 -0800

    [PATCH] kdump: x86_64 kexec on panic
    
    )
    
    From: Vivek Goyal <vgoyal@in.ibm.com>
    
    - Implementing the machine_crash_shutdown for x86_64 which will be called by
      crash_kexec (called in case of a panic, sysrq etc.).  Here we do things
      similar to i386.  Disable the interrupts, shootdown the cpus and shutdown
      LAPIC and IOAPIC.
    
    Changes in this version:
    
    - As the Eric's APIC initialization patches are reverted back, reintroducing
      LAPIC and IOAPIC shutdown.
    
    - Added some comments on CPU hotplug, modified code as suggested by Andi
      kleen.
    
    Signed-off-by: Murali M Chakravarthy <muralim@in.ibm.com>
    Signed-off-by: Vivek Goyal <vgoyal@in.ibm.com>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit c809406b4f2dfac9006d7eb8dca6b9f990f10b61
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Sun Jan 8 01:03:17 2006 -0800

    [PATCH] Updated CPU hotplug documentation
    
    Thanks to Nathan Lynch for the review and comments.  Thanks to Joel Schopp
    for the pointer to add user space scipts.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Nathan Lynch <nathanl@austin.ibm.com>
    Signed-off-by: Joel Schopp <jschopp@austin.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 1fa744e6e91a895750b9980d13fcfc5791a0cd91
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Fri Jan 6 00:12:20 2006 -0800

    [PATCH] cpu hotplug/x86_64: disable interrupt in play_dead
    
    With physical CPU hotplug, the CPU is hot removed and it should not receive
    any interrupts.  Disabling interrupt is much safer.  This basically is what we
    do in ia64 & x86.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 1855a2c4ce708b823b8b824f8b12937b45f5462a
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Fri Jan 6 00:12:08 2006 -0800

    [PATCH] x86: convert bigsmp to use flat physical mode
    
    When we bring up a new CPU via INIT/startup IPI messages, the CPU that's
    coming up sends a xTPR message to the chipset.  Intel chipsets (at least)
    don't provide any architectural guarantee on what the chipset will do with
    this message.  For example, the E850x chipsets uses this xTPR message to
    interpret the interrupt operating mode of the platform.  When the CPU
    coming online sends this message, it always indicates that it is in logical
    flat mode.  For the CPU hotplug case, the platform may already be
    functioning in cluster APIC mode at this time, the chipset can get confused
    and mishandle I/O device and IPI interrupt routing.
    
    The situation eventually gets corrected when the new CPU sends another xTPR
    update when we switch it to cluster mode, but there's a window during which
    the chipset may be in an inconsistent state.  This patch avoids this
    problem by using the flat physical interrupt delivery mode instead of
    cluster mode for bigsmp (>8 cpu) support.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 82584ef75b7c14f22028595b0e8aa904464c5240
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Mon Dec 12 00:37:02 2005 -0800

    [PATCH] x86: fix NMI with CPU hotplug
    
    With CPU hotplug enabled, NMI watchdog stoped working.  It appears the
    violation is the cpu_online check in nmi handler.  local ACPI based NMI
    watchdog is initialized before we set CPU online for APs.  It's quite
    possible a NMI is fired before we set CPU online, and that's what happens
    here.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Acked-by: Zwane Mwaikambo <zwane@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit fbe83e209ad9c8281e29ac17a60f91119d86fa8c
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Sun Nov 20 18:49:06 2005 +0100

    [PATCH] Register disabled CPUs
    
    Needed to make the earlier use disabled CPUs for CPU hotplug patch
    actually work.
    
    Need to register disabled processors as well, so we can count them
    towards cpu_possible_map as hot pluggable cpus.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 420f8f68c9c5148dddf946bebdbc7eacde2172cb
Author: Andi Kleen <ak@suse.de>
Date:   Sat Nov 5 17:25:54 2005 +0100

    [PATCH] x86_64: New heuristics to find out hotpluggable CPUs.
    
    With a NR_CPUS==128 kernel with CPU hotplug enabled we would waste 4MB
    on per CPU data of all possible CPUs.  The reason was that HOTPLUG
    always set up possible map to NR_CPUS cpus and then we need to allocate
    that much (each per CPU data is roughly ~32k now)
    
    The underlying problem is that ACPI didn't tell us how many hotplug CPUs
    the platform supports.  So the old code just assumed all, which would
    lead to this memory wastage.
    
    This implements some new heuristics:
    
     - If the BIOS specified disabled CPUs in the ACPI/mptables assume they
       can be enabled later (this is bending the ACPI specification a bit,
       but seems like a obvious extension)
     - The user can overwrite it with a new additionals_cpus=NUM option
     - Otherwise use half of the available CPUs or 2, whatever is more.
    
    Cc: ashok.raj@intel.com
    Cc: len.brown@intel.com
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 5bfb5d690f36d316a5f3b4f7775fda996faa6b12
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Tue Nov 8 21:39:01 2005 -0800

    [PATCH] sched: disable preempt in idle tasks
    
    Run idle threads with preempt disabled.
    
    Also corrected a bugs in arm26's cpu_idle (make it actually call schedule()).
    How did it ever work before?
    
    Might fix the CPU hotplugging hang which Nigel Cunningham noted.
    
    We think the bug hits if the idle thread is preempted after checking
    need_resched() and before going to sleep, then the CPU offlined.
    
    After calling stop_machine_run, the CPU eventually returns from preemption and
    into the idle thread and goes to sleep.  The CPU will continue executing
    previous idle and have no chance to call play_dead.
    
    By disabling preemption until we are ready to explicitly schedule, this bug is
    fixed and the idle threads generally become more robust.
    
    From: alexs <ashepard@u.washington.edu>
    
      PPC build fix
    
    From: Yoichi Yuasa <yuasa@hh.iij4u.or.jp>
    
      MIPS build fix
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Yoichi Yuasa <yuasa@hh.iij4u.or.jp>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 90d45d17f3e68608ac7ba8fc3d7acce022a19c8e
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Tue Nov 8 21:34:24 2005 -0800

    [PATCH] cpu hotplug: fix locking in cpufreq drivers
    
    When calling target drivers to set frequency, we take cpucontrol lock.
    When we modified the code to accomodate CPU hotplug, there was an attempt
    to take a double lock of cpucontrol leading to a deadlock.  Since the
    current thread context is already holding the cpucontrol lock, we dont need
    to make another attempt to acquire it.
    
    Now we leave a trace in current->flags indicating current thread already is
    under cpucontrol lock held, so we dont attempt to do this another time.
    
    Thanks to Andrew Morton for the beating:-)
    
    From: Brice Goglin <Brice.Goglin@ens-lyon.org>
    
      Build fix
    
    (akpm: this patch is still unpleasant.  Ashok continues to look for a cleaner
    solution, doesn't he?  ;))
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Brice Goglin <Brice.Goglin@ens-lyon.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 97a63ecff4bd06da5d8feb8c0394a4d020f2d34d
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Nov 9 13:50:57 2005 +0000

    [ARM SMP] Add CPU hotplug support for Realview MPcore
    
    Add platform specific parts for hotplug CPU support for the
    Realview board.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit a241ec65aeac3d69a08a7b153cccbdb7ea35063f
Author: Paul E. McKenney <paulmck@us.ibm.com>
Date:   Sun Oct 30 15:03:12 2005 -0800

    [PATCH] RCU torture-testing kernel module
    
    This patch is a rewrite of the one submitted on October 1st, using modules
    (http://marc.theaimsgroup.com/?l=linux-kernel&m=112819093522998&w=2).
    
    This rewrite adds a tristate CONFIG_RCU_TORTURE_TEST, which enables an
    intense torture test of the RCU infratructure.  This is needed due to the
    continued changes to the RCU infrastructure to accommodate dynamic ticks,
    CPU hotplug, realtime, and so on.  Most of the code is in a separate file
    that is compiled only if the CONFIG variable is set.  Documentation on how
    to run the test and interpret the output is also included.
    
    This code has been tested on i386 and ppc64, and an earlier version of the
    code has received extensive testing on a number of architectures as part of
    the PREEMPT_RT patchset.
    
    Signed-off-by: "Paul E. McKenney" <paulmck@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 421c7ce6d001fce28b1fa8fdd2e7ded0ed8a0ad5
Author: Andi Kleen <ak@suse.de>
Date:   Mon Oct 10 22:32:45 2005 +0200

    [PATCH] x86_64: Allocate cpu local data for all possible CPUs
    
    CPU hotplug fills up the possible map to NR_CPUs, but it did that after
    setting up per CPU data. This lead to CPU data not getting allocated
    for all possible CPUs, which lead to various side effects.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 26ff6ad9786abf6f40a6d3cbb89753b4fa50cb00
Author: Srivatsa Vaddagiri <vatsa@in.ibm.com>
Date:   Fri Sep 16 19:27:40 2005 -0700

    [PATCH] CPU hotplug breaks wake_up_new_task
    
    Fix a problem wherein a new-born task is added to a dead CPU.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Acked-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Acked-by: Shaohua Li <shaohua.li@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 5bf97e01192f0fcdff2f0e9eae063e7785f6f915
Author: Andi Kleen <ak@suse.de>
Date:   Mon Sep 12 18:49:24 2005 +0200

    [PATCH] x86-64: Use physflat on Intel for < 8 CPUs with CPU hotplug
    
    This avoids races with the APIC broadcast/mask modes.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 413588c7cb8113c03d0044f1d41b832ad7201c29
Author: Andi Kleen <ak@suse.de>
Date:   Mon Sep 12 18:49:24 2005 +0200

    [PATCH] x86-64: Remove code to resume machine check state of other CPUs.
    
    The resume code uses CPU hotplug now so at resume time
    we only ever see one CPU.
    
    Pointed out by Yu Luming.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit fdf26d933a8171c2a5bd35b4a5ca3b099a216a35
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Fri Sep 9 13:01:52 2005 -0700

    [PATCH] x86_64: Don't do broadcast IPIs when hotplug is enabled in flat mode.
    
    The use of non-shortcut version of routines breaking CPU hotplug.  The option
    to select this via cmdline also is deleted with the physflat patch, hence
    directly placing this code under CONFIG_HOTPLUG_CPU.
    
    We dont want to use broadcast mode IPI's when hotplug is enabled.  This causes
    bad effects in send IPI to a cpu that is offline which can trip when the cpu
    is in the process of being kicked alive.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Acked-by: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit f8d311939f9d2b2a5e935df8dceb98b7cbe08d43
Author: Andi Kleen <ak@suse.de>
Date:   Thu Jul 28 21:15:42 2005 -0700

    [PATCH] x86_64: Support more than 8 cores on AMD systems
    
    Use physical mode instead of logical mode to address more CPUs.  This is also
    used in the CPU hotplug case to avoid a race.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit ac25575203c11145066ea5cb583354cb5f0a8ade
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Sat Jun 25 14:55:15 2005 -0700

    [PATCH] CPU hotplug printk fix
    
    In the cpu hotplug case, per-cpu data possibly isn't initialized even the
    system state is 'running'.  As the comments say in the original code, some
    console drivers assume per-cpu resources have been allocated.  radeon fb is
    one such driver, which uses kmalloc.  After a CPU is down, the per-cpu data
    of slab is freed, so the system crashed when printing some info.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 5a72e04df5470df0ec646029d31e5528167ab1a7
Author: Li Shaohua <shaohua.li@intel.com>
Date:   Sat Jun 25 14:55:06 2005 -0700

    [PATCH] suspend/resume SMP support
    
    Using CPU hotplug to support suspend/resume SMP.  Both S3 and S4 use
    disable/enable_nonboot_cpus API.  The S4 part is based on Pavel's original S4
    SMP patch.
    
    Signed-off-by: Li Shaohua<shaohua.li@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit a9fa06c26f7b7914c8cdf4d309b74df3151cc227
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Sat Jun 25 14:55:05 2005 -0700

    [PATCH] set cpu_state for CPU hotplug (ia64)
    
    Dead CPU notifies online CPU that it's dead using cpu_state variable.
    After switching to physical cpu hotplug, we forgot setting the variable.
    This patch fixes it.  Currently only __cpu_die uses it.  We changed other
    locations for consistency in case others use it.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Acked-by: Ashok Raj <ashok.raj@intel.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit a02c4cb67e4ccd5ce7a13c7f04c2fedb06c35431
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Sat Jun 25 14:55:03 2005 -0700

    [PATCH] x86_64: Provide ability to choose using shortcuts for IPI in flat mode.
    
    This patch provides an option to switch broadcast or use mask version for
    sending IPI's.  If CONFIG_HOTPLUG_CPU is defined, we choose not to use
    broadcast shortcuts by default, otherwise we choose broadcast mode as default.
    
    both cases, one can change this via startup cmd line option, to choose
    no-broadcast mode.
    
            no_ipi_broadcast=1
    
    This is provided on request from Andi Kleen, since he doesnt agree with
    replacing IPI shortcuts as a solution for CPU hotplug.  Without removing
    broadcast IPI's, it would mean lots of new code for __cpu_up() path, which
    would acheive the same results.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Acked-by: Andi Kleen <ak@muc.de>
    Acked-by: Zwane Mwaikambo <zwane@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit cb0cd8d49a9b81aff7a02e2ed826b5cfdfe9a172
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Sat Jun 25 14:55:01 2005 -0700

    [PATCH] x86_64: CPU hotplug sibling map cleanup
    
    This patch is a minor cleanup to the cpu sibling/core map.  It is required
    that this setup happens on a per-cpu bringup time.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Acked-by: Andi Kleen <ak@muc.de>
    Acked-by: Zwane Mwaikambo <zwane@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 76e4f660d9f4c6d1bb473f72be2988c35eaca948
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Sat Jun 25 14:55:00 2005 -0700

    [PATCH] x86_64: CPU hotplug support
    
      Experimental CPU hotplug patch for x86_64
      -----------------------------------------
    This supports logical CPU online and offline.
    - Test with maxcpus=1, and then kick other cpu's off to test if init code
      is all cleaned up. CONFIG_SCHED_SMT works as well.
    - idle threads are forked on demand from keventd threads for clean startup
    
    TBD:
    1. Not tested on a real NUMA machine (tested with numa=fake=2)
    2. Handle ACPI pieces for physical hotplug support.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Acked-by: Andi Kleen <ak@muc.de>
    Acked-by: Zwane Mwaikambo <zwane@arm.linux.org.uk>
    Signed-off-by: Shaohua.li<shaohua.li@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit e6982c671c560da4a0bc5c908cbcbec12bd5991d
Author: Ashok Raj <ashok.raj@intel.com>
Date:   Sat Jun 25 14:54:58 2005 -0700

    [PATCH] x86_64: Change init sections for CPU hotplug support
    
    This patch adds __cpuinit and __cpuinitdata sections that need to exist past
    boot to support cpu hotplug.
    
    Caveat: This is done *only* for EM64T CPU Hotplug support, on request from
    Andi Kleen.  Much of the generic hotplug code in kernel, and none of the other
    archs that support CPU hotplug today, i386, ia64, ppc64, s390 and parisc dont
    mark sections with __cpuinit, but only mark them as __devinit, and
    __devinitdata.
    
    If someone is motivated to change generic code, we need to make sure all
    existing hotplug code does not break, on other arch's that dont use __cpuinit,
    and __cpudevinit.
    
    Signed-off-by: Ashok Raj <ashok.raj@intel.com>
    Acked-by: Andi Kleen <ak@muc.de>
    Acked-by: Zwane Mwaikambo <zwane@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit e1367daf3eed5cd619ee88c9907e1e6ddaa58406
Author: Li Shaohua <shaohua.li@intel.com>
Date:   Sat Jun 25 14:54:56 2005 -0700

    [PATCH] cpu state clean after hot remove
    
    Clean CPU states in order to reuse smp boot code for CPU hotplug.
    
    Signed-off-by: Li Shaohua<shaohua.li@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 0bb3184df537002a742bafddf3f4fb482b7fe610
Author: Li Shaohua <shaohua.li@intel.com>
Date:   Sat Jun 25 14:54:55 2005 -0700

    [PATCH] init call cleanup
    
    Trival patch for CPU hotplug.  In CPU identify part, only did cleaup for intel
    CPUs.  Need do for other CPUs if they support S3 SMP.
    
    Signed-off-by: Li Shaohua<shaohua.li@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit f370513640492641b4046bfd9a6e4714f6ae530d
Author: Zwane Mwaikambo <zwane@linuxpower.ca>
Date:   Sat Jun 25 14:54:50 2005 -0700

    [PATCH] i386 CPU hotplug
    
    (The i386 CPU hotplug patch provides infrastructure for some work which Pavel
    is doing as well as for ACPI S3 (suspend-to-RAM) work which Li Shaohua
    <shaohua.li@intel.com> is doing)
    
    The following provides i386 architecture support for safely unregistering and
    registering processors during runtime, updated for the current -mm tree.  In
    order to avoid dumping cpu hotplug code into kernel/irq/* i dropped the
    cpu_online check in do_IRQ() by modifying fixup_irqs().  The difference being
    that on cpu offline, fixup_irqs() is called before we clear the cpu from
    cpu_online_map and a long delay in order to ensure that we never have any
    queued external interrupts on the APICs.  There are additional changes to s390
    and ppc64 to account for this change.
    
    1) Add CONFIG_HOTPLUG_CPU
    2) disable local APIC timer on dead cpus.
    3) Disable preempt around irq balancing to prevent CPUs going down.
    4) Print irq stats for all possible cpus.
    5) Debugging check for interrupts on offline cpus.
    6) Hacky fixup_irqs() to redirect irqs when cpus go off/online.
    7) play_dead() for offline cpus to spin inside.
    8) Handle offline cpus set in flush_tlb_others().
    9) Grab lock earlier in smp_call_function() to prevent CPUs going down.
    10) Implement __cpu_disable() and __cpu_die().
    11) Enable local interrupts in cpu_enable() after fixup_irqs()
    12) Don't fiddle with NMI on dead cpu, but leave intact on other cpus.
    13) Program IRQ affinity whilst cpu is still in cpu_online_map on offline.
    
    Signed-off-by: Zwane Mwaikambo <zwane@linuxpower.ca>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit a13db56624c2a9d6c0dae0a693b25b0e58de9ea3
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Sat Jun 25 14:54:48 2005 -0700

    [PATCH] CPU hotplug: fix hpet sectioning
    
    With hpet enabled, cpu hotplug uses some routines marked with __init.
    
    Signed-off-by: Shaohua Li<shaohua.li@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit a8ab26fe5bfeef43bdcde5182ca051ae0647607e
Author: Andi Kleen <ak@suse.de>
Date:   Sat Apr 16 15:25:19 2005 -0700

    [PATCH] x86_64: Switch SMP bootup over to new CPU hotplug state machine
    
    This will allow hotplug CPU in the future and in general cleans up a lot of
    crufty code.  It also should plug some races that the old hackish way
    introduces.  Remove one old race workaround in NMI watchdog setup that is not
    needed anymore.
    
    I removed the old total sum of bogomips reporting code.  The brag value of
    BogoMips has been greatly devalued in the last years on the open market.
    
    Real CPU hotplug will need some more work, but the infrastructure for it is
    there now.
    
    One drawback: the new TSC sync algorithm is less accurate than before.  The
    old way of zeroing TSCs is too intrusive to do later.  Instead the TSC of the
    BP is duplicated now, which is less accurate.
    
    akpm:
    
    - sync_tsc_bp_init seems to have the sense of `init' inverted.
    
    - SPIN_LOCK_UNLOCKED is deprecated - use DEFINE_SPINLOCK.
    
    Cc: <rusty@rustcorp.com.au>
    Cc: <mingo@elte.hu>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit ebfcaa96fccc01301a577c5c56a5f00543cf167e
Author: Andi Kleen <ak@suse.de>
Date:   Sat Apr 16 15:25:18 2005 -0700

    [PATCH] x86_64: Rename the extended cpuid level field
    
    It was confusingly named.
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    DESC
    x86_64: Switch SMP bootup over to new CPU hotplug state machine
    EDESC
    From: "Andi Kleen" <ak@suse.de>
    
    This will allow hotplug CPU in the future and in general cleans up a lot of
    crufty code.  It also should plug some races that the old hackish way
    introduces.  Remove one old race workaround in NMI watchdog setup that is not
    needed anymore.
    
    I removed the old total sum of bogomips reporting code.  The brag value of
    BogoMips has been greatly devalued in the last years on the open market.
    
    Real CPU hotplug will need some more work, but the infrastructure for it is
    there now.
    
    One drawback: the new TSC sync algorithm is less accurate than before.  The
    old way of zeroing TSCs is too intrusive to do later.  Instead the TSC of the
    BP is duplicated now, which is less accurate.
    
    Cc: <rusty@rustcorp.com.au>
    Cc: <mingo@elte.hu>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>
